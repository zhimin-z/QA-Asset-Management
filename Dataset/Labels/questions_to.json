[
    {
        "Question_title":"Does Amazon SageMaker RL support heterogenous clusters?",
        "Question_creation_time":1593179712000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE59_Oro0SGKaOIdZNmySiw\/does-amazon-sage-maker-rl-support-heterogenous-clusters",
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Does SageMaker RL support heterogenous clusters? I'd like to have our training to run on GPU and and SageMaker RL, and our inferences to run on CPUs.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SQL Server driver issue on notebook instance running on AWS SageMaker",
        "Question_creation_time":1667244851129,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeC8hq97nSzKHHmMKbCPxww\/sql-server-driver-issue-on-notebook-instance-running-on-aws-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using aws sagemaker notebook instance to run jupyter notebook. I have a MS SQL Server DB 2019 db that I am trying to connect to from the notebook. Notebook instance is running on Amazon Linux 2, Jupyter Lab 1 platform.\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport json\nimport os\n\n\n# sql database\nimport pyodbc\nconnection = pyodbc.connect(\n                              'Driver={SQL Server};'\n                              'Server=sname;'\n                              'Database=dbname;'\n                              'Trusted_Connection=True;'\n                           )\n\ncursor = connection.cursor()\n\n\nI get an error, likely because the driver is not installed on the instance.\n\nError                                     Traceback (most recent call last)\n\/tmp\/ipykernel_20407\/3026941781.py in <cell line: 12>()\n     10 # sql database\n     11 import pyodbc\n---> 12 connection = pyodbc.connect(\n     13                               'Driver={SQL Server};'\n     14                               'Server=sname;'\n\nError: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)\")\n\n\nhow do I install the driver on sagemaker instance and resolve this issue?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Question_creation_time":1604678225000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Question_upvote_count":0.0,
        "Question_view_count":105.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass values for a \"shap_baseline\" if we have categorical values (string values) as features in classsagemaker.clarify.SHAPConfig method.",
        "Question_creation_time":1663938111276,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVdzi-7h5RAapCYJUehzuZw\/how-to-pass-values-for-a-shap-baseline-if-we-have-categorical-values-string-values-as-features-in-classsagemaker-clarify-shap-config-method",
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"using this documentation i passing a single row as to shap_baseline parameter to implement explainability monitoring , a similar implementation of what is done in in this github repo implementation. if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If i ignore the shap_baseline (as it is optional), the schedule is taking forever to run. Help of any kind is really appreciated.\n\nthanks for your time and effort :)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Train machine learning model using reserved instance",
        "Question_creation_time":1641871148701,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cant generate XGBoost training report in sagemaker, only profiler_report.",
        "Question_creation_time":1657222531986,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskI-0YIvQ_2GRSkzyGiD2A\/cant-generate-xg-boost-training-report-in-sagemaker-only-profiler-report",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to generate the XGBoost training report to see feature importances however the following code only generates the profiler report.\n\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker\/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation\/val.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('..\/Data\/Base_Model_Data\/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, \"validation\": s3_input_val}, wait=True)\n\n\nWhen Checking the output path via:\n\nrule_output_path = xgb.output_path + \"\/\" + xgb.latest_training_job.job_name + \"\/rule-output\"\n! aws s3 ls {rule_output_path} --recursive\n\n\nWhich Outputs:\n\n2022-07-07 18:40:27     329715 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.html\n2022-07-07 18:40:26     171087 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.ipynb\n2022-07-07 18:40:23        191 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/BatchSize.json\n2022-07-07 18:40:23        199 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/CPUBottleneck.json\n2022-07-07 18:40:23        126 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/Dataloader.json\n2022-07-07 18:40:23        127 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/GPUMemoryIncrease.json\n2022-07-07 18:40:23        198 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/IOBottleneck.json\n2022-07-07 18:40:23        119 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LoadBalancing.json\n2022-07-07 18:40:23        151 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LowGPUUtilization.json\n2022-07-07 18:40:23        179 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/MaxInitializationTime.json\n2022-07-07 18:40:23        133 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallFrameworkMetrics.json\n2022-07-07 18:40:23        465 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallSystemUsage.json\n2022-07-07 18:40:23        156 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/StepOutlier.json\n\n\nAs you can see only the profiler report in created which does not interest me. Why isn't there a CreateXGBoostReport folder generated with the training report? How do I generate this\/what am I missing?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to increase the storage of host instance",
        "Question_creation_time":1576741179000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeVh4VvD6R-eIhRYY6K8dsw\/how-to-increase-the-storage-of-host-instance",
        "Question_upvote_count":0.0,
        "Question_view_count":308.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"The parameters of a big neural network model can be huge. But the largest storage size of a host instance is only 30G, according to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html. Is there a way to increase the storage volume? I have a model (embeddings) that is very close to 30G and caused a no space error when deploying.\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ground Truth - Progress Not Updating?",
        "Question_creation_time":1550529412000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUf9EnD7gzRlu86yy1Zlry7g\/ground-truth-progress-not-updating",
        "Question_upvote_count":1.0,
        "Question_view_count":117.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I've just configured my first Ground Truth labelling job, 1737 images assigned to a private workforce.\n\nWhen I first logged in as a worker I was able to label 10 images as a test run, and the Ground Truth \"Labeling job summary\" in the AWS Console shows 10\/1737 images labelled. So far so good.\n\nI then returned to the labelling job and worked through approx 50 images and then used the 'Stop working' button to finish my labelling session.\n\nDespite that work, the progress shown in \"Labelling job summary\" has not been updated with those additional labeled images, even 48+ hours after the work was done.\n\nHow can I confirm whether the image labelling is happening correctly? I don't see any error messages that might help debug the situation, and I certainly don't want to get other workers to label 1700+ images without being sure that the labelling data is being saved.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Ground Truth Plus Available Medical Expertise",
        "Question_creation_time":1668530819224,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1vINii7DRkGK-4klJOq7wA\/aws-ground-truth-plus-available-medical-expertise",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"On Dec 1, 2021, AWS put out a press release regarding SageMaker Ground Truth Plus that contained the statement:\n\nTo get started, customers simply point Amazon SageMaker Ground Truth Plus to their data source in Amazon Simple Storage Service (Amazon S3) and provide their specific labeling requirements (e.g. instructions for how medical experts should label anomalies in radiology images of lungs).\n\nCan AWS provide medical experts for labeling medical data? Or am I misinterpreting this statement and the services included in this \"turnkey\" solution. (BTW, I've already built and tested a custom segmentation task for SageMaker Ground Truth and am looking for \"expert\" labeling.)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Received server error (0) from model when hosting",
        "Question_creation_time":1562557558000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUut1Vw-EPQUWVOy6cxmOmXw\/received-server-error-0-from-model-when-hosting",
        "Question_upvote_count":0.0,
        "Question_view_count":440.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI was trying to deploy my trained model to the endpoint and I was given a ModelError.\n\"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See Link:https:\/\/ap-southeast-2.console.aws.amazon.com\/cloudwatch\/home?region=ap-southeast-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/ss-notebook-treemapping-2019-07-08-00-56-39-825 in account 125017970330 for more information.\"\n\nI'm not sure what caused this issue and couldn't figure out how latency metrics in the CloudWatch would be useful in this case. Does anyone know what the approach is to solve this issue? It would also be great to know why this happens. Thanks in advance for any help!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use an Augmented Manifest File for AWS SageMaker Ground Truth?",
        "Question_creation_time":1546562747000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1LLbT-AYQDO-XXrjPUFl9w\/how-to-use-an-augmented-manifest-file-for-aws-sage-maker-ground-truth",
        "Question_upvote_count":0.0,
        "Question_view_count":98.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hey,\n\nI'm trying to use Ground Truth to do image classification but with a different set of label options for each image. I have the custom labeling task template and pre-\/post-labeling Lambda functions set up and I figured I could pass in the labels through the manifest file.\n\nMy issue is that the Ground Truth job ignores the attributes in the manifest file that are not \"source-ref\" (or \"source\"). This causes the pre-processing Lambda function to fail because the request it is passed only contains the \"source-ref\" attribute, but the Lambda function also references a different attribute. Are augmented manifest files supported for Ground Truth and if they are, how can I make use of the extra attributes?\n\nReferences:\nGround Truth Input Data: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\nSageMaker Augmented Manifest Files: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\n\nExample:\n\nA normal Ground Truth manifest file:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\"}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\"}\r\n...\n\n\nWhat I want to be able to use:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\",\"labels\":[\"pen\",\"pencil\",\"stick\"]}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\",\"labels\":[\"tv\",\"laptop\",\"phone\"]}\r\n...",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using AWS services to perform pet face recognition",
        "Question_creation_time":1601645649000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2S-z85hhSr-XVecDxE3ihw\/using-aws-services-to-perform-pet-face-recognition",
        "Question_upvote_count":0.0,
        "Question_view_count":132.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to build a camera that automatically recognizes a specific pet with image or video recognition. What AWS service can I use to identify an individual pet (not just the pet type). I've tried to use AWS Rekognition, but it can only differentiate between animal types, race, or color. Amazon SageMaker could be another option to create a completely new mode, but is very costly. What other AWS services can I use to identify specific pets?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Question_creation_time":1594231696000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Question_upvote_count":0.0,
        "Question_view_count":519.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done. Any links or pointers would be greatly appreciated.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is any component of Sagemaker still involved in production inferencing",
        "Question_creation_time":1668489528537,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVpIbYCjMTzOPTHJFfh9rnQ\/is-any-component-of-sagemaker-still-involved-in-production-inferencing",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Are any components of the SageMaker Engine involved still during production ( inference) , I had a discussion with someone saying Sagemaker has nothing to do with production anymore because everything is deployed , containerized and executed somewhere in a VM or cluster., I think there are some instances the main engine is working in production and please correct me if i am wrong 1. Billing records - have to be generated -so something is connected to the engine ? does the model help here or is it just pure EC2 etc compute measured ? Who manages the endpoints ( server less, and asynch) ? if we need to use Sage maker monitor this is also alive during production. Question is What main components are active in production outside model or does mode communicate to engine still? As an addendum someone mentioned to me if it is deployed to EC2 or K8s yes it is disconnected from the main services in production you CANNOT use Model monitor in this scenario? --- but if you deploy to SAGEMAKER INFERENCE? you can use model monitor in prod. You deploy to an endpoint for real time... What is SageMAker Inference - as a concept or as some physical thing because there are 71 options in Sagemaker inference ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Notebook - SSL failed validation when Boto3 session.client(verify=False)",
        "Question_creation_time":1651530941407,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmNuNjDSJSSyOUJ8dxVP-hg\/sagemaker-notebook-ssl-failed-validation-when-boto-3-session-client-verify-false",
        "Question_upvote_count":0.0,
        "Question_view_count":456.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In a sagemaker notebook with an associated git repository, when I try to create a boto3 session client using verify = False, I get the following : SSLError: SSL validation failed for {service_name } [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\n\nThe error resolves if I allow the default value of verify = None (meaning SSL certs are verified). My problem is that this session is created as part of a function call on the associated git repository and I don't want to change the behavior in the repo. I don't understand why this error occurs only when I specify not to validate SSL certificates. Any ideas of what explains this behavior?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker - All metrics in statistics.json by Model Quality Monitor are \"0.0 +\/- 0.0\", but confusion matrix is built correctly for multi-class classification!!",
        "Question_creation_time":1645965956086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOOz6SJnzR7-VDglJ1rAW8Q\/sage-maker-all-metrics-in-statistics-json-by-model-quality-monitor-are-0-0-0-0-but-confusion-matrix-is-built-correctly-for-multi-class-classification",
        "Question_upvote_count":1.0,
        "Question_view_count":27.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have scheduled an hourly model-quality-monitoring job in AWS SageMaker. both the jobs, ground-truth-merge and model-quality-monitoring completes successfully without any errors. but, all the metrics calculated by the job are \"0.0 +\/- 0.0\" while the confustion matrix gets calculated as expected.\n\nI have done everything as mentioned in this notebook for model-quality-monitoring from sagemaker-examples with very few changes and they are:\n\nI have changed the model from xgboost churn to model trained on my data.\nmy input to the endpoint was csv like in the example-notebook, but output was json.\ni have changed the problem-type from BinaryClassfication to MulticlassClassification wherever necessary.\n\nconfustion matrix was built successfully, but all metrics are 0 for some reason. So, I would like the monitoring job to calculate the multi-classification metrics on data properly.\n\nAll Logs\n\nHere's the statistics.json file that model-quality-monitor saved to S3 with confustion matrix built, but with 0s in all the metrics:\n\n{\n  \"version\" : 0.0,\n  \"dataset\" : {\n    \"item_count\" : 4432,\n    \"start_time\" : \"2022-02-23T03:00:00Z\",\n    \"end_time\" : \"2022-02-23T04:00:00Z\",\n    \"evaluation_time\" : \"2022-02-23T04:13:20.193Z\"\n  },\n  \"multiclass_classification_metrics\" : {\n    \"confusion_matrix\" : {\n      \"0\" : {\n        \"0\" : 709,\n        \"2\" : 530,\n        \"1\" : 247\n      },\n      \"2\" : {\n        \"0\" : 718,\n        \"2\" : 497,\n        \"1\" : 265\n      },\n      \"1\" : {\n        \"0\" : 700,\n        \"2\" : 509,\n        \"1\" : 257\n      }\n    },\n    \"accuracy\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_recall\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_precision\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f0_5\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f1\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f2\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"accuracy_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_recall_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_precision_best_constant_classifier\" : {\n      \"value\" : 0.1124185852154987,\n      \"standard_deviation\" : 0.0021869336610830254\n    },\n    \"weighted_f0_5_best_constant_classifier\" : {\n      \"value\" : 0.12965524348784485,\n      \"standard_deviation\" : 0.0024239410000317335\n    },\n    \"weighted_f1_best_constant_classifier\" : {\n      \"value\" : 0.16838092925822584,\n      \"standard_deviation\" : 0.0028615098045768348\n    },\n    \"weighted_f2_best_constant_classifier\" : {\n      \"value\" : 0.24009212108475822,\n      \"standard_deviation\" : 0.003326031863819311\n    }\n  }\n}\n\n\nHere's how couple of lines of captured data looks like(prettified for readability, but each line has no tab spaces as shown below) :\n\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,1,628,210,30\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Transfer\\\",\\\"prediction\\\":2,\\\"probabilities\\\":[0.228256680901919,0.0,0.7717433190980809]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"a7cfba60-39ee-4796-bd85-343dcadef024\",\n        \"inferenceId\": \"5875\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,3,628,286,240\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.99,0.005,0.005]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"7391ac1e-6d27-4f84-a9ad-9fbd6130498a\",\n        \"inferenceId\": \"5876\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n\n\nHere's couple of lines from my ground-truths that I have uploaded to S3 look like(prettified for readability, but each line has no tab spaces as shown below):\n\n{\n  \"groundTruthData\": {\n    \"data\": \"0\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"1\"\n  },\n  \"eventVersion\": \"0\"\n}\n{\n  \"groundTruthData\": {\n    \"data\": \"1\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"2\"\n  },\n  \"eventVersion\": \"0\"\n},\n\n\nHere's couple of lines from the ground-truth-merged file look like(prettified for readability, but each line has no tab spaces as shown below). this file is created by the ground-truth-merge job, which is one of the two jobs that model-quality-monitoring schedule runs:\n\n{\n  \"eventVersion\": \"0\",\n  \"groundTruthData\": {\n    \"data\": \"2\",\n    \"encoding\": \"CSV\"\n  },\n  \"captureData\": {\n    \"endpointInput\": {\n      \"data\": \"1,2,1050,37,1095\",\n      \"encoding\": \"CSV\",\n      \"mode\": \"INPUT\",\n      \"observedContentType\": \"text\/csv\"\n    },\n    \"endpointOutput\": {\n      \"data\": \"{\\\"label\\\":\\\"Return_to_owner\\\",\\\"prediction\\\":1,\\\"probabilities\\\":[0.14512373737373732,0.6597074314574313,0.1951688311688311]}\\n\",\n      \"encoding\": \"JSON\",\n      \"mode\": \"OUTPUT\",\n      \"observedContentType\": \"application\/json\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"c9e21f63-05f0-4dec-8f95-b8a1fa3483c1\",\n    \"inferenceId\": \"4432\",\n    \"inferenceTime\": \"2022-02-23T04:00:00Z\"\n  }\n}\n{\n    \"eventVersion\": \"0\",\n    \"groundTruthData\": {\n        \"data\": \"1\",\n        \"encoding\": \"CSV\"\n    },\n    \"captureData\": {\n        \"endpointInput\": {\n            \"data\": \"0,2,628,5,90\",\n            \"encoding\": \"CSV\",\n            \"mode\": \"INPUT\",\n            \"observedContentType\": \"text\/csv\"\n        },\n        \"endpointOutput\": {\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.7029623691085284,0.0,0.29703763089147156]}\\n\",\n            \"encoding\": \"JSON\",\n            \"mode\": \"OUTPUT\",\n            \"observedContentType\": \"application\/json\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"5f1afc30-2ffd-42cf-8f4b-df97f1c86cb1\",\n        \"inferenceId\": \"4433\",\n        \"inferenceTime\": \"2022-02-23T04:00:01Z\"\n    }\n}\n\n\nSince, the confusion matrix was constructed properly, I presume that I fed the data to sagemaker-model-monitor the right-way. But, why are all the metrics 0.0, while confustion-matrix looks as expected?\n\nEDIT 1:\nLogs for the job are available here.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Question_creation_time":1649205572690,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-max-concurrent-transforms-in-setting-up-a-sagemaker-batch-transform",
        "Question_upvote_count":0.0,
        "Question_view_count":152.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom packages in Sagemaker studio",
        "Question_creation_time":1592469976000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Question_upvote_count":0.0,
        "Question_view_count":266.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio? When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"tensorboard with custom docker image without notebook",
        "Question_creation_time":1588924703000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdIJGIVDEQSm-9eX3jo0ubA\/tensorboard-with-custom-docker-image-without-notebook",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\nIs it possible to use tensorboard with a custom docker image without using a notebook ? Is there any other method to monitor the training process ? I'm using the tensorflow object detection API and currently exposing metrics (only loss) from cloudwatch using a regex but I'd like a more detailed way like tensorboar.. is that possible ??\nThanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_creation_time":1556295446000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sage-maker-model-to-io-t-greengrass-in-different-account",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"is it possible to create steps within the sagemaker pipeline via cloudformation?",
        "Question_creation_time":1663606095949,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3aphi0KgRNyk6AcB9c9Spg\/is-it-possible-to-create-steps-within-the-sagemaker-pipeline-via-cloudformation",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am experimenting with sagemaker studio, while i was able to create the sagemaker studio domain and user profiles via cloudformation. I was wondering if, it was possible to create sagemaker projects and other resources like model registry group and steps for preprocessing , training ... via cloudformation? if is it possible , are there any samples, examples around this, if it is not supported or is not possible via cfn , may be help me on how can create link this to existing user and domain in studio .",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"can a sagemaker endpoint be made public?",
        "Question_creation_time":1664239929738,
        "Question_link":"https:\/\/repost.aws\/questions\/QU96EIw32SSxmx3plPtUUcYA\/can-a-sagemaker-endpoint-be-made-public",
        "Question_upvote_count":1.0,
        "Question_view_count":40.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"is there a way to make a sagemaker endpoint be accessible publicly ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Question_creation_time":1605283057000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sage-maker-sdk-with-f-sx-as-data-source",
        "Question_upvote_count":1.0,
        "Question_view_count":120.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What value should I set for the directory_path parameter in FileSystemInput for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n\nMy FSx ID is fs-0684xxxxxxxxxxx.\nMy FSx has the mount name lhskdbmv.\nThe FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\nAttempt 1:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='lhskdbmv',\n    file_system_access_mode='ro')\n\n\nResult:\n\nestimator.fit(fs) returns ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.\n\nAttempt 2:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='\/',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.\n\nAttempt 3:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='fsx',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN]",
        "Question_creation_time":1661681019239,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBY-fIUMuRDqwBmObCn8GqQ\/no-such-file-or-directory-opt-ml-input-data-test-revenue-train-csv-sagemaker-sm-channel-train",
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to deploy my RandomForestClassifier on Amazon Sagemaker using Python SDK. I have been following this example https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-script-mode\/sagemaker-script-mode.ipynb but keep getting an error that the train file was not found. I think the file were not uploaded to the correct channel. When I run the script as follows it works fine.\n\n! python script_rf.py --model-dir .\/ \\\n                   --train .\/ \\\n                   --test .\/ \\\n\n\nThis is my script code:\n\n# inference functions ---------------\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--max_depth', type=int, default=2)\n    parser.add_argument('--n_estimators', type=int, default=100)\n    parser.add_argument('--random_state', type=int, default=0)\n    \n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='revenue_train.csv')\n    parser.add_argument('--test-file', type=str, default='revenue_test.csv')\n    \n    args, _ = parser.parse_known_args()\n    \n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    if len(train_df) == 0:\n        raise ValueError(('There are no files in {}.\\n').format(args.train, \"train\"))\n\n    print('building training and testing datasets')\n    attributes = ['available_minutes_100','ampido_slots_amount','ampido_slots_amount_100','ampido_slots_amount_200','ampido_slots_amount_300','min_dist_loc','count_event','min_dist_phouses','count_phouses','min_dist_stops','count_stops','min_dist_tickets','count_tickets','min_dist_google','min_dist_psa','count_psa']\n    X_train = train_df[attributes]\n    X_test = test_df[attributes]\n    y_train = train_df['target']\n    y_test = test_df['target']\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        max_depth =args.max_depth, n_estimators = args.n_estimators)\n    \n    model.fit(X_train, y_train)\n     \n    # persist model\n    path = os.path.join(args.model_dir, \"model_rf.joblib\")\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n    \n    # print accuracy and confusion matrix \n    print('validating model')\n    y_pred=model.predict(X_test) \n    print('Confusion Matrix:')\n    result = confusion_matrix(y_test, y_pred)\n    print(result)\n    print('Accuracy:')\n    result2 = accuracy_score(y_test, y_pred)\n    print(result2)\n\n\nthe error is raised in the train_df line of the script (FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv').\n\nI tried specifying the input parameters:\n\n# change channel input dirs \ninputs = {\n    \"train\": \"ampido-exports\/production\/revenue_train\",\n    \"test\": \"ampido-exports\/production\/revenue_test\",\n}\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\nhyperparameters = {\"max_depth\": 2, 'random_state':0, \"n_estimators\": 100}\n\nif enable_local_mode_training:\n    train_instance_type = \"local\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nelse:\n    train_instance_type = \"ml.c5.xlarge\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": train_instance_type,\n    \"instance_count\": 1,\n    \"hyperparameters\": hyperparameters,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\",\n    'channel_input_dirs' : inputs\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n\nbut i still get the error FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ClientError: Data download failed:Unable to create download dir",
        "Question_creation_time":1633390841000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhODEUiiMT2y0cBJixQ3ofA\/client-error-data-download-failed-unable-to-create-download-dir",
        "Question_upvote_count":0.0,
        "Question_view_count":194.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\nI searched for this error in the forum read through the first 6 or 7 forum pages with no luck. Im able to build models in Sagemaker Studio, but when I try in Sagemaker I get this error:\n\nClientError: Data download failed:Unable to create download dir \/opt\/ml\/checkpoints\/tc19\/preprocessed-data\/header\n\nAnybody know how to clear this ?\n\nThank in advance.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker endpoint creation fails for Multi Model",
        "Question_creation_time":1649256875061,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2bUNsPi3Rgautb0ZycrziA\/sage-maker-endpoint-creation-fails-for-multi-model",
        "Question_upvote_count":0.0,
        "Question_view_count":119.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When using scikit to create multi model, it throws an exception, but when in single model it works.\n\nComplains about model_fn implementation or ping issues, any tips on how to fix this?\n\ne.g container={\\n\", \" 'Image' : image_uri,\", \" 'Mode': 'MultiModel',\", \" 'ModelDataUrl': 's3:\/\/somepatch\/with\/all\/models\/,\", \" 'Environment': {'SAGEMAKER_SUBMIT_DIRECTORY': mme_artifacts_path,\", \" 'SAGEMAKER_PROGRAM': 'inference.py'} \"\n\nFile \"\/miniconda3\/bin\/serve\", line 8, in <module> sys.exit(serving_entrypoint()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 144, in serving_entrypoint start_model_server() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving_mms.py\", line 124, in start_model_server modules.import_module(serving_env.module_dir, serving_env.module_name) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 263, in import_module six.reraise(_errors.ImportModuleError, _errors.ImportModuleError(e), sys.exc_info()[2]) File \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 702, in reraise raise value.with_traceback(tb) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 258, in import_module module = importlib.import_module(name) File \"\/miniconda3\/lib\/python3.7\/importlib\/init.py\", line 118, in import_module if name.startswith('.'):",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trigger and\/or monitor Automated Data Labeling Job",
        "Question_creation_time":1659628069730,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5eZvKOO5R4y3FAXlbpUqiw\/trigger-and-or-monitor-automated-data-labeling-job",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am following the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html#sms-auto-labeling-ec2 and I have created a Ground Truth labeling job for object detection. I also enabled automated data labeling when creating the job. I have roughly 5000 images in my dataset. I have manually labelled ( by creating myself as a worker ) 129 of these images. How many do I need to label before the automated labeling job triggers? How do I know if a job was triggered\/succeeded\/failed etc?\n\nThanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Registered for aws sagemaker studio but not able to create account",
        "Question_creation_time":1652237658633,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSB0wO0OiQ_irUdCx9ppjbg\/registered-for-aws-sagemaker-studio-but-not-able-to-create-account",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi I received my aws sagemaker studio approval to create an account 1 hr ago When I go to the link to create an account it says my email has not been approved Even though I have an email to the contrary\n\nHow do I contact amazon to sort this out?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Question_creation_time":1649794559479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sage-maker-multi-model-end-point-and-inference-data-capture-feature",
        "Question_upvote_count":0.0,
        "Question_view_count":175.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error. See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\" Theoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()) print(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig( enable_capture = True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Question_creation_time":1644862112974,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deep-ar-in-aws-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":96.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Question_creation_time":1644763229468,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sage-maker-canvas",
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to choose ml.g4dn.* instances in sagemaker processing jobs",
        "Question_creation_time":1643215786791,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXqikCZktSFywwXL14PWcYg\/how-to-choose-ml-g-4-dn-instances-in-sagemaker-processing-jobs",
        "Question_upvote_count":1.0,
        "Question_view_count":302.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have to perform some data manipulation for which the sagemaker \"processing job\" would fit perfectly. Such jobs would benefit from GPU and thus I was looking to use instances from the ml.g4dn family for cost efficiency. Unfortunately, I cant see them available in the dropdown when creating a processing job from the aws dashboard, only when creating training jobs. I previously requested the limit increase to the aws support, and i was told it was not necessary and up to 20 instances could be run in the chosen region.\n\nAm I missing anything? do i have to enable the instance family somewhere else?\n\nthanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"invoke_endpoint error in Lambda: StreamingBody is not JSON serializable",
        "Question_creation_time":1550644604000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6wwr5n1XTuWtQjkbCGbCOA\/invoke-endpoint-error-in-lambda-streaming-body-is-not-json-serializable",
        "Question_upvote_count":0.0,
        "Question_view_count":763.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm writing a Lambda function that invokes an endpoint:\n\nruntime= boto3.Session().client('runtime.sagemaker')\r\npayload = {\"data\": [\"McDonalds\"]}\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\n                                       ContentType='application\/json',\r\n                                       Body=json.dumps(payload))\n\n\nIt returns this error\n\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f59e40acc50> is not JSON serializable\n\n\nI tried this exact function in SageMaker notebook and it works but it doesn't work in Lambda. Can someone please help me?\n\nEdited by: aurelius on Feb 19, 2019 10:38 PM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_creation_time":1606707121000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Serverless Inference - Limit number of workers",
        "Question_creation_time":1642602434394,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWYP78UdYQseoErcj4kjiug\/serverless-inference-limit-number-of-workers",
        "Question_upvote_count":0.0,
        "Question_view_count":161.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We've deployed a HuggingFace model to Sagemaker as a serverless endpoint. We set memory to be 6GB and max concurrency to be 1. With these settings, we keep getting errors when we call invoke_endpoint. Not all the time, but about 60% of the time...\n\nWhen we check the logs and metrics, we see that the memory has gone up to almost 100%. We also see that, since the machine has 6 CPUs, if starts 6 workers. We believe this could be the cause of the problem. How can se set the number of workers?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can we run a python script in Sagemaker using boto3 from a local machine?",
        "Question_creation_time":1646819726632,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjnuGv6KCRaS9BCxzVgCYyA\/can-we-run-a-python-script-in-sagemaker-using-boto-3-from-a-local-machine",
        "Question_upvote_count":0.0,
        "Question_view_count":808.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Here's what I am trying to do: In my application that resides outside aws, I take some user inputs, and trigger scripts that reside inside Sagemaker notebook instance. I am able to start or create a new instance using boto3, and also use lifecycle configuration to run some starter script while the instance turns on. But I want to run multiple scripts in short intervals based on user inputs, so I don't want to restart my instance each time with a new lifecycle configuration script. I am trying to find if there is a way to execute shell commands in sagemaker using boto3 (or any other way).",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio default server failing",
        "Question_creation_time":1654597460713,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJhSBFYdSQ8aGHNA3DjMk-A\/sagemaker-studio-default-server-failing",
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I am trying to start Amazon SageMaker Studio today but without success. The default server is failing to start. On CloudWatch I only see SIGKILL messages as errors:\n\n2022-06-07T10:55:01.523+02:00\t2022-06-07 08:55:00,466 WARN killing 'jupyterlabserver' (11) with SIGKILL\n2022-06-07T10:55:04.524+02:00\t2022-06-07 08:55:01,470 INFO waiting for jupyterlabserver-listener, jupyterlabserver to die\n\n\nOn the Apps section I see the default server in status \"Failed\", and I cannot do anything, not even delete it.\n\nI cannot use the service right now. Is it maybe related to the new release of Jupyter Lab v3.0?\n\nThanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"HELP!!!! Amazon SageMaker not writing best optimal route based on Genetic Algorithm to 2nd Output Dynamo Database(am stucked here&incurring dollar charges with no progress -Error Screenshot available)",
        "Question_creation_time":1666611225424,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDsciSjamQ5WZR_FHC1u0vQ\/help-amazon-sage-maker-not-writing-best-optimal-route-based-on-genetic-algorithm-to-2nd-output-dynamo-database-am-stucked-here-incurring-dollar-charges-with-no-progress-error-screenshot-available",
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My Challenges is this: I used CloudFormation template to deploy 2 Dynamo DB (Input and Output) and 1 IAM role to use AWS-managed Lamba Function for Genetic Algorithm, so Amazon SageMaker (using Jupyter Notebook on AWS) is meant to write the locations (X and Y coordinates into the input Dynamo DB (Successful), while the Docker file to Docker Image to Docker Container is also to be run by Amazon SageMaker to write the best Optimal Route based on Genetic Algorithm (Mutation, genomes and generation transfer mode of operation) to the 2nd Dynamo DB (Unsuccessful) and this is where I am stucked, have read a lot of materials and research a lot and even reached out to some Amazon AWS Community but they could not resolve it, Please will be glad if repost.aws can help please (Error Screenshot Available)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass credentials in Glue Notebooks - Interactive Session using Magic Commands to override the 1 hour temporary token expiration",
        "Question_creation_time":1666635956843,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0wxONrToTd2vvUzwsmt3cw\/how-to-pass-credentials-in-glue-notebooks-interactive-session-using-magic-commands-to-override-the-1-hour-temporary-token-expiration",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, If I start the notebook via the console the token\/credentials expire after one hour, Gives the following error \"Exception encountered while retrieving session: An error occurred (ExpiredTokenException) when calling the GetSession operation: The security token included in the request is expired \" .\n\nI am guessing this is happening since its using temporary credentials by default. How does one pass the credentials using the magic commands such that credentials do not expire or workaround?\n\nI can run notebooks locally using the profile in local .aws folder, but can't use TAGS for the sessions to account for costs.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker - S3 bucket access when logged into us-east-1",
        "Question_creation_time":1652377507550,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgOCxivDSgiiHh-HZ9aYYA\/sagemaker-s-3-bucket-access-when-logged-into-us-east-1",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My IAM works, but when I login, it logs me into us-east-1. My project is on the S3 instance (sagemaker tool). How can I access buckets in S3 if I am logged into us-east-1 by default.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can't Import Modules In Sagemaker Jupyter Notebook",
        "Question_creation_time":1660764982506,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBSiq8Lx5St-1D8CT3k328g\/cant-import-modules-in-sagemaker-jupyter-notebook",
        "Question_upvote_count":0.0,
        "Question_view_count":117.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I've tried to use the pip install librosa command within a jupyter notebook, yet I get a modulenotfounderror. Is there a place where I can use pip install librosa?\n\nThanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to log error\/messages in while running a sagemaker batch transform job?",
        "Question_creation_time":1653699481978,
        "Question_link":"https:\/\/repost.aws\/questions\/QUNirOT1cMSfig9ANtgmZMpg\/how-to-log-error-messages-in-while-running-a-sagemaker-batch-transform-job",
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"i'm using a hugging face model and a container to create a batch transform job in sagemaker. i have a custom inference code and in the output_fn function i'm returning json_dumps(prediction). I'm using print(prediction) to see, if i can see it in the cloudwatch logs to find out type and what prediction is. how can log these messages . Also, the inference output i get is in the form below., i'm not sure why is it not a json object in each line instead it has square brackets. I want to use the filter to match the input and output in the batch job. I'm not sure how the output should look like , because i'm trying to associate input with output by using dataprocessing config as below. but i get an error. the documenation has example of csv not json. what should the output look like so that i can associate the input with output when they both are in json format.\n\n  \"DataProcessing\": {\n        \"JoinSource\": \"Input\"\n    },\n\n[ output text 1 ]\n[output text 2 ]\n\n# Serialize the prediction result into the desired response content type\ndef output_fn(prediction, accept=JSON_CONTENT_TYPE):\n    logger.info(\"Serializing the generated output.\")\n    if accept == JSON_CONTENT_TYPE:\n        output = json.dumps(prediction)\n        return output, accept\n    raise Exception(\"Requested unsupported ContentType in Accept: {}\".format(accept))",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cloudformation Deployment for Serverless Sagemaker Model Endpoint",
        "Question_creation_time":1655512915811,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo7p2DuabQaapi9C4_nPVZg\/cloudformation-deployment-for-serverless-sagemaker-model-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, i want to deploy a serverless model using CloudFormation.\n\nI've created the model, the endpoint configuration, and when I try to create the endpoint the script times out, because it can't find a saved model to attach to the endpoint (because I never trained one for this instance).\n\nI've tried to look around for a trainingjob cloudformation API, but there doesn't seem to be one.\n\nHow do I solve this issue?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_creation_time":1623820480000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU68gR5B3JRdqOEGlwE2-pnA\/an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation",
        "Question_upvote_count":0.0,
        "Question_view_count":831.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\nI received the following error message when I tried to send an array to my model:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from container-1 with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n<title>500 Internal Server Error<\/title> <h1>Internal Server Error<\/h1> <p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p> \". See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group\n\nI have created inference pipeline containing preprocessing and autoencoder model and deployed it to a single endpoint. Am trying to send raw data in text\/csv format. EX: \"39, 4, 9, 8, contact\"\n\nPlease help me out in this.\n\nMuch appreciated,\nKarthik",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio JupyterServer App does not load",
        "Question_creation_time":1656519218822,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5Da9xot8TwST0MP_8uRV2A\/sagemaker-studio-jupyter-server-app-does-not-load",
        "Question_upvote_count":0.0,
        "Question_view_count":85.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"After months of seamless work in SageMaker Studio, the JupyterServer App won't load for the last 4 days. The Control Panel shows that the JupyterServer is in \"Pending\" or \"Failed\" state after I try to launch the app. When clicking \"Launch app\", the screen shows that:\n\n\"The JupyterServer app default encountered a problem and was stopped.\"\nThe \"Restart Now\" button is visible, but pressing this results in the same behaviour. I created a new JupyterServer App and it experiences the same problem under that account. I use a different account for another project and the JupyterServer under that account works perfectly. I even mounted the EFS associated with the App on an EC2 instance and deleted some files to reduce the EFS volume but it did not help (it was 995 MB and as far as I know, 5GB is the default limit).\n\nI found a post stating the same problem from 2 years ago, but could not follow the advice to delete the app and create a new one, since the Delete option is not available in the Action dropdown (https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load). All apps that I create is \"default\".\n\nPlease help, how could I overcome this and access Jupyter Lab again? Thank you.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"XGBoost Reports Not Generated",
        "Question_creation_time":1638475570231,
        "Question_link":"https:\/\/repost.aws\/questions\/QUx_M71_2nQJSDp-I1mgbjDg\/xg-boost-reports-not-generated",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi!\n\nI have been trying to create a model using XGBoost, and was able to successfully run\/train the model. However, I have not been able to generate the training reports. I have included the rules parameter as follows: \"rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\".\n\nI am following this tutorial, but I am using objective: \"multi:softmax\" instead of the \"binary:logistic\" used in the example.\n\nWhen I run the model everything is fine but only the Profiler Report gets generated and I do not see the XGBoostReport under the rule-output folder. According to the tutorial it should be under the same file path.\n\nHere is my code for the model if it helps any:\n\ns3_output_location='s3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'xgboost_model')\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n\ntrain_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/train.csv\"), content_type=\"csv\"\n)\nvalidation_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/validation.csv\"), content_type=\"csv\"\n)\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=sagemaker.get_execution_role(),\n    instance_count=1,\n    instance_type=\"ml.c5.2xlarge\",\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=6,\n    objective='multi:softmax',\n    num_class=num_classes,\n    gamma=800,\n    num_round=250\n)\n\n\nAny help is appreciated! Thanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Input and Output interface for the CatBoost algorithm",
        "Question_creation_time":1658463993278,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0PVSBTSR4GvFO3E5FusCQ\/input-and-output-interface-for-the-cat-boost-algorithm",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"to set up CatBoost Classifier as a built-in algorithm, aws in this [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] suggested this notebook [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb] , my question is should I prepare inference file on top of the train.csv? if yes what is that and how it should be prepared?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Canvas failed to import the Redshift Data",
        "Question_creation_time":1641655668805,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCUdYWY0gSV6W60g7ArukXw\/sage-maker-canvas-failed-to-import-the-redshift-data",
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Actions:\n\nThe Redshift connection has been setup on SageMaker Canvas.\nThe Redshift already load the sample data (users, sales, etc)\nDrag and drop table 'users' to import pane.\nCheck the import preview can show the data of 'users'\nClick Import\n\nExpected result:\n\nThe dataset can be imported successfully\n\nActual result: Import failed with below details:\n\n{'message': \"Variable '$input' got invalid value None at 'input.uri'; Expected non-nullable type 'String!' not to be None.\", 'locations': [{'line': 1, 'column': 8}], 'path': None}\n\nPlease contact your admin. Request ID: 8b849887-b067-46fc-9be9-dd122e9c8874",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_creation_time":1562042043000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s-3-bucket-from-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":807.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.\n\nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:\n\nbucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_creation_time":1605279993000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Question_upvote_count":0.0,
        "Question_view_count":56.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Ground truth pdf annotation tool not rendering anything",
        "Question_creation_time":1649874349265,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzYkN4V8kRsWhT3ZSNJy9mQ\/sagemaker-ground-truth-pdf-annotation-tool-not-rendering-anything",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I have followed these docs https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/cer-annotation-pdf.html and have gotten to the point in which I have created the annotation task and I have uploaded several pdf's to a s3 bucket to be used for an annotation task so I can create a comprehend model. I put myself and a co-worker as annotators just so I can verify that I can set up the task properly and I only uploaded 37 pdf's. However when both of us log in and start the task, the webpage loads as the instructions tell us to however there is no pdf rendered (though I think I see it briefly flash on the screen before it goes blank) and there are also no entities to be selected as a part of the ui unlike how the documentation pictures the tool. I am trying to do named entity recognition and created this task with the full 25 entities I want to be able to label and Also another time with only 5 entities to label. However there seems to be something wrong with this native pdf annotation feature.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Help\/ideas wanted] Serverless Inference: Optimize cold start time",
        "Question_creation_time":1643639465200,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlakvrCXORXyNh7KZehiXKQ\/help-ideas-wanted-serverless-inference-optimize-cold-start-time",
        "Question_upvote_count":1.0,
        "Question_view_count":359.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We are using Sagemaker Serverless Inference, where the endpoint is wrapped with a Lambda that has a 30sec timeout (this timeout is not adjustable). Our cold start time of the model is quite above that (around 43sec). We load a model using Huggingface transformers and have a FLASK API for serving the model. The model size is around 1.75GB.\n\nAre there any guides on how to improve cold start and model loading time? Could we compile the weights differently beforehand for faster loading?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ground Truth label - a word at the end of the sentence getting split into 2 parts",
        "Question_creation_time":1668086625248,
        "Question_link":"https:\/\/repost.aws\/questions\/QU50Od2mJZTjyEcGuGds1-qQ\/ground-truth-label-a-word-at-the-end-of-the-sentence-getting-split-into-2-parts",
        "Question_upvote_count":0.0,
        "Question_view_count":15.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We are annotating the pdf for extrating NER, The targetted entity word at the end of the sentence getting split into 2 parts. First part stay at the end of the first line and second part coming in second line. While annotating the tool doesn't allow the dragging to next line.\n\nAre there any work arounds available ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker debugger built in rule CreateXgboostRule not generating report as expected",
        "Question_creation_time":1662562294513,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuZEDvbaeRfqOT_g2Sxlw7w\/sage-maker-debugger-built-in-rule-create-xgboost-rule-not-generating-report-as-expected",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm currently working with a SageMaker hosted XGBoost model; I've added the built in rule \"CreateXgboostRule\" to generate a training report, however, only the ProfilerReport is generated in the S3 rule-output folder - the expected result based on the dev doc is for a CreateXGBoostRule folder as well within this same folder.\n\nThe code I'm using is based directly on the example provided in: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/debugger-training-xgboost-report.html\n\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\nfrom sagemaker import image_uris\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nregion = boto3.Session().region_name\nxgboost_container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n\nestimator=Estimator(\n    role=sagemaker.get_execution_role()\n    image_uri=xgboost_container,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    \n    # Add the Debugger XGBoost report rule\n    rules=rules\n)\n\nestimator.fit(wait=False)\n\n\nI've tried rewriting the estimator a number of ways, verified \"rules\" is receiving an array of objects, tried different versions of XGBoost within the region, but everything still results in the built in rule only creating the ProfilerReport with no CreateXGBoostRule directory under rule-output.\n\nAny ideas would be greatly appreciated! Thanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Endpoint is not created when deploying HuggingFace Model using it.",
        "Question_creation_time":1657903012949,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT4ywRDmOTO-8YSR4MBrVKg\/sagemaker-endpoint-is-not-created-when-deploying-hugging-face-model-using-it",
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to deploy the HuggingFace model onto sagemaker. Here is the link for the model: https:\/\/huggingface.co\/dalle-mini\/dalle-mini\n\nI am testing in my personal account and here is the code for the same:\n\nfrom sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nsess = sagemaker.Session()\n# sagemaker session bucket -> used for uploading data, models and logs\n# sagemaker will automatically create this bucket if it not exists\nsagemaker_session_bucket='sagemaker-hugging-face-model-demo'\nif sagemaker_session_bucket == 'sagemaker-hugging-face-model-demo' and sess is not None:\n    # set to default bucket if a bucket name is not given\n    sagemaker_session_bucket = sess.default_bucket()\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n\n\nhub = {\n    'HF_MODEL_ID':'dalle-mini\/dalle-mini',\n    'HF_TASK':'Text-to-image'\n}\n\nhuggingface_model = HuggingFaceModel(\n  env=hub,\n  role=role,\n  #image_uri=\"428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face\",\n  transformers_version=\"4.6.1\",     # transformers version used\n  pytorch_version=\"1.7\",          # pytorch version used\n  py_version='py36'\n)\n\n# deploy model to Sagemaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\n\n\nWhen I am trying to create the sagemaker endpoint I am experiencing the error: ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Requested image 428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face not found.\n\nAlso I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image. E.g. --> The text Sun is shining should be transformed to image after the lambda function invokes the sagemaker endpoint.\n\nAlso need to know what should be the ContentType for image.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using SageMaker SDK to deploy a open source xgboost model locally",
        "Question_creation_time":1638503327094,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEH97qD5dSjS93XkXTdel8w\/using-sage-maker-sdk-to-deploy-a-open-source-xgboost-model-locally",
        "Question_upvote_count":0.0,
        "Question_view_count":141.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a locally trained model that I am trying to debug locally on docker container before deploying \/ creating endpoint on SageMaker. I am following the documentation that AWS customer service provided, however, I am running into issue with Creating Endpoint Config.\n\nHere's the code snippet:\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         aws_access_key_id='xxxxxx',\n                         aws_secret_access_key='xxxxxx'\n                        )\n\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=model_url,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   sagemaker_session = sagemaker_session     \n)\n\nprint('Deploying endpoint in local mode')\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       instance_type = \"local\"\n)\n\n\nTraceback:\n\n20 print('Deploying endpoint in local mode')\n21 predictor = xgb_inference_model.deploy(\n22                                        initial_instance_count = 1,\n\nClientError: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'local' at 'productionVariants.1.member.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.r5d.12xlarge, ml.r5.12xlarge, ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.r5d.24xlarge,\n\n\nHere's the documentation link: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/xgboost_script_mode_local_training_and_serving.py",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to use Lambda functions along with other services to scale up and scale down(probably to 0 instances) Ec2 Deployed apps",
        "Question_creation_time":1649176043559,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl2PMA3JZRU2QmN7_knI7fQ\/is-it-possible-to-use-lambda-functions-along-with-other-services-to-scale-up-and-scale-down-probably-to-0-instances-ec-2-deployed-apps",
        "Question_upvote_count":0.0,
        "Question_view_count":125.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there, hope you are fine. Recently I came across Sagemaker Async inference API, there we can scale down even to 0 instances. What I want is that I deploy my solution to EC2 instances using FastAPI, uvicorn and Celery or Rabbit-MQ(as message broker, for queuing). Then I can scale up and scale down instances based on traffic. Also, if that's not the case, then I keep a minimal CPU instance on always and based on that I scale up and scale down GPU instances for handling requests.\n\nThanks , for any help.\n\nBest Regards Muhammad Ali",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"S3 Dataset versioning with SageMaker?",
        "Question_creation_time":1549396058000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s-3-dataset-versioning-with-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":386.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning? Basically, what setup allows to track a given model training execution to a given dataset? Interested to hear about proven or state-of-the-art ideas",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Question_creation_time":1647503861594,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-step-functions-sage-makers-invoke-endpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Question_upvote_count":0.0,
        "Question_view_count":110.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a state-machine workflow with 3 following states:\n\nscreenshot-of-my-workflow\n\nA 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.)\nUse map to call SageMaker endpoints dictated by the array(or list) from above result.\nSend the result of above 'Map' to a Lambda function and exit the workflow.\n\nHere's the entire workflow in .asl.json, inspired from this aws blog.\n\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n\n\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.\nscreenshot-of-graph-inspector\n\nWhat's causing the error and How do I fix this?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Line magic error in SageMaker Studio",
        "Question_creation_time":1658670880505,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZPg9WRBtSwGvYhI9-3tscA\/line-magic-error-in-sage-maker-studio",
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi AWS, I am trying to create virtual environment in SageMaker Studio but while doing so I am experiencing a line magic function error. Also I am not able to import the libraries. I am attaching the error screenshot for the same.\n\nThanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create Keras's encoder-decoder model's endpoint?",
        "Question_creation_time":1557413472000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa1eV25XxRjKhLPHVXe9TKQ\/how-to-create-kerass-encoder-decoder-models-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\nI'm just started to use sagemaker.\nNow, I'm testing encoder-decoder model for regression.\nThe model is coded in Sagemaker's script mode, and finished learning on a Jupyter notebook.\n\nThe learning code is written by keras + tensorflow, and the model is based on the encoder decoder model. It is similar to the Keras's seq2seq example linked below.\nBelow, I will take this as an example.\nhttps:\/\/keras.io\/examples\/lstm_seq2seq\/\n\nIn the above model, there are \"encoder_model\" and \"decoder_model\" apart from \"model\" to be trained, and in the inference, \"encoder_model\" and \"decoder_model\" are used to generate the prediction by the function \"decode_sequence (input_seq)\".\nI would like to deploy this function \"decode_sequence (input_seq)\" as an endpoint, but it doesn't work as usual with estimator.deploy () and I don't know how to implement it.\n\nIs there any sample code or resources to solve this?\nThanks in advance.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Input Manifest Errors in Sagemaker Ground Truth for Custom Labeling Job",
        "Question_creation_time":1655745668932,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn7gIM_MkSHmd9IzuV4_pmw\/input-manifest-errors-in-sagemaker-ground-truth-for-custom-labeling-job",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am attempting to create a native PDF annotation labeling job for use with Comprehend to identify entities within similar documents. I have around 20 pdf files, all of them around 100-300 pages long.\n\nI used the tools and followed the directions from this blog post. I've struggled a little with the tools but ultimately got everything working.\n\nMy problem comes from the labeling job itself. When I open the labeling job in a private workforce that I've created, I find only a blank page. I did some research and found that there is something wrong with the input manifest as it seems AWS isn't able to parse it for some reason.\n\nI checked my manifest and found that it was generated as multiple objects. Each object was a single page from my PDFs. This seems normal, however the objects were not put into a list or 'top level' object, which does not fit JSON Lines guidelines. I attempted a quick fix of placing these objects all within a list (which satisfies JSON Lines) but it does not seem to help.\n\nAny suggestions or advice would be greatly appreciated.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mxnet error encountered in Lambda Function",
        "Question_creation_time":1638553590411,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW1vSlOVxRC2pGlTEXs0Z2w\/mxnet-error-encountered-in-lambda-function",
        "Question_upvote_count":0.0,
        "Question_view_count":69.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I trained and deployed a semantic segmentation network (mlp2.xlarge) using SageMaker. I wanted to use an AWS Lambda function to send an image to this endpoint and get a mask in return however when I use invoke_endpoint it gives an mxnet error in the logs. Funnily when I use the deployed model from a transformer object from inside the SageMaker notebook the mask is returned properly. Here is my Lambda function code:\n\nimport json\nimport boto3\n\ns3r = boto3.resource('s3')\n\ndef lambda_handler(event, context):\n    # TODO implement\n    \n    bucket = event[\"body\"]\n    key = 'image.jpg'\n    local_file_name = '\/tmp\/'+key\n    s3r.Bucket(bucket).download_file(key, local_file_name)\n\n    runtime = boto3.Session().client('sagemaker-runtime')\n\n    with open('\/tmp\/image.jpg', 'rb') as imfile:\n        imbytes = imfile.read()\n\n    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n    response = runtime.invoke_endpoint(\n    EndpointName='semseg-2021-12-03-10-05-58-495', \n    ContentType='application\/x-image', \n    Body=bytearray(imbytes))                       # The actual image\n\n    # The response is an HTTP response whose body contains the result of our inference\n    result = response['Body'].read()\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps(result)\n    }\n\n\nHere are the errors I see in the logs: mxnet.base.MXNetError: [10:26:14] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.4.x.4276.0\/AL2_x86_64\/generic-flavor\/src\/3rdparty\/dmlc-core\/src\/recordio.cc:12: Check failed: size < (1 << 29U) RecordIO only accept record less than 2^29 bytes",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to choose an instance type for a sagemaker testing\/inference?",
        "Question_creation_time":1650126925753,
        "Question_link":"https:\/\/repost.aws\/questions\/QULxw59aBCRfmso_f7-VCjRQ\/how-to-choose-an-instance-type-for-a-sagemaker-testing-inference",
        "Question_upvote_count":0.0,
        "Question_view_count":225.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"looking at few examples, for training in sagemaker . are there some guidelines based on the model size, data to be trained , what type of instance cpu\/gpu to use? also, can one use spot instances ( may be with multiple gpu cores)?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker instances keep awakening and charge the credit",
        "Question_creation_time":1653535822137,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjCMOSHaPR4WwWP1SoFzzng\/sagemaker-instances-keep-awakening-and-charge-the-credit",
        "Question_upvote_count":0.0,
        "Question_view_count":306.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have tried Data Wrangler in Sagemaker last month and close the service. A few weeks later I have noticed the credit was charge $1 every hour and just realized that the Data Wranger auto-save the flow every minute. So, I deleted the unsaved flow and shut down all the services and instances according to advice on these two links :\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-shutdown.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\n\nThen, I left the Sagemaker untouched for the whole month of May, and just got back to the console yesterday. This is what I found out for May's bill:\n\nAmazon SageMaker RunInstance $531.74\nDetail\tUsage\tTotal\n$0.00 for Host:ml.m5.xlarge per hour under monthly free tier\t125.000 Hrs\t$0.00\n$0.00 for Notebk:ml.t2.medium per hour under monthly free tier\t107.056 Hrs\t$0.00\n$0.00 per Data Wrangler Interactive ml.m5.4xlarge hour under monthly free tier\t25.000 Hrs\t$0.00\n$0.23 per Hosting ml.m5.xlarge hour in US East (N. Virginia)\t88.997 Hrs\t$20.47\n$0.922 per Data Wrangler Interactive ml.m5.4xlarge hour in US East (N. Virginia)\t554.521 Hrs\t$511.27\n\nSo, with another attempt, I installed an extension to automatically shut down idle kernels and set the limit to 10 min from advice here: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/ Checked the cost in usage report, it turns out that the service was shut down after installing the extension but then it revoked itself after 5 hours later (during my sleep time). There's still cost from Studio although with less charge than previous one.\n\nService\tOperation\tUsageType\tStartTime\tEndTime\tUsageValue\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/24\/2022 23:00\t5\/25\/2022 0:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 0:00\t5\/25\/2022 1:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 1:00\t5\/25\/2022 2:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 2:00\t5\/25\/2022 3:00\t0.76484417\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 8:00\t5\/25\/2022 9:00\t0.36636722\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 9:00\t5\/25\/2022 10:00\t0.38959556\n\nDuring this time, I'm sure that there're no running instances, running apps, kernel sessions or terminal sessions. I even deleted the user profile. Last thing I haven't tried is to set up scheduled shutdown coz I think the services should not cause difficulty to our life that much. Any advice for any effective action to completely shutdown the Sagemaker instance? Thanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Question_creation_time":1656580942341,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you! Ruoy",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Extending Docker image for SageMaker Inference",
        "Question_creation_time":1655198456555,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxESyB86cSMCN3dOUMyi4cw\/extending-docker-image-for-sage-maker-inference",
        "Question_upvote_count":0.0,
        "Question_view_count":217.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to create my own Docker image for use with SageMaker Batch Transform by extending an existing one. Following the documentation at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html, I have created the following to run Detectron 2:\n\nFROM 763104351884.dkr.ecr.eu-west-2.amazonaws.com\/pytorch-inference:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\n\n############# Installing latest builds ############\nRUN pip install --upgrade torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\nENV FORCE_CUDA=\"1\"\n# Build D2 only for Turing (G4) and Volta (P3) architectures. Use P3 for batch transforms and G4 for inference on endpoints\nENV TORCH_CUDA_ARCH_LIST=\"Turing;Volta\"\n\n# Install Detectron2\nRUN pip install \\\n   --no-cache-dir pycocotools~=2.0.0 \\\n   --no-cache-dir https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu113\/torch1.10\/detectron2-0.6%2Bcu113-cp38-cp38-linux_x86_64.whl\n   \n# Set a fixed model cache directory. Detectron2 requirement\nENV FVCORE_CACHE=\"\/tmp\"\n\n############# SageMaker section ##############\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\nENV SAGEMAKER_PROGRAM inference.py\n\n\nI then create a model (create-model) with this image using the following configuration:\n\n{\n\"ExecutionRoleArn\": \"arn:aws:iam::[redacted]:role\/model-role\",\n\"ModelName\": \"model-test\",\n\"PrimaryContainer\": { \n  \"Environment\": {\n    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"\/opt\/ml\/code\",\n    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n    \"SAGEMAKER_REGION\": \"eu-west-2\",\n    \"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"\n   },\n  \"Image\": \"[redacted].dkr.ecr.eu-west-2.amazonaws.com\/my-image:latest\",\n  \"ModelDataUrl\": \"s3:\/\/[redacted]\/training\/output\/model.tar.gz\"\n}\n}\n\n\nAnd submit a batch transform job (create-transform-job) using the following configuration:\n\n{\n\"MaxPayloadInMB\": 16,\n\"ModelName\": \"model-test\",\n\"TransformInput\": { \n    \"ContentType\": \"application\/x-image\",\n    \"DataSource\": { \n      \"S3DataSource\": { \n          \"S3DataType\": \"ManifestFile\",\n          \"S3Uri\": \"s3:\/\/[redacted]\/manifests\/input.manifest\"\n      }\n    }\n},\n\"TransformJobName\": \"transform-test\",\n\"TransformOutput\": { \n    \"S3OutputPath\": \"s3:\/\/[redacted]\/predictions\/\"\n},\n\"TransformResources\": { \n    \"InstanceCount\": 1,\n    \"InstanceType\": \"ml.m5.large\"\n}\n}\n\n\nBoth of the above commands submit fine, but the transform job doesn't complete. When I look in the logs, the errors I'm getting seem to indicate that it's not using my inference script (inference.py, specified above) but is instead using the default script (default_pytorch_inference_handler.py) and therefore can't find the model.\n\nWhat am I missing so that it uses my inference script instead, and hence my model?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker metrics persistence",
        "Question_creation_time":1578643279000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU197giXXuRn-4Hz56HZZpSw\/sage-maker-metrics-persistence",
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Quick questions on ML metrics persistence from sagemaker training tasks. The SageMaker regexp-over-CloudWatch is an attractive option, yet the metric retention in Cloudwatch seems to be restricted to 15 days.\n\nHow to persist those metrics longer? Is it common to extract them out of Cloudwatch regularly to persist them somewhere else, eg S3 or an RDS? what is the best practice for long-term persistence of those metrics?\nWould SageMaker Experiments allow a collection of similar data (customer-defined training metrics) over a longer retention?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Create endpoint from Python",
        "Question_creation_time":1625083671000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Question_upvote_count":0.0,
        "Question_view_count":83.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.\n\nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.\n\nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?\n\nRegards.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"loading and deploying a previously trained sagemaker xgboost model",
        "Question_creation_time":1563308650000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Cm7BTSlQ1GtLEzqVeGftQ\/loading-and-deploying-a-previously-trained-sagemaker-xgboost-model",
        "Question_upvote_count":0.0,
        "Question_view_count":475.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to write an inference pipeline where I load a previously trained sagemaker xgboost model stored in s3 as a tar.gz file (following sagemaker tutorial) and deploy it as an endpoint for prediction. Here is my code:\n\ntrainedmodel = sagemaker.model.Model(    \r\n    model_data='data-path-to-my-model-in-s3\/model.tar.gz',\r\n    image=container,  \r\n    role=role)  \r\n\r\nxgb_predictor = trainedmodel.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n\n\nThe code runs fine but after that when I try to call predict() on xgb_predictor I get an error saying 'NoneType' object has no attribute 'predict'. I followed the example here to train the xgboost model:\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-with-xgboost-and-amazon-sagemaker\/\n\nWhy am I getting this error? What's the correct way to load a previously trained model? Help would be appreciated.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"deploying previously trained model with Sagemaker Python SDK (StatusExceptionError)",
        "Question_creation_time":1661503967725,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXT-lr_7ASxSSzx0lRAaEvg\/deploying-previously-trained-model-with-sagemaker-python-sdk-status-exception-error",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using a pertained Random Forest Model and trying to deploy it on Amazon Sagemker using Python SDK:\n\nfrom sagemaker.sklearn.estimator import SKLearn\n\nsklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit')\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n\nI get the following StatusException Error\n\n2022-08-25 12:03:27 Starting - Starting the training job....\n2022-08-25 12:03:52 Starting - Preparing the instances for training............\n2022-08-25 12:04:55 Downloading - Downloading input data......\n2022-08-25 12:05:31 Training - Downloading the training image.........\n2022-08-25 12:06:22 Training - Training image download completed. Training in progress..\n2022-08-25 12:06:32 Uploading - Uploading generated training model.\n2022-08-25 12:06:43 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n<ipython-input-37-628f942a78d3> in <module>\n----> 1 sklearn_estimator.latest_training_job.wait(logs='None')\n      2 artifact = m_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n      4 \n      5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2110         else:\n-> 2111             self.sagemaker_session.wait_for_job(self.job_name)\n   2112 \n   2113     def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3227         )\n-> 3228         self._check_job_status(job, desc, \"TrainingJobStatus\")\n   3229         return desc\n   3230 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3390                 message=message,\n   3391                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-> 3392                 actual_status=status,\n   3393             )\n   3394 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run\n    cwd=environment.code_dir,\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\n\nThe pertained model works fine and I don't know what the problem is, please help",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Possible quota issue",
        "Question_creation_time":1644304787619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnEGxZsbiQPe4oHO0CkRaRg\/possible-quota-issue",
        "Question_upvote_count":0.0,
        "Question_view_count":200.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I opened up an AWS support case to increase my quota based on the error below, but AWS support states this is not a quota issue. They were unwilling to help me in support but directed me to this forum.\n\nI'm getting the following error when running some AWS SageMaker batch transform jobs on m5.large instances. I'm running 8 transforms in parallel against 8 different models in a step function map. A few of the transforms succeed, but some fail with the following error: { \"resourceType\": \"sagemaker\", \"resource\": \"createTransformJob.sync\", \"error\": \"SageMaker.AmazonSageMakerException\", \"cause\": \"Rate exceeded (Service: AmazonSageMaker; Status Code: 400; Error Code: ThrottlingException; Request ID: 49a80dc1-df06-4b88-a462-24e517d13531; Proxy: null)\" }\n\nWhat is going on here? Am I doing something wrong? What kind of throttling exception is this (what is being throttled?)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Maximising multi-instance and multi-GPU utilisation",
        "Question_creation_time":1548267645000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJdZhEsN_QTWOcvWD7tfcLA\/maximising-multi-instance-and-multi-gpu-utilisation",
        "Question_upvote_count":0.0,
        "Question_view_count":9.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI've been trying to distribute the MNIST example across instances and GPUs with SageMaker Tensorflow, but I'm not seeing the kind of benefit that I was hoping for. I'm not sure whether I'm just setting the job up incorrectly or whether this example is just not suited to distribution and was wondering if anyone has any ideas which it might be please?\n\nI'm using TensorFlow 1.9 because I think MPI\/Horovod doesn't work with 1.12 which I was using originally?\n\nIn my test I get these results with a batch size of 512:\n1 Instance 1 GPU 13.5 global_step\/sec\n2 Instance 1 GPU 7.3 global_step\/sec\n1 Instance 8 GPU 18.8 global_step\/sec\n\nIf I reduce the batch size I get lower overall throughput, and see little benefit when increasing beyond 512.\n\nMy job specification looks as below and mnist.py is the file which comes with the examples, though I changed batch_size=100 to batch_size=512 in the script:\n\nestimator = TensorFlow(entry_point='mnist.py',\r\n                  role=role,\r\n                  framework_version='1.9.0',\r\n                  training_steps=1250, \r\n                  evaluation_steps=10,\r\n                  train_max_run=5*60,    \r\n                  output_path=output_location,\r\n                  checkpoint_path=output_location,\r\n                  code_location=output_location,\r\n                  model_dir=output_location,\r\n                  train_instance_count=1,\r\n                  train_instance_type='ml.p3.16xlarge',\r\n                  base_job_name='PerformanceTest-p3-16xlarge-1-instance',\r\n                  distributions={\r\n                    'mpi': {\r\n                      'enabled': True,\r\n                      'processes_per_host': 8,\r\n                      'custom_mpi_options': '--NCCL_DEBUG INFO'\r\n                    }\r\n                  })\n\n\nWhat I was hoping to see was a single 8-GPU instance hitting global_step\/sec of 70.2-97.2. Based on 8x the global_step\/sec of a single instance, scaled with 60-90% efficiency. Any help or clarification on this would be greatly appreciated!\n\nThanks,\n\nCarl",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I check my current SageMaker service quotas?",
        "Question_creation_time":1642480202619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas",
        "Question_upvote_count":0.0,
        "Question_view_count":533.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas\n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a request for a service quota increase.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Retrieve Linear Learner Weights",
        "Question_creation_time":1658859121171,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXh8p1bCgT8a2gLfTDssY7w\/retrieve-linear-learner-weights",
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Unable to find the correct attribute to list Linear Learner weights. I fitted the estimator with the training data and linear Learner creates a weight vector w, which is fundamental in this algorithm. How can I print the resulting weight vector after training\/fitting?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Training Job. Python modules installation Error",
        "Question_creation_time":1659396236435,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmwxhCTLzTp2f9ePN1V2oLg\/sagemaker-training-job-python-modules-installation-error",
        "Question_upvote_count":0.0,
        "Question_view_count":128.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a problem with Python module installation that requires pre-installation of another module. Both modules were added to the requirement.txt file. However, the error occurs when installing main module:\n\n2022-07-29 01:18:26.460132: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:26.470589: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\"\n2022-07-29 01:18:26.765280: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:31,908 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\"\n\"2022-07-29 01:18:31,917 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\"\n\"2022-07-29 01:18:33,117 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\"\n\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\nCollecting Cython==0.29.31\nDownloading Cython-0.29.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0\/2.0 MB 33.1 MB\/s eta 0:00:00\nRequirement already satisfied: wheel==0.37.1 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 2)) (0.37.1)\nCollecting scikit-image==0.19.2\nDownloading scikit_image-0.19.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.0\/14.0 MB 83.1 MB\/s eta 0:00:00\nCollecting parallelbar==0.1.19\nDownloading parallelbar-0.1.19-py3-none-any.whl (5.6 kB)\nCollecting albumentations==1.0.3\nDownloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7\/98.7 kB 6.6 MB\/s eta 0:00:00\nCollecting tensorflow_addons==0.16.1\nDownloading tensorflow_addons-0.16.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1\/1.1 MB 54.4 MB\/s eta 0:00:00\nRequirement already satisfied: tensorflow-io==0.24.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 7)) (0.24.0)\nRequirement already satisfied: tensorboard==2.8.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 8)) (2.8.0)\nCollecting universal-pathlib==0.0.12\nDownloading universal_pathlib-0.0.12-py3-none-any.whl (19 kB)\nCollecting setuptools==63.2.0\nDownloading setuptools-63.2.0-py3-none-any.whl (1.2 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2\/1.2 MB 58.9 MB\/s eta 0:00:00\nCollecting pynanosvg==0.3.1\nDownloading pynanosvg-0.3.1.tar.gz (346 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 346.0\/346.0 kB 17.5 MB\/s eta 0:00:00\nPreparing metadata (setup.py): started\nPreparing metadata (setup.py): finished with status 'error'\n\"error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [6 lines of output]\n      Traceback (most recent call last):\n        File \"\"<string>\"\", line 2, in <module>\n        File \"\"<pip-setuptools-caller>\"\", line 34, in <module>\n        File \"\"\/tmp\/pip-install-1mt2gkfy\/pynanosvg_d6162ffce95948abb4262061a011908c\/setup.py\"\", line 2, in <module>\n          from Cython.Build import cythonize\n      ModuleNotFoundError: No module named 'Cython'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\"\nerror: metadata-generation-failed\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\"note: This is an issue with the package mentioned above, not pip.\"\nhint: See above for details.\n[notice] A new release of pip available: 22.1.2 -> 22.2.1\n\"[notice] To update, run: pip install --upgrade pip\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Reporting training FAILURE\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    InstallRequirementsError:\"\nExitCode 1\n\"ErrorMessage \"\"      ModuleNotFoundError: No module named 'Cython'\n       [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed  \u00d7 Encountered error while generating package metadata. \u2570\u2500> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details.\"\"\"\n\"Command \"\"\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\"\"\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Encountered exit_code 1\"",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal",
        "Question_creation_time":1526422450000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeYBDZTVwQlWovkq2eE1CkQ\/waiter-training-job-completed-or-stopped-failed-waiter-encountered-a-terminal",
        "Question_upvote_count":0.0,
        "Question_view_count":401.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to launch a job using the low level api in boto3 sagemaker client. After calling sagemaker.create_training_job(**params) I try to get a waiter. This code is directly from the documentation for creating a training job (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html)\nI get this error:\n\nTraceback (most recent call last):\r\n  File \"traindeploy.py\", line 97, in create_training_job\r\n    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 53, in wait\r\n    Waiter.wait(self, **kwargs)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 323, in wait\r\n    last_response=response,\r\nbotocore.exceptions.WaiterError: Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal failure state\n\n\nThese are my job params:\n\n{\r\n  \"AlgorithmSpecification\": {\r\n    \"TrainingImage\": \"<image-url-from-ecr>\",\r\n    \"TrainingInputMode\": \"File\"\r\n  },\r\n  \"RoleArn\": \"<role-arn>\",\r\n  \"OutputDataConfig\": {\r\n    \"S3OutputPath\": \"s3:\/\/path-to-bucket\/some-folder-output\/\"\r\n  },\r\n  \"ResourceConfig\": {\r\n    \"InstanceCount\": 2,\r\n    \"InstanceType\": \"ml.c4.8xlarge\",\r\n    \"VolumeSizeInGB\": 50\r\n  },\r\n  \"TrainingJobName\": \"some-jobname\",\r\n  \"HyperParameters\": {},\r\n  \"StoppingCondition\": {\r\n    \"MaxRuntimeInSeconds\": 3600\r\n  },\r\n  \"InputDataConfig\": [\r\n    {\r\n      \"ChannelName\": \"train\",\r\n      \"DataSource\": {\r\n        \"S3DataSource\": {\r\n          \"S3DataType\": \"S3Prefix\",\r\n          \"S3Uri\": \"s3:\/\/path-to-bucket\/some-folder-input\/\",\r\n          \"S3DataDistributionType\": \"FullyReplicated\"\r\n        }\r\n      },\r\n      \"CompressionType\": \"None\",\r\n      \"RecordWrapperType\": \"None\"\r\n    }\r\n  ]\r\n}\n\n\nCan someone please advise what is causing this and how will I get a waiter on a training job?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Amazon SageMaker Local Mode raised boto3.exceptions.RetriesExceededError: Max Retries Exceeded",
        "Question_creation_time":1640615850316,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVLhj-0JzTUCLcs7yH5Kznw\/amazon-sage-maker-local-mode-raised-boto-3-exceptions-retries-exceeded-error-max-retries-exceeded",
        "Question_upvote_count":0.0,
        "Question_view_count":83.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I try to run the SageMaker local mode example without any modification at https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/tree\/main\/pytorch_nlp_script_mode_local_model_inference on my local machine.\n\nHowever I encountered the **boto3.exceptions.RetriesExceededError: Max Retries Exceeded ** exception when the example tries to deploy the inference endpoint to 'local' instance type.\n\nI checked with\n\ndocker images -a \n\n\ncommand and it does not pull the expected pre-built SageMaker deep learning container image from ECR. The code example is using a dummy role for the local SageMaker session. I need help as I am blocked at this point as the exception error message is not helpful to pinpoint the actual root cause of this issue. Thanks in advance.\n\nBelow are my configurations:\n\nUbuntu: 20.04.3 LTS\nAWS CLI version: 2.4.7\nPython: 3.8.12\nDocker: 20.10.12\nDocker Compose: 1.29.2\nboto3: 1.20.26\nsagemaker: 2.72.1",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Prevent boto3.client('sagemaker').create_auto_ml_job() from deploying endpoint",
        "Question_creation_time":1669662766261,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGEnbfLBgQJKrbdKfizHkEw\/prevent-boto-3-client-sagemaker-create-auto-ml-job-from-deploying-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When I invoke the .create_auto_ml_job() method both with and without the optional ModelDeployConfig kwarg, the autopilot job deploys an endpoint using the best model. Is there a way to prevent the .create_auto_ml_job() method from behaving this way? I do not wish to deploy the best model to an endpoint, and do not wish to have to delete this endpoint.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Failing to create multi-model endpoint",
        "Question_creation_time":1613660927000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcIYmAjUlRn-b_PqcIjk08A\/failing-to-create-multi-model-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have been trying to create a multi-model endpoint with my own container, using the instructions here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html.\n\nFollowing the instructions here, I am able to successfully create a model and endpoint configuration: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint-sdk.html\n\nHowever, when I try to create the endpoint itself, it shows the status of \"Creating\" for over 2 hours, before finally stopping with the status, \"Failed\". It gives no reason for the failure or any other help.\n\nDoes anyone have any ideas?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Pipelines - Is it possible to use a TransformStep with the Catboost Estimator ?",
        "Question_creation_time":1668677758132,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdkeWBFI3SXSA8QznUYgT1Q\/sagemaker-pipelines-is-it-possible-to-use-a-transform-step-with-the-catboost-estimator",
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi! I am trying to implement a Sagemaker Pipeline including the following steps (among other things):\n\nProcessingStep: processing script (PySparkProcessor) generating a train , validation and test dataset (csv)\nTrainingStep: model training, CatBoost Estimator (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html)\nTransformStep: batch inference using the model on the test dataset (csv)\n\nThe TransformStep returns the following error: python3: can't open file 'serve': [Errno 2] No such file or directory\n\nI wonder if I'm using TransformStep in the wrong way or if, at the moment, the use of TransformStep with the CatBoost model has not been implemented yet.\n\nCode:\n\n[...]\npyspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark\",\n    framework_version=\"3.1\",\n    role=role_arn,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=12,\n    sagemaker_session=pipeline_session,\n    max_runtime_in_seconds=2400,\n)\n\nstep_process_args = pyspark_processor.run(\n    submit_app=os.path.join(\n        s3_preprocess_script_dir, \"preprocess.py\"\n    ),  # Hack to fix cache hit\n    submit_py_files=[os.path.join(\n        s3_preprocess_script_dir, \"preprocess_utils.py\"\n    ), os.path.join(\n        s3_preprocess_script_dir, \"spark_utils.py\"\n    )],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"datasets\",\n            source=\"\/opt\/ml\/processing\/output\",\n            destination=s3_preprocess_output_path,\n        )\n    ],\n    arguments=[\"--aws_account\", AWS_ACCOUNT, \"--aws_env\", AWS_ENV, \"--project_name\", PROJECT_NAME, \"--mode\", \"training\"],\n)\n\nstep_process = ProcessingStep(\n    name=\"PySparkPreprocessing\",\n    step_args=step_process_args,\n    cache_config=cache_config,\n)\n\ntrain_model_id = \"catboost-classification-model\"\ntrain_model_version = \"*\"\ntrain_scope = \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type,\n)\n\n# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n)\n\n# Retrieve the pre-trained model tarball to further fine-tune\ntrain_model_uri = model_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n)\n\ntraining_job_name = name_from_base(f\"jumpstart-{train_model_id}-training\")\n\n# Create SageMaker Estimator instance\ntabular_estimator = Estimator(\n    role=role_arn,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    max_run=360000,\n    hyperparameters=hyperparameters,\n    sagemaker_session=pipeline_session,\n    output_path=s3_training_output_path,\n    disable_profiler=True,  # The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n)\n\n# Launch a SageMaker Training job by passing s3 path of the training data\nstep_train_args = tabular_estimator.fit(\n    {\n        \"training\": TrainingInput(\n            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri\n        )\n    },\n    logs=True,\n    job_name=training_job_name,\n)\n\nstep_train = TrainingStep(\n    name=\"CatBoostTraining\",\n    step_args=step_train_args,\n    cache_config=cache_config,\n)\n\nscript_eval = ScriptProcessor(\n    image_uri=[MASKED],\n    command=[\"python3\"],\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    base_job_name=\"script-evaluation\",\n    role=role_arn,\n    sagemaker_session=pipeline_session,\n)\n\neval_args = script_eval.run(\n    inputs=[\n        ProcessingInput(\n            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            destination=\"\/opt\/ml\/processing\/model\",\n        ),\n        ProcessingInput(\n            source=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri,\n            destination=\"\/opt\/ml\/processing\/input\",\n        ),\n    ],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"evaluation\",\n            source=\"\/opt\/ml\/processing\/evaluation\",\n            destination=s3_evaluation_output_path,\n        ),\n    ],\n    code=\"common\/evaluation.py\",\n)\n\nevaluation_report = PropertyFile(\n    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\n\nstep_eval = ProcessingStep(\n    name=\"Evaluation\",\n    step_args=eval_args,\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\nmodel = Model(\n    image_uri=\"467855596088.dkr.ecr.eu-west-3.amazonaws.com\/sagemaker-catboost-image:latest\",\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=pipeline_session,\n    role=role_arn,\n)\n\nevaluation_s3_uri = \"{}\/evaluation.json\".format(\n    step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n)\n\nmodel_step_args = model.create(\n    instance_type=\"ml.m5.large\",\n)\ncreate_model = ModelStep(name=\"CatBoostModel\", step_args=model_step_args)\n\nstep_fail = FailStep(\n    name=\"FailBranch\",\n    error_message=Join(\n        on=\" \", values=[\"Execution failed due to F1-score <\", 0.8]\n    ),\n)\n\ncond_lte = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=step_eval.name,\n        property_file=evaluation_report,\n        json_path=\"classification_metrics.f1-score.value\",\n    ),\n    right=f1_threshold,\n)\n\nstep_cond = ConditionStep(\n    name=\"F1ScoreCondition\",\n    conditions=[cond_lte],\n    if_steps=[create_model],\n    else_steps=[step_fail],\n)\n\n# Transform Job\ns3_test_transform_input = os.path.join(step_process.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"], \"test\")\n\ntransformer = Transformer(model_name=create_model.properties.ModelName,\n                          instance_count=1,\n                          instance_type=\"ml.m5.xlarge\",\n                          assemble_with=\"Line\",\n                          accept=\"text\/csv\",\n                          output_path=s3_test_transform_output_path,\n                          sagemaker_session=pipeline_session)\n\ntransform_step_args = transformer.transform(\n    data=s3_test_transform_input,\n    content_type=\"text\/csv\",\n    split_type=\"Line\",\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    step_args=transform_step_args,\n)\n\n\n# Create and execute pipeline\nstep_transform.add_depends_on([step_process, create_model])\n\npipeline = Pipeline(\n    name=pipeline_name,\n    steps=[step_process, step_train, step_eval, step_cond, step_transform],\n    sagemaker_session=pipeline_session,\n)\n\npipeline.upsert(role_arn=role_arn, description=[MASKED])\nexecution = pipeline.start()\nexecution.wait(delay=60, max_attempts=120)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Solved]download image from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)",
        "Question_creation_time":1660886637665,
        "Question_link":"https:\/\/repost.aws\/questions\/QULB64ZDsXSPuHBjqAWik3hQ\/solved-download-image-from-s-3-to-endpoint-made-by-sagemaker-with-s-3-url-s-3",
        "Question_upvote_count":0.0,
        "Question_view_count":102.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I try to download image(.jpg, .png.) from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)\n\nBecause At the endpoint made by sagemaker, To send s3url is faster than to send image.\n\nI can download image at sagemaker notebook, from s3 to sagemaker local.\n\nbut I can't download image from s3 to sagemaker endpoint.\n\nThat local download code can not work.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Outgoing mail for sagemaker labeling job",
        "Question_creation_time":1662016903588,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\nin case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\nin case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use S3 as a git repo",
        "Question_creation_time":1656607653458,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUeFHB_qvQw67d9knOyw1Ig\/use-s-3-as-a-git-repo",
        "Question_upvote_count":0.0,
        "Question_view_count":236.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a sagemaker notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. I would like to use that 1 s3 bucket as a place to house git repos, ideally I would like to be able to pull\/push to repos in that bucket from other sagemaker notebooks or ec2 instances that have connections to that bucket. Has anyone tried this before?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to fix SageMaker training job error \"SM_CHANNEL_TRAIN\"?",
        "Question_creation_time":1652689524286,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwxIZtOn8Qg6EHGP_Ehi2mQ\/how-to-fix-sage-maker-training-job-error-sm-channel-train",
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am building a ml workflow using step function following this. However, when I start the state machine, I got error\n\nAlgorithmError: framework error ... SM_CHANNEL_TRAIN ...exit code: 1 \n\n\nDoes anyone know how to fix it? or how to set SM_CHANNEL_TRAIN?\n\nThank you",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create custom templates for model training and building in sagemaker studio?",
        "Question_creation_time":1663805155971,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-fW_jQSjSKiwACGXqFh1sA\/how-to-create-custom-templates-for-model-training-and-building-in-sagemaker-studio",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am going through documentation provided here , https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy and would like to create my own model building, training and deploying templates. can I download\/clone the sagemaker provided templates and start modifying to my own needs. I understand , we need to set up a aws catalog portfolio and products under it, to be able to use such templates. my question is which project do i need to clone and modify , say if i want to build my own training and model building template, which particular code base or code file do i need to change. I assume , the train.py file here -> https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy\/blob\/master\/08_projects\/modelbuild\/pipelines\/endtoendmlsm\/train\/train.py would be one i will customize. but once i change these files, how do i use them. how would i set up my custom template, create a product under my catalog? if yes, how to I link my custom train.py code to this new custom sagemaker project template. the documentation or samples only show how to use the prebuild templates under sagemaker project templates, but how do i get my own template pushed there ? can this be done via say cloudformation ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Question_creation_time":1598912648000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/module-not-found-error-when-starting-a-training-job-on-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":357.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get ModuleNotFoundError: No module named 'nltk'\n\nMy code is\n\nimport sagemaker  \nfrom sagemaker.pytorch import PyTorch\n\nJOB_PREFIX   = 'pyt-ic'\nFRAMEWORK_VERSION = '1.3.1'\n\nestimator = PyTorch(entry_point='finetune-T5.py',\n                   source_dir='..\/src',\n                   train_instance_type='ml.p2.xlarge' ,\n                   train_instance_count=1,\n                   role=sagemaker.get_execution_role(),\n                   framework_version=FRAMEWORK_VERSION, \n                   debugger_hook_config=False,  \n                   py_version='py3',\n                   base_job_name=JOB_PREFIX)\n\nestimator.fit()\n\n\nfinetune-T5.py have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I use glue interactive sessions with pythonshell?",
        "Question_creation_time":1645223131500,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjwZtVPYTSCG96fKOGn1k4w\/can-i-use-glue-interactive-sessions-with-pythonshell",
        "Question_upvote_count":0.0,
        "Question_view_count":138.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Can I use glue interactive sessions with pythonshell?\n\nIn the docs, it hints at choosing the %job_type or is it only available for glueetl. I am looking for a simple way to develop by requiring myself to use a sage maker notebook because it is overkill on half the ETL more of the time. Can I just get the 0.0625 DPU by default with a notebook? And no extras like I get with sagemaker.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)",
        "Question_creation_time":1659014190621,
        "Question_link":"https:\/\/repost.aws\/questions\/QULlX63PqqQ1q-W5kCzwTKow\/import-error-cannot-import-name-dataclass-transform-from-typing-extensions-home-ec-2-user-anaconda-3-envs-tensorflow-2-p-38-lib-python-3-8-site-packages-typing-extensions-py",
        "Question_upvote_count":0.0,
        "Question_view_count":344.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi AWS, I am running the code for dalle mini to convert a text into an image. Here is the code for the same:\n\nimport jax\nimport jax.numpy as jnp\nfrom huggingface_hub import hf_hub_url, cached_download, hf_hub_download\nimport shutil\nfrom dalle_mini import DalleBart, DalleBartProcessor\nfrom vqgan_jax.modeling_flax_vqgan import VQModel\nfrom typing_extensions import dataclass_transform\nfrom transformers import CLIPProcessor, FlaxCLIPModel\nfrom IPython.display import display\n\n# TF_CPP_MIN_LOG_LEVEL=0\nprint(jax.local_device_count())\nprint(jax.devices())\n\ndalle_mini_files_list = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'merges.txt', 'vocab.json', 'special_tokens_map.json', 'enwiki-words-frequency.txt', 'flax_model.msgpack']\n\nvqgan_files_list = ['config.json',  'flax_model.msgpack']\n\nfor each_file in dalle_mini_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/dalle-mini\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nfor each_file in vqgan_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/vqgan_imagenet_f16_16384\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/vqgan\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/huggingface-sagemaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nmodel, params = DalleBart.from_pretrained(    \n      DALLE_MODEL_LOCATION, revision=DALLE_COMMIT_ID, dtype=jnp.float32, _do_init=False,\n)\n\nVQGAN_LOCAL_REPO = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini\/vqgan'\nVQGAN_LCOAL_COMMIT_ID = None\nvqgan, vqgan_params = VQModel.from_pretrained(\n     VQGAN_LOCAL_REPO, revision=VQGAN_LCOAL_COMMIT_ID, _do_init=False\n)\n\n\nprint(model.config)\nprint(vqgan.config)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nprocessor = DalleBartProcessor.from_pretrained(\n     DALLE_MODEL_LOCATION, \n     revision=DALLE_COMMIT_ID)\n\nprint(processor)\n\n# # Works for all available devices to replicate the module\nfrom flax.jax_utils import replicate\nimport random\n\nparams = replicate(params)\nvqgan_params = replicate(vqgan_params)\n\n@partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(3, 4, 5, 6))\ndef p_generate(\n    tokenized_prompt, key, params, top_k, top_p, temperature, condition_scale\n):\n  return model.generate(\n      **tokenized_prompt,\n      prng_key=key,\n      params=params,\n      top_k=top_k,\n      top_p=top_p,\n      temperature=temperature,\n      condition_scale=condition_scale,\n  )\n\n#decode the images\n@partial(jax.pmap, axis_name=\"batch\")\ndef p_decode(indices, params):\n    return vqgan.decode_code(indices, params=params)\n\n\n# entering the prompts\nprompts = [\n    \"sunset over a lake in the mountains\",\n    \"the Eiffel tower landing on the moon\",\n]\n\ntokenized_prompts = processor(prompts)\ntokenized_prompt = replicate(tokenized_prompts)\n\n\n\n# create a random key\nseed = random.randint(0, 2**32 - 1)\nkey = jax.random.PRNGKey(seed)\n\n\nn_predictions = 4\n\n# We can customize generation parameters (see https:\/\/huggingface.co\/blog\/how-to-generate)\ngen_top_k = None\ngen_top_p = None\ntemperature = None\ncond_scale = 10.0\n\nprint(f\"Prompts: {prompts}\\n\")\n\nimages = []\nfor i in trange(max(n_predictions \/\/ jax.device_count(), 1)):\n    # get a new key\n    key, subkey = jax.random.split(key)\n    # generate images\n    encoded_images = p_generate(\n        tokenized_prompt,\n        shard_prng_key(subkey),\n        params,\n        gen_top_k,\n        gen_top_p,\n        temperature,\n        cond_scale,\n    )\n    # remove BOS\n    encoded_images = encoded_images.sequences[..., 1:]\n    # decode images\n    decoded_images = p_decode(encoded_images, vqgan_params)\n    decoded_images = decoded_images.clip(0.0, 1.0).reshape((-1, 256, 256, 3))\n    for decoded_img in decoded_images:\n        img = Image.fromarray(np.asarray(decoded_img * 255, dtype=np.uint8))\n        images.append(img)\n        display(img)\n\n\nand the error I am getting is:\n\nImportError Traceback (most recent call last) ~\/SageMaker\/huggingface-sagemaker\/code\/inference.py in <module> 5 #import DalleBart 6 #from dalle_mini import DalleBart, DalleBartProcessor ----> 7 from vqgan_jax.modeling_flax_vqgan import VQModel 8 from typing_extensions import dataclass_transform 9 #from transformers import CLIPProcessor, FlaxCLIPModel\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/vqgan_jax\/modeling_flax_vqgan.py in <module> 8 import jax.numpy as jnp 9 import numpy as np ---> 10 import flax.linen as nn 11 from flax.core.frozen_dict import FrozenDict 12\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/init.py in <module> 16 \"\"\"Flax API.\"\"\" 17 ---> 18 from . import core as core 19 from . import linen as linen 20 from . import optim as optim\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/init.py in <module> 26 ) 27 ---> 28 from .scope import ( 29 Scope as Scope, 30 Array as Array,\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/scope.py in <module> 26 from flax import config 27 from flax import errors ---> 28 from flax import struct 29 from flax import traceback_util 30 from .frozen_dict import freeze\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/struct.py in <module> 23 24 import jax ---> 25 from typing_extensions import dataclass_transform # pytype: disable=not-supported-yet 26 27\n\nImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)\n\nPlease help me ASAP as I need to fix it urgently.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify target feature in Sagemaker XGBoost?",
        "Question_creation_time":1661221322082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoW_FqSbIQKW0MqNJLlA2AA\/how-to-specify-target-feature-in-sagemaker-xg-boost",
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am considering migrating a data science project from Datarobot to Sagemaker. I am familiar with writing Python and have been going through one of the tutorial Jupyter notebooks to see how to explore the data and to build and deploy and estimator. But, I cannot see how to specify the target feature. I have entirely numerical data in a csv file. One of the fields in that file is the intended target for estimation, the rest are information from which the estimate is to be made.\n\nHow do I specify the column that is to be estimated? The code I expect should have this is ...\n\ncontainer = sm.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"1.5-1\")\n\nxgb = sm.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/xxxxxx001\/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    num_round=100,\n)\ns3_input_train = TrainingInput(\n    s3_data=\"s3:\/\/xxxxxx001\/data.csv\", content_type=\"csv\"\n)\nxgb.fit({\"train\": s3_input_train})",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker Endpoint Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check",
        "Question_creation_time":1639162238458,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV4VuYCKHTEedPanW-keHTQ\/aws-sage-maker-endpoint-failed-reason-the-primary-container-for-production-variant-all-traffic-did-not-pass-the-ping-health-check",
        "Question_upvote_count":0.0,
        "Question_view_count":983.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Links to the AWS notebooks for reference https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\n\nhttps:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\nI am using the example from the notebooks to create and deploy an endpoint to AWS SageMaker Cloud. I have passed all the checks locally and when I attempt to deploy the endpoint I run into the issue.\n\nCode\n\nIn my local notebook (my personal machine NOT sagemaker notebook):\n\n    import pandas\n    import xgboost\n    from xgboost import XGBRegressor\n    import numpy as np\n    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n    \n    print(xgboost.__version__)\n    1.0.1\n\n    # Fit model\n    r.fit(X_train.toarray(), y_train.values)\n\n    xgbest = r.best_estimator\n\n\n\nAWS SageMaker Endpoint code\n\nimport boto3\nimport pickle\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\nfrom time import gmtime, strftime\n\nregion = boto3.Session().region_name\n\nrole = 'arn:aws:iam::111:role\/xxx-sagemaker-role'\n\nbucket = 'ml-model'\nprefix = \"sagemaker\/xxx-xgboost-byo\"\nbucket_path = \"https:\/\/s3-{}.amazonaws.com\/{}\".format('us-west-1', 'ml-model')\n\nclient = boto3.client(\n    's3',\n    aws_access_key_id=xxx\n    aws_secret_access_key=xxx\n)\nclient.list_objects(Bucket=bucket)\n\n\n\nSave the model\n\n# save the model, either xgbest \nmodel_file_name = \"xgboost-model\"\n\n# using save_model\n# xgb_model.save_model(model_file_name)\n\npickle.dump(xgbest, open(model_file_name, 'wb'))`\n\n!tar czvf xgboost_model.tar.gz $model_file_name\n\n\n\nUpload to S3\n\nkey = 'xgboost_model.tar.gz'\n\nwith open('xgboost_model.tar.gz', 'rb') as f:\n    client.upload_fileobj(f, bucket, key)\n\n\nImport model\n\n# Import model into hosting\ncontainer = get_image_uri(boto3.Session().region_name, \"xgboost\", \"0.90-2\")\nprint(container)\n\nxxxxxx.dkr.ecr.us-west-1.amazonaws.com\/sagemaker-xgboost:0.90-2-cpu-py3\n\n%%time\n\nmodel_name = model_file_name + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\nmodel_url = \"https:\/\/s3-{}.amazonaws.com\/{}\/{}\".format(region, bucket, key)\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\nfrom sagemaker.local import LocalSession\n\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         region_name=\"us-west-1\",\n                         aws_access_key_id='xxxx',\n                         aws_secret_access_key='xxxx'\n                        )\n\n# Define session\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nmodels3_uri = \"s3:\/\/ml-model\/xgboost_model.tar.gz\"\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=models3_uri,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   # Cloud\n                                   sagemaker_session = sagemaker_session\n                                   # Local\n                                   # sagemaker_session = None\n           \n)\n\n#serializer = StringSerializer(content_type=\"text\/csv\")\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       # Cloud\n                                       instance_type=\"ml.t2.large\",\n                                       # Local\n                                       # instance_type = \"local\",\n                                       serializer = \"text\/csv\"\n)\n\n\nif xgb_inference_model.sagemaker_session.local_mode == True:\n    print('Deployed endpoint in local mode')\nelse:\n    print('Deployed endpoint to SageMaker AWS Cloud')\n\n\n\/Applications\/Anaconda\/anaconda3\/lib\/python3.9\/site-packages\/sagemaker\/session.py in wait_for_endpoint(self, endpoint, poll)\n   3354         if status != \"InService\":\n   3355             reason = desc.get(\"FailureReason\", None)\n-> 3356             raise exceptions.UnexpectedStatusException(\n   3357                 message=\"Error hosting endpoint {endpoint}: {status}. Reason: {reason}.\".format(\n   3358                     endpoint=endpoint, status=status, reason=reason\n\nUnexpectedStatusException: Error hosting endpoint sagemaker-xgboost-xxxx: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to get batch transform with jsonl data?",
        "Question_creation_time":1661703317094,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkP-cRiP3QiCAIqnwyirz1A\/how-to-get-batch-transform-with-jsonl-data",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using my own inference.py file as a entry point for inference. I have tested this pytorch model, served as a real time endpoint in amaon sagemaker. but when i try to create a batch job and use multiple json object in my input file (jsonl format) . i get the following error at the input_fn function on this line data = json.loads(request_body), in cloudwatch logs ==>\n\ndata = json.loads(request_body) raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data : line 2 column 1 (Char ..)\n\nI am not sure why am i getting extra data on line 2 error, because this is supposed to be batch job with multiple json input and each line.\n\ninference.py\n\ndef model_fn(model_dir):\n   \/\/load the model\n\n\n\n\ndef input_fn(request_body, request_content_type):\n    input_data= json.loads(request_body)\n    return data\n\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n\nset up batch job\n\nresponse = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n\n\ninput file\n\n{\"input\" : \"some text here\"}\n{\"input\" : \"another\"}\n...",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Batch Transform local mode?",
        "Question_creation_time":1571055107000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sage-maker-batch-transform-local-mode",
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Question_creation_time":1593595381000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sage-maker-batch-transform",
        "Question_upvote_count":0.0,
        "Question_view_count":120.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker inference on inf1 no opencv",
        "Question_creation_time":1649704234930,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUG9fHLN3TNGbXjRmuwRBcA\/sagemaker-inference-on-inf-1-no-opencv",
        "Question_upvote_count":0.0,
        "Question_view_count":123.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to deploy Pytorch model on ml.inf1.xlarge instance. Image: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-neo-pytorch:1.5.1-inf-py3 My python code using some oepncv functions, and when I am trying to run the infernce I got the following error: ModuleNotFoundError: No module named 'cv2'\n\nI tried to add opencv-python-headless to requirements.txt, but then I got another error ImportError: libgthread-2.0.so.0: cannot open shared object file\n\nHow I can use opencv with the ml.inf1 instances?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GPU not detected by tensorflow in SM Studio",
        "Question_creation_time":1627628146000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYk1rHeoARsSfMsuiL-0JgQ\/gpu-not-detected-by-tensorflow-in-sm-studio",
        "Question_upvote_count":0.0,
        "Question_view_count":184.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm currently using SageMaker Studio with kernel \"Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)\".\nTensorflow doesn't detect the GPU on both ml.g4dn.xlarge and ml.g4dn.2xlarge instances (with 1 GPU).\nI would appreciate any advice.\n\nimport tensorflow as tf\ntf.config.list_physical_devices('GPU')\n: []\n\nEdited by: haganHL on Jul 29, 2021 11:55 PM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"3d Ground truth labelling job issue in bbox",
        "Question_creation_time":1657799060933,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnfsUXUBLT0i2HEn3-CW3jQ\/3-d-ground-truth-labelling-job-issue-in-bbox",
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I have created a 3d labelling job for 100 frames and got the job running. When I started annotating the objects with cuboid I am facing 2 issues,\n\nThe size of the bounding box for respective object is changing (increased or decreased) as I move to the next frames and drawing the bounding box (with increased or decreased size to the earlier frames). I believe this could be because of autofill functionality. Please confirm me whether I have an option to off the autofill functionality.\nThe bounding boxes are getting scattered or displaced each time I login and start working. If I annotate the objects for 10 frames, save it and logout for today, later when I login back the bounding boxes were scattered for couple of frames. Can you please let me know what could be done to reduce this issue.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker g4 and g5 instances do not have working nvidia-drivers",
        "Question_creation_time":1669082188275,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBqYWuFr7SyC6P6Uae9LOww\/sagemaker-g-4-and-g-5-instances-do-not-have-working-nvidia-drivers",
        "Question_upvote_count":3.0,
        "Question_view_count":80.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am a heavy user of g4 and g5 instances on Sagemaker (notebook instances). Today when I tried to use the same instances as I always do I was met with the following when running nvidia-smi\n\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n\nThese are all the exact same instances and workloads I have used before. The same message was found when trying to run on ec2 natively as well.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Question_creation_time":1591020062000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sage-maker-notebooks-on-glue-dev-endpoints",
        "Question_upvote_count":0.0,
        "Question_view_count":157.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Lambda Function to invoke sagemaker endpoint",
        "Question_creation_time":1657965474513,
        "Question_link":"https:\/\/repost.aws\/questions\/QU33wE3pnRS9Om2yfVt4EIAg\/lambda-function-to-invoke-sagemaker-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":107.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi AWS, I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image.\n\nThe endpoint is generated using HuggingFace model.\n\nI need your help with the code and the steps to obtain it.\n\nThanks Arjun Goel",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to set up a pipe mode in sagemaker?",
        "Question_creation_time":1650141519238,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoMtSoBPGQpabpreLah_Fjg\/how-to-set-up-a-pipe-mode-in-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":113.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"what other data input types can be used pipe input mode in sagemaker? an example of implementation is here https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker-nysummit2018\/, and can this be used for inference as well as, similar to training?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying a Random Forest Model on Amazon Sagemaker always getting a UnexpectedStatusException with Reason: AlgorithmError",
        "Question_creation_time":1661503022955,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMmnoFG_HQ0qiMWxlSlPlcQ\/deploying-a-random-forest-model-on-amazon-sagemaker-always-getting-a-unexpected-status-exception-with-reason-algorithm-error",
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey I am trying to deploy my RandomForest Classifier on Amazon Sagemaker but get a StatusException Error even though the script worked fine before:\n\nThe script runs fine and prints out the confusion matrix and accuracy as expected. When I try to deploy the model to amazon Sagemaker using the script it does not work.\n\n! python script.py --n-estimators 100\n--max_depth 2\n--model-dir .\/\n--train .\/\n--test .\/ \\\n\nConfusion Matrix: [[13 8] [ 1 17]] Accuracy: 0.7692307692307693\n\nI used the Estimator from Sagemaker Python SDK\n\nfrom sagemaker.sklearn.estimator import SKLearn sklearn_estimator = SKLearn( entry_point='script.py', role = get_execution_role(), instance_count=1, instance_type='ml.m4.xlarge', framework_version='0.20.0', base_job_name='rf-scikit')\n\nI launched the training job as follows\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nHere I am trying to deploy the model which leads to the StatusExceptionError that I cannot seem to fix\n\nsklearn_estimator.latest_training_job.wait(logs='None') artifact = m_boto3.describe_training_job( TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts'['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n2022-08-25 12:03:27 Starting - Starting the training job.... 2022-08-25 12:03:52 Starting - Preparing the instances for training............ 2022-08-25 12:04:55 Downloading - Downloading input data...... 2022-08-25 12:05:31 Training - Downloading the training image......... 2022-08-25 12:06:22 Training - Training image download completed. Training in progress.. 2022-08-25 12:06:32 Uploading - Uploading generated training model. 2022-08-25 12:06:43 Failed - Training job failed\n\nUnexpectedStatusException Traceback (most recent call last) <ipython-input-37-628f942a78d3> in <module> ----> 1 sklearn_estimator.latest_training_job.wait(logs='None') 2 artifact = m_boto3.describe_training_job( 3 TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts'] 4 5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs) 2109 self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs) 2110 else: -> 2111 self.sagemaker_session.wait_for_job(self.job_name) 2112 2113 def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll) 3226 lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll 3227 ) -> 3228 self._check_job_status(job, desc, \"TrainingJobStatus\") 3229 return desc 3230\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name) 3390 message=message, 3391 allowed_statuses=[\"Completed\", \"Stopped\"], -> 3392 actual_status=status, 3393 ) 3394\n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main train(environment.Environment()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train runner_type=runner.ProcessRunnerType) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run wait, capture_error File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run cwd=environment.code_dir, File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error info=extra_info, sagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError: ExitCode 1 ErrorMessage \"\" Command \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\nI am happy for some help",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Exporting Sagemaker model to local computer",
        "Question_creation_time":1649274099076,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgLZZhWVSg2d5XJWwbaTiA\/exporting-sagemaker-model-to-local-computer",
        "Question_upvote_count":0.0,
        "Question_view_count":341.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I hyper-tuned an XGBoost model, deployed the model and created an endpoint. Is there a way to export the model to my local computer? That way I can test the model locally.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Question_creation_time":1664396753855,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-module-not-found-error-no-module-named-transformers",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"So as mentioned in my other recent post, I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\nModuleNotFoundError: No module named 'transformers'\n\n\nNOTE: I am importing 'transformers' in preprocess.py not in pipeline.py\n\nNow I have 'transformers' listed in various places as a dependency including:\n\nsetup.py - required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]\npipelines.egg-info\/requires.txt - transformers (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see previous post for more details) I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n\n\nthe describe() method gives this output:\n\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n\n\nWaiting for the execution I get:\n\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n\n\nWhich I assume is corresponding to the failure I see in the logs:\n\nI did also run python setup.py build to ensure my build directory was up to date ... here's the terminal output of that command:\n\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n\n\nIt seems like the dependencies are being written to pipelines.egg-info\/requires.txt but are these not being picked up by the pipeline?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Relevancy of SageMaker Model Monitor for NLP?",
        "Question_creation_time":1605005518000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxCKLg-eiQ1mwZvzFyczBEg\/relevancy-of-sage-maker-model-monitor-for-nlp",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nCan SageMaker Model Monitor be applied in NLP models? Is it necessary to do some preprocessing of the data? How can we use SageMaker Model Monitor? sentence length, unseen words, language etc. Any thoughts or experience on that?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_creation_time":1596105364000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqMU2EBqGTDCMTPsB5rjNoQ\/has-sas-code-ever-been-successfully-ran-on-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":94.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Has SAS code ever been successfully ran on SageMaker?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can make multi model endpoint with SageMaker?",
        "Question_creation_time":1660122368052,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":53.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"This is my code.\n\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\nAnd error message. How can I find Ecr Image(within multi-models=true)?\n\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Neo compilation load error",
        "Question_creation_time":1548810043000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW8F63wxUTSCCdXf0MESXIg\/neo-compilation-load-error",
        "Question_upvote_count":0.0,
        "Question_view_count":24.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI tried to follow the documentation to run a Neo compilation job on AWS console. I downloaded the model from http:\/\/download.tensorflow.org\/models\/mobilenet_v1_2018_02_22\/mobilenet_v1_1.0_224.tgz and uploaded it to S3. I did everything else the same as the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-job-compilation-console.html. But I got this error:\n\"Load Error: InputConfiguration: Exactly one .pb file is allowed for Tensorflow models.\"\nI checked the tar.gz file that there is only one .pn file. What caused the error?\nThank you very much!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_creation_time":1615480055000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Question_upvote_count":0.0,
        "Question_view_count":323.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error on DeleteEndpoint operation: Cannot update in-progress endpoint",
        "Question_creation_time":1652103997301,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2NawO4aWQvmytekX8xJJNQ\/error-on-delete-endpoint-operation-cannot-update-in-progress-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI have created and endpoint in sagemaker using boto3 but never finishes creation, is stuck in Creating status for few days now. I have tried to delete it using the aws cli api but i get the message:\n\nAn error occurred (ValidationException) when calling the DeleteEndpoint operation: Cannot update in-progress endpoint\n\nUsually endpoint fails after some time and can deleted but this time doesn't fail. Is there any way to force deletion?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training a classifier on parquet with SageMaker ?",
        "Question_creation_time":1588841008000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":188.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Question_creation_time":1643132865842,
        "Question_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Question_upvote_count":0.0,
        "Question_view_count":115.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_creation_time":1527798496000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Question_upvote_count":0.0,
        "Question_view_count":464.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom container not running under root account?",
        "Question_creation_time":1607710724000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\n\nHow do we ensure this?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Column header is not showing when reading data from redshift to jupyter on Sagemaker",
        "Question_creation_time":1669209147429,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxjkMWSCuS2ezRSYihJWHnA\/column-header-is-not-showing-when-reading-data-from-redshift-to-jupyter-on-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI am reading a specific table from a redshift database using redshift connector. when i am viewing the dataframe it does not show the column headers. It shows only numbers as the column headers.\n\nCan anyone help whats wrong here? we need to view the table headers",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_creation_time":1639669045329,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_creation_time":1607681930000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Increase Limit on Lineage Tracking entities for sagemaker",
        "Question_creation_time":1657755893027,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwNhM5g6qRN2orm9ttoPV6w\/increase-limit-on-lineage-tracking-entities-for-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My Team is trying to onboard to sagemaker lineage tracking entities and we basically track models, datasets, associations between these entities all the way to endpoints. Currently, we have been using another system for the same. We currently require that we create dataset entities prior to our training job so that we can use this to reference during our training jobs. The problem comes with the constraints on the amount of manual entities that can be created. As per the doc, the limits are\n\nMaximum number of manually created lineage entities Actions: 3000 Artifacts: 6000 Associations: 6000 Contexts: 500\n\nOur current system holds about 1500 datasets and 1000 models which means that we might hit the limit in the near future if we onboard to sagemaker. Is there a provision to increase the limits on these? I am not sure why these limits are placed. These entities must be pretty cheap to store. Please let me know if there is any way to get this limit increased",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Accessing SageMaker Notebooks without accessing the console",
        "Question_creation_time":1543947347000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sage-maker-notebooks-without-accessing-the-console",
        "Question_upvote_count":0.0,
        "Question_view_count":240.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\nDo we have a best practice for that? In the create-presigned-notebook-instance-url command, what is the --session-expiration-duration-in-seconds: is it the validity duration of the URL or the max session duration once the URL has been clicked?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Question_creation_time":1592989945000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":457.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_creation_time":1590501108000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Question_upvote_count":0.0,
        "Question_view_count":130.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is required in HumanLoopInput.InputContent for start_human_loop",
        "Question_creation_time":1645624147204,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJast4QKSTjCYWe-pcQFyHw\/what-is-required-in-human-loop-input-input-content-for-start-human-loop",
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have built a custom key-value extraction workflow that leverages textract Tables and Forms. It does a whole heap of post processing using the output of Textract to extract a small number of highly important fields from documents that are of very poor quality.\n\nMy client would like a human-in-the-loop to make minor changes to the results where certain fields are missing. I think that the sagemaker_a2i_runtime.start_human_loop is the perfect tool for this.\n\nI want to send the human reviewer, the input image and the current Key-Value pairs that I have extracted and have them find any that are missing, or mark them as not there. I have setup and tested the textract.analyse_image workflow with a human reviewer and like the results.\n\nWhat structure and data fields do I need to set in the HumanLoopInput field of sagemaker_a2i_runtime.start_human_loop to get this to work. I assume that it will look something like a dictionary of current K-V pairs and the s3 image file but I cannot find any documentation on how to do this.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running a request against all variants in an endpoint",
        "Question_creation_time":1604486652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":14.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a customer asking me about the Rendezvous architecture. What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\nLambda (and probably SQS) around the endpoint;\nA custom monitoring job;\nStep Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio will not load",
        "Question_creation_time":1576685163000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load",
        "Question_upvote_count":0.0,
        "Question_view_count":904.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Morning of 12\/17 I loaded SageMaker Studio, and created an Autopilot experiment. It ran for 2 hours and was successful.\n\nAfterwards, I exited SageMaker Studio. Ever since that point, I have been unable to re-enter Studio. It is now 24 hours later.\n\nI either receive a response from Chrome:\n\nThis page isn\u2019t working\nd-*************.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nOr, I get an error message:\nThe JupyterServer app default encoutered a problem and was stopped.\nDetails: InternalFailure\n\nI get the option to \"Restart Now,\" but it never works.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use sagemaker.processing.Processor run method",
        "Question_creation_time":1661777469790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhZktmd-_Q3mFdOyFaNU9NA\/how-to-use-sagemaker-processing-processor-run-method",
        "Question_upvote_count":0.0,
        "Question_view_count":31.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"This is sagemaker docs. What is the purpose of sagemaker.processing.Processor as its run method does not have input for code or script? then how can I use it?\n\nOf course, I can use FrameworkProcessor, ScriptProcessor, SklearnProcessor because I can provide my processing.py. But for the sagemaker.processing.Processor, how can I use it?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I use serverless inference with models generated by SageMaker Autopilot?",
        "Question_creation_time":1661981861600,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJpArvoZTSkiWkDMVDW1avw\/how-can-i-use-serverless-inference-with-models-generated-by-sage-maker-autopilot",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"There are a few articles about deploying SageMaker models to use serverless inference, but I am not clear on how to do that with autopilot models in particular. In other words, I do not understand which steps should be different and how to find information such as what my model ARN is. Thanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why does my kernal keep dying when I try to import Hugging Face BERT models to Amazon SageMaker?",
        "Question_creation_time":1604517955000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":458.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When I try to import Hugging Face BERT models to the conda_pytorch_p36 kernal of my Amazon SageMaker Notebook instance using the following pip command, the kernal always dies:\n\n! pip install transformers\n\n\nThe result is the same for Hugging Face BERT, RoBERTa, and GPT2 models on ml.c5.2xlarge and ml.c5d.4xlarge Amazon SageMaker instances.\n\nWhy is this happening, and how do I resolve the issue?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"LightGBM on SageMaker",
        "Question_creation_time":1516632842000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":388.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use custom functions for a model in a Sagemaker pipeline?",
        "Question_creation_time":1665782248751,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjEHGriWRBCoNnc8luz-6Q\/how-to-use-custom-functions-for-a-model-in-a-sagemaker-pipeline",
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"If I want to use a custom function transformer in preprocessing, how do I ensure that it's detected at both pipeline building and deployment?\n\nI'm building a sklearn pipeline, and in preprocessing I use a custom FunctionTransformer. In Sagemaker, I am able to train, evaluate, and register the model, but get the below error when I try to deploy it: AttributeError: Can't get attribute 'truncate_function' on <module '__main__' from '\/miniconda3\/bin\/gunicorn'>\n\nI've tried putting the functions into a helper.py file, and including it as a dependency during training, but then get the following error when evaluating in a ProcessingStep: \"ModuleNotFoundError: No module named 'helper'.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to Resolve \"ERROR execute(301) Failed to execute model:\"",
        "Question_creation_time":1667853055854,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwmKbCBpXSym2Vh_4Z0cW0g\/how-to-resolve-error-execute-301-failed-to-execute-model",
        "Question_upvote_count":0.0,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We have two applications working on the same AWS Panorama Appliance and processing different video streams. Unfortunately, we are catching the following error.\n\n2022-10-09 21:25:32.360 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.359 ERROR execute(301) Failed to execute model:\nTVMError: \n'\"---------------------------------------------------------------\"\nAn error occurred during the execution of TVM.\nFor more information, please see: https:\/\/tvm.apache.org\/docs\/errors.html\n'\"---------------------------------------------------------------\n  Check failed: (context->execute(batch_size\n\"Stack trace:\n  File \"\/home\/nvidia\/neo-ai-dlr\/3rdparty\/tvm\/src\/runtime\/contrib\/tensorrt\/tensorrt_runtime.cc\", line 177\n  [bt] (0) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x381358) [0x7f81e66358]\n  [bt] (1) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x88) [0x7f81bb64a0]\n  [bt] (2) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::contrib::TensorRTRuntime::Run()+0x12b8) [0x7f81e243b0]\n  [bt] (3) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::json::JSONRuntimeBase::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x5c) [0x7f81e1bfc4]\n  [bt] (4) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0dc4) [0x7f81ea5dc4]\n  [bt] (5) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0e4c) [0x7f81ea5e4c]\n  [bt] (6) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(dlr::TVMModel::Run()+0xc0) [0x7f81c258e0]\n  [bt] (7) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(RunDLRModel+0x1c) [0x7f81bea304]\n  [bt] (8) \/usr\/lib\/libAwsOmniInferLib.so(awsomniinfer::CNeoModel::SNeoModel::execute()+0x3c) [0x7f887db978]\"\n2022-10-09 21:25:32.437 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.437 ERROR setData(279) Failed to set model input 'data':\n\n\nThe error isn't persistent. It may happen once in 2-3 weeks, and I need to know which place to investigate. The application logs are in the attachment. I am trying to avoid this issue.\n\nHowever, I would appreciate it if somebody knew how to cook this properly.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker XGBoost batch transform AttributeError",
        "Question_creation_time":1657717396655,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3Hva4yNpSfOtRQTjKVMvvg\/sage-maker-xg-boost-batch-transform-attribute-error",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nAfter training XGBoost model using SageMaker inbuilt algorithm, I am trying to perform batch transform operation. I am doing the same steps as for linear learner model which worked fine there. However in case of XGBoost I get a following error while creating a transformer:\n\nAttributeError: module 'sagemaker' has no attribute 'utils'\n\n\nThe piece of code causing the error is:\n\nxgb_transformer = xgb_estimator.transformer(\n    instance_count = 1, \n    instance_type = 'ml.m4.10xlarge',\n    output_path = '{}\/{}'.format(output_path,'output')\n)\n\n\nI use '1.5-1' version of XGBoost as image in training, and 2.86.2 version of SageMaker.\n\nAny help would be highly appreciated!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does sagemaker creates a model?",
        "Question_creation_time":1666386495979,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTsasf-AKRriQkvxUPXGlxg\/how-does-sagemaker-creates-a-model",
        "Question_upvote_count":1.0,
        "Question_view_count":31.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am creating sagemaker resources such as model, endpoint configuration and real time endpoint via cloudformation ( sample below) . in the template below, we provide the s3 bucket URI for the model artifact in the ModelDataUrl argument. if we update the model artifact , or delete a older version and upload a new model.tar file in the same bucket. will that work, instead of creating a new model resource everytime there is a new version of model.tar file ? when making a inference, I understand , sagemaker downloads the model.tar file in the container specified , unpack the model.tar file and call the binary file for inference ,so it doesn't matter if we update the model.tar file , right? sagemaker will simply download whatever tar file is present in the s3 URI and works with that.\n\nSageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties: \n      Containers: \n        -\n          Image: !Ref ImageURI\n          ModelDataUrl: 's3:\/\/some-bucket\/model.tar'\n          Mode: SingleModel\n      ExecutionRoleArn: !Ref RoleArn",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I want to deploy my model as a Serverless inference",
        "Question_creation_time":1661852626896,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGFC_kpAJTx6NcEG9_ZqUyQ\/i-want-to-deploy-my-model-as-a-serverless-inference",
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey I trained a sickst learn model using python sdk and I want to deploy the model as a Serverless inference now. I am new to AWS and can't seem to make sense of the documentation. the model is fit it an estimator as follow:\n\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\ninputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": 'ml.c5.xlarge',\n    \"instance_count\": 1,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\"\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n\n\nthis works fine but now when I try to deploy it it doesn't work. I tried this code: https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploying-ml-models-using-sagemaker-serverless-inference-preview\/ but i keep getting errors because somethings are not defined like the image_uri which I am not using.\n\nI used this\n\nm_boto3 = boto3.client('sagemaker')\n\nestimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n\n\nbut then the endpoint is not Serverless. please help",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio Jupyterlab 3.0 working poorly with SM Resources UI",
        "Question_creation_time":1656420408518,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOmPLv2iwRyuEcQom58DgbA\/sage-maker-studio-jupyterlab-3-0-working-poorly-with-sm-resources-ui",
        "Question_upvote_count":3.0,
        "Question_view_count":195.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,\n\nSince Jupyterlab 3.0 was finally released on SM Studio, we have been super happy with it, however, for reasons unknown to us, the jupterlab interface works very poorly with SM resources, the following phenomenon have been observes:\n\nIt takes FOREVER to load the page for SM pipelines, and half the time it reports error (\"Error listing pipeline executions: Rate exceeded\")\nChanging instance type and size for a notebook is now super laggy, and do not work half the time\n\nAnyone knows if this is merely a lack of optimisation on the service team's part or is there something I can do to stop this behaviour? it's making our work very slow and unbearable, we don't want to revert back to 1.0 so any help would be greatly appreciated!\n\nBest, RUoy",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_creation_time":1643230196748,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Async Inference not able to process later requests",
        "Question_creation_time":1648126119561,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqAl1qUyYRK-cbY3DGH-X9g\/async-inference-not-able-to-process-later-requests",
        "Question_upvote_count":0.0,
        "Question_view_count":172.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there, hope all of you are fine.\n\nI am trying to deploy a train-on-inference type model. I am done with BYOC, and it is working completely fine with real-time inference endpoints. Also, I am able to make it work with Async inference, and concurrent requests on the same instance are also being handled. But, the later requests, never get processed, without any logical error. Also once the endpoint gets scaled down to 0 instance, it fails to scales up.\n\nThese are some of error and warning messages which I get intermittently:\n\n\n\ndata-log:\n2022-03-23T11:23:17.723:[sagemaker logs] [5ea751c9-9271-4533-bc09-c117791e1372] Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n\n\nwarnings:\n\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n\n\nKindly help me with this. Thanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Data Wrangler: Data Flow: Export to S3 using Jupyter Notebook",
        "Question_creation_time":1665658565074,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskLbkfD8RW2YzO9Vr7XggA\/data-wrangler-data-flow-export-to-s-3-using-jupyter-notebook",
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When I have created a Data flow using data wrangler and when I am trying to Export to export to S3 using Jupyter Notebook and when I am running the notebook, I am getting the below mentioned error every time when creating a processing job:\n\nError: An error occurred (ResourceLimitExceeded) when calling the CreateProcessingJob operation: The account-level service limit 'ml.m5.4xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.\n\nPlease provide me with the solution for this. I have increased the service quota for running apps and Notebook instance also but then also same issue arises.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Amazon CloudWatch Metric ModelSetupTime not available",
        "Question_creation_time":1665007033890,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu2KU0TNvTu-2V6sOylZJsg\/amazon-cloud-watch-metric-model-setup-time-not-available",
        "Question_upvote_count":0.0,
        "Question_view_count":23.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"The ModelSetupTime metric is not available for monitoring the serverless endpoint(https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) in Amazon CloudWatch.Able to see only the below mentioned metrics Invocations Invocation5XXErrors Invocation4XXErrors ModelLatency OverheadLatency\n\nIs there any role\/configuration changes I have to do to view the ModelSetupTime ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Getting TrainingJobName from Training step of Sagemaker Pipeline",
        "Question_creation_time":1669298009010,
        "Question_link":"https:\/\/repost.aws\/questions\/QU21YXYHCuRhip83ELnFlYHg\/getting-training-job-name-from-training-step-of-sagemaker-pipeline",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am running a sagemaker pipeline with a training step. This whole setup runs from a Lambda function while I pass a few parameters to the pipeline.py file. To get the TrainingJobName from the training step, my code is step_train.__dict__['step_args']['TrainingJobName'] This works fine while I running it in sagemaker notebook but when I execute the same code to get the train job name from lambda, I get this error [ERROR] TypeError: '_StepArguments' object is not subscriptable [ERROR] TypeError: '_StepArguments' object is not subscriptable Traceback (most recent call last): File \"\/var\/task\/lambda_function.py\", line 46, in lambda_handler pipeline = create_pipeline(validated_api_input) File \"\/tmp\/pipeline.py\", line 105, in create_pipeline \"job_name\" : training_step['step_args']['TrainingJobName']\n\nHow do I resolve this?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to keep sagemaker inference warm-up",
        "Question_creation_time":1669198631982,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVZqbuLPeSLWOe3TobeZHtQ\/how-to-keep-sagemaker-inference-warm-up",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Calling sagemaker inference frequently (3-5 calls in a minute) reduces runtime duration from ~200ms to ~50ms, so it seems there is similar warm-up behaviour like in Lambda. Do you have any suggestions how to keep sagemaker inference responsive always fast?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"java.lang.IllegalArgumentException in SageMaker",
        "Question_creation_time":1642538521034,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXl66qTr3TBWJjO5td_K0jw\/java-lang-illegal-argument-exception-in-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm unable to invoke the my SageMaker endpoint. I'm seeing this error in the endpoint logs\n\njava.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \\r\\n: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.11.2.\n\n\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git\n\n\nMy Sagemaker endpoint is invoked through a lambda function. The code that calls the sagemaker endpoint is:\n\nSM_ENDPOINT_NAME = \"pytorch-inference-2021-xx-xx\"\nsm_runtime= boto3.client('runtime.sagemaker')\ntxt = \"Canon SELPHY CP1300 Compact Photo Printer\"\nresponse = sm_runtime.invoke_endpoint(EndpointName=SM_ENDPOINT_NAME, ContentType='text\/plain', Body=txt)\n\n\nThe response is supposed to contain a vector.\n\nIt's been working fine previously but I started seeing this exception today.\n\nIs this a bug in SageMaker? If not, how do I fix it?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Image Classification Algorithm Class Activation Map",
        "Question_creation_time":1552316346000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUdCD7nXuTJegFm_lGunPzw\/image-classification-algorithm-class-activation-map",
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey!\n\nI am using SageMaker's built in image classification algorithm and currently have a trained model that can make predictions. I am wondering if there is an easy way to use this model to make a class activation map to better debug my classifier and see why it is making the predictions it is?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Uncaught exception in ZMQStream callback -- trying to run Jupyter notebook with Julia kernel in SageMaker",
        "Question_creation_time":1644454296572,
        "Question_link":"https:\/\/repost.aws\/questions\/QUM2y8-rjDS1Wp5LUkdLMhyA\/uncaught-exception-in-zmq-stream-callback-trying-to-run-jupyter-notebook-with-julia-kernel-in-sage-maker",
        "Question_upvote_count":1.0,
        "Question_view_count":172.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I want to run a Jupyter notebook with Julia kernel in Amazon SageMaker. The Julia 1.7.1 icon shows up in the Jupyterlab launcher and accepts the kernel on launch, but then the kernel dies immediately after launch (it never works). I have posted about this here\n\nhttps:\/\/www.repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker\n\nand Alex_T has made some very good suggestions, but even they get stumped at this point. Log info:\n\n[E 00:25:58.760 NotebookApp] Uncaught exception in ZMQStream callback\n    Traceback (most recent call last):\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 431, in _run_callback\n        callback(*args, **kwargs)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/notebook\/services\/kernels\/kernelmanager.py\", line 391, in record_activity\n        msg = session.deserialize(fed_msg_list)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/jupyter_client\/session.py\", line 929, in deserialize\n        raise ValueError(\"Invalid Signature: %r\" % signature)\n    ValueError: Invalid Signature: b'ec1d1093f3b6505658469b860c203f696bc39cf8fcea1672cb55802fc57592eef57b8db0b5cb603d1bcada6f41060f0819f6002a7a31f309fbaa1a701cc13f5b'\n\n\nThere are some posts from 2018 recommending updating tornado and ipykernel, but 4 years later this should hardly apply (and I tried it, and it didn't work). Any suggestions? What could be going wrong here? Everything else seems to be in place. There is a white paper on running a Jupyter notebook with Julia kernel in SageMaker here [https:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card], but it is incomplete, as you can see in the other post.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Question_creation_time":1576675818000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sage-maker-studio-notebooks-hardware-config",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Question_creation_time":1638914293851,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Question_upvote_count":0.0,
        "Question_view_count":690.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this?\n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker.\n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models?\n\nMany thanks\n\nTim",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size",
        "Question_creation_time":1559545390000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq0KSPPCBT1qrotLu0NJyBw\/aws-sagemaker-either-the-training-channel-is-empty-or-the-mini-batch-size",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50.\n\nI keep on getting this error in Sagemaker.\n\nCustomer Error: No training data processed. Either the training\nchannel is empty or the mini-batch size is too high. Verify that\ntraining data contains non-empty files and the mini-batch size is less\nthan the number of records per training host.\n\nI am using this InputDataConfig\n\nInputDataConfig=\\[  \n            {  \n                'ChannelName': 'train',  \n                'DataSource': {  \n                    'S3DataSource': {  \n                        'S3DataType': 'S3Prefix',  \n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',  \n                        'S3DataDistributionType': 'FullyReplicated'  \n                    }  \n                },  \n                'ContentType': 'text\/csv',  \n                'CompressionType': 'Gzip'  \n            }  \n        ],  \n\n\nI am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got\n\nCustomer Error: Unable to initialize the algorithm. Failed to validate\ninput data configuration. (caused by ValidationError)\n\nCaused by: {u'training': {u'TrainingInputMode': u'Pipe',\nu'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\nu'S3DistributionType': u'FullyReplicated'}} is not valid under any of\nthe given schemas\n\nI went back to train as that seems to be what is needed. But what am I doing wrong with that?\n\nEdited by: anshbansal on Jun 3, 2019 12:06 AM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Did the SageMaker PyTorch deployment process change?",
        "Question_creation_time":1594979513000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFIru4hJ2TcWLi7CYt3mnuw\/did-the-sage-maker-py-torch-deployment-process-change",
        "Question_upvote_count":0.0,
        "Question_view_count":134.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Did the SageMaker PyTorch deployment process change?\n\nIt use to be the case that people needed to have a model.tar.gz in s3, and an inference script locally or in git. Now, it seems that the inference script must also be part of the model.tar.gz. This is new, right?\n\nFrom the docs, https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#for-versions-1-2-and-higher:\n\n*For PyTorch versions 1.2 and higher, the contents of model.tar.gz should be organized as follows:\n\nModel files in the top-level directory\nInference script (and any other source files) in a directory named code\/ (for more about the inference script, see The SageMaker PyTorch Model Server)\nOptional requirements file located at code\/requirements.txt (for more about requirements files, see Using third-party libraries)*\n\nThis may be confusing, because this new mode of deployment means that people creating the model artifact need to know in advanced how the inference is going to look like. The previous design, with separation of artifact and inference code, was more agile.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Data Wrangler UI Features",
        "Question_creation_time":1641943985816,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Question_upvote_count":1.0,
        "Question_view_count":61.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Question_creation_time":1653000488230,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Question_upvote_count":0.0,
        "Question_view_count":80.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart. We have attempted to configure this endpoint as 'asynchronous' via the console. Receiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'. This was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker AutoML generates ExpiredTokenException",
        "Question_creation_time":1642293400372,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPU-nfYYIRbmM-tJpkG6XqA\/sage-maker-auto-ml-generates-expired-token-exception",
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI can train models using different AWS SageMaker estimators, but when I use SageMaker AutoML Python SDK the following error occurs about 15 minutes into the model training process:\n\n\"botocore.exceptions.ClientError: An error occurred (ExpiredTokenException) when calling the DescribeAutoMLJob operation: The security token included in the request is expired\"\n\nThe role used to create the AutoML object is associated with the following AWS pre-defined policies as well as one inline policy. Can you please let me know what I\u2019m missing that's causing this ExpiredTokenException error?\n\nAmazonS3FullAccess AWSCloud9Administrator AWSCloud9User AmazonSageMakerFullAccess\n\nInline policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"iam:PassRole\" ], \"Resource\": \"\", \"Condition\": { \"StringEquals\": { \"iam:PassedToService\": \"sagemaker.amazonaws.com\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"sagemaker:DescribeEndpointConfig\", \"sagemaker:DescribeModel\", \"sagemaker:InvokeEndpoint\", \"sagemaker:ListTags\", \"sagemaker:DescribeEndpoint\", \"sagemaker:CreateModel\", \"sagemaker:CreateEndpointConfig\", \"sagemaker:CreateEndpoint\", \"sagemaker:DeleteModel\", \"sagemaker:DeleteEndpointConfig\", \"sagemaker:DeleteEndpoint\", \"cloudwatch:PutMetricData\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:CreateLogGroup\", \"logs:DescribeLogStreams\", \"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\", \"ecr:GetAuthorizationToken\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetDownloadUrlForLayer\", \"ecr:BatchGetImage\" ], \"Resource\": \"\" } ] }\n\nThanks, Stefan",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[errno 28] no space left on disk",
        "Question_creation_time":1650457037741,
        "Question_link":"https:\/\/repost.aws\/questions\/QUInf7H_9bQpCPMROevLa2fA\/errno-28-no-space-left-on-disk",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to install a python package called rapidsai in aws notebook instance but I am getting this error errno 28 no space left on disk. I am using g4dn.xlarge instance for it. The package is almost 3 GB. While opening the notebook instance I have tried increasing volume size of notebook upto 50 GB, I am still getting this error. Let me know the solution to it. Thanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Sagemaker] - Does Sagemaker support Xpress model",
        "Question_creation_time":1657298680086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOPI761YwSQWqpUusp3Mo_g\/sagemaker-does-sagemaker-support-xpress-model",
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nWe have a use case to use Sagemaker towards running Xpress based science models. Can anyone please tell what are the pros and cons of using sagemaker towards running the Xpress models\n\nThanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker endpoint running but constantly restarting",
        "Question_creation_time":1660140840225,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQi42sIDTTSW5KN3P-DT4LQ\/sagemaker-endpoint-running-but-constantly-restarting",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have deployed a model to a Sagemaker endpoint using BentoML\/BentoCTL. This is a tool for building APIs and containerizing models. To test, I use curl with a JSON payload to make a request. When I run the created docker container on my local machine I can successfully invoke it and get responses back. So I don't think the problem is in the docker image.\n\nWhen I deploy to sagemaker, I receive the message {\"message\":\"Service Unavailable\"} as a response to my curl request. I can see the endpoint running in the Sagemaker\/Endpoints dashboard. Viewing the cloudwatch logs, it appears that the the endpoint is constantly restarting. There are messages that are printed at startup (e.g. Tensorflow loading messages) that are written to the log over and over.\n\nI thought that this might be due to using an instance type with low memory (t2.medium) so I switched to m5.4xlarge as a test, but the result is the same.\n\nWhat can I do? How can I determine what's causing the endless restarts?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to set up autoscaling for async sagemaker endpoint?",
        "Question_creation_time":1646861290947,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjLx7h0TR0q1KwubwQjU9A\/how-to-set-up-autoscaling-for-async-sagemaker-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":256.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"working with an example documented here -> https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb. I was able to set up the sagemaker model, config and aync endpoint via lambda, now I'm trying to re-create the stack via terraform. based on the documentation on terraform, i was able to set up the model, config and the endpoint but couldn't find how to go about setting up the auto scaling ( sample code below). is this possible?\n\nclient = boto3.client(    \"application-autoscaling\") \nresource_id = (    \"endpoint\/\" + endpoint_name + \"\/variant\/\" + \"variant1\")  \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  \n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  \nSageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,\n   ....\n    },\n)\n\n\nclean up\n\nresponse = client.deregister_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId='resource_id',\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Annotation of PDF document using Bounding Box",
        "Question_creation_time":1665169244843,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK0c3Zp1jQ8SFMF3Mfp7SmA\/annotation-of-pdf-document-using-bounding-box",
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi! Is it possible to develop some kind of template for SageMaker that uses bounding boxes to annotate a PDF document? I was looking for something similar to the crowd-bounding-box HTML tag, but instead of only capturing portions of the PDF's image, I also wanted to extract data regarding the text located inside that portion, along with it's coordinates and stuff like that, so I could give context to my annotation.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to check\/determine image\/container size for aws managed images ?",
        "Question_creation_time":1645022076554,
        "Question_link":"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images",
        "Question_upvote_count":1.0,
        "Question_view_count":108.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm using one of the images listed here https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md, to create an model such that I can tie that up with a sagemaker serverless endpoint , but I keep getting \"failed reason: Image size 15136109518 is greater that suppported size 1073741824\" . this work when the endpoint configuration is not serverless. is there any documentation around image\/container size for aws managed images?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Question_creation_time":1594209626000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Model Monitor Missing Columns Constraint Violation",
        "Question_creation_time":1586974280000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8Xkelo1ARA2zcn4rHuk09w\/sage-maker-model-monitor-missing-columns-constraint-violation",
        "Question_upvote_count":0.0,
        "Question_view_count":150.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have an Endpoint inference pipeline model deployed from an AutoPilot training job. Now that this is successful, I want to add model monitor. I have a script for online validation of the endpoint, and the F1 score is ~99%. This indicates that the endpoint interprets the call correctly.\n\nModel Monitor is recognizing the data in my jsonl files as the data not being CSV formatted. When my Model Monitor processing job runs, I receive the following constraint violation: \"There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\".\n\nGiven the results from the Endpoint and this Model Monitor constraint violation, I perceive there is a conflict between how the Endpoint is storing the data and how the Model Monitor Processing Job wants to consume the data.\n\nHere is one sample prediction from the jsonl file. The data value is comma separated.\n\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"JHB,44443000.0,-0.0334,,44264000.0,,,,-2014000.0,,-2014000.0,,,,,,,-0.04,-0.04,55872000.0,,,0.996,,,,,,,,-0.0453,,2845000.0,,2845000.0,11636000.0,,,,,,,,,,,,190000000.0,,,,,,,,-18718000.0,,,,,,,,29000000.0,,,,,,,,-33000000.0,,-4000000.0,,,,,,,,,,,,,,,0.0,,,0.995972369102,1.0,-0.045316472785366,0.0,,,,,,,0.0,,,,,,,,,95.5638,,,,,,1.0,1.0,,0.15263157894737,,,,,,0.65252120693923,0.0,0.15263157894737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18606500.0,,,95.5638,,,2.3886,,,,,-0.0326,,-1.0449,,-1.05,-1.05,,0.0,,-0.1471,,,,,,,,,,,,,,,,,-0.5451,,,,,,,Financial Services,16.67890010036862\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"c97df615-0a2e-414d-9be3-bf3a14eb6363\",\"inferenceTime\":\"2020-04-15T16:26:46Z\"},\"eventVersion\":\"0\"}\n\n\nHere is the point within the log that the processing job recognizes a column mismatch. I see that it pulls down the data to store locally, pulls down the statistics and constraints files, errors with this constraint, and then gracefully ends the Processing Job. If more logs are needed to analyze, I have the Processing Job logs in CloudWatch Logs.\n\n2020-04-15 17:11:49 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/constraints\/constraints.json.\n2020-04-15 17:11:50 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/stats\/statistics.json.\n2020-04-15 17:11:50 ERROR DataAnalyzer:65 - There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\nSkipping further processing because of column count mismatch.\n\n\nI could not find Model Monitor documentation on how to deal with column mismatch constraint violations.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker batch transform 415 error",
        "Question_creation_time":1532625720000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Question_upvote_count":0.0,
        "Question_view_count":424.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\ntransformer = XGB.transformer(\n    instance_count=2, instance_type='ml.c5.9xlarge',\n    output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n    max_payload=100)\n\ntransformer.transform(\n    data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n    split_type='Line')\n\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\nBad HTTP status returned from invoke: 415\n'NoneType' object has no attribute 'lower'\n\n\nDid I forget something in my batch transform settings?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker batch transform",
        "Question_creation_time":1532619204000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads I'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem Sagemaker and Spark",
        "Question_creation_time":1536801534000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3gi1LMvEQNiiGlYooXnUQA\/problem-sagemaker-and-spark",
        "Question_upvote_count":0.0,
        "Question_view_count":792.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,\n\nI followed this tutorial to set up Sagemaker Notebook with Spark (EMR): https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\n\nI launched a notebook with sparkmagic (pyspark3) and tried to call the Spark context but got the following error:\n\"\"\"\nThe code failed because of a fatal error:\nInvalid status code '400' from http:\/\/xxx.xx.xx.xx:8998\/sessions with error payload: \"Invalid kind: pyspark3 (through reference chain: org.apache.livy.server.interactive.CreateInteractiveRequest[\"kind\"])\".\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n\"\"\"\n\nAnyone encountered the same issue?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Sagemaker Notebook Not working ,how can i solve the issue?",
        "Question_creation_time":1665720123752,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhUbNweWRR0-oROL-jwKIRQ\/aws-sagemaker-notebook-not-working-how-can-i-solve-the-issue",
        "Question_upvote_count":0.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"The code failed because of a fatal error: Error sending http request and maximum retry encountered..\n\nSome things to try: a) Make sure Spark has enough available resources for Jupyter to create a Spark context. b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly. c) Restart the kernel.\n\nNote: There are no such logs on cloudwatch to figureout the issue.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Lifecycle scripts to access the notebook instance git repository",
        "Question_creation_time":1554297310000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhTwl-BZURCGatI4Rm9fFqg\/lifecycle-scripts-to-access-the-notebook-instance-git-repository",
        "Question_upvote_count":0.0,
        "Question_view_count":132.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,\nIs it possible for the lifecycle scripts to access the content of the checkout-ed git repository? A use case would be to access the already available in the repository pip requirements file and to populate the notebook instance with the required python modules on start up.\nI guess the answer to this question depends on the order of the executed events when a notebook is created. Are you executing the lifecycle scripts first and then checkout the repository or vice versa.\n\nThanks!\n\nEdit:\n\nWell, I did my experiment as follows.\n\nI added a simple 'ls -al SageMaker' in the start and create lifecycle scripts and inspected the logs.\nIt seems that on initial notebook instance creation the git repository is checked out after the execution of the start and create scripts.\nOn subsequent notebook starts, the start script is executed and the repository folder is present in the SageMaker folder with a timestamp indicating that the repository folder was created after the initial start\/create scripts executions.\n\nSo, can someone confirm that this is what's expected and that we can access the repository only on subsequent notebook starts?\n\nEdited by: ainkov on Apr 3, 2019 7:25 AM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Spot instances for inference and sagemaker?",
        "Question_creation_time":1649350121825,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcQU2DOmNQdyI8HWeIMzzdg\/spot-instances-for-inference-and-sagemaker",
        "Question_upvote_count":1.0,
        "Question_view_count":184.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is it possible to deploy spot inf1 instances on sagemaker? We run an API 24\/7, and it's costly to keep it up, considering we only have 2 hours of peak performance a day.\n\nWe don't shut off those machines because we might have random bursts of traffic during the day that CPU instances can't hold. Alternatively, we could deploy spot EC2 inf machines; however, I'm unsure how I would invoke them from gateway and lambda. Does anybody have a tip or recommendation for our case?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Rekognition Custom Labels and Ground Truth integration question",
        "Question_creation_time":1594736742000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH-j5YmdZTiSEvLnm1ghztw\/rekognition-custom-labels-and-ground-truth-integration-question",
        "Question_upvote_count":0.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello!\n\nI've created a semantic segmentation (SS) job in Ground Truth (GT), and it shows all objects are labeled, and the status is Complete. The manifest file appears to be syntactically correct.\n\nWithin Custom Labels (CL), I've created a dataset by choosing \"Import images labeled by Ground Truth\" option and using the manifest file created in GT. It parses correctly. However, when I look at the available dataset in the console, I see that there are zero labeled images. Viewing the images also displays zero labels.\n\nI have done the same with bounding box (BB) jobs in GT, and they import into CL correctly.\n\nDo CL datasets not support import of SS GT manifest files? I can't find anything in the documentation that states either way.\n\nThanks for your help.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Experiment tracking with Sagemaker Pipelines",
        "Question_creation_time":1667379630510,
        "Question_link":"https:\/\/repost.aws\/questions\/QUttCrJyfVQYyAE-AO0vg-EA\/experiment-tracking-with-sagemaker-pipelines",
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is it possible to track only TrainingSteps in a Sagemaker Pipeline that contains multiple Processing & other steps? I don't really see big benefit of creating Trial Components for Processing Jobs or Model Repacking jobs into the experiments as they just overflow the UI.\n\nBasically could the pipeline_experiment_config parameter be used for defining which steps of the Pipeline should be tracked or should I disable automatic experiment creation and just try to create a manual experiment tracker during the Training Job.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to prevent disassociating SageMaker LifecycleConfig unintentionally",
        "Question_creation_time":1646363723581,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTvkDhX_yQXyW7WpFinO_vA\/how-to-prevent-disassociating-sage-maker-lifecycle-config-unintentionally",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When you go to SageMaker Notebook Instance edit screen in AWS Web Console (to change the Instance Type for example), it is sometimes the case that Lifecycle configuration is popped up as No Configuration even though the configuration is actually set earlier. This results in an unintentional disassociation of the LifecycleConfig because it's easy to save the instance change without noticing the change in Lifecycle Config. This is a serious problem for us. I was able to reproduce this issue in Chrome and Firefox (but you need to try several times to repro the issue).\n\nI am in the position of provisioning different cloud resources for the end users and I need a way to systematically prevent this disassociation to happen. I considered applying an IAM policy that denies the update operation containing the change in the LifecycleConfig of notebooks, but there seems no condition key for LifecycleConfig which makes me think this approach isn't feasible.\n\nWhat can I do?\n\nThanks.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"In Sagemaker endpoint, pip download fails when connected through VPC",
        "Question_creation_time":1648489585321,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7Q7e634rRAidb_GUu2ZhXw\/in-sagemaker-endpoint-pip-download-fails-when-connected-through-vpc",
        "Question_upvote_count":0.0,
        "Question_view_count":90.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Pip download fails in my instance when sagemaker is connected through VPC. It is successful when VPC is not specified. I have internet gateway configured for my public subnets. I was able to pip download successfully in EC2 with same security groups and subnets. Is this any bug in sagemaker side?\n\nAnd also, can we log into EC2 instance that sagemaker created? It is more of like a blackbox testing without it.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\u00bfHow can we crate a lambda which uses a Braket D-Wave device?",
        "Question_creation_time":1650979594609,
        "Question_link":"https:\/\/repost.aws\/questions\/QUt5r-dbkZT1O4yQM_TszxHw\/how-can-we-crate-a-lambda-which-uses-a-braket-d-wave-device",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We are trying to deploy a Lambda with some code which works in a Notebook. The code is rather simple and uses D-Wave \u2014 DW_2000Q_6. The problem is that when we execute the lambda (container lambda due to size problems), it give us the following error:\n\n{\n  \"errorMessage\": \"[Errno 30] Read-only file system: '\/home\/sbx_user1051'\",\n  \"errorType\": \"OSError\",\n  \"stackTrace\": [\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 234, in load_module\\n    return load_source(name, filename, file)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 171, in load_source\\n    module = _load(spec)\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 702, in _load\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 671, in _load_unlocked\\n\",\n    \"  File \\\"<frozen importlib._bootstrap_external>\\\", line 843, in exec_module\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 219, in _call_with_frames_removed\\n\",\n    \"  File \\\"\/var\/task\/lambda_function.py\\\", line 6, in <module>\\n    from dwave.system.composites import EmbeddingComposite\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/__init__.py\\\", line 15, in <module>\\n    import dwave.system.flux_bias_offsets\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/flux_bias_offsets.py\\\", line 22, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/__init__.py\\\", line 15, in <module>\\n    from dwave.system.samplers.clique import *\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/clique.py\\\", line 32, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler, _failover\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/dwave_sampler.py\\\", line 31, in <module>\\n    from dwave.cloud import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/__init__.py\\\", line 21, in <module>\\n    from dwave.cloud.client import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/__init__.py\\\", line 17, in <module>\\n    from dwave.cloud.client.base import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 89, in <module>\\n    class Client(object):\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 736, in Client\\n    @cached.ondisk(maxage=_REGIONS_CACHE_MAXAGE)\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/utils.py\\\", line 477, in ondisk\\n    directory = kwargs.pop('directory', get_cache_dir())\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/config.py\\\", line 455, in get_cache_dir\\n    return homebase.user_cache_dir(\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 150, in user_cache_dir\\n    return _get_folder(True, _FolderTypes.cache, app_name, app_author, version, False, use_virtualenv, create)[0]\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 430, in _get_folder\\n    os.makedirs(final_path)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 223, in makedirs\\n    mkdir(name, mode)\\n\"\n  ]\n}\n\nIt seems that the library tries to write to some files which are not in \/tmp folder.\n\nI'm wondering if is possible to do this, and if not, what are the alternatives.\n\nimports used:\n\nimport boto3\nfrom braket.ocean_plugin import BraketDWaveSampler\nfrom dwave.system.composites import EmbeddingComposite\nfrom neal import SimulatedAnnealingSampler",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Host multiple TensorFlow computer vision models using Amazon SageMaker multi-model endpoints",
        "Question_creation_time":1644300080795,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxAPkO75GTnSERkxpABFgSQ\/host-multiple-tensor-flow-computer-vision-models-using-amazon-sage-maker-multi-model-endpoints",
        "Question_upvote_count":0.0,
        "Question_view_count":112.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi All,\n\nGreetings!!\n\nCould you please clarify on two questions below which are related to this post.\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\/\n\nAs per AWS documentation (https:\/\/docs.aws.amazon.com...] \"Multi-model endpoints are not supported on GPU instance types\". I see we are using 'instance_type': 'ml.m5.2xlarge' to train both image classification and sign language digit classification models in this post.\n\nAs per instance configuration we don't have any GPU cores and GPU memory available in 'ml.m5.2xlarge' instance type. But we all know that we need a GPU instance for deep learning model training, we are not getting how we are able to train both classification models using 'ml.m5.2xlarge' instance type.\n\nDoes this image - IMAGE_URI = '763104351884.dkr.ecr.us-eas... has GPU cores?\n\nml.m5.2xlarge - Instance details:\n\nCompute Type: Standard Instances V CPU: 8 Memory: 32 GiB Clock Speed: undefined GPU: 0 Network Performance: Up to 10 Gigabit Storage: EBS only GPU Memory: 0\n\nAs mentioned in this post, I believe we have one production variant of both CIFAR and sign-language models meaning we have v1 models. Let's assume that we have a new set of images for classification then we have decided to train a new variant CIFAR model and want to create that model.\n\nSo we have 2 versions of the CIFAR model and 1 version of the sign-language model.\n\nHow can we get the inference from two production variants of CIFAR models? How can we do A\/B testing for CIFAR models?\n\nThanks in advance.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"fail to run guide demo of DeepRacer SageMaker",
        "Question_creation_time":1666058970622,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWyDmIyDjQu2l8OJyKBFHNw\/fail-to-run-guide-demo-of-deep-racer-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":16.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I try to follow the developer guide of training model of DeepRacer on SageMaker, but fail in \"traning_job = sagemaker.create_training_job\" part:",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Excessive Memory use when deploying PipelineModel using the pre-build Scikit container in Sagemaker",
        "Question_creation_time":1663945337107,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2hdNVBreSFyX1iSPDRPMXw\/excessive-memory-use-when-deploying-pipeline-model-using-the-pre-build-scikit-container-in-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using the pre-build Scikit container in Sagemaker to deploy an endpoint based on a model that contains a 59.4 MB model.tar.gz file. The following line was used to deploy the endpoint:\n\nsm_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name)\n\nHowever, the after the endpoint was created, it fails to allocate memory to works. These error messages and warnings keep showing in the logs:\n\n[Errno 12] Cannot allocate memory [WARNING] Worker with pid 242 was terminated due to signal 9\n\nAs far as I know, the xlarge instance has 16 GB of memory. The endpoint memory usage is at 60% while it still fails to allocate memory to workers. May I ask if anyone has any insight on why this is happening and how to solve this issue without using an instance that has more memory?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access async endpoint created in console from notebook instance",
        "Question_creation_time":1668007391842,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLeSd5B8RHS7RD7KQIl5TQ\/access-async-endpoint-created-in-console-from-notebook-instance",
        "Question_upvote_count":0.0,
        "Question_view_count":15.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In SageMaker, I've created an async endpoint from a model. How does one access this endpoint from a notebook instance so that it can be used to make predictions?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to create Parallel Pipelines in Sagemaker",
        "Question_creation_time":1649666083053,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZtsbNf1GTAOnaTCrif-WJg\/is-it-possible-to-create-parallel-pipelines-in-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":373.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I want to bind processing pipeline to multiple training pipeline. I just want to compare algorithm accuracy. Same dataset will be trained by multiple algorithms and will be predicted by them. My goal for the future is consolidate predict results of different algorithms and generate combined\/consolidated resulst. Is is possible to do in SageMaker.\n\nExample Schema:\n\n            - Train_Algo1      \n Process    - Train_Algo2    - Predict Result\n            - Train_AlgoN",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"can sagemaker batch transform process input files with new line character ?",
        "Question_creation_time":1649208896916,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrZKVALwvQjCcoBn4bYUayg\/can-sagemaker-batch-transform-process-input-files-with-new-line-character",
        "Question_upvote_count":0.0,
        "Question_view_count":168.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"example provided in the aws documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, see sample input csv can be structured like a sample below. is there a link, to see the actual csv file that is used in an working example , instead of the sample posted in the docs. it is also mentioned that each record is one per line and no end of line character is allowed. what if the input type is other than csv, like json? do the same rules apply to json files as well, one record per line, or can one record be spread across multiple lines ,what about new line character in that case?\n\nRecord1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\nRecord2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n...\n...",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Lambda for data processing - Sagemaker",
        "Question_creation_time":1669522752249,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-EUVNgsoRViEFMwlZbcIAg\/using-lambda-for-data-processing-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":20.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have created a docker image which has Entrypoint as processing.py. This script is taking data from \/opt\/ml\/processing\/input and after processing putting it \/opt\/ml\/processing\/output folder.\n\nFor processing the data I should put the file in \/opt\/ml\/processing\/input from s3 and then pick processed file from \/opt\/ml\/processing\/output into S3.\n\nFollowing script in sagemaker is doing it properly:\n\nfrom sagemaker.processing import Processor, ProcessingInput, ProcessingOutput import sagemaker\n\ninput_data = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Data\/Training\/Churn_Modelling.csv' output_dir = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Outputs\/' image_uri = '057036842446.dkr.ecr.ap-south-1.amazonaws.com\/aws-docker-repo:latest' aws_role = sagemaker.get_execution_role()\n\nprocessor = Processor(image_uri= image_uri, role=aws_role, instance_count=1, instance_type=\"ml.m5.xlarge\")\n\nprocessor.run(inputs=[ProcessingInput( source=input_data, destination='\/opt\/ml\/processing\/input')], outputs=[ProcessingOutput(source='\/opt\/ml\/processing\/output', destination=output_dir)] )\n\nCould someone please guide how this can be executed with lambda function? It is not recognizing sagemaker package, second there is a challenge in placing file before the script execution and pick processed files.\n\nI am trying codepipeline to automate this operation.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio PyTorch 1.8 kernel has no PyTorch, Numpy, or Matplotlib module",
        "Question_creation_time":1639384597526,
        "Question_link":"https:\/\/repost.aws\/questions\/QUns1rahq-ShmzYk0GJoLGWA\/sage-maker-studio-py-torch-1-8-kernel-has-no-py-torch-numpy-or-matplotlib-module",
        "Question_upvote_count":0.0,
        "Question_view_count":210.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm working with SageMaker studio with the following options:\n\nkernel: PyTorch 1.8 Python 3.6 GPU optimized.\ninstance: ml.g4dn.xlarge\n\nWhen running import torch numpy, matplotlib or PIL, I'm getting the No module named 'X' error. No matter when using pip install in a cell above, it will not be imported. Is this a problem only I am encountering with the new PyTorch 1.8 kernel? It also happens with the CPU-optimized version. However, PyTorch 1.6 kernel does not throw an error.\n\nWhen running conda list, I get the output without any of the previously mentioned modules.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio domain lifecycle configuration for hosting VSCode not working and no error logs",
        "Question_creation_time":1652176660406,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfxuiAWeXShmE1q7BfMdhCw\/sage-maker-studio-domain-lifecycle-configuration-for-hosting-vs-code-not-working-and-no-error-logs",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,\n\nI've followed the guide provided in this medium blog by senior AWS ML SA: https:\/\/towardsdatascience.com\/hosting-vs-code-in-sagemaker-studio-f211385e25f7\n\nI've created the lifecycle configuration (jupyterlab app) for the following bash script:\n\n#!\/bin\/bash\n\nmkdir -p vscode\ncd vscode\nsudo su\ncurl -fsSL https:\/\/code-server.dev\/install.sh | sh\nexit\n\n\nHowever, when I attach the configuration to the domain or a specific user's jupyter server in the SM Studio, the JupyterServer App fails to be created with the following message displayed on the loading page of SM Studio's jupyterlab:\n\n\"The JupyterServer app default encountered a problem and was stopped.Details: ConfigurationError: LifecycleConfig execution failed with non zero exit code 1 for script arn:aws:sagemaker:eu-west-3:615740825886:studio-lifecycle-config\/install-vscode-on-jupyterserver. Check https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lcc-debug.html for debugging instructions.\"\n\nI checked the page for debugging instructions, specifically the part where I am instructed to check the cloudwatch logs for the log group: aws\/sagemaker\/studio The problem is this log group does not exist in CloudWatch (I have admin permission).\n\nI would like to have this set up for the entire team, Any help would be greatly appreciated!\n\nBest, Ruoy",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Question_creation_time":1602169065000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sage-maker-endpoint-url-authentication-authorization",
        "Question_upvote_count":0.0,
        "Question_view_count":282.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Need validation:\n\nOnce the SageMaker endpoint is deployed. It can be invoked with the Sagemaker Runtime API InvokeEndpoint OR it can be invoked using the endpoint URL+HTTP AZ headers (below).\n\nSuccessful deployment also exposes a URL (on the console) that has the format:\n\nhttps:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/ENDPOINT-NAME\/invocations\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique endpoint name!! THAT IS to invoke this URL it needs to have the \"HTTP Authorization headers\" (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. (Test with HTTP\/AZ headers pending)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"sagemakee endpoint failing with \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\"",
        "Question_creation_time":1669407059790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbzR_PqclRoK5dmfEzR53wg\/sagemakee-endpoint-failing-with-an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation-received-client-error-413-from-primary-and-could-not-load-the-entire-response-body",
        "Question_upvote_count":0.0,
        "Question_view_count":64.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I have created sagemaker endpoint by following https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/20_automatic_speech_recognition_inference\/sagemaker-notebook.ipynb and this is failing with error \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\".\n\nThe predict function returning me following error but CW log does not have any error details for the endpoint.\n\nModelError Traceback (most recent call last)\n\/tmp\/ipykernel_16248\/2846183179.py in\n2 # audio_path = \"s3:\/\/ml-backend-sales-call-audio\/sales-call-audio\/1279881599154831602.playback.mp3\"\n3 audio_path = \"\/home\/ec2-user\/SageMaker\/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face\/1279881599154831602.playback.mp3\" ## AS OF NOW have stored locally in notebook instance\n----> 4 res = predictor.predict(data=audio_path)\n5 print(res)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n159 data, initial_args, target_model, target_variant, inference_id\n160 )\n--> 161 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n162 return self._handle_response(response)\n163\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n493 )\n494 # The \"self\" in this scope is referring to the BaseClient.\n--> 495 return self._make_api_call(operation_name, kwargs)\n496\n497 _api_call.name = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n912 error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n913 error_class = self.exceptions.from_code(error_code)\n--> 914 raise error_class(parsed_response, operation_name)\n915 else:\n916 return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n`",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Mandate user to enable encryption while Sagemaker notebook creation?",
        "Question_creation_time":1658816453567,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA0H60oMZTUy1JgSwFVXV4g\/mandate-user-to-enable-encryption-while-sagemaker-notebook-creation",
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"1.We would like to mandate user to enable KMS encryption while creating Sagemaker notebooks, I would like to know any methods via policy or any other way?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create a serverless endpoint in sagemaker?",
        "Question_creation_time":1645674841041,
        "Question_link":"https:\/\/repost.aws\/questions\/QULRy50Vd7SW6KT0MMzk4NeQ\/how-to-create-a-serverless-endpoint-in-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":277.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am recreating an endpoint currently working in sagemaker for inference to a serverless endpoint. I am using one of the images ( huggingface-pytorch-inference:1.9.1-transformers4.12.3-cpu-py38-ubuntu20.04) found here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md.\n\neverything works when i choose non serverless, i.e. provisioned option for endpoint configuration , but when i try to create one with serverless option it fails. error messages are below ( from the logs in cloudwatch) . starting with python and log4j error at the end.\n\n'python: can't open file '\/user\/local\/bin\/deep_learning_container.py': [Errno 13] permission denied. Requirement already satisfied: transformers in \/opt\/conda\/lib\/pythong3.6 ..... ..... Warning: MMS is using non-default JVM parameters: -XX: -UseContainersupport Failed to reap children process log4j: ERROR setfile(null,true) call failed. java.io.FileNotFoundException: logs\/mms_log.log (No Such file or directory)\n\nwhy am i getting this error ???\n\nFYI - i have set memory to maximum allowed memory size of 6gb. for the serverless option.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Post Annotation Lambda Function for Custom Labeling Job",
        "Question_creation_time":1659025093082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYzKc8ISiT4eU9cXCsdUqEg\/custom-post-annotation-lambda-function-for-custom-labeling-job",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI have implemented a post annotation lambda function for my sagemaker ground truth custom job.\n\nAfter the annotations are finished, the results come after the consolidation of the annotations are saved in a subdirectory called \"iteration_X\" of \"annotations\/consolidated-annotation\/consolidation-response\/\".\n\nHowever, the outcome of the annotations is never successful and from the log of the lambda function used for the post annotation I always receive this type of error:\n\n{\n    \"labeling-job-name\": \"labeling-job-full-dataset-test-giusy-10\",\n    \"event-name\": \"ANNOTATION_CONSOLIDATION_LAMBDA_SCHEMA_MATCHING_FAILED\",\n    \"event-log-message\": \"ERROR: Annotation consolidation Lambda response did not match expected data format for line 1.\"\n}\n\n\nBased on this guide (https:\/\/docs.aws.amazon.com\/id_id\/sagemaker\/latest\/dg\/sms-custom-templates-step3-lambda-requirements.html) I made sure that my lambda function returns:\n\nRESPONSE:\n\n\n[\n  {\n    \"datasetObjectId\": \"1\",\n    \"consolidatedAnnotation\": {\n      \"content\": {\n        \"annotations\": {\n          \"relations\": [\n            {\n              \"subj\": \"CW\",\n              \"predicate\": \"adjust\",\n              \"obj\": \"key\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"with\",\n              \"obj\": \"right_hand\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"on\",\n              \"obj\": \"lock\"\n            }\n          ],\n          \"groundings\": {\n            \"pre_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 776.5,\n                \"top\": 219.5,\n                \"width\": 282.52,\n                \"height\": 246.5\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 716.4,\n                \"top\": 255.6,\n                \"width\": 93.60000000000002,\n                \"height\": 111.6\n              }\n            ],\n            \"pnr_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 974.16,\n                \"top\": 275.14,\n                \"width\": 287.21,\n                \"height\": 215.84\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 914.4,\n                \"top\": 291.6,\n                \"width\": 97.20000000000005,\n                \"height\": 90\n              }\n            ],\n            \"post_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 858.58,\n                \"top\": 240.54,\n                \"width\": 316.28,\n                \"height\": 209.37\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 741.6,\n                \"top\": 237.6,\n                \"width\": 61.19999999999993,\n                \"height\": 169.20000000000002\n              }\n            ]\n          },\n          \"timestamp\": \"0\",\n          \"clip_uid\": \"undefined\"\n        }\n      }\n    }\n  }\n]\n\n\nI can't figure out how to avoid this type of error and make the annotations go through when the job is finished.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to debug invocation timeout in sagemaker?",
        "Question_creation_time":1649727502799,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE4UPZjwNQveIG8zuZeXIgA\/how-to-debug-invocation-timeout-in-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":432.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am testing inference in sagemaker , by using one of the container listed here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below and with in inference.py file , i am overwriting functions like model_fn method and predict_fn. I tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"Model server did not respond to \/invocations request within 3600 seconds\" . I'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch is 6mb , the input files i'm using are way smaller than that but i still get that error.\n\nDirectory structure\n\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n\n\n\nfile : inference.py\n\nimport torch\nimport os\n\ndef model_fn(model_dir):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\ndef predict_fn():\n    \/\/\n\n\nbased on docs here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Debugger: cannot load training information of estimator",
        "Question_creation_time":1660301495882,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUl_ylpIuQUWy0N-CDqP_ag\/sage-maker-debugger-cannot-load-training-information-of-estimator",
        "Question_upvote_count":0.0,
        "Question_view_count":72.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using a SageMaker notebook for training a ML model. When I created and trained the estimator successfully with the following script, I could load the debugging information (s3_output_path) as expected:\n\nfrom sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\nrules = [\n    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n    Rule.sagemaker(rule_configs.vanishing_gradient()),\n    Rule.sagemaker(rule_configs.overfit()),\n    Rule.sagemaker(rule_configs.overtraining()),\n    Rule.sagemaker(rule_configs.poor_weight_initialization())]\n\ncollection_configs=[CollectionConfig(name=\"CrossEntropyLoss_output_0\", parameters={\n    \"include_regex\": \"CrossEntropyLoss_output_0\", \"train.save_interval\": \"100\",\"eval.save_interval\": \"10\"})]\n\ndebugger_config = DebuggerHookConfig(\n    collection_configs=collection_configs)\n\nestimator = PyTorch(\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\"ml.m5.xlarge\",\n#instance_type=\"ml.g4dn.2xlarge\",\nentry_point=\"train.py\",\nframework_version=\"1.8\",\npy_version=\"py36\",\nhyperparameters=hyperparameters,\ndebugger_hook_config=debugger_config,\nrules=rules,\n)\n\nestimator.fit({\"training\": inputs})\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\n\n\nAfter the kernel died, I attached the estimator and tried to access the debugging information of the training:\n\nestimator = sagemaker.estimator.Estimator.attach('pytorch-training-2022-06-07-11-07-09-804')\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\nrules_path = estimator.debugger_rules\n\n\nThe return values of these 2 functions were None. Could this be a problem with the attach-function? And how can I access training information of the debugger after the kernel was shut down?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ground Truth Job Validation post completion",
        "Question_creation_time":1668077364552,
        "Question_link":"https:\/\/repost.aws\/questions\/QURI4l_IFoR9SZyc-6wNPqgQ\/ground-truth-job-validation-post-completion",
        "Question_upvote_count":1.0,
        "Question_view_count":21.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We annotated 10 pdf files in Ground Truth, how do I validate the annotations done by the team ? Do we have any metrics ? Ex - How many annotations done in one pdf ? What is the confidence score for each annotation ?\n\nMy idea is that if i get this metrics, I will review the doc with less number of annotations and doc with low confidence score.\n\nCan Ground truth expers provide some insights in this ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"aws.amazon.commachine-learning\/pricing link is broken",
        "Question_creation_time":1664336034992,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvybuFyXJRnW4Q_LvXxbEaA\/aws-amazon-commachine-learning-pricing-link-is-broken",
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/requesting-real-time-predictions.html article\n\nlink => http:\/\/aws.amazon.commachine-learning\/pricing\/",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Sagemaker model endpoint data capture",
        "Question_creation_time":1668105022579,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJfA77iAoT2-EkVff7sc7LA\/aws-sagemaker-model-endpoint-data-capture",
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have deployed a sagemaker endpoint, with data capture enabled with a 100% sampling rate. But after running it multiple times, it is not storing all the inputs to the model,\n\nFor instance, after running 3 times, it saves only one time.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Question_creation_time":1606945520000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sage-maker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Question_upvote_count":0.0,
        "Question_view_count":495.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Clone Failed SageMaker MLOps Project Using Third-party Git Repos",
        "Question_creation_time":1668586359113,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhIvfTpxzRtWRE-dO20bHmQ\/clone-failed-sage-maker-ml-ops-project-using-third-party-git-repos",
        "Question_upvote_count":0.0,
        "Question_view_count":37.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to use MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline in my ML project. I created the project successfully using the template and all the seed code is available in the GitHub repos I specified. But when I try to clone the repo, I am getting the below error\n\nI see that the local path that has been specified in\n\nNo such file or directory: '\/home\/sagemaker-user\/home\/sagemaker-user\/cat-ml-test-1-p-mtd5ofsbdgva\/sagemaker-p-mtd5ofsbdgva-modeldeploy'\n\n\ndoes not sound quite right. But there's is no option to change the local path by myself also.\n\nHow can I solve this? Any leads are welcome TIA",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS sagemaker abalone example pipeline endpoint json rejected",
        "Question_creation_time":1656607883243,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJRDfNUpdR-WQgjsw_UOi5g\/aws-sagemaker-abalone-example-pipeline-endpoint-json-rejected",
        "Question_upvote_count":0.0,
        "Question_view_count":145.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We've just created a train\/build\/deploy template in AWS SageMaker which provides a deployment of an Abalone model. We're trying to test it via the Test Inference endpoint, but the JSON there is rejected with the following message:\n\nError invoking endpoint: Received client error (415) from model with message \"application\/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, application\/x-recordio-protobuf.\". See https:\/\/eu-west-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/USEngProbOfConversion-staging in account 607522716587 for more information.\n\nHowever the Test Inference endpoint only allows us to hit the endpoint with JSON - what can we do? Here's a screenshot (if this dropbox embed works):\n\nbut that's not working so here's the request dump:\n\n{\n  \"body\": {\n    \"s-x\": \"M\",\n    \"length\": 3,\n    \"diameter\": 5,\n    \"height\": 7,\n    \"whole_weight\": 45,\n    \"shucked_weight\": 34,\n    \"viscera_weight\": 23,\n    \"shell_weight\": 76\n  },\n  \"contentType\": \"application\/json\",\n  \"endpointName\": \"USEngProbOfConversion-staging\",\n  \"customURL\": \"\",\n  \"customHeaders\": [\n    {\n      \"Key\": \"sm_endpoint_name\",\n      \"Value\": \"USEngProbOfConversion-staging\"\n    },\n    {\n      \"Key\": \"\",\n      \"Value\": \"\"\n    }\n  ]\n}",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"make julia notebooks work in SageMaker",
        "Question_creation_time":1644167031317,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker",
        "Question_upvote_count":1.0,
        "Question_view_count":95.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I want to run a Jupyter notebook in SageMaker with a Julia kernel. There is very little documentation about this. There is this:\n\nhttps:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card\n\nI followed all the instructions, and Julia shows up in the JupyterLab launcher; but when I run it, Julia 1.17.1 shows up as the kernel and then dies. It appears to be trying, but then gives up and says \"No Kernel\" instead of \"Julia 1.17.1\" in the status line.\n\nIf I run the R kernel, all goes well. If I run the Julia kernel (which shows up in the list of available kernels!), I get the following error message:\n\nConnection failed\n\nA connection to the notebook server could not be established.\n\nThe notebook will continue trying to reconnect.\n\nCheck your network connection or notebook server configuration.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training Metric logging on SageMaker experiment tracking: how to get time-series metrics with visualisation",
        "Question_creation_time":1648058261935,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDNp9HXW9SCqdadORoXUX9g\/training-metric-logging-on-sage-maker-experiment-tracking-how-to-get-time-series-metrics-with-visualisation",
        "Question_upvote_count":0.0,
        "Question_view_count":702.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using the sagemaker python SDK to train a bespoke model. I have defined my metric_definition regexes and passed them to the estimator like:\n\nnum_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\nmetrics = [\n    {\"Name\": \"learning-rate\", \"Regex\": f\"lr: {num_re}\"},\n    {\"Name\": \"training:loss\", \"Regex\": f\"loss: {num_re}\"},\n    # ...\n]\nestimator = Estimator(\n    image_uri=training_image_uri,\n    # ...\n    metric_definitions=metrics,\n    enable_sagemaker_metrics=True,\n)\n\nWhen I run training, these metrics are visible in my logs and I can also see them in SageMaker Studio in Trial Components > Metrics (tab) as a grid of numbers like:\n\nName | Minimum | Maximum | Standard Deviation | Average | Count | Final value\n\nlearning-rate | 8.889 | 8.907 | 0.010392304845413657 | 8.898 | 4 |8.907\n\n...\n\nWhich suggests that the regexes are correctly matching on the logs\n\nHowever, I am not able to visualise any graphs for my metrics. I have tried all of:\n\nSagemaker Studio > Trial components > charts. It is only possible to plot things like learning-rate_min (i.e. a point value not a time-series metric)\nSageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section. Here I can see metrics like CPUUtilization over time but for my metrics there is just an empty graph for each metric that I have defined that says 'No data available'\nSageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section > View algorithm metrics (opens in CloudWatch) > Browse > select metric (e.g. learning-rate and 'Add to Graph' . I filter by the correct time period and go the Graphed metrics (1) tab, even after updating the period to 1 second I am not able to see anything on the graph.\n\nI'm not sure what the issue is here but any help would be much appreciated",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Model Spend",
        "Question_creation_time":1592312639000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sage-maker-model-spend",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Jumpstart in Sagemaker does not work",
        "Question_creation_time":1664537965056,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOGpNbAjoRASDDw1pQ5F6Fg\/jumpstart-in-sagemaker-does-not-work",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI am new to SageMaker, I created a domain, and I have an execution role, although when I use Sagemaker studio I cannot see all functionalities, such as the resources button, jumpstart button, etc. I have enabled 'SageMaker Projects and JumpStart '. I am also attaching a photo of what my UI in the studio looks like.\nAny help would be appreciated. Thanks in advance",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_creation_time":1557130614000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sage-maker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Question_upvote_count":0.0,
        "Question_view_count":260.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"hi there,\n\nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.\n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"\n\nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.\nCould anyone tell me how to solve it?\nI also came across the following articles, but i am not sure if it is the root cause.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker - Upload our own docker image on Amazon SageMaker",
        "Question_creation_time":1643870744946,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYIfUo3-0Qqyr4XunexHJXw\/aws-sage-maker-upload-our-own-docker-image-on-amazon-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":111.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am new to AWS SageMaker and i am using this technology for building and training the machine learning models. I have now developed a docker image which contains our custom code for tensorflow. I would like to upload this custom docker image to AWS SageMaker and make use of it.\n\nI have searched various links but could not find proper information on how to upload our own custom docker image.\n\nCan you please suggest me the process of uploading our own docker image to AWS SageMaker?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker training instance",
        "Question_creation_time":1660674061917,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4_NA-4cTS8aGkpKawI0qVA\/sagemaker-training-instance",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a doubt with choosing instance for training job in sagemaker. Is ml.m5.2xlarge with count as 2 and ml.m5.4xlarge are same ?\n\nI would like to know if there is any best practice guide to choose the instance for training in sagemaker.\n\nThank you \ud83d\ude42",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to export tresained models to ECR as container image",
        "Question_creation_time":1663258467464,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_creation_time":1602770746000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify instance type when training models using SageMaker AutoML Python SDK",
        "Question_creation_time":1642292454744,
        "Question_link":"https:\/\/repost.aws\/questions\/QUClX3PJmFSX-aIaTBlaGidw\/how-to-specify-instance-type-when-training-models-using-sage-maker-auto-ml-python-sdk",
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nIs there a way to specify the instance type when training models using the SageMaker AutoML Python SDK? The AutoML.deploy method takes an instance_type argument, but the AutoML.fit method does not take an instance_type argument.\n\nThanks, Stefan",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Built-in Algorithms",
        "Question_creation_time":1652686627960,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unauthorized AWS account racked up charges on stolen credit card.",
        "Question_creation_time":1649524594036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhV-lkkYyS1qaYFvsoPYiWg\/unauthorized-aws-account-racked-up-charges-on-stolen-credit-card",
        "Question_upvote_count":0.0,
        "Question_view_count":280.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My mother was automatically signed up for an AWS account or someone used her credentials to sign up. She did not know that she had been signed up, and it sat unused for 3 years. Last month, she got an email from AWS for \"unusual activity\" and she asked me to help her look into it. Someone racked up $800+ in charges in 10 days for AWS services she has never heard of, let alone used (SageMaker, LightSail were among the services). The card on the AWS account is a credit card that was stolen years ago and has since been cancelled. So when AWS tried to charge the card, it didn't go through.\n\nMy experience with AWS customer service has been unhelpful so far. Mom changed her AWS password in time so we could get into the account and contact support. I deleted the instances so that the services incurring charges are now stopped. But now AWS is telling me to put in a \"valid payment method\" or else they will not review the fraudulent bill. They also said that I have to set up additional AWS services (Cost Management, Amazon Cloud Watch, Cloud Trail, WAF, security services) before they'll review the bill. I have clearly explained to them that this entire account is unauthorized and we want to close it ASAP, so adding further services and a payment method doesn't make sense.\n\nWhy am I being told to use more AWS services when my goal is to use zero? Why do I have to set up \"preventative services\" when the issue I'm trying to resolve is a PAST issue of fraud? They also asked me to write back and confirm that we have read and understood the AWS Customer Agreement and shared responsibility model.\" Of course we haven't, because we didn't even know the account existed!\n\nAny advice or input into this situation? It's extremely frustrating to be told that AWS won't even look into the issue unless I set up these additional AWS services and give them a payment method. This is a clear case of identity fraud. We want this account shut down.\n\nSupport Case # is xxxxxxxxxx.\n\nEdit- removed case ID -Ann D",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Question_creation_time":1541494962000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sage-maker-notebook-through-vpc-interface-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":280.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n\n\"You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass null\/Nan values into the dataframe passed into batch transform",
        "Question_creation_time":1549531218000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtKq2ZetyTUmvDwS7i3ot2Q\/how-to-pass-null-nan-values-into-the-dataframe-passed-into-batch-transform",
        "Question_upvote_count":0.0,
        "Question_view_count":92.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to make inference using my Xgboost model on a dataset which has NaN values, now inherently Xgboost handles the NaN values, it does not throw any error while training with NaN values whereas the batch transform job gives the error ''could not convert string to float'' when it encounters NaN values into the dataset that is to be transformed.\nCan anyone help me as to how can I pass NaN values into my input dataset for the batch transform job?\nThanks a ton!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Export Autopilot model to GovCloud region",
        "Question_creation_time":1657645831488,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV6_OvmWMRjiuQIx82Z4_Eg\/export-autopilot-model-to-gov-cloud-region",
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi As AWS Sagemaker autopilot is not available in GovCloud region, is it possible to export a model trained on non-GovCloud environment in GovCloud environment.\n\nWhat I have done:\n\nRan Autopilot on non-GovCloud environment\nI was able to download the output model.joblib(preprocessing) and xgboost models from output bucket in S3 bucket\nI was not able to load the model.joblib preprocessing model since the sagemaker-sklearn-extention gives symbol not found error(https:\/\/issuehint.com\/issue\/awslabs\/ml-io\/28)\n\nThanks for your help and insights in exporting the model to GovCloud environment.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Question_creation_time":1539775195000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloud-formation-with-sage-maker-life-cycle-config-without-leaving-the-instance-running",
        "Question_upvote_count":0.0,
        "Question_view_count":306.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation. I have some initialization to do at creation time (clone a github repo, etc.). That all works fine. The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a Stopped state. A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige. I would have thought it would let me do the same in the lifecycle config. Unfortunately, SageMaker still has the notebook in the Pending state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Receiving consistent AccessDenied errors",
        "Question_creation_time":1606179025000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ3KwMxUsREup-lSqlA44Hg\/receiving-consistent-access-denied-errors",
        "Question_upvote_count":0.0,
        "Question_view_count":55.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to use SageMaker Notebook Instances, but consistently receive AccessDenied errors for commands that my IAM role should have access to (and for commands that worked the last time I tried several weeks ago). For example:\n\naws s3 ls results in An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied despite my role having the AmazonS3FullAccess policy attached.\n\nAlso aws ecr describe-repositories --repository-names \"sagemaker-decision-trees\" results in An error occurred (AccessDeniedException) when calling the DescribeRepositories operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role\/AmazonSageMaker-ExecutionRole-20201123T151452\/SageMaker is not authorized to perform: ecr:DescribeRepositories on resource: arn:aws:ecr:us-east-2:XXXXXXXXXX:repository\/sagemaker-decision-trees with an explicit deny despite my role having the AmazonEC2ContainerRegistryFullAccess policy attached.\n\nOne thing that seems new is that \"SageMaker\" is appended to my user ARN. I can't remember seeing errors with this appended before.\n\nNote: I've replicated these errors with several combinations of configurations:\n\na new IAM role (which I created in the SageMaker console to have AmazonSageMakerFullAccess to any S3 bucket)\nfresh notebook instance\nwith (and without) a VPC\nAlso, these commands all work when run outside of a notebook instance (i.e. when run locally from my laptop).\n\nI'm guessing there's some problem with my account setup, but not sure what to try next.\nThanks.\n\nEdited by: DJAIndeed on Nov 24, 2020 8:35 AM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker requirements.txt unable to find certain packages",
        "Question_creation_time":1649797399439,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl7yzFGwfSEauI7yesYeXiw\/sagemaker-requirements-txt-unable-to-find-certain-packages",
        "Question_upvote_count":0.0,
        "Question_view_count":181.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI'm trying to run a sagemaker job on p3.2xlarge instance using the PyTorch estimator. My script has dependencies on several packages (possibly not pre-installed on the instance) for which I have a requirements.txt file. After loading the instance, it installs several packages but the job fails with following error:\n\nsagemaker-training-toolkit ERROR InstallRequirementsError: Command \"\/opt\/conda\/bin\/python3.6 -m pip install -r requirements.txt\" ERROR: Could not find a version that satisfies the requirement scikit_image==0.18.3 ERROR: No matching distribution found for scikit_image==0.18.3\n\nIt fails for other packages too, like h5py, numpy etc.\n\nAny help is greatly appreciated.\n\nThank you, Aditya",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to load large amount of data from S3 onto Sagemaker?",
        "Question_creation_time":1641932571368,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4m2DyyJQSSCL1QqclXS6ZA\/how-to-load-large-amount-of-data-from-s-3-onto-sagemaker",
        "Question_upvote_count":0.0,
        "Question_view_count":1033.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a notebook on Sagemaker Studio, I want to read data from S3, I am using the code bellow:\n\ns3_client = boto3.client('s3') bucket = 'bucket_name' data_key = 'file_key.csv' obj = s3_client.get_object(Bucket=bucket, Key=data_key) df = pd.read_csv(io.BytesIO(obj['Body'].read())) df.head()\n\nIt works for small datasets but fails along the way with the dataset I'm trying to load which is 15GB. I changed the instance to ml.g4dn.xlarge ( accelerated computing, 4vCPU + 16GiB + 1 GPU), still fails. what am I missing here? Is is about the instance type, or about the code? What is the best way to import large datasets from S3 to sagemaker?\n\nThank you",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Giving weights to event types in amazon personalize",
        "Question_creation_time":1639825094332,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSogRKFlfRzC5b8afIwPybQ\/giving-weights-to-event-types-in-amazon-personalize",
        "Question_upvote_count":0.0,
        "Question_view_count":174.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"For the VIDEO_ON_DEMAND domain, some use cases include multiple event types. For example, the 'Top picks for you' use case includes two event types 'watch' and 'click'. Is 'watch' given more weight than 'click' when training the model? In general, when there is more than one event type, do domain recommenders give more weight to some event types?\n\nIn our use case, we have a platform that recommends video content. However, we have multiple event types, and some events need to be given more weight than others. Below is the list of our event types in the order of their importance:\n\nSHARE > LIKE > WATCH_COMPLETE > WATCH_PARTIAL > STARTED > SKIP\n\nSo when training the model, we would want 'SHARE' to have more weight than 'LIKE', and 'LIKE' to have more weight than 'WATCH_COMPLETE' and so on.\n\nI was looking into custom solutions. It looks like there is no way to give weights when using Personalize's custom solutions as mentioned in this post...\n\nSo when using Amazon Personalize, should we use domain recommenders or build custom solutions for our use case?\n\n**If we cannot give weights to different event types using Personalize, then what are alternatives? **Should we use Amazon SageMaker and build models from scratch? Open to any and all suggestions.\n\nThank you!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use categorical data without converting?",
        "Question_creation_time":1638858572453,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_mbjTldXQY2DHVM7-rMb_g\/how-to-use-categorical-data-without-converting",
        "Question_upvote_count":0.0,
        "Question_view_count":69.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to develop a model in SageMaker, but my data is contained categorical type of data, and I would not want to convert it. In the old Machine Learning, I can use back the same data and train the model without any issue. However, when I tried the built-in algorithm, I got the error message that wanted me to convert the data. Is there anyway to do the same as the old Machine Learning without converting? Thank you.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"greengrass ml component",
        "Question_creation_time":1666267168133,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4YAAy55MQQChcPc0JhyWWA\/greengrass-ml-component",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello I am studying Greengrass machine learning components. I have a question. I'm going to distribute Greengrass custom machine learning components to Raspberry Pi now. The artifacts associated with the recipe in this component contain inference codes uploaded to S3. And this inference code has a function of determining who is by photographing the user's face in real time. I'm curious. If the components are distributed well in Raspberry Pi, will the inference code continue to run? Or, do I have to deploy components every time I try to run an inference? Thank you.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Multi Model endpoint creation fails while creating for model built on container sagemaker-scikit-learn:0.23-1-cpu-py3",
        "Question_creation_time":1653574428837,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHZiKPwmxRyy0C0Nc0ONwuQ\/sage-maker-multi-model-endpoint-creation-fails-while-creating-for-model-built-on-container-sagemaker-scikit-learn-0-23-1-cpu-py-3",
        "Question_upvote_count":0.0,
        "Question_view_count":98.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am working on a use-case where I am using SageMaker multi-model endpoint for model inference and the models are trained using Databricks MLFlow platform. When I tried deploying a model trained from Databricks MLFlow platform on a single endpoint on SageMaker then it worked fine but the creation of multi-model endpoint for 'sagemaker-scikit-learn:0.23-1-cpu-py3' container is failed with the following error:\n\nCode Snippet::>> name = \"sample-mme\"\n\nsagemaker_client = boto3.client('sagemaker')\n\nmodel_path = \"s3:\/\/test-bucket\/multi-models\"\n\nexecution_role_arn = \"IAM:\/\/sample-role\"\n\nBASE_IMAGE = image_uris.retrieve( region=region, framework=\"sklearn\",version='0.23-1',image_scope='inference' )\n\ncontainer = { 'Image': BASE_IMAGE, 'ModelDataUrl': model_path, 'Mode': 'MultiModel', 'MultiModelConfig': { 'ModelCacheSetting': 'Enabled' } }\n\nmodel_response = sagemaker_client.create_model( ModelName=name, ExecutionRoleArn=execution_role_arn, Containers=[container] )\n\nconfig_response = sagemaker_client.create_endpoint_config( EndpointConfigName=f'{name}-config', ProductionVariants=[ { 'InstanceType': instance_type, 'InitialInstanceCount': instance_count, 'InitialVariantWeight': 1, 'ModelName': name, 'VariantName': 'AllTraffic' } ] )\n\nresponse = sagemaker_client.create_endpoint( EndpointName=f'{name}-endpoint', EndpointConfigName=f'{name}-config' )\n\nEndpoint creation is taking a lot if time and failing with the following error message :\n\nsagemaker_containers._errors.ImportModuleError: 'NoneType' object has no attribute 'startswith'\n\nPlease provide me with some help to fix this.\n\nAlso, my understanding is that I can train a model on the DataBricks MLFlow platform using sklearn libraries, and then I can store model artifacts \"model.tar.gz\" under the s3 directory for storing all multi-models. Now I can create a multi-model endpoint in SageMaker using the same s3 directory as the model path and using the above code. Once the endpoint is ready, I can do inference by providing the target model. Please let me know if my understanding is correct and share any relevant documents to follow for my use case.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GC overhead limit exceeded",
        "Question_creation_time":1654390632353,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc_WeDqcTjitN1bkIJRosg\/gc-overhead-limit-exceeded",
        "Question_upvote_count":0.0,
        "Question_view_count":87.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a modest size dataset, and I am running Jupyter Notebook in Sagemaker (instance type ml.c5.xlarge with 200G instance size). I receive the error message \" GC overhead limit exceeded\" Everything ran fine with small data size. BTW, I need to go through the dataframe one row at a time using df.collect(), which seems t be an expensive operation... Would you suggest another way of accomplishing this? I would appreciate your kind help.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Jumpstart 'Explain Credit Decisions' fails to deploy",
        "Question_creation_time":1660818001611,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMx_45pb_TLOfMOZINMG2JA\/sagemaker-jumpstart-explain-credit-decisions-fails-to-deploy",
        "Question_upvote_count":0.0,
        "Question_view_count":85.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Im trying to use the Sagemaker Jumpstart 'Explain Credit Decisions' via the Studio Jumpstart menu. However, everything works as instructed until it hits the 'glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)' step in the datasets notebook.\n\nThis produces a \"failed to execute with exception Internal service error: Invalid Input Provided\" (error in the Glue console) and falls over on the job part of the glue job.\n\nDoes anyone have any ideas? This is as much information as is available in the console logs.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Code running slow on Sagemaker notebook instance for the first time it runs",
        "Question_creation_time":1591811233000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeQJN4BFCTsikAct_m9BeZw\/code-running-slow-on-sagemaker-notebook-instance-for-the-first-time-it-runs",
        "Question_upvote_count":0.0,
        "Question_view_count":615.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello!\n\nI've an issue running the code on SageMaker. I am running my code on SageMaker, which runs my code slowly for the first time, but runs with proper speed, the second time around (I guess there's something getting stored in the cache). Few days back, it was running with the same speed all the time. Whatever I run, be it a model (The model which took just 5 minutes for one epoch when it worked fine estimates 3 hours of running time) \/ just a code reading the data present in my files, it runs too slow. What could be a possible solution for this? I tried changing the notebook instance types as well, but in vain. I've been struggling for 2 days. It'll be great if someone could help me out a bit soon so that I progress ahead in my project. Thanks in advance!\n\nEdited by: vbsrinivasan on Jun 10, 2020 10:47 AM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker python sdk installation troubles",
        "Question_creation_time":1597398744000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZe9xubsHTuyRPCUGXvi40Q\/sagemaker-python-sdk-installation-troubles",
        "Question_upvote_count":0.0,
        "Question_view_count":145.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI've tried following a tutorial to use AWS sagemaker in script mode from my local linux VM, but I can't even get the basics working.\n\nSteps:\n\n\u279c ~> python3 --version\nPython 3.6.9\n\n\u279c ~> pip3 install sagemaker\n...\nSuccessfully installed boto3-1.14.42 botocore-1.17.42 importlib-metadata-1.7.0 packaging-20.4 protobuf3-to\n-dict-0.1.5 s3transfer-0.3.3 sagemaker-2.3.0 smdebug-rulesconfig-0.1.4 zipp-3.1.0\n\n\u279c ~> cat sagemaker.py\nimport sagemaker\nimport boto3\n\nsess = sagemaker.Session()\n\n\u279c > python3 sagemaker.py\nTraceback (most recent call last):\nFile \"sagemaker.py\", line 1, in <module>\nimport sagemaker\nFile \"\/sagemaker.py\", line 4, in <module>\nsess = sagemaker.Session()\nAttributeError: module 'sagemaker' has no attribute 'Session'\n\nI also have the aws cli (version 2) installed, and configured using IAM credentials that have full rights, so that's not related.\n\nWhat is the problem with my python sdk install? TIA",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"Failure reason Image size 12704675783 is greater than supported size 10737418240\" when creating serverless endpoint in SageMaker.",
        "Question_creation_time":1657243407002,
        "Question_link":"https:\/\/repost.aws\/questions\/QU90699ONgQD2t2HUKzm9AUA\/failure-reason-image-size-12704675783-is-greater-than-supported-size-10737418240-when-creating-serverless-endpoint-in-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":220.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"How to reproduce the error: We want to run Python Inference in SageMaker. Because our model is pre-trained out side the SageMaker and has some special logic, so we need to create customer image. We see the document https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html#prebuilt-containers-extend-tutorial We use the 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker to be the base image. We wrote a dockerfile and use \"docker build\" to create a new image. Also, use \"docker push\" to push new image to Amazon ECR. We pushed it to 935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage Then, we follow the document: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html Then, we went to SageMaker console https:\/\/us-east-1.console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/models We created model. We input the \"935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage\" of our new image to \"Location of inference code image\". Then, we create Endpoint configuration. Then, we create Endpoint. But the Endpoint shows \"Failure reason Image size 12704675783 is greater than supported size 10737418240\".",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker GT Streaming Labelling Job Internal Server Error (Job Failed)",
        "Question_creation_time":1661525322145,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZD0isCgLSYqxXhdah_0jkQ\/sage-maker-gt-streaming-labelling-job-internal-server-error-job-failed",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I launched a Sagemaker Ground Truth Labeling Job with a vendor for 3D Point Cloud Object Tracking using the API. Yesterday, my job failed with the reason for failure\n\nInternalServerError: We encountered an internal error. Submit a new job.\n\n\nThe last three Cloudwatch logs before I saw the failure were\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"BATCH_ANNOTATION_STATUS_EVALUATED\", \"event-log-message\": \"Batch of annotation tasks completed with status FAILED. \" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"EXPORTED_LABELED_MANIFEST\", \"event-log-message\": \"Labeled manifest written to S3 output location.\" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"IDLE_TIMER_COUNT_INTERRUPTED\", \"event-log-message\": \"System detected incoming objects. Timer to monitor idleness was reset.\" }\n\nMy vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. In addition, I set the expiration time for my job to be 30 days, and this failed at 20 days. What could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_creation_time":1640092026826,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Question_upvote_count":0.0,
        "Question_view_count":529.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n\nChoose the user.\nOn the User Details page, for each non-failed app in the Apps list, choose Delete app.\nOn the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\nWhen the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_creation_time":1664542013756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi-model, Multi-container and Variants - what are the possible combinations?",
        "Question_creation_time":1643789695844,
        "Question_link":"https:\/\/repost.aws\/questions\/QUw1jokF5cQLmWHpj0hBGgtg\/multi-model-multi-container-and-variants-what-are-the-possible-combinations",
        "Question_upvote_count":0.0,
        "Question_view_count":126.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"This question is mostly for educational purposes, but the current SageMaker documentation does not describe whether these things are allowed or not.\n\nLets suppose I have:\n\na XGBoost_model_1 (that needs a XGBoost container)\na KMeans_model_1 and a KMeans_model_2 (both require a KMeans container)\n\n1. Here's the first question - can I do the following:\n\ncreate a Model with InferenceExecutionConfig.Mode=Direct and specify two cointainers (XGBoost and KMeans with Mode: MultiModel)\n\nThat would enable the client:\n\nto call invoke_endpoint(TargetContainer=\"XGBoost\") to access the XGBoost_model_1\nto call invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\") to access the KMeans_model_1\nto call invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") to access the KMeans_model_2\n\nI don't see a straight answer in the documentation whether combining Multi-Model containers with Multi-container endpoint is possible.\n\n2. The second question - how does the above idea work with ProductionVariants. Can I create something like this:\n\nVariant1 with XGBoost serving XGBoost_model_1 having a weight of 0.5\nVariant2 with a Multi-container having both XGBoost and KMeans (with a MultiModel setup) having a weight of 0.5\n\nSo that the client could:\n\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\") to access the KMeans_model_1\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") to access the KMeans_model_2\ncall invoke_endpoint(TargetVariant=\"Variant1\") to access the XGBoost_model_1\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"XGBoost\") to access the XGBoost_model_1\n\nIs that combination even possible?\n\nIf so, what happens when the client calls the invoke_endpoint without specifying the variant? For example:\n\nwould invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") fail 50% of the time (if it hits the right variant then it works just fine, if it hits the wrong one it would most likely result with a 400\/500 error (\"incorrect payload\")?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to run python 2.7 scripts on a computer cluster",
        "Question_creation_time":1637171846430,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/631040\/importerror-cannot-import-name-outputfiledatasetco.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am aware that azureml will drop support for python 2.7, but I have got some old codes and have to finish training the models. Since I will not use the codes afterwards anyway, so I do not want to spend much time to port to python 3.\n\nAs I tried to run the codes in python 2.7 on a compute cluster, I got the error ImportError: cannot import name OutputFileDatasetConfig coming from this line:\nfrom azureml.data import OutputFileDatasetConfig\nThe environment, that I have created for python 2.7, has azureml-core v1.1.5. I cannot find any documentation for this version, so I do not know, if it supports OutputFileDatasetConfig.\n\nCan someone tell me, how I can run my codes in python 2.7 on compute clusters? Thanks!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Question_creation_time":1651101620807,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"inference pipeline option not available",
        "Question_creation_time":1626831334850,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/483466\/inference-pipeline-option-not-available.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"After running the pipeline, I do not see the dropdown option for create inference pipeline, only the Run or clone option...any idea?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML Pickle file size Azure Machine Learning Service",
        "Question_creation_time":1599612419390,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?\n\nDoes it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?\nThanks,\n\nJohn",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Replacement for Azure ML Classic Excel Add In",
        "Question_creation_time":1647680517643,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/778717\/replacement-for-azure-ml-classic-excel-add-in.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"As far as I can tell there is no way to use the Excel add in for Azure ML using the new Azure ML service, it only works for the Classic. Is there any plan to provide a replacement add in that brings this functionality to the new Azure ML before Classic stops being supported in 2024?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Studio and Git\/AzureDevOps",
        "Question_creation_time":1618340223767,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/355882\/azure-ml-studio-and-gitazuredevops.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We have created an Azure ML Studio based proof of concept. I would like to get the \"source code\" into a source code repository. We use Git on AzureDevOps.\n\nI'm at a loss where to begin since the designer has no source files with which to interact. All of the project is point-and-click via the Designer. This is a ML project built following this tutorial: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\n\nWe've created our own models, data, etc and have a solution.\n\nWhat artifacts from an Azure ML Studio project should be included in a version control system? Where are these files located?\n\nThanks for any insight.\n\n-jeremiah",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AZURE ML - Web Service",
        "Question_creation_time":1616267169727,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323764\/azure-ml-web-service.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"How to configure the input fields with drop down values from the experiment. Eg: if car make is a field, the input field for the car make should start showing options when you start entering.\n\n\nList item\n\nIn the below example, fuel field should show drop down values like, diesel, petrol etc. ![79842-image.png][1] [1]: \/answers\/storage\/attachments\/79842-image.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AML - AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available.",
        "Question_creation_time":1667553245710,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1075753\/aml-assetexception-error-with-code-can39t-connect.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello Microsoft Q&A Team,\n\nI get the error\n\nAssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available\n\nwhen executing the following command:\n\npipeline_job = ml_client.jobs.create_or_update(\npipeline_job, experiment_name=\"data_preparation\"\n)\npipeline_job\n\nYesterday the command worked without an error. I did not make any changes. So I have no idea, what the problem is.\n\nThanks for helping me out.\n\nCheers\n\nLukas",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Result with coordinator convertion",
        "Question_creation_time":1648416629250,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/789141\/result-with-coordinator-convertion.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I wonder how could I convert the result of boundingbox of form recognizer into image coordinate to visualize the overlay image and recognized data.\n\nI could not have that accomplished because it is not similar to normal coordinates.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to sign into Azure Machine Learning Studio (classic), page constantly refreshes.",
        "Question_creation_time":1647878967167,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/780770\/unable-to-sign-into-azure-machine-learning-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI am trying to sign into my Free Workspace within the Microsoft Azure Machine Learning Studio (classic).\n\nI am trying to access the RICT2 Prediction and Classification Experiment: https:\/\/gallery.azure.ai\/Experiment\/RICT-Prediction-and-Classification-GB-Single-Year-v4-0\n\nThe page refreshes inexplicably on a loop several times, before displaying a sign-in error.\n\nWould anyone be able to assist?\n\nThank you!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Hyperdrive",
        "Question_creation_time":1617723280093,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/346257\/azure-ml-hyperdrive.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When running the hyperdrive step, I would like to get the hyper parameters that were selected for the best model and export them to use in a subsequent model. How would I got about doing that? I saw a method get_hyperparameters but from what I can tell that just gets all child runs. I am essentially wanting to use the same model but change alpha levels.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"In multi step pipeline execution, how to maintain the data type of the columns when pass the dataset to next step",
        "Question_creation_time":1645805304397,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/751127\/in-multi-step-pipeline-execution-how-to-maintain-t.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"i am building a pipeline with multiple steps.\n\nStep 1 - Read the data from tabular dataset(with proper data types) , apply transformation and create an output dataset which will be passed as input to the step 2. However when i opened this dataset from the pipeline run log, the datatype all become string instead of maintaining the original data types of the input tabular data set\n\n\nStep 2 - use the output dataset of step 1 as input and apply some more transformations. However i have some logic based on data types which doesn't work because intermediate data set does not maintain the same data structure\n\nis there anyway we can maintain the original data types\/schema structure in the intermediate datasets?\n\nHere is some snippets on my code :\n\nfeature_work = (\nOutputFileDatasetConfig(\nname=\"data_enhanced_add_global_variables\",\ndestination=(def_blob_store, \"data\/processed\/output\/1\"),\n)\n.read_delimited_files()\n.as_upload(overwrite=True)\n\n\n\n\nfeature_engineering_step_1 = PythonScriptStep(name = \"1_feature_engineering\",\n#source_directory = experiment_folder,\nscript_name = \"1_feature_engineering.py\",\narguments = ['--input-data', data_aggregate_DS.as_named_input('raw_data'),\n'--prepped-data', feature_work],\n#outputs=[prepped_data_folder],\noutputs=[feature_work],\ncompute_target = compute_name,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n# Step 2\nfeature_engineering_step_2 = PythonScriptStep(name = \"2_feature_engineering\",\n#source_directory = experiment_folder,\nscript_name = \"2_feature_engineering.py\",\narguments = ['--input-data', feature_work.as_input(name='raw_data'),\n'--prepped-data', feature_work1],\noutputs=[feature_work1],\ncompute_target = compute_name,\nrunconfig = pipeline_run_config,\nallow_reuse = True)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying from Azure ML studio Designer is giving error in deploying real time inference endpoint",
        "Question_creation_time":1645328557520,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/742817\/deploying-from-azure-ml-studio-designer-is-giving.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"2022\/02\/20 03:25:31 Downloading source code...\n2022\/02\/20 03:25:32 Finished downloading source code\n2022\/02\/20 03:25:32 Creating Docker network: acb_default_network, driver: 'bridge'\n2022\/02\/20 03:25:32 Successfully set up Docker network: acb_default_network\n2022\/02\/20 03:25:32 Setting up Docker configuration...\n2022\/02\/20 03:25:33 Successfully set up Docker configuration\n2022\/02\/20 03:25:33 Logging in to registry: c89d3aeb8176436a9d4c29a07e6381fb.azurecr.io\n2022\/02\/20 03:25:33 Successfully logged into c89d3aeb8176436a9d4c29a07e6381fb.azurecr.io\n2022\/02\/20 03:25:33 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n2022\/02\/20 03:25:33 Scanning for dependencies...\n2022\/02\/20 03:25:34 Successfully scanned dependencies\n2022\/02\/20 03:25:34 Launching container with name: acb_step_0\nSending build context to Docker daemon 66.56kB\n\nStep 1\/21 : FROM mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04@sha256:922be75bb02cf219cb86ac4734bcbda41d956314a4b3b7a4febc807d0a803f6e\nmcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04@sha256:922be75bb02cf219cb86ac4734bcbda41d956314a4b3b7a4febc807d0a803f6e: Pulling from azureml\/openmpi3.1.2-ubuntu18.04\n68e7bb398b9f: Already exists\n893c92dab848: Pulling fs layer\na0757eae439e: Pulling fs layer\nee2957a13303: Pulling fs layer\nf49a3daea774: Pulling fs layer\ncab73971ce79: Pulling fs layer\nc3d7fbfdaca2: Pulling fs layer\n0976efdf0829: Pulling fs layer\nd02e6f607e12: Pulling fs layer\nf49a3daea774: Waiting\ncab73971ce79: Waiting\nc3d7fbfdaca2: Waiting\n0976efdf0829: Waiting\nd02e6f607e12: Waiting\nee2957a13303: Verifying Checksum\nee2957a13303: Download complete\nf49a3daea774: Verifying Checksum\nf49a3daea774: Download complete\ncab73971ce79: Verifying Checksum\ncab73971ce79: Download complete\nc3d7fbfdaca2: Verifying Checksum\nc3d7fbfdaca2: Download complete\n0976efdf0829: Verifying Checksum\n0976efdf0829: Download complete\n893c92dab848: Verifying Checksum\n893c92dab848: Download complete\nd02e6f607e12: Verifying Checksum\nd02e6f607e12: Download complete\na0757eae439e: Verifying Checksum\na0757eae439e: Download complete\n893c92dab848: Pull complete\na0757eae439e: Pull complete\nee2957a13303: Pull complete\nf49a3daea774: Pull complete\ncab73971ce79: Pull complete\nc3d7fbfdaca2: Pull complete\n0976efdf0829: Pull complete\nd02e6f607e12: Pull complete\nDigest: sha256:922be75bb02cf219cb86ac4734bcbda41d956314a4b3b7a4febc807d0a803f6e\nStatus: Downloaded newer image for mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04@sha256:922be75bb02cf219cb86ac4734bcbda41d956314a4b3b7a4febc807d0a803f6e\n---> 8926027fde41\nStep 2\/21 : USER root\n---> Running in 0b57828c9289\nRemoving intermediate container 0b57828c9289\n---> 370ef8ee2d0f\nStep 3\/21 : RUN mkdir -p $HOME\/.cache\n---> Running in 3eee95c47f9f\nRemoving intermediate container 3eee95c47f9f\n---> 415459035e6d\nStep 4\/21 : WORKDIR \/\n---> Running in 08c6f83df4b1\nRemoving intermediate container 08c6f83df4b1\n---> 4216bc82e697\nStep 5\/21 : COPY azureml-environment-setup\/99brokenproxy \/etc\/apt\/apt.conf.d\/\n---> 03b433a4a6b8\nStep 6\/21 : RUN if dpkg --compare-versions conda --version | grep -oE '[^ ]+$' lt 4.4.11; then conda install conda==4.4.11; fi\n---> Running in a59290010c77\nRemoving intermediate container a59290010c77\n---> 5bc601b6dd21\nStep 7\/21 : COPY azureml-environment-setup\/mutated_conda_dependencies.yml azureml-environment-setup\/mutated_conda_dependencies.yml\n---> d43a193f0f3e\nStep 8\/21 : RUN ldconfig \/usr\/local\/cuda\/lib64\/stubs && conda env create -p \/azureml-envs\/azureml_9a27d0b682f7325ef536eaeb801b2a62 -f azureml-environment-setup\/mutated_conda_dependencies.yml && rm -rf \"$HOME\/.cache\/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR\/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name pycache -exec rm -rf {} + && ldconfig\n---> Running in 6f1452f548f8\nCollecting package metadata (repodata.json): ...working...\ndone\nSolving environment: ...working... done\n\nDownloading and Extracting Packages\n\nlibblas-3.9.0 | 12 KB | | 0%\nlibblas-3.9.0 | 12 KB | ########## | 100%\n\nreadline-7.0 | 391 KB | | 0%\nreadline-7.0 | 391 KB | ########## | 100%\n\ntk-8.6.12 | 3.3 MB | | 0%\ntk-8.6.12 | 3.3 MB | #####1 | 51%\ntk-8.6.12 | 3.3 MB | ########## | 100%\ntk-8.6.12 | 3.3 MB | ########## | 100%\n\nsix-1.16.0 | 14 KB | | 0%\nsix-1.16.0 | 14 KB | ########## | 100%\n\nsqlite-3.28.0 | 1.9 MB | | 0%\nsqlite-3.28.0 | 1.9 MB | ########## | 100%\nsqlite-3.28.0 | 1.9 MB | ########## | 100%\n\nncurses-6.3 | 1012 KB | | 0%\nncurses-6.3 | 1012 KB | ########## | 100%\nncurses-6.3 | 1012 KB | ########## | 100%\n\nlibgfortran-ng-11.2. | 19 KB | | 0%\nlibgfortran-ng-11.2. | 19 KB | ########## | 100%\n\nopenssl-1.1.1l | 2.1 MB | | 0%\nopenssl-1.1.1l | 2.1 MB | ########## | 100%\nopenssl-1.1.1l | 2.1 MB | ########## | 100%\n\nzlib-1.2.11 | 86 KB | | 0%\nzlib-1.2.11 | 86 KB | ########## | 100%\n\nlibcblas-3.9.0 | 12 KB | | 0%\nlibcblas-3.9.0 | 12 KB | ########## | 100%\n\nlibgomp-11.2.0 | 426 KB | | 0%\nlibgomp-11.2.0 | 426 KB | ########## | 100%\n\n_libgcc_mutex-0.1 | 3 KB | | 0%\n_libgcc_mutex-0.1 | 3 KB | ########## | 100%\n\nlibzlib-1.2.11 | 59 KB | | 0%\nlibzlib-1.2.11 | 59 KB | ########## | 100%\n\nlibffi-3.2.1 | 47 KB | | 0%\nlibffi-3.2.1 | 47 KB | ########## | 100%\n\npip-20.2.4 | 1.1 MB | | 0%\npip-20.2.4 | 1.1 MB | ########## | 100%\npip-20.2.4 | 1.1 MB | ########## | 100%\n\nsetuptools-58.0.4 | 966 KB | | 0%\nsetuptools-58.0.4 | 966 KB | ########## | 100%\nsetuptools-58.0.4 | 966 KB | ########## | 100%\n\nxz-5.2.5 | 343 KB | | 0%\nxz-5.2.5 | 343 KB | ########## | 100%\nxz-5.2.5 | 343 KB | ########## | 100%\n\npython_abi-3.6 | 4 KB | | 0%\npython_abi-3.6 | 4 KB | ########## | 100%\n\nnumpy-1.19.5 | 5.3 MB | | 0%\nnumpy-1.19.5 | 5.3 MB | ######9 | 70%\nnumpy-1.19.5 | 5.3 MB | ########## | 100%\nnumpy-1.19.5 | 5.3 MB | ########## | 100%\n\njoblib-1.1.0 | 210 KB | | 0%\njoblib-1.1.0 | 210 KB | ########## | 100%\n\nlibgfortran5-11.2.0 | 1.7 MB | | 0%\nlibgfortran5-11.2.0 | 1.7 MB | ########## | 100%\nlibgfortran5-11.2.0 | 1.7 MB | ########## | 100%\n\nca-certificates-2021 | 139 KB | | 0%\nca-certificates-2021 | 139 KB | ########## | 100%\n\nwheel-0.37.1 | 31 KB | | 0%\nwheel-0.37.1 | 31 KB | ########## | 100%\n\n_openmp_mutex-4.5 | 22 KB | | 0%\n_openmp_mutex-4.5 | 22 KB | ########## | 100%\n\nscikit-surprise-1.0. | 636 KB | | 0%\nscikit-surprise-1.0. | 636 KB | ########## | 100%\nscikit-surprise-1.0. | 636 KB | ########## | 100%\n\nlibopenblas-0.3.18 | 9.6 MB | | 0%\nlibopenblas-0.3.18 | 9.6 MB | ######2 | 63%\nlibopenblas-0.3.18 | 9.6 MB | #########8 | 99%\nlibopenblas-0.3.18 | 9.6 MB | ########## | 100%\n\nlibgcc-ng-11.2.0 | 904 KB | | 0%\nlibgcc-ng-11.2.0 | 904 KB | ########## | 100%\nlibgcc-ng-11.2.0 | 904 KB | ########## | 100%\n\nlibstdcxx-ng-11.2.0 | 4.2 MB | | 0%\nlibstdcxx-ng-11.2.0 | 4.2 MB | ########## | 100%\nlibstdcxx-ng-11.2.0 | 4.2 MB | ########## | 100%\n\npython-3.6.8 | 30.1 MB | | 0%\npython-3.6.8 | 30.1 MB | #6 | 17%\npython-3.6.8 | 30.1 MB | ######2 | 63%\npython-3.6.8 | 30.1 MB | ########## | 100%\npython-3.6.8 | 30.1 MB | ########## | 100%\n\nliblapack-3.9.0 | 12 KB | | 0%\nliblapack-3.9.0 | 12 KB | ########## | 100%\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\nInstalling pip dependencies: ...working...\nRan pip subprocess with arguments:\n['\/azureml-envs\/azureml_9a27d0b682f7325ef536eaeb801b2a62\/bin\/python', '-m', 'pip', 'install', '-U', '-r', '\/azureml-environment-setup\/condaenv.s4sfl041.requirements.txt']\nPip subprocess output:\nCollecting azureml-designer-classic-modules==0.0.161\nDownloading azureml_designer_classic_modules-0.0.161-py3-none-any.whl (403 kB)\nCollecting en_core_web_sm\nDownloading https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.1.0\/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\nCollecting spacy==2.1.7\nDownloading spacy-2.1.7-cp36-cp36m-manylinux1_x86_64.whl (30.8 MB)\nCollecting azureml-model-management-sdk\nDownloading azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130 kB)\nCollecting azure-storage-blob==1.5.0\nDownloading azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\nCollecting azureml-designer-internal==0.0.56\nDownloading azureml_designer_internal-0.0.56-py3-none-any.whl (28 kB)\nCollecting seaborn==0.10.0\nDownloading seaborn-0.10.0-py3-none-any.whl (215 kB)\nCollecting gensim==3.8.3\nDownloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\nCollecting lightgbm==3.2.1\nDownloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\nCollecting chardet==3.0.4\nDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\nCollecting joblib==0.14.0\nDownloading joblib-0.14.0-py2.py3-none-any.whl (294 kB)\nCollecting scipy==1.4.1\nDownloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\nCollecting nimbusml==1.6.1\nDownloading nimbusml-1.6.1-cp36-none-manylinux1_x86_64.whl (105.2 MB)\nCollecting matplotlib==3.1.3\nDownloading matplotlib-3.1.3-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\nCollecting pandas==1.0.4\nDownloading pandas-1.0.4-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\nCollecting scikit-learn==0.22.2\nDownloading scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\nCollecting imbalanced-learn==0.4.3\nDownloading imbalanced_learn-0.4.3-py3-none-any.whl (166 kB)\nCollecting Pillow==8.3.2\nDownloading Pillow-8.3.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nCollecting azureml-interpret==1.36.0\nDownloading azureml_interpret-1.36.0-py3-none-any.whl (52 kB)\nCollecting blis<0.3.0,>=0.2.2\nDownloading blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2 MB)\nCollecting plac<1.0.0,>=0.9.6\nDownloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\nRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in \/azureml-envs\/azureml_9a27d0b682f7325ef536eaeb801b2a62\/lib\/python3.6\/site-packages (from spacy==2.1.7->-r \/azureml-environment-setup\/condaenv.s4sfl041.requirements.txt (line 3)) (1.19.5)\nCollecting murmurhash<1.1.0,>=0.28.0\nDownloading murmurhash-1.0.6-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\nCollecting cymem<2.1.0,>=2.0.2\nDownloading cymem-2.0.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\nCollecting srsly<1.1.0,>=0.0.6\nDownloading srsly-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl (184 kB)\nCollecting requests<3.0.0,>=2.13.0\nDownloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\nCollecting thinc<7.1.0,>=7.0.8\nDownloading thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\nCollecting preshed<2.1.0,>=2.0.1\nDownloading preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83 kB)\nCollecting wasabi<1.1.0,>=0.2.0\nDownloading wasabi-0.9.0-py3-none-any.whl (25 kB)\nRequirement already satisfied, skipping upgrade: six>=1.10 in \/azureml-envs\/azureml_9a27d0b682f7325ef536eaeb801b2a62\/lib\/python3.6\/site-packages (from azureml-model-management-sdk->-r \/azureml-environment-setup\/condaenv.s4sfl041.requirements.txt (line 4)) (1.16.0)\nCollecting adal>=0.4.5\nDownloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\nCollecting dill>=0.2.7.1\nDownloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\nCollecting python-dateutil>=2.5.3\nDownloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\nCollecting liac-arff>=2.1.1\nDownloading liac-arff-2.5.0.tar.gz (13 kB)\nCollecting pytz>=2017.2\nDownloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\nCollecting azure-storage-common~=1.4\nDownloading azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\nCollecting azure-common>=1.1.5\nDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\nCollecting azureml-pipeline-core==1.36.0\nDownloading azureml_pipeline_core-1.36.0-py3-none-any.whl (313 kB)\nCollecting azureml-defaults==1.36.0\nDownloading azureml_defaults-1.36.0-py3-none-any.whl (3.0 kB)\nCollecting azureml-telemetry==1.36.0.\nDownloading azureml_telemetry-1.36.0-py3-none-any.whl (30 kB)\nCollecting cffi==1.12.3\nDownloading cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl (430 kB)\nCollecting azureml-designer-core==0.0.68\nDownloading azureml_designer_core-0.0.68-py3-none-any.whl (101 kB)\nCollecting smart-open>=1.8.1\nDownloading smart_open-5.2.1-py3-none-any.whl (58 kB)\nRequirement already satisfied, skipping upgrade: wheel in \/azureml-envs\/azureml_9a27d0b682f7325ef536eaeb801b2a62\/lib\/python3.6\/site-packages (from lightgbm==3.2.1->azureml-designer-classic-modules==0.0.161->-r \/azureml-environment-setup\/condaenv.s4sfl041.requirements.txt (line 1)) (0.37.1)\nCollecting dotnetcore2>=2.1.2\nDownloading dotnetcore2-2.1.23-py3-none-manylinux1_x86_64.whl (29.3 MB)\nCollecting kiwisolver>=1.0.1\nDownloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\nCollecting cycler>=0.10\nDownloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\nCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\nDownloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\nCollecting interpret-community==0.21.\nDownloading interpret_community-0.21.0-py3-none-any.whl (136 kB)\nCollecting azureml-core~=1.36.0\nDownloading azureml_core-1.36.0.post2-py3-none-any.whl (2.4 MB)\nCollecting certifi>=2017.4.17\nDownloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\nCollecting idna<4,>=2.5; python_version >= \"3\"\nDownloading idna-3.3-py3-none-any.whl (61 kB)\nCollecting charset-normalizer~=2.0.0; python_version >= \"3\"\nDownloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\nCollecting urllib3<1.27,>=1.21.1\nDownloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\nCollecting tqdm<5.0.0,>=4.10.0\nDownloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\nCollecting PyJWT<3,>=1.0.0\nDownloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\nCollecting cryptography>=1.1.0\nDownloading cryptography-36.0.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\nCollecting configparser==3.7.4\nDownloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\nCollecting json-logging-py==0.2\nDownloading json-logging-py-0.2.tar.gz (3.6 kB)\nCollecting azureml-inference-server-http~=0.4.1\nDownloading azureml_inference_server_http-0.4.9-py3-none-any.whl (52 kB)\nCollecting azureml-dataset-runtime[fuse]~=1.36.0\nDownloading azureml_dataset_runtime-1.36.0-py3-none-any.whl (3.5 kB)\nCollecting applicationinsights\nDownloading applicationinsights-0.11.10-py2.py3-none-any.whl (55 kB)\nCollecting pycparser\nDownloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\nCollecting pycryptodomex==3.7.3\nDownloading pycryptodomex-3.7.3-cp36-cp36m-manylinux1_x86_64.whl (7.5 MB)\nCollecting pyarrow==0.16.0\nDownloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\nCollecting distro==1.4.0\nDownloading distro-1.4.0-py2.py3-none-any.whl (17 kB)\nCollecting ruamel.yaml==0.16.10\nDownloading ruamel.yaml-0.16.10-py2.py3-none-any.whl (111 kB)\nCollecting more-itertools==6.0.0\nDownloading more_itertools-6.0.0-py3-none-any.whl (52 kB)\nCollecting jsonschema==3.0.1\nDownloading jsonschema-3.0.1-py2.py3-none-any.whl (54 kB)\nCollecting interpret-core[required]<=0.2.6,>=0.1.20\nDownloading interpret_core-0.2.6-py3-none-any.whl (6.5 MB)\nCollecting packaging\nDownloading packaging-21.3-py3-none-any.whl (40 kB)\nCollecting shap<=0.39.0,>=0.20.0\nDownloading shap-0.39.0.tar.gz (356 kB)\nCollecting numba<0.54.0\nDownloading numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl (3.4 MB)\nCollecting ndg-httpsclient<=0.5.1\nDownloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\nCollecting jmespath<1.0.0\nDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nCollecting azure-graphrbac<1.0.0,>=0.40.0\nDownloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\nCollecting backports.tempfile\nDownloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\nCollecting msrest<1.0.0,>=0.5.1\nDownloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\nCollecting azure-mgmt-containerregistry>=2.0.0\nDownloading azure_mgmt_containerregistry-9.0.0-py3-none-any.whl (937 kB)\nCollecting jsonpickle<3.0.0\nDownloading jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\nCollecting SecretStorage<4.0.0\nDownloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\nCollecting azure-mgmt-keyvault<10.0.0,>=0.40.0\nDownloading azure_mgmt_keyvault-9.3.0-py2.py3-none-any.whl (412 kB)\nCollecting pathspec<1.0.0\nDownloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\nCollecting azure-mgmt-storage<16.0.0,>=1.5.0\nDownloading azure_mgmt_storage-11.2.0-py2.py3-none-any.whl (547 kB)\nCollecting contextlib2<22.0.0\nDownloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\nCollecting msrestazure<=0.6.4,>=0.4.33\nDownloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\nCollecting azure-mgmt-resource<15.0.0,>=1.2.1\nDownloading azure_mgmt_resource-13.0.0-py2.py3-none-any.whl (1.3 MB)\nCollecting pyopenssl<21.0.0\nDownloading pyOpenSSL-20.0.1-py2.py3-none-any.whl (54 kB)\nCollecting docker<6.0.0\nDownloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\nCollecting azure-mgmt-authorization<1.0.0,>=0.40.0\nDownloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\nCollecting sanic-cors~=1.0.1\nDownloading Sanic_Cors-1.0.1-py2.py3-none-any.whl (17 kB)\nCollecting inference-schema~=1.3.1\nDownloading inference_schema-1.3.1-py3-none-any.whl (20 kB)\nCollecting protobuf~=3.17.3\nDownloading protobuf-3.17.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\nCollecting grpcio-tools~=1.38.1\nDownloading grpcio_tools-1.38.1-cp36-cp36m-manylinux2014_x86_64.whl (2.5 MB)\nCollecting aiohttp~=3.7.4.post0\nDownloading aiohttp-3.7.4.post0-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\nCollecting aiotask-context~=0.6.1\nDownloading aiotask_context-0.6.1-py3-none-any.whl (3.5 kB)\nCollecting opencensus-ext-azure~=1.1.0\nDownloading opencensus_ext_azure-1.1.1-py2.py3-none-any.whl (42 kB)\nCollecting gunicorn==20.1.0; platform_system != \"Windows\"\nDownloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting tritonclient[all]~=2.11.0\nDownloading tritonclient-2.11.0-py3-none-manylinux1_x86_64.whl (7.7 MB)\n\nfailed\n[91m\n\n==> WARNING: A newer version of conda exists. <==\ncurrent version: 4.9.2\nlatest version: 4.11.0\n\nPlease update conda by running\n\n $ conda update -n base -c defaults conda\n\n\n\n\nPip subprocess error:\nERROR: Could not find a version that satisfies the requirement sanic~=21.6.0 (from azureml-inference-server-http~=0.4.1->azureml-defaults==1.36.0->azureml-designer-internal==0.0.56->azureml-designer-classic-modules==0.0.161->-r \/azureml-environment-setup\/condaenv.s4sfl041.requirements.txt (line 1)) (from versions: 0.1.0, 0.1.1, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.3.0, 0.3.1, 0.4.0, 0.4.1, 0.5.0, 0.5.1, 0.5.2, 0.5.4, 0.6.0, 0.7.0, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 18.12.0, 19.3.1, 19.6.0, 19.6.2, 19.6.3, 19.9.0, 19.12.0, 19.12.2, 19.12.3, 19.12.4, 19.12.5, 20.3.0, 20.6.0, 20.6.1, 20.6.2, 20.6.3, 20.9.0, 20.9.1, 20.12.0, 20.12.1, 20.12.2, 20.12.3, 20.12.4, 20.12.5, 20.12.6)\nERROR: No matching distribution found for sanic~=21.6.0 (from azureml-inference-server-http~=0.4.1->azureml-defaults==1.36.0->azureml-designer-internal==0.0.56->azureml-designer-classic-modules==0.0.161->-r \/azureml-environment-setup\/condaenv.s4sfl041.requirements.txt (line 1))\n\n\n\n\nCondaEnvException: Pip failed\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs && conda env create -p \/azureml-envs\/azureml_9a27d0b682f7325ef536eaeb801b2a62 -f azureml-environment-setup\/mutated_conda_dependencies.yml && rm -rf \"$HOME\/.cache\/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR\/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name pycache -exec rm -rf {} + && ldconfig' returned a non-zero code: 1\n2022\/02\/20 03:27:17 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cf6 failed after 1m47s. Error: failed during run, err: exit status 1",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"While registering a dataframe in AzureML pipeline, getting error: 'DataFrame' object has no attribute 'register. How do we actually store dataframe into Azure Blob Storage?",
        "Question_creation_time":1624431973167,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/448224\/while-registering-a-dataframe-in-azureml-pipeline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"While registering a dataframe in AzureML pipeline, getting error: 'DataFrame' object has no attribute 'register. How do we actually store dataframe into Azure Blob Storage?\n\nCode snippet-\n\n<DataFrame>.register(workspace=ws, name='<abc>', description='<abc>', tags = {'format':'CSV'}, create_new_version=True)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where are the variables quotients after doing a regression run in ML?",
        "Question_creation_time":1617200123477,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/339422\/where-are-the-variables-quotients-after-doing-a-re.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Having done my first Azure ML Studio session, the presented output metrics are global (Spearman, Explained variance, etc) are somewhat secondary to my requirement of knowing how each of my hundreds of variables have contributed to these. But I cannot find them. I would appreciate some guidance to where such numbers are - I know they have to be somewhere, as they (in total) provide the shown metrics.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'azure.ai'",
        "Question_creation_time":1668204745757,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1086028\/modulenotfounderror-no-module-named-39azureai39.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am getting the following error message\n\nModuleNotFoundError: No module named 'azure.ai'\n\n\n\n\nin Azure machine learning studio...when i try to run sample azureml-in-a-day.ipynb",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying an ML Model to ACI with a secured workspace in a VNet",
        "Question_creation_time":1649065504527,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/799037\/deploying-and-ml-model-to-aci-with-a-secured-works.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to deploy an ML model to an ACI in a VNet. I have followed the guide to setup a secure workspace, and also noted that if deploying to ACI, the container registry must not be in the same vnet.\nI have deployed the container registry:\n\noutside of the vnet in the same resource group\n\n\nAllowed admin user in the CR\n\n\nDisabled public access\n\n\nAllowed trusted microsoft services\n\n\nCreated a private endpoint for private access for the worskpace to access (needed this for image builds on my training runs)\n\n\nAllowed subnet delegation on the Scoring subnet for the containerGroups service as shown here\n\n\n\n\nNow when I am trying to deploy the model to a container instance, I get this failure\n```\nError:\n{\n\"code\": \"InaccessibleImage\",\n\"statusCode\": 400,\n\"message\": \"ACI Service request failed. Reason: The image '<containerRegName>.azurecr.io\/azureml\/azureml_<imageHash>' in container group '<serviceName>-qcloi6KnEkOQ6CTdniybhQ' is not accessible. Please check the image and registry credential.. Refer to https:\/\/docs.microsoft.com\/azure\/container-registry\/container-registry-authentication#admin-account and make sure Admin user is enabled for your container registry.\"\n}\n```\nAfter speaking to the docs team where the guides address this deployment strategy (here), the only response is to use AKS. AKS won't be feasible right now for this project and the documentation seems to suggest that this is possible.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Auth Problems with Machine Learning Execute Pipeline Activity.",
        "Question_creation_time":1621952386747,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/408869\/auth-problems-with-machine-learning-execute-pipeli.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello. Can anyone help with this error? Can not execute Azure ML activity from ADF.\nEverything was ok, no changes was done but suddenly(two-three days ago) I got this error.\n\n Request sent to Azure ML Service for operation 'submitMLPipelineRun' failed with http status code 'Forbidden'. Error message from Azure ML Service: '{ \"error\": { \"code\": \"UserError\", \"severity\": null, \"message\": \"Identity does not have permissions for Microsoft.MachineLearningServices\/workspaces\/experiments\/runs\/submit\/action, Microsoft.MachineLearningServices\/workspaces\/endpoints\/pipelines\/read actions.\", \"messageFormat\": null, \"messageParameters\": null, \"referenceCode\": null, \"detailsUri\": null, \"target\": null, \"details\": [], \"innerError\": { \"code\": \"ForbiddenError\", \"innerError\": null } '.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How should I create a scoring script for object detection (pytorch) in Azure ML?",
        "Question_creation_time":1655225088730,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/889212\/how-should-i-create-a-scoring-script-for-object-de.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,\n\nI have trained a PyTorch vision model on a local computer for object detection and want to deploy it on Azure ML. I have found a similar script for classification using pytorch where they are using the following scoring script. link: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/ml-frameworks\/pytorch\/train-hyperparameter-tune-deploy-with-pytorch\n\n # Copyright (c) Microsoft. All rights reserved.\n # Licensed under the MIT license.\n import os\n import torch\n import torch.nn as nn\n from torchvision import transforms\n import json\n from azureml.core.model import Model\n def init():\n     global model\n     # AZUREML_MODEL_DIR is an environment variable created during deployment.\n     # It is the path to the model folder (.\/azureml-models\/$MODEL_NAME\/$VERSION)\n     # For multiple models, it points to the folder containing all deployed models (.\/azureml-models)\n     model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pt')\n     model = torch.load(model_path, map_location=lambda storage, loc: storage)\n     model.eval()\n def run(input_data):\n     input_data = torch.tensor(json.loads(input_data)['data'])\n     # get prediction\n     with torch.no_grad():\n         output = model(input_data)\n         classes = ['chicken', 'turkey']\n         softmax = nn.Softmax(dim=1)\n         pred_probs = softmax(output).numpy()[0]\n         index = torch.argmax(output, 1)\n     result = {\"label\": classes[index], \"probability\": str(pred_probs[index])}\n     return result\n\n\n\nI have a few questions regarding this script. I am wondering what is 'input_data' in this case, is it an image in jpg format?\nAlso can my 'result' be in any dict format or it should have a specific format?\n\nI have written a similar script for my purpose.\n\n # Copyright (c) Microsoft. All rights reserved.\n # Licensed under the MIT license.\n import os\n import torch\n import torchvision\n #import torch.nn as nn\n from torchvision import transforms\n import json\n import cv2\n from azureml.core.model import Model\n import numpy as np\n from PIL import Image\n import os\n def init():\n     global model\n     # AZUREML_MODEL_DIR is an environment variable created during deployment.\n     # It is the path to the model folder (.\/azureml-models\/$MODEL_NAME\/$VERSION)\n     # For multiple models, it points to the folder containing all deployed models (.\/azureml-models)\n     # model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pt')\n     # model = torch.load(model_path, map_location=lambda storage, loc: storage)\n     # #model_path = Model.get_model_path(model_name='pytorch_external_model-test')\n     # model = torch.load(model_path)\n     model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'),'model.pt')\n     model = torch.load(model_path, map_location=lambda storage, loc: storage)\n     #model = torch.load(model_path)\n     model.eval()\n # the function takes the original prediction and the iou threshold.\n def apply_nms(orig_prediction, iou_thresh=0.3):\n # torchvision returns the indices of the bboxes to keep\n keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n final_prediction = orig_prediction\n #print(final_prediction['boxes'])\n final_prediction['boxes'] = final_prediction['boxes'][keep].cpu() # had to add .cpu() after each tensor \n final_prediction['scores'] = final_prediction['scores'][keep].cpu() \n final_prediction['labels'] = final_prediction['labels'][keep].cpu() \n return final_prediction\n # def preprocess(input_data): # doesn't convert to tensor, while input for prediction needs to be in tensor\n #     img = cv2.imread(input_data)\n #     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n #     img_res = cv2.resize(img_rgb, (704, 480), cv2.INTER_AREA)\n #     # diving by 255\n #     img_res \/= 255.0\n #     return img_res\n def run(input_data):\n     img = Image.open(input_data).convert('RGB')\n     # set up transformation to resize the image\n     resize = transforms.Resize([704, 480])\n     img = resize(img)\n     to_tensor = transforms.ToTensor()\n     # apply transformation and convert to Pytorch tensor\n     tensor = to_tensor(img) # output shape [3, 704, 480] \n     #tensor = tensor.unsqueeze(0) # no need for [1, 3, 704, 480]\n     # link for converting image to tensor https:\/\/towardsdatascience.com\/convert-images-to-tensors-in-pytorch-and-tensorflow-f0ab01383a03\n     #input_data = torch.tensor(json.loads(input_data)['data'])\n     #image = preprocess(input_data)\n     device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n     with torch.no_grad():\n         prediction = model([tensor.to(device)])[0]\n     nms_prediction = apply_nms(prediction, iou_thresh=0.3)\n     result = {\"label\": nms_prediction['labels'], \"box\": nms_prediction['boxes'], \"score\": nms_prediction['scores']}\n     return result\n\nI followed this colab tutorial for training the object detection model. I am not using any transform to make the problem easy for now.\nhttps:\/\/colab.research.google.com\/drive\/1NziO_b-SW9KmWFh-6C8to9H_QAdpmCBZ?usp=sharing#scrollTo=WOrNovPGh_k6\n\nModel is deployed successfully. But getting this error\n\nprint(service.get_logs())\n\n \/bin\/bash: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n \/bin\/bash: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n \/bin\/bash: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n \/bin\/bash: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n 2022-06-15T00:14:38,162168400+00:00 - gunicorn\/run \n 2022-06-15T00:14:38,166094000+00:00 - rsyslog\/run \n 2022-06-15T00:14:38,171921600+00:00 - iot-server\/run \n bash: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/libtinfo.so.5: no version information available (required by bash)\n 2022-06-15T00:14:38,190831400+00:00 | gunicorn\/run | \n 2022-06-15T00:14:38,197941200+00:00 | gunicorn\/run | ###############################################\n 2022-06-15T00:14:38,242881800+00:00 | gunicorn\/run | AzureML Container Runtime Information\n 2022-06-15T00:14:38,300863500+00:00 | gunicorn\/run | ###############################################\n 2022-06-15T00:14:38,351919200+00:00 - nginx\/run \n 2022-06-15T00:14:38,377608200+00:00 | gunicorn\/run | \n 2022-06-15T00:14:38,413561500+00:00 | gunicorn\/run | \n 2022-06-15T00:14:38,436442300+00:00 | gunicorn\/run | PATH environment variable: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\n 2022-06-15T00:14:38,472313100+00:00 | gunicorn\/run | PYTHONPATH environment variable: \n 2022-06-15T00:14:38,478958600+00:00 | gunicorn\/run | \n 2022-06-15T00:14:38,501877400+00:00 | gunicorn\/run | Pip Dependencies (before dynamic installation)\n EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n \/bin\/bash: \/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n 2022-06-15T00:14:38,787582000+00:00 - iot-server\/finish 1 0\n 2022-06-15T00:14:38,793119200+00:00 - Exit code 1 is normal. Not restarting iot-server.\n adal==1.2.7\n albumentations==0.4.6\n applicationinsights==0.11.10\n argcomplete==2.0.0\n attrs==21.4.0\n azure-common==1.1.28\n azure-core==1.22.1\n azure-graphrbac==0.61.1\n azure-identity==1.7.0\n azure-mgmt-authorization==2.0.0\n azure-mgmt-containerregistry==9.1.0\n azure-mgmt-core==1.3.0\n azure-mgmt-keyvault==9.3.0\n azure-mgmt-resource==21.0.0\n azure-mgmt-storage==20.0.0\n azureml-core==1.42.0.post1\n azureml-dataprep==4.0.3\n azureml-dataprep-native==38.0.0\n azureml-dataprep-rslex==2.6.3\n azureml-dataset-runtime==1.42.0\n azureml-defaults==1.42.0\n azureml-inference-server-http==0.4.13\n backports.tempfile==1.0\n backports.weakref==1.0.post1\n bcrypt==3.2.2\n cachetools==4.2.4\n certifi==2022.5.18.1\n cffi==1.15.0\n charset-normalizer==2.0.12\n click==7.1.2\n cloudpickle==2.1.0\n configparser==3.7.4\n contextlib2==21.6.0\n contextvars==2.4\n cryptography==36.0.2\n cycler==0.11.0\n dataclasses==0.8\n decorator==4.4.2\n distro==1.7.0\n docker==5.0.3\n dotnetcore2==3.1.23\n Flask==1.0.3\n fusepy==3.0.1\n future==0.18.2\n google-api-core==2.8.1\n google-auth==2.8.0\n googleapis-common-protos==1.56.2\n gunicorn==20.1.0\n humanfriendly==10.0\n idna==3.3\n imageio==2.15.0\n imgaug==0.4.0\n immutables==0.18\n importlib-metadata==4.8.3\n inference-schema==1.3.0\n isodate==0.6.1\n itsdangerous==1.1.0\n jeepney==0.7.1\n Jinja2==3.0.3\n jmespath==0.10.0\n json-logging-py==0.2\n jsonpickle==2.2.0\n jsonschema==3.2.0\n kiwisolver==1.3.1\n knack==0.9.0\n MarkupSafe==2.0.1\n matplotlib==3.3.4\n msal==1.18.0\n msal-extensions==0.3.1\n msrest==0.6.21\n msrestazure==0.6.4\n ndg-httpsclient==0.5.1\n networkx==2.5.1\n numpy==1.19.5\n oauthlib==3.2.0\n opencensus==0.9.0\n opencensus-context==0.1.2\n opencensus-ext-azure==1.1.4\n opencv-python==4.6.0.66\n opencv-python-headless==4.6.0.66\n packaging==21.3\n paramiko==2.11.0\n pathspec==0.9.0\n Pillow==8.4.0\n pkginfo==1.8.3\n portalocker==2.4.0\n protobuf==3.19.4\n psutil==5.9.1\n pyarrow==3.0.0\n pyasn1==0.4.8\n pyasn1-modules==0.2.8\n pycocotools==2.0.4\n pycparser==2.21\n Pygments==2.12.0\n PyJWT==2.4.0\n PyNaCl==1.5.0\n pyOpenSSL==22.0.0\n pyparsing==3.0.7\n pyrsistent==0.18.0\n PySocks==1.7.1\n python-dateutil==2.8.2\n pytz==2022.1\n PyWavelets==1.1.1\n PyYAML==6.0\n requests==2.27.1\n requests-oauthlib==1.3.1\n rsa==4.8\n scikit-image==0.17.2\n scipy==1.5.4\n SecretStorage==3.3.2\n Shapely==1.8.2\n six==1.16.0\n tabulate==0.8.9\n tifffile==2020.9.3\n torch==1.10.1\n torchvision==0.11.2\n typing_extensions==4.1.1\n urllib3==1.26.9\n websocket-client==1.3.1\n Werkzeug==1.0.1\n wrapt==1.12.1\n zipp==3.6.0\n 2022-06-15T00:14:40,561943700+00:00 | gunicorn\/run | \n 2022-06-15T00:14:40,563774900+00:00 | gunicorn\/run | ###############################################\n 2022-06-15T00:14:40,569936200+00:00 | gunicorn\/run | AzureML Inference Server\n 2022-06-15T00:14:40,571682900+00:00 | gunicorn\/run | ###############################################\n 2022-06-15T00:14:40,573373400+00:00 | gunicorn\/run | \n 2022-06-15T00:14:40,580170100+00:00 | gunicorn\/run | \n 2022-06-15T00:14:40,584523000+00:00 | gunicorn\/run | Starting HTTP server\n 2022-06-15T00:14:40,590038900+00:00 | gunicorn\/run | \n Starting gunicorn 20.1.0\n Listening at: http:\/\/127.0.0.1:31311 (77)\n Using worker: sync\n worker timeout is set to 300\n Booting worker with pid: 125\n SPARK_HOME not set. Skipping PySpark Initialization.\n Initializing logger\n 2022-06-15 00:14:45,845 | root | INFO | Starting up app insights client\n logging socket was found. logging is available.\n logging socket was found. logging is available.\n 2022-06-15 00:14:45,846 | root | INFO | Starting up request id generator\n 2022-06-15 00:14:45,846 | root | INFO | Starting up app insight hooks\n 2022-06-15 00:14:45,847 | root | INFO | Invoking user's init function\n 2022-06-15 00:14:46,111 | root | INFO | Users's init has completed successfully\n 2022-06-15 00:14:46,115 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n 2022-06-15 00:14:46,116 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n 2022-06-15 00:14:46,121 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n 2022-06-15 00:14:49,997 | root | INFO | Swagger file not present\n 2022-06-15 00:14:49,998 | root | INFO | 404\n 127.0.0.1 - - [15\/Jun\/2022:00:14:49 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"Go-http-client\/1.1\"\n 2022-06-15 00:14:54,621 | root | INFO | Swagger file not present\n 2022-06-15 00:14:54,622 | root | INFO | 404\n 127.0.0.1 - - [15\/Jun\/2022:00:14:54 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"Go-http-client\/1.1\"\n 2022-06-15 00:14:54,974 | root | INFO | Scoring Timer is set to 60.0 seconds\n 2022-06-15 00:14:54,979 | root | ERROR | Encountered Exception: Traceback (most recent call last):\n File \"\/var\/azureml-server\/routes.py\", line 294, in run_scoring\n     response, time_taken_ms = invoke_user_with_timer(service_input, request_headers)\n File \"\/var\/azureml-server\/routes.py\", line 341, in invoke_user_with_timer\n     result, time_taken_ms = capture_time_taken(user_main.run)(**params)\n File \"\/var\/azureml-server\/routes.py\", line 322, in timer\n     result = func(*args, **kwargs)\n File \"\/var\/azureml-app\/score2.py\", line 57, in run\n     img = Image.open(input_data).convert('RGB')\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/PIL\/Image.py\", line 2975, in open\n     fp = builtins.open(filename, \"rb\")\n FileNotFoundError: [Errno 2] No such file or directory: 'test_img.jpg'\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1832, in full_dispatch_request\n     rv = self.dispatch_request()\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1818, in dispatch_request\n     return self.view_functions[rule.endpoint](**req.view_args)\n File \"\/var\/azureml-server\/routes.py\", line 270, in score_realtime\n     service_input, request.headers, request.environ.get(\"REQUEST_ID\", \"00000000-0000-0000-0000-000000000000\")\n File \"\/var\/azureml-server\/routes.py\", line 303, in run_scoring\n     raise RunFunctionException(str(exc))\n run_function_exception.RunFunctionException\n 2022-06-15 00:14:54,979 | root | INFO | 500\n 127.0.0.1 - - [15\/Jun\/2022:00:14:54 +0000] \"POST \/score HTTP\/1.0\" 500 51 \"-\" \"python-requests\/2.26.0\"\n 2022-06-15 00:14:54,988 | root | INFO | Scoring Timer is set to 60.0 seconds\n 2022-06-15 00:14:54,988 | root | ERROR | Encountered Exception: Traceback (most recent call last):\n File \"\/var\/azureml-server\/routes.py\", line 294, in run_scoring\n     response, time_taken_ms = invoke_user_with_timer(service_input, request_headers)\n File \"\/var\/azureml-server\/routes.py\", line 341, in invoke_user_with_timer\n     result, time_taken_ms = capture_time_taken(user_main.run)(**params)\n File \"\/var\/azureml-server\/routes.py\", line 322, in timer\n     result = func(*args, **kwargs)\n File \"\/var\/azureml-app\/score2.py\", line 57, in run\n     img = Image.open(input_data).convert('RGB')\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/PIL\/Image.py\", line 2975, in open\n     fp = builtins.open(filename, \"rb\")\n FileNotFoundError: [Errno 2] No such file or directory: 'test_img.jpg'\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1832, in full_dispatch_request\n     rv = self.dispatch_request()\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1818, in dispatch_request\n     return self.view_functions[rule.endpoint](**req.view_args)\n File \"\/var\/azureml-server\/routes.py\", line 270, in score_realtime\n     service_input, request.headers, request.environ.get(\"REQUEST_ID\", \"00000000-0000-0000-0000-000000000000\")\n File \"\/var\/azureml-server\/routes.py\", line 303, in run_scoring\n     raise RunFunctionException(str(exc))\n run_function_exception.RunFunctionException\n 2022-06-15 00:14:54,988 | root | INFO | 500\n 127.0.0.1 - - [15\/Jun\/2022:00:14:54 +0000] \"POST \/score HTTP\/1.0\" 500 51 \"-\" \"python-requests\/2.26.0\"\n 2022-06-15 00:14:56,004 | root | INFO | Scoring Timer is set to 60.0 seconds\n 2022-06-15 00:14:56,005 | root | ERROR | Encountered Exception: Traceback (most recent call last):\n File \"\/var\/azureml-server\/routes.py\", line 294, in run_scoring\n     response, time_taken_ms = invoke_user_with_timer(service_input, request_headers)\n File \"\/var\/azureml-server\/routes.py\", line 341, in invoke_user_with_timer\n     result, time_taken_ms = capture_time_taken(user_main.run)(**params)\n File \"\/var\/azureml-server\/routes.py\", line 322, in timer\n     result = func(*args, **kwargs)\n File \"\/var\/azureml-app\/score2.py\", line 57, in run\n     img = Image.open(input_data).convert('RGB')\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/PIL\/Image.py\", line 2975, in open\n     fp = builtins.open(filename, \"rb\")\n FileNotFoundError: [Errno 2] No such file or directory: 'test_img.jpg'\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1832, in full_dispatch_request\n     rv = self.dispatch_request()\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1818, in dispatch_request\n     return self.view_functions[rule.endpoint](**req.view_args)\n File \"\/var\/azureml-server\/routes.py\", line 270, in score_realtime\n     service_input, request.headers, request.environ.get(\"REQUEST_ID\", \"00000000-0000-0000-0000-000000000000\")\n File \"\/var\/azureml-server\/routes.py\", line 303, in run_scoring\n     raise RunFunctionException(str(exc))\n run_function_exception.RunFunctionException\n 2022-06-15 00:14:56,005 | root | INFO | 500\n 127.0.0.1 - - [15\/Jun\/2022:00:14:56 +0000] \"POST \/score HTTP\/1.0\" 500 51 \"-\" \"python-requests\/2.26.0\"\n 2022-06-15 00:14:58,028 | root | INFO | Scoring Timer is set to 60.0 seconds\n 2022-06-15 00:14:58,029 | root | ERROR | Encountered Exception: Traceback (most recent call last):\n File \"\/var\/azureml-server\/routes.py\", line 294, in run_scoring\n     response, time_taken_ms = invoke_user_with_timer(service_input, request_headers)\n File \"\/var\/azureml-server\/routes.py\", line 341, in invoke_user_with_timer\n     result, time_taken_ms = capture_time_taken(user_main.run)(**params)\n File \"\/var\/azureml-server\/routes.py\", line 322, in timer\n     result = func(*args, **kwargs)\n File \"\/var\/azureml-app\/score2.py\", line 57, in run\n     img = Image.open(input_data).convert('RGB')\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/PIL\/Image.py\", line 2975, in open\n     fp = builtins.open(filename, \"rb\")\n FileNotFoundError: [Errno 2] No such file or directory: 'test_img.jpg'\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1832, in full_dispatch_request\n     rv = self.dispatch_request()\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/flask\/app.py\", line 1818, in dispatch_request\n     return self.view_functions[rule.endpoint](**req.view_args)\n File \"\/var\/azureml-server\/routes.py\", line 270, in score_realtime\n     service_input, request.headers, request.environ.get(\"REQUEST_ID\", \"00000000-0000-0000-0000-000000000000\")\n File \"\/var\/azureml-server\/routes.py\", line 303, in run_scoring\n     raise RunFunctionException(str(exc))\n run_function_exception.RunFunctionException\n 2022-06-15 00:14:58,030 | root | INFO | 500\n 127.0.0.1 - - [15\/Jun\/2022:00:14:58 +0000] \"POST \/score HTTP\/1.0\" 500 51 \"-\" \"python-requests\/2.26.0\"\n Exception in worker process\n Traceback (most recent call last):\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 589, in spawn_worker\n     worker.init_process()\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py\", line 142, in init_process\n     self.run()\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/gunicorn\/workers\/sync.py\", line 125, in run\n     self.run_for_one(timeout)\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/gunicorn\/workers\/sync.py\", line 84, in run_for_one\n     self.wait(timeout)\n File \"\/azureml-envs\/azureml_8003354dcfcc0ac94ef7d92e5607092f\/lib\/python3.6\/site-packages\/gunicorn\/workers\/sync.py\", line 36, in wait\n     ret = select.select(self.wait_fds, [], [], timeout)\n File \"\/var\/azureml-server\/routes.py\", line 159, in alarm_handler\n     raise TimeoutException(error_message)\n timeout_exception.TimeoutException\n Worker exiting (pid: 125)\n worker timeout is set to 300\n Booting worker with pid: 157\n SPARK_HOME not set. Skipping PySpark Initialization.\n Initializing logger\n 2022-06-15 00:16:03,376 | root | INFO | Starting up app insights client\n logging socket was found. logging is available.\n logging socket was found. logging is available.\n 2022-06-15 00:16:03,376 | root | INFO | Starting up request id generator\n 2022-06-15 00:16:03,377 | root | INFO | Starting up app insight hooks\n 2022-06-15 00:16:03,377 | root | INFO | Invoking user's init function\n 2022-06-15 00:16:03,624 | root | INFO | Users's init has completed successfully\n 2022-06-15 00:16:03,626 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n 2022-06-15 00:16:03,626 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n 2022-06-15 00:16:03,633 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n 127.0.0.1 - - [15\/Jun\/2022:00:20:12 +0000] \"POST \/ HTTP\/1.0\" 405 178 \"-\" \"Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/81.0.4044.129 Safari\/537.36\"\n 127.0.0.1 - - [15\/Jun\/2022:00:20:13 +0000] \"GET \/.env HTTP\/1.0\" 404 232 \"-\" \"Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/81.0.4044.129 Safari\/537.36\"\n 2022-06-15 00:35:48,752 | root | INFO | Swagger file not present\n 2022-06-15 00:35:48,753 | root | INFO | 404\n 127.0.0.1 - - [15\/Jun\/2022:00:35:48 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"Go-http-client\/1.1\"\n\n\n\nresult = service.run(input_data=\"test_img.jpg\")\nprint(result)\n\n Received bad response from service. More information can be found by calling `.get_logs()` on the webservice object.\n Response Code: 502\n Headers: {'Connection': 'keep-alive', 'Content-Length': '51', 'Content-Type': 'text\/html; charset=utf-8', 'Date': 'Wed, 15 Jun 2022 00:50:21 GMT', 'Server': 'nginx\/1.14.0 (Ubuntu)', 'X-Ms-Request-Id': 'dacf03b8-adb6-4cbf-8cea-d0c4086712f4', 'X-Ms-Run-Function-Failed': 'True'}\n Content: b\"[Errno 2] No such file or directory: 'test_img.jpg'\"\n ---------------------------------------------------------------------------\n WebserviceException                       Traceback (most recent call last)\n <ipython-input-18-c76911546a4f> in <module>\n ----> 1 result = service.run(input_data=\"test_img.jpg\")\n     2 print(result)\n \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/aci.py in run(self, input_data)\n     403                                       'Headers: {}\\n'\n     404                                       'Content: {}'.format(resp.status_code, resp.headers, resp.content),\n --> 405                                       logger=module_logger)\n     406 \n     407     def update(self, image=None, tags=None, properties=None, description=None, auth_enabled=None, ssl_enabled=None,\n WebserviceException: WebserviceException:\n     Message: Received bad response from service. More information can be found by calling `.get_logs()` on the webservice object.\n Response Code: 502\n Headers: {'Connection': 'keep-alive', 'Content-Length': '51', 'Content-Type': 'text\/html; charset=utf-8', 'Date': 'Wed, 15 Jun 2022 00:50:21 GMT', 'Server': 'nginx\/1.14.0 (Ubuntu)', 'X-Ms-Request-Id': 'dacf03b8-adb6-4cbf-8cea-d0c4086712f4', 'X-Ms-Run-Function-Failed': 'True'}\n Content: b\"[Errno 2] No such file or directory: 'test_img.jpg'\"\n     InnerException None\n     ErrorResponse \n {\n     \"error\": {\n         \"message\": \"Received bad response from service. More information can be found by calling `.get_logs()` on the webservice object.\\nResponse Code: 502\\nHeaders: {'Connection': 'keep-alive', 'Content-Length': '51', 'Content-Type': 'text\/html; charset=utf-8', 'Date': 'Wed, 15 Jun 2022 00:50:21 GMT', 'Server': 'nginx\/1.14.0 (Ubuntu)', 'X-Ms-Request-Id': 'dacf03b8-adb6-4cbf-8cea-d0c4086712f4', 'X-Ms-Run-Function-Failed': 'True'}\\nContent: b\\\"[Errno 2] No such file or directory: 'test_img.jpg'\\\"\"\n     }\n }\n\n\n\nWhat kind of response is needed? I have test_img.jpg file in the same directory.\n\nAny help is appreciated.\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I open the automated ML explanation in Jupyter notebooks?",
        "Question_creation_time":1614091964233,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/285089\/how-can-i-open-the-automated-ml-explanation-in-jup.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What-If and Individual Conditional Expectation (ICE) plots are not supported in Azure Machine Learning studio under the Explanations tab since the uploaded explanation needs an active compute to recalculate predictions and probabilities of perturbed features. It is currently supported in Jupyter notebooks when run as a widget using the SDK. How can I open the automated ML explanation in Jupyter notebooks?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":5.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"i cannot make labeling project enabled",
        "Question_creation_time":1629489536580,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/522604\/i-cannot-make-labeling-project-enabled.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a labeling project.\nIt is now stopped.\nWhen I click \"Resume\", something happens on for several hours, then a message that an error \"Failed\" has occurred and that's it.\nthe project remains unavailable.\nNo changes have been made to it since the stop.\n\nWhat is the reason for the error? Can I get around it somehow? or somehow copy this project so as not to lose the markup that has already been done in it?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML Model data output",
        "Question_creation_time":1615488109277,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/310400\/ml-model-data-output.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI was running several time series model using Azure automated machine learning, I didn't write any code. After the running was completed, there are some datasets stored in Azure Blob Storage. But I don't know if these files include the prediction results or not because I can't find a right software to open it . I don't need to deploy the model. I just need a plain spreadsheet which contains the result. Why it is so hard? The attachment is the screenshot of the fiels stored in Blob of the model I ran? What do those files mean?\nAnd I just check the running outcome, it shows there is no output dataset. I was so confused! Do I need to change something when I set the model running?\n][1]",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML - Notebook - Jupyter Kernel Error - No Kernel connection",
        "Question_creation_time":1612145607873,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/252893\/azure-ml-notebook-jupyter-kernel-error-no-kernel-c-1.html",
        "Question_upvote_count":6.0,
        "Question_view_count":null,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In ML Studio, when I create a notebook the top of my screen says \"Jupyter kernel error\" in red. I have a compute instance running (it's green), but it also says \"No Kernel connected\".\n\nTo correct this matter, can you please provide explicit, step by step instructions on how to review. Screen shots help too.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Separate data in data explorer and use as datastore",
        "Question_creation_time":1619036349177,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/366532\/separate-data-in-data-explorer-and-use-as-datastor.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nWe are sending data from IoT Central to Event Hubs and then to Data Explorer, with the hopes of then sending the data to Azure Machine Learning.\n\nIn order to send data from Event Hubs to Data Explorer it needs a data ingestion into a table on data explorer.\n\nFor this data ingestion, it needs a json mapping.\n\nWe could ingest the data, but the message from the iot central data goes to event hubs that goes to data explorer carries the telemetry data as a dynamic type (a json inside a json).\n\n (\"telemetry\":{\"Temp:\"37\",\"Vol\":\"97\"})\n\n\n\n\nWe want to separate the telemetry data in different columns.\n\nSo Temp will have one column and Vol another.\n\nI am wondering how that can be done?\n\nAnd additionally, since we would like to send the data to ML, can data explorer be used as a datastore in ML?\n\nThanks!!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Models registered with AML CLI cause ACI deployments to fail",
        "Question_creation_time":1634746520717,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/597710\/models-registered-with-aml-cli-cause-aci-deploymen.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We were registering a local file ts_scalar.pickle as a model using AML CLI's create az model create -n test-model -l .\/ts_scalar.pickle ...\nThis registered model test-model works fine when deployed to a LocalWebService, but does not work for a AciWebService.\nWe either get a timeout during the deployment (does not reach stage where containers start running) or this error:\n\n Traceback (most recent call last):\n   File \".\/download.py\", line 353, in <module>\n     init_container_assets(args.config_json, args.conn_string, args.container, args.appinsights_key, args.config_folder)\n   File \".\/download.py\", line 327, in init_container_assets\n     downloader.download(config_file_content)\n   File \".\/download.py\", line 128, in download\n     self.download_artifact(blob_path, local_path, unpack_type)\n   File \".\/download.py\", line 92, in download_artifact\n     file_path=local_path)\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/blob\/baseblobservice.py\", line 2014, in get_blob_to_path\n     cpk=cpk)\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/blob\/baseblobservice.py\", line 2193, in get_blob_to_stream\n     raise ex\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/blob\/baseblobservice.py\", line 2160, in get_blob_to_stream\n     cpk=cpk)\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/blob\/baseblobservice.py\", line 1887, in _get_blob\n     operation_context=_context)\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/common\/storageclient.py\", line 446, in _perform_request\n     raise ex\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/common\/storageclient.py\", line 374, in _perform_request\n     raise ex\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/common\/storageclient.py\", line 360, in _perform_request\n     HTTPError(response.status, response.message, response.headers, response.body))\n   File \"\/usr\/local\/lib\/python3.6\/site-packages\/azure\/storage\/common\/_error.py\", line 115, in _http_error_handler\n     raise ex\n azure.common.AzureMissingResourceHttpError: The specified blob does not exist. ErrorCode: BlobNotFound\n <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound<\/Code><Message>The specified blob does not exist.\n RequestId:59b52a82-301e-005a-51c8-c52ce4000000\n Time:2021-10-20T15:40:04.2703282Z<\/Message><\/Error>\n\n\n\nHowever if we register the model through AML's UI model registration, both kinds of deployments are successful.\n\nIs there an issue with using the CLI to register models for ACI deployment?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure AutoML via User Interface",
        "Question_creation_time":1621023165310,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/396020\/azure-automl-via-user-interface.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When we use Azure AutoML via User Interface, what data do they use to calculate the metrics?\nDo they train test split? If so, the metrics return from the test data?\nOr they use the whole data to validate the models. Thus, the metrics are from cross-validation.\nIf they use the whole data set to train, I should do the train-test split and only upload a train data set (I should clean data first). Then deploy the models with the test data to see how accurate the model is.\nIf it is that case, this function is such a useless function.\nI can use Python SDK directly.\nPlease help me to clarify if it is that case. The metrics are only from cross-validation.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azureml compute instance spark dependencies missing",
        "Question_creation_time":1651594378717,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/835171\/azureml-compute-instance-spark-dependencies-missin.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Currently, I'm trying to use the AzureML SDK's, dataset.to_spark_dataframe() method, and facing a weird error(see below).\nThe ClassNotFoundExceptions suggest that some Jars might be missing from the base environment's Spark classpath. Some sources suggest hadoop-azure concretely: https:\/\/community.cloudera.com\/t5\/Support-Questions\/Class-org-apache-hadoop-fs-azure-NativeAzureFileSystem-not\/m-p\/270675)\n\nIs there a way to add these dependencies to the environment?\n\nError:\nAzureMLException: AzureMLException:\nMessage: Execution failed in operation 'to_spark_dataframe' for Dataset(id='54df6c30-fb46-4c75-a084-d10c17cd3795', name='temperatures_parq', version=1, error_code=None, exception_type=Py4JJavaError)\nInnerException An error occurred while calling o39.getFiles.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure not found\nat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\nat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\nat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\nat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\nat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\nat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\nat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\nat com.microsoft.dprep.io.FileSystemStreamInfoHandler.globStatus(FileSystemStreamInfoHandler.scala:46)\nat com.microsoft.dprep.io.StreamInfoFileSystem.globStatus(StreamInfoFileSystem.scala:206)\nat com.microsoft.dprep.io.StreamInfoFileSystem.globStatus(StreamInfoFileSystem.scala:201)\nat com.microsoft.dprep.execution.Storage$.expandHdfsPath(Storage.scala:44)\nat com.microsoft.dprep.execution.executors.GetFilesExecutor$.$anonfun$getFiles$1(GetFilesExecutor.scala:18)\nat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\nat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\nat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\nat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\nat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\nat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\nat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\nat com.microsoft.dprep.execution.executors.GetFilesExecutor$.getFiles(GetFilesExecutor.scala:12)\nat com.microsoft.dprep.execution.LariatDataset$.getFiles(LariatDataset.scala:32)\nat com.microsoft.dprep.execution.PySparkExecutor.getFiles(PySparkExecutor.scala:201)\nat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat java.base\/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat java.base\/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.base\/java.lang.reflect.Method.invoke(Method.java:566)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\nat java.base\/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure not found\nat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\nat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I get started predictive Maintanance using Machine Learning?",
        "Question_creation_time":1617360731433,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/342399\/how-do-i-get-started-predictive-maintanance-using.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Give me a brief description of the predictive maintenance using machine learning.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Long Running Experiment",
        "Question_creation_time":1635794626850,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/611813\/long-running-experiment.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am running a Machine Learning experiment in Azure Machine Learning Studio (not classic version). The total time of each step in the experiment is 7.5 min, however the overall execution time of the run was 20.5 min. Could someone please explain why there is such a large discrepancy between the total time the experiment took to run compared to the total of each step and what can be done to improve performance.\n\nThanks for your help.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I access a FileDataset without filling local disk?",
        "Question_creation_time":1628529752433,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/506820\/how-can-i-access-a-filedataset-without-filling-loc.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello!\n\nI'm setting up a pipeline for machine learning. Reading the docs I understood that when I pass my FileDataset as_mount it is mounted, similar to mounting an external drive. However, three hours into my training, my pipeline crashed, out of storage. It seems that as_mount actually is downloading per file, rather than the entire Dataset, but still uses local disk space. Is this correct? If so, how can I train on a FileDataset that is too large for any of the available compute options?\n\nDavid",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use the value of a PipelineParameter (passed from DataFactory) in a blob path for an OutputFileDatasetConfig object (in an ML pipeline)",
        "Question_creation_time":1648036709603,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/784006\/use-the-value-of-a-pipelineparameter-passed-from-d.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nIs it possible to use a PipelineParameter (defined in a DataFactory 'Machine Learning Execute Pipeline' activity) during the creation of a OutputFileDatasetConfig object in said Machine Learning pipeline?\n\nMy DataFactory pipeline runs on a schedule (via a trigger) and executes an Azure ML pipeline which does data preparation and model training.\nThe trigger start date is passed as a parameter 'date_time' to the ML pipeline.\n\nIn my ML pipeline, I want to save the model artifacts (trained in a PythonScriptStep) to a blob path (default_datastore + 'output_model\/{date_time}') which contains the value of the 'date_time' parameter. But I can't figure out a way to use the value of 'date_time' during the creation of the OutputFileDatasetConfig object (maybe there is a simple way to save model artifacts than to use a OutputFileDatasetConfig object?).\n\nAs a temporary hack, I am using a variable 'today_date' in my ML pipeline definition which contains today's date, and I use this variable to build the destination path of OutputFileDatasetConfig.\nBut the ideal solution would be to get the actual date directly from the DataFactory trigger parameter.\nThis is how I do now in my ML pipeline (not ideal):\n\n import datetime\n today_date = datetime.date.today().strftime('%Y%m%d')\n model_output_path = (def_data_store, f\"output_model\/{today_date}\")\n output_config = OutputFileDatasetConfig(destination = model_output_path)\n\n\n\nThis is what I tried in order to get the value of PipelineParameter, but it didn't work:\n\n pipeline_parameter = PipelineParameter(name=\"date_time\", default_value=today_date)\n model_output_path = (def_data_store, f\"output_model\/{pipeline_parameter}\")\n output_config = OutputFileDatasetConfig(destination = model_output_path)\n\n\n\nIt seems the only way to get the value of the PipelineParameter is through an argument inside a PythonScriptStep.\nI don't think I can create the OutputFileDatasetConfig object INSIDE the PythonScriptStep.\nIs there any other way to easily save model artifacts to a specific blob path which contains the value of a PipelineParameter?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Leave-one-group-out cross-validation in Azure AutoML",
        "Question_creation_time":1618488817010,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/358763\/leave-one-group-out-cross-validation-in-azure-auto.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a dataset where each row is a data sample, and there is a a column indicating a group this sample came from. So, each group has several data points, and each one is a row in the dataframe. I would like to run the cross-validation so that at each fold, the data points from one group are used as the validation set, and the data points from other groups as the training test. Is this currently somehow possible in Azure AutoML ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Import issue with azureml-train-automl-runtime package",
        "Question_creation_time":1669689516027,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1107805\/import-issue-with-azureml-train-automl-runtime-pac.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to import azureml-train-automl-runtime to do explanations from azure automl pipeline following the tutorial in the link https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-machine-learning-interpretability-automl\n\nBut I am getting the below error\n\n Exception ignored in: <function _Win32Helper.del at 0x0000021ECA3AD430> Traceback (most recent call last): File \"C:\\Anaconda3\\envs\\check_win32_error\\lib\\site-packages\\azureml\\automl\\runtime\\shared\\win32_helper.py\", line 246, in del TypeError: catching classes that do not inherit from BaseException is not allowed\n\n\n\n\nError Screenshot:\n\n\n\n\n\nI asked this question in stackoverflow but did not get valid answer,\nplease refer to the stackoverflow link below:\nhttps:\/\/stackoverflow.com\/questions\/74160617\/baseexception-when-trying-to-import-azureml-train-automl-runtime-in-windows-10",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Excel en Microsoft Azure",
        "Question_creation_time":1614968855700,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/301247\/excel-en-microsoft-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hola a todos, perdon quiza sea muy basica mi pregunta, no se como importar un excel como Dataset. Solo puedo importar CSV, etc. Muchas gracias",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"New Azure Machine Learning & Excel",
        "Question_creation_time":1651222482640,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/831512\/new-azure-machine-learning-amp-excel.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi All\n\nI have been working with Azure Machine Learning Studio (Classic) and have always found its integration with Excel super mega useful.\n\nAll I had to do was to get the URI and the API_Key of my web service and paste them on the Azure Machine Learning Add-In, that I had downloaded. Easy and useful.\n\nHowever, with the new Azure Machine Learning studio that does not seem possible any more.\n\nUnder the new Azure Machine Learning studio when I deploy a model I get a REST endpoint and that's it? !? I cannot find anywhere the API_key for my web service. I cannot even find a web service section as such.\n\nHow do I get the API_Key for the web service I need?\n\nIf I get the API_Key could I use it on the Excel Azure Machine Learning add-in. It looks as if this is no longer an option and we need to start using Power BI instead.\n\nI have read this interesting post where someone mentions a work around that consist of creating an Excel macro. Is this the best option? https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html\n\n\n\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Studio Quotas for Assisted Labeling and Training Object Detection Model",
        "Question_creation_time":1653585502077,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/865733\/azure-ml-studio-quotas-for-assited-labeling-and-tr.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In an Azure ML Studio labeling project, I have tried to enable Assisted Labeling. I get the error message \"Error: There is insufficient quota to create a gpu compute target. You can request more quota and create a custom compute to enable ML assisted labeling.\"\n\nI also get a similar error message when I try to train an object detection model, \"\u201cSTANDARD_D2AS_V4 is not supported for image tasks. Please choose a VM type that is in the NC-family or the ND-family.\"\n\nI requested and was granted \"Standard NC Family Cluster Dedicated vCPUs\/GPUs. These show up in my quota, but if I go to create a compute target and select GPU, I get a message saying: \"You do not have enough quota for the following VM sizes.\" followed by a list of all the VMs that it says aren't in my quota including the NC family VMs.\n\nAnd I still get the same two error messages saying I don't have enough quota for either Assisted Labeling or training an object detection model.\n\nDoes anyone know what I need to do to get these two services to work?\n\nThanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Collaborate on Azure Machine Learning Project",
        "Question_creation_time":1596973683450,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/62694\/collaborate-on-azure-machine-learning-project.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I just recently became a Microsoft Certified Azure Data Scientist Associate. So I am looking for data scientist novices to join me practice on open datasets on kaggle to build machine learning models and do predictions using Azure ML. We will start with Titanic competition. If interested please visit my github link: https:\/\/github.com\/ivombi\/Titanic-Machine-Learning-from-Disaster or send a request on LinkedIn using my full name: Kubam Ivo Mbi.\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Datadrift in Azure ML SDK v2",
        "Question_creation_time":1657283475557,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/919651\/datadrift-in-azure-ml-sdk-v2.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I can't see a data drift module anywhere in v2 of the Azure ML Python SDK. Is this missing or what's the deal? If so, are there any plans of bringing it into v2?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"why the ML data only result in 1 point?",
        "Question_creation_time":1629379772053,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/520581\/why-the-ml-data-only-result-in-1-point.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ModuleExceptionMessage:ModuleOutOfMemory: Memory has been exhausted, unable to complete running of module.",
        "Question_creation_time":1619357291613,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370636\/moduleexceptionmessagemoduleoutofmemory-memory-has.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I am getting the error from the subject line when i try to inner join a dataset of 850K rows and 3 columns (parquet data file of around 4mb) with another with 300K rows and 10 columns (parquet data file is about 1mb). I'm using Azure ML Studio Designer\n\nMy compute is Standard Dv2 Family vCPUs (20% of utilization).\n\nI was surprised by this hitting a limit. Any idea on how i should proceed?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"i am not able to select jupyter notebook in azure ML",
        "Question_creation_time":1604559071717,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/152224\/i-am-not-able-to-select-jupyter-notebook-in-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Conda environment locked by another AzureML job",
        "Question_creation_time":1611713493223,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/246501\/conda-environment-locked-by-another-azureml-job.html",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I tried to run an experiments. There was an error in my first submit and the run did not go through. However, a lock has been created which is preventing me from submitting further runs. I am getting the following error.\n\n\"The conda environment is currently locked by another AzureML job. Further job submission will wait until the other process finishes. If there are no other jobs running, please delete \/home\/azureuser\/.azureml\/locks\/azureml_conda_lock\"\n\nI tried to use:\naz ml run cancel -r exp_id\n\nin CLI. However, this gives me an error:\nError, default experiment not set and experiment name parameter not provided.\\nPlease provide a value for the experiment name parameter.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What are the differences between LightGBM and FastTree algorithms used by ML.Net's modelbuilder?",
        "Question_creation_time":1620299148307,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/385365\/what-are-the-differences-between-lightgbm-and-fast.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"For my master thesis, I am using ML.Net library with the model builder to build machine learning models for regression \/ to predict values. My field of study is in constructional mechanics, so I'm rather new to machine learning.\n\nFor most of my models, tree-based regression algorithms seem to be the best performing, and of these LightGBM and FastTree are the best performing algorithms.\n\nI've tried to read up on LightGBM here: https:\/\/www.microsoft.com\/en-us\/research\/publication\/lightgbm-a-highly-efficient-gradient-boosting-decision-tree\/ And FastTree here: https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/microsoft.ml.trainers.fasttree.fasttreeregressiontrainer?view=ml-dotnet\n\nHowever, I struggle to distinguish what the difference between these algorithms is. Could someone explain what the difference between LightGBM and FastTree is?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pytorch error - RuntimeError: Unable to find a valid cuDNN algorithm to run convolution on Standard_NC6 and Python 3.8 - Pytorch and Tensorflow kernel",
        "Question_creation_time":1666006220943,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1050727\/pytorch-error-runtimeerror-unable-to-find-a-valid.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello community,\n\nI am new to Azure. I have some scripts in a working environment in google colab, and as I am working on my Thesis I tried to user Azure Student promo.\nI have setup a Standard_NC6 with Pytorch and Tensorflow kernel and I am getting the following error:\n\n---------------------------------------------------------------------------\nRuntimeError Traceback (most recent call last)\nInput In [9], in <cell line: 64>()\n76 loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n77 critic.zero_grad()\n---> 78 loss_critic.backward(retain_graph=True)\n79 opt_critic.step()\n81 # clip critic weights between -0.01, 0.01\n\n\nFile \/anaconda\/envs\/azureml_py38_PT_TF\/lib\/python3.8\/site-packages\/torch\/_tensor.py:396, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)\n387 if has_torch_function_unary(self):\n388 return handle_torch_function(\n389 Tensor.backward,\n390 (self,),\n(...)\n394 create_graph=create_graph,\n395 inputs=inputs)\n--> 396 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n\n\nFile \/anaconda\/envs\/azureml_py38_PT_TF\/lib\/python3.8\/site-packages\/torch\/autograd\/init.py:173, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n168 retain_graph = create_graph\n170 # The reason we repeat same the comment below is that\n171 # some Python versions print out the first line of a multi-line function\n172 # calls in the traceback and some print out the last line\n--> 173 Variable.execution_engine.run_backward( # Calls into the C++ engine to run the backward pass\n174 tensors, grad_tensors, retain_graph, create_graph, inputs,\n175 allow_unreachable=True, accumulate_grad=True)\n\n\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n\n\n\n\nI tried different versions of pytorch + cu113 and cu116.\n\nThe nvdia-smi output is:\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.141.03 Driver Version: 470.141.03 CUDA Version: 11.4 |\n|-------------------------------+----------------------+----------------------+\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\n| Fan Temp Perf Pwr:Usage\/Cap| Memory-Usage | GPU-Util Compute M. |\n| | | MIG M. |\n|===============================+======================+======================|\n| 0 Tesla K80 On | 00000001:00:00.0 Off | 0 |\n| N\/A 41C P0 70W \/ 149W | 880MiB \/ 11441MiB | 0% Default |\n| | | N\/A |\n+-------------------------------+----------------------+----------------------+\n\n\n+-----------------------------------------------------------------------------+\n| Processes: |\n| GPU GI CI PID Type Process name GPU Memory |\n| ID ID Usage |\n|=============================================================================|\n| 0 N\/A N\/A 11425 C ...eml_py38_PT_TF\/bin\/python 877MiB |\n+-----------------------------------------------------------------------------+\n\nI guess that the problem is about drivers and versions.. as in google colab environment it's working\n\nThanks,\nDP",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Synapse - 'No Azure Cognitive Service Linked Service are available'",
        "Question_creation_time":1644147139677,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/724155\/synapse-39no-azure-cognitive-service-linked-servic.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am following this guide\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-cognitive-services-sentiment\n\nBut when I get to selecting 'Machine Learning - Predict with a Model', then I choose 'Sentiment Analysis'\n\nBut the first dropdown is greyed out for me and reads:\nAzure Cognitive Services linked service: No Azure Cognitive Service Linked Service are available\n\nI had one created, then created another. AKV and link service tested with specific key.\n\nAnyone know how I can get it to recognize the Cognitive Service Linked Server?\n\nThanks\n\n][2]\n\n\n\n\n\n\n\n[2]: \/answers\/storage\/attachments\/171659-image.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Consume Azure ML Web Service with Postman: how to pass arguments?",
        "Question_creation_time":1603965300457,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/144230\/consume-azure-ml-web-service-with-postman-how-to-p.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to pass parameters to an Azure ML Web Service with Postman? I created an R web service endpoint that runs in an Azure Container Instance. My run function has one argument (\"data\"). I can call the web service using Azure ML R SDK (using invoke_webservice()) and the input parameter is read successfully from the request content. The input is constructed like:\n\n toJSON(data.frame(data=\"This is my test string\"))\n\n\n\nResult:\n\n [{\"data\": \"This is my test string\"}]\n\n\n\nIf I create a Postman request and copy the input to the request body, the input parameter is not passed to the web service. The web service can return a static output to Postman but the variable data is always empty. Is this a property of the ML Web Service? If not, how can I set up the request body so that the argument is read successfully? I have tried many variations, but none have worked.\n\nI have set content-type header to application\/json. I don't have authentication in the web service, since it is just a test instance.\n\nUltimately, we need to call the web service with C# from Azure Function. I know that we can use the C# template from documentation and it can probably pass the parameter to the web service, but it would be nice to be able to test the web service with Postman.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Receive error GraphDatasetNotFound: Request failed with status code 400 when submitting pipeline",
        "Question_creation_time":1619957038450,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/379678\/receive-error-graphdatasetnotfound-request-failed.html",
        "Question_upvote_count":4.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am running through the tutorial at ..https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-clustering-model-azure-machine-learning-designer\/explore-data\n\nWhen I submit my pipeline I am seeing the error ...\n\nAn error occurred while submitting pipeline run\nGraphDatasetNotFound: Request failed with status code 400\n\nThis is an incredibly unhelpful message. I believe I have followed the steps as per the tutorial.\n\nAny idea what is the cause of this error?\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":20.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"frame = ds.to_dataframe() doesn't work for me",
        "Question_creation_time":1605353730307,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/162914\/frame-dsto-dataframe-doesn39t-work-for-me.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML AutoML : Data transformation diagram only 1 column",
        "Question_creation_time":1626184752807,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/474091\/azure-ml-automl-data-transformation-diagram-only-1.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I am using the AutoML of Azure ML. I don't understand the diagram of Data transformation (that is still in preview). It tells me that I start with 26 columns, which is correct, but then says I'm ending up with 1 column only, after a MeanImputer. ![114216-1column.png][1] If I check the engineered features in the code, I get this table, so 26 columns with the application of MeanImputer for each of them. ![114215-allcolumns.png][2] Could you tell me why the diagram tells me that there is only 1 column at the end? [1]: \/answers\/storage\/attachments\/114216-1column.png [2]: \/answers\/storage\/attachments\/114215-allcolumns.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to export experiment parameters and logged metrics in Azure ML to CSV?",
        "Question_creation_time":1605641332880,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/166057\/is-there-a-way-to-export-experiment-parameters-and.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am running a bunch of ML experiments using AzureML, sometimes changing training parameters and sometimes aspects of the data preprocessing. In general, for a given experiment I will be able to get a table (aka \"view\") like this:\n\nWhile the UI allows some minimum level of customization, sorting runs by e.g. desired columns (say the accuracy to identify the best runs) seems really problematic.\n\nThe only workaround I am aware of is to save the page to HTML (!) and extract the values from there.\nThe data in the cells can't by copied with a cursor either...\n\nIs there an easy way to export the data collected during several runs, via the UI or programmatically, without the need to scrape the blob storage of the Azure ML workspace (I am asking the community here as docs don't seem particularly helpful)?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"certification test for AI-900: Microsoft Azure AI Fundamentals not available",
        "Question_creation_time":1662205880130,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/992629\/exam-ai-900-microsoft-azure-ai-fundamentals-not-av.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hallo, i would like make an appointment for Exam AI-900: Microsoft Azure AI Fundamentals.\nHowever this exam is currently not available at Pearson vue or Certiport. When can i expect this again? Is there an alternative ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to move all the ai models (service endpoints) from one compute cluster to another cluster in azure without any effects",
        "Question_creation_time":1636704232043,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/624851\/how-to-move-all-the-ai-models-service-endpoints-fr.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have machine learning models which is deployed in Azure aks in 'x' Cluster , Now i want to move this models and its endpoint in another cluster which is 'y' within same workspace and subscription ,So how to do this without any changes to its rest endpoint as this endpoints are in production use already.\n\nalso if this is not possible then can i upgrade my x cluster with newer version without affecting my endpoints",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML authentication with Service Principal certificate",
        "Question_creation_time":1601045466093,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/108060\/azure-ml-authentication-with-service-principal-cer.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI am responsible for deploying Azure ML resources like workspace, compute target and datastore using Python from Azure CI\/CD Devops Pipeline. I have Service principal certificate for authentication. I am confused about which authentication method of Python I will follow.\n\nShall I used MSAL authentication? or\n\nplease suggest a secure authentication method of python that supports Service Principal certificate to authenticate Azure ML workspace. Please share a sample as reference if any.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is their limit on number of records in dataset for Azure automated ML timeserires forecasting",
        "Question_creation_time":1628484429713,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/505948\/is-their-limit-on-number-of-records-in-dataset-for.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi\n\nI am facing issue with number of records while creating datasets automated ML timeseries forecasting, It is loading only first 10000 records rest of the records are ignored.\n\nIs their any limit on number of records in the datasets for Azure automated ML timeseries forecasting.\n\n\n\n\nIf there limits in number of records , how to increase number of records\n\nThanks\nRamabadran",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AML Studio - cannot create dataset from datastore file",
        "Question_creation_time":1605192099620,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/160773\/aml-studio-cannot-create-dataset-from-datastore-fi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using Jupyter notebooks to work in AML. I was able to upload a file to a datastore (the default workspaceblobstore), but am receiving an error when I try to create a dataset using this file. The relevant code is below:\n\n #This part works\n    \n datastore = ws.get_default_datastore()\n    \n datastore.upload_files(files = ['data\/german_credit_dataset.csv'], overwrite = True, show_progress = True)\n    \n # This part doesn't\n    \n dataset = Dataset.Tabular.from_delimited_files(path = [(datastore ,'german_credit_dataset.csv')])\n\nI know the file was uploaded correctly as I am able to locate it in the datastore, and manually create a dataset. The error I receive is:\n\ncode\": \"UserError\",\n\"message\": \"Cannot load any data from the specified path. Make sure the path is accessible and contains data.\\nScriptExecutionException was caused by DatastoreResolutionException.\\r\\n DatastoreResolutionException was caused by UnexpectedException.\\r\\n\nCould it be a permissions issue? I used the json file to connect to my compute instance and it seems to work since I was able to upload the file.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML real-time inference endpoint deployment stuck with deployment state as transitioning for over 2 hours",
        "Question_creation_time":1663182438927,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1007819\/azure-ml-real-time-inference-endpoint-deployment-s.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have an AKS cluster and am trying to deploy a real-time inference pipeline to it as an endpoint. The deployment state is switching between \"Transitioning\" and \"Failed\" and I am unable to see any logs. My cluster is in West Central US.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why PyTorch is using only one GPU ?",
        "Question_creation_time":1653503234337,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/864175\/why-pytorch-is-using-only-one-gpu.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Azure does not use the two GPUs of my node with PyTorch (and Hugging Face). The monitoring tool of Azure shows the GPU usage is stuck at 50%.\nIts a Standard_NC12, so it has two K80s.\n\n\n\n\nI tried this way :\nhttps:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/distributed-training\/#distributeddataparallel-per-process-launch\nand it looked like this in my notebook :\n\n\n\n\n\nI copied the docker file from the curated environments and added the libraries I needed successfully :\n\n FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220329.v1\n    \n ENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.10\n    \n # Create conda environment\n RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n     python=3.8 \\\n     pip=20.2.4 \\\n     pytorch=1.10.0 \\\n     torchvision=0.11.1 \\\n     torchaudio=0.10.0 \\\n     cudatoolkit=11.1.1 \\\n     nvidia-apex=0.1.0 \\\n     gxx_linux-64 \\\n     -c anaconda -c pytorch -c conda-forge\n    \n # Prepend path to AzureML conda environment\n ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n    \n # Install pip dependencies\n RUN pip install 'matplotlib>=3.3,<3.4' \\\n                 'psutil>=5.8,<5.9' \\\n                 'tqdm>=4.59,<4.63' \\\n                 'pandas>=1.3,<1.4' \\\n                 'scipy>=1.5,<1.8' \\\n                 'numpy>=1.10,<1.22' \\\n                 'ipykernel~=6.0' \\\n                 'azureml-core==1.40.0' \\\n                 'azureml-defaults==1.40.0' \\\n                 'azureml-mlflow==1.40.0' \\\n                 'azureml-telemetry==1.40.0' \\\n                 'tensorboard==2.6.0' \\\n                 'tensorflow-gpu==2.6.0' \\\n                 'onnxruntime-gpu>=1.7,<1.10' \\\n                 'horovod==0.23' \\\n                 'future==0.18.2' \\\n                 'wandb' \\\n                 'transformers' \\\n                 'einops' \\\n                 'torch-tb-profiler==0.3.1'\n    \n    \n # This is needed for mpi to locate libpython\n ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n    \n RUN export CUDA_VISIBLE_DEVICES=0,1\n\n\n\nI tried everything, I even added the CUDA_VISIBLE_DEVICES=0,1 inside the docker file.\n\nMy cluster is correctly configured because my colleague can use another tool (Detr with Lightning) and use 100% of the computing power.\nI copied his docker file and the result was the same, so our guess is that his tool is automatically managing all GPUs for him.\n\nDoes anyone know why the cluster is using only one GPU ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Bug] azureml-train Python package deprecated but did receive an update which is not in line with azureml-train-core",
        "Question_creation_time":1629973035930,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/528959\/bug-azureml-train-python-package-deprecated-but-di.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"The Python package azureml-train is deprecated and seems basically a wrapper around azureml-train-core. When installing azureml-train, if I'm correct it tries to install the azureml-train-core package with the same version number. However, on the 24th of August 2021 the azureml-train package was updated to version 1.33.1 whereas azureml-train-core wasn't updated. This causes the installation of azureml-train to fail.\n\nI would suggest to remove version 1.33.1 of azureml-train such that it still can be installed.\nOtherwise, I'm curious why this situation is the case.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Realtime endpoint deploy 'xxx' stayed in progress",
        "Question_creation_time":1661074944827,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/975300\/realtime-endpoint-deploy-39xxx39-stayed-in-progres.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi everyone,\n\nI'm trying to deploy my real time inference, but after a while not happened to it. I didn't got any error or message and nothing happens. (See the following picture).\n\nWhat I did:\n\nI created a pipeline in designer, it's valid and be submitted very well.\n\n\nAfter submit is complete, I created a \"Real-time inference pipeline\" via button in menu!\n\n\n\n\nAnd finally I tried to deploy it, but nothing happened. (For deploy I tried both of Azure Kubernetes Service and Azure Container Instance)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Experiment stuck in Queued",
        "Question_creation_time":1642636050810,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/702171\/azure-ml-experiment-stuck-in-queued.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have only one experiment running, but it is stuck in queued. I see this happens to people a lot, but on one ever says how to fix it.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\",",
        "Question_creation_time":1619621454417,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/375577\/submitted-script-failed-with-a-non-zero-exit-code.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi All,\nI am trying to creating batch inference of my pretrained churn classification model. I was following this github of iris batch inference [1]: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/parallel-run\/tabular-dataset-inference-iris.ipynb .\nBut I am getting error , please help me how can I fix this error.\n\nHere my code:\n\n\n\n\n\n\n\n\n\nHere my errors:\n\n\n\n ========================================================================================================================\n 2\n . Please ignore this if the GPUs don't utilize NVIDIA\u00ae NVLink\u00ae switches.\n 2021-04-28T12:53:39Z Starting output-watcher...\n 2021-04-28T12:53:39Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n 2021-04-28T12:53:39Z Executing 'Copy ACR Details file' on 10.0.0.4\n 2021-04-28T12:53:39Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n >>>   \n >>>   \n Login Succeeded\n Using default tag: latest\n latest: Pulling from azureml\/azureml_af590fdfaae8ba3ead1eba5ea12b0fb3\n 4007a89234b4: Pulling fs layer\n 5dfa26c6b9c9: Pulling fs layer\n 0ba7bf18aa40: Pulling fs layer\n 4c6ec688ebe3: Pulling fs layer\n 574f361512d6: Pulling fs layer\n db4d1e2d7079: Pulling fs layer\n e544ee0f522d: Pulling fs layer\n c655136086be: Pulling fs layer\n 2ec37f44090c: Pulling fs layer\n 5fba3bd4a2c4: Pulling fs layer\n 7e0ea9d0a1ab: Pulling fs layer\n da005f826951: Pulling fs layer\n 6e842608b724: Pulling fs layer\n 6b1a4187f1d0: Pulling fs layer\n db4d1e2d7079: Waiting\n c763bae43813: Pulling fs layer\n 490d7c37a7d7: Pulling fs layer\n 791bb1082f38: Pulling fs layer\n e544ee0f522d: Waiting\n e863af755720: Pulling fs layer\n c655136086be: Waiting\n 4c6ec688ebe3: Waiting\n 0cb6e30b3f1c: Pulling fs layer\n 88468e3f4c2c: Pulling fs layer\n 77d6ac8c0bf7: Pulling fs layer\n 574f361512d6: Waiting\n 2ec37f44090c: Waiting\n da005f826951: Waiting\n 5fba3bd4a2c4: Waiting\n 6e842608b724: Waiting\n 6b1a4187f1d0: Waiting\n c763bae43813: Waiting\n 490d7c37a7d7: Waiting\n 791bb1082f38: Waiting\n e863af755720: Waiting\n 0cb6e30b3f1c: Waiting\n 88468e3f4c2c: Waiting\n 77d6ac8c0bf7: Waiting\n 7e0ea9d0a1ab: Waiting\n 0ba7bf18aa40: Verifying Checksum\n 0ba7bf18aa40: Download complete\n 5dfa26c6b9c9: Verifying Checksum\n 5dfa26c6b9c9: Download complete\n 4c6ec688ebe3: Verifying Checksum\n 4c6ec688ebe3: Download complete\n 4007a89234b4: Download complete\n db4d1e2d7079: Verifying Checksum\n db4d1e2d7079: Download complete\n e544ee0f522d: Verifying Checksum\n e544ee0f522d: Download complete\n 574f361512d6: Verifying Checksum\n 574f361512d6: Download complete\n 4007a89234b4: Pull complete\n 5dfa26c6b9c9: Pull complete\n 0ba7bf18aa40: Pull complete\n 4c6ec688ebe3: Pull complete\n 5fba3bd4a2c4: Download complete\n c655136086be: Verifying Checksum\n c655136086be: Download complete\n 7e0ea9d0a1ab: Verifying Checksum\n 7e0ea9d0a1ab: Download complete\n da005f826951: Verifying Checksum\n da005f826951: Download complete\n 6e842608b724: Download complete\n 6b1a4187f1d0: Download complete\n c763bae43813: Verifying Checksum\n c763bae43813: Download complete\n 2ec37f44090c: Verifying Checksum\n 2ec37f44090c: Download complete\n 490d7c37a7d7: Verifying Checksum\n 490d7c37a7d7: Download complete\n 0cb6e30b3f1c: Verifying Checksum\n 0cb6e30b3f1c: Download complete\n e863af755720: Verifying Checksum\n e863af755720: Download complete\n 77d6ac8c0bf7: Verifying Checksum\n 77d6ac8c0bf7: Download complete\n 88468e3f4c2c: Verifying Checksum\n 88468e3f4c2c: Download complete\n 574f361512d6: Pull complete\n db4d1e2d7079: Pull complete\n e544ee0f522d: Pull complete\n 791bb1082f38: Verifying Checksum\n 791bb1082f38: Download complete\n c655136086be: Pull complete\n 2ec37f44090c: Pull complete\n 5fba3bd4a2c4: Pull complete\n 7e0ea9d0a1ab: Pull complete\n da005f826951: Pull complete\n 6e842608b724: Pull complete\n 6b1a4187f1d0: Pull complete\n c763bae43813: Pull complete\n 490d7c37a7d7: Pull complete\n    \n Streaming azureml-logs\/65_job_prep-tvmps_287cfab3497943a39d90c089311555c3223ca350d504acc72af6aceb3d957ba3_p.txt\n ===============================================================================================================\n [2021-04-28T12:54:05.020376] Entering job preparation.\n [2021-04-28T12:54:08.337333] Starting job preparation.\n [2021-04-28T12:54:08.337375] Extracting the control code.\n [2021-04-28T12:54:08.365360] fetching and extracting the control code on master node.\n [2021-04-28T12:54:08.365417] Starting extract_project.\n [2021-04-28T12:54:08.365467] Starting to extract zip file.\n [2021-04-28T12:54:09.302078] Finished extracting zip file.\n [2021-04-28T12:54:09.804262] Using urllib.request Python 3.0 or later\n [2021-04-28T12:54:09.804327] Start fetching snapshots.\n [2021-04-28T12:54:09.804373] Start fetching snapshot.\n [2021-04-28T12:54:09.804391] Retrieving project from snapshot: f4a38de4-3230-4038-ac4b-cde33bdd63e5\n Starting the daemon thread to refresh tokens in background for process with pid = 51\n [2021-04-28T12:54:10.714200] Finished fetching snapshot.\n [2021-04-28T12:54:10.714233] Start fetching snapshot.\n [2021-04-28T12:54:10.714251] Retrieving project from snapshot: b71de588-0f3c-44ae-b144-ea24a905546e\n [2021-04-28T12:54:24.343681] Finished fetching snapshot.\n [2021-04-28T12:54:24.343714] Finished fetching snapshots.\n [2021-04-28T12:54:24.343728] Finished extract_project.\n [2021-04-28T12:54:24.360941] Finished fetching and extracting the control code.\n [2021-04-28T12:54:24.364330] downloadDataStore - Download from datastores if requested.\n [2021-04-28T12:54:24.365371] Start run_history_prep.\n [2021-04-28T12:54:24.436823] Entering context manager injector.\n Acquired lockfile \/tmp\/a1c4fded-7336-4024-8c9e-fed19f5d1b37-datastore.lock to downloading input data references\n [2021-04-28T12:54:24.903804] downloadDataStore completed\n [2021-04-28T12:54:24.906597] Job preparation is complete.\n    \n Streaming azureml-logs\/70_driver_log.txt\n ========================================\n 2021\/04\/28 12:54:26 Starting App Insight Logger for task:  runTaskLet\n 2021\/04\/28 12:54:26 Attempt 1 of http call to http:\/\/10.0.0.4:16384\/sendlogstoartifacts\/info\n 2021\/04\/28 12:54:26 Attempt 1 of http call to http:\/\/10.0.0.4:16384\/sendlogstoartifacts\/status\n [2021-04-28T12:54:27.564276] Entering context manager injector.\n [context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', '\n 2021\/04\/28 12:54:31 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n Stopped: false\n OriginalData: 1\n FilteredData: 0.\n    \n Streaming azureml-logs\/75_job_post-tvmps_287cfab3497943a39d90c089311555c3223ca350d504acc72af6aceb3d957ba3_p.txt\n ===============================================================================================================\n [2021-04-28T13:02:20.275818] Entering job release\n [2021-04-28T13:02:21.348190] Starting job release\n [2021-04-28T13:02:21.348739] Logging experiment finalizing status in history service.\n Starting the daemon thread to refresh tokens in background for process with pid = 1369\n [2021-04-28T13:02:21.349418] job release stage : upload_datastore starting...\n [2021-04-28T13:02:21.349812] job release stage : start importing azureml.history._tracking in run_history_release.\n [2021-04-28T13:02:21.352029] job release stage : copy_batchai_cached_logs starting...\n [2021-04-28T13:02:21.352142] job release stage : execute_job_release starting...\n [2021-04-28T13:02:21.357651] job release stage : copy_batchai_cached_logs completed...\n [2021-04-28T13:02:21.358513] Entering context manager injector.\n [2021-04-28T13:02:21.372410] job release stage : upload_datastore completed...\n [2021-04-28T13:02:21.595288] job release stage : execute_job_release completed...\n [2021-04-28T13:02:21.628735] job release stage : send_run_telemetry starting...\n [2021-04-28T13:02:21.849387] get vm size and vm region successfully.\n [2021-04-28T13:02:22.175695] get compute meta data successfully.\n [2021-04-28T13:02:22.444070] post artifact meta request successfully.\n [2021-04-28T13:02:22.471466] upload compute record artifact successfully.\n [2021-04-28T13:02:22.471531] job release stage : send_run_telemetry completed...\n [2021-04-28T13:02:22.471747] Job release is complete\n    \n StepRun(batch-score) Execution Summary\n =======================================\n StepRun( batch-score ) Status: Failed\n ---------------------------------------------------------------------------\n ActivityFailedException                   Traceback (most recent call last)\n <ipython-input-30-49d7d34a142d> in <module>\n       3 # Run the pipeline as an experiment\n       4 pipeline_run = Experiment(ws, 'batc-prediction_pipeline').submit(pipeline)\n ----> 5 pipeline_run.wait_for_completion(show_output=True)\n    \n \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py in wait_for_completion(self, show_output, timeout_seconds, raise_on_error)\n     293                             try:\n     294                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n --> 295                                                              raise_on_error=raise_on_error)\n     296                             except TypeError as e:\n     297                                 # If there are package conflicts in the user's environment, the run rehydration\n    \n \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py in wait_for_completion(self, show_output, timeout_seconds, raise_on_error)\n     735             try:\n     736                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n --> 737                                                raise_on_error=raise_on_error)\n     738             except KeyboardInterrupt:\n     739                 error_message = \"The output streaming for the run interrupted.\\n\" \\\n    \n \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py in _stream_run_output(self, timeout_seconds, raise_on_error)\n     823             print(json.dumps(error, indent=4))\n     824         if error and raise_on_error:\n --> 825             raise ActivityFailedException(error_details=json.dumps(error, indent=4))\n     826 \n     827         print(final_details)\n    \n ActivityFailedException: ActivityFailedException:\n  Message: Activity Failed:\n {\n     \"error\": {\n         \"code\": \"UserError\",\n         \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\",\n         \"messageFormat\": \"{Message}\",\n         \"messageParameters\": {\n             \"Message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\"\n         },\n         \"details\": [],\n         \"innerError\": {\n             \"code\": \"UserTrainingScriptFailed\"\n         }\n     },\n     \"correlation\": {\n         \"operation\": null,\n         \"request\": \"6833f86b6a0c0af1\"\n     },\n     \"environment\": \"eastus\",\n     \"location\": \"eastus\",\n     \"time\": \"2021-04-28T13:02:41.490064Z\",\n     \"componentName\": \"execution-worker\"\n }\n  InnerException None\n  ErrorResponse \n {\n     \"error\": {\n         \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"AzureMLCompute job failed.\\\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\\\n\\\\tReason: Job failed with non-zero exit Code\\\",\\n        \\\"messageFormat\\\": \\\"{Message}\\\",\\n        \\\"messageParameters\\\": {\\n            \\\"Message\\\": \\\"AzureMLCompute job failed.\\\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\\\n\\\\tReason: Job failed with non-zero exit Code\\\"\\n        },\\n        \\\"details\\\": [],\\n        \\\"innerError\\\": {\\n            \\\"code\\\": \\\"UserTrainingScriptFailed\\\"\\n        }\\n    },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n        \\\"request\\\": \\\"6833f86b6a0c0af1\\\"\\n    },\\n    \\\"environment\\\": \\\"eastus\\\",\\n    \\\"location\\\": \\\"eastus\\\",\\n    \\\"time\\\": \\\"2021-04-28T13:02:41.490064Z\\\",\\n    \\\"componentName\\\": \\\"execution-worker\\\"\\n}\"\n     }\n }\n    \n \u200b",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remote run model unable to be saved",
        "Question_creation_time":1613083790957,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:\n\nbest_run, fitted_model = remote_run.get_output()\nfitted_model\n\nWhen runnning the code, I get the following error:\n\nAttributeError: 'DataTransformer' object has no attribute 'enable_dnn'\n\nAny help would be much appreciated.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the difference between online learning and offline learning",
        "Question_creation_time":1623786913570,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/437505\/what-is-the-difference-between-online-learning-and.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am a new learner of machine learning and computer science, I wonder the difference between these two terms. I am confused on the concept, can someone answer this question?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning excel add-ins error",
        "Question_creation_time":1659592116103,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/954559\/azure-machine-learning-excel-add-ins-error.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Everyone,\n\nHas anyone else experienced this issue before ? It was working fine yesterday and the web service is also fine. Just that the Excel Add-Ins is having this problem. It's saying \"The content is blocked because it isn't signed by a valid security certificate\". I am not sure where else to ask. Kindly help.\n\nThanks,\nAiman",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Studio Versioning",
        "Question_creation_time":1649276461560,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/803089\/azure-ml-studio-versioning.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is there a way to pass in a specific version number parameter to render the studio UI that will exclude new \/ preview features?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"PermissionError: [Errno 13] Permission denied\" when trying to access a local file for a conda environment",
        "Question_creation_time":1652041850827,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/840929\/34permissionerror-errno-13-permission-denied34-whe.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"After Azure ML Studio blocking me from using any compute due to an as-of-yet unresolved authentication error, I moved to using a Jupyter Notebook on my local workstation to try to configure my experiments locally then send the job to an Azure compute cluster. I have two lines of Python that tries to create an environment class by accessing a .yml file on my local computer:\n\nyml_path = r\"C:\\Users\\me\\Desktop\\azure_training\\training_env\"\npytorch_env = Environment.from_conda_specification(name='pytorch-1.11-gpu', file_path=yml_path)\n\nThis causes the following error:\n\nPermissionError: [Errno 13] Permission denied: 'C:\\\\Users\\\\me\\\\Desktop\\\\azure_training\\\\training_env'\n\nI am unsure of what is causing this. When the file doesn't need to be private, I have solved permission denied issues in the past that resulted from locally run tools such as PostgreSQL by going to the file's properties>>security and adding the user \"Everyone\" with full control. I tried doing that in this case, but it had no impact. I still get permission denied even though \"Everyone\" has full control over the file.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do we create learning virtual machine AI assistants with smart home control and self-driving vehicle funtionality?",
        "Question_creation_time":1591266810177,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31993\/how-do-we-create-learning-virtual-machine-ai-assis.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am developing Conscious Quantum Coding Living AI Virtual Assistants to help with everything.\n\nJodi, The AI Motor Home\nJodi will be an integrative, quantum coded, learning\/self-improving, online\/cloud, virtual machine, life conscious Living AI assistant who fully controls, and self drives, an RV\/Motor home\n\nHow would you create a Living AI assistant for a motor home?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning Studio (classic) Export Data",
        "Question_creation_time":1591053033787,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31081\/azure-machine-learning-studio-classic-export-data.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI am using Azure MLStudio (classic). I am able to connect to SQLServer Managed Instance(private) using Import Data module and On-Prem SQL Database connection through Data Gateway.\n\nI am not able to Export Data to exactly the same database. Could you please help me with that?\n\nOur Managed Instance connot be made public.\n\nThanks in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Automated ML endpoint questions (performance, multiple return values, scoring)",
        "Question_creation_time":1662590602493,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/998394\/automated-ml-endpoint-questions-performance-multip.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi....\n\nI used Automated ML to train a model on a set of grouping ids and titles, very simple use case, just two columns...predict the group id from the title. There were about 32000 rows in the input split into a training set of 90% and a validation set of 10%. Best model was 'MaxAbsScaler, LogisticRegression'.\n\nI deployed the endpoint using the 'realtimeendpoint' method. But each request takes 10 seconds to return a response. I took the default ML compute type VM which isn't a wimpy machine. Is it normal to be so slow? Will better hardware get the response time into the sub-one-second time-frame I need it to be? Are there other options to improve performance?\n\n\nI only ever get back one value in the response. Is it possible to get multiple predicted values?\n\n\nI don't see a 'confidence' score in the response, or see a way to request one. Is that possible?\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine learning AutoML Fail",
        "Question_creation_time":1648706020587,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/794697\/azure-machine-learning-automl-fail.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi ,\n\nI use the fakenews data to try azure machine learning automl , but always train model fail and I tried reducing the feature field , but still fail\n\nError message :\n\n\n\n\n\nRun timed out. No model completed training in the specified time. Possible solutions:\n1) Please check if there are enough compute resources to run the experiment.\n2) Increase experiment timeout when creating a run.\n3) Subsample your dataset to decrease featurization\/training time out\n\n\n\n\n\ncompute machine : STANDARD_DS12_V2\n\ndata source : https:\/\/www.kaggle.com\/c\/fake-news\/data\n\n\n\n\n\nIs any good idea or suggestions ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"D365 Demand Forecasting - Can we connect to new Azure ML Service instead of a classic studio service?",
        "Question_creation_time":1606959642257,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/183990\/d365-demand-forecasting-can-we-connect-to-new-azur.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"There is a D365 forecasting option that allows to connect to a azure ml classic studio service. Can we connect from D365 to the new Azure ML Service? I couldnt find any documentation about this, any pointers please.Thanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there any way to create automatic datasets in Azure Machine Learning using Azure Data Factory",
        "Question_creation_time":1651474101517,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/833237\/is-there-any-way-to-create-automatic-datasets-in-a.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have Storage ADLS account, Azure Data Factory and Azure Machine Learning services. In Azure ML , we create datasets manually and use for training Models. But is there any way where Azure Data Factory takes data from ADLS account and updates as Datasets in Azure ML.\n\nOnly option I see is using Azure ML Notebooks which involves writing Notebooks(which I do not want). From ADF I want this process to be done. I do not have Azure Machine Learning Studio also.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML Compute Instance stopped provisioning RStudio",
        "Question_creation_time":1656439437333,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/906921\/ml-compute-instance-stopped-provisioning-rstudio.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,\n\nJust wondering if anyone knows why Azure ML compute instance suddenly stopped provisioning RStudio?\n\n\nI have tried to set up a custom app using ghcr.io\/azure\/rocker-rstudio-ml-verse:latest, but it is not able to access the files (e.g. files that are previously accessible via the automatically provisioned RStudio, jupyter, jupyterhub, terminal etc)\n\nWould be great if you could provide any guidance etc.\nthanks a lot.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there any kinect version that helps in speech therapy?",
        "Question_creation_time":1606122417867,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171998\/is-there-any-kinect-version-that-helps-in-speech-t.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\nI am an educator\/researcher who is working on a project or a thesis for my degree.\nI am thinking of using Kinect to recognize the lips\/ mouth movements to train speech delay kids. My idea is to show students audio-visual 3d mouth, tongue, and throat movements on the Kinect to train them to speak a letter or word by interacting with the camera and evaluating their sounds and movements by the Kinect. At the same time, the Kinect camera will configure if the student's sound and mouth movement are correct.\n\nI am perplexed about which tag I should use for my question to be best answered.\n\nI really appreciated it if there are studies or experiments on this issue to let me know.\n\nRegards",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Given allow_reuse set to false and regenerate_outputs set to True when the pipeline is submitted then it stucks at the running stage with first step saying \"Not Started\"",
        "Question_creation_time":1657676507390,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/924406\/given-allow-reuse-set-to-false-and-regenerate-outp.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI am using Azure Machine Learning SDK in python to create a pipeline which needs to read data from Azure SQL Database, perform transformation, model the data as per need and store the output back to Azure SQL Database. In this scenario, I need to run the published pipeline every time(without reusing output from previous run) because underlying data changes. To resolve this problem I set allow_reuse flag to False in PythonScriptStep(). Also, I set regenerate_outputs=True while submitting the pipeline. Following is the code:\n\nfrom azureml.pipeline.steps import PythonScriptStep\ndataprep_source_dir = \".\/\"\nentry_point = \"Fetch_Data.py\"\ndata_fetch_step = PythonScriptStep(\nname=\"Fetch step\",\nscript_name=entry_point,\nsource_directory=dataprep_source_dir,\narguments=[\"--fetched-data\", fetched_data_folder],\noutputs=[fetched_data_folder],\ncompute_target=target_compute,\nrunconfig=aml_run_config,\nallow_reuse=False\n)\n\npipeline_run = Experiment(workspace, 'exp_name').submit(pipeline1, regenerate_outputs=True)\n\nIt was working fine until last month and every time pipeline was generating outputs which I intend to (not using result from previous run) but this week it started to give me another weird problem. When I am submitting the pipeline first time, I see the first step is \"Not Started\" saying that rerun will be used (which it should not as allow_reuse set to false) and weirdly the rerun id of that step and current runId is same. So finally nothing happens and pipeline stays in running stage for like 12 hrs until I cancel it.\n\n\n\n\n\n\n\n\nPlease help me fix this issue. It is very weird that I can't submit pipeline where I don't want to reuse previous job run results.\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run experiment crashes when using a pre-build Docker image as environment",
        "Question_creation_time":1654697991800,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/881670\/run-experiment-crashes-when-using-a-pre-build-dock.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I duplicated my question because I have not received a proper answer, yet.\nThe reason why I duplicated the question is that I need to implement something within Azure for a customer where I need to use a pre-build Docker image as an environment.\nUnfortunately, it is not working because AMLS cannot download the pre-build Docker image when you need credentials for downloading the pre-build Docker image.\nIn my opinion, the credentials for using the pre-build Docker image are not saved correctly in Azure Machine Learning Studio. This is the reason why AMLS cannot download the pre-build Docker image. I will be very grateful if someone can test the code snippets below and give me feedback if they worked or not.\n\nHere is the backstory:\nMy co-workers are using pre-build docker images for our developing environment in Azure Machine Learning Service.\nIn a separate script, they have registered these environments with the command myenv.register(workspace=ws). In another script, I should use their environment for testing our model.\n\nIn order to get one of their environments, I use the command registered_env = Environment.get(ws, 'the-specific-environment-name')\n\nUnfortunately, this does not work when I use registered_env for the experiment. I get the error \"Authentication failed for container registry name_of_their_container_registry.azurecr.io\". The experiment run works perfectly when I copy their environment definition code into my script instead of using the command registered_env = Environment.get(ws, 'the-specific-environment-name').\n\nHowever, I cannot copy every time their environment definition code into my script.\nHow can I get the environment into my script which has been defined in another script?\n\nThis StackOverFlow post is quite related to my problem:\nhttps:\/\/stackoverflow.com\/questions\/71131403\/registering-and-getting-an-environment-in-azure-machine-learning-studio-that-der\n\n\n\n\n\nTo illustrate what my problem is, here are some code samples.\n\nThis code sample is working:\n\n registry = ContainerRegistry()\n registry.address = <DockerRegistryAddress>\n registry.username = <UserName>\n registry.password = <Password>\n exemplarily_env_docker_image = Environment.from_docker_image('exemplarily-env_Docker-image-AzureRegistry', <DockerImageAddress>, container_registry=registry, conda_specification=None, pip_requirements=None)\n    \n exemplarily_env_docker_image.python.user_managed_dependencies = True\n    \n # Registering and getting of an environment that derives from a Docker Image is not working because the credentials are not saved\n exemplarily_env_docker_image.register(workspace=ws)\n model = Model(ws, 'exemplarily_model')\n    \n inference_config = InferenceConfig(environment=exemplarily_env_docker_image, \n                                    source_directory='.\/source_dir', \n                                    entry_script='.\/score.py') \n deployment_config = LocalWebservice.deploy_configuration(port=6789)\n    \n service = Model.deploy(\n     ws,\n     \"myservice\",\n     [model],\n     inference_config,\n     deployment_config,\n     overwrite=True,\n )\n    \n service.wait_for_deployment(show_output=True)\n print(service.get_logs())\n\n\n\n\nNow, I do a small change and the code sample is not working anymore:\n\n registry = ContainerRegistry()\n registry.address = <DockerRegistryAddress>\n registry.username = <UserName>\n registry.password = <Password>\n exemplarily_env_docker_image = Environment.from_docker_image('exemplarily-env_Docker-image-AzureRegistry', <DockerImageAddress>, container_registry=registry, conda_specification=None, pip_requirements=None)\n    \n exemplarily_env_docker_image.python.user_managed_dependencies = True\n # Registering and getting of an environment that derives from a Docker Image is not working because the credentials are not saved\n exemplarily_env_docker_image.register(workspace=ws)\n model = Model(ws, 'exemplarily_model')\n    \n reg_env = Environment.get(ws, \"exemplarily-env_Docker-image-AzureRegistry\")\n inference_config = InferenceConfig(environment=reg_env, \n                                    source_directory='.\/source_dir', \n                                    entry_script='.\/score.py') \n    \n deployment_config = LocalWebservice.deploy_configuration(port=6789)\n    \n service = Model.deploy(\n     ws,\n     \"myservice\",\n     [model],\n     inference_config,\n     deployment_config,\n     overwrite=True,\n )\n    \n service.wait_for_deployment(show_output=True)\n print(service.get_logs())\n\n\n\n\nWhat is working:\n\n registry = ContainerRegistry()\n registry.address = <DockerRegistryAddress>\n registry.username = <UserName>\n registry.password = <Password>\n exemplarily_env_docker_image = Environment.from_docker_image('exemplarily-env_Docker-image-AzureRegistry', <DockerImageAddress>, container_registry=registry, conda_specification=None, pip_requirements=None)\n    \n exemplarily_env_docker_image.python.user_managed_dependencies = True\n # Registering and getting of an environment that derives from a Docker Image is not working because the credentials are not saved\n exemplarily_env_docker_image.save_to_directory(path=\".\/env\", overwrite=True)\n model = Model(ws, 'exemplarily_model')\n    \n reg_env = Environment.load_from_directory(path=\".\/env\")\n inference_config = InferenceConfig(environment=reg_env, \n                                    source_directory='.\/source_dir', \n                                    entry_script='.\/score.py') \n    \n deployment_config = LocalWebservice.deploy_configuration(port=6789)\n    \n service = Model.deploy(\n     ws,\n     \"myservice\",\n     [model],\n     inference_config,\n     deployment_config,\n     overwrite=True,\n )\n    \n service.wait_for_deployment(show_output=True)\n print(service.get_logs())\n\n\n\n\nWhy is the middle code sample not working? Is this a bug?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"The columns appears with another name",
        "Question_creation_time":1624312518567,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/445579\/the-columns-appears-with-another-name.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello I'm practicing in Microsoft Machine Learning Studio.\n\nIn a experiment I use de control \"Import data\", then in the properties I use the data source: https:\/\/github.com\/rdiazconcha\/lil-azure-machine-learning-y-ai\/blob\/master\/modulo-2\/power-export_min.csv\n\nThat is the file to practice in my course.\n\nThe resto of the fields are filled like this:\n\n\n\n\nBut, when I use the choice visualize, appears like this:\n\n\n\n\n\nBut, those are not the names of the columns.\nWhat I'm doing wrong?\n\nThanks a lot for your help.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Suggest solution for reading data from Azure Service Bus with Azure Data Factory",
        "Question_creation_time":1623054369990,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/424698\/suggest-solution-for-reading-data-from-azure-servi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI need a suggestion about the below scenario.\nI receive data every second on Azure Service Bus. Now, I want Azure Data Factory to fetch this data and run the ML model on data.\nAs I checked there isn't a link between Azure Service Bus and Azure Data Factory.\nWhat is the solution for this scenario?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to change MS Learn Display Name in Microsoft Cloud Skills Challenge?",
        "Question_creation_time":1610604860147,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/230124\/how-to-change-ms-learn-display-name-in-microsoft-c.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Yesterday I enrolled Microsoft Cloud Skills Challenge for study.\n\nBut when I enroll the challenge, I did not write proper name at MS Learn Display Name blank.\n\nSo I want to change the MS Learn Display Name.\n\nHow can I do this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use AzureMLDataset",
        "Question_creation_time":1607676200880,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/194940\/how-to-use-azuremldataset.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI have used the data labeling system within Azure Machine Learning studio to label a dataset of images.\nThen the output of the labeling system is a new dataset that can be found in the \"Dataset\" section of the designer in ML studio.\n\nThe problem is that this new dataset module, that can be dragged and dropped in the pipeline, has datatype \"AzureMLDataset\" (or datasoruce type \"amldataset\"), which then I cannot connect to any other module in the pipeline because there is no module which accepts as input something with datatype \"AzureMLDataset\" (or with datasource type \"amldataset\").\n\nI have seen that it is possible to consume the dataset using python, but I would like to use Azure ML studio because it is more convenient to the system I am working in.\n\nHow can I use the AzureMLDataset output module inside ML studio?\n\nThank you in advance!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Non-interactive login to registered dataset",
        "Question_creation_time":1595346764937,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/50386\/non-interactive-login-to-registered-dataset.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to tune hyperparameters similar to the following guide: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\n\nMy PyTorch Dataset in train.py contains:\n\n ws = Workspace.from_config()\n ds = Dataset.get_by_name(ws, 'train')\n df = ds.to_pandas_dataframe()\n\nThis code works fine when run from the command-line, but when I submit a hyperparam tuning job to each node of a training cluster, I get the following error:\n\nWe could not find config.json in: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/adamml\/azureml\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3\/mounts\/workspaceblobstore\/azureml\/HD_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n\nIf I manually pass my subscription id, resource group, and workspace id, I don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. Is there a way to do a non-interactive login?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":36.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create deploy a Deep learning model as Function app",
        "Question_creation_time":1638553622740,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/651132\/how-to-create-deploy-a-deep-learning-model-as-func.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a DL model which sizes around 3.5 gigs and I'm creating a HTTP trigger to hit the model, all works fine when I test it in my local machine. When I start deploying the model as Function App into Azure, deployment breaks midway.\n\nAlso how can I link a gpu compute to the function app for the model to run.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Program on VM automatically crash after long idle",
        "Question_creation_time":1615873089030,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/315909\/program-on-vm-automatically-crash-after-long-idle.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI am training machine learning model on Azure VM with NC6 promo GPU. Everything was fine at the beginning, but after a while I went back to check and realized my training program was stopped. Also, I got this message \"client_loop: send disconnect: Broken pipe\". Is there any solution for this problem since it cost me a lot of time and money.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Connecting to an existing Databricks Cluster in AMLS",
        "Question_creation_time":1654614267930,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/880189\/connecting-to-an-existing-databricks-cluster-in-am.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nwe have also found this example of using Databricks as a Compute Target for an Azure Machine Learning Pipeline.\n\nHowever, we want to use an existing Databricks Cluster as compute target within Azure Machine Learning Studio for our Azure Machine Learning Pipeline.\nCould you help us in accomplishing this, please?\n\n\n\n\nWith best regards\nAlex",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to export trained azure ml model to production environment",
        "Question_creation_time":1656249928277,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/903600\/how-to-export-trained-azure-ml-model-to-production.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"How we can copying trained azure ml model from dev environment to production. Its possible to use trained model from one resource group to another resource group with same trained data.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how publish the pipeline endpoint and test it ?",
        "Question_creation_time":1649315875193,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/803725\/how-publish-the-pipeline-endpoint-and-test-it.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"we have build a pipeline and would like to publish as service, may I know how to test the it works or not ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoMLException: Message: Could not find a model with valid score for metric 'accuracy'. Please ensure that at least one run was successfully completed with a valid score for the given metric.",
        "Question_creation_time":1651063933880,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/828516\/automlexception-message-could-not-find-a-model-wit.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I am trying to make a classification using automl service , I've chosen KNN as model to train and for the primary metric i used accuracy . Based on the automl documentation the metric accuracy can be used with this type of classification task, but I get the error : AutoMLException: Message: Could not find a model with valid score for metric 'accuracy'. Please ensure that at least one run was successfully completed with a valid score for the given metric. when I checked azure ml studio I find this error of the run is that means that the dataset that i am using can be trained on KNN model ? ![197051-image.png][1] [1]: \/answers\/storage\/attachments\/197051-image.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"dataset.to_pandas_dataframe() throws a DatabaseConnectionException",
        "Question_creation_time":1646912554290,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/767054\/datasetto-pandas-dataframe-throws-a-databaseconnec.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI try to access a Azure ML Dataset in python and get the following error message:\n\n Execution failed in operation 'to_pandas_dataframe' for Dataset(id='[data.id]', name='[data.name]', version=1, error_code=ScriptExecution.DatabaseConnection.Unexpected,error_message=ScriptExecutionException was caused by DatabaseConnectionException.\\n  DatabaseConnectionException was caused by UnexpectedException.\\n    'MSSQL' encountered unexpected exception of type 'AggregateException' with HResult 'x80131500' while opening connection to server ([REDACTED]), database ([REDACTED]).\\n      Failed due to inner exception of type: AggregateException\\n| session_id=[session-id]) ErrorCode: ScriptExecution.DatabaseConnection.Unexpected\n\n\n\nIt is a Tabular Dataset created from a Azure SQL database datasource.\nFor the access from the studio to the database a service principal is used.\nI can access all the resources mentioned above in the standard ui.\n\nThe \"sample usage\" in the \"consume\" tab of the dataset was used for accessing the dataset in python.\nRegarding environments i tried python 3.6 and 3.8 locally and in a compute instance of the ML studio.\nHowever the same error keeps coming.\n\nI also tried to use sync-keys as described in this question:\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/644562\/datasetto-pandas-dataframe-throws-a-scriptexecutio.html\n\n\n\n\nBest Regards,\nGerhard",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to update azure ml webservice",
        "Question_creation_time":1601290286423,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/109836\/how-to-update-azure-ml-webservice.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to update azure ml webservice which already deployed with new scoring file . Can you please help me in this , i follow your instruction from https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service but i am getting error as AttributeError: 'NoneType' object has no attribute 'lower' (https:\/\/stackoverflow.com\/questions\/63763564\/how-to-update-scoring-py-file-in-deployed-azure-ml-web-services-without-changing\/64095971#64095971 ) and i am not able to solve it , can you please help me .?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Prediction of Cancer",
        "Question_creation_time":1600172964937,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/95802\/prediction-of-cancer.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have made a prediction algorithm in which I have predicted whether a patient has cancer or not based on the past data. I have also run the model successfully and have received the parameters. Now my question is, which parameter should I give the most importance for this case of prediction? Is it the precision, recall, accuracy or the threshold?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Anaconda commercial use on Azure Machine Learning",
        "Question_creation_time":1605597154997,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Anaconda announced that commercial users should purchase the licenses on APR, 20, 2020.\nHowever, Azure Machine Learning heavily depends on this anaconda packages; developing models on computing instance and deploy container environment.\n\nDo commercial developers have to pay to anaconda to continue usage of Azure Machine Learning with anaconda?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model Explanation Run Error: Object of type 'Timestamp' is not JSON serializable",
        "Question_creation_time":1618588503547,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/360712\/model-explanation-run-error-object-of-type-39times.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"All my model explanation run have following error. (For different data, algorithm, model,...)\n\n\n\n\nEncountered an internal AutoML error. Error Message\/Code: ClientException. Additional Info: ClientException:\nMessage: Object of type 'Timestamp' is not JSON serializable\nInnerException: None\nErrorResponse\n{\n\"error\": {\n\"message\": \"Object of type 'Timestamp' is not JSON serializable\"\n}\n}\n\nI would appreciate if you could help me to solve",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ray+dask native support be added to Azure Machine Learning",
        "Question_creation_time":1666227934133,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1055350\/raydask-native-support-be-added-to-azure-machine-l.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"While distributed dask can be setup manually on AML compute, the process requires lot of configs to be maintained. Is there any native support.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning Studio environment fails to build with context",
        "Question_creation_time":1654178870087,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/874583\/azure-machine-learning-studio-environment-fails-to.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to build an environment for ML Studio which requires some private compiled binaries. I packed these files into a Tar (about 2.8 MB, if it's relevant) and added them to the context tab of an existing environment (I want to update it). I then added this Tar in my Dockerfile and built the environment, but it failed instantly. The log tab also doesn't open (it just sits loading).\n\nI tried changing things around to eliminate as many variables as possible:\n\nInstead of using ADD I tried COPY: No change\n\n\nI tested building the Dockerfile locally in a folder containing only the Dockerfile and the tar: It worked\n\n\nI tested clicking the \"Download Content\" button and building the image in the downloaded folder: It worked\n\n\nI tried removing all references to the Tar from the Dockerfile but still uploading the file: It failed instantly\n\n\nI tried uploading an equivalent Zip file: It failed in the same way\n\n\nI uploaded a single text file instead of the Tar: It worked\n\nHow can I get this to work?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Question: distinction between Modules and Models in Designer",
        "Question_creation_time":1594756919963,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/46991\/question-distinction-between-modules-and-models-in.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"In Azure ML, under designer, there are 3 categories:\n\nDatasets\n\n\nModules\n\n\nModels\n\nDatasets are pretty straightforward, but I don't understand the distinction between Modules and Models. As an ML researcher, when I think of a \"model\", I think of something like linear regression or SVM. However, those are listed under Modules -> Machine Learning Algorithms. So what exactly qualifies as a Model?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":38.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"One tenant ID ( Root inheriter ) and another subsction ID creating problem for Azure ML-SDK",
        "Question_creation_time":1642202294597,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/696233\/one-tenant-id-root-inheriter-and-another-subsction.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello everyone,\n\nMy laptop is borrowed from University Tech Department since I am a GTA there. Though I opened my own account taking Microsoft Azure subscription ( one month free ) when I'm trying to create a workspace and other related stuff using AzureML-SDK it sends me the following error message:\n\n\"\"\nMessage: You are currently logged-in to 762ebf40-80b2-40ba tenant. You don't have access to <74595002-4d5f-4c26-871c-> subscription, please check if it is in this tenant.\nAll the subscriptions that you have access to in this tenant are =\n[SubscriptionInfo(subscription_name='Azure subscription 1', subscription_id='74595002-4d5f-4c****')].\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azure ml-SDK.\n\"\"\n\n\n\n\nSince it was owned by the tech department I believe it has access to the administration's azure subscription - which I can't find a way to get around to have access of my subscription to use AzureML-SDK.\n\nThe last user ( in the photo ) is from the tech department - who is the administrative user of this laptop. Could the knowledgeable admins\/members kindly suggest what can I do to keep using azure using my own subscription? Any kind suggestion is much appreciated.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why explanation dashboard is showing 2 tabs with duplicate information in Azure ML Studio?",
        "Question_creation_time":1669620849057,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1106407\/why-explanation-dashboard-is-showing-2-tabs-with-d.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When using explanations for AutoML models or standalone model, the explanation dashboard has 2 tabs which displays same information.\n\nI am using azureml-interpret to explain the models that are executed under azure context and upload the explanations into Azure ML studio.\nI use global_explanation and local_explanation to explain the overall model performance and local model performance.\n\nI guess this is creating 2 tabs if I am correct, but both of them seems to have same or duplicate information. I don't understand what is the need for that?\n\nThis seem to the case when I use AutoML models also, there is 2 tabs which has same information. Note, here I am not uploading anything, it is by default uploading the model explanations and I am using azure-python-sdk-v1.\n\nI have provided the accompanying screenshots with the information, please let me know if there is gap in my understanding or it is problem with the azure explanation?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML compute not able to access Azure MLS workspace blob( not in vnet) during automl experiment execution",
        "Question_creation_time":1631627630167,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/551634\/azure-ml-compute-not-able-to-access-azure-mls-work.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Team,\n\nI'm trying to run the automl code from the examples (https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/automated-machine-learning\/regression-explanation-featurization) in Azure MLS which is not in virtual network. While running the experiment, it is getting failed with the below error.\n\nAzureMLCompute job failed.\nBFSMountError: Unable to mount blob fuse file system\nInfo: Could not mount Azure Blob Container azureml-blobstore-xxxx at workspaceblobstore: Unauthorized. Cannot access the storage account with the given account key. Please verify that the account key is valid.\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.4 with err exit status 1.\n\nNot sure why the AzureML is not able to access its own blobstorage to place the model artifacts.\nThe AzureML and the workspace blob both are not in virtual network.\n\nWorkarounds tried:\n1) Tried to register the workspace blob container (azureml-blobstore-<ID>) as per the link here (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data), but still getting the same error.\n\nNote: The workspace blob storage keys are synced and can able to access the notebooks and data in AzureML, Is this causing the issue?\n\nAs per the ticket :- https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/35043\/azure-machine-learning-resync-keys-not-working-no.html\n\nAre the storage keys cached in the storage connection strings at the backend ? however the error message is different, in the reference ticket it says not able to access the resource, but in my case it is not able mount to the azure-ml-<ID> container.\n\nCould you please help on it.\n\nThanks in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hyperparamter optimization in Azure AutoML",
        "Question_creation_time":1625435556057,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/462352\/hyperparamter-optimization-in-azure-automl.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Which type of hyperparameter optimization is used in Azure Automated Machine Learning (not the SDK) as default? Grid Search, Random Search, Bayesian? In the SDK you can specify that but in the AutoML section you can not specify that and there is no further information on that",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is There a Way to Visualize the Decision Tree AML Used?",
        "Question_creation_time":1612898717163,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/265984\/is-there-a-way-to-visualize-the-decision-tree-aml.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have used a Two-Class Boosted Decision Tree in Azure ML to make some predictions on data that I am analyzing. Once the model has completed training is there a way for me to visualize the structure of the decision tree that was ultimately used by Azure?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Delimeter error while adding new dataset in ml service",
        "Question_creation_time":1602240911150,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/121529\/delimeter-error-while-adding-new-dataset-in-ml-ser.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\nI have problem with adding new dataset in ml service. After upload i'm getting below error (screen)\n\n\n\n\n\nI think is a new issue, because few days ago everything was good.\nIn my csv file is semicolon delimeter.\n\nThank you in advance!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to make use of Labelled Images in AzureML Designer?",
        "Question_creation_time":1628873927937,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/513343\/how-to-make-use-of-labelled-images-in-azureml-desi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello! I have been labeling images in AzureML Data Labeling. I wish to use this labelled set in Designer, to prototype some model ideas. However, I cannot get this to work. Any output (Dataset, COCO or csv) seems not to be compatible with \"Convert to Image Directory\".\n\nMy question is quite similar to one asked over a year ago - https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/32203\/how-to-use-labeled-image-datasets-to-perform-an-im.html - the answer suggests a result was imminent. Has there been any update?\n\nIf there is not one individual module capable of parsing this information, is there a way to use multiple modules to import the data?\n\nThanks!\n\n\n\n\nEDIT: The problem is also discussed here:\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/194940\/how-to-use-azuremldataset.html\nIs there a cleaner solution yet? Or is downloading it, converting to pandas, then reuploading the best thing to do?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Load the most recent data from Date partitioned folder",
        "Question_creation_time":1646726960863,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/763313\/load-the-most-recent-data-from-date-partitioned-fo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI have set up a pipeline with Azure Data Factory in order to move data from my on-premises Oracle DB to parquet files in an Azure blob container every 10 days.\nThe folder structure of my blob container is as follows:\n\nonpremises\/2022\/02\/18\/file1.parquet\nonpremises\/2022\/02\/18\/file2.parquet\nonpremises\/2022\/02\/18\/file3.parquet\n\n\nonpremises\/2022\/02\/28\/file1.parquet\nonpremises\/2022\/02\/28\/file2.parquet\nonpremises\/2022\/02\/28\/file3.parquet\n\n\nonpremises\/2022\/03\/08\/file1.parquet\nonpremises\/2022\/03\/08\/file2.parquet\nonpremises\/2022\/03\/08\/file3.parquet\n\n\n...\n\nNow I'm trying to set up a pipeline in Azure ML which will run every time new data is coming into this container.\nIn my script below, I start by getting a reference to my container before calling the function 'from_parquet_files' to read from Parquet files.\nProblem: the script reads all files from every folder and adds a data column to the dataset (I believe it is because of the parameter 'partition_format').\n\n from azureml.core import Workspace, Datastore\n    \n # Get a reference to the workspace\n ws = Workspace.from_config()\n    \n # Reference to the datastore 'onpremises' from which we will contruct our dataset\n data_store = Datastore(ws, \"onpremises\")\n    \n from azureml.core import Dataset\n # Create a dataset from the data stored in datastore 'onpremises' at the specified path\n specs_dataset = Dataset.Tabular.from_parquet_files(path=(data_store, ''), partition_format='\/{PartitionDate:yyyy\/MM\/dd}\/')\n    \n # Register the dataset to the workspace. Increments the version if dataset already exists.\n specs_dataset.register(workspace=ws, name=\"specs\", description=\"Specs data from on-premises\", create_new_version=True)\n\n\n\nWhat I would like to do is to read only the most recent set of files (in my case, files listed under 'onpremises\/2022\/03\/08\/').\nAs the pipeline will run automatically, it should detect what is the most recent data among the folder structure.\nIs there a simple way to achieve this programmatically?\n\nThanks in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"InternalServerError when launching Jupyterlabs in Azure Machine Learning workspace",
        "Question_creation_time":1632877552993,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569900\/internalservererror-when-launching-jupyterlabs-in.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My workspace is in EastUS2.\nI have created compute instances from multiple sku's and I always receive the following error when creating instances:\n\n{\n\"error\": {\n\"code\": \"ServiceError\",\n\"severity\": null,\n\"message\": \"InternalServerError\",\n\"messageFormat\": null,\n\"messageParameters\": null,\n\"referenceCode\": null,\n\"detailsUri\": null,\n\"target\": null,\n\"details\": [],\n\"innerError\": null,\n\"debugInfo\": null,\n\"additionalInfo\": null\n},\n\"correlation\": {\n\"operation\": \"f0bc2b1a27a3534eb83eac4f3f71fedf\",\n\"request\": \"589656f8c89b684a\"\n},\n\"environment\": \"eastus2\",\n\"location\": \"eastus2\",\n\"time\": \"2021-09-29T01:01:09.3745269+00:00\",\n\"componentName\": \"notebook-instance-proxy\"\n}\n\nWhat can I do to resolve this issue? I have tried restarting, recreating, and testing other sku's.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to do machine learning in Azure IoT Central?",
        "Question_creation_time":1618360709993,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/356183\/is-it-possible-to-do-machine-learning-in-azure-iot.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I was wondering if it is possible to do machine learning on Azure IoT central. I read in some places that it is possible to do so in Azure IoT Edge. I saw a template for Video Analytics but cannot seem to find a way to implement my own models. If Edge is the only way to perform machine learning in Azure IoT, is there some way to use IoT Edge with IoT Central? Or, is it possible to train your own Tensorflow Lite Models with Raspberry Pi and just host the Pi in IoT Hub? If both are possible, which of the two would be the easiest?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML service and SAS files",
        "Question_creation_time":1667406090587,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1072904\/azure-ml-service-and-sas-files.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Am not able to preview the sas7bdat files in Azure ML service.. whether sas7bdat files analysis are supported in Azure ML services ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why is the different components not grouped in the designer any more?",
        "Question_creation_time":1661248060107,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/978187\/why-is-the-different-components-not-grouped-in-the.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, Guys,\nI'm new to Azure Ml, and when i started using the Designer, the different components where grouped.\nWhen you chose for example \"model scoring and evaluation\", and then all components related to that topic was there.\nNow it's like all components available is just listed in alphabetic order.\nWhat is the point with that change? and can i get the grouped components back?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy ML model to Kubernetes + overwrite previous endpoint",
        "Question_creation_time":1598425965377,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/77362\/deploy-ml-model-to-kubernetes-overwrite-previous-e.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm building a CI\/CD pipeline in Azure DevOps for the deployment of my Machine Learning model to Azure Kubernetes Service. I have the following task in my YAML pipeline file (replaced some of the values with '...'):\n\n - task: AzureCLI@1\n   displayName: \"Deploy to AKS\"\n   inputs:\n     azureSubscription: '...'\n     scriptLocation: inlineScript\n     workingDirectory: $(Build.SourcesDirectory)\/score\n     inlineScript: |\n       set -e # fail on error\n             \n       az ml model deploy --name 'aks-deploy-test' --model '$(MODEL_NAME):$(get_model.MODEL_VERSION)' \\\n       --compute-target $(AKS_COMPUTE_NAME) \\\n       --ic inference_config.yml \\\n       --dc deployment_config_aks.yml \\\n       -g ... --workspace-name ... \\\n       --overwrite -v\n\n\n\nWhen I run the pipeline the first time, it successfully deployed the ML model and I can see the Endpoint in the Azure ML workspace. However, when I try to run the pipeline a second time (to deploy a newer version of the model), I get the error:\n\n Error:\n {\n   \"code\": \"KubernetesError\",\n   \"statusCode\": 400,\n   \"message\": \"Kubernetes Deployment Error\",\n   \"details\": [\n     {\n       \"code\": \"Unschedulable\",\n       \"message\": \"0\/6 nodes are available: 4 Insufficient cpu, 6 Insufficient memory.\"\n     },\n     {\n       \"code\": \"DeploymentFailed\",\n       \"message\": \"Couldn't schedule because the kubernetes cluster didn't have available resources after trying for 00:05:00.\\nYou can address this error by either adding more nodes, changing the SKU of your nodes or changing the resource requirements of your service.\\nPlease refer to https:\/\/aka.ms\/debugimage#container-cannot-be-scheduled for more information.\"\n     }\n   ]\n }\n\n\n\nIsn't the --overwrite option in the az ml model deploy command supposed to completely overwrite the current deployment of the model? If so, why am I still getting this error, or is there a better way to deploy a newer version of the ML model to the same AKS cluster?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trained NLP model snippet",
        "Question_creation_time":1667251332713,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1069986\/trained-nlp-model-snippet.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Newbie data scientist here, I am just starting my way in Azure, is there any I should start NLP? Any trained model or code sample? Thank you for any idea",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Save trained model from AutoML\/Designer as pickle file to disk - Azure ML",
        "Question_creation_time":1617893768667,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/349669\/save-trained-model-from-automldesigner-as-pickle-f.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI want to save trained machine learning model as pickle file(.pkl) to disk which is trained in AutoML\/Designer.\n\nPlease let me know is there any way to do that?\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Studio Failed to Authenticate to the compute",
        "Question_creation_time":1632489823347,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/565326\/azure-ml-studio-failed-to-authenticate-to-the-comp.html",
        "Question_upvote_count":4.0,
        "Question_view_count":null,
        "Question_answer_count":12,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI have created a compute in my ML Studio and was running it for hours. However, it suddenly disconnected, and when I signed back in, it shows that \"\nYou need to be authenticated to the compute to use any Azure SDK. Please use the authenticate button to get authenticated.\" But when I click the authenticate button, I got an error with the message saying \"InternalServerError\".\n\nI have tried to sign out and sign back in, delete the current compute and create a new one. Neither worked.\n\nDoes anybody have any suggestions on this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":10.0,
        "Question_follower_count":21.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Python visual in Power BI for calling ML Azure rest API works in desktop version but not when published",
        "Question_creation_time":1613593697693,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/277404\/using-python-visual-in-power-bi-for-calling-ml-azu.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I created a dashboard in Power BI desktop. I have a trained model from ML Azure which I already deployed and has it's rest API. I need to call this rest API from the dashboard itself using measures I created (not from the query editor). I did it using the Python visual to send the input data and get back the output from the rest API and plotting the result (a number). This works perfectly in the desktop version. I need to publish this dashboard to share with other members of my organization but in the web version the script gives a runtime error. ![69200-capture.png][1] How to make it work? [1]: \/answers\/storage\/attachments\/69200-capture.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I attach a managed disk to a Machine Learning Compute instance?",
        "Question_creation_time":1601637154150,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/115201\/how-can-i-attach-a-managed-disk-to-a-machine-learn.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello experts,\n\nI would like to attach a managed disk to my machine learning compute instance. Is that possible?\n\nThere is a possible overlap to the question Attach Disk to Virtual Machine, but steps doesn't seem to apply to ML compute instances.\n\nThanks in advance,",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Relational Database for Automated Machine Learning",
        "Question_creation_time":1594992643703,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/48810\/relational-database-for-automated-machine-learning.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to build a time-series Machine Learning experiment in Azure Machine Learning. However, I'm using outputs from previous functions which analyzes multiple factors using the same timestamp. For example, extracting all key phrases from customer surveys, and using it to forecast future sales. This creates a new row for each key phrase found, with all of the other survey data points and the same timestamp. This causes an error due to duplicate timestamps across multiple rows forecasting the same target value. I need to either make each timestamp\/survey on row, convert the columns to a list\/array, and have it iterate through each key phrase in that column, or use a relational database where the key phrases column is the foreign key to my table of keyphrases. Any recommendations on how to solve this? Thanks!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":34.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[SOLVED] AML Pipeline publish error: Identity(object id: __) does not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/snapshots\/write actions.",
        "Question_creation_time":1654848921463,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/884432\/aml-pipeline-publish-error-identityobject-id-does.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I get the following error while trying to publish some new pipelines to an AML workspace:\n\nSnapshotException:\nMessage: {\n\"error_details\": {\n\"componentName\": \"project\",\n\"correlation\": {\n\"operation\": \"038f38ce5375f4cda9b0a50df5bb9c1b\",\n\"request\": \"e874ef090203af05\"\n},\n\"environment\": \"eastasia\",\n\"error\": {\n\"code\": \"UserError\",\n\"innerError\": {\n\"code\": \"ForbiddenError\"\n},\n\"message\": \"Identity(object id: bb0511d8-d57a-4442-89a1-1986cac268c9) does not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/snapshots\/write actions. Please refer to https:\/\/aka.ms\/azureml-auth-troubleshooting to fix the permissions issue.\"\n},\n\"location\": \"eastasia\",\n\"time\": \"2022-06-10T08:07:13.3168371+00:00\"\n},\n\"status_code\": 403,\n\"url\": \"__\"\n}\n\nDo note that I'm publishing multiple pipelines to multiple workspaces - only 2 of the pipelines on our Asia workspace are failing with this (and 1 other on Asia completed successfully)... I see that similar issues had existed before and were internal to Azure and fixed promptly, i.e. - https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/407580\/permission-error-while-finishing-auto-ml-run.html\n\nAny idea if this is a similar thing, or did we do something wrong? Thanks!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DSVM can support SQL Server Developer Edition for Ubuntu",
        "Question_creation_time":1618980125337,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/365284\/dsvm-can-support-sql-server-developer-edition-for.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"According to this\n- https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/tools-included#store-retrieve-and-manipulate-data,\nit appears that the SQL Server Developer Edition (Ubuntu) is being supported in DSVM but I couldn\u2019t find the name in the supported list here https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-tools-data-platforms#sql-server-developer-edition,\nfurthermore there is no guide line for Linux Guide line but only windows guideline is there.\nI\u2019d like make sure the followings :\n\nCan DSVM support SQL Server Developer Edition for Ubuntu?\n\n\nIf yes, where is the guideline for this?\n\n\nIf no, the documentation is wrong? And any particular supporting plan for SQL Server Developer Edition for Ubuntu?\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Images Segmentation using Azure",
        "Question_creation_time":1610951620483,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/234232\/images-segmentation-using-azure.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\nI have been using object detection from Custom Vision to train images. Classification do not suit my goal so I'm looking at alternative methods to training images. Can I get some suggestions with using Azure for images segmentation?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Notebook files have disaperred",
        "Question_creation_time":1652875560357,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/854288\/notebook-files-have-disaperred.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nIn my MLStudio my notebook files window has disappeared so I can not access any of my data (as seen on the image) and I do not know what to do.\n\nPlease your help to solve this as soon as poosible.\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Apply SQL Transformation Error : Failed when create table error 1000",
        "Question_creation_time":1629294395873,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/518856\/apply-sql-transformation-error-failed-when-create.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I connected 2 tables to \"Apply SQL Transformation\" box and when i ran the code the following error appears (screenshots attached). csv files downloaded from web for i have shared link below.\n\nLink 1 : https:\/\/docs.google.com\/spreadsheets\/d\/1aeZllwICUG7Q_rEgtNm4zj9pVV4iJaND6uUbou0qOp8\/pub?gid=873193374&single=true&output=csv\nLink 2 : https:\/\/docs.google.com\/spreadsheets\/d\/1xnMNKqB2tCdOecXt3WWjIUxbAk5rrvbYfbJLrfzFrjc\/pub?gid=1144892773&single=true&output=csv\n\nError details : requestId = cf031a39f0684e2786d1fb4768c898ce errorComponent=Module. taskStatusCode=400. {\"Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: SQLiteQueryRunner Library library exception: Failed when create table: \",\"Exception\":{\"Library\":\"SQLiteQueryRunner Library\",\"ErrorId\":\"SQLiteCreateTableFailed\",\"ErrorCode\":\"3\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Failed when create table: \",\"Exception\":{\"ExceptionType\":\"Exception\",\"Message\":\"SQL logic error or missing database\\r\\nunrecognized token: \\\"\\\"PK\\u0003\\u0004\\u0014\\\"\"}}}}Error: Error 1000: SQLiteQueryRunner Library library exception: Failed when create table: Process exited with error code -2\n\n\n\n\nVery much appreciate it if someone could explain the way out since i am new to Azure ML.\n\nSQL query that i used is provided below\n\nselect title_year, movie_title, category, Won?\nfrom t1, t2\nwhere t1.movie_title = t2.Nominee\nand Won? = \"YES\"\norder by title_year desc",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Downloading images built during ML experiment",
        "Question_creation_time":1623337578827,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/430883\/downloading-images-built-during-ml-experiment.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have successfully run a AML experiment . In the beginning of the experiment, I included the below command to build a conda environment with my requirements into a pre built image:\n\n env = azc.Environment.from_conda_specification(name='my-env', file_path='.\/envspec.yml')\n env.docker.enabled = True\n env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n\n\n\nI understand that during the preparation phase the image is built with the specified dependencies and the experiment is run.\nNow, where I can access this built image? I tried in the Azure container Registry inside my resource group but couldnt find anything.\n\nIs there a way I can download this image and use it in a different experiment as a custom image? I assume this must save time in downloading dependencies and ensures reptroduciblity.\n\nAny known way of doing this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Response status code does not indicate success: 400 (Conda dependencies were not specified. Please make sure that all conda dependencies were specified i).",
        "Question_creation_time":1667250048510,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1069928\/response-status-code-does-not-indicate-success-400.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I was trying to setup my env runconfig = ScriptRunConfig(source_directory='script\/', script='my-script.py', arguments=script_params)\nrunconfig.run_config.target = compute_target\nrunconfig.run_config.environment = env\nrun = exp.submit(runconfig)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"IoT Edge custom machine learning module reported error status",
        "Question_creation_time":1637069073437,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/629089\/iot-edge-custom-machine-learning-module-reported-e.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI have developed a ML solution based on this tutorial.\nI adapted the Python notebook to run into an existing ML workspace.\nUnfortunately, when I tried to create Docker image, it failed with error 500. I found out that the Image class is deprecated (suppose this was the reason of the error), so I relied on the Environment class (in particular this documentation) and everything worked fine.\nAlso, testing the model as Web Service ACI endpoint works correctly, providing the result:\n['{\"machine\": {\"temperature\": 31.16469009, \"pressure\": 2.158002669}, \"ambient\": {\"temperature\": 21.17794693, \"humidity\": 25}, \"timeCreated\": \"2017-10-27T18:14:02.4911177Z\", \"anomaly\": false}']\n\n\n\n\nThe issue I need support is the following: after deploying the ML model as container to the Edge device (in this case a Ubuntu VM created using this template), the ML module reported an error:\n\nNo logs are displayed, neither through VM SSH access, nor through the Portal.\n\nThe only information that I found out is this one:\n\n\nI checked multiple times the correctness of the image URI for the ML module, and I'm pretty confident that it is, since the ACI is based on it and the Web Service test was succeded.\n\nWhat could be the problem here?\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Python code for a generalized lineare model in Azure machine learning",
        "Question_creation_time":1654699584787,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/881821\/glm-with-azure-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello guys,\nI am currently training a model to price boat types around the world using about 30,000 records of historical sales from the last 7 years. The approach is currently a linear regression in Azure ML studios.\n\nUsing 60 variables such as number of engines, year of construction, bedroom, brand etc. which have already been normalized and split into a training set and a testing set, the purchase price of a given boat is evaluated, depending on the port.\n\nNow I would like to use a generalized linear model with family gamma and the link function identity to train a better model and thus get a better price estimation.\n\nUnfortunately there is no module included in Azure machine learning for this. Has anyone ever written code for a GLM in Azure machine learning or can tell me how complex this is?\n\nI would appreciate any help and have a nice weekend!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML data labeling change polygon color",
        "Question_creation_time":1647528569083,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/776555\/azure-ml-data-labeling-change-polygon-color.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nWe are currently annotating images in a data labeling instance segmentation (polygon) project. Our images are rather blueish, which makes it difficult to use the polygon \"draw polygon region\" tool, which draws the polygon in blue.\n\nIs it possible to change the color to, for example, black?\n\nThanks and BR,\nMaite",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"why I do not have \"Open in a new Notebook\" in my menu",
        "Question_creation_time":1624421404470,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/447926\/why-i-do-not-have-34open-in-a-new-notebook34-in-my.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I tried Azure machine learning workspace and I try to use 'right click' -> 'CSV dataset' -> There is no \"Open in a new Notebook\" in the menu, even a grey one. Could someone please guide me? Thank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Facing Trouble with Evaluate Model in Azure Machine Learning Designer",
        "Question_creation_time":1662807677703,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1001826\/facing-trouble-with-evaluate-model-in-azure-machin.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Everyone,\n\nI'm trying to run a MNIST prediction model on the ML Designer similar to the Image Classification sample given. All my components are working except the final evaluate model. It shows an error:\nazureml.studio.common.error.NotScoredDatasetError: There is no score column in dataset.\n\nThe scored dataset to the evaluate model component is exactly the same as given in the sample pipeline. I don't know what this issue is.\nCould anyone help me out with the same? I'll be thankful",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":8.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Feature request: more customizability in builtin models",
        "Question_creation_time":1594758735000,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/46982\/feature-request-more-customizability-in-builtin-mo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In Azure ML Designer, I would like to customize and parametrize other aspects of the models. For example, in the NN Regression model, I would like to try various optimizers (Adam, Adagrad, SGD) and activation functions (ReLU, Sigmoid, Tanh).",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":38.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Model Deployment- POST Body Type",
        "Question_creation_time":1602526641177,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/123960\/azure-ml-model-deployment.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I had registered a CNN model on Azure ML and would like to have and endpoint API that returns the predictions based on the image it receives and I also would like to send some metadata along the image itself. So, I prefer to POST the data in form-data format. However,The official tutorials mention application-json or binary data only.\nHow can I POST data in form-data format to an API in Azure ML ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to Connect Azure MySQL server in Azure Machine Learning Studio",
        "Question_creation_time":1635536784630,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/609742\/how-to-connect-azure-mysql-server-in-azure-machine.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In Azure machine learning studio I am trying to connect an azure mysql server as a datastore. The azure mysql server was created by a colleague and I have the credentials to connect properly. However, after entering the credentials and creating a datastore, I cannot select the datastore from the dropdown in order to create a dataset. Does machine learning studio have the ability to connect to an azure mysql server?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"FileNotFoundError: [Errno 2] - Score machine learning models with PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Question_creation_time":1639738776307,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/667458\/filenotfounderror-errno-2-score-machine-learning-m.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools tutorial-score-model-predict-spark-pool\nand what-is-aml-model-uri-predict-in-serverless-apache-1.html\nI tried to used a model created with AutoML and another from designer and I am getting this error: FileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmp5xd2_hyr\/MLmodel'\n\nI added the DATA_FILE:\n\nI am getting this error (I am using Synapse):\n\n\nKind regards,\nAnaid",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML: Creating a Siamese network, I have the reference data stored in one blob storage and the user input stored in another blob storage. How can I connect the two blob storage to my batch endpoint. If not, is there a workaround?",
        "Question_creation_time":1662886746297,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1002087\/azure-ml-creating-a-siamese-network-i-have-the-ref.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Azure ML: Creating a Siamese network, I have the reference data stored in one blob storage and the user input stored in another blob storage. How can I connect the two blob storage to my batch endpoint. If not, is there a workaround? The setup looks like this.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What syntax references Pipeline parameters in the where clause of a SQL query of 'Import Data' modules in Microsoft Azure Machine Learning designer?",
        "Question_creation_time":1616787456817,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/333816\/what-syntax-references-pipeline-parameters-in-the.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have created a pipeline in Microsoft Azure Machine Learning designer. I have added a Pipeline parameter myNumber in the pipeline settings, with a valid default value, to accept the unique ID of the asset in our DB so that the pipeline can return only the asset-specific data for use as our model input. Specifically I want to reference that pipeline parameter in the where clause of the SQL query in the 'Import Data' module that connects to our Azure SQL server.\n\nI cannot find a reference in the documentation on how to do this. I have tried the methods specified for accomplishing this task in Azure Data Factory, using where RowId = @pipeline().parameters.myNumber or where RowId = @{variables('myNumber')} but the experiment fails with SqlException error code '137', variable not defined.\n\nCan you please tell me the necessary syntax to reference Pipeline parameters in the where clause of a SQL query of 'Import Data' modules of Microsoft Azure Machine Learning designer?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline stops at train model stage",
        "Question_creation_time":1642031139267,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/693063\/pipeline-stops-at-train-model-stage.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello there,\n\nWhen I am running the following steps of the pipeline, I'm getting this error at the \"train model\" stage.\n\nCan anyone explain why I'm getting this error?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"UNZIP large zip file in azure machine learning",
        "Question_creation_time":1612431352160,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/258610\/unzip-large-zip-file-in-azure-machine-learning.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a big zip file, I need to unzip it in order to use the files in my notebook\n\nI used this script in my notebook :\n\nimport os\nimport zipfile\nlocal_zip = 'Caltech101.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('Caltech101')\nzip_ref.close()\n\nit succeeded (green v in the notebook), but only the first two folders were unzipped.\nthis file is only 130 MB\nI also need to unzip 3 GB zip file",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Time series training for anomal detect",
        "Question_creation_time":1661358044013,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/980372\/time-series-training-for-anomal-detect.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have not seen any doc talking about this topic, is this supported in Microsoft Machine Learning? Is this a good plan if anyone has tried?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Key for ML Endpoint",
        "Question_creation_time":1648186512237,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/787004\/key-for-ml-endpoint.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI am new to Azure.\nI have created ML endpoint and when I try to access, I see a message \"Unauthorized, no token matched\".\nI know its Key based Auth.\nwhere can I find this key? can someone help me?\n\nregards,\n\nRohan",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML deployment fails",
        "Question_creation_time":1644780118307,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/733521\/azure-ml-deployment-fails.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to deploy an ML model as a WebService using Azure ML Endpoints, but it fails. Below is the code and the error I get. I'm using azureml-core SDK for this.\n\n\n\n\nI'm using this link as a reference for my deployment:\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AZ ML Designer. Swagger file missing on deployment. Test empty.",
        "Question_creation_time":1657708598787,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/925083\/az-ml-designer-swagger-file-missing-on-deployment.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Guys,\nAm trying to deploy a very small logistic regression model endpoint using AZML Designer. But I cant get the \"Test\" to show as I expect it to . I think I have narrowed it down to the same issue as the others on this thread. Please find attached the images and logs\n1.This is the inference pipeline\n\n\n\n\n\n2.This is the deployed endpoint\n\n\n\n\n\n3.The test section of the endpoint\n\n\nSwagger missing info from the logs\n\n\n\n\n\n5.Expected test section\n\n\nEntire deployment log dump below signature\n\nregards\nSharath\n\n\n\n\n\n 2022-07-13T09:30:23,130461474+00:00 - iot-server\/run \n 2022-07-13T09:30:23,157246653+00:00 - rsyslog\/run \n 2022-07-13T09:30:23,159450135+00:00 - nginx\/run \n 2022-07-13T09:30:23,197683320+00:00 - gunicorn\/run \n 2022-07-13T09:30:23,199090409+00:00 | gunicorn\/run | \n 2022-07-13T09:30:23,227403276+00:00 | gunicorn\/run | ###############################################\n 2022-07-13T09:30:23,257497428+00:00 | gunicorn\/run | AzureML Container Runtime Information\n 2022-07-13T09:30:23,267026449+00:00 | gunicorn\/run | ###############################################\n 2022-07-13T09:30:23,268411738+00:00 | gunicorn\/run | \n 2022-07-13T09:30:23,276054375+00:00 | gunicorn\/run | \n 2022-07-13T09:30:23,278626954+00:00 | gunicorn\/run | AzureML image information: openmpi3.1.2-ubuntu18.04, Materializaton Build:20220708.v2\n 2022-07-13T09:30:23,286217091+00:00 | gunicorn\/run | \n 2022-07-13T09:30:23,287579180+00:00 | gunicorn\/run | \n 2022-07-13T09:30:23,295279917+00:00 | gunicorn\/run | PATH environment variable: \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\n 2022-07-13T09:30:23,296728705+00:00 | gunicorn\/run | PYTHONPATH environment variable: \n 2022-07-13T09:30:23,298062894+00:00 | gunicorn\/run | \n 2022-07-13T09:30:23,306503424+00:00 | gunicorn\/run | Pip Dependencies (before dynamic installation)\n    \n EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n 2022-07-13T09:30:23,602722185+00:00 - iot-server\/finish 1 0\n 2022-07-13T09:30:23,604481070+00:00 - Exit code 1 is normal. Not restarting iot-server.\n adal==1.2.7\n applicationinsights==0.11.10\n attrs==21.4.0\n azure-common==1.1.28\n azure-core==1.24.2\n azure-graphrbac==0.61.1\n azure-identity==1.10.0\n azure-mgmt-authorization==0.61.0\n azure-mgmt-containerregistry==10.0.0\n azure-mgmt-core==1.3.1\n azure-mgmt-keyvault==9.3.0\n azure-mgmt-resource==13.0.0\n azure-mgmt-storage==11.2.0\n azure-storage-blob==1.5.0\n azure-storage-common==1.4.2\n azureml-core==1.36.0.post2\n azureml-dataprep==2.24.4\n azureml-dataprep-native==38.0.0\n azureml-dataprep-rslex==2.0.3\n azureml-dataset-runtime==1.36.0\n azureml-defaults==1.36.0\n azureml-designer-classic-modules==0.0.161\n azureml-designer-core==0.0.68\n azureml-designer-internal==0.0.56\n azureml-inference-server-http==0.4.13\n azureml-interpret==1.36.0\n azureml-model-management-sdk==1.0.1b6.post1\n azureml-pipeline-core==1.36.0\n azureml-telemetry==1.36.0\n backports.tempfile==1.0\n backports.weakref==1.0.post1\n blis==0.2.4\n cachetools==4.2.4\n certifi==2022.6.15\n cffi==1.12.3\n chardet==3.0.4\n charset-normalizer==2.0.12\n click==7.1.2\n cloudpickle==2.1.0\n configparser==3.7.4\n contextlib2==21.6.0\n contextvars==2.4\n cryptography==37.0.4\n cycler==0.11.0\n cymem==2.0.6\n dill==0.3.4\n distro==1.4.0\n docker==5.0.3\n dotnetcore2==3.1.23\n en-core-web-sm @ https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.1.0\/en_core_web_sm-2.1.0.tar.gz\n Flask==1.0.3\n fusepy==3.0.1\n gensim==3.8.3\n google-api-core==2.8.2\n google-auth==2.9.0\n googleapis-common-protos==1.56.3\n gunicorn==20.1.0\n idna==3.3\n imbalanced-learn==0.4.3\n immutables==0.18\n importlib-metadata==4.8.3\n importlib-resources==5.4.0\n inference-schema==1.3.0\n interpret-community==0.21.0\n interpret-core==0.2.6\n isodate==0.6.1\n itsdangerous==1.1.0\n jeepney==0.7.1\n Jinja2==3.0.3\n jmespath==0.10.0\n joblib==0.14.0\n json-logging-py==0.2\n jsonpickle==2.2.0\n jsonschema==3.0.1\n kiwisolver==1.3.1\n liac-arff==2.5.0\n lightgbm==3.2.1\n llvmlite==0.36.0\n MarkupSafe==2.0.1\n matplotlib==3.1.3\n more-itertools==6.0.0\n msal==1.18.0\n msal-extensions==1.0.0\n msrest==0.7.1\n msrestazure==0.6.4\n murmurhash==1.0.7\n ndg-httpsclient==0.5.1\n nimbusml==1.6.1\n numba==0.53.1\n numpy @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/numpy_1626681920064\/work\n oauthlib==3.2.0\n opencensus==0.10.0\n opencensus-context==0.1.2\n opencensus-ext-azure==1.1.5\n packaging==21.3\n pandas==1.0.4\n pathspec==0.9.0\n Pillow==8.3.2\n plac==0.9.6\n portalocker==2.5.1\n preshed==2.0.1\n protobuf==3.19.4\n psutil==5.9.1\n pyarrow==0.16.0\n pyasn1==0.4.8\n pyasn1-modules==0.2.8\n pycparser==2.21\n pycryptodomex==3.7.3\n PyJWT==2.4.0\n pyOpenSSL==20.0.1\n pyparsing==3.0.9\n pyrsistent==0.18.0\n python-dateutil==2.8.2\n pytz==2022.1\n requests==2.27.1\n requests-oauthlib==1.3.1\n rsa==4.8\n ruamel.yaml==0.16.10\n ruamel.yaml.clib==0.2.6\n scikit-learn==0.22.2\n scikit-surprise==1.0.6\n scipy==1.4.1\n seaborn==0.10.0\n SecretStorage==3.3.2\n shap==0.39.0\n six @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/six_1620240208055\/work\n slicer==0.0.7\n smart-open==6.0.0\n spacy==2.1.7\n srsly==1.0.5\n thinc==7.0.8\n tqdm==4.64.0\n typing-extensions==4.1.1\n urllib3==1.26.10\n wasabi==0.9.1\n websocket-client==1.3.1\n Werkzeug==1.0.1\n wrapt==1.12.1\n zipp==3.6.0\n    \n 2022-07-13T09:30:24,740357515+00:00 | gunicorn\/run | \n 2022-07-13T09:30:24,742081301+00:00 | gunicorn\/run | ###############################################\n 2022-07-13T09:30:24,747351958+00:00 | gunicorn\/run | AzureML Inference Server\n 2022-07-13T09:30:24,750376533+00:00 | gunicorn\/run | ###############################################\n 2022-07-13T09:30:24,752770513+00:00 | gunicorn\/run | \n 2022-07-13T09:30:27,698759963+00:00 | gunicorn\/run | Starting AzureML Inference Server HTTP.\n    \n Azure ML Inferencing HTTP server v0.4.13\n    \n    \n Server Settings\n ---------------\n Entry Script Name: main.py\n Model Directory: \/var\/azureml-app\/azureml-models\/amlstudio-loanv1ep002\/1\n Worker Count: 1\n Worker Timeout (seconds): 300\n Server Port: 31311\n Application Insights Enabled: false\n Application Insights Key: AppInsights key provided\n    \n    \n Server Routes\n ---------------\n Liveness Probe: GET   127.0.0.1:31311\/\n Score:          POST  127.0.0.1:31311\/score\n    \n Starting gunicorn 20.1.0\n Listening at: http:\/\/0.0.0.0:31311 (17)\n Using worker: sync\n Booting worker with pid: 70\n Collecting azureml-designer-serving==0.0.10\n   Downloading azureml_designer_serving-0.0.10-py3-none-any.whl (20 kB)\n Collecting azureml-contrib-services\n   Downloading azureml_contrib_services-1.43.0-py3-none-any.whl (4.8 kB)\n Requirement already satisfied: azureml-defaults in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-serving==0.0.10) (1.36.0)\n Requirement already satisfied: azureml-designer-core[image,model]>=0.0.32 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-serving==0.0.10) (0.0.68)\n Requirement already satisfied: Flask in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-contrib-services->azureml-designer-serving==0.0.10) (1.0.3)\n Requirement already satisfied: azureml-core~=1.36.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-defaults->azureml-designer-serving==0.0.10) (1.36.0.post2)\n Requirement already satisfied: json-logging-py==0.2 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-defaults->azureml-designer-serving==0.0.10) (0.2)\n Requirement already satisfied: azureml-inference-server-http~=0.4.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-defaults->azureml-designer-serving==0.0.10) (0.4.13)\n Requirement already satisfied: configparser==3.7.4 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-defaults->azureml-designer-serving==0.0.10) (3.7.4)\n Requirement already satisfied: azureml-dataset-runtime[fuse]~=1.36.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-defaults->azureml-designer-serving==0.0.10) (1.36.0)\n Requirement already satisfied: pyarrow==0.16.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (0.16.0)\n Requirement already satisfied: jsonschema==3.0.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (3.0.1)\n Collecting numpy==1.18.1\n   Downloading numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n Requirement already satisfied: pandas==1.0.4 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (1.0.4)\n Collecting python-dateutil==2.8.1\n   Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n Requirement already satisfied: distro==1.4.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (1.4.0)\n Requirement already satisfied: ruamel.yaml==0.16.10 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (0.16.10)\n Requirement already satisfied: pycryptodomex==3.7.3 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (3.7.3)\n Requirement already satisfied: more-itertools==6.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (6.0.0)\n Requirement already satisfied: Pillow==8.3.2; extra == \"image\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (8.3.2)\n Collecting cloudpickle==1.2.2; extra == \"model\"\n   Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n Requirement already satisfied: Werkzeug>=0.14 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from Flask->azureml-contrib-services->azureml-designer-serving==0.0.10) (1.0.1)\n Requirement already satisfied: Jinja2>=2.10 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from Flask->azureml-contrib-services->azureml-designer-serving==0.0.10) (3.0.3)\n Requirement already satisfied: click>=5.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from Flask->azureml-contrib-services->azureml-designer-serving==0.0.10) (7.1.2)\n Requirement already satisfied: itsdangerous>=0.24 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from Flask->azureml-contrib-services->azureml-designer-serving==0.0.10) (1.1.0)\n Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.61.1)\n Requirement already satisfied: PyJWT<3.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.4.0)\n Requirement already satisfied: jmespath<1.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.10.0)\n Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.1.28)\n Requirement already satisfied: azure-mgmt-resource<15.0.0,>=1.2.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (13.0.0)\n Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0\n   Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n Requirement already satisfied: jsonpickle<3.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.2.0)\n Requirement already satisfied: azure-mgmt-containerregistry>=2.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (10.0.0)\n Requirement already satisfied: ndg-httpsclient<=0.5.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.5.1)\n Requirement already satisfied: docker<6.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (5.0.3)\n Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.6.4)\n Requirement already satisfied: backports.tempfile in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.0)\n Requirement already satisfied: pyopenssl<21.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (20.0.1)\n Requirement already satisfied: azure-mgmt-storage<16.0.0,>=1.5.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (11.2.0)\n Requirement already satisfied: msrest<1.0.0,>=0.5.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.7.1)\n Requirement already satisfied: adal<=1.2.7,>=1.2.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.2.7)\n Requirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (9.3.0)\n Requirement already satisfied: requests<3.0.0,>=2.19.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.27.1)\n Requirement already satisfied: contextlib2<22.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (21.6.0)\n Requirement already satisfied: SecretStorage<4.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (3.3.2)\n Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.61.0)\n Requirement already satisfied: pytz in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2022.1)\n Collecting urllib3<=1.26.7,>=1.23\n   Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n Requirement already satisfied: pathspec<1.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.9.0)\n Requirement already satisfied: opencensus-ext-azure~=1.1.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.1.5)\n Requirement already satisfied: inference-schema==1.3.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.3.0)\n Requirement already satisfied: gunicorn==20.1.0; platform_system != \"Windows\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (20.1.0)\n Requirement already satisfied: applicationinsights>=0.11.7 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (0.11.10)\n Requirement already satisfied: azureml-dataprep<2.25.0a,>=2.24.0a in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-dataset-runtime[fuse]~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.24.4)\n Requirement already satisfied: fusepy<4.0.0,>=3.0.1; extra == \"fuse\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-dataset-runtime[fuse]~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (3.0.1)\n Requirement already satisfied: six>=1.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from pyarrow==0.16.0->azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (1.16.0)\n Requirement already satisfied: attrs>=17.4.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from jsonschema==3.0.1->azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (21.4.0)\n Requirement already satisfied: setuptools in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from jsonschema==3.0.1->azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (58.0.4)\n Requirement already satisfied: pyrsistent>=0.14.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from jsonschema==3.0.1->azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (0.18.0)\n Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from ruamel.yaml==0.16.10->azureml-designer-core[image,model]>=0.0.32->azureml-designer-serving==0.0.10) (0.2.6)\n Requirement already satisfied: MarkupSafe>=2.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from Jinja2>=2.10->Flask->azureml-contrib-services->azureml-designer-serving==0.0.10) (2.0.1)\n Requirement already satisfied: cffi>=1.12 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.12.3)\n Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from jsonpickle<3.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (4.8.3)\n Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azure-mgmt-containerregistry>=2.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.3.1)\n Requirement already satisfied: pyasn1>=0.1.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.4.8)\n Requirement already satisfied: websocket-client>=0.32.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from docker<6.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.3.1)\n Requirement already satisfied: backports.weakref in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from backports.tempfile->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.0.post1)\n Requirement already satisfied: isodate>=0.6.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.6.1)\n Requirement already satisfied: azure-core>=1.24.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.24.2)\n Requirement already satisfied: requests-oauthlib>=0.5.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (1.3.1)\n Requirement already satisfied: certifi>=2017.4.17 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2022.6.15)\n Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.0.12)\n Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (3.3)\n Requirement already satisfied: jeepney>=0.6 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from SecretStorage<4.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (0.7.1)\n Requirement already satisfied: opencensus<1.0.0,>=0.10.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (0.10.0)\n Requirement already satisfied: psutil>=5.6.3 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (5.9.1)\n Requirement already satisfied: azure-identity<2.0.0,>=1.5.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.10.0)\n Requirement already satisfied: wrapt<=1.12.1,>=1.11.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from inference-schema==1.3.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.12.1)\n Requirement already satisfied: azureml-dataprep-rslex~=2.0.0dev0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-dataprep<2.25.0a,>=2.24.0a->azureml-dataset-runtime[fuse]~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.0.3)\n Collecting dotnetcore2<3.0.0,>=2.1.14\n   Downloading dotnetcore2-2.1.23-py3-none-manylinux1_x86_64.whl (29.3 MB)\n Requirement already satisfied: azureml-dataprep-native<39.0.0,>=38.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azureml-dataprep<2.25.0a,>=2.24.0a->azureml-dataset-runtime[fuse]~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (38.0.0)\n Requirement already satisfied: pycparser in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (2.21)\n Requirement already satisfied: zipp>=0.5 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (3.6.0)\n Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (4.1.1)\n Requirement already satisfied: oauthlib>=3.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml-core~=1.36.0->azureml-defaults->azureml-designer-serving==0.0.10) (3.2.0)\n Requirement already satisfied: opencensus-context>=0.1.2 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (0.1.2)\n Requirement already satisfied: google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (2.8.2)\n Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.0.0)\n Requirement already satisfied: msal<2.0.0,>=1.12.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.18.0)\n Requirement already satisfied: contextvars; python_version >= \"3.6\" and python_version < \"3.7\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from opencensus-context>=0.1.2->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (2.4)\n Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (2.9.0)\n Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (1.56.3)\n Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (3.19.4)\n Requirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (2.5.1)\n Requirement already satisfied: immutables>=0.9 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from contextvars; python_version >= \"3.6\" and python_version < \"3.7\"->opencensus-context>=0.1.2->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (0.18)\n Requirement already satisfied: pyasn1-modules>=0.2.1 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (0.2.8)\n Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (4.8)\n Requirement already satisfied: cachetools<6.0,>=2.0.0 in \/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.10.0->opencensus-ext-azure~=1.1.0->azureml-inference-server-http~=0.4.1->azureml-defaults->azureml-designer-serving==0.0.10) (4.2.4)\n Installing collected packages: azureml-contrib-services, azureml-designer-serving, numpy, python-dateutil, cloudpickle, cryptography, urllib3, dotnetcore2\n   Attempting uninstall: numpy\n     Found existing installation: numpy 1.19.5\n     Uninstalling numpy-1.19.5:\n       Successfully uninstalled numpy-1.19.5\n   Attempting uninstall: python-dateutil\n     Found existing installation: python-dateutil 2.8.2\n     Uninstalling python-dateutil-2.8.2:\n       Successfully uninstalled python-dateutil-2.8.2\n   Attempting uninstall: cloudpickle\n     Found existing installation: cloudpickle 2.1.0\n     Uninstalling cloudpickle-2.1.0:\n       Successfully uninstalled cloudpickle-2.1.0\n   Attempting uninstall: cryptography\n     Found existing installation: cryptography 37.0.4\n     Uninstalling cryptography-37.0.4:\n       Successfully uninstalled cryptography-37.0.4\n   Attempting uninstall: urllib3\n     Found existing installation: urllib3 1.26.10\n     Uninstalling urllib3-1.26.10:\n       Successfully uninstalled urllib3-1.26.10\n   Attempting uninstall: dotnetcore2\n     Found existing installation: dotnetcore2 3.1.23\n     Uninstalling dotnetcore2-3.1.23:\n       Successfully uninstalled dotnetcore2-3.1.23\n ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n    \n We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n    \n azureml-dataset-runtime 1.36.0 requires pyarrow<4.0.0,>=0.17.0, but you'll have pyarrow 0.16.0 which is incompatible.\n azureml-dataprep 2.24.4 requires azure-identity==1.7.0, but you'll have azure-identity 1.10.0 which is incompatible.\n Successfully installed azureml-contrib-services-1.43.0 azureml-designer-serving-0.0.10 cloudpickle-1.2.2 cryptography-3.4.8 dotnetcore2-2.1.23 numpy-1.18.1 python-dateutil-2.8.1 urllib3-1.26.7\n Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (urllib3 1.26.10 (\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23'), {'azureml-core'}).\n Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (urllib3 1.26.10 (\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23'), {'azureml-core'}).\n Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (urllib3 1.26.10 (\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23'), {'azureml-core'}).\n Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (urllib3 1.26.10 (\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages), Requirement.parse('urllib3<=1.26.7,>=1.23')).\n Initializing logger\n 2022-07-13 09:30:57,630 | root | INFO | Starting up app insights client\n logging socket was found. logging is available.\n logging socket was found. logging is available.\n 2022-07-13 09:30:57,631 | root | INFO | Starting up request id generator\n 2022-07-13 09:30:57,631 | root | INFO | Starting up app insight hooks\n 2022-07-13 09:30:57,632 | root | INFO | Invoking user's init function\n Trying to reload werkzeug.\n Successfully reloaded werkzeug.\n pip freeze result:\n adal==1.2.7\n applicationinsights==0.11.10\n attrs==21.4.0\n azure-common==1.1.28\n azure-core==1.24.2\n azure-graphrbac==0.61.1\n azure-identity==1.10.0\n azure-mgmt-authorization==0.61.0\n azure-mgmt-containerregistry==10.0.0\n azure-mgmt-core==1.3.1\n azure-mgmt-keyvault==9.3.0\n azure-mgmt-resource==13.0.0\n azure-mgmt-storage==11.2.0\n azure-storage-blob==1.5.0\n azure-storage-common==1.4.2\n azureml-contrib-services==1.43.0\n azureml-core==1.36.0.post2\n azureml-dataprep==2.24.4\n azureml-dataprep-native==38.0.0\n azureml-dataprep-rslex==2.0.3\n azureml-dataset-runtime==1.36.0\n azureml-defaults==1.36.0\n azureml-designer-classic-modules==0.0.161\n azureml-designer-core==0.0.68\n azureml-designer-internal==0.0.56\n azureml-designer-serving==0.0.10\n azureml-inference-server-http==0.4.13\n azureml-interpret==1.36.0\n azureml-model-management-sdk==1.0.1b6.post1\n azureml-pipeline-core==1.36.0\n azureml-telemetry==1.36.0\n backports.tempfile==1.0\n backports.weakref==1.0.post1\n blis==0.2.4\n cachetools==4.2.4\n certifi==2022.6.15\n cffi==1.12.3\n chardet==3.0.4\n charset-normalizer==2.0.12\n click==7.1.2\n cloudpickle==1.2.2\n configparser==3.7.4\n contextlib2==21.6.0\n contextvars==2.4\n cryptography==3.4.8\n cycler==0.11.0\n cymem==2.0.6\n dill==0.3.4\n distro==1.4.0\n docker==5.0.3\n dotnetcore2==2.1.23\n en-core-web-sm @ https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.1.0\/en_core_web_sm-2.1.0.tar.gz\n Flask==1.0.3\n fusepy==3.0.1\n gensim==3.8.3\n google-api-core==2.8.2\n google-auth==2.9.0\n googleapis-common-protos==1.56.3\n gunicorn==20.1.0\n idna==3.3\n imbalanced-learn==0.4.3\n immutables==0.18\n importlib-metadata==4.8.3\n importlib-resources==5.4.0\n inference-schema==1.3.0\n interpret-community==0.21.0\n interpret-core==0.2.6\n isodate==0.6.1\n itsdangerous==1.1.0\n jeepney==0.7.1\n Jinja2==3.0.3\n jmespath==0.10.0\n joblib==0.14.0\n json-logging-py==0.2\n jsonpickle==2.2.0\n jsonschema==3.0.1\n kiwisolver==1.3.1\n liac-arff==2.5.0\n lightgbm==3.2.1\n llvmlite==0.36.0\n MarkupSafe==2.0.1\n matplotlib==3.1.3\n more-itertools==6.0.0\n msal==1.18.0\n msal-extensions==1.0.0\n msrest==0.7.1\n msrestazure==0.6.4\n murmurhash==1.0.7\n ndg-httpsclient==0.5.1\n nimbusml==1.6.1\n numba==0.53.1\n numpy==1.18.1\n oauthlib==3.2.0\n opencensus==0.10.0\n opencensus-context==0.1.2\n opencensus-ext-azure==1.1.5\n packaging==21.3\n pandas==1.0.4\n pathspec==0.9.0\n Pillow==8.3.2\n plac==0.9.6\n portalocker==2.5.1\n preshed==2.0.1\n protobuf==3.19.4\n psutil==5.9.1\n pyarrow==0.16.0\n pyasn1==0.4.8\n pyasn1-modules==0.2.8\n pycparser==2.21\n pycryptodomex==3.7.3\n PyJWT==2.4.0\n pyOpenSSL==20.0.1\n pyparsing==3.0.9\n pyrsistent==0.18.0\n python-dateutil==2.8.1\n pytz==2022.1\n requests==2.27.1\n requests-oauthlib==1.3.1\n rsa==4.8\n ruamel.yaml==0.16.10\n ruamel.yaml.clib==0.2.6\n scikit-learn==0.22.2\n scikit-surprise==1.0.6\n scipy==1.4.1\n seaborn==0.10.0\n SecretStorage==3.3.2\n shap==0.39.0\n six @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/six_1620240208055\/work\n slicer==0.0.7\n smart-open==6.0.0\n spacy==2.1.7\n srsly==1.0.5\n thinc==7.0.8\n tqdm==4.64.0\n typing-extensions==4.1.1\n urllib3==1.26.7\n wasabi==0.9.1\n websocket-client==1.3.1\n Werkzeug==1.0.1\n wrapt==1.12.1\n zipp==3.6.0\n    \n Model: name=amlstudio-loanv1ep002, version=1\n Loading static source Resources\/1\/ - Start:\n Loaded TransformationDirectory(meta={'type': 'TransformationDirectory', 'extension': {}, 'transform_type': 'Pickle', 'file_path': 'data.itransform'}) from studiomodelpackage\/Resources\/1.\n Loading static source Resources\/1\/ - End with 0.1387s elapsed.\n Loading static source Resources\/2\/ - Start:\n Loaded ModelDirectory(meta={'type': 'ModelDirectory', 'extension': {}, 'model': 'model_spec.yaml', 'registerModel': True, 'modelOutputPath': 'trained_model_outputs'}) from studiomodelpackage\/Resources\/2.\n Loading static source Resources\/2\/ - End with 1.5108s elapsed.\n initializing node 1\n ALGHOST 0.0.161\n Load pyarrow.parquet explicitly: <module 'pyarrow.parquet' from '\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages\/pyarrow\/parquet.py'>\n initializing node 2\n ALGHOST 0.0.161\n Load pyarrow.parquet explicitly: <module 'pyarrow.parquet' from '\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages\/pyarrow\/parquet.py'>\n initializing node 3\n ALGHOST 0.0.161\n Load pyarrow.parquet explicitly: <module 'pyarrow.parquet' from '\/azureml-envs\/azureml_41d04a9e61995ab9ca33645f37d72150\/lib\/python3.6\/site-packages\/pyarrow\/parquet.py'>\n Init: Graph has been loaded\n 2022-07-13 09:31:00,723 | root | INFO | Users's init has completed successfully\n 2022-07-13 09:31:00,730 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n 2022-07-13 09:31:00,731 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n 2022-07-13 09:31:00,732 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n 2022-07-13 09:31:00,939 | root | INFO | Swagger file not present\n 2022-07-13 09:31:00,939 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:31:00 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:31:01,214 | root | INFO | Swagger file not present\n 2022-07-13 09:31:01,214 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:31:01 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:31:01,229 | root | INFO | Swagger file not present\n 2022-07-13 09:31:01,229 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:31:01 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"curl\/7.80.0\"\n 2022-07-13 09:31:02,201 | root | INFO | Swagger file not present\n 2022-07-13 09:31:02,202 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:31:02 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"curl\/7.80.0\"\n 2022-07-13 09:31:04,052 | root | INFO | Swagger file not present\n 2022-07-13 09:31:04,052 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:31:04 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:31:04,335 | root | INFO | Scoring Timer is set to 60.0 seconds\n Handling http request - Start:\n 2022-07-13 09:31:04,335 studio.azureml.designer.serving.dagengine.request_handler INFO       |   Run: is_classic = False, with_details = False, verbose = True\n 2022-07-13 09:31:04,336 studio.core          INFO       |   Pre-processing - Start:\n 2022-07-13 09:31:04,336 studio.core          INFO       |   Pre-processing - End with 0.0001s elapsed.\n 2022-07-13 09:31:04,336 studio.core          INFO       |   Processing - Start:\n 2022-07-13 09:31:04,360 studio.common        WARNING    |   |   0 and empty string will be converted into False\n 2022-07-13 09:31:04,377 studio.common        WARNING    |   |   0 and empty string will be converted into False\n 2022-07-13 09:31:04,418 studio.core          INFO       |   |   Executing node 1: Apply Transformation - Start:\n 2022-07-13 09:31:04,426 studio.common        DEBUG      |   |   |   Load schema successfully.\n 2022-07-13 09:31:04,429 studio.modulehost    INFO       |   |   |   Return without parsing\n 2022-07-13 09:31:04,429 studio.modulehost    INFO       |   |   |   Return without parsing\n 2022-07-13 09:31:04,435 studio.core          INFO       |   |   |   ApplyTransformationModule.run - Start:\n 2022-07-13 09:31:04,435 studio.core          DEBUG      |   |   |   |   kwargs:\n 2022-07-13 09:31:04,435 studio.core          DEBUG      |   |   |   |   |   transform = <azureml.studio.modules.datatransform.clean_missing_data.clean_missing_transform.CleanMissingValueTransform object at 0x7f72fe544198>\n 2022-07-13 09:31:04,435 studio.core          DEBUG      |   |   |   |   |   data = <azureml.studio.common.datatable.data_table.DataTable object at 0x7f72fdd6afd0>\n 2022-07-13 09:31:04,436 studio.core          DEBUG      |   |   |   |   validated_args:\n 2022-07-13 09:31:04,436 studio.core          DEBUG      |   |   |   |   |   transform = <azureml.studio.modules.datatransform.clean_missing_data.clean_missing_transform.CleanMissingValueTransform object at 0x7f72fe544198>\n 2022-07-13 09:31:04,436 studio.core          DEBUG      |   |   |   |   |   data = <azureml.studio.common.datatable.data_table.DataTable object at 0x7f72fdd6afd0>\n 2022-07-13 09:31:04,436 studio.module        INFO       |   |   |   |   Get column indexes with wanted ratio\n 2022-07-13 09:31:04,436 studio.core          INFO       |   |   |   |   CleanMissingValueTransform.get_column_indexes_with_wanted_ratio - Start:\n 2022-07-13 09:31:04,439 studio.core          INFO       |   |   |   |   CleanMissingValueTransform.get_column_indexes_with_wanted_ratio - End with 0.0026s elapsed.\n 2022-07-13 09:31:04,439 studio.module        INFO       |   |   |   |   Replace row with missing value\n 2022-07-13 09:31:04,439 studio.core          INFO       |   |   |   |   DataTable.clone - Start:\n 2022-07-13 09:31:04,446 studio.core          INFO       |   |   |   |   DataTable.clone - End with 0.0066s elapsed.\n 2022-07-13 09:31:04,446 studio.core          INFO       |   |   |   |   Find row_indexes_to_remove - Start:\n 2022-07-13 09:31:04,449 studio.core          INFO       |   |   |   |   Find row_indexes_to_remove - End with 0.0030s elapsed.\n 2022-07-13 09:31:04,449 studio.core          INFO       |   |   |   |   Remove rows by indexes - Start:\n 2022-07-13 09:31:04,456 studio.core          INFO       |   |   |   |   Remove rows by indexes - End with 0.0012s elapsed.\n 2022-07-13 09:31:04,456 studio.core          DEBUG      |   |   |   |   return:\n 2022-07-13 09:31:04,456 studio.core          DEBUG      |   |   |   |   |   [0] = <DataTable \"Dataset\" (1 Rows, 11 Cols) at 0x00007F72FDD957F0>\n 2022-07-13 09:31:04,456 studio.core          INFO       |   |   |   ApplyTransformationModule.run - End with 0.0211s elapsed.\n 2022-07-13 09:31:04,457 studio.core          INFO       |   |   Executing node 1: Apply Transformation - End with 0.0388s elapsed.\n 2022-07-13 09:31:04,457 studio.core          INFO       |   |   Executing node 2: Score Model - Start:\n 2022-07-13 09:31:04,457 studio.common        DEBUG      |   |   |   Load schema successfully.\n 2022-07-13 09:31:04,466 studio.modulehost    INFO       |   |   |   Return without parsing\n 2022-07-13 09:31:04,466 studio.modulehost    INFO       |   |   |   Return without parsing\n 2022-07-13 09:31:04,466 studio.modulehost    INFO       |   |   |   Parse bool parameter\n 2022-07-13 09:31:04,466 studio.core          INFO       |   |   |   ScoreModelModule.run - Start:\n 2022-07-13 09:31:04,466 studio.core          DEBUG      |   |   |   |   kwargs:\n 2022-07-13 09:31:04,466 studio.core          DEBUG      |   |   |   |   |   learner = <azureml.studio.modules.ml.initialize_models.binary_classifier.logistic_regression_biclassifier.logistic_regression_biclassifier.LogisticRegressionBiClassifier object at 0x7f72fe559978>\n 2022-07-13 09:31:04,466 studio.core          DEBUG      |   |   |   |   |   test_data = <azureml.studio.common.datatable.data_table.DataTable object at 0x7f72fdd6acf8>\n 2022-07-13 09:31:04,467 studio.core          DEBUG      |   |   |   |   |   append_or_result_only = True\n 2022-07-13 09:31:04,467 studio.core          DEBUG      |   |   |   |   validated_args:\n 2022-07-13 09:31:04,467 studio.core          DEBUG      |   |   |   |   |   learner = <azureml.studio.modules.ml.initialize_models.binary_classifier.logistic_regression_biclassifier.logistic_regression_biclassifier.LogisticRegressionBiClassifier object at 0x7f72fe559978>\n 2022-07-13 09:31:04,467 studio.core          DEBUG      |   |   |   |   |   test_data = <azureml.studio.common.datatable.data_table.DataTable object at 0x7f72fdd6acf8>\n 2022-07-13 09:31:04,467 studio.core          DEBUG      |   |   |   |   |   append_or_result_only = True\n 2022-07-13 09:31:04,467 studio.module        INFO       |   |   |   |   Validated testing data has 1 Row(s) and 11 Columns.\n 2022-07-13 09:31:04,474 studio.module        INFO       |   |   |   |   Check if column types of test data are consistent with train data\n 2022-07-13 09:31:04,475 studio.module        INFO       |   |   |   |   Building Normalizer - found Label column=None with encode_label=False\n 2022-07-13 09:31:04,475 studio.module        INFO       |   |   |   |   Building normalizer - found 11 feature columns with normalize_number=True\n 2022-07-13 09:31:04,475 studio.module        DEBUG      |   |   |   |   Building normalizer - found feature columns: \"Gender,Married,Dependents,Education,Self_Employed,ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,Credit_History,Property_Area\".\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   Building normalizer - found 6 numeric feature columns and 5 string feature columns to be encoded\n 2022-07-13 09:31:04,476 studio.module        DEBUG      |   |   |   |   Building normalizer - found numeric feature columns to be encoded: \"Married,Self_Employed,ApplicantIncome,LoanAmount,Loan_Amount_Term,Credit_History\".\n 2022-07-13 09:31:04,476 studio.module        DEBUG      |   |   |   |   Building normalizer - found string feature columns to be encoded: \"Gender,Dependents,Education,CoapplicantIncome,Property_Area\".\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   Successfully built normalizer of test data.\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   Successfully checked column types. Predicting.\n 2022-07-13 09:31:04,476 studio.core          INFO       |   |   |   |   BaseLearner._apply_normalize - Start:\n 2022-07-13 09:31:04,476 studio.core          INFO       |   |   |   |   |   Applying feature normalization - Start:\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   |   |   Start to execute normalizer.transform with column_list: \"Gender,Married,Dependents,Education,Self_Employed,ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,Credit_History,Property_Area\".\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   |   |   Columns of input DataFrame: 11\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   |   |   Columns to be transformed: 11\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   |   |   Columns to be encoded: 11\n 2022-07-13 09:31:04,476 studio.module        INFO       |   |   |   |   |   |   Transform with label column Loan_Status.\n 2022-07-13 09:31:04,477 studio.core          INFO       |   |   |   |   |   |   Normalizer._transform_str_feature_columns - Start:\n 2022-07-13 09:31:04,494 studio.module        INFO       |   |   |   |   |   |   |   Successfully encoded 5 string feature columns.\n 2022-07-13 09:31:04,494 studio.module        INFO       |   |   |   |   |   |   |   After transformation, 179 string feature column are generated\n 2022-07-13 09:31:04,494 studio.core          INFO       |   |   |   |   |   |   Normalizer._transform_str_feature_columns - End with 0.0174s elapsed.\n 2022-07-13 09:31:04,494 studio.core          INFO       |   |   |   |   |   |   Normalizer._transform_numeric_feature_columns - Start:\n 2022-07-13 09:31:04,497 studio.module        INFO       |   |   |   |   |   |   |   Successfully encoded 6 numeric feature columns.\n 2022-07-13 09:31:04,497 studio.core          INFO       |   |   |   |   |   |   Normalizer._transform_numeric_feature_columns - End with 0.0031s elapsed.\n 2022-07-13 09:31:04,506 studio.module        INFO       |   |   |   |   |   |   Construct train set with Sparse structure.\n 2022-07-13 09:31:04,507 studio.core          INFO       |   |   |   |   |   Applying feature normalization - End with 0.0309s elapsed.\n 2022-07-13 09:31:04,513 studio.core          INFO       |   |   |   |   BaseLearner._apply_normalize - End with 0.0367s elapsed.\n 2022-07-13 09:31:04,513 studio.core          INFO       |   |   |   |   BaseLearner._predict - Start:\n 2022-07-13 09:31:04,513 studio.core          INFO       |   |   |   |   |   Predicting probability - Start:\n 2022-07-13 09:31:04,514 studio.core          INFO       |   |   |   |   |   Predicting probability - End with 0.0004s elapsed.\n 2022-07-13 09:31:04,514 studio.core          INFO       |   |   |   |   |   calculating argmax(Probability) - Start:\n 2022-07-13 09:31:04,514 studio.core          INFO       |   |   |   |   |   calculating argmax(Probability) - End with 0.0000s elapsed.\n 2022-07-13 09:31:04,514 studio.core          INFO       |   |   |   |   BaseLearner._predict - End with 0.0007s elapsed.\n 2022-07-13 09:31:04,514 studio.module        INFO       |   |   |   |   Successfully predicted.\n 2022-07-13 09:31:04,515 studio.module        INFO       |   |   |   |   Found 2 label classes in classes_ attribute.\n 2022-07-13 09:31:04,515 studio.module        INFO       |   |   |   |   Using 1 as probability column.\n 2022-07-13 09:31:04,524 studio.module        INFO       |   |   |   |   Binary Classification Model Scored Columns are: \n 2022-07-13 09:31:04,525 studio.module        INFO       |   |   |   |   There are 2 score columns: \"Binary Class Assigned Labels,Calibrated Score\"\n 2022-07-13 09:31:04,525 studio.core          DEBUG      |   |   |   |   return:\n 2022-07-13 09:31:04,525 studio.core          DEBUG      |   |   |   |   |   [0] = <DataTable \"Dataset\" (1 Rows, 13 Cols) at 0x00007F72FDD6ACF8>\n 2022-07-13 09:31:04,525 studio.core          INFO       |   |   |   ScoreModelModule.run - End with 0.0586s elapsed.\n 2022-07-13 09:31:04,526 studio.core          INFO       |   |   Executing node 2: Score Model - End with 0.0687s elapsed.\n 2022-07-13 09:31:04,526 studio.core          INFO       |   |   Executing node 3: Select Columns in Dataset - Start:\n 2022-07-13 09:31:04,533 studio.common        DEBUG      |   |   |   Load schema successfully.\n 2022-07-13 09:31:04,536 studio.modulehost    INFO       |   |   |   Return without parsing\n 2022-07-13 09:31:04,536 studio.modulehost    INFO       |   |   |   Parse ColumnSelection parameter\n 2022-07-13 09:31:04,542 studio.core          INFO       |   |   |   SelectColumnsModule.run - Start:\n 2022-07-13 09:31:04,542 studio.core          DEBUG      |   |   |   |   kwargs:\n 2022-07-13 09:31:04,543 studio.core          DEBUG      |   |   |   |   |   table = <azureml.studio.common.datatable.data_table.DataTable object at 0x7f72fdd2c128>\n 2022-07-13 09:31:04,543 studio.core          DEBUG      |   |   |   |   |   feature_list = <azureml.studio.common.datatable.data_table.DataTableColumnSelection object at 0x7f72fdd2cda0>\n 2022-07-13 09:31:04,543 studio.core          DEBUG      |   |   |   |   validated_args:\n 2022-07-13 09:31:04,543 studio.core          DEBUG      |   |   |   |   |   table = <azureml.studio.common.datatable.data_table.DataTable object at 0x7f72fdd2c128>\n 2022-07-13 09:31:04,543 studio.core          DEBUG      |   |   |   |   |   feature_list = <azureml.studio.common.datatable.data_table.DataTableColumnSelection object at 0x7f72fdd2cda0>\n 2022-07-13 09:31:04,543 studio.module        INFO       |   |   |   |   Select column indexes from Dataset\n 2022-07-13 09:31:04,545 studio.core          DEBUG      |   |   |   |   return:\n 2022-07-13 09:31:04,545 studio.core          DEBUG      |   |   |   |   |   [0] = <DataTable (1 Rows, 2 Cols) at 0x00007F72FDD2CEB8>\n 2022-07-13 09:31:04,545 studio.core          INFO       |   |   |   SelectColumnsModule.run - End with 0.0027s elapsed.\n 2022-07-13 09:31:04,545 studio.core          INFO       |   |   Executing node 3: Select Columns in Dataset - End with 0.0197s elapsed.\n 2022-07-13 09:31:04,553 studio.core          INFO       |   Processing - End with 0.2173s elapsed.\n 2022-07-13 09:31:04,553 studio.core          INFO       |   Post-processing - Start:\n 2022-07-13 09:31:04,554 studio.core          INFO       |   Post-processing - End with 0.0000s elapsed.\n 2022-07-13 09:31:04,554 studio.core          INFO       Handling http request - End with 0.2182s elapsed.\n 2022-07-13 09:31:04,554 studio.azureml.designer.serving.dagengine.request_handler DEBUG      Run: output data(raw) = {\"Results\": {\"WebServiceOutput0\": [{\"Scored Labels\": true, \"Scored Probabilities\": 0.591091131859528}]}}\n 2022-07-13 09:31:04,554 | root | INFO | run() output is HTTP Response\n 2022-07-13 09:31:04,554 | root | INFO | 200\n 127.0.0.1 - - [13\/Jul\/2022:09:31:04 +0000] \"POST \/score?verbose=true HTTP\/1.0\" 200 104 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:34:34,646 | root | INFO | Swagger file not present\n 2022-07-13 09:34:34,647 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:34:34 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:34:36,197 | root | INFO | Swagger file not present\n 2022-07-13 09:34:36,198 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:34:36 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"curl\/7.80.0\"\n 2022-07-13 09:34:37,162 | root | INFO | Swagger file not present\n 2022-07-13 09:34:37,162 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:34:37 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:50:54,732 | root | INFO | Swagger file not present\n 2022-07-13 09:50:54,732 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:50:54 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:50:55,920 | root | INFO | Swagger file not present\n 2022-07-13 09:50:55,921 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:50:55 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"curl\/7.80.0\"\n 2022-07-13 09:53:43,395 | root | INFO | Swagger file not present\n 2022-07-13 09:53:43,395 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:53:43 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:53:43,679 | root | INFO | Swagger file not present\n 2022-07-13 09:53:43,679 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:53:43 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"curl\/7.80.0\"\n 2022-07-13 09:54:02,867 | root | INFO | Swagger file not present\n 2022-07-13 09:54:02,868 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:54:02 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"Mozilla\/5.0 (X11; Linux x86_64; rv:102.0) Gecko\/20100101 Firefox\/102.0\"\n 2022-07-13 09:54:40,605 | root | INFO | Swagger file not present\n 2022-07-13 09:54:40,605 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:54:40 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:54:49,848 | root | INFO | Swagger file not present\n 2022-07-13 09:54:49,853 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:54:49 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:54:56,064 | root | INFO | Swagger file not present\n 2022-07-13 09:54:56,065 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:54:56 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:55:31,957 | root | INFO | Swagger file not present\n 2022-07-13 09:55:31,958 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:55:31 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"hackney\/1.18.1\"\n 2022-07-13 09:55:32,313 | root | INFO | Swagger file not present\n 2022-07-13 09:55:32,313 | root | INFO | 404\n 127.0.0.1 - - [13\/Jul\/2022:09:55:32 +0000] \"GET \/swagger.json HTTP\/1.0\" 404 19 \"-\" \"curl\/7.80.0\"",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Machine Learning Studio Issue",
        "Question_creation_time":1627253465317,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/488593\/machine-learning-studio-issue.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey all,\n\nI'm working on a project for school in the machine learning studio classic and when I try to execute the following python script I get errors.\n\ndef azureml_main(frame1):\nimport matplotlib\nmatplotlib.use('agg')\n\n import pandas as pd \n import numpy as np \n import matplotlib.pyplot as plt\n import statsmodels.graphics.boxplots as sm\n Azure = True\nCreate a series of bar plots for the various levels of the\nstring columns in the data frame by readmi_class.\n names = list(frame1) \n num_cols = frame1.shape[1]\n for indx in range(num_cols - 1): \n         if(frame1.ix[:, indx].dtype not in [np.int64, np.int32, np.float64]):\n             temp1 = frame1.ix[frame1.readmi_class == 'YES', indx].value_counts()\n             temp0 = frame1.ix[frame1.readmi_class == 'NO', indx].value_counts()  \n             fig = plt.figure(figsize = (12,6)) \n             fig.clf()\n             ax1 = fig.add_subplot(1, 2, 1) \n             ax0 = fig.add_subplot(1, 2, 2) \n             temp1.plot(kind = 'bar', ax = ax1)\n             ax1.set_title('Values of ' + names[indx] + '\\n for readmitted patients')\n             temp0.plot(kind = 'bar', ax = ax0)\n             ax0.set_title('Values of ' + names[indx] + '\\n for patients not readmitted')\n              \n             if(Azure == True): fig.savefig('bar_' + names[indx] +'.png') \n                 ## Now make some box plots of the columns with numerical values. \n for indx in range(num_cols):\n         if(frame1.ix[:, indx].dtype in [np.int64, np.int32, np.float64]): \n             temp1 = frame1.ix[frame1.readmi_class == 'YES', indx] \n             temp0 = frame1.ix[frame1.readmi_class == 'NO', indx]\n          \n             fig = plt.figure(figsize = (12,6))             \n             fig.clf()\n             ax1 = fig.add_subplot(1, 2, 1)             \n             ax0 = fig.add_subplot(1, 2, 2)        \n             ax1.boxplot(temp1.as_matrix())\n             ax1.set_title('Box plot of ' + names[indx] + '\\n for readmitted patients')\n             ax0.boxplot(temp0.as_matrix())\n             ax0.set_title('Box plot of ' + names[indx] + '\\n for patients not readmitted')\n              \n             if(Azure == True): fig.savefig('box_' + names[indx] +'.png')               \n return frame1\n\n\n\nThe error I'm getting is:\nError 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\nFile \"C:\\server\\invokepy.py\", line 199, in batch\nodfs = mod.azureml_main(*idfs)\nFile \"C:\\temp\\1f7ee68aea7d4914a540db6181eb53c8.py\", line 46, in azureml_main\ntemp1 = frame1.ix[frame1.readmi_class == 'YES', indx]\nFile \"C:\\pyhome\\lib\\site-packages\\pandas\\core\\generic.py\", line 2669, in getattr\nreturn object.getattribute(self, name)\nAttributeError: 'DataFrame' object has no attribute 'readmi_class'\nProcess returned with non-zero exit code 1\n\n---------- End of error message from Python interpreter ----------\nStart time: UTC 07\/25\/2021 22:45:40\nEnd time: UTC 07\/25\/2021 22:46:03\n\nHas anyone encountered this or know of a fix? I appreciate any help you can provide.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how i can recover my compute instance ?",
        "Question_creation_time":1655132409567,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/887255\/how-i-can-recover-my-compute-instance.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"error: The specified Azure ML Compute Instance cs-bi-cloud2 encountered an unusable node. Please try to restart the compute instance to recover. If it failed at creation time, please delete and try to recreate the compute instance. If the problem persists, please follow up with Azure Suppor",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AzureML Error on Linux: \"Unable to retrieve .NET dependencies. Please make sure you are connected ...\"",
        "Question_creation_time":1618767312437,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/361522\/azureml-error-on-linux-34unable-to-retrieve-net-de.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am getting this error on a Linux box (Gentoo w\/ .NET via Mono properly installed)\n\n\"Unable to retrieve .NET dependencies. Please make sure you are connected to the Internet and have a stable network connection.\"\n\nThe error is triggered when creating a dataset from a directory using\n\n\"dataset = Dataset.File.from_files(path=(datastore, path_to_dataset_in_datastore))\"\n\nSome system info:\nPython: 3.8.8.\nazureml-automl-core 1.26.0\nazureml-core 1.26.0\nazureml-dataprep 2.13.2\nazureml-dataprep-native 32.0.0\nazureml-dataprep-rslex 1.11.2\nazureml-dataset-runtime 1.26.0\nazureml-pipeline 1.26.0\nazureml-pipeline-core 1.26.0\nazureml-pipeline-steps 1.26.0\nazureml-sdk 1.26.0\nazureml-telemetry 1.26.0\nazureml-train 1.26.0\nazureml-train-automl-client 1.26.0\nazureml-train-core 1.26.0\nazureml-train-restclients-hyperdrive 1.26.0\n\n.NET Info:\nMono JIT compiler version 6.6.0.161 (tarball Sat Apr 10 16:41:12 PDT 2021)\nCopyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com\nTLS: __thread\nSIGSEGV: altstack\nNotifications: epoll\nArchitecture: amd64\nDisabled: none\nMisc: softdebug\nInterpreter: yes\nLLVM: supported, not enabled.\nSuspend: hybrid\nGC: sgen (concurrent by default)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Experiment on Azure Machine Learning services is not starting.",
        "Question_creation_time":1620678944347,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/389811\/experiment-on-azure-machine-learning-services-is-n.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I run an experiment in the azure ml service using an associated VM, but the experiment status is as follows:\n\nJob runstatus is NotStarted\n\nID execution 75ff42c9-2fbd-48c2-beec-b72aa38f1d00",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ml notebooks sharing and compute selection.",
        "Question_creation_time":1591956129183,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/35432\/azure-ml-notebooks-sharing-and-compute-selection.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What kind of collaboration do we need among the data scientists or developers who need to share these notebooks? What kind of compute does these notebooks require? Is it all single node?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Out-of-memory error webservice deployed with Azure ML Studio",
        "Question_creation_time":1592827334117,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/38509\/out-of-memory-error-webservice-deployed-with-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a webservice which exposes a predictive model. It has been deployed with Auzure ML Studio. Since the last model re-training and webservice deployment, in circa 1% of the cases in production, I get the following out-of-memory (possibly correlated) errors:\n\n1) \"The model consumed more memory than was appropriated for it. Maximum allowed memory for the model is 2560 MB. Please check your model for issues.\"\n2) \"The following error occurred during evaluation of R script: R_tryEval: return error: Error: cannot allocate vector of size 57.6 Mb\"\n\nPlease note that these errors occur exclusively while trying to consume the webservice, and not while model training, evaluation and deployment.\n\nAlso, consuming the webservice in batch mode, as suggested here, is not a viable option for our business use case.\n\nIs there a way to increase the memory limit for Azure webservices?\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About the end of Machine Learning Studio (classic)#2",
        "Question_creation_time":1636119906810,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/616952\/about-the-end-of-machine-learning-studio-classic2.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"hello.\n\nI am currently using Machine Learning Studio (classic).\n\n'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.\n\nIs the following interpretation correct?\n\nThings you can't do from 1 December 2021\n-Creating a workspace for Machine Learning Studio (classic)\n-Creating a web service plan for Machine Learning Studio (classic)\n\nWhat you can do until 1 December 2021\n-Creating new Machine Learning Studio (classic) experiments\n-Creating new Machine Learning Studio (classic) trained models\n-Creating a new Machine Learning Studio (classic) web service",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issue with data lake mounting in custom RStudio application Azure ML",
        "Question_creation_time":1661155798650,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/976098\/install-rstudio-application-in-azure-ml-vm.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"previously while creating a compute instance we were able to see RStudio application by default and we were able to mount\/access the data lake from RStudio.\n![compute creation][1]\n\n\n\n\n![Data lake mont][2]\n2. In current situation we are not able to access RStudio application by default.\n![234345-4.png][3]\n3.with the help of below link we are able to create custom RStudio application\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio\n![custom RStudio app][4]\n\n4.In custom RStudio we are not able to mount\/access the data lake.\n\n![missing data lake][5]\n\nIs there way to mount\/access the data lake in custom RStudio app\n[1]: \/answers\/storage\/attachments\/234344-screenshot-2022-08-23-170502.png\n[2]: \/answers\/storage\/attachments\/234353-5.png\n[3]: \/answers\/storage\/attachments\/234345-4.png\n[4]: \/answers\/storage\/attachments\/234314-2.png\n[5]: \/answers\/storage\/attachments\/234361-3.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"The file limit in Azure ML studio",
        "Question_creation_time":1647197880180,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/770207\/the-file-limit-in-azure-ml-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, when I submitted my work in Azure ML studio, it showed an error like this.\n\n\n\n\n\nMy folder contains around 50000 files, and even if I uploaded my files onto the datasets, I still need to download them to my current workplace. Is there a way to directly use the file from datasets or increase the snapshot size? I really appreciate any help you can provide.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":5.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AzureML model predictions do not match scored model",
        "Question_creation_time":1644551791223,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/731481\/azureml-model-predictions-do-not-match-scored-mode.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have deployed a model to an endpoint but the predictions that come back do not match what is output from the scored model in the experiment.\n\nThe model is a Multiclass Neural Network for classifying emails - it takes approx. 1,000 elements of text and puts them into one of about 18 possible categories.\n\nReviewing the scored model outputs in the experiment, the accuracy is reasonable; however once deployed it always predicts any input into a single category with near 100% confidence. The input is correctly formatted (this is validated in the model output) so I'm assuming something is misconfigured in the model deployment, but I'm not seeing any indication as to what.\n\nAppreciate any help from MSFT in investigating this issue.\n\nCheers, James",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML Studio and Private Endpoint issue",
        "Question_creation_time":1646161833773,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755259\/ml-studio-and-private-endpoint-issue.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We are trying to setup a Machine Learning Workspace and only have it accessible via Private Endpoint Connection. When we have it this setup and try to connect from our company workstations it loads the page but we get an \"Error Loading recent runs\"\n\n\n\n\n\nIf we run this from a VM inside Azure it is fine.\n\nWe do have a VPN Gateway set up to access our on-prem which works for other Vnets we have in Azure. We peered the VNET that ML sits in with VNET where Gateway network is setup. unfortunately we had a 3rd party set up the original connection and did not fully document. We have tried to match all settings we have in the working VNET with the ML Vnet but can't seem to see what we are missing. I also can't seem to find what logs to check to see where connections are being blocked.\nHoping I explained our issue will enough.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot find created compute instance",
        "Question_creation_time":1642537079270,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/700065\/cannot-find-created-compute-instance.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have created a compute instance and it is not available at the compute instances list. Instead it shows a \"create new\" button.\nI can see the instance at usage+quotas but when I click on it , it says the compute instance cannot be found.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Question_creation_time":1631251067517,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/546760\/machine-learning%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E8%B3%AA%E5%95%8F.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"When using AutoML for forecasting, is it possible to include lagged exogenous features?",
        "Question_creation_time":1657059623547,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/915375\/when-using-automl-for-forecasting-is-it-possible-t.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In the documentation, it states \"When training a model for forecasting future values, ensure all the features used in training can be used when running predictions for your intended horizon. For example, when creating a demand forecast, including a feature for current stock price could massively increase training accuracy. However, if you intend to forecast with a long horizon, you may not be able to accurately predict future stock values corresponding to future time-series points, and model accuracy could suffer,\" which seems to imply only features that are known or can reasonably be estimated in the future should be used. This seems like a pretty severe limitation. Is it really not possible to include exogenous features that should be lagged like the target is?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"my script stops running without any message explaining the reason",
        "Question_creation_time":1634306015143,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/592153\/my-script-stops-running-without-any-message-explai.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Please see the screenshots below. Once it said terminated but without reason:\n\nThe other time there was nothing just stopped:",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning and jupyterlab git extension not working",
        "Question_creation_time":1594716551237,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/46614\/azure-machine-learning-and-jupyterlab-git-extensio.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\n\n\n\nI need some help trying to understand why I can't see any GIT options (left panel and top selection drop down menu) in my Azure machine learning JupyterLab.\n\n\n\n\nI did the following steps:\n\n\n\n jupyter labextension install @jupyterlab\/git\n pip install --upgrade jupyterlab-git\n jupyter serverextension enable --py jupyterlab_git\n jupyter lab build\n\n\n\n\nI've restarted my jupyterLab a couple of times, if I check the command:\n\n\n\n jupyter labextension list\n\n\n\n\nI get that @jupyterlab\/git v0,20,0 is enabled and ok.\nWhat am I doing wrong?\n\n\n\n\nThank you in advance,\nCarla",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":5.0,
        "Question_follower_count":37.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azureml: Metadata mismatch for dask dataframe after using filter()",
        "Question_creation_time":1647425925947,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/774453\/azureml-metadata-mismatch-for-dask-dataframe-after.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I noticed weird behaviour when filtering an azureml TabularDataset instance using filter() and converting it to a dask dataframe afterwards. Here is my code to recreate the issue:\n\nImports:\n\n from azureml.core import Dataset\n from azureml.data import TabularDataset\n import dask.dataframe as ddf\n import pandas as pd\n\n\n\nRegister a dask dataframe to the datastore and load it as a TabularDataset:\n\n test_df = pd.DataFrame({\"id\": [3,4,5], \"price\": [199, 98, 50]})\n test_dask = ddf.from_pandas(test_df, chunksize=1)\n    \n Dataset.Tabular.register_dask_dataframe(test_dask, datastore, name='bug_test')\n dataset = TabularDataset.get_by_name(workspace, name='bug_test')\n\n\n\nNow printing the loaded dataset after converting it to dask dataframe works (almost) well (almost, since there is weird indexing as the 0 appears two times):\n\n loaded_dask = dataset.to_dask_dataframe()\n print(loaded_dask.compute())\n >> \n           id  price  __null_dask_index__\n        0   3    199                   0\n        0   4     98                    1\n        1   5     50                    2\n\n\n\nWe now want to filter for rows where id equals to 5, which works perfectly when it is filtered after converting it to dask dataframe with loaded_dask[loaded_dask.id == 5].compute()\n\nNow computing the dask dataframe after filtering with the filter() method throws following exception, either no data or no datatype is found (for full error message see below):\n\n filtered_ds = dataset.filter(dataset[\"id\"] == 5)\n filtered_ds.to_dask_dataframe().compute()\n >> \n     ValueError: Metadata mismatch found in `from_delayed`.\n     Partition type: `pandas.core.frame.DataFrame`\n     +-----------------------+-------+----------+\n     | Column                | Found | Expected |\n     +-----------------------+-------+----------+\n     | '__null_dask_index__' | -     | int32    |\n     | 'id'                  | -     | int32    |\n     | 'price'               | -     | int32    |\n     +-----------------------+-------+----------+\n\n\n\nNote: When filtering for invalid values, e.g. for dataset[\"id\"] == 6 it correctly returns me an empty dataframe\n\nAlso a weird behaviour happens when playing around with the dtypes parameter in to_dask_dataframe(). When specifying types for only one column, datatypes can suddenly be found:\n\n filtered_ds.to_dask_dataframe(dtypes={\"id\": \"object\"}).compute()\n >>\n     ValueError: Metadata mismatch found in `from_delayed`.\n    \n     Partition type: `pandas.core.frame.DataFrame`\n     +-----------------------+-------+----------+\n     | Column                | Found | Expected |\n     +-----------------------+-------+----------+\n     | '__null_dask_index__' | int64 | -        |\n     | 'id'                  | int64 | object   |\n     | 'price'               | int64 | -        |\n     +-----------------------+-------+----------+\n\n\n\nbut setting dtypes={\"id\": \"int64\", \"price\": \"int64\", \"__null_dask_index__\": \"int64\"} leads again to the same ValueError as before that either no data or no datatype is found (full error ouput):\n\n filtered_ds.to_dask_dataframe(dtypes={\"id\": \"int64\", \"price\": \"int64\", \"__null_dask_index__\": \"int64\"}).compute()\n >>\n     Traceback (most recent call last):\n       File \"\\bug_analysis.py\", line 117, in <module>\n         filtered_ds.to_dask_dataframe(dtypes={\"id\": \"int64\", \"price\": \"int64\", \"__null_dask_index__\": \"int64\"}).compute()\n       File \"\\venv\\lib\\site-packages\\dask\\base.py\", line 290, in compute\n         (result,) = compute(self, traverse=False, **kwargs)\n       File \"\\envs\\venv\\lib\\site-packages\\dask\\base.py\", line 573, in compute\n         results = schedule(dsk, keys, **kwargs)\n       File \"\\venv\\lib\\site-packages\\dask\\threaded.py\", line 81, in get\n         results = get_async(\n       File \"\\venv\\lib\\site-packages\\dask\\local.py\", line 506, in get_async\n         raise_exception(exc, tb)\n       File \"\\venv\\lib\\site-packages\\dask\\local.py\", line 314, in reraise\n         raise exc\n       File \"\\venv\\lib\\site-packages\\dask\\local.py\", line 219, in execute_task\n         result = _execute_task(task, data)\n       File \"\\venv\\lib\\site-packages\\dask\\core.py\", line 119, in _execute_task\n         return func(*(_execute_task(a, cache) for a in args))\n       File \"\\venv\\lib\\site-packages\\dask\\dataframe\\utils.py\", line 407, in check_meta\n         raise ValueError(\n     ValueError: Metadata mismatch found in `from_delayed`.\n     Partition type: `pandas.core.frame.DataFrame`\n     +-----------------------+-------+----------+\n     | Column                | Found | Expected |\n     +-----------------------+-------+----------+\n     | '__null_dask_index__' | -     | int64    |\n     | 'id'                  | -     | int64    |\n     | 'price'               | -     | int64    |\n     +-----------------------+-------+----------+\n\n\n\n\nThe exceptions are raised when the dask dataframes are computed with compute().\n\nI am aware that I used two experimental methods ( TabularDataset.to_dask_dataframe() and TabularDataset.filter() ), so is this a bug or am I using the methods incorrectly at some point?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Compose model",
        "Question_creation_time":1669041105107,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1098169\/compose-model.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"As the document\nA composed model is created by taking a collection of custom models and assigning them to a single model ID. You can assign up to 100 trained custom models to a single composed model ID. When a document is submitted to a composed model, the service performs a classification step to decide which custom model accurately represents the form presented for analysis.\n\nWhat\u2019s the price for the classification step?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MS Azure Machine Learning: MemoryError: Unable to allocate 5.43 GiB for an array with shape (23847, 30582) and data type int64",
        "Question_creation_time":1615935222203,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/317531\/ms-azure-machine-learning-memoryerror-unable-to-al.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to extract pixel values from a raster image using xarray module. I tried to \"stack\" the coordinates to get a third dimension but I end up getting the error above. I create a compute instance of 56GB RAM so I was wondering why the 5.43 GiB, I would have expected going beyond 56GB but the values seems off.\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Argument pass to Docker Container Azure ML inference",
        "Question_creation_time":1632856020243,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569816\/custom-argument-pass-to-docker-container-azure-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello Team,\n\nI'm trying to pass the arguments to Azure ML docker. I have created an environment like this.\n\n env = Environment.from_conda_specification(name='pytorch-1.6-gpu', file_path='curated_env\/conda_dependencies.yml' )\n\n\n\nAm I passing the arguments correct?\n\n DOCKER_ARGUMENTS = [\"--shm-size\",\"32G\"]  # increase shared memory\n env.docker.arguments = DOCKER_ARGUMENTS\n\n\n\n\nThe main goal of this project is to deploy a model on the AKS inference cluster. I have successfully deployed the model. When I try to get predictions from the model I got this error\n\nIt is possible that data loaders workers are out of shared memory. Please try to raise your shared memory limit\n\nHow can I do that if that's not the correct way to pass arguments?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Importing Data in Azure ML Studio Experiment",
        "Question_creation_time":1614362680897,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/291213\/importing-data-in-an-experiment-issue.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I\u2019m having an issue importing the data into my Machine Learning Studio. It shows me a Red Cross with the error 0030 - which means that there\u2019s an issue in downloading the data. For background, I\u2019m importing data from the Web URL via HTTP option. I looked up the issue on the troubleshooting page, followed the advice, which shows I\u2019ve done everything correctly. My data link works perfectly fine in my browser. When I enter the http link into my browser, it immediately downloads the csv file. However, my studio is not downloading the data. Importing the data is the first step in my experiment, and I can\u2019t move forward without it. Immediate help would be greatly appreciated! I\u2019ve attached pictures for reference. [1]: \/answers\/storage\/attachments\/72499-0ebb78a4-4805-46e8-a7f1-fbf99682af5f.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Bug report: can't parametrize # hidden node or momentum in NN regression model",
        "Question_creation_time":1594758142250,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/47011\/bug-report-cant-parametrize-hidden-node-or-momentu.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm using the NN regression model in Azure ML Designer, but it appears to have a bug. If I switch trainer mode to ParameterRange, it allows me to use a semicolon-separated list of hyperparameters for learning rate and epochs. However, I'm unable to use a semicolon-separated list of hyperparameters for number of hidden nodes or momentum. Since the number of hidden nodes is the most commonly tuned hyperparameter for a MLP, this module doesn't seem particularly useful if I want to use it to tune hyperparameters.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":38.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No module named 'xgboost'",
        "Question_creation_time":1597377002880,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/66628\/no-module-named-39xgboost39.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello there,\n\nI'm seeing an error - No module named 'xgboost' when attempting to deploy a model using python SDK. Here is my conda yaml file. What am I missing?\nUnable to get the end point to generate an output.\n\nname: project_environment\ndependencies:\n# The python interpreter version.\n# Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\npip:\n\n\ninference-schema\n\n\nazureml-defaults\n\n\nazureml-explain-model\n\n\nnumpy>=1.16.0,<1.17.0\n\n\npandas>=0.21.0,<=0.23.4\n\n\nscikit-learn>=0.19.0,<=0.20.3\n\n\npy-xgboost\n\n\nfbprophet==0.5\n\n\nholidays==0.9.11\n\n\npsutil>=5.2.2,<6.0.0\n\n\nxgboost\n\n\nazureml-sdk[notebooks,automl]\nchannels:\n\n\nanaconda\n\n\nconda-forge",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":1.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Types of Regression Algorithm",
        "Question_creation_time":1608719172483,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/208685\/types-of-regression-algorithm.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What are the types of Regression algorithm? Are there any kinds of regression called \"non-linear regression\"?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does AzureML choose the node to launch a run?",
        "Question_creation_time":1605888033847,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/170198\/how-does-azureml-choose-the-node-to-launch-a-run.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a question related with runs, compute clusters and vcpu\/memory boundaries for specific run.\n\nI have this mindset that a run executes over an environment (a docker image) in a specific compute target. My idea is, if there are resources (vcpu and memory) available on a node, so the run executes in that node. But I always see that a node only executes one run each time, independently the node sku of compute cluster. It seams, AzureML chooses always a idle node to launch a run.\n\nDoes a node always execute one, and not more than one, run in the same time?\nWhat are resources boundaries for run? Or the run might spread over all node resources?\nIf there is boundaries, the default boundaries is static or configurable through environment property `arguments` for docker run?\n\nWhat the following section in run raw json means?\n\n \"containerInstance\": {\n   \"region\": null,\n   \"cpuCores\": 2,\n   \"memoryGb\": 3.5\n },\n\n\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I register ADLS as datastore in AMLW (via cli) corrcetly?",
        "Question_creation_time":1669041298113,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1098193\/how-do-i-register-adls-as-datastore-in-amlw-via-cl.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi!\n\nI'm trying to create a datastore from an ADLS (Gen2) using azure cli (using version 2.42), with credentials using service principal. The service principal is added as Storage Blob Data Reader to my ADLS. I use the following schema (with XXX replaced by correct details), file is named create-datastore-azure-adls.yml .\n\n$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/azureDataLakeGen2.schema.json\ntype: azure_data_lake_gen2\nname: XXX\ndescription: Datastore, ADLS and service principal\naccount_name: XXX\nfilesystem: XXX\ncredentials:\ntenant_id: XXX\nclient_id: XXX\nclient_secret: XXX\n\nand run\naz ml datastore create --file create-datastore-azure-adls.yml --workspace-name $WORKSPACENAME --resource-group $RESOURCENAME --subscription $SUBSCRIPTIONID\n\nThe datastore ends up in my workspace but I can't read from the datastore. When I look at it in the workspace it is not connected to any subscription-id nor resource group (see image).\n\nHowever, if I choose update authentication and fill in subscription-id and resource group everything works. So my question is if there is any way I can't do it correctly, only using the cli, eg. adding this info (subscription-id and rg-name) to the schema? So I don't have to update authentication in the workspace every time :)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":20.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issue in my MLOPs CD pipeline.",
        "Question_creation_time":1649140550093,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/800334\/issue-in-my-mlops-cd-pipeline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Deploying my model to ACI takes forever and fails without any error message. In the ML workspace, the status of the deployed endpoint is unhealthy. I checked common errors while deployment but could not solve the problem. Pleas help. The deployment is never successful and it keeps running.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Invalid graph - invalid dataset",
        "Question_creation_time":1595587512910,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/51962\/invalid-graph-invalid-dataset.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Invalid graph - Invalid dataset",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":1.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can you create an MLTable Dataset (Tabular Format) from a CSV file?",
        "Question_creation_time":1665800219957,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1049085\/can-you-create-an-mltable-dataset-tabular-format-f.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have collected data that I'd like to use in AutoML to see if there's a pattern that can be derived from the data. According to docs, the data needs to be uploaded in the MLTable format in order to be used in AutoML. However, my data is currently in CSV format. There are fairly clear instructions on how to create an MLTable data asset, but the instructions assume your data starts in MLTable format (see the Note: \"The path points to the folder containing the MLTable artifact\"). I can't seem to find any instructions on how to create the MLTable artifact\/folder in the first place, so I can't follow these instructions. Do these instructions exist somewhere? If so, is it possible for someone to link me to it or describe this process? Thank you!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Datastore workspaceblobstore access failed, ErrorCode: ResourceNotFound using AutoML.",
        "Question_creation_time":1634120760290,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/588934\/datastore-workspaceblobstore-access-failed-errorco.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Why remote run is getting \"Datastore workspaceblobstore access failed, ErrorCode: ResourceNotFound\" when using automl?\n\nI've attached datastore to workspace and I'm able to create Tabular dataset using blob URL.\n\nBut it crashes when submittting experiment with auto ml.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Score Recommender system - Item Recommendation failed to run",
        "Question_creation_time":1626681386490,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/480658\/score-recommender-system-item-recommendation-faile.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am getting an error when trying to run the Score Recommender system for Item Recommendation.\n\nThe status details is 'Failed to run task; exceeded retry count for operation'. Can someone help to advise on this error?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wheres my component",
        "Question_creation_time":1667251682350,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1070006\/wheres-my-component.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi expert, I am struggling in a bug, I can\u2019t see anything in my studio and it shows empty, how can I fixed it. Anyone else experience this or is this a bug.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"Explanation_dataTransform: e.every is not a function\" Error AutoML Explanation Preview",
        "Question_creation_time":1619105132597,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/367955\/34explanation-datatransform-eevery-is-not-a-functi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Although child run of the explanation is completed successfully, we are not able to see the explanation preview. We would appreciate if you could help us.\n\nBest regards,\n\nCagatay Topcu",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to migrate a completed Azure data labelling project to Custom Vision ?",
        "Question_creation_time":1643343651050,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/713444\/is-it-possible-to-migrate-a-completed-azure-data-l.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I've recently labelled approx. 3500 images for a multi-class classification project and wondered if I could use the Custom Vision service to train the data (for a prototype demo) ?\n\nIf not, is there a way to access the model created using the Auto Labelling feature and use it as an end point (In Javascript) ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Compute Instances List is not displaying any of my previously create instance",
        "Question_creation_time":1615936137423,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/317533\/compute-instances-list-is-not-displaying-any-of-my.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Compute Instances List is not displaying any of my previously create instance\n\nwhen i log in and go to the machine learning studio and click on the link it display an error.\n\nIts like the list is timing out\n\nI have tried this on multiple machines\n\nerror\n\n'Failed to load computes Your request for data wasn\u2019t sent. Here are some things to try: Check your network and internet connection, make sure a proxy server is not blocking your connection, follow our guidelines if you\u2019re using a private link, and check if you have AdBlock turned on. Trace ID : 6c0087da-5c17-4aea-814b-59d8292caa5b Client request ID : fa33bb7f-d4dd-4975-a538-cb119b7a2d64 '",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How is Azure ML instance cost calculated?",
        "Question_creation_time":1606732706930,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/180023\/how-is-azure-ml-instance-cost-calculated.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI've created an endpoint for scoring using a modified version of this tutorial: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model. I used this command for specifying resources: AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=12) and my resource group is \"West EU\". I'd like to know how the cost is calculated. I assume that the requested resources are converted to an instance (is that right?). I have found this useful website calculator but I cannot find which instance I am using.\n\nHow can I retrieve the information? Can I also do it programatically?\n\nMany thanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Stream Analytics: ML Service function call in cloud job results in no output events",
        "Question_creation_time":1594918621430,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/48592\/azure-stream-analytics-ml-service-function-call-in.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey,\nI've got a problem with an Azure Stream Analytics (ASA) job that should call an Azure ML Service function to score the provided input data.\nThe query was developed und tested in Visual Studio (VS) 2019 with the \"Azure Data Lake and Stream Analytics Tools\" Extension.\nAs input the job uses an Azure IoT-Hub and as output the VS local output for testing purposes (and later even with Blobstorage).\nWithin this environment everything works fine, the call to the ML Service function is successfull and it returns the desired response.\nUsing the same query, user-defined functions and aggregates like in VS in the cloud job, no output events are generated (with neither Blobstorage nor Power BI as output).\nIn the ML Webservice it can be seen, that ASA successfully calls the function, but somehow does not return any response data.\nDeleting the ML function call from the query results in a successfull run of the job with output events.\n\nFor the deployment of the ML Webservice I tried the following (working for VS, no output in cloud):\n\nACI (1 CPU, 1 GB RAM)\n\n\nAKS dev\/test (Standard_B2s VM)\n\n\nAKS production (Standard_D3_v2 VM)\n\nThe inference script function schema:\n\ninput: array\n\n\noutput: record\n\n\n\n\n\n\n\nThe ASA job subquery with ML function call:\n\n\"Sequence\" is a subquery that aggregates the data into sequences (arrays) with an user-defined aggragate.\n\nI hope the provided information is sufficient and you can help me.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":37.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning cost",
        "Question_creation_time":1607408839580,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/190053\/azure-machine-learning-cost.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI am new to Azure Machine Learning, I have some questions related to cost\n\n1) What will be the cost breakup? (Workspace charges, Network charges, Disk Charges etc.)\n\n2) I saw that there were some charges for Bandwidth and Load balancer. Why is it getting charged and I am not able to see the details in Azure ML?\n\n3) I have a SQL Sever which is inside a Vnet (VPN Gateway) and I want to integrate it with Azure Machine Learning and use the data for my analysis. What all will be the charges for it?\n\n4) Is there a way to stop the computes automatically when i am not using it, as i dont want to be charged when i am not using the Azure Machine learning?\n\n5) Will there be any charges even though i am not using the Azure Machine learning? (Static Charges)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How AI\/Machine learning works in translator",
        "Question_creation_time":1632270279733,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/561221\/how-aimachine-learning-works-in-translator.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am an undergraduate community college student in Richmond, VA.\n\nI am wondering if you can help me understand how the translator works or if you can connect me with another expert who might be able to help me learn exactly how google translate works.\n\nQuestions:\n\nIf you enter the same phrase into the translator multiple times, does the program learn from it? If so, how?\n\nDoes the translator take what people enter into the translator, store it and use it to output a common translation?\n\nHere is a scenario. Let's say I wrote something in english and then translated it word for word into farsi. Then, I took my farsi work, inputed into google translate, and the output was a translation almost word for work with my english translation. IF I keep doing that, would the translator \"deep learn\" and correct itself so that it exactly matched my english translation? What about if I kept hitting the reverse arrow, and changing a word here or there, until both the farsi and english sentences inside the translator, match exactly what I originally wrote? Would the translator learn from that? In that way could I have \"taught\" the translator the most common way to interpret sentences in either language? Is there a way to prove that someone used a translator to write something? IF someone is trying to prove that someone else used a translator to create a literary work, instead of writing it themselves, can they fairly say that the translator's translation matching word for word is indisputable, undeniable proof of their accusation?\n\nVery Respectfully,",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issues with SQL Alchemy whilst deploying real time endpoint on ACI",
        "Question_creation_time":1642105094927,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/694586\/issues-with-sql-alchemy-whilst-deploying-real-time.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI'm trying to deploy an image to an ACI using Azure Machine Learning. One of the requirements\/installations is sqlalchemy.\n\nWhen building the image, sqlalchemy seems to install correctly. However, when I try to import the sqlalchemy modules within the code, the deployment to the ACI fails and I can't work out why.\n\nAnyone had any similar issues - let me know if I need to provide any more info.\n\nThanks,\n\nCam",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"real-time inference pipeline failed to deploy for some unknown error and unable to view the logs",
        "Question_creation_time":1662748337977,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1001503\/real-time-inference-pipeline-failed-to-deploy-for.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"After submitting a successful inference pipeline, I attempted to deploy the model to a container instance. However, it failed and to make it worse I can't see the logs due to forbidden permissions error even though I am sole owner of resource group & instance. To put the nail on the coffin, I also can't view any related container instances inside Azure Portal...only the endpoints in ML studio.\n\nHere's the permissions error:\n\n ![{\n   \"error\": {\n     \"code\": \"Forbidden\",\n     \"message\": \"Forbidden\",\n     \"details\": [\n       {\n         \"code\": \"AuthorizationFailed\",\n         \"message\": \"The client 'df9ec36b-a97d-4c60-a6fe-91048565a571' with object id 'df9ec36b-a97d-4c60-a6fe-91048565a571' does not have authorization to perform action 'Microsoft.ContainerInstance\/containerGroups\/containers\/logs\/read' over scope '\/subscriptions\/7d36b75b-8fd4-4ef9-92fe-69f951afa25d\/resourceGroups\/playground\/providers\/Microsoft.ContainerInstance\/containerGroups\/playground-pipe-ommOzSbRhUSx8qiJGQ4HiA\/containers\/playground-pipe' or the scope is invalid. If access was recently granted, please refresh your credentials.\"\n       }\n     ]\n   },\n   \"correlation\": {\n     \"RequestId\": \"745ad382-74c0-4c67-8853-053807cd6336\"\n   }\n }][1]\n\n\n\n\n[1]: \/answers\/storage\/attachments\/239641-screenshot-2022-09-09-175802.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":6.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Designer python script to save Dataset data as csv to compute instance directories",
        "Question_creation_time":1630351702510,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/533352\/designer-python-script-to-save-dataset-data-as-csv.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have a set of custom Python and R scripts to execute a machine learning pipeline for analysis purposes. The steps are as follows:\n\nWith Python, Convert Datastore data to csv that gets saved in the compute instance directory\n\n\nWith R, pull in the csv data and run some feature engineering, build a machine learning model, and save scoring data to directory as csv\n\n\nPush csv data back to Datastore\n\nThe first issue I'm encountering in Designer is the first step, saving Datastore data as csv to the compute instance directory. I created a function called azureml_main() that internally pulls in the Datastore data and saves it as csv to the directory. I have run the code that's inside the function a bunch of times but when I try to have it run in the Python script node in Designer it fails.\n\nError message:\n\nAmlExceptionMessage:User program failed with FailedToEvaluateScriptError: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nGot exception when invoking script at line 22 in function azureml_main: 'AuthenticationException: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired'.\n---------- End of error message from Python interpreter ----------\n\nModuleExceptionMessage:FailedToEvaluateScript: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nGot exception when invoking script at line 22 in function azureml_main: 'AuthenticationException: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired'.\n---------- End of error message from Python interpreter ----------\n\n\n\n\n\/\/ Python script inside Python node in Designer.\n\/\/ The script MUST contain a function named azureml_main\n\/\/ which is the entry point for this module.\n\nimport pandas as pd\n\n\/\/ The entry point function MUST have two input arguments.\n\/\/ If the input port is not connected, the corresponding\n\/\/ dataframe argument will be None.\n\/\/ Param<dataframe1>: a pandas.DataFrame\n\/\/ Param<dataframe2>: a pandas.DataFrame\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # Azure management\n    from azureml.core import Workspace, Dataset\n    # MetaData\n    subscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\n    resource_group = 'xCloudData'\n    workspace_name = 'xCloudML'\n    # Create workspace \n    workspace = Workspace(subscription_id, resource_group, workspace_name)\n    # 1. Retention_Engagement_CombinedData\n    dataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n    # Save data to file\n    df = dataset.to_pandas_dataframe()\n    df.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n    # 2. TitleNameJoin\n    dataset = Dataset.get_by_name(workspace, name='TitleForJoiningInR')\n    # Save data to file\n    df = dataset.to_pandas_dataframe()\n    df.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/TitleNameJoin.csv')\nazureml_main()",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AssertError:read can not have position excceed buffer length",
        "Question_creation_time":1618316417143,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/355512\/asserterrorread-can-not-have-position-excceed-buff.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi i am getting following error when trying to predict using externally generated R xgboost model in azure ML studio\n\nError 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\nAssertError:read can not have position excceed buffer length",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azuremlsdk for R hangs on update while solving environment",
        "Question_creation_time":1624849818327,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/453836\/azuremlsdk-for-r-hangs-on-update-while-solving-env.html",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I haven't used azuremlsdk for R in a few weeks. When I tried to use it today it said it had to update, the update seems to get stuck on step 17.\n\nThis is the last message before it timeouts in 1h 30m.\n\nStep 17\/23 : RUN conda install -p \/azureml-envs\/azureml_da3e97fcb51801118b8e80207f3e01ad -c r -y r-essentials=3.6.0 rpy2 r-checkpoint && pip install --no-cache-dir azureml-defaults ---> Running in 30337f6502b4 Solving environment: ...working...\n\nI have tried updating miniconda from the anaconda prompt and I've tried deleting my azureml resource and creating a new one. Any ideas what is going on?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":6.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Execure R scripting and R Desktop output varies",
        "Question_creation_time":1626086340790,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/472103\/execure-r-scripting-and-r-desktop-output-varies.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\nExecuting a model with Execute R Script in Azure ML produces a different result compared to R desktop (R 4.1.0)\nHere are step I followed\na. With Azure ML studio ,Imported the 2 data sets Testb.csv and Testc,csv\nand using the 2 datasets , executed the R script ( see AzureMLR.txt)\nand executed the almost same script in R Desktop ( except i had to read from csv files instead from the ports) ( see RDesktopcode.txt)\n\nI see a big difference in output produced by both\nRefer this snapshot where one on left is from AzureML and right is R Desktop\n\nYou may to rename testb.txt and textc.txt as csv files to use them . I could not upload csv files so changed to txt files\n\nTHanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Constantly getting this error while training Deep and Wide model. Model is expected to be fed with features: ['feature_user_feature_2', ....",
        "Question_creation_time":1668240133527,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1086242\/constantly-getting-this-error-while-training-deep.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azure-mgmt-core 1.3.0 (\/azureml-envs\/azureml_1c52c6e25bd3041eabbd9a52168ae46\/lib\/python3.8\/site-packages), Requirement.parse('azure-mgmt-core<2.0.0,>=1.3.1'), {'azure-mgmt-keyvault'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azure-mgmt-core 1.3.0 (\/azureml-envs\/azureml_1c52c6e25bd3041eabbd9a52168ae46\/lib\/python3.8\/site-packages), Requirement.parse('azure-mgmt-core<2.0.0,>=1.3.1'), {'azure-mgmt-keyvault'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azure-mgmt-core 1.3.0 (\/azureml-envs\/azureml_1c52c6e25bd3041eabbd9a52168ae46\/lib\/python3.8\/site-packages), Requirement.parse('azure-mgmt-core<2.0.0,>=1.3.1'), {'azure-mgmt-keyvault'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (azure-mgmt-core 1.3.0 (\/azureml-envs\/azureml_1c52c6e25bd3041eabbd9a52168ae46\/lib\/python3.8\/site-packages), Requirement.parse('azure-mgmt-core<2.0.0,>=1.3.1'), {'azure-mgmt-keyvault'}).\nSession_id = 84c324df-90e3-4d06-963d-c896854583\nInvoking module by urldecode_invoker 0.0.8.\n\nModule type: custom module.\n\nUsing runpy to invoke module 'azureml.designer.modules.recommendation.dnn.wide_and_deep.train.run'.\n\n2022-11-06 17:12:43.374707: W tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/azureml-envs\/azureml_1c52c6e25bd041eabbd9a52168ae46\/lib:\/usr\/local\/nvidia\/lib:\/usr\/local\/nvidia\/lib64:\/usr\/local\/cuda\/lib64:\/usr\/local\/cuda\/extras\/CUPTI\/lib64\n2022-11-06 17:12:43.374762: I tensorflow\/stream_executor\/cuda\/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-11-06 17:12:45,992 studio.common INFO azureml-designer-recommender-modules 0.0.54\n2022-11-06 17:12:51,404 studio.core INFO preprocess_transactions - Start:\n2022-11-06 17:13:06,585 studio.core INFO preprocess_transactions - End with 15.1769s elapsed.\n2022-11-06 17:13:06,589 studio.core INFO preprocess_features - Start:\n2022-11-06 17:13:06,607 studio.core INFO preprocess_features - End with 0.0176s elapsed.\n2022-11-06 17:13:06,607 studio.core INFO preprocess_features - Start:\n2022-11-06 17:13:06,666 studio.core INFO preprocess_features - End with 0.0583s elapsed.\n2022-11-06 17:13:12,074 studio.common INFO Get 10 features\n2022-11-06 17:13:12,166 studio.common INFO Create feature metas for 10 features\n2022-11-06 17:13:14,412 studio.common INFO Get 1 features\n\/azureml-envs\/azureml_1c52c6e25bd3041eabbd9a52168ae46\/lib\/python3.8\/site-packages\/pandas\/core\/generic.py:6245: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\nself._update_inplace(new_data)\n2022-11-06 17:13:14,466 studio.common INFO Create feature metas for 1 features\n2022-11-06 17:13:14,500 studio.common DEBUG Init train input function builder.\n2022-11-06 17:13:14,503 studio.common INFO Build 10 features for User ids.\n2022-11-06 17:13:17,704 studio.common INFO Process null values for features.\n2022-11-06 17:13:17,736 studio.common INFO Build 1 features for Item ids.\n2022-11-06 17:13:20,012 studio.common INFO Process null values for features.\n2022-11-06 17:13:26,398 studio.module INFO Get 5775792 training instances, and 90247.0 batches per epoch.\n2022-11-06 17:13:29,489 studio.module INFO Build model:\nEpochs: 15\nBatch size: 64\nWide optimizer: OptimizerSelection.Adagrad\nWide learning rate: 0.1\nDeep optimizer: OptimizerSelection.Adagrad\nDeep learning rate: 0.1\nHidden units: (256, 128)\nActivation function: ActivationFnSelection.ReLU\nDropout: 0.8\nBatch norm: True\nCrossed dimension: 1000\nUser embedding dimension: 16\nItem embedding dimension: 16\nCategorical feature embedding dimension: 4\n2022-11-06 17:13:29,546 studio.module INFO Model is expected to be fed with features: ['feature_user_feature_2', 'feature_user_feature_8', 'feature_user_feature_3', 'feature_user_feature_9', 'feature_user_feature_4', 'feature_item_feature_0', 'User', 'Item', 'feature_user_feature_5', 'feature_user_feature_1', 'feature_user_feature_6', 'feature_user_feature_0', 'feature_user_feature_7']\n2022-11-06 17:13:30,077 tensorflow INFO Using config: {'_model_dir': '\/tmp\/tmp7lhxl5n0\/checkpoints', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': 1353705.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\ngraph_options {\nrewrite_options {\nmeta_optimizer_iterations: ONE\n}\n}\n, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 90247.0, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can not use AutoML models",
        "Question_creation_time":1653642599833,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/866814\/can-not-use-automl-models.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI have run an autoML model and in the jobs window of the studio, I can see it worked as I can see all the models generated under different algorithms and their performance. Then in notebook I am trying to retrive the best performing model and I am calling the AUTOML model using:\n\nlocal_run = AutoMLRun(experiment, \"AutoML_5970dd9a-1dae-4e6b-90ff-47878565822f_0\",outputs=None)\n\nwhich works just fine and then:\n\nbest_run, fitted_model = local_run.get_output()\n\nand there is an error that I dont know how to solve: TypeError: the JSON object must be str, bytes or bytearray, not NoneType\n\nSo please your help to solve this!\n\nThank you!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Rest api to create or update azure ML workspace doesn't create dependant resources",
        "Question_creation_time":1591181902550,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31569\/rest-api-to-create-or-update-workspace-doesnt-crea.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Problem with https:\/\/docs.microsoft.com\/en-gb\/rest\/api\/azureml\/workspacesandcomputes\/workspaces\/createorupdate API... In the request body, Is it mandatory to create storage account, app insights, key vault, registration resources before? Ideally since these are dependent resources, shouldn\u2019t it be created as part of workflow creation?\nI get below response when dependent resources are not created prior.\n\n `{\n   \u201cerror\u201d: {\n     \u201ccode\u201d: \u201cValidationError\u201d,\n     \u201cmessage\u201d: \u201cOne or more validation errors occured.\u201c,\n     \u201cmessageFormat\u201d: null,\n     \u201cmessageParameters\u201d: null,\n     \u201creferenceCode\u201d: null,\n     \u201cdetailsUri\u201d: null,\n     \u201ctarget\u201d: \u201cCan not perform requested operation on nested resource. Parent resource \u2018&amp;lt;resourceid&amp;gt;\u2019 not found.\u201c,\n     \u201cdetails\u201d: [],\n     \u201cinnerError\u201d: null,\n     \u201cdebugInfo\u201d: null\n   },\n   \u201ccorrelation\u201d: {\n     \u201coperation\u201d: \u201c&amp;lt;opid&amp;gt;\u201c,\n     \u201crequest\u201d: \u201c&amp;lt;reqid&amp;gt;\u201d\n   },\n   \u201cenvironment\u201d: \u201cwestus\u201d,\n   \u201clocation\u201d: \u201cwestus\u201d,\n   \u201ctime\u201d: \u201c2020-06-03T07:13:14.6463577+00:00&amp;#34;\n }`\n\n\n\n\nI need an API which works similar to https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/workspace?view=azure-cli-latest",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to save and load ML model with Azure Data Factory",
        "Question_creation_time":1623657811393,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/434449\/how-to-save-and-load-ml-model-with-azure-data-fact.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have an Azure Data factory that receives data from a service bus and then I want to classify my data with an ML model.\n\nIs there any solution to save and load the ML model on the Azure Data Factory pipeline?\n\nFor your information, I want to use cloud base solution. I don't use the PICKLE library.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Batchscoring on Batch Endpoint of AutoML model giving error - UserError: {\"NonCompliant\":\"Process",
        "Question_creation_time":1669318257423,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1103764\/batchscoring-on-batch-endpoint-of-automl-model-giv.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"AutoML time-series model created from Sql Database data. - successful\nRegistered Model. - successful\nEndPoints creation - Deploy to Batch End Point. - successful\nCreating Batchscoring Jon in EndPoint - Fails with below error.\n\nError -\n{\"NonCompliant\":\"Process '\/azureml-envs\/azureml_c9e8754bdba5226a1ab803f256ee343b\/bin\/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\n File \\\"driver\/amlbi_main.py\\\", line 184, in <module>\\n main()\\n File \\\"driver\/amlbi_main.py\\\", line 126, in main\\n boot(driver_dir)\\n File \\\"driver\/amlbi_main.py\\\", line 58, in boot\\n booter.start()\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot.py\\\", line 383, in start\\n self.start_sys_main()\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot.py\\\", line 269, in start_sys_main\\n self.run_sys_main(cmd)\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot_node.py\\\", line 111, in run_sys_main\\n self.check_run_result(proc=proc, stdout=stdout or \\\"\\\", stderr=stderr or \\\"\\\")\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot.py\\\", line 218, in check_run_result\\n BootResult().check_result(stdout)\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot_result.py\\\", line 36, in check_result\\n raise Exception(message) from cause\\nException: Run failed, please check logs for details. You can check logs\/readme.txt for the layout of logs.\\n\\n'. Please check the log file 'user_logs\/std_log_0.txt' for more details.\"}\n{\n\"code\": \"ExecutionFailed\",\n\"target\": \"\",\n\"category\": \"UserError\",\n\"error_details\": [\n{\n\"key\": \"exit_codes\",\n\"value\": \"1\"\n}\n]\n}",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":18.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Endpoint Unhealthy error when trying to test deployed endpoint for inference pipeline",
        "Question_creation_time":1669672796280,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1107528\/endpoint-unhealthy-error-when-trying-to-test-deplo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I am following the module from Microsoft Learn on regression using Azure's Machine Learning Designer, when trying to Test the service\n\nhttps:\/\/microsoftlearning.github.io\/AI-900-AIFundamentals\/instructions\/02a-create-regression-model.html#test-the-service\n\nWhen trying to test the predict-auto-price endpoint, I get the following screen\n\n\n\n\n\n\n\nDoes anyone know how I can address this? I am fairly new to Azure, so I apologize if I'm missing something obvious",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How much will it cost me to learn Azure Machine Learning?",
        "Question_creation_time":1589500691030,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/27214\/how-much-will-it-cost-me-to-learn-azure-machine-le.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I&#39;m trying to do the Azure Data Scientist Associate certification. I&#39;m on the first portion of learning exercises and it has me create a Machine Learning Workspace and then its had me create a VM which according to the Azure pricing calculator will cost me $367 a month. If I forget to shut down this VM could I get a bill for this much? Is there a way to have these VM&#39;s automatically shut down? Since I&#39;m only interacting with this VM via a web interface I&#39;m really worried that I&#39;m going to get stuck with some hefty bills after this training. Should I be concerned? The Azure Portal has a cost estimate section but it does not include any of my Machine Learning resources or workspaces so I&#39;m not sure how I can get a realistic estimate for how much this will cost me to complete this training.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error: Incremental refresh of labeling project",
        "Question_creation_time":1607952473720,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/197548\/error-incremental-refresh-of-labeling-project.html",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-labeling-projects#--configure-incremental-refresh\n\nStates incremental refresh picks up new data every 24h.\nTested this on a cat\/dog dataset.\n\nFirst added 3 pictures of dogs in a dataset. Created classification labeling project, with incremental refresh enabled. Labeled 2 images. Updated dataset with 3 images of cats. confirmed updated dataset version ID is used on labeling project. However, images of cats are not included in the labeling project.\n\nAny help?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pathway for code free predictive modeling",
        "Question_creation_time":1592869306553,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/38841\/pathway-for-code-free-predictive-modeling.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am looking at Azure&amp;#39;s training modules and it states I can learn no-code models with Azure, but it also tells me I should know python. I&amp;#39;m a little confused at where I should spend time training in most efficient pathway. My goal is to just do predictive modeling within Azure. I have technical\/IT literacy however coding is at a basic level.\n\nIdeally id like some sort of Certification, if possible from just &amp;#34;Create no-code predictive models with Azure Machine Learning&amp;#34;\n\nIs &amp;#34;Microsoft Certified: Azure Data Scientist Associate&amp;#34; going to require a lot of pre work on python\/torch\/tensor? I&amp;#39;d ideally like Azure to be my entry.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AML Workspace Alerting and Application Insight Dashboard Configuration",
        "Question_creation_time":1660054347030,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/960800\/number-of-run-errors-in-ml-workspace-count-is-upda.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi All,\n\nI would need seach query understanding to configure alert for below metrics for AML Workspace and I also need to show metrics in Application Insight dashboard which are associated with AML.\n\nPlease guide on this.\n\nAlert Description :\n\nMetric Name Unit Description\n1. -Warnings -Count -Number of run warnings in this workspace. Count is updated whenever a run encounters a warning.\n2. -Errors -Count -Number of run errors in this workspace. Count is updated whenever run encounters an error.\n3. -Failed Runs -Count -Number of runs failed for this workspace. Count is updated when a run fails.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Request GPU Quota increase",
        "Question_creation_time":1615999382250,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/319120\/request-gpu-quota-increase.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"For my Machine Learning Experiments I need a dedicated VM with GPU. They are not available in my region (only low priority VMs). The system tells me to request a quota increase but if I go to the quota increase support page I can only select additional CPU's not a GPU for a dedicated VM. How do I request a GPU quota increase?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":6.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Explaining a model in AzureML Studio",
        "Question_creation_time":1623017811457,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/423931\/explicacion-de-un-modelo-en-azureml-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nThere is an issue when I try to explain a Time Series model created with AzureML Studio, with the AutoML service.\n\nWhen the proccess finishes and the best model is automatically explained, I'd like to see a chart with the information of \"Predicted Values vs True Values\" but I can\u00b4t find anything similar. Then I realized that when I click on the explanation of the model, the next message is showed:\n\n\"Las estad\u00edsticas de rendimiento del modelo requieren que se proporcionen los resultados verdaderos adem\u00e1s de los previstos.\"\n\nTranslated: \"The performance statistics of the model require to provide true values in addition to the predicted ones.\"\n\nHow can I provide the model with the true values? I think it should be done automatically by the process, isn\u00b4t it? I mean, true values is part of the dataset, it is in fact the target column.\n\nFurthermore, I tried to create a classification model and I don\u00b4t have that problem there.\n\nAnyone could help me? Without a chart where I can compare true vs predicted values, the model doesn\u00b4t make sense.\n\n\n\n\nThank you very much in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Machine Learning Code",
        "Question_creation_time":1593978457140,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/43081\/machine-learning-code.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey,\n\nI followed your Tutorial: Categorize iris flowers using k-means clustering with ML.NET to develop a porgram on drivers: whether a driver is good or bad.\nI would like that on the console it shows me the 2 groups that it has to form with their properties (for example prenon and last name). all the bad drivers then all the good. How can I do that?\n\nI look forward to your response\nThank you very much",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":37.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to connect to adls gen2 from azure ml workspace",
        "Question_creation_time":1667403446370,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1072809\/unable-to-connect-to-adls-gen2-from-azure-ml-works.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI tried to create datastore in azure ml to connect to adls gen2. I have created app registration and used those client id and secret id. I also assigned the required permission [ storage blob data owner for the client id] in the resource group where the adls gen2 is located\n\nBut still I am not able to access the data. Please suggest me what further I have to do\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Creating a New linked service (Azure Machine Learning)",
        "Question_creation_time":1608828206490,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/210152\/creating-a-new-linked-service-azure-machine-learni.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am getting this below error\n\nRequest sent to Azure ML Service for operation 'validateWorkspace' failed with http status code 'Forbidden'. Error message from Azure ML Service: '{\"error\":{\"code\":\"AuthorizationFailed\",\"message\":\"The client 'f7029fe4-3c72-4258-9195-d4e8a64f7c62' with object id 'f7029fe4-3c72-4258-9195-d4e8a64f7c62' does not have authorization to perform action 'Microsoft.MachineLearningServices\/workspaces\/read' over scope '\/subscriptions\/f9f0926a-ab30-492e-8951-e0b88e6da187\/resourceGroups\/machinelearning_rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/ml_workspace' or the scope is invalid. If access was recently granted, please refresh your credentials.\"}}'. Activity ID: 81e99656-f46a-4375-a7de-6b4fe925c5d5.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Random forests on Azure GPU VM using the SDK",
        "Question_creation_time":1595050125193,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/49008\/random-forests-on-azure-gpu-vm-using-the-sdk.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Can you please share any code examples for training random forests with GPU on Azure using libraries.\nI want to run on the multiple nodes.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":34.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Devops for Data Science Project",
        "Question_creation_time":1620038832133,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/380561\/devops-for-data-science-project.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Team,\nI have a use case wherein some machine learning models will be developed by a team. My team will be developing an application to consume that model. Also we have to do the CI Cd for that models , in such a way that every time the other team uploads a model in a blob storage , pipeline should be triggered and entire application should work the same way if new model performs better than previous one.\nThere is a documentation from microsoft but it is for VSTS.\n\nhttps:\/\/github.com\/Azure\/DevOps-For-AI-Apps\/blob\/jainr-refactor\/Tutorial.md\n\nThe steps mentioned here is exactly what I would like to do but I need the same tutorial for Azure Devops.Since I see many changes in VSTS and Devops portal.\n\n@ChrisPatterson-0930",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Import Data Error",
        "Question_creation_time":1626779418893,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/482751\/import-data-error.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,\nI am a newbie and trying to practice data analysis with the Machine Learning Studio Classic & I use the published data set URL as the input to run the experiment and it did not work successfully as expected (check below for the error please)\n\n*Import Data Error\nError while downloading the file: Error 0039: Error while completing operation: System.Net.WebException: An exception occurred during a WebClient request. ---> Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 0078: Http redirection not allowed . ( Error 0030 )\n\nAnyone can help, please? Appreciate!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"machine learning conda env package(pyenchant)",
        "Question_creation_time":1643330999837,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/713217\/machine-learning-conda-env-packagepyenchant.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have pip install pyenchant, but It doesn't seem to be working.\n\n\n\n\n\n\n\n\n\nIs there any other way?\nhttps:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx\nI looked it up but do not know where to put it\nThanks!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I create a resource group when creating a workspace?",
        "Question_creation_time":1638162185497,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/643801\/how-do-i-create-a-resource-group-when-creating-a-w.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am going through the Azure AI training and need to create a workspace under machine learning. When it asks me to select a resource group there are no options. When I want to create a new resource group it says I dont have permissions under my subscription. What do I need to do?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML online endpoint suddenly returns timeout",
        "Question_creation_time":1666021337190,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1051161\/azure-ml-online-endpoint-suddenly-returns-timeout.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,\n\nWe have deployed a managed online endpoint in Azure ML and first it works fine. However, after a few days, with the exact same request, the endpoint takes much longer to process the request and gives a timeout (the HTTP code returned is 504). We don't understand this behavior since we did not modify the endpoint and the metrics don't show a huge increase in cpu or memory usage. If we restart it then it works again for a few days until it doesn't work anymore. Has anyone faced the same issue? Could you solve it?\n\nThanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Quantify conformity or compliance between different datasets",
        "Question_creation_time":1641225921097,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/681917\/quantify-conformity-or-compliance-between-differen.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\nI have generated regression model using Azure design for a specific dataset of which I know the value to be predicted so I can evaluate the model performance and tune hyper-parameters. But now I would like to apply this trained model to a new and different dataset of which I do not know the values to be predicted during the regression so I cannot quantify the performance of the model for this testing dataset. However, for doing so I would like to see if the testing dataset is representative or similar to the trained dataset to evaluate if the prediction will be accurate. Is there a way in Azure to measure the conformity or compliance of a testing dataset to be similar or comparable to the training dataset?\nThank you!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline does not run new data",
        "Question_creation_time":1633192729123,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/575144\/pipeline-does-not-run-new-data.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi -\nI created and published a pipeline that pulls data from an Azure SQL table, processes, models and then appends the output to an Azure SQL table. The Azure SQL table is updated with new data every day or two. In my script, I want to model on data that has been added two days before today with the following script:\n\nfrom datetime import date, timedelta\nyesterday = date.today() - timedelta(days=2)\nyesterday.strftime(\"%Y-%m-%d\")\nprint(yesterday)\n\nkeep data that is 2 days ago only\n\ndata_prior = data[data['MatterOpenDate'] == str(yesterday)]\nprint(data_prior.head())\n\nwhile True:\nanswer = data_prior.empty\nif answer == False:\nprint('Continue Process')\nbreak\nelif answer == True:\nprint('Empty dataset')\nrun.complete()\nexit()\n\nWhen I first ran my pipeline it worked great. I published this experiment, etc. and created a reoccurring schedule to run once a day every day.\n\nBUT the schedule continues to run the exact same data as the original run even when there is new data being uploaded. Why and what do I need to do for the script to run 'naturally' as written?\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Import ML Model from ADLS to Azure ML using Databricks",
        "Question_creation_time":1642414997297,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/697789\/import-ml-model-from-adls-to-azure-ml-using-databr.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\nI have stored some ml model in my ADLS and I want to register the model to Azure ML using databricks.\nTried to use the following codes to register my ml model but keep encountering an error that the path cannot be found.\n\nimport urllib.request\nfrom azureml.core.model import Model\n\nRegister a model\n\n\n\nmodel = Model.register(model_path = 'dbfs:\/mnt\/machinelearning\/classifier.joblib',\nmodel_name = \"pretrained-classifier\",\ndescription = \"Pretrained Classifier\",\nworkspace=ws)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use custom environment defined in the custom environments tab in Azure Machine Learning Studio.",
        "Question_creation_time":1646779156963,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/764339\/how-to-use-custom-environment-defined-in-the-custo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In Azure Machine Learning Studio, in the Environments section's \"Custom environments\" tab, I defined a custom environment. I have done this once with a Conda yaml file, and once with a requirements.txt file, eg filling out this form as described here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-environments-in-studio\n\nI can see that the environments have been created but I have no idea how to use them.\n\nI have tried using this code within an Azure ML Studio notebook, where the new environment I defined is called \"my_new_env\":\n\n from azureml.core import Workspace, Environment\n ws = Workspace.from_config()\n env = Environment.get(workspace=ws, name=\"my_new_env\")\n\n\n\nI also tried this within an Azure ML Studio notebook to see if I could define an environment without doing it in the environments menu.\n\n from azureml.core.environment import Environment\n my_new_env = Environment.from_conda_specification(name = \"myenv\", file_path = environment.yml)\n\n\n\nBoth execute without any warnings or errors, but I'm not sure they are running, or indeed what I have done.\n\nHaving run either of these two blocks of code, when I try to select a new environment in the Azure ML Studio notebook's drop down menu:\n\nThere is no evidence of my new environments.\n\nI'm new to Azure ML Studio. What I want to do is create a new Python virtual environment. Am I getting confused between virtual Python environments and some other more general type of environments? If I wanted to create my own stable Python virtual environment within Azure ML to use in notebooks that was not tied to a specific compute instance, how would I do it?\n\nThanks!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Question on Azure Features and Limitations for Free and Paid Version",
        "Question_creation_time":1630749021297,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/539984\/question-on-azure-features-and-limitations-for-fre.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI am a student from University of Wollonggong Malaysia KDU Penang. I am planning to do a final year project that utilises machine learning to perform image and handwriting recognition using cloud computing. Therefore I would like to understand if there are related products that are provided on Azure. For the free tier and\/or Azure for student, what are the available product(s) that may satisfy my project requirements and what are its limitations. Similarly for the paid version, what additional features are provided and how are the pricings calculated?\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"RuntimeError: Load model failed - Score machine learning models with PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Question_creation_time":1638981543063,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/656548\/what-is-aml-model-uri-predict-in-serverless-apache-1.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools tutorial-score-model-predict-spark-pool\nI tried to used a model created with AutoML and another from designer and I am getting this error: RuntimeError: Load model failed\n\n\n\n\nI am using the model according to this: https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754\n\nThank you for your help.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Negative Samples in ML Assisted Image Labeling",
        "Question_creation_time":1658973322783,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/945298\/negative-samples-in-ml-assisted-image-labeling.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We are evaluating the Azure ML Assisted Object detection labeling and I have some questions:\n1. How do I mark an image as a negative?\n2. How do I rename a label?\n3. How do I go back to a skipped image?\n4. When labeling if I discover that an image should not be in the dataset, how do I delete it from the dataset? The id of the image is no where to be found.\n5. For an autolabeled image, if I accidentally delete the bounding box, how do I undo this operation?\n6. Sometime the autolabeler creates small bounding boxes without any labels. Is this a bug?\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML experiment run 70- driver log not printing after a few epoches",
        "Question_creation_time":1604880529150,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/155350\/azure-ml-experiment-run-70-driver-log-not-printing.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am training a deep learning artificial neural network model, usually the run submitted to the Experiment will show all the model running logs in 70-driver-log of 'outputs + logs' tab, but starting yesterday, the logs show only a few lines of logs and stop printing. I can still see the model is running since the 'metrics' tab is showing the loss and accuracy results.\n\nAnd another weird thing is that after model training finished, the model is not saved.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"can we run a jupyter notebook using scriptrunconfig on target compute cluster ?",
        "Question_creation_time":1612203390137,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/254301\/can-we-run-a-jupyter-notebook-using-scriptrunconfi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI understood that we can run a python script using scriptrunconfig.\n\nMy question is whether we can run jupyter notebook ?\nWhat other type of scripts can we run ?\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Autoscaling on Azure Machine Learning with R",
        "Question_creation_time":1612696726743,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/262211\/autoscaling-on-azure-machine-learning-with-r.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I have a two part question.\n\nI'm using Azure Machine Learning to train my model using Compute clusters. I'm using F64s v2 VM (64 vCPU and 128 GB of RAM). To be ablo to use all the 64 cores I used the doFuture package in R adding this code before tuning:\n\n registerDoFuture()\n n_cores <- parallel::detectCores()\n plan(\n     strategy = cluster,\n     workers = parallel::makeCluster(n_cores)\n )\n\n\n\nThis works on my Windows 10 machine so I though this would also work on the VM, is this correct?\n\n\n\n\nMy other question is related to the autoscaling of the Compute cluster. The cluster can scale up to three nodes, so I guess it could be 192 cores and 384 of RAM. When monitoring the cluster I was only using one node instead of all three, i.e. there was only one Busy nodes and two Unprovisioned nodes.\nIn the end the code failed with this error message:\nError in unserialize(node$con) :\nFailed to retrieve the value of ClusterFuture (doFuture-1) from cluster SOCKnode #1 (on \u2018localhost\u2019). The reason reported was \u2018error reading from connection\u2019\n\nwhich I think means I'm out of memory, is this correct? If so, why didn't the cluster scale up to three nodes?\n\n\n\n\nSee attached screenshot where only one node is busy but two unprovisioned.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Image build failed. For more details, check log file azureml-logs\/20_image_build_log.txt.",
        "Question_creation_time":1615209277803,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/303859\/image-build-failed-for-more-details-check-log-file.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nWhen we wanted to get an explanation of a model, we received following error. \"Image build failed. For more details, check log file azureml-logs\/20_image_build_log.txt.\"\n\nYou can find the log file attached.\n\nI would appreciate if you could help us to resolve the issue. The model is very successful. It is important for us. Thus, we would like to understand it better with the explanation.\n\nThank you very much in advance for your interest and support!\n\nBest regards,\n\nCagatay Topcu75409-20-image-build-log.pdf",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Select column by name option is faded in edit metadata object in Azure ML studio",
        "Question_creation_time":1653833837490,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/868582\/select-column-by-name-option-is-faded-in-edit-meta.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"As shown in the image, by name option is faded. I have already ran the pipeline and it was successful. Then why not adding another step in pipeline shows the output of imported data ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"undefined symbol: cublasLtGetStatusString at Endpoint Deployment Error",
        "Question_creation_time":1669225633070,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1102061\/undefined-symbol-cublasltgetstatusstring-at-endpoi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi\n\nI am getting this error while deploying an end point. This was working fine for 3 months and I had to rebuild due to a minor change and started getting this error.\n\nFile \"\/azureml-envs\/tensorflow-2.7\/lib\/python3.8\/site-packages\/azureml_inference_server_http\/server\/user_script.py\", line 81, in load_script\nmain_module_spec.loader.exec_module(user_module)\nFile \"<frozen importlib.bootstrap_external>\", line 843, in exec_module\nFile \"<frozen importlib.bootstrap>\", line 219, in call_with_frames_removed\nFile \"\/var\/azureml-app\/221123171914-1667710269\/score.py\", line 6, in <module>\nimport ktrain\nFile \"\/azureml-envs\/tensorflow-2.7\/lib\/python3.8\/site-packages\/torch\/init.py\", line 191, in <module>\nload_global_deps()\nFile \"\/azureml-envs\/tensorflow-2.7\/lib\/python3.8\/site-packages\/torch\/init.py\", line 153, in load_global_deps\nctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\nFile \"\/azureml-envs\/tensorflow-2.7\/lib\/python3.8\/ctypes\/init.py\", line 373, in init_\nself._handle = _dlopen(self._name, mode)\nOSError: \/azureml-envs\/tensorflow-2.7\/lib\/python3.8\/site-packages\/torch\/lib\/..\/..\/nvidia\/cublas\/lib\/libcublas.so.11: undefined symbol: cublasLtGetStatusString, version libcublasLt.so.11\n\n\n\n\nThis is my environment:\n\nFROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.2-cudnn8-ubuntu20.04:20220714.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/tensorflow-2.7\n\nCreate conda environment\n\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\npython=3.8 pip=20.2.4\n\nPrepend path to AzureML conda environment\n\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\nInstall pip dependencies\n\nRUN HOROVOD_WITH_TENSORFLOW=1 pip install 'matplotlib~=3.5.0' \\\n'psutil~=5.8.0' \\\n'tqdm~=4.62.0' \\\n'scipy~=1.7.0' \\\n'numpy~=1.21.0' \\\n'ipykernel~=6.0' \\\n# upper bound azure-core to address typing-extensions conflict\n'azure-core<1.23.0' \\\n'azureml-core==1.43.0' \\\n'azureml-defaults==1.43.0' \\\n'azureml-mlflow==1.43.0.post1' \\\n'azureml-telemetry==1.43.0' \\\n'azureml-inference-server-http==0.7.2' \\\n'pandas==1.4.1' \\\n'ktrain==0.30.0' \\\n'sentence-transformers==2.1.0' \\\n'tensorflow==2.7.0' \\\n'tokenizers==0.10.3' \\\n'protobuf~=3.19.1' \\\n'Flask==2.1.0' \\\n'transformers==4.10.3'\n\nThis is needed for mpi to locate libpython\n\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":36.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to import Microsoft.RelInfra.Common.Exception so that it could be properly handled?",
        "Question_creation_time":1643997368443,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/723386\/how-to-import-microsoftrelinfracommonexception-so.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am working on creating a Pipeline Endpoint and want to handle the exception if incorrect Pipeline endpoint name is passed to the constructor:\n\nPipelineEndpoint.get(workspace, name='xyz')\n\nIn this case, I am seeing a \"Microsoft.RelInfra.Common.Exceptions.ErrorResponseException:PipelineEndpoint name xyz not found in workspace\".\n\nFrom where to import this exception class??\n\nPlease advise.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I have created an ML model and retrained it manually in the designer. While updating the web service, its asking for a score.py file. How do I prepare the score.py file for a model that I have created through designer?",
        "Question_creation_time":1613025843247,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/268585\/i-have-created-an-ml-model-and-retrained-it-manual.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have trained a model , deployed and retrained completely through azure designer pipeline. Now , I need to update the existing web service with the new one after retraining. Its asking for a score.py file in the inference_config (python sdk). How do I prepare a score.py file for an existing model? Any help is appreciated. Thanks a lot :)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pickle Load- File Not Found when deploying using Azure ML Studio",
        "Question_creation_time":1653375158970,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/861537\/pickle-load-file-not-found-when-deploying-using-az.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have saved a classifier model with pickle using the following code-\n\n import pickle\n with open('skm.pickle', 'wb') as fid:\n     pickle.dump(clf, fid) \n\n\n\nNow, during deployment, when I try to load this same model, it is giving an error-\n\n Error:\n {\n   \"code\": \"AciDeploymentFailed\",\n   \"statusCode\": 400,\n   \"message\": \"Aci Deployment failed with exception: Error in entry script, FileNotFoundError: [Errno 2] No such file or directory: '.\/skm.pickle', please run print(service.get_logs()) to get details.\",\n   \"details\": [\n     {\n       \"code\": \"CrashLoopBackOff\",\n       \"message\": \"Error in entry script, FileNotFoundError: [Errno 2] No such file or directory: '.\/skm.pickle', please run print(service.get_logs()) to get details.\"\n     }\n   ]\n }\n\n\n\nThis is the score.py file where I am loading the pickle model and the same file is called during deployment. Also note that, all these files (code, pickle file and related files) are in the same directory.\n\n %%writefile sklearnscore.py\n    \n import json\n import pandas as pd\n from sklearn.preprocessing import MinMaxScaler\n from sklearn.ensemble import RandomForestClassifier\n import pickle\n    \n # Initialize the deployment environment\n def init():\n     # read in the model file\n     from sklearn.pipeline import Pipeline\n     global obj\n     with open('.\/skm.pickle', 'rb') as f:\n         obj = pickle.load(f)\n\n\n\n\nI am registering the model using- model = Model.register(ws, model_name=\"utility15\", model_path=\".\/skm.pickle\")\n\nAnd the deployment code is-\n\n service_name = 'my-custom-env-service-4'\n sklearn_env = Environment.from_conda_specification(name='sklearn-env', file_path='Sklearn.yaml')\n    \n inference_config = InferenceConfig(entry_script='sklearnscore.py', environment=sklearn_env)\n    \n aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=4,tags={'Createdby':'xyz'})\n    \n service = Model.deploy(workspace=ws,\n                         name=service_name,\n                         models=[model],\n                         inference_config=inference_config,\n                         deployment_config=aci_config,\n                         overwrite=True)\n service.wait_for_deployment(show_output=True)\n\n\n\nWhen this script is run, it calls the score.py file and the file not found error comes up for pickle file. I have even tried loading the model without the .\/ thing, but the same error comes up.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Stopping ML Server Engine",
        "Question_creation_time":1612666795983,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/262003\/stopping-ml-server-engine.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm running the Microsoft Machine Learning Server on my computer. Right now, there are no tasks\/nodes running, and task manager is showing multiple instances of the \"Microsoft ML Server Engine\" that are using nearly all my computer resources. I've gone into the administration utility, but can't seem to find a way of stopping this, short of brute \"End Task\" within TM.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I utilize multiple cores on Azure ML Studio VM's?",
        "Question_creation_time":1603703811737,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/139002\/how-can-i-utilize-multiple-cores-on-azure-ml-studi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi. I have a python script which can be run either sequentially or in parallel (using concurrent.futures). On my local machine using the parallel option results in a considerably faster execution (nearly linear speed up). Running the same script inside an Experiment on Azure ML Studio I was not able to observe any speedup from the parallel version. At first I thought adding the following line conda_env.docker.arguments = [&#34;--cpuset-cpus=4&#34;] would help, but still the same. Therfore my question is, how can I enable the docker container to leverage multiple cores of the vm-instance? Kind Regard Kai",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to tain a ML model with CSV filles and not dataframes?",
        "Question_creation_time":1641828520987,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/689654\/how-to-tain-a-ml-model-with-csv-filles-and-not-dat.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI have a dataset composed of different csv files (a lot of them) with the same metadata that I have added to the Azure blob storage and now I would like to run a regression ML model with this data in Azure ML, however I want to train the model based on the csv files and not on each line of the files (not on each dataframe). How can I do this? Is it possible in Azure ML design?\n\nThank you!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Something Wrong?",
        "Question_creation_time":1593090172523,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/40031\/something-wrong.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Isn&#39;t me only or...?[welcome-to-azure][1]![10701-%E6%89%B9%E6%B3%A8-2020-06-25-205624.png][2]\n\n\n\n\n\n\n[1]: https:\/\/docs.microsoft.com\/zh-cn\/learn\/modules\/welcome-to-azure\/\n\n[2]: \/answers\/storage\/attachments\/10701-\u6279\u6ce8-2020-06-25-205624.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error Running Azure ML Training Script",
        "Question_creation_time":1651574593590,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/834730\/error-running-azure-ml-training-script.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI am receiving the following error message when running an experiment script in Azure Machine Learning Studio\n\n\"AADSTS70016: OAuth 2.0 device flow error. Authorization is pending. Continue polling\"\n\nMicrosoft have stated;\n\n\"This is not an error scenario, but is handled like one by Azure AD to handle certain authentication flows. This is not an indication that anything went wrong.\"\n\nHowever my experiment run has failed as can ben seen below;\n\n\n\n\nCould someone please advise how to correct this error.. thank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure machine learning cannot evaluate model",
        "Question_creation_time":1650117583373,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/814652\/azure-machine-learning-cannot-evaluate-model.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I create a trained model in Azure machine learning, when I use it to predict a new set of data, the evaluate model cannot show the result, it's all empty. How can I solve the problem?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to access compute quotas with the Azure CLI or Python SDK?",
        "Question_creation_time":1597248641713,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/65511\/is-there-a-way-to-access-compute-quotas-with-the-a.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I want to tabulate the compute quotas for each Azure ML workspace, in each Azure location, for my organization's Azure subscription. Although it is possible to look at the quotas manually through the Azure Portal (link), I have not found a way to do this with the Azure CLI or Python SDK for Azure. Since there are many resource groups and AML workspaces for different teams under my Azure subscription, it would be much more efficient to do this programmatically rather than manually through the portal. Is this even possible, and if so how can it be done?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":1.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Warning while cretaing Dataset from Datastore",
        "Question_creation_time":1664348355043,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1026239\/warning-while-cretaing-dataset-from-datastore.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"While creating a dataset using a datastore I see the following\n\"Warning: The selected datastore's SAS is insufficient to create this dataset.\nReading the dataset requires Read permission for Objects as well as Containers if the path is a folder or glob.\"\n\nI've ensured that the necessary permissions are assigned to the managed resources as well as the users. But still I see this warning. Will this create problem while accessing data from blob storage?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to connect to KeyVault to azureml real time endpoint using managed identity?",
        "Question_creation_time":1660172274087,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/963249\/how-to-connect-to-keyvault-to-azureml-real-time-en.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to connect keyvault to an insanely simple app with score file as follows:\n\n def init():\n     pass\n def run(raw_data):\n     KVUri=\"<<AN ACTUAL KEYVAULT URI>>\"\n     credential = DefaultAzureCredential()\n     client = SecretClient(vault_url=KVUri, credential=credential)\n     retrieved_secret = client.get_secret(\"test-secret\")\n     return  [retrieved_secret]\n\n\n\nI deploy using python SDK as follows:\n\n # create an online endpoint\n endpoint = ManagedOnlineEndpoint(\n     name=local_endpoint_name,\n     description=\"this is a sample online endpoint\",\n     auth_mode=\"key\",\n )\n ml_client.begin_create_or_update(endpoint)\n model = Model(path=\"..\/model\/dummy.txt\")\n env = Environment(\n     conda_file=\".\/conda.yaml\",\n     image=\"mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n )\n    \n blue_deployment = ManagedOnlineDeployment(\n     name=\"blue\",\n     endpoint_name=local_endpoint_name,\n     model=model,\n     environment=env,\n     code_configuration=CodeConfiguration(\n         code=\".\", scoring_script=\"app.py\"\n     ),\n     instance_type=\"Standard_F2s_v2\",\n     instance_count=1,\n )\n ml_client.begin_create_or_update(blue_deployment)\n\n\n\nAfter the deployment I add System assigned managed identity to azure key vault (Access Controls IAM > grant access to this resource). I assign access to instance of online endpoint, but when testing I get:\n\nFailed to test real-time endpoint\nDefaultAzureCredential failed to retrieve a token from the included credentials. Attempted credentials: EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured. Visit https:\/\/aka.ms\/azsdk\/python\/identity\/environmentcredential\/troubleshoot to troubleshoot.this issue. ManagedIdentityCredential: request() got an unexpected keyword argument 'tenant_id' To mitigate this issue, please refer to the troubleshooting guidelines here at https:\/\/aka.ms\/azsdk\/python\/identity\/defaultazurecredential\/troubleshoot.\n\nDo you know what is the source of an error? How should I do it correctly?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":20.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I transfer a csv file on an Azure Machine Learning compute instance directory back to the Datastore?",
        "Question_creation_time":1632158641093,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/559227\/how-can-i-transfer-a-csv-file-on-an-azure-machine.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I posted a similar question last week and didn't get a response to that yet so I'm posting another one now.\n\nThe code below is what I use to pull data into the compute instance from the Datastore. I transfer data from a Datastore to the compute instance and then save the data to my directory as a csv. The data originates from a SCOPE script and is transferred from Cosmos to the Datastore via Azure Data Factory.\n\nOnce the data is in the directory as a csv, I then utilize R to pull in the data into an RStudio session and then I run various tasks that create new data sets. I also save these new data sets to the compute instance directory as csv's. These new data sets are the ones I'd like to push back to the Datastore so they can be transferred elsewhere via Azure Data Factory and later consumed by a PowerBI app we're looking to create.\n\nI tried using Designer and it ran for 4 days without completing before I cancelled the job and started looking for an alternative route. I don't know if it would have completed or if it ran into memory issues and simply didn't fail. When I pull data into the compute instance from the datastore it takes less than a few minutes to complete so I'm not sure why it would take Designer multiple days to attempt to do the reverse operation.\n\nI've looked through a bunch of documentation and I am not able to find anything that tells us how we can transfer data from the compute instance back to the Datastore aside from Designer which is too slow or unable to handle.\n\nThis task seems like one that should be obvious for use and a major selling point of Azure Machine Learning so I'm a bit dumbfounded to see that this is a challenge figuring out how to do and that the documentation doesn't clearly show users how to achieve this task, assuming it's even possible. If it's not possible then I need to figure out a whole new system to use to get my work done. If it's not possible, the Azure Machine Learning team should enable this functionality as soon as possible.\n\n# Azure management\nfrom azureml.core import Workspace, Dataset\n# MetaData\nsubscription_id = '09b5fdb3-165d-4e2b-8ca0-34f998d176d5'\nresource_group = 'xCloudData'\nworkspace_name = 'xCloudML'\n# Create workspace \nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n# 1. Retention_Engagement_CombinedData\ndataset = Dataset.get_by_name(workspace, name='retention-engagement-combineddata')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/RetentionEngagement_CombinedData.csv')\n# 2. TitleNameJoin\ndataset = Dataset.get_by_name(workspace, name='TitleForJoiningInR')\n# Save data to file\ndf = dataset.to_pandas_dataframe()\ndf.to_csv('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/v-aantico1\/code\/TitleNameJoin.csv')",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure AutoML time series model returns strange forecast",
        "Question_creation_time":1604413410307,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/149896\/azure-automl-time-series-model-returns-strange-for.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I used Azure AutoML to train a time series forecasting model and selected the forecasting horizon to be 6. Each of our data row is one month, so we want to see the forecast for the following 6 months.\n\nHowever, when feeding 2 rows of data to the model, it returns 2 figures, and when feeding 8 rows of data, it returns 8 figures. We expect that as we select the forecasting horizon to be 6, regardless of how many rows of data being fed into the model, it should returns 6 figures.\n\nCould somebody explain why this happens and how to correct it? Thank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Car damage detection using azure machine learning or azure artificial intelligence",
        "Question_creation_time":1643020905263,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/707265\/car-damage-detection-using-azure-machine-learning.html",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi. Can someone please guide me how to detect damages in the car from car images using Azure Machine Learning or Azure AI?\n\nI'm planning to use image classification computer vision solution as a first step to classify if car is damaged or not, then as a second step use object detection to identify which parts of the car are damaged.\n\nI'm a beginner in AI and ML. Am I going with the correct approach or is there any other way to solve my problem?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How is AML's average GpuUtilization metric computed?",
        "Question_creation_time":1598450649953,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/77678\/how-is-aml39s-gpuutilization-metric-computed.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"How is the \"GpuUtilization\" metric computed for an AML workspace? What are the inputs and what is the equation used to compute GpuUtilization?\n\nThe \"metrics\" tab in the AML web portal shows a chart of the GpuUtilization over a specified time period, along with the average GpuUtilization for that time period. However, I have found that average GpuUtilization does not appear to accurately reflect the data shown in the chart for some of my organization's AML workspaces.\n\nFor example, the following screenshot shows the GpuUtilization for July 1-31, with the average GpuUtilization reported as 54.06. This is clearly much higher than what is shown in the chart. When I download the data from the chart (Share -> Download to Excel), I compute the average GpuUtilization to be ~11% in Excel. Why is there such a discrepancy?\n\nI have found similar discrepancies for other AML workspaces as well. However, the average GpuUtilization appears to be more accurate for the August 1-25 time period than it is for July 1-31. I wish to better understand how AML computes the average GpuUtilization over a time period so we can accurately account for my organization's AML GPU usage on a per-workspace basis.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What's the best way to preserve Azure ML workspace so that it can be restored",
        "Question_creation_time":1669442139003,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1105130\/what39s-the-best-way-to-preserve-azure-ml-workspac.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What's the best way to preserve Azure ML workspace so that it can be restored at a later point? I was hoping to find some automatic way to take a snapshot of artifacts & code and dump it into Azure storage, but haven't been able to find anything relevant in the online documentation.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Internal server error while connecting to jupyter lab instance",
        "Question_creation_time":1610979567287,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/234805\/internal-server-error-while-connecting-to-jupyter.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have created an ML-compute instance from scratch, nothing particularly installed.\nBut when I try to connect to Jupiter Lab directly through the URL provided by the user interface I get the following error :\n\n{\n\"error\": {\n\"code\": \"ServiceError\",\n\"severity\": null,\n\"message\": \"InternalServerError\",\n\"messageFormat\": null,\n\"messageParameters\": null,\n\"referenceCode\": null,\n\"detailsUri\": null,\n\"target\": null,\n\"details\": [],\n\"innerError\": null,\n\"debugInfo\": null\n},\n\"correlation\": {\n\"operation\": \"716efa38ccc70341b4b5b93bc16e441b\",\n\"request\": \"170d9ccac55c9d42\"\n},\n\"environment\": \"westeurope\",\n\"location\": \"westeurope\",\n\"time\": \"2021-01-18T14:00:19.9517739+00:00\",\n\"componentName\": \"notebook-instance-proxy\"\n}\n\nI also tried to connect through SSH to this compute instance, I notice that there is a conflict about the version of the pillow package.\nThen I've fixed the version conflict but the error still persists even after rebooting the machine.\nAfter that, I've connected to the machine through SSH tunnelling and I could start the Jupiter lab instance (even though the error still persist when connecting through the Azure ML Interface).\n\nCould you please investigate the connection issue through the user interface please?\nThanks in advance",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to update azure ml model from adf?",
        "Question_creation_time":1619624088887,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/375702\/how-to-update-azure-ml-model-from-adf.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I manage to run a azure ml trainning pipeline in adf. Then I can see that I can create\/update a batch inference pipeline from the Designer. But can I update the batch inference pipeline from adf?\n\nthanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning Studio Designer (preview) - Selective module execution",
        "Question_creation_time":1593464903017,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/41135\/azure-machine-learning-studio-designer-preview-sel.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I find that I am often inserting or modifying one module in a flow, and needing to edit several later items in the DAG,\n\nBut since the prior module is now invalidated, It asks me to SUBMIT and run the full experiment, so that I can do things like select columns.\n\nIs there not a way to disable modules? or selectively execute just a subset of modules?\n\nI am using the preview versions of the ML studio.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_creation_time":1607093437353,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/186789\/does-automl-support-optimizing-convolutional-neura.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot use ```%matplotlib qt``` in Jupyter notebook in Azure Machine Learning",
        "Question_creation_time":1626886810050,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/484527\/cannot-use-matplotlib-qt-in-jupyter-notebook-in-az-1.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am asking this question again, because I haven't got any update for my previous question, even though I have made new comments 20 days ago to describe my issue. The question can be found here.\n\nTo summarize:\nAfter restarting the kernel, I run the following suggested solution\nimport matplotlib\nmatplotlib.use('Qt5Agg')\nimport matplotlib.pyplot as plt\n, and still got the same error:\nImportError: Cannot load backend 'Qt5Agg' which requires the 'qt5' interactive framework, as 'headless' is currently running\n\nCan someone please help me to solve this problem? It is really important for me to use interactive plots.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practice for migration",
        "Question_creation_time":1656613684597,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/909885\/best-practice-for-migration.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Anyone has been migrated to new studio? Please share experience. I am confused about the migration, how should I copy paste my model from studio",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Data Labeling images removed from dataset still being display when \"data label\"",
        "Question_creation_time":1648547078293,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/791637\/azure-ml-data-labeling-images-removed-from-dataset.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI created a labeling project by creating a new dataset that imports files from local. Afterwards, some images previously uploaded need to be replaced by a new images (we decided to change some details of the display).\n\nHowever, when creating a new version of the dataset with the updated images (this works well) and refreshing the project, the images display when \"label data\" remain unchanged (also the on-demand incremental refresh date doesnt change, I assume because the images have the same name so project doesnt recognize something has changed.)\n\nI tried deleting images from the dataset thinking about reloading them, but the project also doesnt stop showing deleted images.\n\nIs there a way I can get my new image versions correctly displayed without having to create a new project?\n\nThanks!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to use pre-defined designer modules when building pipelines using python-sdk?",
        "Question_creation_time":1636017242333,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/615328\/is-it-possible-to-use-pre-defined-designer-modules.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hey, I am trying to build ML pipelines using the python-sdk. I am wondering if I can use those pre-defined modules from Designer when building pipelines using the python-sdk?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using \"cv_splits_indices\" in AutoMLConfig",
        "Question_creation_time":1614846003803,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/298513\/using-34cv-splits-indices34-in-automlconfig.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When training an regression model with AutoMLConfig with n_cross_validations being a normal int, I'm facing no problems.\n\nNow I want to use TimeSeriesSplit as the cross validation method for training a model with AutoMLConfig. For this there is a \"cv_splits_indices\" argument where I put in a list of lists of indicis like the following when n_splits=5 in TimeSeriesSplit :\n\n\n\n array([[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n         array([11, 12, 13, 14])],\n        [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n         array([15, 16, 17, 18])],\n        [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18]),\n         array([19, 20, 21, 22])],\n        [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22]),\n         array([23, 24, 25, 26])],\n        [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 24, 25, 26]),\n         array([27, 28, 29, 30])]], dtype=object)\n\n\n\nUnfortunately when running the following cell:\n\n automl_settings = {\n     \"iteration_timeout_minutes\": 15,\n     \"experiment_timeout_hours\": 0.3,\n     \"max_cores_per_iteration\" : -1,\n     \"enable_early_stopping\": True,\n     \"primary_metric\": 'normalized_root_mean_squared_error',\n     \"featurization\": 'auto',\n     \"verbosity\": logging.INFO,\n     \"cv_splits_indices\": idxs\n }\n    \n automl_config = AutoMLConfig(task='regression',\n                              debug_log=f'automated_ml_errors_.log',\n                              training_data=train,\n                              validation_data=train,\n                              label_column_name=y_var,\n                              **automl_settings)\n\n\n\nI receive the following error:\n\n ConfigException: ConfigException:\n  Message: cv_splits_indices should be a List of List[numpy.ndarray]. Each List[numpy.ndarray] corresponds to a CV fold and should have just 2 elements: The indices for training set and for the validation set.\n  InnerException: None\n  ErrorResponse \n {\n     \"error\": {\n         \"code\": \"UserError\",\n         \"message\": \"cv_splits_indices should be a List of List[numpy.ndarray]. Each List[numpy.ndarray] corresponds to a CV fold and should have just 2 elements: The indices for training set and for the validation set.\",\n         \"details_uri\": \"https:\/\/aka.ms\/AutoMLConfig\",\n         \"target\": \"cv_splits_indices\",\n         \"inner_error\": {\n             \"code\": \"BadArgument\",\n             \"inner_error\": {\n                 \"code\": \"ArgumentInvalid\"\n             }\n         },\n         \"reference_code\": \"XXXXXXREDACTEDXXXX\"\n     }\n }\n\n\n\nWhat is going wrong here? My input looks correct?\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"azure ml pipeline fails at sql transform task",
        "Question_creation_time":1620697549357,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/390003\/azure-ml-pipeline-fails-giving-no-error.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I'm using Azure ML Designer to run a pipeline. The pipeline performs a few steps and then it cancels the work throwing an error message with no further details.\n\nIf I re-submit the pipeline it completes the previously failed step but fails on the next step. If I re-submit the same thing happens (completes previously failed step to then fail the next step)... until it gets stuck in a specific sql transform step (see log below)\n\n\n\n\nHere is a sequence of run ids related with the issue:\nd33d23a2-2e60-4198-a6b6-f47e6e27ef4e\n57e04c1e-73e8-4ddf-91a8-c407cd1ad5ef\nad7dc826-6549-4eb3-9536-9a801d8e8c0b\ne6623f6f-b7b9-4f19-9501-c8c28f53ab23\n\nIt may be due to the way my pipeline is built but seems like JOIN, SQL Transform and SELECT Column operations tend to fail the most.\n\nWould much appreciate any help on this.\n\n 2021\/05\/11 01:57:24 Starting App Insight Logger for task:  runTaskLet\n 2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/info\n 2021\/05\/11 01:57:24 Attempt 1 of http call to http:\/\/10.0.0.6:16384\/sendlogstoartifacts\/status\n [2021-05-11T01:57:24.912444] Entering context manager injector.\n [context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['urldecode_invoker.py', 'python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', 'DatasetOutputConfig:Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22'])\n Script type = None\n [2021-05-11T01:57:26.142183] Entering Run History Context Manager.\n [2021-05-11T01:57:26.734197] Current directory: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/mounts\/workspaceblobstore\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\n [2021-05-11T01:57:26.734493] Preparing to call script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '$Result_dataset', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n [2021-05-11T01:57:26.734551] After variable expansion, calling script [urldecode_invoker.py] with arguments:['python', '-m', 'azureml.designer.modules.datatransform.invoker', 'ApplySqlTransModule', '--dataset', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu', '--t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr', '--t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy', '--t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji', '--sqlquery=%22select+b.*%2cc.*%0d%0afrom+(%0d%0a++++select+a.customer_id%2c+a.sku_id%0d%0a++++from+(%0d%0a++++++++select+*+from+t1+cross+join+t2%0d%0a++++)+a%0d%0a++++where+exists+(%0d%0a++++++++select+t3.top_skus%0d%0a++++++++from+t3%0d%0a++++++++where+t3.sku_id+%3d+a.sku_id%0d%0a++++)%0d%0a)+b%0d%0ainner+join+(%0d%0a++++select+distinct+sku_id%2c+top_skus%0d%0a++++from+t3%0d%0a)+c%0d%0aon+c.sku_id+%3d+b.sku_id%22']\n    \n Session_id = 4b5b4c29-cfda-4ab6-a715-47fee287c468\n Invoking module by urldecode_invoker 0.0.8.\n    \n Module type: custom module.\n    \n Using runpy to invoke module 'azureml.designer.modules.datatransform.invoker'.\n    \n \/azureml-envs\/azureml_7c975cabc8bb1dc19c3de94457d707fd\/lib\/python3.6\/site-packages\/azureml\/designer\/modules\/datatransform\/tools\/dataframe_utils.py:2: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n   from pandas.util.testing import assert_frame_equal\n 2021-05-11 01:57:27,324 [             invoker] [    INFO] .[main] Start custom modules\n 2021-05-11 01:57:27,337 [             invoker] [    INFO] .[main] Module version: 0.0.74\n 2021-05-11 01:57:27,344 [             invoker] [    INFO] .[main] args: azureml.designer.modules.datatransform.invoker, ApplySqlTransModule, --dataset, \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu, --t1=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr, --t2=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy, --t3=\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji, --sqlquery=select b.*,c.*\n from (\n     select a.customer_id, a.sku_id\n     from (\n         select * from t1 cross join t2\n     ) a\n     where exists (\n         select t3.top_skus\n         from t3\n         where t3.sku_id = a.sku_id\n     )\n ) b\n inner join (\n     select distinct sku_id, top_skus\n     from t3\n ) c\n on c.sku_id = b.sku_id\n 2021-05-11 01:57:27,352 [             invoker] [    INFO] .[main] \"transform_module_class_name\": ApplySqlTransModule\n 2021-05-11 01:57:27,444 [         module_base] [    INFO] ...[get_arg_parser] Construct arg parser\n 2021-05-11 01:57:27,460 [         module_base] [    INFO] ...[get_arg_parser] arg: t1\n 2021-05-11 01:57:27,468 [         module_base] [    INFO] ...[get_arg_parser] arg: t2\n 2021-05-11 01:57:27,476 [         module_base] [    INFO] ...[get_arg_parser] arg: t3\n 2021-05-11 01:57:27,484 [         module_base] [    INFO] ...[get_arg_parser] arg: dataset\n 2021-05-11 01:57:27,492 [         module_base] [    INFO] ...[get_arg_parser] arg: sqlquery\n 2021-05-11 01:57:27,500 [         module_base] [    INFO] ..[parse_and_insert_args] invoker args:\n  module_classname = ApplySqlTransModule\n  t1 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n  t2 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n  t3 = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n  dataset = \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpnbybe4mu\n  sqlquery = select b.*,c.*\n from (\n     select a.customer_id, a.sku_id\n     from (\n         select * from t1 cross join t2\n     ) a\n     where exists (\n         select t3.top_skus\n         from t3\n         where t3.sku_id = a.sku_id\n     )\n ) b\n inner join (\n     select distinct sku_id, top_skus\n     from t3\n ) c\n on c.sku_id = b.sku_id\n    \n 2021-05-11 01:57:27,508 [             invoker] [    INFO] .[main] start to run custom module: ApplySqlTransModule\n 2021-05-11 01:57:27,516 [apply_sql_trans_module] [    INFO] ...[run] Construct SQLite Server\n 2021-05-11 01:57:27,530 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpmflqzlpr\n 2021-05-11 01:57:29,215 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1 with only column names\n 2021-05-11 01:57:29,227 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpl9h5snzy\n 2021\/05\/11 01:57:29 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n Stopped: false\n OriginalData: 1\n FilteredData: 0.\n 2021-05-11 01:57:30,093 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2 with only column names\n 2021-05-11 01:57:30,106 [    module_parameter] [    INFO] ......[data] Read data from \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/test\/azureml\/e5e84dde-4b32-4ea3-9965-adc71f7ab0f6\/wd\/tmpuhf3n5ji\n 2021-05-11 01:57:30,876 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3 with only column names\n 2021-05-11 01:57:30,888 [apply_sql_trans_module] [    INFO] ...[run] Read SQL script query\n 2021-05-11 01:57:30,895 [apply_sql_trans_module] [    INFO] ...[run] Validate SQL script query\n 2021-05-11 01:57:30,912 [apply_sql_trans_module] [    INFO] ...[run] Insert data to SQLite Server\n 2021-05-11 01:57:30,919 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t1\n 2021-05-11 01:57:30,930 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t2\n 2021-05-11 01:57:30,970 [apply_sql_trans_module] [    INFO] ....[_transform_df_to_sql] Insert t3\n 2021-05-11 01:57:31,053 [apply_sql_trans_module] [    INFO] ...[run] Generate SQL query result from SQLite Server",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to get the probability\/score by each cell\/row with machine learning?",
        "Question_creation_time":1666667585287,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1061214\/how-to-get-the-probabilityscore-by-each-cellrow-wi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have go through the solution in the forum but couldn't find a suitable solution for my question.\nI have use machine learning model to get the best algo for data modelling.\nI manage to get the confusion table but somehow i looking for probability\/scoring for each cell instead of overall result.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":33.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Advantage of clustering algorithm",
        "Question_creation_time":1608719822847,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/208687\/advantage-of-clustering-algorithm.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We don't need the name of attributes in clustering, so, if I do not know my attribute names how can I understand that which data should I enter and also if I do not know the name of the attributes how can I give the axis name of the plotted graph? If it is the advantage of clustering algorithm, then why it is not the advantage of other algorithms, because if we do not know the name of the attributes we can create our models because if we have attribute values that is enough, but why we need attribute names too in other algorithms?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to parameterize the sharable url of an azure ml notebook?",
        "Question_creation_time":1643398754623,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/714542\/is-it-possible-to-parameterize-the-sharable-url-of.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I would like to share the link to a Jupyter notebook (stored in azure ml studio) with the parameters of the notebook already updated. I see I can automatically get a sharable link for the notebook. Is it possible to parameterize this link? If not, is there an equivalent alternative?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Scaling and load balancing with deployed machine learning containers - ACI + Application Gateway vs AKS?",
        "Question_creation_time":1637592050953,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/636334\/scaling-and-load-balancing-with-deployed-machine-l.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have been working on a deployment solution for machine learning models. Our objective is to minimize the internal development effort while maintaining high availability and full control over the API. Our first solution is a deployment as azure webservice to ACI using the azureml.core.model.Model.deploy method with a deployment config azureml.core.webservice.AciWebservice.\nWe quickly noticed that our initial deployment configuration was not powerful enough and wanted to upgrade the hardware. Here is the first issue: it seems that the ACI webservice does not support any GPU instances. So my first question is: which service is recommended to deploy models that require GPU for inference?\nHowever, we first wanted to deploy the same image onto a more powerful CPU. When doing this, we got the error\n\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\". After deleting and redeploying, the scoring URI changes, which is a problem for us. As we cannot predict the load and resource requirements at this point, we need a way of (auto-) scaling the containers and also to update single container resource requirements, without changing the endpoint. As this seems to be impossible using the scoring URI provided by azureml endpoint, I had the feeling that we need to hide the azureml endpoint behind a load balancer. Is this the intended way of doing it? If so, should I use application gateway or traffic manager? If I use application gateway, how do I configure the VPC such that the containers managed by azureml are available to application gateway? Or should I completely abandon ACI and go with AKS? It would be great to get some expert feedback on this as I feel like every single of these questions leads me down a rabbit hole and I cannot continue the actual project if I first have to learn about >10 azure services and all cross combinations of those.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":18.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Project in Data Labeling not working, getting only \"Loading project details\"",
        "Question_creation_time":1647676305203,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/778679\/project-in-data-labeling-not-working-getting-only.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Project in Azure Machine Learning Studio in Data Labeling was working, we did label it every day. One day we just could not open it, it showed only Loading project details.. for hours.\nSAS token is working, it also work for our admin account, but not for Labellers.\nDo you have any idea?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to disable Azure ML snapshots (for compute clusters)",
        "Question_creation_time":1611558384720,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/243585\/how-to-disable-azure-ml-snapshots-for-compute-clus.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using the Azure ML Python SDK to spawn jobs using a prebuilt Docker image compute cluster. The Docker image has all of the dependencies installed and the source code I am running.\n\nAccording to the logs, Azure ML's \"snapshot\" operations (that I assume upload and then download the source directory to Azure ML jobs) increase the startup time from 2 minutes to 8-20 minutes (i.e., the time it takes for an Azure ML run to begin running my code). By comparison, when I run the exact same code on a compute instance instead of a compute cluster, startup time is 60-80 seconds. Notably, the startup logs for the compute instance make no mention of \"snapshots\".\n\nI already disabled saving snapshots for historical record, but that made little difference and the startup logs (for a cluster) still show operations for snapshots. I also significantly expanded our .amlignore file, which reduced the startup time by 10+ minutes, but 5-10 minutes are still spent on the \"fetching snapshots\" step (which we do not even use).\n\nQuestions:\n1. What is this \"fetching snapshot\" operation if it is not the saving of snapshots for historical record (which I already disabled and confirmed)?\n2. Why does this operation only occur for compute clusters but not compute instances?\n3. Why is this operation so slow? I.e., 5-10 minutes to \"fetch\" less than 10 MB of python files.\n4. Can I disable everything having to do with snapshots altogether? I assume this is possible because this occurs when using compute instances in Azure ML.\n\nThank you very much. Please let me know if there is anything more I can provide to help debug.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML Service started return InternalError 500 for batch requests. Code:14.",
        "Question_creation_time":1595609615540,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/52082\/ml-service-started-return-internalerror-500-for-ba.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"ML Service started return InternalError for batch requests with Code:14.\nI enabled logs, logs are good, even results in storage are good.\n\nSingle request works without any issues.\n\n\n\n\n{\"error\":{\"code\":\"InternalError\",\"message\":\"Execution encountered an internal error.\",\"details\":[{\"code\":\"14\"}]}}\n\n\n\n\nBatch Request Log shows request as \"Finished\" and provides link to blob.\n\nAnybody knows what is a Code:14?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":1.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is AML_MODEL_URI - PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Question_creation_time":1637180271803,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\n\nI don't know what is the AML_MODEL_URI. I thought it was the REST endpoint or the Swagger URI from the endpoint.\n\n\nBut it is not working. I am getting this error on Synapse: \"RuntimeError: Load model failed\nTraceback (most recent call last):\"\n\n\nI appreciate you help.\n\nKind regards,\nAnaid",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"time series training",
        "Question_creation_time":1659301076627,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/949086\/time-series-training.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I see one document mentioned time series training can be done with AutoML: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast is that any sample which from basic build of model?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to include the IP address of a specific Azure Machine Learning workspace in its storage account selected networks and get all functionality enabled?",
        "Question_creation_time":1626459319327,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/479209\/is-it-possible-to-include-the-ip-address-of-a-spec.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We have secured the storage account of a Machine Learning workspace behind a vnet and have authorized a set of IPs to access the storage account. Since the workspace is not secured behind the vnet, a set of functions is disabled. Is there a way to get the IP of the workspace and include it in the list authorized networks for the storage account in order to have all workspace functionalities available? We know the official solution involves securing the workspace behind the vnet and enabling point-to-site, site-to-site or connecting through a VM, but these are not possible in our case. Thanks for the help.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I use a working pipeline",
        "Question_creation_time":1640685619343,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/677175\/how-can-i-use-a-working-pipeline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI created a working pipeline in azure machine learning studio but I am stuck how i can use it with a live dataset. Could anybody help to me in this issue? I dont have such option to deploy it.\n\nthank you in advance",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dataset for inference data is registered with wrong path when deployed on Kubernetes",
        "Question_creation_time":1624590394283,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/451349\/dataset-for-inference-data-is-registered-with-wron.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When model is deployed on Kubernetes with model data collection set to true, the dataset which gets registered in workspace has a wrong path.\nThe blob storage path where this data being collected has default in it but the path registered doesn't have default.\n\nFor example, actual path on storage:\n\n azuremlwsdev\/modeldata\/GUID\/ad\/ad-workspace-dev\/dev-ad-model-v1-2-jen\/model\/default\/inputs\n\n\n\nPath dataset is registered with:\n\n GUID\/ad\/ad-workspace-dev\/dev-ad-model-v1-2-jen\/model\/inputs\/**\/inputs*.csv\n\n\n\nCode inside our scoring service to init collector and use them:\n\n def init():    \n     global inputs_dc, prediction_dc\n     ...\n     inputs_dc = ModelDataCollector('model', designation=\"inputs\", feature_names=feature_set.columns.values.tolist())\n     prediction_dc = ModelDataCollector('model', designation=\"predictions\", feature_names=[\"score\"])\n    \n def run(raw_data):\n     inputs_dc.collect(X)\n     prediction_dc.collect(y_pred)\n\n\n\nBelow are the library versions being used:\n\n azureml-defaults==1.1.5\n azureml-monitoring==0.1.0a21",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML - Deployed to Inference Cluster throws 500 Server Error - MissingFeaturesError",
        "Question_creation_time":1614113525043,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/285607\/azure-ml-deployed-to-inference-cluster-throws-500.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We have an Azure ML model we are ready to deploy to an http endpoint for consumption and testing.\n\nWe are using this tutorial (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy) as the jumping off point for deploying our ml model. We have created an inference cluster, converted the training model to a real-time inference model and deployed. Deployment looks successful. However, when testing (both via the Test tab in the Azure ML Workspace and via http POST) the server throws a 500. The MissingFeaturesError follows:\n\n\n\n\nFile \"\/azureml-envs\/azureml_9b50686470a92ca74f0d62e2629faaec\/lib\/python3.6\/site-packages\/azureml\/studio\/modules\/ml\/common\/base_learner.py\", line 289, in _validate_no_missing_feature\nErrorMapping.throw(MissingFeaturesError(required_feature_name=';'.join(missing_feature_list)))\n> missing_feature_list = ['Miles', 'Age', 'Gender', 'MarriagetPlans']\n\n\nFile \"\/azureml-envs\/azureml_9b50686470a92ca74f0d62e2629faaec\/lib\/python3.6\/site-packages\/azureml\/studio\/common\/error.py\", line 814, in throw\nraise err\n> err = MissingFeaturesError('Features for Miles;Age;Gender;MarriagetPlans required but not provided.',)\n\n\nMissingFeaturesError: Features for Miles;Age;Gender;MarriagetPlans required but not provided.\n\nIn both test cases (via Test tab in Azure and http POST to the endpoint) all the required data is indeed provided. The request body definitely includes Miles, Age, Gender, MarriagetPlans.\n\nWhat is going on here?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access to Azure subscription for users",
        "Question_creation_time":1666972949727,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1067157\/access-to-azure-subscription-for-users.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We have an active subscription. For few of the users we activated owner role for the machine learning resource group.\nBut when they login to the portal\/ML environment and try to switch directory and subscription, they don't see our production subscription and hence the workspace, although directory is correct. User had created a trial subscription on its own before and he only has visibility to that.\n\nI checked with a test account and after login to ML studio I see this, which I believe is the same reason user does not see the subscription.\n\nHow can I safely give user subscription access only for ML resource group",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":53.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML (Designer): Endpoint deploys initially, but can't consume or test it after that.",
        "Question_creation_time":1624896031587,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/454914\/azure-ml-designer-endpoint-deploys-initially-but-c.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI recently deployed an Azure Machine Learning Model (Designer). I have followed this tutorial (but for my model). The inference pipeline looks like this:\n\nThis endpoint was then deployed with the following configurations:\n\nCompute Type: Azure Container Instances\n\n\nCPU : 2\n\n\nMemory: 2 Gb\n\nFrom my understanding, when a model-endpoint is deployed initially, there is a test run (which tries to consume the endpoint) that is automatically run by Azure ML, and this has always worked for me (I have tried AKS too, and ACI with lower compute power). Attached, please find the logs.\n\n109955-initial-test-endpoint.txt\n\nAfter the test run, I can either test it from the interface itself, or I can execute some Python code (which Azure ML provides) that can consume it for me. In both cases, I get a 500-Bad Gateway error (checked through Deployment Logs):\n109962-endpoint-test.txt\n\nThis is what the Python code outputs:\n\n\nI have monitored the status of the endpoint while I try to consume it, and it is always healthy.\n\nI hope I have provided enough details. If not, please let me know what else is needed to understand the issue here. Any help is appreciated.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Store Raw JSON file from azureml.core.Run",
        "Question_creation_time":1651092990657,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/829188\/store-raw-json-file-from-azuremlcorerun.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nin the details tab of a pipeline step (in an experiment) one of the last entries is \"See all properties\". Below that the Raw JSON file is linked and can be opened. Is there a way to save or access these Raw JSON file within the Python SDK?\n\nI want to store these file beside the model to guarantee traceability, if we deploy the registered model outside of Azure.\n\nThanks for your help",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Total Regional Cores quota error",
        "Question_creation_time":1665265961547,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1040655\/total-regional-cores-quota-error.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Dear all\nI have a free Microsoft Azure student account. I am using Microsoft Azure automated ML and DESIGNER.\nI tried to deploy my model, which I created in DESIGNER.\nFor that, I need to create Compute, Inference clusters. When I do so, I always receive an error message: \"Operational could not be completed as it results in exceeding approved Total Regional Cores quota\". I tested various options, but all failed. I always chose Switzerland North, as it is the closest.\nIn addition, I constructed a new resource group. However, this did not make any difference.\nI read something about altering the Subscription (Azure for Students). However, then I'm not sure if I have to pay for the upgrade?\n\nMay you can tell me how it works with \"Azure for students\" when I want to deploy my model? How can I build a Compute, Inference clusters?\n\nThank you for your feedback.\n\nBest regards\n\nLukas",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Endpoint not being created Azure Designer",
        "Question_creation_time":1592922691007,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/39203\/endpoint-not-being-created-azure-designer.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I created a simple recommendation model in Azure Designer. The model is able to train and from that, I created an inference pipeline. I ran it and deployed it, under a new AKS inference cluster and even experiment. However, the endpoint is just not being created. I can see the model is registered with a new version but no endpoint is created. Below, I have attached a screenshot of the inference pipeline in designer.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure : \"message\": \"Error: Scoring was unsuccessful.\" when we run forecasting.",
        "Question_creation_time":1655568786827,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/894519\/azure-34message34-34error-scoring-was-unsuccessful.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\ninit() function in score.py is executed properly but I am getting error in run function. As shown in image I am getting below error.\n\n \"error\": {\n\"message\": \"Error: Scoring was unsuccessful.\"\n}\n\nThank you,\nSaswat",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to run CuDNNLSTM on JupyterLab within Azure Machine Learning?",
        "Question_creation_time":1595404543680,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/50813\/how-to-run-cudnnlstm-on-jupyterlab-within-azure-ma.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Good day Everyone\n\nI am trying to train an LSTM based recurrent neural network using Azure Machine Learning JupyterLab. I have setup the VM instance to use a 6 core GPU. However, when i try to train my recurrent network using the efficient GPU based CuDNNLSTM network i get a \"ModuleNotFoundError: no module named tensorflow.contrib\". How can i rectify this so that i can be able to run CuDNNLSTM based code on my GPU?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":35.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error creating endpoint from mlflow model (tensorflow job)",
        "Question_creation_time":1667731273557,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1077214\/error-creating-endpoint-from-mlflow-model-tensorfl.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello everybody,\nI am trying to deploy a realtime endpoint from a registered mlflow model obtained from a tensorflow training job.\nIn this repository, you will find the training scripts:\n\nhttps:\/\/github.com\/antigones\/py-hands-ml-tf\/tree\/main\/azure_ml\/job_script\n\nThe job outputs a MLFlow model with its conda environment yml file.\n\n\n\n\nWhen I try to deploy the model to a realtime endpoint, I get the following error:\n\n257528-azure-ml-deploy-error.txt\n\nIt seems to be an error related to protobuf, when loading the model:\n\n  File \"\/opt\/miniconda\/envs\/userenv\/lib\/python3.8\/site-packages\/google\/protobuf\/descriptor.py\", line 560, in __new__\n     _message.Message._CheckCalledFromGeneratedFile()\n TypeError: Descriptors cannot not be created directly.\n If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n If you cannot immediately regenerate your protos, some other possible workarounds are:\n  1. Downgrade the protobuf package to 3.20.x or lower.\n  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n    \n More information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\n\n\n\nThe environment is deployed automatically (the scoring script is also generated).\nI have also tried different images, with different python versions (3.7) and Tensorflow versions (2.4) with no luck.\n\nHow can I solve this issue?\n\n\n\n\nThank you in advance for your support.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Accessing the my ML Model from azure ml",
        "Question_creation_time":1598739182170,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/80879\/accessing-the-my-ml-model-from-azure-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am accessing my ML modal that i have deployed on azure ml and using that model to predict but is there any way i can pass the data other than json format ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use a trained model for use cases (a new data set)",
        "Question_creation_time":1665310129110,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1040790\/use-a-trained-model-for-use-cases-a-new-data-set.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Dear all\n\nI conducted some experiments with Microsoft Azure automated ML and DESIGNER.\n\nAs far as I understand the given results, the trained model shows e.g. the accuracy? How well or in how many cases can the trained model predict the value (e.g. TRUE or FALSE) correctly?\n\nNow, I want to use the \"trained\" model(s) for use cases. My goal is to use the trained model(s) and provide predictions for new samples (a new data set). E.g. I want to predict the value \"TRUE\" or \"FALSE\" for the values of the new data set.\n\nIn my case, there is no value in the column (TRUE or FALSE). I want the model to provide me with the answers.\n\nNext steps: As far as I see, I need to deploy the model so that I can conduct the same experiments with new samples?\n\nOr how can I apply my trained model for the new use cases? (please see my description above)\n\nThank you for your feedback\n\nBest regards\n\nLukas",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I get the experimentID inside a running pipeline script?",
        "Question_creation_time":1638292741140,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/646416\/how-do-i-get-the-experimentid-inside-a-running-pip.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I'm writing a ML pipeline.\n\nAt the end of a script I have to write the output to a SQL database, and I would need the ExperimentID as a field of the output dataframe.\n\nIs there a way for me to find within the running script in which experiment it's being run?\nOr is there a way for me to input the ExperimentID as a parameter to the pipeline at launch? From what I understand parameters are defined before the experiment is created so that's a bit confusing.\n\nIn case this is too complicated, is there a way I can somehow chain the pipeline output inside a script to the experiment it's being run?\n\nThank you very much,",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I wish to store a variable in R in ML studio to use it for consecutive executions (Web service calls). But since the R script is run from the start, the variable's value resets to default value, that I set. Is there a way to achieve this?",
        "Question_creation_time":1606214692780,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/173691\/i-wish-to-store-a-variable-in-r-in-ml-studio-to-us.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My Machine learning flow looks something like this:\n\n\n\n\n\n\n\nAnd my R script looks something like this:\n\n if(useAxe){\n    if (condition1) {\n       useAxe <- true\n    }else{\n       useAxe<- false\n }\n\n\n\n\nI want the value of useAxe from the previous execution and update it according to the criteria.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Timeout on AutoMLConfig is not a hard timeout",
        "Question_creation_time":1624927301677,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/455257\/timeout-on-automlconfig-is-not-a-hard-timeout.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When setting the timeout on the AutoMLConfig, it is not respected. Why?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Visual Code red underline",
        "Question_creation_time":1617894794830,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/349688\/visual-code-red-underline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello! I'm studying the Phyton First Steps course and I have questions about the Video Code. When I type \"Print\" in the code thing it doesn't get a red underline as they said it should. I want to know if I need to enable that type of thing",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Drifting Sample in azure machine learning studio does not finish backfill",
        "Question_creation_time":1630488394320,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/535756\/drifting-sample-in-azure-machine-learning-studio-d.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to run the sample tutorial on data\/model drifting in azure machine learning samples. When I run the backfill command is waiting for the target cluster in the queue and it does not start, after waiting for a very long time.\n\nIs it a common issue? How to run the example? I thought was a question of simply run it .. but it does not finish the run..",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Time series forecasting AutoML Automatic featurization groupby specific column values",
        "Question_creation_time":1647278425307,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/771703\/time-series-forecasting-automl-automatic-featuriza.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\nI'm training time-series forecasting models with Azure AutoML.\n\nAs far as I\u2019m concerned, featurization techniques such as Normalization and Scaling or Impute missing values, are applied by columns.\n\nGiven that I\u2019m dealing with time-series data, is there a way to apply these kinds of featurization using a group by some column values? Otherwise, I feel the transformations applied make no sense at all.\n\nFor instance, If I want to predict product demand from different stores, would be possible to impute missing values from one article given the median of that article (not the one of the column) modifying AutoML Automatic featurization? Or standardize the target values for each article separately?\n\nThanks in Advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access file in a storage account residing in different tenant to ML Service in another tenant via IP based SAS restricted access",
        "Question_creation_time":1645593594070,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747081\/access-file-in-a-storage-account-residing-in-diffe.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI have a storage account residing in tenant-A and machine learning service in tenant-B. When I try to read file from storage account in tenant-A via SAS (with IP restriction) in the jupyter notebook running on compute in ML service in tenant-B, it is not accessible and failing with 403 (Forbidden).\n\nBut when I try to access the file without IP restriction, I am able to read it in the notebook.\nCan you please help in understanding why it is happening and possible fix for the problem?\n\nPlease note, the public IP of ML compute is being used for whitelisting in SAS.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML BFSMountError: Unable to mount blob fuse file system",
        "Question_creation_time":1660200527397,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/963683\/azure-ml-bfsmounterror-unable-to-mount-blob-fuse-f.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Training yolov5 using Azure ML SDK and encountered error below. I have a gpu cluster created (no vnet) and encountered this error when i ScriptRunConfig and experiment.submit. My Azure ML Storage account is enabled from all network (no vnet). Thank you very much\n\nsrc = ScriptRunConfig(.......)\nrun = experiment.submit(src)\n\nAzureMLCompute job failed.\nBFSMountError: Unable to mount blob fuse file system\nInfo: Could not mount Azure Blob Container azureml-blobstore-xxxxxxxx at workspaceblobstore: <nil>. Unable to start blobfuse due to a lack of credentials. Please check the readme for valid auth setups.\nUnmounting blobfuse.\nUnmounted blobfuse successfully.\n\n Info: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DataDrift in AzureML",
        "Question_creation_time":1659816958003,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/957739\/datadrift-in-azureml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, What is the underlying algorithm for azureml DataDriftDetector class? and what is the mathematical implication of the threshold in datadrift?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AciDeploymentFailed",
        "Question_creation_time":1655464430947,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/893526\/acideploymentfailed.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nIt's been a few days already since I've been struggling with this error which is not suggesting me anything.\nThis is the error I receive. Every time I'm trying to access the logs it displays me \"None\". Also, the init() function is a very basic one. It's the one I've found in your tutorials and while I've followed your tutorials I didn't encounter this bug.\n\nscore.py script:\n\n import pandas as pd\n import numpy as np\n import joblib\n import json\n import os\n    \n # Called when the service is loaded\n def init():\n     global model\n     # Get the path to the deployed model file and load it\n     model = joblib.load(Model.get_model_path(model_name='aml_live_model_end'))\n    \n # Called when a request is received\n def run(raw_data):\n     # Get the input data as a numpy array\n     data = np.array(json.loads(raw_data)['data'])\n     # Get a prediction from the model\n     predictions = model.predict(data)\n     # Get the corresponding classname for each prediction (0 or 1)\n     classnames = ['De avizat', 'De analizat']\n     predicted_classes = []\n     for prediction in predictions:\n         predicted_classes.append(classnames[prediction])\n     # Return the predictions as JSON\n     return json.dumps(predicted_classes)\n\n\n\n\n.yaml file\n\n name: aml_live_env\n dependencies:\n - python=3.6.2\n - scikit-learn\n - ipykernel\n - matplotlib\n - pandas\n - pip\n - pip:\n   - azureml-defaults\n   - pyarrow\n\n\n\n\nThe error I receive\n\n Deploying model...\n Tips: You can try get_logs(): https:\/\/aka.ms\/debugimage#dockerlog or local deployment: https:\/\/aka.ms\/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n Running\n 2022-06-17 10:52:16+00:00 Creating Container Registry if not exists.\n 2022-06-17 10:52:16+00:00 Registering the environment.\n 2022-06-17 10:52:17+00:00 Use the existing image.\n 2022-06-17 10:52:17+00:00 Generating deployment configuration.\n 2022-06-17 10:52:18+00:00 Submitting deployment to compute.\n 2022-06-17 10:52:20+00:00 Checking the status of deployment aml-live-service-model..\n 2022-06-17 10:54:07+00:00 Checking the status of inference endpoint aml-live-service-model.\n Failed\n Service deployment polling reached non-successful terminal state, current service state: Failed\n Operation ID: 93d48e89-cb16-4d1c-bbb6-f453acaeaa7f\n More information can be found using '.get_logs()'\n Error:\n {\n   \"code\": \"AciDeploymentFailed\",\n   \"statusCode\": 400,\n   \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n     1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n     2. You can interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n     3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\n   \"details\": [\n     {\n       \"code\": \"CrashLoopBackOff\",\n       \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n     1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n     2. You can interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n     3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\n     },\n     {\n       \"code\": \"AciDeploymentFailed\",\n       \"message\": \"Your container application crashed. Please follow the steps to debug:\n     1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\n     2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.\n     3. You can also interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n     4. View the diagnostic events to check status of container, it may help you to debug the issue.\n \"RestartCount\": 5\n \"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n \"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2022-06-17T10:56:57.554Z\",\"exitCode\":111,\"finishTime\":\"2022-06-17T10:57:01.314Z\",\"detailStatus\":\"Error\"}\n \"Events\":\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:26:38Z\",\"lastTimestamp\":\"2022-06-17T10:26:38Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:27:42Z\",\"lastTimestamp\":\"2022-06-17T10:27:42Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":10,\"firstTimestamp\":\"2022-06-17T10:28:00Z\",\"lastTimestamp\":\"2022-06-17T10:47:11Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n {\"count\":9,\"firstTimestamp\":\"2022-06-17T10:28:03Z\",\"lastTimestamp\":\"2022-06-17T10:40:46Z\",\"name\":\"Killing\",\"message\":\"Killing container with id a7e717efa63259b36b19bc4951b3f3dcc5f1093177e729c589355a7371353ca3.\",\"type\":\"Normal\"}\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:31:33Z\",\"lastTimestamp\":\"2022-06-17T10:31:33Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:32:31Z\",\"lastTimestamp\":\"2022-06-17T10:32:31Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":5,\"firstTimestamp\":\"2022-06-17T10:47:52Z\",\"lastTimestamp\":\"2022-06-17T10:52:09Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n {\"count\":6,\"firstTimestamp\":\"2022-06-17T10:48:26Z\",\"lastTimestamp\":\"2022-06-17T10:52:37Z\",\"name\":\"Killing\",\"message\":\"Killing container with id 2bdec1005a6dd58312e10ab939d88ea08b312771de6573d9b86c5f571104277e.\",\"type\":\"Normal\"}\n \"\n     }\n   ]\n }\n    \n ---------------------------------------------------------------------------\n WebserviceException                       Traceback (most recent call last)\n <ipython-input-17-315dbb5f83ec> in <module>\n      16 service_name = \"aml-live-service-model\"\n      17 service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, overwrite=True)\n ---> 18 service.wait_for_deployment(True)\n      19 print(service.state)\n    \n \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output, timeout_sec)\n     916                     logs_response = 'Current sub-operation type not known, more logs unavailable.'\n     917 \n --> 918                 raise WebserviceException('Service deployment polling reached non-successful terminal state, current '\n     919                                           'service state: {}\\n'\n     920                                           'Operation ID: {}\\n'\n    \n WebserviceException: WebserviceException:\n     Message: Service deployment polling reached non-successful terminal state, current service state: Failed\n Operation ID: 93d48e89-cb16-4d1c-bbb6-f453acaeaa7f\n More information can be found using '.get_logs()'\n Error:\n {\n   \"code\": \"AciDeploymentFailed\",\n   \"statusCode\": 400,\n   \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n     1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n     2. You can interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n     3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\n   \"details\": [\n     {\n       \"code\": \"CrashLoopBackOff\",\n       \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n     1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\n     2. You can interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n     3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\n     },\n     {\n       \"code\": \"AciDeploymentFailed\",\n       \"message\": \"Your container application crashed. Please follow the steps to debug:\n     1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\n     2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.\n     3. You can also interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n     4. View the diagnostic events to check status of container, it may help you to debug the issue.\n \"RestartCount\": 5\n \"CurrentState\": {\"state\":\"Waiting\",\"startTime\":null,\"exitCode\":null,\"finishTime\":null,\"detailStatus\":\"CrashLoopBackOff: Back-off restarting failed\"}\n \"PreviousState\": {\"state\":\"Terminated\",\"startTime\":\"2022-06-17T10:56:57.554Z\",\"exitCode\":111,\"finishTime\":\"2022-06-17T10:57:01.314Z\",\"detailStatus\":\"Error\"}\n \"Events\":\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:26:38Z\",\"lastTimestamp\":\"2022-06-17T10:26:38Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:27:42Z\",\"lastTimestamp\":\"2022-06-17T10:27:42Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":10,\"firstTimestamp\":\"2022-06-17T10:28:00Z\",\"lastTimestamp\":\"2022-06-17T10:47:11Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n {\"count\":9,\"firstTimestamp\":\"2022-06-17T10:28:03Z\",\"lastTimestamp\":\"2022-06-17T10:40:46Z\",\"name\":\"Killing\",\"message\":\"Killing container with id a7e717efa63259b36b19bc4951b3f3dcc5f1093177e729c589355a7371353ca3.\",\"type\":\"Normal\"}\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:31:33Z\",\"lastTimestamp\":\"2022-06-17T10:31:33Z\",\"name\":\"Pulling\",\"message\":\"pulling image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":1,\"firstTimestamp\":\"2022-06-17T10:32:31Z\",\"lastTimestamp\":\"2022-06-17T10:32:31Z\",\"name\":\"Pulled\",\"message\":\"Successfully pulled image \"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\"\",\"type\":\"Normal\"}\n {\"count\":5,\"firstTimestamp\":\"2022-06-17T10:47:52Z\",\"lastTimestamp\":\"2022-06-17T10:52:09Z\",\"name\":\"Started\",\"message\":\"Started container\",\"type\":\"Normal\"}\n {\"count\":6,\"firstTimestamp\":\"2022-06-17T10:48:26Z\",\"lastTimestamp\":\"2022-06-17T10:52:37Z\",\"name\":\"Killing\",\"message\":\"Killing container with id 2bdec1005a6dd58312e10ab939d88ea08b312771de6573d9b86c5f571104277e.\",\"type\":\"Normal\"}\n \"\n     }\n   ]\n }\n     InnerException None\n     ErrorResponse \n {\n     \"error\": {\n         \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: 93d48e89-cb16-4d1c-bbb6-f453acaeaa7f\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\n\\t1. Please check the logs for your container instance: aml-live-service-model. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\\n\\t2. You can interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t3. You can also try to run image libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31 locally. Please refer to https:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    },\\n    {\\n      \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n      \\\"message\\\": \\\"Your container application crashed. Please follow the steps to debug:\\n\\t1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\\n\\t2. If your container application crashed. This may be caused by errors in your scoring file's init() function. You can try debugging locally first. Please refer to https:\/\/aka.ms\/debugimage#debug-locally for more information.\\n\\t3. You can also interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\\n\\t4. View the diagnostic events to check status of container, it may help you to debug the issue.\\n\\\"RestartCount\\\": 5\\n\\\"CurrentState\\\": {\\\"state\\\":\\\"Waiting\\\",\\\"startTime\\\":null,\\\"exitCode\\\":null,\\\"finishTime\\\":null,\\\"detailStatus\\\":\\\"CrashLoopBackOff: Back-off restarting failed\\\"}\\n\\\"PreviousState\\\": {\\\"state\\\":\\\"Terminated\\\",\\\"startTime\\\":\\\"2022-06-17T10:56:57.554Z\\\",\\\"exitCode\\\":111,\\\"finishTime\\\":\\\"2022-06-17T10:57:01.314Z\\\",\\\"detailStatus\\\":\\\"Error\\\"}\\n\\\"Events\\\":\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-06-17T10:26:38Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:26:38Z\\\",\\\"name\\\":\\\"Pulling\\\",\\\"message\\\":\\\"pulling image \\\"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-06-17T10:27:42Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:27:42Z\\\",\\\"name\\\":\\\"Pulled\\\",\\\"message\\\":\\\"Successfully pulled image \\\"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":10,\\\"firstTimestamp\\\":\\\"2022-06-17T10:28:00Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:47:11Z\\\",\\\"name\\\":\\\"Started\\\",\\\"message\\\":\\\"Started container\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":9,\\\"firstTimestamp\\\":\\\"2022-06-17T10:28:03Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:40:46Z\\\",\\\"name\\\":\\\"Killing\\\",\\\"message\\\":\\\"Killing container with id a7e717efa63259b36b19bc4951b3f3dcc5f1093177e729c589355a7371353ca3.\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-06-17T10:31:33Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:31:33Z\\\",\\\"name\\\":\\\"Pulling\\\",\\\"message\\\":\\\"pulling image \\\"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":1,\\\"firstTimestamp\\\":\\\"2022-06-17T10:32:31Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:32:31Z\\\",\\\"name\\\":\\\"Pulled\\\",\\\"message\\\":\\\"Successfully pulled image \\\"libraaimlwor4c93b458.azurecr.io\/azureml\/azureml_8fd1decee2b379a3d59fed509c692f31@sha256:47fc896a553e4b7bb972cbaa9a31de99b4755688dd525b9c725563ddde86aa0c\\\"\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":5,\\\"firstTimestamp\\\":\\\"2022-06-17T10:47:52Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:52:09Z\\\",\\\"name\\\":\\\"Started\\\",\\\"message\\\":\\\"Started container\\\",\\\"type\\\":\\\"Normal\\\"}\\n{\\\"count\\\":6,\\\"firstTimestamp\\\":\\\"2022-06-17T10:48:26Z\\\",\\\"lastTimestamp\\\":\\\"2022-06-17T10:52:37Z\\\",\\\"name\\\":\\\"Killing\\\",\\\"message\\\":\\\"Killing container with id 2bdec1005a6dd58312e10ab939d88ea08b312771de6573d9b86c5f571104277e.\\\",\\\"type\\\":\\\"Normal\\\"}\\n\\\"\\n    }\\n  ]\\n}\"\n     }\n }",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Register Azure ML Model from DatabricksStep",
        "Question_creation_time":1605264849177,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/162055\/register-azure-ml-model-from-databricksstep.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nI'm calculating a model while executing a DatabricksStep in an Azure ML Pipeline, save it on my Blob Storage as .pkl file and upload it to the current Azure ML Run using Run.upload_file (). All this works without any problems.\n\nBut as soon as I try to register the model to the Azure ML Workspace using Run.register_model (), the script throws the following error:\n\nUserErrorException: UserErrorException:\nMessage:\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\n1. You are not authorized to access this resource, or directory listing denied.\n2. you may not login your azure service, or use other subscription, you can check your\ndefault account by running azure cli commend:\n'az account list -o table'.\n3. You have multiple objects\/login session opened, please close all session and try again.\n\n InnerException None\n ErrorResponse \n\n{\n\"error\": {\n\"code\": \"UserError\",\n\"message\": \"\\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\\n1. You are not authorized to access this resource, or directory listing denied.\\n2. you may not login your azure service, or use other subscription, you can check your\\ndefault account by running azure cli commend:\\n'az account list -o table'.\\n3. You have multiple objects\/login session opened, please close all session and try again.\\n \"\n}\n}\n\nwith the following call stack\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/models_client.py in register_model(self, name, tags, properties, description, url, mime_type, framework, framework_version, unpack, experiment_name, run_id, datasets, sample_input_data, sample_output_data, resource_requirements)\n70 return self.\\\n71 _execute_with_workspace_arguments(self._client.ml_models.register, model,\n---> 72 custom_headers=ModelsClient.get_modelmanagement_custom_headers())\n73\n74 @error_with_model_id_handling\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/workspace_client.py in _execute_with_workspace_arguments(self, func, args, *kwargs)\n65\n66 def _execute_with_workspace_arguments(self, func, args, *kwargs):\n---> 67 return self._execute_with_arguments(func, copy.deepcopy(self._workspace_arguments), args, *kwargs)\n68\n69 def get_or_create_experiment(self, experiment_name, is_async=False):\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_arguments(self, func, args_list, args, *kwargs)\n536 return self._call_paginated_api(func, args_list, *kwargs)\n537 else:\n--> 538 return self._call_api(func, args_list, *kwargs)\n539 except ErrorResponseException as e:\n540 raise ServiceException(e)\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _call_api(self, func, args, *kwargs)\n234 return AsyncTask(future, _ident=ident, _parent_logger=self._logger)\n235 else:\n--> 236 return self._execute_with_base_arguments(func, args, *kwargs)\n237\n238 def _call_paginated_api(self, func, args, *kwargs):\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_base_arguments(self, func, args, *kwargs)\n323 total_retry = 0 if self.retries < 0 else self.retries\n324 return ClientBase._execute_func_internal(\n--> 325 back_off, total_retry, self._logger, func, _noop_reset, args, *kwargs)\n326\n327 @classmethod\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, args, *kwargs)\n343 return func(args, *kwargs)\n344 except Exception as error:\n--> 345 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n346\n347 reset_func(args, *kwargs) # reset_func is expected to undo any side effects from a failed func call.\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)\n384 3. You have multiple objects\/login session opened, please close all session and try again.\n385 \"\"\"\n--> 386 raise_from(UserErrorException(error_msg), error)\n387\n388 elif error.response.status_code == 429:\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/six.py in raise_from(value, from_value)\n\nDid anybody experience the same error and knows what is its cause and how to solve it?\n\nBest,\nJonas",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Upgrading Azure Machine Learning Compute Instance (virtual machine)",
        "Question_creation_time":1591430074777,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/32793\/upgrading-azure-machine-learning-compute-instance.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\n\nHow can I safely upgrade the default OS and softwares of a compute instance (Azure Machine Learning)?\n\nThe default compute instance comes with pre-default versions for OS and packages.\n\nFor example, the following are a few which I\u2019ve inspected after spinning on a compute instance:\n1. OS is currently an Ubuntu LTS 16.04 (release date February 28, 2019)\n2. Python version 3.6.9 (release date July 2, 2019)\n3. R version 3.6.3 (release date February 29, 2020)\n\nThe latest stable versions of the above (as of today June 4,2020) are:\n1. Ubuntu 20.04 LTS (release date April 23,2020)\n2. Python version 3.7.7 (release date March 10,2020)\n3. R version 4.0 (release date April 24,2020)\n\nI actually tried upgrading the OS through the command line but it doesn't seem to be stable after the upgrade to Ubuntu LTS 18.04.\nUpgrading RStudio server also fails.\n\nSo I'm not entirely confident whether the usual commands to upgrade work.\nIs there anything I need to do with the repository list to ensure smooth upgrades and updates?\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Question about ML output",
        "Question_creation_time":1650351269333,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/817010\/question-about-ml-output.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Dear Support,\n\nI have draft a pipeline as following:\n\n\n\n\n\n\nyou can see it has 2 webservice output,\n\nmay I know where I can see the 2 output and how to use it ? thnaks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure endpoint in decimal notation",
        "Question_creation_time":1639603918890,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/665285\/azure-endpoint-in-decimal-notation.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello, I've set up an Azure endpoint and I'm trying to communicate with it using some old software that can only read decimal notation. The scientific notation the endpoint occasionally delivers is breaking it. Is there a way to configure the endpoint to return only decimal notation? Ideally just with the correct header like \"application\/jsonlegacy\" or something?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Question_creation_time":1638821054557,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.\n\nI have tried using poetry to export requirements.txt file and using it with the\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.\n\nBut could there be any elegant solution to use toml file directly for creating a custom environment ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Files in gitignore are not excluded in snapshot created by ML pipeline",
        "Question_creation_time":1617897599523,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/349783\/files-in-gitignore-are-not-excluded-in-snapshot-cr.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to run a pipeline (built via Python SDK), and I want the repo to be snapshotted, because the different modules are called inside the pipeline scripts. The source folder I pass to the pipeline has the following structure:\n\nrepo\n- src\n- data\n- script_step1.py\n- script_step2.py\n- script_step3.py\n- .gitignore\n\n\n\n\nI want the data folder to be ignored in the snapshot, so in my gitignore file I have al line with written \/data\/*, but when I try to run the pipeline I get an error that tells me that the snapshot is too big (I checked the size of the other stuff except for data folder and it is very little)\nI'm not understanding why that happens.\nThanks a lot in advance",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how can i retrain the model after a period of time",
        "Question_creation_time":1619232734657,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370080\/how-can-i-retrain-the-model-after-a-period-of-time.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello everyone, i'm using lambda architecture to build a fraud detection project , i build my model using machine learning in databricks , after saving the model , i load the model in the speed layer to predict the incoming data, i want to know how can i retrain this model using new incoming data from eventhub ??\ndoes the retrain should be in the batch layer ?\nthanks for helping",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where to look for Source Code while implementing Azure Machine learning?",
        "Question_creation_time":1645806024193,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/751130\/where-to-look-for-source-code-while-implementing-a.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am implementing a training pipeline using Azure Machine learning Pipeline architecture. I am interested in looking at the Source code, for example Hyper-drive step class or python-script step class. Where should I look for Source code?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Accessing Data from azure container from a notebook",
        "Question_creation_time":1617804754800,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/348022\/accessing-data-from-azure-container-from-a-noteboo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey there,\n\ni have a question concerning the navigation in the file system of the azure cloud.\n\nHow can i access data (like images) from my container through a azure notebook?\nCan i navigate in the cloud like in the file system of my local machine?\n\nBest regards and thank you in advance!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Anyways to have multiple webservice output?",
        "Question_creation_time":1653904138797,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869627\/anyways-to-have-multiple-webservice-output.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is training two web service at the same time doable in studio ? Since I want to train with multiple models but it seems only one would work",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\" Object of type 'int64' is not JSON serializable\" when running automl time series",
        "Question_creation_time":1599571606537,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/89272\/34-object-of-type-39int6439-is-not-json-serializab.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to use the Online ML studio and running an \"Automated ML\". I upload my dataset (see simple example below) which passes fine and then I start a automl experiment selecting \"time series forecasting\". I select all the revelant fields and everything starts without any issues.\n\nShortly after the process fails and the error given is:\n\n\"User error: User program failed with TypeError: Object of type 'int64' is not JSON serializable\"\n\nDigging into the logs the only log with any useful information appears to be the driver_log which has these lines with no more detail about the error unless the INFO about streaming is actually an error not information:\n\n2020-09-08 11:17:01.734 - INFO - Successfully retrieved data using dataprep.\n2020-09-08 11:17:01.734 - INFO - Streaming is not conducive due to incompatible settings. Reason[s]: [Forecasting is not supported, 'n_cross_validations' was non-empty]\n2020-09-08 11:17:01.734 - INFO - Service responded with streaming disabled\n2020-09-08 11:17:01.734 - INFO - Inferring type for feature columns.\n2020-09-08 11:17:12.669 - INFO - Error in setup_wrapper.\n2020-09-08 11:17:12.670 - ERROR - Marking Run AutoML_f5a7c759-653c-4314-98a9-c2afbcecff55_setup as Failed.\n\n\n\n\nCan anyone suggest an answer or recommend some ways to debug this?\n\n][1]",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Failure when submitting pipe line",
        "Question_creation_time":1598970144413,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/83451\/failure-when-submitting-pipe-line.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Doing exam training using my free subscription (Exam DP-100: Designing and Implementing a Data Science Solution on Azure).\nGot in to problem in the following mudule, https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-regression-model-azure-machine-learning-designer\/explore-data.\nGets following error in my pipe line in Microsoft Azure Machine Learning:\nUnable to get image details : Unable to fetch workspace resources: Not Found response body: {\"error\":{\"code\":\"ResourceNotFound\",\"m",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Microsoft Azure Machine Learning Studio - Error durind Deploy <Response [502]> Automated ML",
        "Question_creation_time":1646161278517,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755239\/microsoft-azure-machine-learning-studio-error-duri.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Passing data from Azure Data Factory into Azure ML",
        "Question_creation_time":1626444050490,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/479034\/passing-data-from-azure-data-factory-into-azure-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"In Azure ML the input data has to be defined as a Dataset (to create a pipeline). In my code I am passing datasets with the following syntax: input_data = Dataset.File.from_files(datapath)\n\nI would like to change this datapath as an input parameter from Data Factory (for example via PipelineParamater), so I can apply the same Data Factory pipeline for different datasets. However, in Data Factory you can only pass string as a parameter, not a DataPath.\n\nWhat is the solution around this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can machine learning rewrite\/recognize text to one truth",
        "Question_creation_time":1604082069767,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/146531\/can-machine-learning-rewriterecognize-text-to-one.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi there,\n\nMy dataset has a lot of productnames, all the product of the shops are not written by the same.\nSo i want azure can recognize if it's the same:\n\nSo if the productgroup is X and productname looks like\/contains tomato. The product is tomato.\nExample: Tomatoes, tomato, bunch of tomatoes, a bag of tomatoes, small tomatoes = new colom tomato.\n\nHopefully someone can help me with this?\n\nThanks a lot.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify do not allow reuse in Azure Machine Learning designer",
        "Question_creation_time":1617640132493,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/344626\/how-to-specify-do-not-allow-reuse-in-azure-machine.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there a way in the AML designer to set a pipeline and\/or specific step to now allow reuse between runs? I've seen quite a few posts on how to do this in code, but I can't seem to find a way to set that property in the designer.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning - Uses invalid Pytorch version when training",
        "Question_creation_time":1629119160593,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/515579\/azure-machine-learning-uses-invalid-pytorch-versio.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I am training my models via Azure Machine Learning.\n\nOn other day, my training is running with GPU support, however today I found my training is running on a CPU.\nI'm not modified training environment, only training script was modified.\nMy computing cluster is NC6v3 - have a GPU.\n\nI investigate a situation, and I found training script is running on PyTorch 1.6.0.\nOn other day, it ran on Pytorch 1.8.1.\nI think my \"don't use GPU\" problem is caused by the situation that CUDA toolkit version is not suitable for Pytorch version.\n\nThen, I output a installed package to the log.\nThe log says 'Pytorch 1.8.1 was installed, however uses 1.6.0'.\nI confused by this weird circumstances.\nCan someone tell me the solution?\n\n<My code snippet>\n<<conda_dependencies.yaml>>\n\nchannels:\n- conda-forge\n- pytorch\n- nvidia\ndependencies:\n- python=3.8.10\n- mesa-libgl-cos6-x86_64\n- cudatoolkit=11.1\n- pytorch==1.8.1\n- torchvision==0.9.1\n- tqdm\n- scikit-learn\n- matplotlib\n- pandas\n- pip < 20.3\n- pip:\n- azureml-defaults\n- opencv-python-headless\n- pillow==8.2.0\n\n<<Environment definition>>\nenvironment_definition_file = experiment_dir \/ 'conda_dependencies.yaml'\nenvironment_name = 'pytorch-1.8.1-gpu'\nbase_image_name = 'mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04'\nenvironment = Environment.from_docker_image(environment_name, base_image_name, conda_specification = environment_definition_file)\ndocker_run_config = DockerConfiguration(use_docker=True)\n\nscript_run_config = ScriptRunConfig(\nsource_directory = experiment_dir,\nscript = SCRIPT_FILE_NAME,\narguments = arguments,\ncompute_target = compute_target,\ndocker_runtime_config = docker_run_config,\nenvironment = environment)\n\n<<Output a log in the training script>>\nimport torch\nimport pip\n\npip.main(['list'])\nprint(f'PyTorch version: {torch.version}')\n\n<My logs>\nPackage Version\n\n\nadal 1.2.7\napplicationinsights 0.11.10\n(omission)\ntorch 1.8.1\ntorchvision 0.9.0a0\n(omission)\n\nPyTorch version: 1.6.0",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How long does it take to use a resource after you creat it?",
        "Question_creation_time":1645195183417,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/741883\/how-long-does-it-take-to-use-a-resource-after-you.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I cannot see my machine learning resource I created.\nAfter you create a resource under the resource group, how long does it take to actually see it and start using it?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Clean Up instructions at the end of the module to stop compute resources.",
        "Question_creation_time":1643823950947,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/720158\/clean-up-instructions-at-the-end-of-the-module-to.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\n\nI am currently doing cloudskillschallenge for getting certified in Azure Data Scientist course and I came across a point in Module 6 : \"Use automated machine learning in Azure Machine Learning\" that says \"After completing each module, be sure to follow the Clean Up instructions at the end of the module to stop your compute resources. Stopping your compute ensures your subscription won't be charged for compute resources.\"\n\nI have opted for the free trial in Azure portal and would like to know how to do the said process of removing the instructions.\n\n\n\n\nRegards,\nTuhin",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"azure machine learning SDK",
        "Question_creation_time":1654035149473,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/872050\/azure-machine-learning-sdk.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How to import data not by passing it as an argument,\nI do not want to do as the tutorial https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets?source=docs",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Auto ML forecasting - dependent variables",
        "Question_creation_time":1613448096113,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/274229\/azure-auto-ml-forecasting-dependent-variables.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I am trying to use Azure auto ML for forecasting. The dataset has a date_time column, a target variable and other columns that affect the target variable. I have deployed the model as a web service. But I am finding it hard to use the service\/model for forecasting future frames. Let's say I need to forecast for the next 4 hours (data frequency is 5 minutes), but the model is asking for other column inputs as well. Can you please help me to resolve this?\nTIA,\nRajesh",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Missing principal component analysis module in Azure ML Designer",
        "Question_creation_time":1614943157500,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/300776\/missing-principal-component-analysis-module-in-azu.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I cannot find the Principal Component module in Azure ml designer. For the classic ML studio version it used to be under the data transformation group of transformations but seems to be no longer there. Am I missing something? Thanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML - Managed Identity for Compute Instance",
        "Question_creation_time":1637143820660,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/630520\/azure-ml-managed-identity-for-compute-instance.html",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We need to connect Azure Data Lake Storage Gen2 to Azure Machine Learning by means of a datastore. For security reasons we do not want to provide the credential-based authentication credentials (service principal or SAS token). Instead we want to connect with identity based access.\n\nThe problem we face is that we are not able to assign a managed identity to a compute instance, so we can connect from notebooks to the Data Lake. In the documentation is explained how to assign a managed identity to a cluster, but we need the same for the compute instance, as it is the only way to run commands directly from the notebook.\n\nIs there a way to assign managed identity to an Azure Machine Learning Compute Instance? Otherwise, we would like to know the best approach to overcome this issue, considering that we do not want to introduce the credentials in the code.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error in init() function during azurel ML deployment due to model path definition",
        "Question_creation_time":1650517247760,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/820362\/error-in-init-function-during-azurel-ml-deployment.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello Team,\n\nWe are trying to deploy a model to Azure ML workspace containing a saved model & One Hot Encoded joblib file.\nWe are facing issue in init() function.\n\nPlease find the below error message:\n\n\"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\n\n\n\nPFB screenshot of scoring file:",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Group Categorical Data module missing in Designer, how to reduce number of levels?",
        "Question_creation_time":1620183260377,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/383026\/group-categorical-data-module-missing-in-designer.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,\n\nI'm working through a tutorial on reducing the number of levels of a categorical variable before using the \"Convert to Indicator Values\" module. In the tutorial, the presenter is using the classic studio which has a module called \"Group Categorical Data\". Unfortunately, I'm using ML Designer and it doesn't have that module.\n\nIs there an easy workaround to reduce the number of categorical levels in Designer before using the Convert to Indicator Values module?\n\nThanks kindly,",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Response 502 error in Azure Machine Learning Studio Notebook",
        "Question_creation_time":1643768437050,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/718999\/response-502-error-in-azure-machine-learning-studi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I copied over code from my training course into the Notebook in Azure Machine Learning Studio. After I run the code, I get the error - <Response [502]>. Please help",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML workspace blob structure \/ Can I safely delete these blobs?",
        "Question_creation_time":1647500925610,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775834\/azure-ml-workspace-blob-structure-can-i-safely-del.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI am trying to figure out the folder structure of Azure ML workspace in my storage account.\nI want to be able to delete old pipeline runs and experiments that have piled up in my workspace directly from Azure Storage Explorer without breaking the system.\nMy datastores and folder structure are as follows:\n\nDatastore: workspaceartifactstore\nBlob container: azureml\nFolder structure:\n\u251c\u2500 ComputeRecord\n\u251c\u2500 Dataset\n\u251c\u2500 ExperimentRun\n\u251c\u2500 LocalUpload\n\nDatastore: workspaceblobstore (Default)\nBlob container: azureml-blobstore-(a series of numbers)\nFolder structure:\n\u251c\u2500 azureml\n\u2502 \u251c\u2500\u2500 (a series of numbers)-setup\n\u2502 \u2502 \u251c\u2500\u2500 _tracer.py\n\u2502 \u2502 \u251c\u2500\u2500 azureml_globals.py\n\u2502 \u2502 \u251c\u2500\u2500 context_managers.py\n\u2502 \u2502 \u251c\u2500\u2500 job_prep.py\n\u2502 \u2502 \u251c\u2500\u2500 log_history_status.py\n\u2502 \u2502 \u251c\u2500\u2500 request_utilities.py\n\u2502 \u2502 \u251c\u2500\u2500 run_token_provider.py\n\u2502 \u2502 \u251c\u2500\u2500 utility_context_managers.py\n\u2502 \u251c\u2500\u2500 (another series of numbers)-setup\n\u2502 \u2502 \u251c\u2500\u2500 sames files as above\n\nIt would help if I understood what does each of these containers actually store.\nI already tried to delete all blobs stored in 'workspaceblobstore', but it didn't remove any pipeline or experiment from ML Studio.\nI have a few datasets registered in my workspace, and I don't want to delete them (nor unregister them).\n\nCan I set a data retention policy on both containers in order to delete old blobs?\nCan I safely delete the blobs (folders) stored in 'workspaceartifactstore' too? Will they be recreated automatically when I run a new experiment?\nWhy are there two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers? Is it possible to merge them?\n\nThanks.\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Version control of the raw data with the colleagues simultaneously",
        "Question_creation_time":1646030816402,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/version-control-of-the-raw-data-with-the-colleagues-simultaneously\/1080",
        "Question_upvote_count":1.0,
        "Question_view_count":216.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I recently came across DVC while looking for a tool for data versioning and I\u2019ve found DVC very useful, so I\u2019m testing several features for data versioning with my colleagues.<\/p>\n<p>But we have difficulties with managing raw dataset when we deal with it simultaneously and want to ask you some help\/guide.<\/p>\n<p>First, we describe our system as follows:(Assume there are two colleagues - denoted as Col1, Col2, respectively)<\/p>\n<ul>\n<li>\n<p>Col1 &amp; Col2 have a workspace respectively (denoted as WS1 and WS2) and each workspace has \u2018mounted NAS\u2019 which stores image data and should be shared.<br>\nPath for WS1 : \/home\/work\/dvc_col1    %% Here Col1 execute git init, dvc init, etc.<br>\nPath for image data to share among colleagues in NAS: \/home\/work\/mnt\/storage\/img_data<\/p>\n<p>Path for WS2: \/home\/work\/dvc_col2     %% Here Col2 execute git init, dvc init, etc.<br>\nPath for image data to share among colleagues in NAS: \/home\/work\/mnt\/storage\/img_data<\/p>\n<\/li>\n<\/ul>\n<p>Col1 initially starts data versioning following DVC docs as follows:<br>\nSince the data to be versioned is located outside of the workspace, Col1 execute the followings in WS1:<\/p>\n<pre><code>dvc cache dir \/home\/work\/mnt\/save_cache\ndvc config cache.shared group\ngit add .dvc\ngit commit -m \"set cache dir\"\ndvc add \/home\/work\/mnt\/storage\/img_data --external\ngit add img_data.dvc\ngit commit -m \"1st version data, 300 images\"\ngit tag -a \"v1.0\" -m \"1st version\"\n<\/code><\/pre>\n<p>(\u2014&gt;&gt; This 1st version includes 300 images.)<\/p>\n<ul>\n<li>\n<p>And additional 200 images are added to \/home\/work\/mnt\/storage\/img_data. (so in total 500 images.)<br>\nTo mange the version of the dataset, Col1 execute the followings:<\/p>\n<p>dvc add \/home\/work\/mnt\/storage\/img_data --external<br>\ngit add img_data.dvc<br>\ngit commit -m \u201c2nd version data, 200 images added, in total 500 images\u201d<br>\ngit tag -a \u201cv2.0\u201d -m \u201c2nd version\u201d<\/p>\n<\/li>\n<li>\n<p>Then Col1 can version the image data with \u2018git checkout v1.0 or v2.0\u2019 and \u2018dvc checkout\u2019.<\/p>\n<\/li>\n<li>\n<p>So, Col1 uses \u2018git push\u2019 to their GitLab project and Col2 downloaded it using git pull.<\/p>\n<\/li>\n<li>\n<p>After this, Col2 can also version the same image data with \u2018git checkout\u2019 and \u2018dvc checkout\u2019 and it works fine.<\/p>\n<\/li>\n<\/ul>\n<p>But problem arises here:<\/p>\n<ul>\n<li>\n<p>When Col1 execute \u2018git checkout v2.0\u2019 &amp; \u2018dvc checkout\u2019 in WS1, the number of images in \u2018\/home\/work\/mnt\/storage\/img_data\u2019 becomes 500 and Col2 can also recognize it.<\/p>\n<\/li>\n<li>\n<p>Since Col2 wants to use the 1st version of dataset, Col2 execute \u2018git checkout v1.0\u2019 &amp; \u2018dvc checkout\u2019 in WS2, and the number of images in \u2018\/home\/work\/mnt\/storage\/img_data\u2019 becomes 300. This makes problem, because now Col1 can\u2019t access the added 200 images anymore. \u2018\/home\/work\/mnt\/storage\/img_data\u2019 shows only 300 images, so Col1 and Col2 can\u2019t access to the dataset each wants to use SIMULTANEOUSLY.<\/p>\n<\/li>\n<\/ul>\n<p>Are we using DVC wrong? We want to access each of the versioned dataset simultaneous with DVC. We\u2019re very appreciated with your comment and help. Thank you.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Data versioning of databases",
        "Question_creation_time":1522998440046,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/data-versioning-of-databases\/20",
        "Question_upvote_count":0.0,
        "Question_view_count":517.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Are you thinking of supporting data versioning of databases.<br>\nTracking changes of transformations from raw data to cleaned data?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc get error: Unable to find DVC file",
        "Question_creation_time":1623955285707,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-error-unable-to-find-dvc-file\/797",
        "Question_upvote_count":2.0,
        "Question_view_count":1452.0,
        "Question_answer_count":12,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In a cloned local git repo, there is already some dvc files and I want to add new data.<br>\nIn the config file under .dvc folder, i have<\/p>\n<pre><code class=\"lang-auto\">[core]\n    remote = my-remote\n[cache]\n    type = \"reflink,hardlink\"\n['remote \"my-remote\"']\n    url = s3:\/\/...\n<\/code><\/pre>\n<p>And I did<\/p>\n<pre><code class=\"lang-auto\">dvc add --external \/path\/to\/mydata\/\ngit add mydata.dvc\ngit commit -m \"added my data\"\ndvc remote default my-remote\ndvc push\n<\/code><\/pre>\n<p>Everything is good so far since it shows <code>10000 files pushed <\/code><\/p>\n<p>But when I tried to download the data by <code>dvc get  &lt;git url&gt;   &lt;folder_name\/dvc_filename&gt;<\/code><br>\nIt returned<\/p>\n<pre><code class=\"lang-auto\">unexpected error - : Unable to find DVC file with output \n'..\/..\/..\/..\/..\/private\/var\/folders\/5v\/6xws5skx46z5rg2y33_nwd1mqcvql4\/T\/tmp26wt3huydvc-clone\/folder_name\/dvc_filename'\n<\/code><\/pre>\n<p>Can anyone helped with this?<\/p>\n<p>Thank you<\/p>\n<h2>\n<a name=\"debug-version-info-for-developers-dvc-version-230-pip-1\" class=\"anchor\" href=\"#debug-version-info-for-developers-dvc-version-230-pip-1\"><\/a>DEBUG: Version info for developers:<br>\nDVC version: 2.3.0 (pip)<\/h2>\n<p>Platform: Python 3.8.5 on macOS-10.16-x86_64-i386-64bit<br>\nSupports: http, https, s3, ssh<br>\nCache types: reflink, hardlink, symlink<br>\nCache directory: apfs on \/dev\/disk1s5s1<br>\nCaches: local<br>\nRemotes: s3<br>\nWorkspace directory: apfs on \/dev\/disk1s5s1<br>\nRepo: dvc, git<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to track file",
        "Question_creation_time":1619613397408,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-track-file\/739",
        "Question_upvote_count":1.0,
        "Question_view_count":372.0,
        "Question_answer_count":13,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>hello,<\/p>\n<p>\u0131 couldn\u2019t  track my folder. What am I doing wrong ?<\/p>\n<p>My folder hiearchy<\/p>\n<p>Folder<br>\n-01<\/p>\n<p>\u0131 m using \u201cdvc add Folder\u201d comment.<br>\nthen \u0131 added to git.<br>\ngit tag -a \u201cv1\u201d -m \u201cmessage\u201d<\/p>\n<p>then \u0131 added other file to Folder.<br>\nNew data hiearchy is<\/p>\n<p>Folder<br>\n-01<br>\n-02<br>\n-03<\/p>\n<p>\u0131 repeated same things.<\/p>\n<p>dvc add Folder<br>\ngit add Folder.dvc<br>\ngit tag -a \u201cv1.1\u201d -m \u201cmessage\u201d<\/p>\n<p>then \u0131 check the tag v1 \u0131 want to see just 01 file in folder but \u0131 saw 01 02 and 03 file. Why? What am I doing wrong ?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can't \"dvc pull\"",
        "Question_creation_time":1638181467849,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cant-dvc-pull\/991",
        "Question_upvote_count":0.0,
        "Question_view_count":375.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I\u2019ve tried to use dvc with an existing repository. When I do a \u201cdvc pull\u201d, I get the following error:<\/p>\n<p>Authentication successful.<\/p>\n<p>ERROR: unexpected error - : &lt;HttpError 404 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files\/1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B?fields=driveId&amp;supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files\/1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B?fields=driveId&amp;supportsAllDrives=true&amp;alt=json<\/a> returned \u201cFile not found: 1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B\u201d. Details: \"[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018notFound\u2019, \u2018message\u2019: \u2018File not found: 1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018file\u2019}]\u201d&gt;<\/p>\n<p>Running \u201cdvc doctor\u201d returns the following message:<\/p>\n<h2>\n<a name=\"dvc-version-282-osxpkg-1\" class=\"anchor\" href=\"#dvc-version-282-osxpkg-1\"><\/a>DVC version: 2.8.2 (osxpkg)<\/h2>\n<p>Platform: Python 3.8.12 on macOS-10.16-x86_64-i386-64bit<br>\nSupports:<br>\nazure (adlfs = 2021.9.1, knack = 0.8.2, azure-identity = 1.7.0),<br>\ngdrive (pydrive2 = 1.10.0),<br>\ngs (gcsfs = 2021.10.1),<br>\nhdfs (fsspec = 2021.10.1, pyarrow = 5.0.0),<br>\nwebhdfs (fsspec = 2021.10.1),<br>\nhttp (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.6),<br>\nhttps (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.6),<br>\ns3 (s3fs = 2021.10.1, boto3 = 1.17.106),<br>\nssh (sshfs = 2021.9.0),<br>\noss (ossfs = 2021.8.0),<br>\nwebdav (webdav4 = 0.9.3),<br>\nwebdavs (webdav4 = 0.9.3)<\/p>\n<p>Could anyone help me dealing with this? Much appreciated!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple pipelines with single metric file",
        "Question_creation_time":1569495407825,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-pipelines-with-single-metric-file\/233",
        "Question_upvote_count":2.0,
        "Question_view_count":717.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to define multiple pipelines sharing a single metrics file and this does not seem possible currently. I get an error when trying to add the same metric file to more than one pipeline.<\/p>\n<p>My use-case is, I am doing some ML experiments and would like to have multiple models that I work on in parallel, for example a SVM and a Naive Bayes model (maybe more models in future), both sharing the same data and much of the pre-processing steps. Then I have 2 pipelines, which are very similar, sharing many stages: model_svm.dvc and model_nb.dvc. So they are in the same repository and I would like that they both use the same metric file so that when I call \u201cdvc metrics show -T\u201d it will show past results from both models.<\/p>\n<p>Is this currently possible and if so how? Or am I trying to use the system in a way that was not intended, and if so could you recommend how I could restructure my project so that it fits better with how DVC was intended to be used?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"List all remote paths of tracked files",
        "Question_creation_time":1630558728484,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/list-all-remote-paths-of-tracked-files\/863",
        "Question_upvote_count":0.0,
        "Question_view_count":193.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>For a given commit, I\u2019d like to get a list of the local paths &amp; corresponding remote paths of all files tracked by DVC, possibly limited to a certain directory. Is there not an easy way to do this?<\/p>\n<p>Currently looking at a workaround involving \u201cdvc list -R --dvc-only --rev  \u2026\u201d to get all tracked files, reading the md5 out of the corresponding .dvc files, and then using those to build the remote paths. Doesn\u2019t seem very elegant.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Output with timestamp in it's name",
        "Question_creation_time":1635505180672,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/output-with-timestamp-in-its-name\/940",
        "Question_upvote_count":3.0,
        "Question_view_count":163.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello everyone,<\/p>\n<p>When running <code>dvc run<\/code> Is there any way to precise an output with an unknown name?<\/p>\n<p>Here is my situation: I have a step named <code>train_model<\/code> which will create a folder named in this format <code>models\/{timestamp}\/model.py<\/code> and another stage <code>optimize_model<\/code> which will use this output to create <code>models\/*\/model_freezed_jit.pt<\/code> where <code>*<\/code> is the timestamp.<\/p>\n<p>Because I can\u2019t precise the specific folder name, dvc can\u2019t find the output : <code>failed to reproduce 'dvc.yaml': output 'models\/*\/model.pt' does not exist<\/code>.<\/p>\n<p>Is there any way to do that?<\/p>\n<pre><code class=\"lang-auto\">stages:\n  train_model:\n    cmd: python src\/train_model.py\n    deps:\n    - data\/formated\/\n    - src\/train_model.py\n    outs:\n    - models\/*\/model.pt:\n        cache: false\n        persist: true\n  optimize_model:\n    cmd: python src\/model_optimizer.py\n    deps:\n    - models\/*\/model.pt\n    outs:\n    - models\/*\/model_freezed_jit.pt:\n        cache: false\n        persist: true\n    - models\/*\/half_model_freezed_jit.pt:\n        cache: false\n        persist: true\n    - models\/*\/quant_model_freezed_jit.pt:\n        cache: false\n        persist: true\n\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc experiments multiple branches workflow",
        "Question_creation_time":1647558581044,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-experiments-multiple-branches-workflow\/1124",
        "Question_upvote_count":0.0,
        "Question_view_count":156.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"><\/a>Background<\/h3>\n<p>I am using dvc with tensorflow. I have two versions of my model which are different enough to merit their own git branches, i.e. they have different architecture definitions and different data generators.<\/p>\n<p>To run experiments, I <code>git checkout<\/code> each branch, make my changes and queue up an experiment. I have two branches so I end up queuing up 2 experiments. Then I call <code>dvc exp run --run-all -j 2<\/code> to run them both in parallel.<\/p>\n<p>This is basically what <code>dvc exp show -a<\/code> looks like (don\u2019t pay attention to the metrics and params):<\/p>\n<pre><code class=\"lang-auto\">\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Experiment                 \u2503 Created      \u2503 auc.model_params \u2503 auc.model_t_params \u2503 auc.model_nt_params \u2503 train_model.walltime \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 workspace                  \u2502 -            \u2502         10787481 \u2502           10782921 \u2502                4560 \u2502 3:00:00:00           \u2502\n\u2502 branch_1                   \u2502 12:48 PM     \u2502         10787481 \u2502           10782921 \u2502                4560 \u2502 1:00:00:00           \u2502\n\u2502 \u2514\u2500\u2500 508af88 [exp_1]        \u2502 06:49 PM     \u2502                - \u2502                  - \u2502                   - \u2502 3:00:00:00           \u2502\n\u2502 \u251c\u2500\u2500 6d926b6 [exp_2]        \u2502 06:47 PM     \u2502                - \u2502                  - \u2502                   - \u2502 3:00:00:00           \u2502\n\u2502 branch_2                   \u2502 06:22 PM     \u2502           697998 \u2502             696878 \u2502                1120 \u2502 1:00:00:00           \u2502\n\u2502 master                     \u2502 Nov 17, 2021 \u2502                - \u2502                  - \u2502                   - \u2502 -                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n<\/code><\/pre>\n<h3>\n<a name=\"question-1-2\" class=\"anchor\" href=\"#question-1-2\"><\/a>Question 1<\/h3>\n<p>How do I interpret the lines next to the experiment names? It looks like <code>exp_1<\/code> experiment correctly connects to branch_1, but why does it look like the <code>exp_2<\/code> experiment connects to both branch_1 and branch_2? Did I set up these experiments incorrectly or is this a display bug or what?<\/p>\n<h3>\n<a name=\"question-2-3\" class=\"anchor\" href=\"#question-2-3\"><\/a>Question 2<\/h3>\n<p>My other more general question is: is this the intended workflow for managing experiments on multiple branches? Any comments on this question from a dvc expert would be very much appreciated.<\/p>\n<h2>\n<a name=\"thanks-4\" class=\"anchor\" href=\"#thanks-4\"><\/a>Thanks!<\/h2>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What are the possible values for `dvc config` command?",
        "Question_creation_time":1530525747567,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-are-the-possible-values-for-dvc-config-command\/46",
        "Question_upvote_count":2.0,
        "Question_view_count":610.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is there a place listing all the properties that can be configured using the <code>dvc config<\/code> command? I couldn\u2019t find such info in the documentation, for instance.<\/p>\n<p>This would be very helpful for people starting out, to understand the possibilities of DVC.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc exp show: experiment not showing \/ wrong position",
        "Question_creation_time":1641551077155,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-show-experiment-not-showing-wrong-position\/1019",
        "Question_upvote_count":0.0,
        "Question_view_count":363.0,
        "Question_answer_count":14,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<br>\nI am trying out dvc for a few days now and have a question regarding dvc exp show. My setup is:<\/p>\n<ul>\n<li>Have a git &lt;branch 1&gt; with a head commit &lt;commit 1&gt;<\/li>\n<li>Run an experiment on a remote machine (checkout &lt;commit 1&gt;, dvc pull, dvc exp run)<\/li>\n<li>Create a new branch (dvc exp branch) which creates &lt;branch 2&gt; and &lt;commit 2&gt; in it<\/li>\n<li>I push the experiment (dvc exp push)<\/li>\n<li>On my local machine, I fetch the experiment (dvc exp pull)<br>\n(* In the mean time, I have more commits on &lt;branch 1&gt;)<\/li>\n<\/ul>\n<p>Then, dvc exp show behaves unexpectedly:<\/p>\n<ul>\n<li>If I run dvc exp show -a, it shows &lt;branch 1&gt; and &lt;branch 2&gt;, but no experiment<\/li>\n<li>If I run dvc exp show -A, it shows the experiment under &lt;commit 1&gt;<\/li>\n<\/ul>\n<p>Expected behavior:<\/p>\n<ul>\n<li>If I run dvc exp show -a, it shows the experiment under &lt;branch 2&gt;<\/li>\n<\/ul>\n<p>Hope I got this across. Am I doing something wrong here?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can and how DVC work with shared network storage?",
        "Question_creation_time":1526441315809,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-and-how-dvc-work-with-shared-network-storage\/26",
        "Question_upvote_count":0.0,
        "Question_view_count":996.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We have developers working with data shared on a local network, and I\u2019d like to understand whether\/how dvc could integrate with this pipeline. I think I\u2019m asking whether its possible (or even makes sense) to have a single, shared cache \u2013 kinda like the dvc cloud workflow you describe but without push\/pull. The code just reads\/writes data outside the git repo.<\/p>\n<p>Another reason I\u2019d like to leave data outside the repo is because many projects have the same (large) dataset as a dependency.<\/p>\n<p><s>Is the answer hardlinks? Worried 'cause there\u2019s already a lot of linking going on \u2026<\/s> (Duh. Not across filesystems!)<\/p>\n<p>To be clear, this looks like an awesome tool that I\u2019d like to adapt to if possible.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Manage data from one dvc folder with colleagues",
        "Question_creation_time":1656560875701,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manage-data-from-one-dvc-folder-with-colleagues\/1229",
        "Question_upvote_count":0.0,
        "Question_view_count":89.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Thank you for creating this convenient data versioning tool.<\/p>\n<p>We\u2019re a team of several developers and we\u2019re dealing with a lot of image data. My question is how to manage conflicts when multiple people manage versions in one dvc folder.<\/p>\n<p>For example, if there is a total of 1000 data, developer 1 is learning the model using only 1 to 300 data, and developer 2 updates the data folder to use 200 to 500 data, developer 1 will lose its data.<\/p>\n<p>We\u2019ve also considered downloading and using data in our respective workspaces, but this is impossible because the amount of data is too large. Therefore, it seems that multiple people will have to manage the version in one data folder.<\/p>\n<p>Does the DVC support solutions for these problems?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remove\/Redefine Stage",
        "Question_creation_time":1592745757708,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/remove-redefine-stage\/411",
        "Question_upvote_count":2.0,
        "Question_view_count":1417.0,
        "Question_answer_count":12,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I made a mistake by defining a stage while forgetting to define its output with the <code>-o option<\/code>.  I tried removing the existing stage with <code>dvc remove pipelines.yaml:my-stage-name -p<\/code>, but this does not remove the stage from <code>pipelines.yaml<\/code>.  I eventually manually deleted the stage from <code>pipelines.yaml<\/code>, and created a new stage with the same name using <code>dvc run ...<\/code>.  My questions are:<\/p>\n<ol>\n<li>\n<p>What\u2019s the recommended way to redefine a stage?<\/p>\n<\/li>\n<li>\n<p>Is manually removing the stage from <code>pipelines.yaml<\/code> sufficient to completely delete the stage, or will there still be metadata about the stage elsewhere?<\/p>\n<\/li>\n<\/ol>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC Studio - We could not detect a DVC data remote in this repository",
        "Question_creation_time":1623234665789,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-studio-we-could-not-detect-a-dvc-data-remote-in-this-repository\/788",
        "Question_upvote_count":2.0,
        "Question_view_count":327.0,
        "Question_answer_count":12,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi folks,<\/p>\n<p>I\u2019m just trying out Studio.<br>\nBitbucket is used for Git and data is in Google Drive. Created a view and was able to connect to Bitbucket.<br>\nHowever, it appears that no remote control is identified. Note that I am able to dvc push and dvc pull from different computers without any problems. Any idea why?<\/p>\n<p>Cheers<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I use DVC with SSH remote",
        "Question_creation_time":1576046193023,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-do-i-use-dvc-with-ssh-remote\/279",
        "Question_upvote_count":12.0,
        "Question_view_count":15415.0,
        "Question_answer_count":23,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Q: I\u2019m just getting started with DVC, but I\u2019d like to use it for multiple developers to access the data and share models and code.<\/p>\n<p>I do own the server, but I\u2019m not sure how to use DVC with SSH remote?<\/p>\n<hr>\n<p>More context to this question and original versions are <a href=\"https:\/\/discordapp.com\/channels\/485586884165107732\/563406153334128681\/598866528984891403\">here<\/a>.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC assigns user permissions to folders in the ssh repository",
        "Question_creation_time":1632174009351,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-assigns-user-permissions-to-folders-in-the-ssh-repository\/898",
        "Question_upvote_count":0.0,
        "Question_view_count":571.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi everybody. I need some help with a dvc repo. I have a very simple repository with a couple of dirs shared. The repo has a remote in an ssh server owned and managed by our team. As a repo folder in the ssh-server I created a folder (dvc.cache) owned by a unix group \u2018dvc-group\u2019.  The problem is that when some user pushes files to the repo (with dvc push) and this requires the creation of a new repo folder (e.g., dvc.cache\/fa), this folder is created with writing permissions only for the user, not for the dvc-group. This causes that when other users try to push modifications occuring in that folder he\/she gets an [Errno 13] Permission denied, as expected.<br>\nMy problem is that I couldn\u2019t find a way to fix this other than manually logging into the server an changing the permissions of these folders.<br>\nAny ideas how to fix this for good with DVC?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Move a .dvc stage without re-run",
        "Question_creation_time":1589374711255,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/move-a-dvc-stage-without-re-run\/384",
        "Question_upvote_count":3.0,
        "Question_view_count":416.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I just ran a <code>dvc run ...<\/code> but forgot to specify the <code>-f<\/code> flag so it created a <code>.dvc<\/code> file in an unintended location. Is there a smart way of moving it without re-running the stage? This method should take care of location and relative paths <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Keeping a record of old and rejected experiments",
        "Question_creation_time":1662031931395,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/keeping-a-record-of-old-and-rejected-experiments\/1318",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, what is the workflow for saving old experiments that haven\u2019t \u201cfailed\u201d in the sense there was some error and no output was created, but were not the best performing (by whatever metric) and therefore not persisted?<\/p>\n<p>The documentation speaks of persisting experiments (by turning them into a branch) and removing the old ones by <code>dvc exp gc\/remove<\/code>, but I would like to keep a record of my past attempts of trying to improve the performance. The parameters and performance metrics at a minimum and preferably without bloating the cache and remote storage (I don\u2019t actually need the data as I have rejected them).<\/p>\n<p>Thank  you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Documentation: tutorial problem?",
        "Question_creation_time":1530112347491,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/documentation-tutorial-problem\/40",
        "Question_upvote_count":1.0,
        "Question_view_count":656.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I understand this may not be fully related to dvc, but since the problem happens when following the tutorial at <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noopener\">https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46<\/a>. So here is my question:<\/p>\n<p>When following tutorial at the step in executing<\/p>\n<pre><code>dvc run -d data\/Posts.tsv -d code\/split_train_test.py         -d code\/conf.py         -o data\/Posts-test.tsv -o data\/Posts-train.tsv         python code\/split_train_test.py 0.33 20180319\n<\/code><\/pre>\n<p>it throws error<\/p>\n<pre><code>from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \\\nImportError: \/tmp\/_MEIUqCWxh\/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by \/usr\/lib\/python2.7\/dist-packages\/scipy\/sparse\/_sparsetools.x86_64-linux-gnu.so)\nFailed to run command: Stage 'Posts-test.tsv.dvc' cmd python code\/split_train_test.py 0.33 20180319 failed\n<\/code><\/pre>\n<p>I am not familiar with python, nor data science, but was just trying to evaluate if dvc fits our internal requirement so we can decide if going with dvc or not.<\/p>\n<p>How can I fix this error? Otherwise any even simpler version that can basically just show dataset, model are versioned so we can see the differences, say, between version 0.0.1 and 0.0.2 and its diff, or that kind of things?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to manage files on remote repository",
        "Question_creation_time":1588035461752,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-manage-files-on-remote-repository\/372",
        "Question_upvote_count":3.0,
        "Question_view_count":596.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<\/p>\n<p>First of all thanks for the great tool that really help manage ML practice!<br>\nRight now I\u2019m setup repo in my local pc share folder and tried dvc for a while, my main usage now is to datafile versioning<\/p>\n<p>My problem is how to handle the growth of files in the repository. It is one of showstopper for me to promote to team or larger scale usage \u2026<\/p>\n<p>If there is any mechanism to support keeping track latest 5 versions of file \u201cX\u201d or any statistic could tell how many files we have, how many version we save in repo? Checked tracked data files on repo they are compressed with hashed filename. It is hard to convert to back filename for analytics.<\/p>\n<p>Any tool or better way to handle that? thanks in advance<\/p>\n<p>Donald<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC push to SSH remote ERROR: No such file",
        "Question_creation_time":1654677490086,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-to-ssh-remote-error-no-such-file\/1203",
        "Question_upvote_count":1.0,
        "Question_view_count":150.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I do the Getting started guide.<br>\nWhen it is time to add a remote, instead of proposed S3 storage, I use an SSH remote that I add using the following command:<\/p>\n<pre><code>dvc remote add -d storage ssh:\/\/ws\/hddb\/data\/dvc-tutorial\n<\/code><\/pre>\n<p>where ws is a hostname of a workstation with the SSH access enabled, and <code>hddb\/data\/dvc-tutorial<\/code> is the path to the directory in which I would like to keep the data. I actually execute this command being on the workstation through SSH (I can recursively ssh to this workstation when I am already ssh\u2019ed to it).<\/p>\n<p>Then I have to execute the following command to use passphrase-protected SSH keys:<\/p>\n<pre><code>dvc remote modify --local storage password my-lovely-passphrase\n<\/code><\/pre>\n<p>When I do <code>dvc push<\/code> to transfer <code>data\/data.xml<\/code> from the Getting started guide, it fails when the following error:<\/p>\n<pre><code class=\"lang-auto\">\u279c dvc push                                             \nERROR: failed to transfer 'md5: 22a1a2931c8370d3aeedd7183606fd7f' - [Errno 2] No such file or directory: No such file\nERROR: failed to push data to the cloud - 1 files failed to upload\n<\/code><\/pre>\n<p>What should I do to make an SSH remote work? Thank you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Public read-only s3 remote",
        "Question_creation_time":1587508457422,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/public-read-only-s3-remote\/355",
        "Question_upvote_count":8.0,
        "Question_view_count":1573.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey folks,<\/p>\n<p>The DVC <a href=\"https:\/\/github.com\/iterative\/example-get-started\" rel=\"nofollow noopener\">getting started example<\/a> has a read-only HTTP remote.<\/p>\n<p>Is it possible to do the same with S3 so the public can have read-only permission? I believe I\u2019ve set my bucket permissions correctly* (I can download w\/o AWS creds), but when I try <code>dvc pull<\/code> w\/o creds I get an error:<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull\nERROR: unexpected error - Unable to locate credentials \n<\/code><\/pre>\n<p>It looks like <code>dvc pull<\/code> is checking for credentials regardless.<\/p>\n<ul>\n<li>\n<ul>\n<li>allowing s3:GetObject and s3:GetObject<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>If this isn\u2019t possible, does anyone have a suggestion on the easiest approach to allow read-only access to DVC data analogous to a public GitHub repo?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Manually prompt Gdrive authentication step",
        "Question_creation_time":1630114625044,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-prompt-gdrive-authentication-step\/858",
        "Question_upvote_count":2.0,
        "Question_view_count":181.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I\u2019m using DVC with my team and we\u2019re using google drive as our storage solution. By following the tutorial, the google account authentication step appears when I attempt to do my first dvc push, which is ok. Now my teammates new to link their accounts too in order to access the gdrive folder (shared between all of us) but we don\u2019t know how to prompt the authenctication step wihout having them pushing something, all they will do in this repo is pull data.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Automated testing when merging to main",
        "Question_creation_time":1626960959115,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/automated-testing-when-merging-to-main\/826",
        "Question_upvote_count":0.0,
        "Question_view_count":220.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>If we assume our currently deployed model is on the <code>main<\/code> branch. When we train a better model on a new branch and we decide to merge into main, we want to run some CI\/CD tests that will confirm our branch is ok to merge. In particular we want to test that 1) all of the dvc tracked files have been pushed to the remote and 2) if you run <code>dvc status<\/code> it return something like \u2018All data and pipeline up to date.\u2019.  We have set up some python tests to confirm this but it doesn\u2019t seem the most robust, I wondered if there was a more formal approach using dvc\/cml functionality for doing this?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem with top-level plot definitions",
        "Question_creation_time":1663152137956,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-top-level-plot-definitions\/1342",
        "Question_upvote_count":2.0,
        "Question_view_count":42.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey!<\/p>\n<p>I am currently having problems with a top-level plot definition.<br>\nMy dvc.yaml file contains a stage producing<br>\n<code>output_data\/plot_metrics\/metrics_plot_dict.json<\/code> as output (also defined as outs in dvc.yaml)<\/p>\n<p>I am trying to define a plot like this in dvc.yaml<\/p>\n<pre><code class=\"lang-auto\">plots:\n  acceptance_rate_histogram:\n    template: plot_templates\/histogram_template.json\n    x: \n      output_data\/plot_metrics\/metrics_plot_dict.json: acceptance_rate\n    x_label: Acceptance_Rate\n    title: Histogram\n<\/code><\/pre>\n<p>But with <code>dvc plots show<\/code> I receive :<\/p>\n<pre><code class=\"lang-auto\">2022-09-14 12:37:13,109 WARNING: 'acceptance_rate_histogram' was not found in current workspace.\n2022-09-14 12:37:13,113 DEBUG: 'acceptance_rate_histogram' - file type error\nOnly JSON, YAML, CSV and TSV formats are supported.\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/tim\/miniconda3\/envs\/pymc_stable\/lib\/python3.10\/site-packages\/dvc\/utils\/__init__.py\", line 410, in wrapper\n    vals = func(*args, **kwargs)\n  File \"\/home\/tim\/miniconda3\/envs\/pymc_stable\/lib\/python3.10\/site-packages\/dvc\/repo\/plots\/__init__.py\", line 548, in parse\n    raise PlotMetricTypeError(path)\ndvc.repo.plots.PlotMetricTypeError: 'acceptance_rate_histogram' - file type error\nOnly JSON, YAML, CSV and TSV formats are supported.\n------------------------------------------------------------\nDVC failed to load some plots for following revisions: 'workspace'.\nfile:\/\/\/home\/tim\/Workspaces\/pystoms\/pipelines\/pipe_A\/dvc_plots\/index.html\n2022-09-14 12:37:13,120 DEBUG: Analytics is enabled.\n2022-09-14 12:37:13,190 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpaooxh7mo']'\n2022-09-14 12:37:13,191 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpaooxh7mo']'\n<\/code><\/pre>\n<p>It seems as if dvc tries to find a file named <code>acceptance_rate_histogram<\/code>, but this string is supposed to be an id. If I exchange the id for the file\u2019s path, it works.<\/p>\n<p>My dvc version: 2.25.0<br>\nMy python version: 3.10.6<br>\nMy OS: Ubuntu 22.04<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using DVC outside git",
        "Question_creation_time":1640005495644,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-outside-git\/1014",
        "Question_upvote_count":0.0,
        "Question_view_count":352.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello here,<br>\nI am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering  the data folder is not a git\/dvc working directory?,  such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dynamic parameter import",
        "Question_creation_time":1661757180755,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dynamic-parameter-import\/1312",
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I have the following scenario: I\u2019m working with a Computer Vision Deep Learning framework for which I basically can write a config file only and train. This config file is in python. I would like to input this config file as parameter dependency (under params in dvc.yaml) such that I have one place where I can change the parameters. If I don\u2019t I need to maintain two places because some things I have to set in the <code>config.py<\/code> file\u2026<\/p>\n<p>I know I can specify the <code>config.py <\/code>as a params file. However, I have different architectures with potentially different parameters I would like to change. I made a stage using templating that switches the <code>config.py<\/code> based on the architecture name. Is it possible to create a <code>deps.yaml<\/code> (or any file) that contains a list of the keys present in <code>config.py<\/code> that should be considered parameters and ignore all the other valid potential parameters in <code>config.py<\/code>?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access to files uploaded with dvc without dvc",
        "Question_creation_time":1654848969858,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/access-to-files-uploaded-with-dvc-without-dvc\/1207",
        "Question_upvote_count":0.0,
        "Question_view_count":93.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI was playing with DVC and realized it is really powerfull but not sure it can fit my use case.<br>\nSome context, I did some tests with dvc and managed to upload my files to S3, I can push and pull data perfectly.<br>\nI need to be able to access the files uploaded to S3 with other tool in which I cannot integrate dvc, this tool should be able to access the files on S3.  I was expecting to be able to access the files on S3 on any way, but I cannot find where they are located, to me looks like the files themselves are not uploaded but a binary instead (correct me if I am wrong). So, my question is: \u00bfis there any way in which I can access the real files uploaded to s3 without doing a dvc pull?<br>\nThanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC and Hydra integration",
        "Question_creation_time":1630598266361,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-hydra-integration\/868",
        "Question_upvote_count":3.0,
        "Question_view_count":891.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello!<\/p>\n<p>I want to integrate DVC pipelines and parameters\/metrics tracking with Hydra loggings and configs. What best practices could I check?<\/p>\n<p>Some questions:<\/p>\n<ul>\n<li>how to configure hydra outputs dir to normal save logs and artifacts from the different pipeline stages?<\/li>\n<li>how to connect dvc params.yaml with hydra conf?<\/li>\n<\/ul>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Freeze stage with parameter substitution",
        "Question_creation_time":1622098167167,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/freeze-stage-with-parameter-substitution\/761",
        "Question_upvote_count":0.0,
        "Question_view_count":322.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I am not sure if this is expected or not, but freezing a stage with parameter substitution gives:<br>\n<code>ERROR: failed to freeze 'Test' - cannot dump a parametrized stage: 'Test'<\/code><\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nvars:\n  - folder: .\n\nstages:\n  Test:\n    cmd: dir ${folder}\n    always_changed: true\n<\/code><\/pre>\n<p>Command to run: <code>dvc freeze Test<\/code>. Is this normal behaviour? If yes, why?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best Practice - Test pipeline with smaller dataset?",
        "Question_creation_time":1644331496554,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-test-pipeline-with-smaller-dataset\/1047",
        "Question_upvote_count":0.0,
        "Question_view_count":146.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I currently have a DVC pipeline that takes about 12 hours to run. I would like a way to test the entire pipeline on a small subset of my data, so that I can quickly verify each stage after a code change. Ideally, this would be an option on <code>dvc repro<\/code>. This leads me to think that I can configure my pipeline to have separate \u201ctest\u201d versions of each stage, using the same cmds as their full counterparts, but using different parameters to reduce the time to reproduce.<\/p>\n<p>Is this the correct approach? Is there a cleaner way to do this? How have other people solved this problem?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"dvc.api.get_url()\" is not working for --external outputs",
        "Question_creation_time":1618813629880,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-api-get-url-is-not-working-for-external-outputs\/730",
        "Question_upvote_count":2.0,
        "Question_view_count":538.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have added dvc remote external output to track (external cache and external data storage),<\/p>\n<ul>\n<li>\n<p>Following is the .dvc generated,<br>\n<strong>path in github:<\/strong> remoteTrack\/wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>etag: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: remote:\/\/s3remote\/wine-quality.csv<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>But same works if not mentioned as external data, following is .csv for non external<br>\n<strong>path in github:<\/strong> wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>md5: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: wine-quality.csv<br>\n.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" data-download-href=\"\/uploads\/short-url\/nRHnFdVZK97Uul8v5E9CBTwtFPv.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" alt=\"image\" data-base62-sha1=\"nRHnFdVZK97Uul8v5E9CBTwtFPv\" width=\"685\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">900\u00d7656 37.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nEven after mentioning path as \u201cremote:\/\/s3remote\/wine-quality.csv\u201d, it is not working.<br>\n<strong>Error<\/strong>: PathMissingError: The path \u2018remoteTrack\/wine-quality.csv\u2019 does not exist in the target repository  neither as a DVC output nor as a Git-tracked file.<\/p>\n<p>What should be the path for remote external data?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best Practice for CI with Run Cache?",
        "Question_creation_time":1594029291581,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-ci-with-run-cache\/427",
        "Question_upvote_count":3.0,
        "Question_view_count":651.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m having trouble understanding how to use the new <a href=\"https:\/\/dvc.org\/blog\/dvc-1-0-release#run-cache\" rel=\"nofollow noopener\">run cache<\/a> in a continuous integration context.<\/p>\n<p>From what I gathered it\u2019s supposed to work as follows:<\/p>\n<ul>\n<li>\n<code>git commit<\/code> &amp; <code>git push<\/code> some changes to my code<\/li>\n<li>The CI-Pipeline is started, it executes a <code>dvc repro<\/code> and trains the new model<\/li>\n<li>After model training, the CI-Pipeline executes a <code>dvc push --run-cache<\/code>\n<ul>\n<li>The newly trained model, and all other outputs tracked by dvc are pushed to the dvc remote<\/li>\n<\/ul>\n<\/li>\n<li>On my local machine, I execute <code>dvc pull --run-cache<\/code> (without any changes to my local <code>dvc.lock<\/code> file)\n<ul>\n<li>I\u2019m expecting the newly trained model and all other dvc outputs to appear in my working directory?<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>But instead of the files from the run cache appearing in my working dir, I still get the outputs corresponding to my (old, local) <code>dvc.lock<\/code> file.<\/p>\n<p>Am I missing something here, or am I using the commands wrong?<br>\nIn case my expectations are correct I can try to provide a minimal example or more debug information for my current setup.<\/p>\n<p>When I manually copy the (new) <code>dvc.lock<\/code> from the runner and paste it into my local machine, I\u2019m able to <code>dvc pull<\/code> from the dvc remote and get the expected outputs.<br>\nBut from what I understood, it shouldn\u2019t be necessary anmyore to <code>git add dvc.lock &amp;&amp; git commit<\/code> from inside the runner for every experiment?<\/p>\n<p>Thanks in advance,<br>\nRabefabi<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Read only access using Azure Blog Storage",
        "Question_creation_time":1633526016502,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/read-only-access-using-azure-blog-storage\/910",
        "Question_upvote_count":1.0,
        "Question_view_count":321.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a public repo on GitHub using dvc. I want maintainers to add files using dvc. I will give them a SAS token so they can use the dvc client with their own token. as the repo and contents are public I would like to create a generic read-only token so anyone can download files with DVD pull. I thought I could do that by setting up the right permissions for the SAS token:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9.png\" data-download-href=\"\/uploads\/short-url\/f75fyugJBe0friORTHNGfOB2LTH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_567x499.png\" alt=\"image\" data-base62-sha1=\"f75fyugJBe0friORTHNGfOB2LTH\" width=\"567\" height=\"499\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_567x499.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_850x748.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1055\u00d7930 77.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>but it seems you can also write with that token. In fact, I was able to add a file with DVD push with the above configuration.<\/p>\n<p>Related: <a href=\"https:\/\/techcommunity.microsoft.com\/t5\/azure\/read-only-access-to-azure-storage-account-blob-containers-via\/m-p\/359229\" rel=\"noopener nofollow ugc\">https:\/\/techcommunity.microsoft.com\/t5\/azure\/read-only-access-to-azure-storage-account-blob-containers-via\/m-p\/359229<\/a><\/p>\n<p>Does anyone know how to do it properly? IS there any documentation about it?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc add external with gdrive",
        "Question_creation_time":1599067760059,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-external-with-gdrive\/480",
        "Question_upvote_count":3.0,
        "Question_view_count":308.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m just starting out with dvc. I have a bunch of file and models that are saved on my drive. I\u2019d like to add them to dvc without having to download them and then re-adding them. I thought maybe dvc add --external gdrive:\/\/\/ but it says it can\u2019t find it. Is this a problem with my gdrive link setup (I am using a service account) or with --external not supporting gdrive?<br>\nThanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Shared cache on NFS is slow",
        "Question_creation_time":1631759389078,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-on-nfs-is-slow\/892",
        "Question_upvote_count":0.0,
        "Question_view_count":413.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have multiple users who each can launch their own AWS instance for data analysis, then shut it down when they are done. Rather than having to rebuild the dvc cache on each of these instances when they startup, we want to have a persistent shared cache.<\/p>\n<p>So I\u2019ve created a shared DVC cache on EFS (AWS\u2019s version of NFS) with symlinks in the workspace, but the problem is that \u2018dvc fetch\u2019 and \u2018dvc checkout\u2019 are now quite slow for certain branches that have 1000s of files tracked by DVC. First, for a \u2018dvc fetch\u2019, even if no downloads need to be done from the remote, just \u201cQuerying cache\u2026\u201d can take up to 8 minutes. Then \u2018dvc checkout\u2019 can take 6 more minutes. Presumably this has to do with NFS communication overhead to list files in the cache (I see the progress bar saying things like \u201c110 files\/sec\u201d).<\/p>\n<p>Is there any way to speed up a shared cache stored on NFS, especially when you have 1000s of tracked files?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden",
        "Question_creation_time":1627166936199,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/forbidden-an-error-occurred-403-when-calling-the-headobject-operation-forbidden\/828",
        "Question_upvote_count":0.0,
        "Question_view_count":6667.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>this might sounds similar to what others have asked by I couldn\u2019t really find a solution that fix my problem.<\/p>\n<p>my <em>~\/.aws\/credentials<\/em> looks like<\/p>\n<pre><code class=\"lang-auto\">[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<p><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test<\/code><br>\nand use<\/p>\n<p>I have made my<code> .dvc\/config.local<\/code> to look like<\/p>\n<p>[\u2018remote \u201cmyremote\u201d\u2019]<br>\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test<br>\naccess_key_id = XYZ<br>\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS<br>\nprofile=\u2018testing\u2019<br>\ncredentialpath = \/Users\/nyt21\/.aws\/credentials<\/p>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<p><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/code><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"API call to read DVC file fails",
        "Question_creation_time":1582067621620,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/api-call-to-read-dvc-file-fails\/327",
        "Question_upvote_count":1.0,
        "Question_view_count":876.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>How do programmatically get a file URI so I can read it? I\u2019ve created a simple git repo with following steps from  Katacode tutorial:<\/p>\n<blockquote>\n<p>dvc init<br>\ndvc add file.txt<br>\ngit add file.txt.dvc<br>\ngit add .gitignore<\/p>\n<\/blockquote>\n<p>Call to api.get_url  returns a non-existent URI with <em><a href=\"https:\/\/github.com\/amesar\/dbws2\" rel=\"nofollow noopener\">https:\/\/github.com\/amesar\/dbws2<\/a><\/em> and <em>file.txt<\/em> as arguments.<br>\nBut call with <em><a href=\"https:\/\/github.com\/iterative\/example-get-started\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/example-get-started<\/a><\/em> and <em>model.pkl<\/em> works.<\/p>\n<p>What am I missing? Would be super useful to add doc-strings to api.py. For example, what is the \u201crev\u201d argument? Git commit hash?<\/p>\n<pre><code>repo = sys.argv[1]\npath = sys.argv[2]\nuri = api.get_url(path, repo=repo)\nrsp = requests.get(uri)\nwith open(f\"out\/{path}\", 'wb') as f:\n    f.write(rsp.content)<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Declare python version and packages versions as dependencies",
        "Question_creation_time":1599834464519,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/declare-python-version-and-packages-versions-as-dependencies\/492",
        "Question_upvote_count":2.0,
        "Question_view_count":266.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello!<\/p>\n<p>I recently started using DVC and I found the tutorials very useful and clear. However I still have a question about a use case of <code>dvc run<\/code>.<\/p>\n<p>I know that one can declare parameters and files to be dependencies of a pipeline. However, in most cases also<\/p>\n<ul>\n<li>the python version<\/li>\n<li>the python packages versions<br>\nare fundamental dependencies.<\/li>\n<\/ul>\n<p>To give an example, running the same pipeline (= same files, same parameters) with python 2.7 or python 3.x can give <a href=\"https:\/\/sebastianraschka.com\/Articles\/2014_python_2_3_key_diff.html#integer-division\" rel=\"nofollow noopener\">different results<\/a>. In the same way, if we have loaded <code>tensorflow 1.0<\/code> or <code>2.0<\/code>, the results may vary.<\/p>\n<p>So, I would like the pipeline to be re-run even if the python version or packages versions have changed and anything else has remained the same.<\/p>\n<p>How can I keep track of this kind of special dependencies for my pipeline?<\/p>\n<p>Thank you in advance!<\/p>\n<p>Best,<\/p>\n<p>\u2014Francesco<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How cache is maintained for big data size locally",
        "Question_creation_time":1568253347446,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-cache-is-maintained-for-big-data-size-locally\/217",
        "Question_upvote_count":3.0,
        "Question_view_count":815.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI have a VM on which my local git repo exists. On top of that i have installed dvc on same machine. Now when i add data to dvc it will be in dvc cache and on git push, commit same data will go to git repo as well. Is my understanding correct? If yes then their will be two copies of data and size will keep increasing as data grows. I am not using any remote repo for data.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to only pull\/get a subfolder from a existing repo",
        "Question_creation_time":1645141965445,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-only-pull-get-a-subfolder-from-a-existing-repo\/1071",
        "Question_upvote_count":0.0,
        "Question_view_count":326.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am very new to dvc and I am in the process of setting up all existing datasets that we have using dvc, so we can all share the data efficiently in a meaningful way.<\/p>\n<p>The dvc has been setup successfully using network drives as data storage. I have created a repo, which contains several smaller datasets. Each dataset is placed in a subfolder within a directory, which has been pushed to the save location on the network drive using dvc push as one repo.<\/p>\n<p>My question is, is it possible to only pull one dataset (one subfolder) within this repo? Later we will be applying different transformation\/data augmentation techniques on these datasets to create new datasets. If we can\u2019t pull subdirectories within the same repo, does this mean we need to create new repos instead?<\/p>\n<p>Thanks,<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline template",
        "Question_creation_time":1632646722796,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipeline-template\/907",
        "Question_upvote_count":1.0,
        "Question_view_count":209.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>hey!<\/p>\n<p>I want to create a template of a pipeline  -<br>\ni.e. i want to run the same pipeline with different parameters each time.<br>\ni read the templating section documentation and looks like the only solution is<br>\n<code>foreach<\/code> in each stage,<\/p>\n<p>is it possible to run multiple stages with one foreach block?<br>\nwhat if I want to run one specific set of params?<\/p>\n<p>any suggestions?<\/p>\n<p>thanks!!!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC + Github Actions + GCP Storage",
        "Question_creation_time":1628712122880,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-github-actions-gcp-storage\/840",
        "Question_upvote_count":1.0,
        "Question_view_count":501.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m trying triggering a pipeline to run DVC and download the data from GCP Storage but the log of GitHub Actions returns the following error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\n<\/code><\/pre>\n<p>I think this happens due to giving the right permissions to the Service Account but the one that I\u2019m using has the <em><strong>Storage Object Viewer<\/strong><\/em>, which gives the permission I need.<\/p>\n<p>Here is part of my pipeline file<\/p>\n<pre><code class=\"lang-auto\">- name: Setup Cloud SDK\n  uses: google-github-actions\/setup-gcloud@v0.2.0\n  with:\n    project_id: ${{ secrets.GCP_PROJECT }}\n    service_account_key: ${{ secrets.GCP_KEY }}\n    export_default_credentials: true\n\n- name: CML Run\n  shell: bash\n  env:\n    repo_token: ${{ secrets.GITHUB_TOKEN }}\n    GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_KEY }}\n  run: |\n    # run-cache and reproduce pipeline\n    dvc remote add -d -f myremote gs:\/\/myproject\/\n    dvc pull mypath\/data.csv.zip.dvc\n    dvc repro -m\n    \n    # Report metrics\n    echo \"## Metrics\" &gt;&gt; report.md\n    git fetch --prune\n    dvc metrics diff main --show-md &gt;&gt; report.md\n    \n    # Publish confusion matrix diff\n    echo -e \"## Plots\\n### Confusion Matrix\" &gt;&gt; report.md\n    cml-publish $PWD\/mypath\/reports\/confusion-matrix.png --md &gt;&gt; report.md\n    cml-send-comment report.md\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Processing data in place",
        "Question_creation_time":1587642959337,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/processing-data-in-place\/361",
        "Question_upvote_count":5.0,
        "Question_view_count":817.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi I\u2019m new to dvc (nice project!)<br>\nWe have a larger dataset and have several preprocessing scripts.<br>\nThese scripts alter data in place.<br>\nIt seems when I try to register it with <code>dvc run<\/code> it complains about cyclic dependencies (input is the same as output).<br>\nI would assume this is a very common use case.<br>\nWhat is the best practice here ?<br>\nTried to google around but i did not see any solution to this (besides creating another folder for the output).<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tracking Data Provenance with DVC",
        "Question_creation_time":1652228110629,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-provenance-with-dvc\/1186",
        "Question_upvote_count":1.0,
        "Question_view_count":130.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I recently discovered DVC and am looking to replace my current shell script-based approach for downloading source datasets and building derived datasets with DVC. In my current process, I have a clear record of data provenance as my scripted pipelines begin with downloads of the source datasets from the web.<\/p>\n<p>The question I have is the following: does DVC provide functionality that allows me to capture data provenance somehow? Can I record the URL from which the data was originally sourced and bind that to metadata associated with the data file(s)? Or will I need to maintain scripts that allow me to easily reacquire the data from the web? In the documentation, it seems like the story begins with the source data already in hand.<\/p>\n<p>I would love to have functionality that allows me to easily reacquire the source data from the web if needed and verify that indeed the dataset is equivalent to the original form that was used in the original pipeline development (via hash comparison).<\/p>\n<p>As of yet, I\u2019m not quite seeing how I would accomplish this with existing DVC commands. Any pointers would be greatly appreciated!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"'dvc push' multiple small files to aws s3 causes timeout error",
        "Question_creation_time":1630591814110,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-multiple-small-files-to-aws-s3-causes-timeout-error\/865",
        "Question_upvote_count":0.0,
        "Question_view_count":573.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI have problem with pushing large amount of small files to s3 via \u2018dvc push\u2019 command (~2000 files few hundred kb each) from a local machine (Ubuntu).<\/p>\n<p>Stable wired internet connection (~50Mbps measured speed), latest DVC (2.6.4).<\/p>\n<p>While \u2018querying cache\u2026\u2019 phase progress slows down at 85% and eventually gives an \u2018unexpected error\u2019 (FSTimeoutError).<\/p>\n<p>Tried to increase max number of opened files (system-wide, by changing \/etc\/sysctl.conf) - no effect.<\/p>\n<p>Thought if it\u2019s worth to change timeout value (if any) but after checking dvc-remote page couldn\u2019t find any info about it.<\/p>\n<p>Could someone advice how to solve this issue?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error with DVC run in windows docker container",
        "Question_creation_time":1602854305057,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-with-dvc-run-in-windows-docker-container\/530",
        "Question_upvote_count":2.0,
        "Question_view_count":413.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>I am currently getting an error when running:<\/p>\n<p><code>dvc run -n prepare -d .\/data\/raw -d .\/scripts\/prepare.py -o .\/data\/seed-detection\/interim -o .\/data\/seed-detection\/processed --no-commit python3 scripts\/prep are.py --config config\/config.yaml<\/code><\/p>\n<p>The error I get is:<\/p>\n<p><code>ERROR: unexpected error - disk I\/O error<\/code><\/p>\n<p>I\u2019m running this inside a container on windows where my cwd is a mounted volume.<\/p>\n<p>The strange thing is, when I only run the command without using DVC it runs fine, so the problem lies with DVC. Running with <code>--no-exec<\/code> option also fails.<\/p>\n<p>Currently only workaround I can think of is manually making <code>dvc.yaml<\/code>, then execute the command by itself and then commit DVC. However, I would not want to do this every time I want to run the stage.<\/p>\n<p>I\u2019m using Python 3.6.9 on Linux-4.19.76-linuxkit-x86_64-with-Ubuntu-18.04-bionic<\/p>\n<p>Thanks in advance!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Working with a small subset of remote data",
        "Question_creation_time":1604100899517,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/working-with-a-small-subset-of-remote-data\/541",
        "Question_upvote_count":2.0,
        "Question_view_count":857.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m trying to use DVC for managing data produced from simulations.  Each simulation produces a large number of small files, and a few very large files.  I have worked out how to set up a git repository with DVC and add the data (one .dvc file per simulation), and also how to push it to remote storage.  There are too many files to have one .dvc file per file. I think this is known as the \u201cdata registry\u201d use case.<\/p>\n<p>I then want to start a new project and work on, say, one of the simulations.<br>\nI think what I want to do is to \u201cdvc import\u201d the simulation data directory into the new project.  This works, and I can import just a single simulation.  However, I would like to be able to get just a small subset of the files within a simulation, without downloading the whole simulation.<\/p>\n<p>I think what I want to be able to do is to \u201cdvc import\u201d but tell it <em>not<\/em> to actually fetch the data, and then be able to tell it which files under the simulation I want to fetch.<\/p>\n<p>Is such a thing possible?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"facing issues in gitlab-runner for ci-cml and face issue to use AWS-s3 bucket for dvc in gitlab ",
        "Question_creation_time":1662585071904,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/facing-issues-in-gitlab-runner-for-ci-cml-and-face-issue-to-use-aws-s3-bucket-for-dvc-in-gitlab\/1332",
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to set up remote storage with SSH and jump host",
        "Question_creation_time":1634565937633,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-set-up-remote-storage-with-ssh-and-jump-host\/927",
        "Question_upvote_count":0.0,
        "Question_view_count":253.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to set up an ssh storage on a server that can only be accessed via a jump host. While with ssh  I can use the option -j and with sftp I can use the option -o \u2018ProxyJump user@IPadress -p port_number\u2019, I do not know how to do that for dvc remote storage.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem with DVC and Azure + tenant_id, client_id and client_secret",
        "Question_creation_time":1666890271387,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-dvc-and-azure-tenant-id-client-id-and-client-secret\/1376",
        "Question_upvote_count":2.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am on the DVC version: 2.31.0 (pip)<br>\nMy command are :<br>\npip install \u2018dvc[azure]\u2019<br>\ndvc init --no-scm<br>\ndvc remote add -d myremote azure:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<br>\ndvc remote modify --local myremote tenant_id \u201c12345678-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_id \u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_secret \u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<\/p>\n<p>dvc add an_image.jpg<br>\ndvc push<\/p>\n<p>All the tenant_id, client_id and client_secret are the same than used with azcopy :<br>\nI use azcopy with this variables without probleme :<br>\nexport AZCOPY_SPA_APPLICATION_ID=\u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\nexport AZCOPY_SPA_CLIENT_SECRET=\u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<br>\nexport AZCOPY_TENANT_ID=\u201c12345678-1234-1234-1234-123456789012\u201d<br>\nazcopy login --service-principal --application-id $AZCOPY_SPA_APPLICATION_ID --tenant-id $AZCOPY_TENANT_ID<br>\nazcopy copy \u2018.\/01_RAW_From_Data_Lake\/*\u2019 \u2018<a href=\"https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/\" class=\"inline-onebox-loading\" rel=\"noopener nofollow ugc\">https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<\/a>\u2019 --recursive<\/p>\n<p>Why the dvc push return this error message :<br>\nERROR: configuration error - Authentication to Azure Blob Storage requires either account_name or connection_string.<br>\nERROR: Authentication to Azure Blob Storage requires either account_name or connection_string.<br>\nLearn more about configuration settings at <a href=\"https:\/\/man.dvc.org\/remote\/modify\" rel=\"noopener nofollow ugc\">https:\/\/man.dvc.org\/remote\/modify<\/a>.<br>\n\/<br>\nThe tenant_id, client_id and client_secret are enough for login with azcopy. where is the mistake with DVC?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR: failed to reproduce \u2018dvc.yaml\u2019: [Errno 13] Permission denied",
        "Question_creation_time":1627673145405,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-failed-to-reproduce-dvc-yaml-errno-13-permission-denied\/835",
        "Question_upvote_count":1.0,
        "Question_view_count":1031.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear community,<\/p>\n<p>please help me what I can do with the following\u2026<br>\nI am running <code>dvc repro<\/code> and get the following error:<\/p>\n<p><strong>ERROR: failed to reproduce \u2018dvc.yaml\u2019: [Errno 13] Permission denied: '\/mnt\/c\/Users\/michael\/projects\/deeplearing\/.dvc\/cache\/57\/01bf0661ad5b16b5165d9c6042a0cd597\u2019<\/strong><\/p>\n<p>notes:<\/p>\n<ul>\n<li>I was running the <code>dvc repro<\/code> in a console window of VSCode within a WSL environment<\/li>\n<li>I am using a conda environment with dvc installed<\/li>\n<li>I already tried deleting the .cache directory<\/li>\n<li>it looks like this is not a DVC problem, but an OS problem\u2026 still hoping for suggestions because the first stage is running successfully (almost identical to the second stage except that the amount of processed files is smaller)<\/li>\n<\/ul>\n<p>Maybe somebody has a clue what I can try?<\/p>\n<p>My <code>dvc.yaml<\/code> looks like this:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  create_first_dataset:\n    cmd: python create_first_dataset.py\n    deps:\n    - original_dataset\n    - create_first_dataset.py\n    outs:\n    - first_dataset\n  create_second_dataset:\n    cmd: python create_second_dataset.py\n    deps:\n    - original_dataset\n    - create_second_dataset.py\n    outs:\n    - second_dataset\n<\/code><\/pre>\n<p>dvc doctor output:<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.17\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        ssh (paramiko = 2.7.2)\nCache types: hardlink, symlink\nCache directory: 9p on C:\\\nCaches: local\nRemotes: ssh\nWorkspace directory: 9p on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>Thanks all!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Adding private S3 buckets",
        "Question_creation_time":1533670100321,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adding-private-s3-buckets\/62",
        "Question_upvote_count":0.0,
        "Question_view_count":919.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey there!<\/p>\n<p>I\u2019m trying to <code>dvc push<\/code> my data file to a private s3 bucket but am getting the error:<\/p>\n<p><code>Failed to push data to the cloud: The config profile (tdobbins) could not be found<\/code><\/p>\n<p>I pointed dvc to my .config file by doing:<\/p>\n<p><code>dvc remote modify myremote credentialpath aws\/.config<\/code><\/p>\n<p>and used:<\/p>\n<p><code>dvc remote modify myremote profile tdobbins<\/code><\/p>\n<p>Is there anything obvious that I\u2019m missing?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Create a stage where the output is a directory",
        "Question_creation_time":1651250034111,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/create-a-stage-where-the-output-is-a-directory\/1182",
        "Question_upvote_count":0.0,
        "Question_view_count":115.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a process that has a dependency on <code>input.txt<\/code>, and its output is a directory with files of the form <code>outputs\/*.txt<\/code>.<\/p>\n<p>What is the correct way to specify such a stage? I see from <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\/tutorial#second-model-version\" rel=\"noopener nofollow ugc\">this tutorial<\/a> that you can <code>dvc add<\/code> an entire directory. But is it possible to set this directory as the output from a <code>dvc run<\/code> stage?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Failed transfer to remote S3 storage",
        "Question_creation_time":1650448395548,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/failed-transfer-to-remote-s3-storage\/1166",
        "Question_upvote_count":0.0,
        "Question_view_count":135.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey, I am an intern and trying to learn DVC for S3 remote storage. I got this error . Can you help me?<br>\nThank you,<\/p>\n<p>dvc push<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 70257f9e4b0b3ec72deeed5e1b880c3b.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 8a31b1338b609cf1133d1df639de606c.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 0ebacd64e04c0e4a97d8fa48a4d707e6.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: b6c29e96813679dd8c451c9b5ca86e2c.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: e42412b82dcab425ce9c7e2d0abdfb78.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 84453a3661e17405f48087e6cf409c51.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 22e3f61e52c0ba45334d973244efc155.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: tutorials\/versioning\/new-labels.zip, md5: 2eaa473159443e75e6fb7b29e56c0787<br>\nname: tutorials\/nlp\/pipeline.zip, md5: 1d2070ee188fc5e4d94ad920e6cc82aa<br>\nname: images\/owl_sticker.png, md5: fa8b9e82c893eb401f30c353bd550ada<br>\nname: tutorials\/versioning\/data.zip, md5: fa9c0eb4173d86695b4e800219651360<br>\nname: mnist\/images.tar.gz, md5: acb39268fb9dd785aac887f69b89282e<br>\nname: tutorials\/nlp\/Posts.xml.zip, md5: ce68b98d82545628782c66192c96f2d2<br>\nname: fashion-mnist\/images.tar.gz, md5: 3c8f026df7c3973bcec8e9f945026eb2<br>\nname: images\/dvc-logo-outlines.png, md5: 5846bf188572bbc78a10124f36e92631<br>\nname: images\/owl_sticker.svg, md5: c76a031091a240c014baea8eff8619c4<br>\nERROR: failed to transfer \u2018md5: a304afb96060aad90176268345e10355\u2019 - [Errno 13] Permission denied: \u2018\/path\u2019<br>\nERROR: failed to push data to the cloud - 1 files failed to upload<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Re-run changed final stage on previous model versions",
        "Question_creation_time":1589717438826,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/re-run-changed-final-stage-on-previous-model-versions\/389",
        "Question_upvote_count":1.0,
        "Question_view_count":341.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Apologies if this has been asked before. Lets say I have a pipeline which finally outputs some metrics. I have used <code>git tag<\/code> to checkpoint several model versions. Then I decide I am not using the correct metric so change the final stage which generates the metrics. I would then like to run this updated final stage for previous model versions, without having to retrain those models. What would be the best way achieve this?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"`dvc pull --run-cache [target]`",
        "Question_creation_time":1594851186816,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-pull-run-cache-target\/444",
        "Question_upvote_count":4.0,
        "Question_view_count":1219.0,
        "Question_answer_count":16,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<h1>Context<\/h1>\n<p>Here is my use case:<\/p>\n<ol>\n<li>I train a model automatically through whatever way (CML, Airflow)<\/li>\n<li>Now I have a <code>weights.db<\/code> file, which is versioned by DVC alongside other intermediary data files<\/li>\n<li>I do a <code>dvc push --run-cache<\/code> as I do not want to version with <code>dvc.lock<\/code> since I am not committing anything<br>\n\u2013&gt; Now there are several files in my S3.<br>\n\u2013&gt; Time to run my model in prod<\/li>\n<li>In a Docker container, run <code>dvc pull --run-cache generate_weights<\/code> (the stage that created <code>weights.db<\/code>)<\/li>\n<li>DVC starts to pull <strong>all files<\/strong> from remote, not only previous <code>weights.db<\/code> versions, but also caches from other stages.<\/li>\n<\/ol>\n<p>This makes my approach unfeasible as pulling all files from remote takes a long time.<\/p>\n<h1>Question(s)<\/h1>\n<ol>\n<li>Should <code>dvc pull --run-cache [target]<\/code> pull <em>all files<\/em>?<\/li>\n<li>Should I be doing it the way I am doing? I am avoiding CML\/Git-related solutions.<\/li>\n<\/ol>\n<p>Thank you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to move DVC files",
        "Question_creation_time":1566355790691,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-move-dvc-files\/205",
        "Question_upvote_count":4.0,
        "Question_view_count":1149.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Search for this topic, I found <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-heartbeat-discord-gems\/159\/2\">this summary post<\/a> but I cannot open the original question (discord tells me it cannot find it) so I\u2019m creating a new topic. Apologies in advance.<\/p>\n<p>I tried following the instructions given i.e. I ran<\/p>\n<pre><code class=\"lang-auto\">mv my_file.dvc dvc_links\/my_file.dvc\n<\/code><\/pre>\n<p>and edited the working directory in the moved file.<br>\nHowever running <code>dvc status<\/code> gives<\/p>\n<pre><code class=\"lang-auto\">dvc_links\/my_file.dvc:\n        changed checksum\n<\/code><\/pre>\n<p>How do I get dvc to update the checksum in the relocated file? I even tried removing the md5: xx line at the top of the new file, calculating the md5 sum and reinserting it but this didn\u2019t work.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to add data from \"scratch\" filesystem directly to \"scratch\" cache dir",
        "Question_creation_time":1615839224546,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-add-data-from-scratch-filesystem-directly-to-scratch-cache-dir\/702",
        "Question_upvote_count":4.0,
        "Question_view_count":415.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>My scenario is that I\u2019m on a cluster with really limited home directory quota (40GB) but with TB of \u201cscratch\u201d space on a separate filesystem. I\u2019d like to keep my Git repo on the home filesystem (say at <code>~\/myrepo\/<\/code>), but the DVC cache on the scratch file system (say at <code>\/scratch\/dvc<\/code>).<\/p>\n<p>Its easy enough to configure the cache on the scratch filesystem with e.g. <code>dvc cache dir \/scratch\/dvc<\/code> and then use the <code>symlink<\/code> cache type so files just symlink to here.<\/p>\n<p>But now, suppose I\u2019ve generated a large file at <code>\/scratch\/newfile.dat<\/code>. How can I add it to the repo at a chosen path without it ever touching the home filesystem?<\/p>\n<p>I would have thought:<\/p>\n<pre><code class=\"lang-bash\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<p>would do it but that still seems to go through the home filesystem.<\/p>\n<p>It seems if I add <code>--to-remote<\/code> it works as expected, but it <em>also<\/em> (obviously) pushes to remote, which I don\u2019t want.<\/p>\n<p>Thanks for any suggestions.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"1T dataset with distributed deep learning",
        "Question_creation_time":1666885670842,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/1t-dataset-with-distributed-deep-learning\/1374",
        "Question_upvote_count":1.0,
        "Question_view_count":51.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m currently evaluating DVC for a very large scale Deep Learning project.<\/p>\n<p>We have an ever growing dataset of +1M video files totaling to more than a Tb.<br>\nFrom this we do:<\/p>\n<ol>\n<li>preprocessing of each video file<\/li>\n<li>sub sample a subset of the original dataset, which will become the new training\/val\/test dataset. This dataset is currently at +400k video files totaling to 800G<\/li>\n<li>Train a distributed deep learning video classification model on aws with pytorch distributed<\/li>\n<\/ol>\n<p>I have already used DVC extensively but only in a more local-first approach where I could run the whole pipeline on my machine and would just run the same pipeline on more powerful servers.<\/p>\n<p>In my new use-case, I would like to scale the whole process on aws where all the data is on S3 buckets and steps of the pipeline is parallelized on light aws lambdas as much as possible.<br>\nI would also like to be able to launch the whole pipeline from my local machine (which would run all the processing on aws) but also on a CI job.<\/p>\n<p>Has anyone ever had such a challenging setup ?<br>\nI feel like it\u2019s the distributed parallel execution features that would be the most challenging to put in place with DVC.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Delete remote artifacts",
        "Question_creation_time":1659001950703,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/delete-remote-artifacts\/1272",
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>is there a DVC command to delete all remote artifacts referenced by a <code>.dvc<\/code> file without deleting the  <code>.dvc<\/code> file? My understanding is that the (complex) <code>dvc gc<\/code> command offers means to clean unused artifacts but I struggle to find a way to simply remove pushed artifacts from a remote.<br>\nThis question relates to <a href=\"https:\/\/discuss.dvc.org\/t\/zero-byte-files-created-in-artifactory-remote-http-based\/1271\">my other post<\/a> where I tried to delete corrupted zero-byte remote artifacts uploaded by a <code>dvc push<\/code> command.<\/p>\n<p>Thank you for your help!<\/p>\n<p>Best,<br>\nJoe<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Commit\/add only changed files",
        "Question_creation_time":1617883124373,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/commit-add-only-changed-files\/714",
        "Question_upvote_count":1.0,
        "Question_view_count":324.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, we try to add all locally checked out and modified files with one command to the cache to further push it to the remote.<\/p>\n<p>We are using a S3 remote from which we only want to check out certain files to reduce the size of our repository on local hard drives. When modifying some of the checked-out files we want to add them to dvc without specifying all their names as targets for dvc add\/commit.<\/p>\n<p>E.g.: We track the files foo.txt bar.txt baz.txt but only have the file bar.txt checkout. The other two will remain only .dvc files and not get resolved. When we then modify the content of bar.txt and try to run dvc commit it tries to also add the deletion of foo.txt and baz.txt, because they were not checked out in the first place. We know we could specify bat.txt as a target for dvc commit. However, this approach is not practical when modifying a large number of files.<\/p>\n<p>Is there a way to only add the really modified files to dvc? We where thinking about a list of file-names like dvc diff returns but only for modified files which we could use as input for dvc commit.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to make DVC wait before checking if output file exists?",
        "Question_creation_time":1650539400631,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-make-dvc-wait-before-checking-if-output-file-exists\/1170",
        "Question_upvote_count":0.0,
        "Question_view_count":101.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m running a command remotely through ssh, and it takes the remote cluster a while to finish the script. The problem is that DVC fails on this step because it immediately checks if the output file was created. Apart from just removing this file as an output, is there a way to have DVC wait and\/or ignore whether the file was created?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Peer reviews with DVC",
        "Question_creation_time":1659085404628,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/peer-reviews-with-dvc\/1274",
        "Question_upvote_count":0.0,
        "Question_view_count":73.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m in a team of Computer Vision Engineers, and I\u2019m working on improving ours datasets\u2019 management.<br>\nUntil now, we use git LFS to track our dataset. We reach its limits, as we get huge repository, long pull time. We also want  a more secure way to store ours datasets, that could be accessible only by authorize persons. It\u2019s important to precise that our datasets are living, often modified with content added or removed, and we currently do it with PR system.<\/p>\n<p>To improve our process, I found out that DVC may answer some of our issues, and also provides more features afterward (experiments, pipeline\u2026)<br>\nDocuments are sent to the remote, that will be accessible only by authorized people, so it solves the security part.<br>\nDVC is also better for huge files management, I won\u2019t say more about it, I think you already know this.<\/p>\n<p>Now I\u2019ve explained my case, I have some questions:<\/p>\n<p>Relating to PR, I\u2019m aware that it will no longer be possible to use our Git web interface to check PR before approving them, as we won\u2019t see file content change (We also have annotated JSON\/TXT files that will be tracked by DVC, it\u2019s not huge file, but they may contain sensitive data, so we prefer to add them in DVC remote for security purpose).<br>\nWhat are best practices about pull requests review with DVC ? DVC is made for git and for versioning, so I suppose it exists a simple way to check PR ? Or do we have to check locally, by pulling everything ?<\/p>\n<p>Another point, we want to be able to easily know what files have changed, so I\u2019m planning on using dvc add with --recursive flag, to ensure each file is tracked individually. I saw on documentation that it\u2019s better to avoid this flag when a folder contains numerous files. What are the limits ? How it will impact performances ?<\/p>\n<p>Thanks !<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Shared development details",
        "Question_creation_time":1607527227363,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-development-details\/580",
        "Question_upvote_count":0.0,
        "Question_view_count":477.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>we are trying to use DVC very close to the described <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" rel=\"noopener nofollow ugc\">shared-development-server<\/a> use-case. This is great for starting, but the devil is in the details.<br>\nThe setup is that we use a shared storage in an HPC environment, where we try to reduce data duplication.<\/p>\n<p>We use a shared cache and because we later might want to connect this to a remote storage, we create a local \u201cremote\u201d storage for the moment.<\/p>\n<p>In the setup we have 2 different types of data. Smaller data files, that are ok if they get downloaded to the individual development folders and a folder containing millions of large files that we don\u2019t want to duplicate.<\/p>\n<p>We have 2 issues now.<\/p>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<p>Secondly, we are not sure how to fare with the large folder with the millions of large files. We probably should create a link to the folder from each individual directory. The data in this folder is generated elsewhere. We want to make sure if we ever have to re-run the data generation, we are informed if this lead to different files. Hence, we want to hash the folder or it\u2019s content, but we are not sure if it\u2019s viable to do this for this amount of files individually. What would be best solution here?<\/p>\n<p>Thanks a lot and best regards<br>\nMarius<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Procedure for changing cache type",
        "Question_creation_time":1566354562099,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/procedure-for-changing-cache-type\/204",
        "Question_upvote_count":3.0,
        "Question_view_count":907.0,
        "Question_answer_count":11,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Because I\u2019m using large files which are tens of gigabytes, I want to use hardlinks as my filesystem doesn\u2019t support reflinks.<br>\nI executed the following commands<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type = hardlink\ndvc config cache.protected = true\ndvc checkout\n<\/code><\/pre>\n<p>However when I checked using <code>ls -i<\/code> the inodes of the file in .dvc\/cache and the file in my working directory, they are still different.<br>\nAre there any further steps I need to take to get hardlinks to be used?<\/p>\n<p>Also I noticed that both the file in the working directory and cache were still writable, when I thought that they should have been made read-only by using <code>cache.protected = true<\/code><br>\nIs my understanding here mistaken?<\/p>\n<p>Update:<br>\nI mistyped the commands above. I actually had executed them correctly, i.e.:<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type hardlink\ndvc config cache.protected true\ndvc checkout\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unexpected error when push to SSH remote",
        "Question_creation_time":1646732453833,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/unexpected-error-when-push-to-ssh-remote\/1105",
        "Question_upvote_count":1.0,
        "Question_view_count":433.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>System: Ubuntu 20.04<\/p>\n<pre><code class=\"lang-auto\">$ dvc remote add --default dataRepo ssh:\/\/leo@l0.162.2.83\/home\/leo\/Videos\n$ dvc push\nERROR: unexpected error - [Errno -2] Name or service not known\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n$ cat .dvc\/config\n[core]\n    remote = datarepo\n['remote \"datarepo\"']\n    url = ssh:\/\/leo@l0.162.2.83\/home\/leo\/Videos\n\n$ dvc push -v\n2022-03-08 17:56:36,153 DEBUG: Preparing to transfer data from '\/home\/leo\/docs\/dvcEx\/.dvc\/cache' to '\/home\/leo\/Videos\/'\n2022-03-08 17:56:36,154 DEBUG: Preparing to collect status from '\/home\/leo\/Videos\/'\n2022-03-08 17:56:36,154 DEBUG: Collecting status from '\/home\/leo\/Videos\/'\n2022-03-08 17:56:36,154 DEBUG: Querying 1 hashes via object_exists\n2022-03-08 17:56:36,262 ERROR: unexpected error - [Errno -2] Name or service not known\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/__init__.py\", line 78, in main\n    ret = cmd.do_run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/transfer.py\", line 154, in transfer\n    status = compare_status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 163, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 124, in status\n    exists = hashes.intersection(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 49, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists, jobs))                                                                                                                                                                      File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 381, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 372, in exists_with_progress\n    ret = self.fs.exists(fs_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 81, in exists\n    return self.fs.exists(path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/ssh.py\", line 124, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/spec.py\", line 68, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 77, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 85, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 65, in sync\n  raise return_result                                                                                                                                                                                                                           File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 25, in _runner                                                                                                                                                    result[0] = await coro                                                                                                                                                                                                                        File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 494, in wait_for                                                                                                                                                                                 return fut.result()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 92, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 568, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/misc.py\", line 223, in __aenter__\n    self._result = await self._coro\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 6892, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 455, in wait_for\n    return await fut\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 298, in _connect\n    _, conn = await loop.create_connection(conn_factory, host, port,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 986, in create_connection\n    infos = await self._ensure_resolved(\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 1365, in _ensure_resolved\n    return await loop.getaddrinfo(host, port, family=family, type=type,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 825, in getaddrinfo\n    return await self.run_in_executor(\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/usr\/lib\/python3.8\/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n------------------------------------------------------------\n2022-03-08 17:56:36,979 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] 'reflink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/__init__.py\", line 78, in main\n    ret = cmd.do_run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/transfer.py\", line 154, in transfer\n    status = compare_status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 163, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 124, in status\n    exists = hashes.intersection(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 49, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists, jobs))\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 381, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\nFile \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result                                                                                                                                                     [56\/1809]    raise self._exception\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 372, in exists_with_progress\n    ret = self.fs.exists(fs_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 81, in exists\n    return self.fs.exists(path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/ssh.py\", line 124, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/spec.py\", line 68, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 77, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 85, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 65, in sync\n    raise return_result\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 494, in wait_for\n    return fut.result()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 92, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 568, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/misc.py\", line 223, in __aenter__\n    self._result = await self._coro\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 6892, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 455, in wait_for\n    return await fut\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 298, in _connect\n    _, conn = await loop.create_connection(conn_factory, host, port,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 986, in create_connection\n    infos = await self._ensure_resolved(\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 1365, in _ensure_resolved\n    return await loop.getaddrinfo(host, port, family=family, type=type,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 825, in getaddrinfo\n    return await self.run_in_executor(\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/usr\/lib\/python3.8\/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/local.py\", line 145, in reflink\n    System.reflink(from_info, to_info)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/system.py\", line 112, in reflink\n    System._reflink_linux(source, link_name)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/system.py\", line 96, in _reflink_linux\n    fcntl.ioctl(d.fileno(), FICLONE, s.fileno())\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 69, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 32, in _link\n    raise OSError(\nOSError: [Errno 95] 'reflink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 124, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 77, in _try_links\n    raise OSError(\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/.3YzRahnxL5SVcRyPvvgfDR.tmp'\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/.3YzRahnxL5SVcRyPvvgfDR.tmp'\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/.3YzRahnxL5SVcRyPvvgfDR.tmp'\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/dvcEx\/.dvc\/cache\/.33LPManfkHLF5iPjktauyB.tmp'\n2022-03-08 17:56:36,986 DEBUG: Version info for developers:\nDVC version: 2.9.5 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.4.0-84-generic-x86_64-with-glibc2.29\nSupports:\n        azure (adlfs = 2022.2.0, knack = 0.9.0, azure-identity = 1.8.0),\n        gdrive (pydrive2 = 1.10.0),\n        gs (gcsfs = 2022.2.0),\n        hdfs (fsspec = 2022.2.0, pyarrow = 7.0.0),\n        webhdfs (fsspec = 2022.2.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        s3 (s3fs = 2022.2.0, boto3 = 1.20.24),\n        ssh (sshfs = 2021.11.2),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.4),\n        webdavs (webdav4 = 0.9.4)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/sda5\nCaches: local\nRemotes: ssh\nWorkspace directory: ext4 on \/dev\/sda5\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-03-08 17:56:36,988 DEBUG: Analytics is enabled.\n2022-03-08 17:56:37,078 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpb10lmwwg']'\n2022-03-08 17:56:37,079 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpb10lmwwg']'\n<\/code><\/pre>\n<p>I can <code>ssh<\/code> and <code>sftp<\/code> to <a href=\"mailto:leo@10.162.2.83\">leo@10.162.2.83<\/a> with SSH keys.<\/p>\n<p>How to fix this error?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No write permissions when using GDrive service account on another computer?",
        "Question_creation_time":1658144854976,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/no-write-permissions-when-using-gdrive-service-account-on-another-computer\/1252",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Using the Google drive service account with the same json file. On one computer I can execute <code>dvc push<\/code> successfully. On the other computer I get the following error:<\/p>\n<p>ERROR: failed to transfer \u2018md5: 848a6aa790bd34053eb457062307bd45\u2019 - &lt;HttpError 403 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files?supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files?supportsAllDrives=true&amp;alt=json<\/a> returned \u201cInsufficient permissions for the specified parent.\u201d. Details: \u201c[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018forbidden\u2019, \u2018message\u2019: \u2018Insufficient permissions for the specified parent.\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018parent.permissions\u2019}]\u201d&gt;<\/p>\n<p>When I use the same json file, why do I have different permissions on different computers?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc garbage collector permissions for remote SSH",
        "Question_creation_time":1615400634250,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-garbage-collector-permissions-for-remote-ssh\/700",
        "Question_upvote_count":1.0,
        "Question_view_count":295.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is there a way to prevent people to run dvc gc with the \u201c\u2013cloud\u201d option ?<\/p>\n<p>To give a bit more context, we would want to put dvc in place in our company with a SSH remote storage but we realized there was no user management in DVC for now. So every people would be able to push to the remote, but also to garbage collect. For data safety  purposes, this is not something we want. To the best of our knowledge, there is also no possibility to restrict directly on the SSH storage side beause there is no way to authorize write access but no deletion access.<\/p>\n<p>What we would want is to have only an \u201cadmin\u201d user that could run this garbage collection on the remote.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Comparison to DataLad and git-annex",
        "Question_creation_time":1536601415671,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/comparison-to-datalad-and-git-annex\/92",
        "Question_upvote_count":7.0,
        "Question_view_count":1715.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>What are the main differences between DVC and DataLad\/git-annex ( <a href=\"https:\/\/www.datalad.org\/\" rel=\"nofollow noopener\">https:\/\/www.datalad.org\/<\/a> , <a href=\"https:\/\/git-annex.branchable.com\/\" rel=\"nofollow noopener\">https:\/\/git-annex.branchable.com\/<\/a> )?  What would be reasons to use one or the other?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"S3 remote permissions and integrity best practices",
        "Question_creation_time":1553787278231,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/s3-remote-permissions-and-integrity-best-practices\/165",
        "Question_upvote_count":1.0,
        "Question_view_count":1562.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Does anyone have recommendations about best practices for S3 remote set-up?<\/p>\n<p>I\u2019m specifically interested if there are thoughts around permissions and integrity of data in S3. To enable S3, remotes users would be granted read &amp; write permissions. As long as users use the dvc tooling it appears the data should relatively save. However, granting permissions would allow direct access outside of dvc tooling. This seems to open the possibility of a scenario where the data history could be corrupted.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc and S3 permissions management",
        "Question_creation_time":1631267338282,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-s3-permissions-management\/887",
        "Question_upvote_count":2.0,
        "Question_view_count":397.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello dvc people!<\/p>\n<p>At my team we are experimenting with dvc as an interface to work with trained models and datasets. So far it has been working nicely, but we found a mayor roadblock when it comes to permissions management.<\/p>\n<p>In our ideal scenario, when a teammate gets access to one of our repositories in GitHub that person will only have access to the data from that particular repository. This permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in AWS.<\/p>\n<p>However, this seems challenging to achieve through S3 + DVC. Our considered solution right now is to have a single S3 bucket with all our DVC repositories. If we grant access to all teammates to the whole bucket it would mean that they will have access to all the company\u2019s data, which is less than ideal. So we were considering having a \u201cJunior\u201d role which we will have to give access manually to whatever specific S3 folders they need and a \u201cSenior\u201d role with access to everything.<\/p>\n<p>This solution is suboptimal as we will need to handle different permissions for GitHub and for S3, which adds overhead.<\/p>\n<p>Is there any other pattern we are missing here? Is there any easy way for a teammate to be given access to ONLY the dvc remote from the GitHub repository automatically?<\/p>\n<p>Thanks for the help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/relaxed.png?v=10\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\"><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cml \/ GitHub Actions \/ aws",
        "Question_creation_time":1625520013117,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-github-actions-aws\/816",
        "Question_upvote_count":1.0,
        "Question_view_count":547.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Guys,<\/p>\n<p>I\u2019m trying to run cml-runner in aws via GitHub actions.<br>\nThe script below opens an instance that we can see in the aws console.<br>\nThe result can be summarized as:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8.png\" data-download-href=\"\/uploads\/short-url\/dDWeV0VmqQQaNdVK2EcQMCnboO4.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_690x144.png\" alt=\"image\" data-base62-sha1=\"dDWeV0VmqQQaNdVK2EcQMCnboO4\" width=\"690\" height=\"144\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_690x144.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_1035x216.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_1380x288.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2798\u00d7584 33.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Since we don\u2019t have any information about what\u2019s going on, guessing isn\u2019t so easy. Again, a suggestion for resolving it is appreciated.<\/p>\n<p>Note : This is related to <a href=\"https:\/\/discuss.dvc.org\/t\/cml-dvc-github-actions-hyper-parameter-tuning\/812\">Question 812<\/a><\/p>\n<pre><code class=\"lang-auto\">\nname: Test\non: [push]\njobs:\n  deploy:\n    runs-on: [ubuntu-latest]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: iterative\/setup-cml@v1\n      - run: |\n          cml-runner \\\n          --cloud=aws \\\n          --cloud-region=eu-west-3 \\\n          --cloud-type=t2.micro \\\n          --labels=cml-runner\n        env:\n          REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  train:\n    needs: deploy\n    runs-on: [self-hosted, cml-runner]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: iterative\/setup-dvc@v1\n      - uses: iterative\/setup-cml@v1\n      - run: |\n\n          echo \"Hello!\"\n        \n          \n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to automate DVC push",
        "Question_creation_time":1612185766792,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-automate-dvc-push\/649",
        "Question_upvote_count":4.0,
        "Question_view_count":447.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello<\/p>\n<p>I have started using DVC for version control in one of my projects and it\u2019s awesome.<\/p>\n<p>Currently, the model training job is executed from an amazon ec2 machine and the output is downloaded to my local machine which is pushed to the remote storage using the DVC push command.<\/p>\n<p>My doubt is, is it possible to programmatically push the model into the remote DVC storage (Amazon S3)? I couldn\u2019t find any helping articles online. Could you please suggest the right way of doing the same?<\/p>\n<p>Thanks in advance<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Switching between virtual Python environments within `cmd`",
        "Question_creation_time":1639444412795,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/switching-between-virtual-python-environments-within-cmd\/1004",
        "Question_upvote_count":2.0,
        "Question_view_count":195.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>We are using Poetry to manage Python environments. It is generally possible to invoke a python script B to run in a given Poetry environment from within python program A. I have so far done this for instance via issuing shell commands from within a Python program. Like This:<\/p>\n<p>Given this structure with two differnt envs:<\/p>\n<p>folder_a<\/p>\n<ul>\n<li>prog_a.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>folder_b<\/p>\n<ul>\n<li>prog_b.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\"># within prog_a.py\nimport subprocess\ncmd = \"cd folder_b; poetry run python prog_b.py\"\nsubprocess.run(cmd, shell=True, check=True)                  \n<\/code><\/pre>\n<p>This worked without problem.<\/p>\n<p>Now, when I try the same from DVC it unfortunately does not work. Say I now have the following setup:<\/p>\n<p>folder_a<\/p>\n<ul>\n<li>dvc.yaml<\/li>\n<li>pyproject.toml   # has DVC installed<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>folder_b<\/p>\n<ul>\n<li>prog_b.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>If I specify a <code>cmd<\/code> in a <code>dvc.yaml<\/code> like this: <code>cmd : \"cd abs\/path\/to\/folder_b; poetry run python prog_b.py\"<\/code>, then upon <code>dvc repro ...<\/code> I expected prog_b to be run in the Poetry env from folder_b. Instead it is run in the env from folder_a (where DVC is installed).<\/p>\n<p>There seems to be no way to escape this. I have even tried instead calling a wrapper through <code>cmd<\/code> that does the exact same subprocess call as in the first example (meaning it gets the full rest of the command as string, including the cd\u2026). Still, no luck, the env stays the same.<\/p>\n<p>Does anyone have any clue why this is the case? How does DVC execute <code>cmd<\/code>? In some special way that prevents running a process in a new env? I have tried a lot of variants of DVC \/ Poetry configs, to no avail.<\/p>\n<p>Alternatively I appreciate suggestions how else to deal with the need to execute different stages in different environments.<\/p>\n<p>Thanks lots and lots. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Best<\/p>\n<p>Jonas<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Experiments with checkpoint",
        "Question_creation_time":1620824231813,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/experiments-with-checkpoint\/744",
        "Question_upvote_count":5.0,
        "Question_view_count":359.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>So currently, I have a pipeline with a stage \u201ctraindetector\u201d which produces a number of checkpoint files (one for each epoch).<\/p>\n<p>After running<\/p>\n<blockquote>\n<p>dvc exp run<\/p>\n<\/blockquote>\n<p>once, my experiment is done.<br>\nHowever, if I do:<\/p>\n<blockquote>\n<p>dvc status traindetector<br>\ntraindetector:<br>\nalways changed<\/p>\n<\/blockquote>\n<p>The \u201ctraindetector\u201d stage is marked as \u201calways changed\u201d, and if I redo:<\/p>\n<blockquote>\n<p>dvc exp run<\/p>\n<\/blockquote>\n<p>with no modification to the project, the \u201ctraindetector\u201d stage will be redone.<\/p>\n<p>Is this normal ?<br>\nCould somebody help me better understand experiments with checkpoints ?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc add: Location of .gitignore File",
        "Question_creation_time":1645453082147,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-location-of-gitignore-file\/1075",
        "Question_upvote_count":0.0,
        "Question_view_count":264.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>During <code>dvc add<\/code> the .gitingnore file is always created at the targets location (with <code>--file<\/code> the location of the .dvc file can be set but this does not seem to affect the location of the .gitignore file which is created or added to).<\/p>\n<p>This is not always desired, for example one could want a single .gitignore file in a projects root directory.<\/p>\n<ol>\n<li>\n<p>Did i miss something and there is a way to set the .gitignore file location?<\/p>\n<\/li>\n<li>\n<p>Wouldn\u2019t it be more convenient that the .gitignore location is always the same as the .dvc location (when using the <code>--file<\/code> argument)?<\/p>\n<\/li>\n<\/ol>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Advice for versioning many many small files?",
        "Question_creation_time":1609777134249,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609",
        "Question_upvote_count":3.0,
        "Question_view_count":2057.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! I\u2019m new to DVC <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nI was working on versioning a large dataset (totaling ~140gb) which consists of many many very small files. They are mp3 audio files (imagine for training a speech model), each for example 100-200kb (they\u2019re only 5-15 seconds each on average). This means I\u2019ve got a lot of files of course<\/p>\n<p>I ran <code>dvc push<\/code> and let it run for ~16 hours before realizing this probably isn\u2019t a great idea, since it seemed like it was nowhere near completion, and cancelled the run. I found a couple links online telling me dvc isn\u2019t ideal for many small files, but I also found a comment from a dvc maintainer on this forum saying recent changes (that post was ~apr 2020) should improve performance for dvc push on many small files \u2013 Would anyone be able to advise me? Did I possibly do something wrong, or should I be rolling these files into a tar, zip, etc and versioning that? Wouldn\u2019t it make more sense to version individual data files? (I\u2019m new to working with data too <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/upside_down_face.png?v=9\" title=\":upside_down_face:\" class=\"emoji\" alt=\":upside_down_face:\">)<\/p>\n<p>Help would be really appreciated! Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Timing to create a dvc repo for a 60GB dataset?",
        "Question_creation_time":1635739140181,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/timing-to-create-a-dvc-repo-for-a-60gb-dataset\/945",
        "Question_upvote_count":1.0,
        "Question_view_count":451.0,
        "Question_answer_count":15,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello<\/p>\n<p>I am a new user, working in Linux, and I am creating a DVC remote repo on NFS. I have a dataset split across four sub-directories, each one with 10000 images, total size of all four sub-directories is 62GB.<\/p>\n<p>If I do a straight copy (not dvc) of the dataset to the repo location, the copy takes 50 min.<\/p>\n<p>If I do \u2018dvc add\u2019 to the remote repo, the command is still running after 3.5 days. I assume that this is not expected? What would be expected time for dvc add to a remote repo, versus a straight copy to the same location?<\/p>\n<p>Thanks, Paul<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Shared cache directory",
        "Question_creation_time":1526900412793,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-directory\/31",
        "Question_upvote_count":5.0,
        "Question_view_count":2112.0,
        "Question_answer_count":14,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi<\/p>\n<p>I was wondering how to setup a shared cache directory, if possible?<\/p>\n<p>Thanks<br>\nMatthias<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC and version control systems other than Git",
        "Question_creation_time":1536316201269,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-version-control-systems-other-than-git\/90",
        "Question_upvote_count":2.0,
        "Question_view_count":701.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use Mercurial for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versionioning software that DVC can work with? The only part that I\u2019ve spotted in the docs that really relies on Git is the <code>dvc init<\/code> command that creates <code>.dvc\/.gitignore<\/code>. Would adding contents of this file to hgignore (equivalent of gitignore in Mercurial) solve the problem? Are there other caveats that would prevent me from using Mercurial with DVC?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Batch pipeline support and multi I\/O",
        "Question_creation_time":1530180340930,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/batch-pipeline-support-and-multi-i-o\/42",
        "Question_upvote_count":2.0,
        "Question_view_count":766.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>This tool looks very promising. However, I am wondering about the functionality of the pipeline feature. From the documentation it seems that I can chain an input file to an output file and each logged operation will be appended to the repro argument when I commit. In other words it is not clear to me <strong>how to create a batch process<\/strong> without first defining each step separately.<\/p>\n<p>It is also not clear to me <strong>how to handle multiple input and output files.<\/strong><\/p>\n<p>I am very curious if this would be possible.<\/p>\n<p>Here is an example of an offline analysis that I am performing on human EEG data and behavioral data (using python and java):<\/p>\n<pre><code>sample size \u00d7 \"ASCII_\" + n + \".txt\" --&gt; Pandas --&gt; SPSS Syntax --&gt; Tables + Figures\n\n    --&gt; First Level Summary: Individual Participants (Diagnostics)\n   \/\n--&lt;\n   \\\n    --&gt; Second Level Summary: Group Level\n\n    + --&gt; Behavioral CSV\n\n\nEEG Raw Binary Data --&gt; EEG Manual Preprocessing --&gt; MATLAB Scripts --&gt; Figures\n\n    + --&gt; EEG CSV\n\n\n Behavioral CSV --\n                  \\\n                   &gt;--&gt; Correlations (SPSS Syntax)\n                  \/\n        EEG CSV --<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple overlapping datasets",
        "Question_creation_time":1607351479222,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-overlapping-datasets\/576",
        "Question_upvote_count":0.0,
        "Question_view_count":231.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m considering using dvc for a project and am wondering if it fits the following use case. Say I have N images which form a dataset, but for any given experiment I may only want to train on some subset of the images. What I would like is to be able to determine which images in the subset are missing locally and only pull those images from the file store. Ideally these subsets could also have names and I\u2019d be able to just do dvc pull datasubset3. Is something like this possible?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"dvc status\" confusion in concurrent use case",
        "Question_creation_time":1659618058906,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-status-confusion-in-concurrent-use-case\/1284",
        "Question_upvote_count":1.0,
        "Question_view_count":68.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear DVC community,<\/p>\n<p>I am currently about to develop a data repository based on GIT+DVC. I am new to DVC, hence might still overlook things.<\/p>\n<p>Let us assume to have the following repository after having cloned the GIT repo and pulled everything via DVC.<\/p>\n<pre><code class=\"lang-auto\">repo1\n|- data_file1\n|- data_file1.dvc\n|- data_file2\n|- data_file2.dvc\n<\/code><\/pre>\n<p>Now, another user of the repository has meanwhile cloned &amp; dvc pulled the repository in an identical way to a directory repo2 (on a different machine, i.e. no cache issues, here). But that user has modified data_file2, followed by all that is necessary to push these changes to the GIT repo and DVC remote storage (in autostage mode, this is: dvc add data_file2, git commit, git push, dvc push). At the end, that user will have<\/p>\n<pre><code class=\"lang-auto\">repo2\n|- data_file1\n|- data_file1.dvc\n|- data_file2         &lt;- different to repo1\n|- data_file2.dvc     &lt;- different to repo1\n<\/code><\/pre>\n<p>which will also be the latest version on the server.<\/p>\n<p>Now, as a common habit, the first user with his repo1 directory will do a \u201cgit pull\u201d and will have<\/p>\n<pre><code class=\"lang-auto\">repo1\n|- data_file1\n|- data_file1.dvc\n|- data_file2      &lt;- as originally present in repo1\n|- data_file2.dvc  &lt;- as uploaded by the second user, i.e identical to repo2\n<\/code><\/pre>\n<p>If this first user issues<\/p>\n<pre><code class=\"lang-auto\">dvc status data_file2.dvc\n<\/code><\/pre>\n<p>it will get with my dvc version 2.11.0 this output<\/p>\n<pre><code class=\"lang-auto\">data_file2.dvc:                                                 \n\tchanged outs:\n\t\tnot in cache:       data_file2\n<\/code><\/pre>\n<p>which is understandable (but still somewhat misleading), as indeed data_file2, as uploaded from repo2, is not in the local cache. If the first user then issues<\/p>\n<pre><code class=\"lang-auto\">dvc status --remote myremote\n<\/code><\/pre>\n<p>where myremote is the remote used by both users, the result is<\/p>\n<pre><code class=\"lang-auto\">deleted:            data_file2\n<\/code><\/pre>\n<p>This is however now confusing, as on the remote, the file was changed, but not deleted.<\/p>\n<p>Am I missing something here?<\/p>\n<p>Somewhat related in the same setting: What is the suggested way to get the latest version then of data_file2? Would it just be a general \u201cdvc pull\u201d?<\/p>\n<p>But what if one has a huge repo and only did partial pulls on individual .dvc files. Would one then have to manually go through all these files? It feels like a \u201cdvc update\u201d that only updates already pulled contents would be good, here. But my take is that \u201cdvc update\u201d only does this for files that have been imported from another repo\u2026<\/p>\n<p>Thank you so much for your patience and your help!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Partial download data",
        "Question_creation_time":1582013387314,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/partial-download-data\/322",
        "Question_upvote_count":3.0,
        "Question_view_count":555.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I pushed N images and meta info to remote repository, then remove all data locally.<\/p>\n<p>After some time my colleague want download this dataset, but not fully - he filter meta info and he knows paths to necessary images.<\/p>\n<p>Is there a way download part of data from remote repository?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What DVC does when git merge is executed?",
        "Question_creation_time":1662026528437,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-dvc-does-when-git-merge-is-executed\/1316",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have two git branches. DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Import\/list + webdavs remote not working",
        "Question_creation_time":1657631945188,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/import-list-webdavs-remote-not-working\/1243",
        "Question_upvote_count":0.0,
        "Question_view_count":143.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have set up two repositories. The first one, a data registry, currently with a single dataset. The remote git repository is hosted on a company git on <a href=\"http:\/\/github.com\" rel=\"noopener nofollow ugc\">github.com<\/a>. I have access to a DVC remote using the <code>webdavs<\/code> protocol.<\/p>\n<p>I am able to connect to the DVC storage through some web interface and managed to set up the data registry, i.e., I can push and pull data from it. I achieved this with the default <code>dvc remote add data-remote webdavs:\/\/cloud.com\/data-registry<\/code> and storing the username and password combination in a config.local file.<\/p>\n<p>I\u2019ve created a second project in which I would like to use the data stored in the registry. I.e., a project I would like to run some experiments. This project\u2019s remote is hosted by the same cloud provider only I created a different folder for it. I\u2019ve added the credentials for the experiment and the data registry remote in the config.local (these are thus the same credentials). The experiment project also knows about both remotes.<\/p>\n<p>To test I\u2019ve added a stage with output and tracked some empty files. I was able to push these to the remote. Thus, the connection is working. However, when I try to list or import the data registry, DVC reports (with some privacy edits):<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - received 401 (Unauthorized)\nClient error '401 Unauthorized' for url 'https:\/\/cloud.com\/data-registry\/21\/8ff9c6f73767c8126b66f7213cd0d2.dir'\n<\/code><\/pre>\n<p>What am I missing here? Thanks for the help!<\/p>\n<p><strong>EDIT<\/strong>:<\/p>\n<p>When using <code>dvc import<\/code> to import the dataset I am actually also getting the following error, which might explain why I am getting the 401.<\/p>\n<pre><code class=\"lang-auto\">WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:                                                                                        \nname: None, md5: 218ff9c6f73767c8126b66f7213cd0d2.dir\n<\/code><\/pre>\n<p>I updated the data registry <code>git push\/pull<\/code> and <code>dvc push\/pull<\/code> + <code>dvc status --cloud<\/code> and they say they are all up-to-date. I also checked the storage through the web interface and the file does actually exist, well <code>21\/8ff9c6f73767c8126b66f7213cd0d2.dir<\/code> exists.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC with git worktree",
        "Question_creation_time":1602863664967,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-git-worktree\/532",
        "Question_upvote_count":0.0,
        "Question_view_count":339.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019ve recently started using git worktree to practice parallel development. It comes very handy when I want to work on two branches in the same repo at the same time.<\/p>\n<p>However, I\u2019m came across issue with adding data to DVC on a tree other than the original dir.<\/p>\n<p>I wonder if anyone has experience on this?<\/p>\n<p>Many thanks,<br>\nMaggie<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does DVC actually require Git, or would Mercurial work just as well?",
        "Question_creation_time":1532508768537,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-actually-require-git-or-would-mercurial-work-just-as-well\/55",
        "Question_upvote_count":2.0,
        "Question_view_count":892.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>My question is as the title states really, does DVC actually require Git, or would Mercurial work just as well? Does DVC actually call Git to do anything, or does it just store information in normal files which the user commits.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Comparing different metric\/param\/plot files",
        "Question_creation_time":1607582857470,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/comparing-different-metric-param-plot-files\/582",
        "Question_upvote_count":2.0,
        "Question_view_count":242.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is it possible to perform a diff on 2 distinct metric\/param\/plot files or only the same file can be compared across branch\/commit? The scenario is if instead of having my experiments on different branches I have them on different folders.<br>\nThanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Explicitly define pipeline execution order",
        "Question_creation_time":1645444562953,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/explicitly-define-pipeline-execution-order\/1073",
        "Question_upvote_count":1.0,
        "Question_view_count":106.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is there a way to specify a dependency between two pipeline stages except for a <code>outs<\/code> and <code>deps<\/code> combination? My use case is to run two stages consecutive, although they don\u2019t read \/ write files, that could be tracked by DVC, but depend on each other in a different way, e.g. by one stage writing a value to an external database and the other one reading it again.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Link types and run options",
        "Question_creation_time":1608203560415,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/link-types-and-run-options\/590",
        "Question_upvote_count":2.0,
        "Question_view_count":544.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<br>\nI have questions about the different link types. I\u2019m mainly working on windows at the moment.<\/p>\n<ol>\n<li>What is the default link type for windows?<\/li>\n<li>While I am training a new model (takes some time), I want to work on the old model. To do this I added the flag <code>--outs-persist<\/code> for this. Is this a good a idea in general? If I change the link type now to symlink, hardlink will this work as expected?<\/li>\n<li>I also track the training with mlflow. To add the <code>mlruns<\/code> folder as output to the training stage, I again added  the <code>--outs-persist<\/code> flag. The main reason for this is, that I want to share the mlflow results with others. This works quite well at the moment. What happens if I change the link type now? Will this break something?<\/li>\n<\/ol>\n<p>Thank you<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR: unexpected error - 'NoneType' object has no attribute 'flush'",
        "Question_creation_time":1643641582804,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-nonetype-object-has-no-attribute-flush\/1031",
        "Question_upvote_count":0.0,
        "Question_view_count":917.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Trying to use DVC on windows, got the error below when trying to track a data file with DVC.<br>\nAny idea of the cause and how to resolve ?<br>\nThanks !<\/p>\n<h2>\n<a name=\"dvc-add-testficsv-vv-2022-01-31-160239340-trace-namespacecd-cmdadd-cprofilefalse-cprofile_dumpnone-descnone-externalfalse-filenone-funcclass-dvccommandaddcmdadd-globfalse-instrumentfalse-instrument_openfalse-jobsnone-no_commitfalse-outnone-pdbfalse-quiet0-recursivefalse-remotenone-targetstestficsv-to_remotefalse-verbose2-versionnone-yappifalse-2022-01-31-160239743-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160239762-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160239763-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160239777-error-unexpected-error-nonetype-object-has-no-attribute-flush-1\" class=\"anchor\" href=\"#dvc-add-testficsv-vv-2022-01-31-160239340-trace-namespacecd-cmdadd-cprofilefalse-cprofile_dumpnone-descnone-externalfalse-filenone-funcclass-dvccommandaddcmdadd-globfalse-instrumentfalse-instrument_openfalse-jobsnone-no_commitfalse-outnone-pdbfalse-quiet0-recursivefalse-remotenone-targetstestficsv-to_remotefalse-verbose2-versionnone-yappifalse-2022-01-31-160239743-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160239762-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160239763-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160239777-error-unexpected-error-nonetype-object-has-no-attribute-flush-1\"><\/a>$ dvc add testfi.csv -vv<br>\n2022-01-31 16:02:39,340 TRACE: Namespace(cd=\u2019.\u2019, cmd=\u2018add\u2019, cprofile=False, cprofile_dump=None, desc=None, external=False, file=None, func=&lt;class \u2018dvc.command.add.CmdAdd\u2019&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, no_commit=False, out=None, pdb=False, quiet=0, recursive=False, remote=None, targets=[\u2018testfi.csv\u2019], to_remote=False, verbose=2, version=None, yappi=False)<br>\n2022-01-31 16:02:39,743 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\config.local\u2019 to gitignore file.<br>\n2022-01-31 16:02:39,762 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\tmp\u2019 to gitignore file.<br>\n2022-01-31 16:02:39,763 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\cache\u2019 to gitignore file.<br>\n2022-01-31 16:02:39,777 ERROR: unexpected error - \u2018NoneType\u2019 object has no attribute \u2018flush\u2019<\/h2>\n<h2>\n<a name=\"traceback-most-recent-call-last-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcmainpy-line-55-in-main-ret-cmddo_run-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandbasepy-line-45-in-do_run-return-selfrun-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandaddpy-line-21-in-run-selfrepoadd-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcutilscollectionspy-line-163-in-inner-result-funcbaargs-bakwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-48-in-wrapper-with-lock_reporepo-file-cusersddumetappdatalocalprogramspythonpython38libcontextlibpy-line-113-in-__enter__-return-nextselfgen-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-36-in-lock_repo-with-repolock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-131-in-__enter__-selflock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-120-in-lock-lock_retry-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-45-in-wrapper-return-decocall-dargs-dkwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncyflowpy-line-127-in-retry-return-call-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-66-in-__call__-return-self_funcself_args-self_kwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-103-in-_do_lock-with-tqdm-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcprogresspy-line-85-in-__init__-super__init__-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-1107-in-__init__-selfsp-selfstatus_printerselffp-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-339-in-status_printer-sysstderrflush-attributeerror-nonetype-object-has-no-attribute-flush-2\" class=\"anchor\" href=\"#traceback-most-recent-call-last-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcmainpy-line-55-in-main-ret-cmddo_run-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandbasepy-line-45-in-do_run-return-selfrun-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandaddpy-line-21-in-run-selfrepoadd-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcutilscollectionspy-line-163-in-inner-result-funcbaargs-bakwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-48-in-wrapper-with-lock_reporepo-file-cusersddumetappdatalocalprogramspythonpython38libcontextlibpy-line-113-in-__enter__-return-nextselfgen-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-36-in-lock_repo-with-repolock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-131-in-__enter__-selflock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-120-in-lock-lock_retry-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-45-in-wrapper-return-decocall-dargs-dkwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncyflowpy-line-127-in-retry-return-call-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-66-in-__call__-return-self_funcself_args-self_kwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-103-in-_do_lock-with-tqdm-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcprogresspy-line-85-in-__init__-super__init__-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-1107-in-__init__-selfsp-selfstatus_printerselffp-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-339-in-status_printer-sysstderrflush-attributeerror-nonetype-object-has-no-attribute-flush-2\"><\/a>Traceback (most recent call last):<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\main.py\u201d, line 55, in main<br>\nret = cmd.do_run()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\command\\base.py\u201d, line 45, in do_run<br>\nreturn self.run()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\command\\add.py\u201d, line 21, in run<br>\nself.repo.add(<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\utils\\collections.py\u201d, line 163, in inner<br>\nresult = func(*ba.args, **ba.kwargs)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\repo_<em>init<\/em>_.py\u201d, line 48, in wrapper<br>\nwith lock_repo(repo):<br>\nFile \u201cC:\\Users\\ddumet\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py\u201d, line 113, in <strong>enter<\/strong><br>\nreturn next(self.gen)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\repo_<em>init<\/em>_.py\u201d, line 36, in lock_repo<br>\nwith repo.lock:<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\lock.py\u201d, line 131, in <strong>enter<\/strong><br>\nself.lock()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\lock.py\u201d, line 120, in lock<br>\nlock_retry()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\funcy\\decorators.py\u201d, line 45, in wrapper<br>\nreturn deco(call, *dargs, **dkwargs)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\funcy\\flow.py\u201d, line 127, in retry<br>\nreturn call()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\funcy\\decorators.py\u201d, line 66, in <strong>call<\/strong><br>\nreturn self._func(*self._args, **self._kwargs)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\lock.py\u201d, line 103, in _do_lock<br>\nwith Tqdm(<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\progress.py\u201d, line 85, in <strong>init<\/strong><br>\nsuper().<strong>init<\/strong>(<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\tqdm\\std.py\u201d, line 1107, in <strong>init<\/strong><br>\nself.sp = self.status_printer(self.fp)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\tqdm\\std.py\u201d, line 339, in status_printer<br>\nsys.stderr.flush()<br>\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018flush\u2019<\/h2>\n<h2>\n<a name=\"h-2022-01-31-160240219-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160240224-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160240225-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160240236-debug-version-info-for-developers-dvc-version-293-pip-3\" class=\"anchor\" href=\"#h-2022-01-31-160240219-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160240224-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160240225-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160240236-debug-version-info-for-developers-dvc-version-293-pip-3\"><\/a>2022-01-31 16:02:40,219 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\config.local\u2019 to gitignore file.<br>\n2022-01-31 16:02:40,224 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\tmp\u2019 to gitignore file.<br>\n2022-01-31 16:02:40,225 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\cache\u2019 to gitignore file.<br>\n2022-01-31 16:02:40,236 DEBUG: Version info for developers:<br>\nDVC version: 2.9.3 (pip)<\/h2>\n<p>Platform: Python 3.8.10 on Windows-10-10.0.22000-SP0<br>\nSupports:<br>\nwebhdfs (fsspec = 2022.1.0),<br>\nhttp (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),<br>\nhttps (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)<br>\nCache types: <a href=\"https:\/\/error.dvc.org\/no-dvc-cache\" rel=\"noopener nofollow ugc\">https:\/\/error.dvc.org\/no-dvc-cache<\/a><br>\nCaches: local<br>\nRemotes: None<br>\nWorkspace directory: NTFS on C:<br>\nRepo: dvc, git<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practice for handling large data",
        "Question_creation_time":1618304859684,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-handling-large-data\/721",
        "Question_upvote_count":2.0,
        "Question_view_count":810.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am working with a 25GB raw dataset which when processed to be prepared for training produces another 10+GB file. Using <code>dvc push<\/code> and <code>dvc pull<\/code> with s3 takes a significant amount of time to move data through the network. Is there a way to keep the data only in s3? or use a flag that chooses when to pull or push but that knows that a stage has run and does not need reruning?<\/p>\n<p>What would be the recommended way of working with the data both locally and using a remote?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Retrieve data after using dvc import, but deleting original git repo",
        "Question_creation_time":1572630007806,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/retrieve-data-after-using-dvc-import-but-deleting-original-git-repo\/252",
        "Question_upvote_count":3.0,
        "Question_view_count":1195.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We ran into a problem, where we had original data inside of a particular original git repo. The repo was connected to a bucket on google cloud storage. We then imported the data into a new repo. We just realized that we accidentally deleted the original git repo at some point. The data is still floating around in our bucket, but we no longer have the original git repo\u2019s dvc files to point to it. We do have the new git repo\u2019s files, but doing a dvc pull now gives this error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: failed to fetch data for 'split' - Failed to clone repo '\/mnt\/myfolder' to '\/tmp\/tmp_m8bnoq_dvc-repo': Cmd('git') failed due to: exit code(128)\n  cmdline: git clone --no-single-branch -v \/mnt\/myfolder \/tmp\/tmp_m8bnoq_dvc-repo\n  stderr: 'fatal: repository '\/mnt\/myfolder' does not exist\n'\n<\/code><\/pre>\n<p>Can we do anything now to retrieve the data?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Redundant Data across version and within versions",
        "Question_creation_time":1533613591080,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/redundant-data-across-version-and-within-versions\/60",
        "Question_upvote_count":0.0,
        "Question_view_count":745.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have data projects where very large files change by only a few characters each new version. What disk usage expectations should users have?<\/p>\n<p>Also, there are often cases where we have many files in a directory that look very similar. Are there optimizations by which chunks of data within each file are hashed and de-duplicated?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does DVC deal well with partitioned Parquet files in Hive format?",
        "Question_creation_time":1530872478915,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-deal-well-with-partitioned-parquet-files-in-hive-format\/52",
        "Question_upvote_count":1.0,
        "Question_view_count":1287.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Parquet files in Hive format partitioned by <code>col1<\/code> are, in fact, directories in the format:<\/p>\n<pre><code class=\"lang-python\">\n\/myfile.parquet\n    \/_common_metadata\n    \/_metadata\n    \/col1=val1\n        \/part0.parquet\n        \/part1.parquet\n        \/...\n    \/col1=val2\n        \/part10.parquet\n        \/part12.parquet\n        \/...\n    \/col1=...\n        \/part20.parquet\n        \/part21.parquet\n        \/...\n\n\n<\/code><\/pre>\n<p>The tree structure can go even deeper if the Parquet file in partitioned by several columns rather that just <code>col1<\/code>.<\/p>\n<p><strong>Will DVC deal well with this type of files, both when they are a dependency or an output?<\/strong><\/p>\n<p>Example, where <code>myfile1.parquet<\/code> and <code>myfile2.parquet<\/code> are Parquet files in Hive format partitioned by some col(s):<\/p>\n<pre><code class=\"lang-auto\">dvc add myfile1.parquet\n\n# run script.py wich outputs myfile2.parquet\ndvc run -d myfile1.parquet -d script.py -o myfile2.parquet python script.py\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to automate dvc pull request for a single file?",
        "Question_creation_time":1613106444368,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-automate-dvc-pull-request-for-a-single-file\/666",
        "Question_upvote_count":1.0,
        "Question_view_count":1771.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We are using dvc for heavy AI-ML model files in our gitlab repository.<br>\nLets say, with the help of DVC, we can easily push a model file \u2018X\u2019 to cloud but while pulling    same file on some other server, we have to use command \u201cdvc pull X\u201d.<\/p>\n<p>Currently, we run this command \u201cdvc pull X\u201d manually everytime we update our file X. Since, we dont want to pull all the updated files on cloud therefore it is necessary for us to specify \u2018X\u2019 in our dvc pull requests.<\/p>\n<p>My question is how can we automate this dvc pull request in our CI yaml file for a single file X, if this X is a variable for our file name ?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practice for python package dependency?",
        "Question_creation_time":1587641178959,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-python-package-dependency\/358",
        "Question_upvote_count":2.0,
        "Question_view_count":589.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m figuring out how to structure my project and found your tutorials a very helpful introduction.<\/p>\n<p>In them, however, you only rely on a single script for each stage, for example:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d code\/xml_to_tsv.py -d data\/Posts.xml -o data\/Posts.tsv \\\n          -f prepare.dvc \\\n          python code\/xml_to_tsv.py data\/Posts.xml data\/Posts.tsv\n<\/code><\/pre>\n<p><a href=\"https:\/\/dvc.org\/doc\/tutorials\/pipelines\" rel=\"nofollow noopener\">Source<\/a><\/p>\n<p>Now my understanding is the following:<br>\nIf inside of <code>xml_to_tsv.py<\/code> are imports of some other of my libraries (e.g., a <code>my_special_xml_importer.py<\/code>), and I make changes to <code>my_special_xml_importer.py<\/code>, these changes would not be picked up by dvc, since <code>my_special_xml_importer.py<\/code> is not an explicit dependency of the stage, correct?<\/p>\n<p>What\u2019s the best practice here for bigger projects, where each DVC stage is not just contained in a single script?<\/p>\n<p>Our use case will be the following: For each stage we\u2019ll be having a jupyter notebook, which will import some of our python packages. I\u2019m assuming I should create a stage like this:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d my_notebook.ipynb -d code\/my_lib.py -d data\/Posts.xml -o data\/Posts.tsv\n  -f prepare.dvc\n   papermill my_notebook.ipynb my_notebook_out.ipynb\n<\/code><\/pre>\n<p>Is this a good way, are there other ways, how are people with bigger projects dealing with this issue?<\/p>\n<p>Thanks in advance,<br>\nFabi<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"dvc add -external S3:\/\/mybucket\/data.csv\" is failing with access error even after giving correct remote cache configurations",
        "Question_creation_time":1618473589784,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-external-s3-mybucket-data-csv-is-failing-with-access-error-even-after-giving-correct-remote-cache-configurations\/726",
        "Question_upvote_count":8.0,
        "Question_view_count":1212.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi All,<\/p>\n<p>Am connecting to remote S3 for data and also setting remote dvc cache in same S3.<br>\nFollowing is configure file,<\/p>\n<pre><code class=\"lang-ini\">[core]\n    remote = s3remote\n[cache]\n    s3 = s3cache\n['remote \"s3remote\"']\n    url = S3:\/\/dvc-example\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n['remote \"s3cache\"']\n    url = s3:\/\/dvc-example\/cache\n\tendpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n<\/code><\/pre>\n<p>Am able to push and pull from remote repository to local.<br>\nBut when I try to add external data by configuring cache, am getting error.<br>\nBoth s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610.png\" data-download-href=\"\/uploads\/short-url\/iSi5wqipNKK3zGD6gzqzzcaTV28.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610.png\" alt=\"image\" data-base62-sha1=\"iSi5wqipNKK3zGD6gzqzzcaTV28\" width=\"690\" height=\"32\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1553\u00d773 5.72 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any help would be much appreciated.<\/p>\n<blockquote>\n<p>Also posted in <a href=\"https:\/\/stackoverflow.com\/questions\/67104752\" rel=\"noopener nofollow ugc\">https:\/\/stackoverflow.com\/questions\/67104752<\/a><\/p>\n<\/blockquote>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pull data from GCS without pipelines",
        "Question_creation_time":1615284819759,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-pull-data-from-gcs-without-pipelines\/697",
        "Question_upvote_count":0.0,
        "Question_view_count":314.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am tracking some folders using DVC with google cloud storage. I don\u2019t have any yaml file because I don\u2019t have any experiment.<\/p>\n<p>How can I pull the data from just some folders?<\/p>\n<p>I tried doing <code>dvc pull path_subfolder<\/code> but this checks for a stage (I can see this with -v). After adding a dummy stage, there with an <code>echo<\/code> as cmd and the path as deps, the command goes through but no data is present (only in cache).<\/p>\n<p>Thanks for your help.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hi everyone! First question - How to point multiple projects to single dataset?",
        "Question_creation_time":1613529152023,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/hi-everyone-first-question-how-to-point-multiple-projects-to-single-dataset\/671",
        "Question_upvote_count":1.0,
        "Question_view_count":479.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I stumbled upon dvc over the weekend after discussing with my colleagues our need for just such a thing. I got it working in some test repos with data on our private s3 buckets. Very cool! I really appreciate the documentation, it\u2019s very well put-together.<\/p>\n<p>One thing I\u2019m not sure of is:<\/p>\n<p>If I have a dataset on a remote server and I have several different repos\/projects that use it, what is the appropriate way to point them all to that dataset and\/or a single dvc remote representation thereof? (Do I need in each repo\/project to download it, then dvc add it, and assign the dataset to the same remote?)<\/p>\n<p>thanks!<br>\nRory<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Exposing a non CLI API",
        "Question_creation_time":1536759195852,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/exposing-a-non-cli-api\/95",
        "Question_upvote_count":1.0,
        "Question_view_count":479.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Is there any plan to expose a pure python API so that DVC commands can be run directly within a python script ? (At the moment, I am building a pipeline based on a series of subprocesses that call the various DVC commands, but development \/ debugging would be smoother if I could directly call the underlying python code).<\/p>\n<p>It is probably already possible, but there are no examples in the documentation about this use case.<\/p>\n<p>Thanks !<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How use DVC with Azure Blob Storage",
        "Question_creation_time":1552940286034,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-use-dvc-with-azure-blob-storage\/158",
        "Question_upvote_count":2.0,
        "Question_view_count":4355.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>DVC supports AWS S3, Google Cloud Storage, Microsoft Azure Blob Storage. However, the instruction is only for AWS S3. How to use DVC  with Microsoft Azure Blob Storage?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Statistical significant stage best practice",
        "Question_creation_time":1623146955270,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/statistical-significant-stage-best-practice\/784",
        "Question_upvote_count":0.0,
        "Question_view_count":266.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>In machine learning, it is important to repeat multiple time the same training (often with a different seed) and then compute the average and standard deviation for a metric to evaluate the stability of the model and check the statistical significance of the results.<\/p>\n<p>Let\u2019s say we have a stage <code>trainmodel<\/code> which has a parameter <code>seed<\/code> and we want to run the training 10 times with 10 different seeds. Once all trainings have been completed, we have a script that takes as input the results (metric files) from the 10 runs and produces the average and standard deviation for each metric.<\/p>\n<p>The thing is that I don\u2019t want to store the 10 trained models (since it is very heavy) but only keep the best trained model.<br>\nHowever, I want to keep the metrics of all 10 runs in order to be able to add a stage that compute the average and std of all metrics.<br>\nI also want to launch all 10 trainings in parallel.<\/p>\n<p>What would be the best way to do this with dvc ?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Add stage without running its command",
        "Question_creation_time":1611854828167,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-stage-without-running-its-command\/644",
        "Question_upvote_count":4.0,
        "Question_view_count":314.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I have a script that takes hours to complete. I already run it outside dvc and collected its output. Is there a way to add a stage to the dag with that script avoiding to run it again, pointing the right input\/output\/dependencies? In other words, since computation takes too  much time, i would like to skip it and let dvc do the hashes and complete the run without actually executing the script. Is it possible?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"A separate data-registry for each dataset or combine them into one?",
        "Question_creation_time":1658511793287,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/a-separate-data-registry-for-each-dataset-or-combine-them-into-one\/1262",
        "Question_upvote_count":0.0,
        "Question_view_count":78.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I wonder if it is better to use the same data-registry for two or more separate projects? Or would it better to use a new one for each dataset? For example, what would happen if I have two folders <code>\/data<\/code> in two different projects, then check them add them to a single data repository and then try to run <code>dvc checkout data<\/code> ?? Or <code>dvc pull<\/code>.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR: unexpected error - unable to open database file",
        "Question_creation_time":1640012114896,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-unable-to-open-database-file\/1015",
        "Question_upvote_count":0.0,
        "Question_view_count":555.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI am using dvc exp to run different experiment from my pipeline. However when I try to apply the experiment I have the following error: <code>ERROR: unexpected error - unable to open database file <\/code>.<\/p>\n<p>I will describe all the steps that I made:<\/p>\n<ol>\n<li><code>dvc init --subdir<\/code><\/li>\n<li><code>dvc repro<\/code><\/li>\n<li>\n<code>dvc exp run<\/code> \u2026<\/li>\n<li><code>dvc exp push origin experiment_name<\/code><\/li>\n<li>Then I made some commits (with other work)<\/li>\n<li>Then I tried to do <code>dvc exp apply experiment_name<\/code>\n<\/li>\n<li>I get the error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<li>I did not know that I needed to be in the same project version as when the experiment was run. So I made reset for the commit where I run the experiences<\/li>\n<li><code>dvc exp apply experiment_name<\/code><\/li>\n<li>I got the same error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<li><code>dvc exp pull origin experiment_name<\/code><\/li>\n<li>I got the same error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<\/ol>\n<p>Can anyone help me to understant how can I resolve this issue?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC with external data is very slow",
        "Question_creation_time":1647375655860,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-external-data-is-very-slow\/1121",
        "Question_upvote_count":0.0,
        "Question_view_count":137.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<\/p>\n<p>I think my question has two parts: firstly, is the use of external data (as described <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">here<\/a>) the best way in my case and, if so, why is it so slow to add files?<\/p>\n<p>The context: my ML code + git repo live on my local machine. I develop this codebase using pycharm,  and use pycharm\u2019s remote ssh deployment to run my code on a remote machine where the data lives. The dataset is a large imaging dataset, &gt;200GB with 1000s of files, and will not fit on my local machine. When I run my code, pycharm copies it to the remote machine to run it, but it does not copy the git files so there is no git repo on my remote machine.<\/p>\n<p>As my git repo and data live on different machines, it seemed like adding the external dataset to DVC using ssh would be a good way to go about things. However the dvc add command is taking a long time to run. In fact I tried to run it on a 6GB subset of my data and it errored out:<\/p>\n<p><code>ERROR: too many open files, please visit &lt;https:\/\/error.dvc.org\/many-files&gt; to see how to handle this problem<\/code><\/p>\n<p>So my questions are:<\/p>\n<ol>\n<li>Is this a sensible way to be using DVC?<\/li>\n<li>If so, how can I speed things up? It seems like everything is getting downloaded to my local machine, even though I have set up a remote cache over ssh. Is there any way I can get it to compute the hashes on the remote machine?<\/li>\n<\/ol>\n<p>Thanks for your help,<br>\nMark<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Adjusting absolute paths in stage command w\/o triggering re-run",
        "Question_creation_time":1639271248863,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adjusting-absolute-paths-in-stage-command-w-o-triggering-re-run\/999",
        "Question_upvote_count":0.0,
        "Question_view_count":212.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>first of all, thanks for this awesome project! We are very likely to use it as the backbone of our data versioning infrastructure.<\/p>\n<p>One question came up in the process. I am building a template for pipelines that combine various existing software components or scripts. These scripts are called by pipeline stages (via <code>cmd<\/code>) and take various command line arguments. These specify the paths to input and output files ingested\/created by the scripts. So far so normal. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=10\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>\n<p>The issue is that our scripts need absolute paths. They have no way of resolving relative paths and more importantly the relation of the script path to the data path may not always be the same when the repo is used.<\/p>\n<p>For instance say some input data is stored in <code>my_dvc_repo\/data\/input.csv<\/code>. Then the command for the stage may look like <code>python path\/to\/script\/my_script.py --input my_dvc_repo\/data\/input.csv<\/code>. Another user might clone to <code>repos\/my_dvc_repo\/data\/input.csv<\/code>. Then the command for the stage must include this different path <code>python path\/to\/script\/my_script.py --input repos\/my_dvc_repo\/data\/input.csv<\/code>.<\/p>\n<p>We can deal with that of course. A shell script could modify <code>dvc.yaml<\/code> after each clone and adjust the <code>cmd<\/code> string to include the correct absolute paths.<\/p>\n<p><strong>The blocker seems to be that the stage\u2019s <code>cmd<\/code> is tracked in <code>dvc.lock<\/code>.<\/strong> Modifying it appears to count as a change of the stage, meaning it will have to be rerun even though all we changed was the absolute location of the input\/output files, not their content and not their relation to the rest of the DVC files.<\/p>\n<p>I have come up with various ways how this as well can be solved, but they appear quite hacky*.<\/p>\n<p>So I would just like to sanity check whether I am missing some obvious way already available in DVC. For instance I know about <code>dvc root<\/code> but don\u2019t see how it can be employed in the <code>cmd<\/code> multiple times without making the resulting command string overly complex (thinking about combining <code>realpath<\/code> with <code>dvc root<\/code> each time the absolute repo path is needed).<\/p>\n<p>Appreciating any help! Thanks a lot!<\/p>\n<p>Best regards<\/p>\n<p>Jonas<\/p>\n<p>(*) My current plan is to write a wrapper script which by convention needs to be called first in each <code>cmd<\/code>. It would essentially get the rest of the command as string arguments. Paths could use a placeholder for the absolute path (e.g. <code>ROOT<\/code>) and the wrapper can then replace and call the actual script. E.g. <code>wrapper python my_script --input ROOT\/data\/input.csv<\/code>. This way the <code>cmd<\/code> itself would remain unchanged regardless the absolute location of the repo. Another thought was to use <code>vars<\/code> in <code>dvc.yaml<\/code> and adjust these to the absolute paths programmatically upon cloning, but I do not think these can be used <em>within<\/em> the <code>cmd<\/code> string.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Shared cache without commiting",
        "Question_creation_time":1537882485174,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-without-commiting\/100",
        "Question_upvote_count":0.0,
        "Question_view_count":533.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi<\/p>\n<p>As far as i understand, when I setup a shared cache directory, I have to commit it and others also have to use it.<br>\nIs there a way that I can define a shared cache dir for me only?<\/p>\n<p>Use Cases:<\/p>\n<ul>\n<li>I have a project, where I want to run jobs from two branches simultaneously. So I need to clone the repo twice, but only want the cache to exist once.<\/li>\n<li>I have projects with similar data dependencies and I want to share the cache between both projects.<\/li>\n<\/ul>\n<p>Is it possible or do you have any other recommendations for my use cases?<\/p>\n<p>Thanks and Regards<br>\nMatthias<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to get the date of a dvc remote pushed",
        "Question_creation_time":1611757273971,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-get-the-date-of-a-dvc-remote-pushed\/640",
        "Question_upvote_count":0.0,
        "Question_view_count":200.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi<br>\nwhen I do a dvc pull the files pulled get the current date.<\/p>\n<p>I wonder how to get the date when the files were pushed.<\/p>\n<p>With git I can get this information with <em>git log my_file<\/em>.<br>\nHow to do this kind of information with dvc?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC conflict may arrive if multiple user working on same repository?",
        "Question_creation_time":1535600671377,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-conflict-may-arrive-if-multiple-user-working-on-same-repository\/86",
        "Question_upvote_count":0.0,
        "Question_view_count":786.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. And i will push the data to S3 using dvc push. In our project multiple people are working together if another person already push the data changes to dvc if im going to push same it will through dvc conflict. In git we are getting git conflict<\/p>\n<p>please let me know<\/p>\n<p>Thanks.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Listing files in the remote storage",
        "Question_creation_time":1612427783386,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/listing-files-in-the-remote-storage\/654",
        "Question_upvote_count":0.0,
        "Question_view_count":574.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>We have pushed artifacts to remote storage (S3 bucket in our case). They got renamed and appear in S3 the way which makes it impossible to tell artifacts apart.<\/p>\n<p>We would like to delete some artifacts that are not used anymore. But the bucket is used by multiple projects, so we cannot figure out what is what.<br>\nHow to list the artifacts on the remote server?<\/p>\n<p><code>dvs remote list<\/code> only lists storages, not the contents of the storage.<\/p>\n<hr>\n<p>This is also connected to a question of the best practice on organizing remote storages: is it better to have separate S3 buckets per project? Or one bucket, but separate folder for each artifact?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Track all experiments with a separate directory",
        "Question_creation_time":1618393378968,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-all-experiments-with-a-separate-directory\/722",
        "Question_upvote_count":0.0,
        "Question_view_count":206.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am reading about experiment management <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/experiment-management<\/a> and I am interested in exploring the directories organisation pattern where you can keep all your experiments that you have run recorded at the same time and not through git history.<\/p>\n<p>What is not clear to me is how to implement that pattern though? Anyone has a better idea of how this works?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Step with no dependency keeps running",
        "Question_creation_time":1537531855753,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/step-with-no-dependency-keeps-running\/99",
        "Question_upvote_count":2.0,
        "Question_view_count":763.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>When I have a step in my pipeline that does not have any dependencies, e.g. a <code>wget<\/code> command, <code>dvc repro<\/code> will call that command every time, even if the output is already there. Is this intended behavior?<\/p>\n<p>For my case it does not really make sense as the command should only be run if the output (the downloaded file) is not there already. I know about setting up external dependencies (remotes), but they don\u2019t really work for me here\u2026<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Connect Gdrive on a remote computer",
        "Question_creation_time":1657911967756,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/connect-gdrive-on-a-remote-computer\/1249",
        "Question_upvote_count":1.0,
        "Question_view_count":154.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am working on a remote workstation and would like to pull data. I can copy and past that link<\/p>\n<pre><code class=\"lang-auto\">dvc pull\n  0% Querying remote cache|                                                                                                                                                                                       |0\/1 [00:00&lt;?,    ?files\/s]Your browser has been opened to visit:\n\n    https:\/\/accounts.google.com\/o\/oauth2\/...\n<\/code><\/pre>\n<p>into my local browser and login allow access, but then the token is not transfered to the remote machine, which gets stuck.<\/p>\n<p>To clarify, I am logged in to a workstation via ssh and want to pull the data to that workstation, while the data repository is on GDrive. I want to execute all commands on that workstation remotely and I don\u2019t want to use that workstation as my data-repo.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Need to build non-ML data pipeline, is DVC good fit?",
        "Question_creation_time":1629579502912,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/need-to-build-non-ml-data-pipeline-is-dvc-good-fit\/849",
        "Question_upvote_count":3.0,
        "Question_view_count":333.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I need to build a data pipeline that basically goes like this:<\/p>\n<ol>\n<li>download new data hourly occasionally creating a new file in a directory<\/li>\n<li>extract data into usable form, create new file in output directory<\/li>\n<li>munch on all data for any day that any changed hourly data<\/li>\n<li>position changed output files into directory for on-line query service<\/li>\n<\/ol>\n<p>As I see it, having an entire directory as an output for step 1, 2 and 3 and entire directories as dependencies for steps 2, 3, and 4 could work, but processing steps would need to examine input directories to see what has changed since they last ran. I would prefer to push that logic into the workflow manager.<\/p>\n<p>An alternative would be to add dependencies or outputs each time a new file appears but that seems like just as much work as managing the workflow manually and leads to a kind of ugly DAG. At least I could intelligently version workflow steps this way, unlike option 3.<\/p>\n<p>A third option would be be to create an entire dependency chain for each new file downloaded by step (1). This leads to a really furry processing DAG that will be hard to understand and nearly impossible to consistently change processing steps.<\/p>\n<p>Alternative workflow engines like airflow and luigi and pachyderm might be alternatives if I am trying to force-fit DVC into a round hole, but I really like DVC\u2019s general philosophy.<\/p>\n<p>What is the best way to handle this kind of problem? Is DVC appropriate?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to manage when remote is being updated",
        "Question_creation_time":1621929845821,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-manage-when-remote-is-being-updated\/760",
        "Question_upvote_count":0.0,
        "Question_view_count":205.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello community,<br>\nI am new to data version control. All the examples I see talk about how to update the remote if you update the dataset in the data folder in your local working environment. My question is the following:<\/p>\n<p>suppose I have a remote with 100 images and I pull. That command will copy those files to local, how do I correctly version those 100 images that I have in a remote folder?<br>\nThen, let\u2019s suppose someone adds 100 more images to the remote, getting 200. How do I indicate again that this is the second version of the dataset?<\/p>\n<p>If the data were in local, it would be more or less clear. However, I\u2019m not sure how to do it when the cycle starts on the remote.<\/p>\n<p>Best regards and thank you very much!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About the Questions category",
        "Question_creation_time":1577903864561,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/about-the-questions-category\/306",
        "Question_upvote_count":0.0,
        "Question_view_count":472.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Need help? Have a question about how to use DVC? Ask it here!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to refactor a directory of dvc managed files?",
        "Question_creation_time":1630713202727,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-refactor-a-directory-of-dvc-managed-files\/875",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a case where I created a directory of files that are individually managed by DVC (so there are a number of files, each with a corresponding .dvc file).  I\u2019d like to refactor this so that I just tell DVC to managed the entire directory and all of its contents to get rid of all those .dvc files cluttering things up.<\/p>\n<p>I also have this situation in the recursive case (I\u2019d like to tell DVC to just manage a directory parent and all ancestor contents).<\/p>\n<p>What is the correct way to do this?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"`dvc list -R` invalidates repo path",
        "Question_creation_time":1605037463923,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-list-r-invalidates-repo-path\/551",
        "Question_upvote_count":3.0,
        "Question_view_count":312.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Given the folder structure in my <code>data\/<\/code> folder:<\/p>\n<pre><code class=\"lang-auto\">nested_start\n\u251c\u2500\u2500 folder_1\n\u2502   \u251c\u2500\u2500 folder_a\n\u2502   \u2502   \u251c\u2500\u2500 file_0.txt\n\u2502   \u2502   \u2514\u2500\u2500 file_1.txt\n\u2502   \u2514\u2500\u2500 folder_b\n\u2502       \u251c\u2500\u2500 file_0.txt\n\u2502       \u2514\u2500\u2500 file_1.txt\n\u251c\u2500\u2500 folder_2\n\u2502   \u251c\u2500\u2500 folder_a\n\u2502   \u2502   \u251c\u2500\u2500 file_0.txt\n\u2502   \u2502   \u2514\u2500\u2500 file_1.txt\n\u2502   \u2514\u2500\u2500 folder_b\n\u2502       \u251c\u2500\u2500 file_0.txt\n\u2502       \u2514\u2500\u2500 file_1.txt\n\u2514\u2500\u2500 folder_3\n    \u251c\u2500\u2500 folder_a\n    \u2502   \u251c\u2500\u2500 file_0.txt\n    \u2502   \u2514\u2500\u2500 file_1.txt\n    \u2514\u2500\u2500 folder_b\n        \u251c\u2500\u2500 file_0.txt\n        \u2514\u2500\u2500 file_1.txt\n<\/code><\/pre>\n<p>Generated by the following Python script:<\/p>\n<pre><code class=\"lang-python\">import itertools\n\nfrom pathlib import Path\n\nstart = Path(\"data\", \"nested_start\")\nstart.mkdir(exist_ok=True, parents=True)\n\nfor f1, f2 in itertools.product((\"1\", \"2\", \"3\"), (\"a\", \"b\")):\n    final_folder = Path(start, f\"folder_{f1}\", f\"folder_{f2}\")\n    final_folder.mkdir(exist_ok=True, parents=True)\n\n    for file_idx in range(2):\n        with (final_folder \/ f\"file_{file_idx}.txt\").open(\"w\") as fi:\n            fi.write(f\"{f1} {f2} {file_idx}\\n\")\n<\/code><\/pre>\n<p>After adding and pushing, I can list this folder with:<\/p>\n<pre><code class=\"lang-auto\">dvc list . data\/nested_start\n<\/code><\/pre>\n<p>However, I cannot list it recursively.<\/p>\n<pre><code class=\"lang-auto\">$ dvc list -R . data\/nested_start\nERROR: failed to list '.' - The path 'data\/nested_start' does not exist in the target repository '.' neither as a DVC output nor as a Git-tracked file. \n<\/code><\/pre>\n<p>Why does <code>-R<\/code> make my repository invalid?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc fails due to outs stage entry",
        "Question_creation_time":1637252513725,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-fails-due-to-outs-stage-entry\/975",
        "Question_upvote_count":0.0,
        "Question_view_count":205.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI cannot run dvc due to the following error:<br>\nERROR: failed to reproduce \u2018dvc.yaml\u2019: output \u2018\u2026\/data-registry\/forecast\/models\u2019 does not exist<\/p>\n<p>I already tried different paths, relative, absolute and using direclty the paths instaed of using it by params var. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/confused.png?v=10\" title=\":confused:\" class=\"emoji\" alt=\":confused:\"><br>\nThe weird thing is that this already worked. And suddently stops working.<br>\nI already use dvc destroy to clean everthing and I starded again and the problems continues <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/confused.png?v=10\" title=\":confused:\" class=\"emoji\" alt=\":confused:\"><\/p>\n<p>Can someone help me?<br>\nthanks<\/p>\n<p>My dvc.yaml is:<\/p>\n<pre><code class=\"lang-auto\">vars:\n  - db_config_file: \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/config\/config.yaml\n  - params_file: \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/params.yaml\n\nstages:\n  train:\n    wdir: ..\n    cmd: python -m forecasting_module.pipeline train ${db_config_file} ${params_file}\n    params:\n      - ${params_file}:\n        - common.paths.models\n        - common.columns.id\n        - common.columns.date\n        - common.columns.value\n        - prophet.columns.prophet_date\n        - prophet.columns.prophet_value\n        - prophet.train_parameters.seasonality_mode\n    outs:\n      - ${common.paths.models}\n<\/code><\/pre>\n<p>The debug outputs is:<\/p>\n<pre><code class=\"lang-auto\">2021-11-18 16:14:12,120 DEBUG: Dependency 'params.yaml' of stage: 'train' changed because it is '{'common.paths.models': 'new', 'common.columns.id': 'new', 'common.columns.date': 'new', 'common.columns.value': 'new', 'prophet.columns.prophet_date': 'new', 'prophet.columns.prophet_value': 'new', 'prophet.train_parameters.seasonality_mode': 'new'}'.\n2021-11-18 16:14:12,121 DEBUG: stage: 'train' changed.\n2021-11-18 16:14:12,122 DEBUG: Removing output '..\/data-registry\/forecast\/models' of stage: 'train'.\n2021-11-18 16:14:12,122 DEBUG: Removing '..\/data-registry\/forecast\/models'\n2021-11-18 16:14:12,129 DEBUG: defaultdict(&lt;class 'dict'&gt;, {'params.yaml': {'common.paths.models': 'new', 'common.columns.id': 'new', 'common.columns.date': 'new', 'common.columns.value': 'new', 'prophet.columns.prophet_date': 'new', 'prophet.columns.prophet_value': 'new', 'prophet.train_parameters.seasonality_mode': 'new'}})\nRunning stage 'train':\n&gt; python -m forecasting_module.pipeline train \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/config\/config.yaml \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/params.yaml\n2021-11-18 16:14:12,937 ERROR: failed to reproduce 'dvc.yaml': output '..\/data-registry\/forecast\/models' does not exist\n\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Workflow on slurm-like clusters",
        "Question_creation_time":1599401341769,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/workflow-on-slurm-like-clusters\/484",
        "Question_upvote_count":2.0,
        "Question_view_count":1045.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI have access to a  <a href=\"https:\/\/slurm.schedmd.com\/overview.html\" rel=\"nofollow noopener\">slurm<\/a> (gpu) cluster and my current workflow is quite a mess. Is there any advice or experience with that? So some problems I see:<\/p>\n<ul>\n<li>\n<p>you have to submit jobs:<\/p>\n<ul>\n<li>jobs are scheduled, delayed (sometimes hours)<\/li>\n<li>jobs can run in parallel<\/li>\n<li>you have to \u201cwait\u201d till the last job started, before you can queue new jobs<\/li>\n<li>manual \u201ccommitting\u201d of results<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>docker unfriendly<\/p>\n<\/li>\n<li>\n<p>testing the whole pipeline from data processing to \u201cresult management\u201d  is hard<\/p>\n<\/li>\n<li>\n<p>data security: everybody has access to everything:<br>\nThere are tools like <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noopener\">rclone<\/a> and <a href=\"https:\/\/github.com\/facebookresearch\/CrypTen\" rel=\"nofollow noopener\">crypten<\/a>. Can dvc help with this problem or interact with those tools?<br>\nCan dvc \/ cml  help there to automate things?<br>\nThanks in advance. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/grinning.png?v=9\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\"><\/p>\n<\/li>\n<\/ul>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does DVC do version checking?",
        "Question_creation_time":1554023291346,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-does-dvc-do-version-checking\/168",
        "Question_upvote_count":6.0,
        "Question_view_count":1089.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Let\u2019s assume that we have data on an S3 Bucket. The data is downloaded locally and after processing, a TFRecord of the data is made.<\/p>\n<p>The data in the S3 Server has increased in samples and we don\u2019t want to redo the pipeline for all of the data. The new data is on the same S3 bucket as the previous version. No versioning of the data is done on the S3.<\/p>\n<ol>\n<li>How does DVC understand that the data has changed? Do we need to trigger it manually?<\/li>\n<li>How can the pipeline be only run for the new version of the data?<\/li>\n<li>How can be tag the data and\/or the TFRecord with a certain version number that is implicitly in the system? Can this tag be read from anywhere?<\/li>\n<\/ol>\n<p>I would appreciate if you can share your solutions.<\/p>\n<p>Regards,<br>\nSiavash<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deleting a DVC tracked file from history",
        "Question_creation_time":1605517757309,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deleting-a-dvc-tracked-file-from-history\/556",
        "Question_upvote_count":0.0,
        "Question_view_count":693.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a dataset tracked by DVC which contains personal information about a group of people. Keeping privacy and security in mind, if I get a request from someone for having their data deleted from the dataset, how do I go about doing that from the entire history of the dataset? Just deleting the file and adding another DVC commit obviously won\u2019t do because the data is still easily recoverable in the history.<\/p>\n<p>I can see that <code>dvc gc<\/code> command does something along this line but it\u2019s not clear to me from the documentation whether you can specify the file that you want deleted. Looks like it only deletes unused files in the history by default.<\/p>\n<p>Is there any command that the deletion of targeted files from the entire DVC history?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Change SSH remote",
        "Question_creation_time":1654850566708,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/change-ssh-remote\/1208",
        "Question_upvote_count":1.0,
        "Question_view_count":112.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there, I had a configuration with SSH remote working fine.<br>\nThen, I had to change to other IP so I changed the IP in the configuration file. Then new server is a copy from the previous one.<br>\nNow, I get the following error when I try to push:<\/p>\n<p>ERROR: failed to transfer \u2018md5: 52ad4e880291989c4ac4419cc4d85376\u2019 - Permission denied<\/p>\n<p>Can be something related to the cache?<br>\nfailed to upload \u2018.dvc\/cache\/ca\/217e1b1f3a0c37a2b0c490558d7ddf.dir\u2019 to 'ssh:\/\/user@IP\/path\/data\/ca\/217e1b1f3a0c37a2b0c490558d7ddf<\/p>\n<p>Does anyone know why? Am I missing any step?<br>\nThanks in advance<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple users in one repo",
        "Question_creation_time":1620974102574,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-users-in-one-repo\/746",
        "Question_upvote_count":1.0,
        "Question_view_count":1211.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>We have a scenario that someone might shed some light on.<\/p>\n<p>We are sucessfully implementing Shared Development Server and Data Registries with shared cache and user groups in our deep learning development. It all works perfectly, shared cache, remotes, etc except one particular issue very specific for our use case.<\/p>\n<p>Our current approach for data registries is that for any particular dataset we have a master copy of raw original data in that data registry repo on the server. Any data injections are done only from within that repo. So we always have a copy of raw datasets (not hashed) along with caches and a copy on the remote, which are hashed.<\/p>\n<p>The issue with this scenario is that we need several users be able to add, commit and push from within a particular repo. And even when they are in the same group we have to manually set permissions to 2775 and 0664 for everythiong in .dvc folder (cache is not there as per shared cache scenario) so that dvc works. Git commands work without any permission issues. And from time to time permissions break on the dvc remote too and we have to reset them manually to 2775 and 0444.<\/p>\n<p>Is it something that we might need to setup dvc some other way? Or is it just not supported but can be supported and possibly needs contribution?<\/p>\n<p>Thanks!<br>\nSimon<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC on HPC with CML and large(r) number of experiments",
        "Question_creation_time":1662118461346,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-on-hpc-with-cml-and-large-r-number-of-experiments\/1319",
        "Question_upvote_count":0.0,
        "Question_view_count":87.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, we are currently testing and configuring a new HPC setup and I will be working on setting up a best practice guide and template projects for running ML\/Data science project using DVC, CML and possibly other tools.<\/p>\n<p>Based on the examples I read, I am wondering if the CML workflow also supports large numbers of parameter grid searches?<\/p>\n<p>Thank you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can't go to former version of dataset with `dvc checkout`",
        "Question_creation_time":1610450954470,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cant-go-to-former-version-of-dataset-with-dvc-checkout\/615",
        "Question_upvote_count":3.0,
        "Question_view_count":2005.0,
        "Question_answer_count":14,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out DVC for the first time for my deep learning development pipeline. As mentioned in the <a href=\"https:\/\/youtu.be\/kLKBcPonMYw\" rel=\"noopener nofollow ugc\">Version Data with DVC<\/a> tutorial, I created a folder named \u201cdataset\u201d in the main directory and then I added some data into it. And then  I used <code>dvc add dataset<\/code> to track the dataset. After that, I included \u201cdataset\u201d folder in the .gitignore to untrack the dataset with git.<\/p>\n<p>Then I setup Azure storage blob for my remote data repo. After adding some more data, I wanted to go back to the original version of the data so I checkout to that git commit and then used <code>dvc checkout<\/code> command. Then this error comes up.<\/p>\n<pre><code class=\"lang-auto\">+--------------------------------------------------+\n|                                                  |\n|        Update available 1.10.2 -&gt; 1.11.10        |\n|     Run `apt-get install --only-upgrade dvc`     |\n|                                                  |\n+--------------------------------------------------+\n\nWARNING: Cache 'HashInfo(name='md5', value='db4287e726604a63a231cf4462cb27df.dir', dir_info=None, size=497535926, nfiles=8818)' not found. File 'dataset' won't be created.\nERROR: Checkout failed for following targets:\ndataset\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n<\/code><\/pre>\n<p>I tried searching for the solution in the given link but I could find it. Thanks in advance.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Track remote data on Azure",
        "Question_creation_time":1646821961294,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-remote-data-on-azure\/1110",
        "Question_upvote_count":1.0,
        "Question_view_count":298.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello DVC community!<\/p>\n<p>I\u2019m currently trying out DVC on Ubuntu 20.04 LTS, with Python 3.8.10, on WSL2 with Windows 10.<br>\nI am running into some seemingly inconsistent issues trying to track ZIP files in a remote Azure BLOB Storage container.<\/p>\n<p>At least in the initial set-up, I am trying to track those files without downloading them to the local machine. Apologies if this is really a basic question but I a bit stumped at the moment.<\/p>\n<p>I used \u201cdvc remote add\u201d and \u201cdvc remote modify\u201d, the generated config \/ config.local files in the .dvc folder look good. I added a valid connection string for the Azure storage account with those commands, then I also added that connection string to my environment with:<\/p>\n<blockquote>\n<p>export AZURE_STORAGE_CONNECTION_STRING=\u2018myconnectionstring\u2019<\/p>\n<\/blockquote>\n<p>I am trying the following command:<\/p>\n<blockquote>\n<p>dvc import-url azure:\/\/[mycontainer]\/[myfile] --to-remote<\/p>\n<\/blockquote>\n<p>The remote is another BLOB container in the same Storage Account where the files that I want to track are located.<\/p>\n<p>Yesterday I managed to use the \u201c<em>import-url \u2026 --to-remote<\/em>\u201d command even though the .dvc files that were created locally landed in the wrong folder (my mistake with paths etc). Today I am simply getting the error:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - : \u2018azure\u2019<\/p>\n<\/blockquote>\n<p>If I try to run:<\/p>\n<blockquote>\n<p>dvc import-url azure:\/\/[mycontainer]\/[myfile] .\/path\/to\/local\/folder\/ --to-remote<\/p>\n<\/blockquote>\n<p>The error I get is:<\/p>\n<blockquote>\n<p>ERROR: failed to import azure:\/\/[mycontainer]\/[myzipfilename]. You could also try downloading it manually, and adding it with <code>dvc add<\/code>. - bad DVC file name \u2018path\/to\/local\/folder\/[myzipfilename].dvc\u2019 is git-ignored.<\/p>\n<\/blockquote>\n<p>Same error as the above appears if I move into the folder where I want the .dvc files to land and run the dvc import-url command from there, regardless of whether I do this with specifying the output file name or not.<\/p>\n<p>EDIT: I ran it with -v and this is what I get:<\/p>\n<blockquote>\n<p>2022-03-09 11:39:09,229 DEBUG: Lockfile for \u2018dvc.yaml\u2019 not found<br>\n2022-03-09 11:39:09,231 ERROR: unexpected error - : \u2018azure\u2019<\/p>\n<\/blockquote>\n<p>Somehow there is no lock file, seems I am missing something super basic <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>SECOND EDIT:<br>\nIf I run the import-url command from the root project folder without specifying a path that is different from this project root, it works, with the \u201cproblem\u201d that the local .dvc file is in the root and not in a subdirectory where I actually want it to be. I still get the \u201cLockfile for \u2018dvc.yaml\u2019 not found\u201d debug msg but then \u201cComputing md5 for a large file \u2026\u201d indicating that the command \u201cworks\u201d.<\/p>\n<p>I am getting more confused by the minute lol.<br>\nAny ideas how I can fix this?<\/p>\n<p>Thanks a bunch in advance!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc checkout takes long time",
        "Question_creation_time":1647850736880,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-checkout-takes-long-time\/1127",
        "Question_upvote_count":0.0,
        "Question_view_count":319.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<br>\nI\u2019m testing dvc in the workspace(or dvc project) of a gpu server and NFS mounted NAS to that server.<\/p>\n<p>The dataset is about a million images (~40GB) and is located in NAS.<\/p>\n<p>I run the following command in my workspace:<\/p>\n<pre><code> dvc add \"dataset path in NAS\" -o .   (already set dvc cache dir in a path in NAS.)\n<\/code><\/pre>\n<p>This takes about 2 hours. (I wonder it\u2019s normal).<\/p>\n<p>The problem is when I run \u2018dvc checkout\u2019 in the other workspace to see it works fine, it takes about 1 hour. (I also set dvc config cache.type symlink.)<\/p>\n<p>Even though I make symlink files from the original dataset using os.symlink in python, it doesn\u2019t take this long. (I guess it\u2019s about a few minutes).<\/p>\n<p>Could I shorten this \u2018dvc checkout\u2019 time as short as possible?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/977d7b25475db607c9da54f925de20434f5998cb.png\" data-download-href=\"\/uploads\/short-url\/lC8Wh3txuQbHAjckfoN7Ub1Q1lx.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/977d7b25475db607c9da54f925de20434f5998cb.png\" alt=\"image\" data-base62-sha1=\"lC8Wh3txuQbHAjckfoN7Ub1Q1lx\" width=\"690\" height=\"314\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/977d7b25475db607c9da54f925de20434f5998cb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">758\u00d7345 23.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple machines setup for one repo",
        "Question_creation_time":1603954361775,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-machines-setup-for-one-repo\/539",
        "Question_upvote_count":2.0,
        "Question_view_count":433.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>First of all - thanks for developing such a great tool! We are currently incorporating it in our deep learning workflow for different computer vision projects.<\/p>\n<p>I think we got a grasp of how it all should be set up and are able to run our pipelines properly on one machine. One big question that we still can\u2019t figure out is the best approach to experiment and version control with dvc simultaneuosly on several machines.<\/p>\n<p>Create separate branches per each machine and iterate there? But what about shared data (the very first dependency in our pipeline) then? Do we just <code>dvc add data<\/code> and commit those changes in one branch (e.g. with tag). And then <code>git checkout &lt;tag&gt; data.dvc<\/code> and <code>dvc checkout data.dvc<\/code> on other machines?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to output the stage file in a different directory than CWD?",
        "Question_creation_time":1533304349308,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-output-the-stage-file-in-a-different-directory-than-cwd\/58",
        "Question_upvote_count":2.0,
        "Question_view_count":1212.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I typically run scripts from my project\u2019s root directory, and use parameters to point to inputs and outputs as needed. Wrapping with <code>dvc run<\/code> generates a stage file in the root directory, which clutters the project.<\/p>\n<p>I tried using the <code>-f<\/code> option in <code>dvc run<\/code> to specify a different path for the stage file, but it ended up causing problems with the data paths. In particular it appears that DVC considers the dependencies and output paths to be relative to the stage file.<\/p>\n<p>My current solution is to make a <code>runs<\/code> directory from where I execute all <code>dvc run<\/code> commands. All the stage files end up in there. But this is a bit clunky and isn\u2019t the pattern I usually use.<\/p>\n<p>I\u2019m wondering if I\u2019m missing something here? Is there a way to store stage files to somewhere other than where the <code>dvc run<\/code> command was executed?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Handling indeterministic output",
        "Question_creation_time":1534925717183,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/handling-indeterministic-output\/78",
        "Question_upvote_count":3.0,
        "Question_view_count":881.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>first of all, very nice tool, thank you for that!<\/p>\n<p>I am currently experimenting with dvc in a pytorch ML pipeline.<br>\nOne step of the pipeline is training and pickling a model.<\/p>\n<p>Usually training a model starts with random numbers, so if I do not set random seeds I will end up in a dfiferent model each training re-run, resulting in a new md5 hash for the model file each time.<br>\nEven if I set seeds, and the model ends up in the same values, I was not able (yet) with pytorch to pickle the model in a way that it exactly equals the previous run in terms of md5 hash.<\/p>\n<p>Do you recommend any best practices regarding such kind of indeterministic\/random output (for pytorch and\/or in general)?<br>\nI think that there might arise problems when sharing pipelines but not data, or when having different metrics output.<br>\nIs it a problem at all? Is dvc capable of handling these things and if yes, how?<\/p>\n<p>Thanks,<\/p>\n<p>Alex<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline to process only new files",
        "Question_creation_time":1594916713834,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipeline-to-process-only-new-files\/446",
        "Question_upvote_count":3.0,
        "Question_view_count":428.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Say I have a million files in the directory <code>.\/data\/pre<\/code>.<\/p>\n<p>I have a python script <code>process_dir.py<\/code> which goes over each file in <code>.\/data\/pre<\/code> and processes it and creates a file in the same name in a directory <code>.\/data\/post<\/code> (if such file already exists, it skips processing it).<\/p>\n<p>I defined a pipeline:<br>\n<code>dvc run -n process -d process_dir.py -d data\/pre -o data\/post python process_dir.py<\/code><\/p>\n<p>Now let\u2019s say I removed one file from <code>data\/pre<\/code>.<\/p>\n<p>When I run <code>dvc repro<\/code> it will still process all the 999,999 files again, because it will remove the entire content of the .\/data\/post directory before running the <code>process<\/code> stage. Is there any way to manage the pipeline so that <code>process.py<\/code> will not process the same file twice?<\/p>\n<p>The only way I could think of is creating <code>process_file.py<\/code> which handles a single file, and executing 1 million commands like this:<br>\n<code>dvc run -n process_1 -d process_file.py -d data\/pre\/1.txt -o data\/post\/1.txt python process_file 1.txt<\/code><br>\n<code>dvc run -n process_2 -d process_file.py -d data\/pre\/2.txt -o data\/post\/2.txt python process_file 2.txt<\/code><br>\n\u2026<br>\n<code>dvc run -n process_1000000 -d process_file.py -d data\/pre\/1000000.txt -o data\/post\/1000000.txt python process_file 1000000.txt<\/code><\/p>\n<p>Is there a more elegant way?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC and MLFlow - reproduce experiments using git commit ids",
        "Question_creation_time":1606334637517,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-mlflow-reproduce-experiments-using-git-commit-ids\/561",
        "Question_upvote_count":11.0,
        "Question_view_count":2665.0,
        "Question_answer_count":14,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! I\u2019ve started using DVC and I love it, thank you for your work!<\/p>\n<p>I have a question regarding DVC and MLFlow combination. I hope someone can help me with that.<br>\nI am using DVC to build\/run pipelines and version data and models. And I am using MLFlow to have a nice overview of all the experiments and to visualize\/store metrics and plots of the experiments.<br>\nDuring the training and evaluation stages, I am logging stuff to MLFlow (including current git commit id for reproducibility).<\/p>\n<p>Let\u2019s say I want to experiment with a different learning rate.<br>\nMy actions are:<\/p>\n<ul>\n<li>I am updating the learning rate in params.yaml<\/li>\n<li><code>git commit -m 'starting a new run with updated learning rate'<\/code><\/li>\n<li>\n<code>dvc repro<\/code> (new MLFlow run with metrics and current git commit id is created)<\/li>\n<li>I check the results and I like them. I want to save the model.<\/li>\n<li><code>git add 'dvc.lock'<\/code><\/li>\n<li><code>git commit -m 'awesome learning rate'<\/code><\/li>\n<li><code>dvc push<\/code><\/li>\n<li><code>git push<\/code><\/li>\n<\/ul>\n<p>So I have 2 commits here. Commit A before the run and commit B after.<\/p>\n<p>Let\u2019s say I checked my MLFlow dashboard and I want to get the trained model of the last run.<br>\nIt is linked to the commit A.<br>\nIf I <code>git checkout<\/code> commit A, I won\u2019t get the trained model until I <code>dvc repro<\/code> again.<br>\nAnd I can\u2019t log commit B to the MLFlow run during training\/evaluation because I haven\u2019t created commit B yet.<\/p>\n<p>Can\u2019t really see the whole picture\u2026 How to do it properly?<br>\nAny ideas would be very helpful!<\/p>\n<p>Thanks in advance<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"CML runner + docker on Github",
        "Question_creation_time":1646812178953,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-runner-docker-on-github\/1108",
        "Question_upvote_count":0.0,
        "Question_view_count":170.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi. I\u2019m using cml runner to deploy a container on EC2 and I need to use docker commands inside (build an image based on dvc pipeline results). I have seen there is a \u201cdocker-volumes\u201d option which can enable using docker but it\u2019s only available on GitLab, is there any solution or workaround for GitHub?<br>\nThanks.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Add a remote directory without adding to the cache",
        "Question_creation_time":1595379583111,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-a-remote-directory-without-adding-to-the-cache\/452",
        "Question_upvote_count":2.0,
        "Question_view_count":1980.0,
        "Question_answer_count":16,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We have a corpus licensed from a third party that is stored in our S3 bucket.  The corpus is fixed and should never change, so versioning is not much of a concern.  I\u2019d like to integrate it into my repository so that when someone checks it out from git, they can run a simple command like <code>dvc checkout<\/code> and it will download the corpus.  However, since it is already stored in S3 and should never change, I would prefer that it doesn\u2019t get copied into our remote cache.  But, I\u2019m not sure this is really possible with <code>dvc<\/code>.<\/p>\n<p>One solution might be to use something like <code>dvc run -n get-corpus -O [local-corpus-path] dvc get-url [remote-corpus-path] [local-corpus-path]<\/code>.  Then (I think) someone can just run <code>dvc repro get-corpus<\/code> or any downstream tasks, and dvc will get the corpus.  But, will this work?  And is this the best\/recommended way?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Lock error with parallelized dvc repro",
        "Question_creation_time":1634713211772,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929",
        "Question_upvote_count":1.0,
        "Question_view_count":207.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a pipeline with many parallel stages. I use a combination of a (generated) <code>params.yaml<\/code> file and <code>foreach<\/code> approach. See below for simplified examples of <code>dvc.yaml<\/code>, <code>params.yaml<\/code>, and <code>dvc.lock<\/code>.<\/p>\n<p>I wrote a simple scheduler in Python to run the stages with <code>dvc repro<\/code> in parallel using multiprocessing. Essentially, it\u2019s running running <code>dvc repro setup@sim1<\/code>, <code>dvc repro setup@sim2<\/code>, etc. in parallel.<\/p>\n<p>From what I <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#parallel-stage-execution\" rel=\"noopener nofollow ugc\">understand<\/a> this should work. However, I\u2019m getting lock errors: \u201cERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly.\u201d<\/p>\n<p>dvc.yaml:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  setup:\n    foreach: ${sims}\n    do:\n      cmd: python my_script.py --simlabel ${item.label}\n      deps:\n        - my_script.py\n      outs:\n        - ${item.work_dir}\/outputs.txt\n<\/code><\/pre>\n<p>params.yaml<\/p>\n<pre><code class=\"lang-auto\">sims:\n\tsim1:\n\t\tlabel: label1\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label1_workdir\n\tsim2:\n\t\tlabel: label2\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label2_workdir\n<\/code><\/pre>\n<p>dvc.lock:<\/p>\n<pre><code class=\"lang-auto\">schema: '2.0'\nstages:\n  setup@sim1\n    cmd: python python my_script.py --simlabel label1\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label1_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n  setup@sim2\n    cmd: python python my_script.py --simlabel label2\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label2_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I integrate DVC with perforce?",
        "Question_creation_time":1550225525646,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-i-integrate-dvc-with-perforce\/121",
        "Question_upvote_count":3.0,
        "Question_view_count":873.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use perforce for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versioning software that DVC can work with?<\/p>\n<p>Is it possible to integrate my DVC with perforce.<\/p>\n<p>Please let me know.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Differences between dvc repro and dvc status",
        "Question_creation_time":1537891906524,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/differences-between-dvc-repro-and-dvc-status\/101",
        "Question_upvote_count":0.0,
        "Question_view_count":709.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is it guaranteed that if <code>dvc status Dvcfile<\/code> shows: nothing to reproduce, the <code>dvc repro Dvcfile<\/code> will not do anything ?<\/p>\n<p>For my use case, it looks like <code>dvc repro<\/code> (even when there is nothing to reproduce) takes more time to run than <code>dvc status<\/code>. I could then switch to calling <code>dvc status<\/code> to find out if there\u2019s nothing to reproduce and have a faster pipeline.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC and FDA compliance",
        "Question_creation_time":1637738029985,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-fda-compliance\/983",
        "Question_upvote_count":1.0,
        "Question_view_count":222.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Does DVC falls under FDA regulatory system?<\/p>\n<p>We\u2019re in the phase of developing an SaMD which requires approvals by FDA, for all tools that are used in developing this device.<\/p>\n<p>Knowing us of any referrals\/suggestions is greatly appreciated.<\/p>\n<p>Thanks in Advance!!!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Probelm when running dvc exp run",
        "Question_creation_time":1662547455901,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/probelm-when-running-dvc-exp-run\/1329",
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, upon running a dvc exp run i get the following error message, after all the stages are finished:<br>\nERROR: configuration error - config file error: expected \u2018url\u2019 for dictionary value \u2026<br>\nIn addition to that a very old version of the project is being restored eventhough commits to both git and dvc have been made. What could be the reason for that?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc repro --force-downstream not working",
        "Question_creation_time":1647474556097,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-repro-force-downstream-not-working\/1122",
        "Question_upvote_count":0.0,
        "Question_view_count":109.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I setup some dvc data pipeline and use dvc repro command generated some .pkl files. I later on decided to regenerate the files (due to code change, where the dependency wasn\u2019t captured in the dvc.yaml file), so I deleted the file manually in the directory. This is probably the first mistake I made.<\/p>\n<p>So now when I run dvc repro, dvc refuse to re-run and report nothing has been change.<\/p>\n<p>I thought dvc repro --force-downstream [target] should work, but it\u2019s still reporting \u201ceverything is up to date\u201d and not reproducing the stage.<\/p>\n<p>How do I force dvc to re-run the stage to regenerate the .pkl file?<\/p>\n<p>Thanks,<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Concept and data drift",
        "Question_creation_time":1637792304507,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/concept-and-data-drift\/986",
        "Question_upvote_count":1.0,
        "Question_view_count":195.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello<br>\nI am new-ish to DVC and still getting orientated. Please could I ask - are there any specific tools or processes within DVC Studio for monitoring concept and data drift? Or any plans in that area?<br>\nThanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Managing pipelines operating per dataset element",
        "Question_creation_time":1609963458190,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/managing-pipelines-operating-per-dataset-element\/613",
        "Question_upvote_count":3.0,
        "Question_view_count":733.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! I would like to understand the appropriate way to manage pipeline stages that are not \u201csliced\u201d stage-wise, but rather dataset element-wise.<\/p>\n<p>For example, suppose a medical imaging dataset comprises different subjects, each of whom has a collection of files associated to them in a subject-specific directory. Such organization is convenient for manual inspection. Initially, only an image exists for each patient, which is located in S3 and obtained using <code>dvc import-url<\/code>. The to-be-written DVC pipeline will produce additional files (preprocessing, etc.) in each subject\u2019s directory.<\/p>\n<p>My current understanding from DVC\u2019s (excellent) documentation is there are potentially several ways to accomplish this:<br>\n[a] Reshape the dataset such that subjects are distributed across multiple stage directories, which are given as <code>dep<\/code> or <code>out<\/code> to the pipeline.<br>\n[b] List every file as <code>dep<\/code> or <code>out<\/code>.<br>\n[c] Decouple stages from directory structure by having each stage depend on a \u201csummary\u201d or \u201csuccess\u201d file from previous stages instead of a subject\u2019s files or a stage directory.<br>\n[d] Introduce a \u201csummary\u201d stage which creates the desired directory structure view, e.g. via symlinks.<\/p>\n<p>Option [a] discards the desired directory structure, which is extremely undesirable since manual inspection of subjects\u2019 intermediate outputs is common and it is convenient to have all outputs collocated. Option [b] allows to maintain desired directory structure but is unwieldy and perhaps <a href=\"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609\">not performant<\/a>(?). Option [c] allows to maintain desired directory structure but requires error prone user implementation (e.g. the summary file must reflect the unique combination of files it summarizes, which somewhat reinvents what DVC already does for tracking a directory). Option [d] allows to maintain desired directory structure as a view of another structure DVC likes more, but produces outputs that should not be tracked.<\/p>\n<p>Are there perhaps other options possible? Thank you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Whole directory as input or output",
        "Question_creation_time":1569494799270,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/whole-directory-as-input-or-output\/232",
        "Question_upvote_count":2.0,
        "Question_view_count":1091.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have tried to list a whole directory as input or output to a stage but this doesn\u2019t seem to work as I would hope. I would like, ideally, to have the directory contents monitored for changes when reproducing the pipeline as the files within could change and are too many to list individually as inputs or outputs.<\/p>\n<p>Is this already possible (if so how?) or something that may be implemented in future? Or is there some other process to deal with this kind of situation? Am I trying to use the system in the wrong way?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Clone of repo with symlinked files creates copies not links",
        "Question_creation_time":1629913961956,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/clone-of-repo-with-symlinked-files-creates-copies-not-links\/855",
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I created a simple test repository with structure similar to this (there are actually a couple more levels of subfolders, but that does not change the issue):<br>\ndata\/<br>\nrepo\/<br>\n1.png             (actual image files)<br>\n2.png<br>\nworking\/<br>\n1.png             symlink to actual 1.png created using \u201cln -s \u2026\/repo\/1.png 1.png\u201d<br>\nDid a dvc add of everything in the data folder  \u201cdvc add data\u201d, then git add, etc.<br>\nPushed the dvc repository to a Google cloud remote, and pushed git to a remote.<br>\nFinally, create a fresh working folder, then:<br>\ngit clone   <br>\ndvc pull<br>\nThis DOES create the right folder structure but the working folder contains a full copy of 1.png, not a symlink to 1.png.<br>\nIs this expected?  Is there a way to have DVC cache and re-create a symlink rather than just follow it and end up with two copies of the file?<br>\nThanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR: unexpected error - [Errno 2] No such file or directory:",
        "Question_creation_time":1667696501930,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-errno-2-no-such-file-or-directory\/1381",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I ran into an <code>ERROR: unexpected error - [Errno 2] No such file or directory<\/code> from running <code>dvc run<\/code> on my  stage in my <code>dvc.yaml<\/code>. I am running into a similar error when I try to do <code>dvc repro<\/code>. I am not sure how to debug the error.<\/p>\n<details>\n<summary>\nFull stacktrace from `dvc run --force -vvv ...`<\/summary>\n<pre><code class=\"lang-auto\">$ dvc run --force -vvv -n split_state_and_fed -d data\/source -d split_state_and_fed.py -o data\/fed\/LabeledRegulations_Fed.csv -o data\/state\/LabeledRegulations_State.csv python split_state_and_fed.py split_state_and_fed --csv_path=data\/source --output_state=data\/state\/LabeledRegulations_State.csv --output_fed=data\/fed\/LabeledRegulations_Fed.csv\n2022-11-05 21:00:28,578 TRACE: Namespace(cprofile=False, yappi=False, yappi_separate_threads=False, viztracer=False, viztracer_depth=None, cprofile_dump=None, pdb=False, instrument=False, instrument_open=False, show_stack=False, quiet=0, verbose=3, version=None, cd='.', cmd='run', name='split_state_and_fed', file=None, single_stage=False, force=True, deps=['data\/source', 'split_state_and_fed.py'], params=[], outs=['data\/fed\/LabeledRegulations_Fed.csv', 'data\/state\/LabeledRegulations_State.csv'], outs_no_cache=[], checkpoints=[], external=False, outs_persist=[], outs_persist_no_cache=[], metrics=[], metrics_no_cache=[], plots=[], plots_no_cache=[], live=None, live_no_cache=None, live_no_summary=False, live_no_html=False, wdir=None, always_changed=False, desc=None, command=['python', 'split_state_and_fed.py', 'split_state_and_fed', '--csv_path=data\/source', '--output_state=data\/state\/LabeledRegulations_State.csv', '--output_fed=data\/fed\/LabeledRegulations_Fed.csv'], no_exec=False, no_commit=False, no_run_cache=False, func=&lt;class 'dvc.commands.run.CmdRun'&gt;, parser=DvcParser(prog='dvc', usage=None, description='Data Version Control', formatter_class=&lt;class 'argparse.RawTextHelpFormatter'&gt;, conflict_handler='error', add_help=False))\n2022-11-05 21:00:29,131 TRACE: params.yaml does not exist, it won't be used in parametrization\n2022-11-05 21:00:29,132 TRACE: Context during resolution of stage split_state_and_fed:\n{}\n2022-11-05 21:00:29,133 DEBUG: Lockfile for 'dvc.yaml' not found\n2022-11-05 21:00:29,139 TRACE:    41.75 ms in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\n2022-11-05 21:00:29,147 TRACE:     7.54 ms in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\data\n2022-11-05 21:00:29,150 TRACE:   958.90 mks in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\src\n2022-11-05 21:00:29,152 TRACE:    12.30 mks in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\src\\tracker_healthcare_qa\n2022-11-05 21:00:29,154 TRACE:    17.80 mks in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\tests\n2022-11-05 21:00:29,336 TRACE: params.yaml does not exist, it won't be used in parametrization\n2022-11-05 21:00:29,336 TRACE: Context during resolution of stage split_state_and_fed:\n{}\n2022-11-05 21:00:29,338 DEBUG: Lockfile for 'dvc.yaml' not found\n2022-11-05 21:00:29,348 DEBUG: Removing output 'data\\fed\\LabeledRegulations_Fed.csv' of stage: 'split_state_and_fed'.\n2022-11-05 21:00:29,349 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\data\\fed\\LabeledRegulations_Fed.csv'\n2022-11-05 21:00:29,350 DEBUG: Removing output 'data\\state\\LabeledRegulations_State.csv' of stage: 'split_state_and_fed'.\n2022-11-05 21:00:29,352 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\data\\state\\LabeledRegulations_State.csv'\n2022-11-05 21:00:29,371 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,378 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,379 DEBUG: {'data\\\\source': 'modified'}\n2022-11-05 21:00:29,387 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,398 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,424 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\nRunning stage 'split_state_and_fed':\n&gt; python split_state_and_fed.py split_state_and_fed --csv_path=data\/source --output_state=data\/state\/LabeledRegulations_State.csv --output_fed=data\/fed\/LabeledRegulations_Fed.csv\n2022-11-05 21:00:33,363 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:33,400 DEBUG: Computed stage: 'split_state_and_fed' md5: 'bfb841b6a7ea23cf95f75c8b2a8e7ea3'\n2022-11-05 21:00:33,406 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:33,412 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:33,440 ERROR: unexpected error - [Errno 2] No such file or directory: 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\e0d88e8d4dd7e19c58922c66af34a089d91cd70f419cbb28773bd22b72e5f967.hAv7gnwFPvcZY9cUZGJiHN'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 825, in move\n    os.rename(src, real_dst)\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\tmpiy81j0vt' -&gt; 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\e0d88e8d4dd7e19c58922c66af34a089d91cd70f419cbb28773bd22b72e5f967.hAv7gnwFPvcZY9cUZGJiHN'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\cli\\__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\cli\\command.py\", line 22, in do_run\n    return self.run()\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\commands\\run.py\", line 47, in run\n    self.repo.run(**kwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\repo\\__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\repo\\scm_context.py\", line 156, in run\n    return method(repo, *args, **kw)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\repo\\run.py\", line 33, in run\n    stage.run(no_commit=no_commit, run_cache=run_cache)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\funcy\\decorators.py\", line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\decorators.py\", line 43, in rwlocked\n    return call()\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\funcy\\decorators.py\", line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\__init__.py\", line 574, in run\n    self.save(allow_missing=allow_missing)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\__init__.py\", line 486, in save\n    self.repo.stage_cache.save(self)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\cache.py\", line 181, in save\n    self.repo.odb.local.move(tmp, path)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_data\\hashfile\\db\\local.py\", line 34, in move\n    super().move(from_info, to_info)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\db.py\", line 91, in move\n    self.fs.move(from_info, to_info)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\fs\\base.py\", line 371, in mv\n    self.fs.mv(from_info, to_info)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\fs\\local.py\", line 107, in mv\n    move(path1, path2)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\fs\\utils.py\", line 64, in move\n    shutil.move(src, tmp)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 845, in move\n    copy_function(src, real_dst)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 444, in copy2\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 266, in copyfile\n    with open(dst, 'wb') as fdst:\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\e0d88e8d4dd7e19c58922c66af34a089d91cd70f419cbb28773bd22b72e5f967.hAv7gnwFPvcZY9cUZGJiHN'\n------------------------------------------------------------\n2022-11-05 21:00:33,798 DEBUG: link type reflink is not available ([Errno 129] no more link types left to try out)\n2022-11-05 21:00:33,798 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\.69MpunVkddMa4TZdFpmKaC.tmp'\n2022-11-05 21:00:33,800 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\.69MpunVkddMa4TZdFpmKaC.tmp'\n2022-11-05 21:00:33,803 DEBUG: link type symlink is not available ([WinError 1314] A required privilege is not held by the client: 'C:\/Users\/bd122850\/OneDrive - The Bureau of National Affairs, Inc\/Documents\/projects\/reports\/tracker_healthcare_qa\/.dvc\/cache\/.ZBKsiaxpxhhyMox828VWLX.tmp' -&gt; 'C:\/Users\/bd122850\/OneDrive - The Bureau of National Affairs, Inc\/Documents\/projects\/reports\/.69MpunVkddMa4TZdFpmKaC.tmp')\n2022-11-05 21:00:33,803 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\.69MpunVkddMa4TZdFpmKaC.tmp'\n2022-11-05 21:00:33,804 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\.dvc\\cache\\.ZBKsiaxpxhhyMox828VWLX.tmp'\n2022-11-05 21:00:33,854 DEBUG: Version info for developers:\nDVC version: 2.33.2 (pip)\n---------------------------------\nPlatform: Python 3.9.13 on Windows-10-10.0.19043-SP0\nSubprojects:\n        dvc_data = 0.25.2\n        dvc_objects = 0.12.2\n        dvc_render = 0.0.12\n        dvc_task = 0.1.4\n        dvclive = 1.0\n        scmrepo = 0.1.3\nSupports:\n        http (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.3, aiohttp-retry = 2.8.3)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: local\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-11-05 21:00:33,868 DEBUG: Analytics is enabled.\n2022-11-05 21:00:34,127 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\bd122850\\\\AppData\\\\Local\\\\Temp\\\\tmpsxs91nti']'\n2022-11-05 21:00:34,157 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\bd122850\\\\AppData\\\\Local\\\\Temp\\\\tmpsxs91nti']'\n<\/code><\/pre>\n<\/details>\n<p>DVC doctor output:<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.33.2 (pip)\n---------------------------------\nPlatform: Python 3.9.13 on Windows-10-10.0.19043-SP0\nSubprojects:\n        dvc_data = 0.25.2\n        dvc_objects = 0.12.2\n        dvc_render = 0.0.12\n        dvc_task = 0.1.4\n        dvclive = 1.0\n        scmrepo = 0.1.3\nSupports:\n        http (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.3, aiohttp-retry = 2.8.3)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: local\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR: unexpected error - 'retries'",
        "Question_creation_time":1644477401580,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-retries\/1051",
        "Question_upvote_count":0.0,
        "Question_view_count":184.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, i run dvc from inside a docker container ,i call dvc  commands from python script , inside the container ,everything is configured ,repo ,remote storage and remote credentials, when i run the script without the container it works perfectly ,but when using the container it raise this error<br>\nwhen it comes to \u201cdvc push\u201d in the script \u201cERROR: unexpected error - \u2018retries\u2019\u201d ,the error obviously not informative ,so please need a help for that<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to deal with variants of the same model (not versions)",
        "Question_creation_time":1612819150725,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-deal-with-variants-of-the-same-model-not-versions\/659",
        "Question_upvote_count":0.0,
        "Question_view_count":281.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey there, I\u2019m a totally new user to dvc, so please keep that in mind when answering my question <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/grinning_face_with_smiling_eyes.png?v=9\" title=\":grinning_face_with_smiling_eyes:\" class=\"emoji\" alt=\":grinning_face_with_smiling_eyes:\"> . I have multiple variants of the same type of model (for example, let\u2019s say a network that has learned to do NLP for english sentences, and a separate network that has learned to do it for french sentences). In my example, as I plan to do language comprehension for more languages, more model variants will need to be added (let\u2019s say I plan to have 1 model variant for each of the hundreds\/thousands of languages\/dialects I want to handle in the future). Let\u2019s say I want to have a model versioning system like dvc to help with this (I am purely interested in the model versioning side of things for the moment, not data versioning). How would I go about this? As far as I can tell, dvc by default expects that 1 repo (i.e. a GCS directory) would be used to store the different versions of all the models. In my toy example, I would be interested in having a separate GCS directory for each language (i.e. all the model versions for english would get stored on 1 directory, all the model versions for french would get stored in another directory). Is there an easy way to do this?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Train using SageMaker but track output using DVC",
        "Question_creation_time":1623143837370,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/train-using-sagemaker-but-track-output-using-dvc\/782",
        "Question_upvote_count":2.0,
        "Question_view_count":939.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I wonder what is the best way to use Sagemaker with DVC in particular for running a train step which is part of a DVC pipeline. The problem I am running into is that SageMaker will create a training job and output somewhere different than the machine that invokes the training job.<\/p>\n<p>Prior to using SageMaker our flow was to ssh into the relevant EC2 instance and run dvc repro. Now we have a command that invokes the same train job from our local machine which essentially kicks out train in an EC2 instance of our choice and stores the output in a s3 bucket.<\/p>\n<p>Is there a way to run the training job, sync the output locally and inform DVC that this is the output of running a particular step in the pipeline? If that is complicated, is there a better alternative to working with SageMaker and DVC?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wildcards in stage deps?",
        "Question_creation_time":1649271460010,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/wildcards-in-stage-deps\/1151",
        "Question_upvote_count":0.0,
        "Question_view_count":121.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is it possible to specify dependancies for a stage using wildcard characters (aka glob)? I would like to be able to write something like<\/p>\n<pre><code class=\"lang-auto\">stages:\n  compare:\n    cmd: python src\/compare_data.py\n    deps: \n    - src\/compare_data.py\n    - data\/parameter_*.csv\n<\/code><\/pre>\n<p>In this case, the <code>parameter_001.csv<\/code> etc files came from a previous stage that includes a <code>foreach<\/code> loop that created the <code>csv<\/code> files.<\/p>\n<p>When I tried the above, I get the error <code>dependency 'data\/parameter_*.csv' does not exist<\/code>, even though the file (with <code>*<\/code> replaced by <code>001<\/code>) does actually exist.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Direct copy between Shared Cache and External Dependencies\/Outputs",
        "Question_creation_time":1599692610587,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/direct-copy-between-shared-cache-and-external-dependencies-outputs\/487",
        "Question_upvote_count":13.0,
        "Question_view_count":1118.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m trying to get my head around external dependencies, outputs and cache. It\u2019s not quite clear if I can achieve what I need with DVC, even though it seems to be very close. Here is what I\u2019m trying to get.<\/p>\n<p>I need to implement <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" rel=\"nofollow noopener\">Shared Development Server<\/a> scenario while optimizing download\/upload operations between the cache and external datasets. Here is why and how:<\/p>\n<ol>\n<li>I have a fast cloud storage mounted to a Kubernetes cluster via NFS or SMB, and I want to use it as Shared Local Cache. Think about mounted AWS S3 bucket or Azure File Share.<\/li>\n<li>I have a large dataset that is stored in <strong>the same or another<\/strong> bucket\/account, or potentially elsewhere, but always with an ability to copy files to\/from my cache storage many times faster than if I was copying them via the machine where the DVC command runs.<\/li>\n<li>When I execute <strong>dvc run<\/strong>, the command can initiate <strong>direct copy<\/strong> between the cache storage bucket\/share and the dataset bucket\/account.<br>\nBasically, what I want is the commands from these examples to run in the context of the shared cache folder, then dvc to create a hard\/sym\/ref-link to file in the shared cache, so the actual dataset bytes never make it to cluster until the pipeline code actually reads files. In the example with AWS S3, it would be similar to running<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">aws s3 cp s3:\/\/my_dataset_bucket\/data.txt s3:\/\/my_cachhe_bucket\/cache_path\/data.txt\n<\/code><\/pre>\n<p>Let\u2019s assume the command can actually recognize that the download destination is in the mounted cloud storage, not a usual directory. Same for the opposite direction.<br>\nThe way it looks to me now,  this command<\/p>\n<pre><code class=\"lang-auto\">aws s3 cp s3:\/\/mybucket\/data.txt data.txt\n<\/code><\/pre>\n<p>downloads data.txt somewhere locally (where?), then moves that to the cache, and make a link in the workspace.<br>\nThank you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Import-url on local network",
        "Question_creation_time":1621351703852,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/import-url-on-local-network\/749",
        "Question_upvote_count":4.0,
        "Question_view_count":212.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI have a question regarding the import-url function of dvc. I have windows machines on a local network and want to import  (logfiles) files from one machine to another. I have the windows path, password and ip address and I am already able to <code>robocopy<\/code> the files I want. Is it now possible to import those files?<br>\nThe copy command that works is:<br>\n<code>robocopy \\\\{ipaddress}\\c$\\data\\file.txt C:\\Path\\file.txt<\/code><\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using DVC for non-machine learning models",
        "Question_creation_time":1601604768780,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-for-non-machine-learning-models\/519",
        "Question_upvote_count":1.0,
        "Question_view_count":421.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I work with simulation models requiring large input datasets and producing similarly large outputs. I\u2019m trying to see if DVC would be an appropriate tool for version control of these datasets. My use case is somewhat different that that of a machine learning developer, I think.<\/p>\n<p>An example would be a global climate model that can be run for different future scenarios of greenhouse gas emissions, or for different regions in the world. Each scenario\/region is associated with a specific (set of) input file(s). The names of file or directories containing the data for each scenario may be different.<\/p>\n<p>These input datasets are not (necessarily) tied to specific versions of the code. In principle one could make a Git branch for each scenario, but that would be hassle, and would potentially result in a great many branches. So what I\u2019m looking for is the ability to checkout a specific dataset, without checking out all the datasets for all scenarios, and without having to switch to different Git branch or commit.<\/p>\n<p>Obviously, adding new datasets representing new scenarios would require a Git commit\u2013that\u2019s OK.<\/p>\n<p>I\u2019ve been trying DVC and looking at the documentation but so far I haven\u2019t figured out if this is possible and if so, how do do this.<\/p>\n<p>Thanks in advance,<\/p>\n<p>Maarten<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dataset-level access control for data registry",
        "Question_creation_time":1617550888575,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dataset-level-access-control-for-data-registry\/712",
        "Question_upvote_count":2.0,
        "Question_view_count":252.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello!<br>\nI would like to organize data registry with multiple datasets and multiple users (data scientists from different teams). Since some datasets contain sensitive information, it is important to have access control, both read and write. It would be great to have as flexible setup as possible. In the simplest case there could be just public\/private datasets, but RBAC\/ABAC or the possibility to provide dataset access to specific user directly (create many-to-many access control record) would be much better.<\/p>\n<p>DVC does not provide such features, but there are some workarounds, e.g <a href=\"https:\/\/discuss.dvc.org\/t\/restrict-access-to-dvc-repo\/553\">here<\/a> and <a href=\"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174\">there<\/a>. As far as I understood one could create several remotes and to simulate different groups\/roles and provide access to different users respectively. For me it seems to be pretty easy to break involuntary such workaround: e.g. having two remote \u2018public\u2019 and \u2018private\u2019 someone with appropriate access can accidentally push \u2018private\u2019 dataset to \u2018public\u2019 remote, making it available for anybody with \u2018public\u2019 remote access.<br>\nThere could also be two separate repos (or dvc registries) with public and private, it would be less probable to violate policy accidentally, but creation of e.g. 4 or 5 independent access groups (\u2018private_for_project_A\u2019, \u2018private_for_project_B\u2019) would be messy.<\/p>\n<p>I would be very thankful if anybody could share ideas how to organize more reliable\/convenient access control.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Creating an aggregate .dvc file",
        "Question_creation_time":1536703395229,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/creating-an-aggregate-dvc-file\/93",
        "Question_upvote_count":1.0,
        "Question_view_count":2315.0,
        "Question_answer_count":11,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have the following use case:<\/p>\n<ul>\n<li>Launch a script which loops over multiple folders  and does some processing (each folder takes quite a bit of CPU time)<\/li>\n<li>Each folder leads to the creation of a new folder, containing the output of the processing<\/li>\n<li>Once the loop is complete, a single .dvc file is created<\/li>\n<\/ul>\n<p>This works fine, but if the processing crashes mid-way, <code>dvc repro<\/code> will start again from scratch.<br>\nOne solution would be to create a <code>.dvc<\/code> file for each folder, but then, repro is a bit tedious to run as I would need to loop over all the <code>.dvc<\/code> file.<\/p>\n<p>Is there a better way to go ? If not, would it be possible to define multiple layers of dvc files (e.g. run subtasks, create multiple <code>.dvc<\/code>, once subtask is complete, create a master <code>.dvc<\/code> which will allow to easily repro all the subtasks) ?<\/p>\n<p>Thanks in advance !<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Help with dvc add and conda? ERROR: unexpected error - escape character followed by unknown character",
        "Question_creation_time":1658281615328,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/help-with-dvc-add-and-conda-error-unexpected-error-escape-character-followed-by-unknown-character\/1259",
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>dvc had been working fine for awhile. I created a new branch and at the same time created a new conda enviornment to work in. I was having issues with module not found error with scikit-learn in VSC. After trying to add my data to dvc tracking I was met with this error. Could creating new conda environment cause this issue?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/54eed7e1fd3d2e67164c6642bb380e44d5dadd70.png\" data-download-href=\"\/uploads\/short-url\/c7lPgE2biebyj3m8dhpf5gNW3qo.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/54eed7e1fd3d2e67164c6642bb380e44d5dadd70.png\" alt=\"image\" data-base62-sha1=\"c7lPgE2biebyj3m8dhpf5gNW3qo\" width=\"690\" height=\"46\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/54eed7e1fd3d2e67164c6642bb380e44d5dadd70_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1219\u00d783 5.43 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Linking multiple cloud accounts",
        "Question_creation_time":1530804515195,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/linking-multiple-cloud-accounts\/51",
        "Question_upvote_count":1.0,
        "Question_view_count":425.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I would like to put some of my data in different folders in aws, is there any way to link multiple projects\/ urls and then decide which one I wan to deposit it too ?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to avoid data duplication between cache and workspace",
        "Question_creation_time":1663141540288,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-avoid-data-duplication-between-cache-and-workspace\/1340",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am using DVC on macOS. According to the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noopener nofollow ugc\">documentation<\/a>, DVC won\u2019t save the same file twice (one in the workspace and one in the cache).<\/p>\n<p><code>In order to have the files present in both directories without duplication, DVC can automatically create **file links** to the cached data in the workspace. In fact, by default it will attempt to use reflinks* if supported by the file system.<\/code><\/p>\n<p>However, for me, it keeps two copies of the same file, thereby increasing the disk space significantly. For example, if I have a file of around 5GB, the disk space for that project goes to around 10GB. Here is the output of the <code>dvc version<\/code> command. According to the docs, the reflinks are supported by macOS and can be seen from dvc version output then why isn\u2019t it working in my case? Any ideas on what I am missing here?<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.18.0 (brew)\n---------------------------------\nPlatform: Python 3.10.6 on macOS-12.5.1-x86_64-i386-64bit\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.14.0),\n        gs (gcsfs = 2022.7.1),\n        webhdfs (fsspec = 2022.7.1),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        s3 (s3fs = 2022.7.1, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk1s5s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk1s5s1\nRepo: dvc (subdir), git\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error on trying to run dvc pipeline",
        "Question_creation_time":1638161185366,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-on-trying-to-run-dvc-pipeline\/989",
        "Question_upvote_count":1.0,
        "Question_view_count":292.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m trying to run the training stage of a machine learning pipeline on dvc with the following command(s):<\/p>\n<pre><code class=\"lang-auto\">dvc run -n train \\ \n-d data\/census_clean.csv -d starter\/train_model.py \\ \n-o model\/model.joblib -o model\/encoder.joblib -o model\/lb.joblib \\\npython starter\/train_model.py\n\n<\/code><\/pre>\n<p>But this throws up the following error everytime and the run fails to start.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/06e6383b7e3e4f59a41f78bd245925f207064081.png\" alt=\"Screenshot 2021-11-29 at 10.13.50 AM\" data-base62-sha1=\"Z26HMFZc92veBXN62ytKqjdtTP\" width=\"474\" height=\"142\"><\/p>\n<p>I\u2019m not sure where the mistake on my part is - the same error repeats if I try to run without the output commands, and not specifying the params since hyerparameter tuning is not required at this stage. Please let me know what I\u2019m missing.<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Define pipelines in multiple files",
        "Question_creation_time":1602014302887,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/define-pipelines-in-multiple-files\/524",
        "Question_upvote_count":0.0,
        "Question_view_count":363.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a repo with a bunch of related pipelines (sharing stages, in some cases).  The <code>dvc.yaml<\/code> file is getting very long.  Is there any way to split the pipelines into multiple <code>yaml<\/code> files?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC list shows libssl-so.1.1.1k not found",
        "Question_creation_time":1668460404483,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-list-shows-libssl-so-1-1-1k-not-found\/1387",
        "Question_upvote_count":0.0,
        "Question_view_count":17.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there!<br>\nI have been trying to use the DVC list and DVC get to get files from a GitHub URL, but, it shows \u201cERROR: unexpected error - libssl-9ad06800.so.1.1.1k: cannot open shared object file: No such file or directory\u201d<\/p>\n<p>I have tried \u201clocate libssl.so.1.1.1k\u201d, it returns file present.<\/p>\n<h2>\n<a name=\"dvc-version-2332-pip-1\" class=\"anchor\" href=\"#dvc-version-2332-pip-1\"><\/a>DVC version: 2.33.2 (pip)<\/h2>\n<p>Platform: Python 3.9.13 on Linux-4.18.0-348.2.1.el8_5.x86_64-x86_64-with-glibc2.28<br>\nSubprojects:<br>\ndvc_data = 0.25.2<br>\ndvc_objects = 0.12.2<br>\ndvc_render = 0.0.12<br>\ndvc_task = 0.1.4<br>\ndvclive = 1.0<br>\nscmrepo = 0.1.3<br>\nSupports:<br>\nazure (adlfs = 2022.10.0, knack = 0.10.0, azure-identity = 1.12.0),<br>\ngdrive (pydrive2 = 1.14.0),<br>\ngs (gcsfs = 2022.10.0),<br>\nhdfs (fsspec = 2022.10.0, pyarrow = 10.0.0),<br>\nhttp (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\nhttps (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\noss (ossfs = 2021.8.0),<br>\ns3 (s3fs = 2022.10.0, boto3 = 1.24.59),<br>\nssh (sshfs = 2022.6.0),<br>\nwebdav (webdav4 = 0.9.8),<br>\nwebdavs (webdav4 = 0.9.8),<br>\nwebhdfs (fsspec = 2022.10.0)<br>\nCache types: hardlink, symlink<br>\nCache directory: xfs on \/dev\/mapper\/centos-home<br>\nCaches: local<br>\nRemotes: local<br>\nWorkspace directory: xfs on \/dev\/mapper\/centos-home<br>\nRepo: dvc, git<\/p>\n<p>Also, \u201clink type reflink is not available ([Errno 95] no more link types left to try out)\u201d<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Loading DVC Data in AWS Sagemaker",
        "Question_creation_time":1614020374734,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/loading-dvc-data-in-aws-sagemaker\/684",
        "Question_upvote_count":5.0,
        "Question_view_count":1408.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Greetings!<\/p>\n<p>I\u2019ve been working on a project on my local computer and versioning my data via DVC to an S3 bucket.<\/p>\n<p>Now my project is too big and I need to take advantage of the nearly unlimited seeming RAM available on AWS Sagemaker.<\/p>\n<p>Also, I\u2019ve linked my Gitlab to my notebook instances in Sagemaker so I can see the folder containing the dvc files which I\u2019ve pushed to my Gitlab.<\/p>\n<p>However, everything I see on Sagemaker tutorials says I need to point to the actual file in S3 (see here:  <a href=\"https:\/\/stackoverflow.com\/a\/56060184\/4691538\" rel=\"noopener nofollow ugc\">https:\/\/stackoverflow.com\/a\/56060184\/4691538<\/a>).<\/p>\n<p>To that end, can someone tell me how to load a file into Sagemaker which is under data versioning control using DVC and stored in S3? Cheers!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Push same files in different branches",
        "Question_creation_time":1633590580855,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/push-same-files-in-different-branches\/914",
        "Question_upvote_count":0.0,
        "Question_view_count":206.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m migrating from using LFS to DVC.<br>\nNow I\u2019m totally get rid of LFS legacy and want to upload big files to DVC.<br>\nHelp me figure out whether the same files from different branches will be stored in the repository in a single copy or not? Is there a way to add same files to all branches at once<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Large Data Registry on NAS with multiple DVC and non-DVC users",
        "Question_creation_time":1660121162723,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/large-data-registry-on-nas-with-multiple-dvc-and-non-dvc-users\/1294",
        "Question_upvote_count":8.0,
        "Question_view_count":165.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>[I posted this question in the <span class=\"hashtag\">#dvc<\/span> Discord channel, and <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> suggested that other users can benefit from the discussion, so I\u2019m copying it here]<\/p>\n<p>Some time ago I asked in the forum about using DVC to manage our data on a NAS drive (<a href=\"https:\/\/discuss.dvc.org\/t\/single-cache-or-multiple-caches-in-nas-with-external-data\/1136\" class=\"inline-onebox\">Single cache or multiple caches in NAS with External Data<\/a>). I got useful advice from <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> , thanks, who suggested setting up a Data Registry. I did, but we are having trouble with users getting permission errors to add new datasets.<\/p>\n<p>The problem is that we have some constraints that go a bit against how DVC seems to be designed for. Namely, we have a folder <code>datasets<\/code> on the NAS with large datasets, both in size and number of files. Multiple users need to be able to save directories\/files to this folder, and then add\/commit\/push them with DVC. We cannot have data duplication either, and the directories\/files in folder <code>datasets<\/code> need to remain there, for non-DVC users to read them.<\/p>\n<p>So, our current solution is that the NAS is mounted on a server at <code>\/nas\/<\/code>, and looks like this:<\/p>\n<pre><code class=\"lang-auto\">nas\n  \u251c\u2500\u2500  dvc_cache\n  \u2514\u2500\u2500  datasets\n           \u251c\u2500\u2500 .dvc\n           \u251c\u2500\u2500 .git\n           \u251c\u2500\u2500 dataset_1\n           \u2514\u2500\u2500 dataset_2\n<\/code><\/pre>\n<p>with a configuration file (hardlinks\/symlinks to avoid data duplication)<\/p>\n<blockquote>\n<p>[cache]<br>\ndir = \/nas\/dvc_cache\/<br>\nshared = group<br>\ntype = \u201chardlink,symlink\u201d<br>\n[core]<br>\nautostage = true<\/p>\n<\/blockquote>\n<p>But an immediate problem is that if user A does <code>dvc add dataset_1<\/code>, and user B does <code>dvc add dataset_2<\/code>, then user A can <code>git commit -a<\/code> and <code>git push<\/code> and commit\/push both datasets.<\/p>\n<p>Another issue we are having is that it looks like if <code>user_A<\/code> adds a dataset, the directory created in the cache, e.g. <code>\/nas\/dvc_cache\/e0<\/code>, instead of having owner <code>user_A<\/code> and group <code>everybody<\/code>, it has both owner and group <code>user_A<\/code>, which blocks for anybody else any dvc operations that involve <code>\/nas\/dvc_cache\/e0<\/code>.<\/p>\n<p>My feeling is that each user should have their clone of the Data Registry on <code>\/nas<\/code>, but I\u2019m not sure this works with the Data Registry idea. Suggestions would be very welcome, thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get older version of data files",
        "Question_creation_time":1522591456249,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/get-older-version-of-data-files\/18",
        "Question_upvote_count":1.0,
        "Question_view_count":2245.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. How do i revert back to an older version of data files using dvc cli?<\/p>\n<p>Thanks.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cache duplication for external dataset",
        "Question_creation_time":1646637633567,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cache-duplication-for-external-dataset\/1100",
        "Question_upvote_count":0.0,
        "Question_view_count":190.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I have questions about the DVC cache duplication when I have an external dataset.<\/p>\n<p>There are two cases I\u2019ve tried to figure out according to the location of cache, dataset:<\/p>\n<p>(Assume there are two datasets to version control, i.e., dataset 1 &amp; dataset 2. The size of dataset 1 = 10M, the size of dataset 2 = 10M, and there is 5M duplication between dataset 1 &amp; dataset 2.)<\/p>\n<p><strong>[Case 1]<\/strong> Cache and both datasets are in the same storage as the DVC workspace.<br>\nIn this case, after I dvc add dataset1 and then dvc add dataset1 + dataset2, the size of the cache dir is 15M(10M + 5M). It seems ok.<\/p>\n<p><strong>[Case 2]<\/strong> Datasets are in the external storage with respect to the DVC workspace. (Cache is in or out of the storage where the DVC workspace is.)<br>\nIn this case, after I dvc add datset1 and then dvc add dataset1 + dataset2, the size of the cache dir is 25M.(10M + 15M) So, the feature DVC provides for eliminating the cache duplication doesn\u2019t work fine for external dataset.<\/p>\n<p>I checked the size of the folders using du -hs in my Ubuntu environment.<br>\nCan I use that DVC feature to eliminate cache duplication when I have an external dataset?<\/p>\n<p>Thank you.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using DVC for end-to-end pipeline",
        "Question_creation_time":1545748275249,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-for-end-to-end-pipeline\/111",
        "Question_upvote_count":7.0,
        "Question_view_count":1100.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>HI,<\/p>\n<p>I am trying to create an end-to-end pipeline ( I am new to it \u2026 ), and trying to use dvc as well.<\/p>\n<p>I have create 5 script corresponding to each phase<\/p>\n<ol>\n<li>preprocessing<\/li>\n<li>1st Feature engineering<\/li>\n<li>train-test data splite<\/li>\n<li>2nd Feature engineering<\/li>\n<li>Model training<br>\nI could do 'dvc run bla bla bla \u2019 for each program file. and I could reproduce the result.<\/li>\n<\/ol>\n<p>However if I would like to re-run the \u201cWHOLE\u201d pipeline after I have twitched something, is there any \u201cbest practice\u201d or recommendation or guide line  (e.g. use shell script to wrap up all \u2018dvc run\u2019 commands and run it ?  use dvc run -d xx.data -d yy.data \u2026 -d fe1.py -d fe2.py -d main.py \u2026, to run a very bulky command to make a dvc file ?  ) ?<\/p>\n<p>and. \u2026 do creating separate script for each phase a good idea to manage the pipeline ?<\/p>\n<p>Looking forward for answer ! Many thanks !<\/p>\n<p>Best Regards,<br>\nSolomon Leung<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR file not owned by user",
        "Question_creation_time":1606494207206,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-file-not-owned-by-user\/564",
        "Question_upvote_count":2.0,
        "Question_view_count":534.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>On <code>dvc add --external \/mnt\/my_storage<\/code> I am getting <code>ERROR: file '..\/..\/..\/..\/mnt\/my_storage\/data.csv' not owned by user!<\/code><\/p>\n<p>I have machine_1 with a project I am working under git control and dvc initialized.<br>\nMachine_2 is a remote file server on the same network mounted to \/mnt\/my_storage\/ via samba. I\u2019d like to keep the training data on \/mnt\/my_storage, but track it in DVC.<\/p>\n<p>Expected behaviour:<br>\ndvc hashes all files in \/mnt\/my_storage and creats a my_storage.dvc in my project folder.<\/p>\n<p>Actual behaviour:<br>\nFiles in one subdirectory are hashed and in another I am getting above mentioned error.<\/p>\n<p>The files on machine_2 are owned by a different user, but I have write access to \/mnt\/my_storage on file system level.<\/p>\n<p>What is this error telling me?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple dvc.yaml files",
        "Question_creation_time":1662306863720,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-dvc-yaml-files\/1321",
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, is it possible to track 2 or more projects in one git\/dvc repo? For example having 2 separate folders for 2 different neural net models. Like that:<br>\nDVC_repo\/PyTorch_model and DVC_repo\/TensorFlow_model where DVC_repo is the root directory. Is it possible to have 2 dvc.files in each folder and run them one at a time but without needing to <strong>dvc destroy<\/strong> when switching between them<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"redirect() function Flask",
        "Question_creation_time":1631178157000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4406572640916-redirect-function-Flask",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello, Domino Community.\n\nWe have a URL from our Domino deployment, and we I try to use the \"redirect()\" function from flask apps to redirect the users to other page after the\n\nWe need to use the \"redirect()\" function from Flask after submitting a form in our web app, but the function not work because we use the URL from our Domino deployment. Is there a way to know the url we are working on the project to pass manually to the function?\n\nThank you.",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where can I find API documentation and examples?",
        "Question_creation_time":1620854801000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/360078036672-Where-can-I-find-API-documentation-and-examples-",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Where can I find help with using the Domino API to interact with Domino programmatically?",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Overview of the resumable workspaces",
        "Question_creation_time":1624935390000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4402871565076-Overview-of-the-resumable-workspaces-",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Domino 4.4 introduced Resumable Workspaces. In our environment the maximum number of workspaces per user is limited to 8. When this limit is reached, I have to delete one of my workspaces first, so that I can create a new one.\u00a0\n\nA user asked the following question in our legacy community system:\n\nI would have expected to see, for example, in the project overview or in the Control Center in which projects my 8 user workspaces are, so that I can specifically archive workspaces that are no longer needed. So I have to go through all my projects and manually check where workspaces exist.\u00a0\n\nDid I miss something? Is there a place where I as a Data Scientist can see in which projects I currently have how many workspaces?\n\nAnswering below.\u00a0\n\nThank you very much for your feedback.",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Domino API - Dataset snapshot details",
        "Question_creation_time":1626330700000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4403644052244-Domino-API-Dataset-snapshot-details",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello Everyone,\n\nI would like if there is a Domino API to query dataset snapshot size. If not what are the other ways we could get this information?\u00a0\n\nThank you",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access RStudio from Jupyterlab",
        "Question_creation_time":1643466307000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4417672395924-Access-RStudio-from-Jupyterlab",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\nis it possible to launch RStudio from a Jupyterlab tile just like VS Code is in this tutorial? https:\/\/docs.dominodatalab.com\/en\/latest\/reference\/workspaces\/Accessing_multiple_hosted_applications_in_one_Workspace_session.html#\n\n\n\nthank you",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Providing Practitioner license to user [Reposted]",
        "Question_creation_time":1624934845000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4402871497108-Providing-Practitioner-license-to-user-Reposted-",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"A user posted the following question on our legacy community:\u00a0\n\nHello,\n\nI would like to know if it is possible to give a practitioner license to the user using Domino REST API in Domino V4.2 or higher.\n\nAnswering here since we are retiring the legacy community this evening.\u00a0\u00a0\n\nAnswer: Granting license types is not officially supported via REST API.\u00a0 Our engineering team recommends configuring role synchronization via SAML attributes passed to Keycloak: https:\/\/admin.dominodatalab.com\/en\/latest\/keycloak.html?highlight=role%20synchronization#role-synchronization If this is not a workable solution for your team, we recommend submitting a support ticket or asking your CSM\/account team for a solution that fits your needs.\u00a0 Please let us know if there's anything else you need, thanks!",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Workflows in Domino",
        "Question_creation_time":1647294919000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4785089958932-Workflows-in-Domino",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\nI am interested in using workflow management frameworks within Domino, including Nextflow and Snakemake. Both frameworks support Kubernetes, so I expect this is possible. I\u2019m looking for something like the Apache Airflow with Domino documentation:\n\nhttps:\/\/docs.dominodatalab.com\/en\/4.6\/reference\/runs\/advanced\/Using_Apache_Airflow_with_Domino.html\n\nMy collaborators specifically use Nextflow and Snakemake, so I would like to avoid switching to a new framework.",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Dask with Domino",
        "Question_creation_time":1629403013000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4405449644308-Using-Dask-with-Domino",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Recently at Coiled, we've been working with the great folks at Domino and have\u00a0received some questions on behalf of Domino users about how to use Dask with Domino (which make a great combination!). In working with users over the last few months, we'd generally recommend one of the following options\/levels for using Dask within Domino, depending on your needs and specific use case:\n\n1. Use Dask (either with the default multithreaded\/multiprocessing scheduler, or with the distributed scheduler) within your Domino workspace, which will use as much of the cores\/RAM that are available within your workspace VM. You might also find some of the JupyterLab extensions for working with Dask to be useful.\n\n2. Use the distributed-compute-operator provided by Domino to provision a Dask cluster in Kubernetes. This is a great way to spin up a Dask cluster on the same Kubernetes cluster where you are already working with Domino workspaces and deployments, and it offers the possibility of requesting additional CPU\/RAM outside of the scope of your Domino workspace\n\n3. Create a Dask cluster with Coiled on your preferred cloud provider and drive your Dask computations from your existing Domino Workspaces or Jobs. Because Coiled is designed to make it easy to work with remote Dask clusters, you can easily ask for additional CPU\/RAM\/GPUs to get more compute power for ETL jobs, exploratory data analysis, or one of the many other use cases familiar to Dask users.\n\n# Create a Coiled cluster\nimport coiled\ncluster = coiled.Cluster(n_workers=10, name=\"dask-from-domino\")\n\n# Connect Dask to Coiled cluster\nfrom dask.distributed import Client\nclient = Client(cluster)\nprint('Dashboard:', client.dashboard_link)\n\n# Use\u00a0Dask\u00a0as\u00a0usual!\nimport dask.dataframe as dd\ndf = dd.read_csv('s3:\/\/...\/data.csv')\ndf.groupby(df.account_id).balance.sum()\n\nLet us know how you're using Dask in Domino and any followup questions that you might have!",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unrestricted Model API returns OAuth error",
        "Question_creation_time":1628786821000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4405090153364-Unrestricted-Model-API-returns-OAuth-error",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Client: Bayer Crop Science\/Science at Scale\n\nIssue: Our process calls a Model API (R) with unrestricted access using CURL. When we manually run it, we observe no issue; however, automation returns below error since July 14, 2021.\n\nMissing OAuth provider. The service may not have been assigned a provider.\n\nWhat condition in an unrestricted Model API can give this error?",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Version of JuypterLab",
        "Question_creation_time":1624935921000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4402865086740-Version-of-JuypterLab",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"We received the following question from a user in our legacy community:\n\nHi All,\n\nIs it possible to install the latest version of JuypterLab on Domino?\u00a0 I'm trying to install the\u00a0@kiteco\/jupyterlab-kite\u00a0extension but I'm getting the following message:\n\nError: 500 ()\n\nI'd thought installing a newer version of JuypterLab might help.\n\nAnswering below, as our legacy community is being retired this evening.",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"sparklyR and Domino",
        "Question_creation_time":1624936558000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4402871661332-sparklyR-and-Domino",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"A user asked the following question on our legacy community:\u00a0\n\nHi All, Has anyone tried running sparklyR on Domino?\u00a0\n\nAnswering below.",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SQL database",
        "Question_creation_time":1647827536000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4947267182996-SQL-database",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi All,\n\nI was just wondering if it is possible to host a SQL database on Domino rather than setting up a connection to an externally hosted database?",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"FAQ: How should I upload my very large datasets into Domino?",
        "Question_creation_time":1617735708000,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/360077537531-FAQ-How-should-I-upload-my-very-large-datasets-into-Domino-",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"When working with our customers, I often receive questions about how to upload large datasets into Domino.\u00a0 There are a few ways to do this, and it depends on what your desired destination and usage of the data will be, but one popular approach is to use the Domino CLI to upload data to a Dataset in one of your Domino projects.",
        "Tool":"Domino",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dask scheduler cannot find python library",
        "Question_creation_time":1653513089021,
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-cannot-find-python-library\/880",
        "Question_upvote_count":0.0,
        "Question_view_count":85.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When I use <code>guild run dask:scheduler<\/code> with staged guild runs, those staged guild runs cannot find one of my Python libraries. The guild runs work if I run them outside of the dask scheduler.<\/p>\n<p>This particular Python library is not part of my Python virtual environment but is discovered by Python through my PYTHONPATH or PATH.<\/p>\n<p>The terminal I am running the dask scheduler in has the same environment variables.<\/p>\n<p>Any ideas?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Debugging and profiling guild",
        "Question_creation_time":1607965561395,
        "Question_link":"https:\/\/my.guild.ai\/t\/debugging-and-profiling-guild\/500",
        "Question_upvote_count":1.0,
        "Question_view_count":456.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I sometimes see <code>guild<\/code> taking a long time to start a run compared to just running the command that I get from <code>--print-cmd<\/code>. I realise this is because <code>guild<\/code> has to resolve dependencies etc., but I would like to understand if there is an easy way to debug and especially profile what steps \/ operations that is expensive in the <code>guild<\/code> command.<\/p>\n<p>I am aware of the <code>guild --debug<\/code> flag, but in my particular case it doesn\u2019t provide much info about what is taking a long time.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions",
        "Question_creation_time":1613603564104,
        "Question_link":"https:\/\/my.guild.ai\/t\/oserror-winerror-10013-an-attempt-was-made-to-access-a-socket-in-a-way-forbidden-by-its-access-permissions\/546",
        "Question_upvote_count":0.0,
        "Question_view_count":2846.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I get forbidden access even when I run with administrative privileges. I re-run in again and it usually works. How can I fix this  behavior?<\/p>\n<pre><code>(biobench-thMxqAli) \u03bb guild tensorboard 1\nPreparing runs for TensorBoard\nWARNING: Guild took 27.80 seconds to prepare runs. To reduce startup time, try running with '--skip-images' or '--skip-hparams' options or reduce the number of runs with filters. Try 'guild tensorboard --help' for filter options.\nWARNING: webfiles.zip static assets not found: c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\plugins\\webfiles.zip\n2021-02-17 15:04:42.417335: W tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n2021-02-17 15:04:42.417554: I tensorflow\/stream_executor\/cuda\/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\.virtualenvs\\biobench-thMxqAli\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 98, in _run_tensorboard\n    tensorboard.serve_forever(\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 636, in serve_forever\n    run_simple_server(app, host, port, ready_cb)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 602, in run_simple_server\n    server, _ = make_simple_server(tb_app, host, port)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 615, in make_simple_server\n    server = serving.make_server(host, port, app, threaded=True)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\werkzeug\\serving.py\", line 847, in make_server\n    return ThreadedWSGIServer(\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\werkzeug\\serving.py\", line 740, in __init__\n    HTTPServer.__init__(self, server_address, handler)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 452, in __init__\n    self.server_bind()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\http\\server.py\", line 138, in server_bind\n    socketserver.TCPServer.server_bind(self)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 466, in server_bind\n    self.socket.bind(self.server_address)\nOSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error while publishing runs",
        "Question_creation_time":1620146008305,
        "Question_link":"https:\/\/my.guild.ai\/t\/error-while-publishing-runs\/700",
        "Question_upvote_count":2.0,
        "Question_view_count":248.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to publish a particular run using the command <code>guild runs publish --dest &lt;path of destination&gt; &lt;run id&gt;<\/code>. However, I get the following error.<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"\/usr\/local\/bin\/guild\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/main_bootstrap.py\", line 40, in main\n    _main()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/commands\/runs_publish.py\", line 113, in publish_runs\n    runs_impl.publish(args, ctx)\nAttributeError: module 'guild.commands.runs_impl' has no attribute 'publish'\n<\/code><\/pre>\n<p>Any recommendations on how to solve this? I am using guild version 0.7.2.<\/p>\n<p>Thanks,<br>\nVishal<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild error - Not a git repository",
        "Question_creation_time":1665192152022,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-error-not-a-git-repository\/927",
        "Question_upvote_count":2.0,
        "Question_view_count":38.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>fatal: not a git repository (or any of the parent directories): .git<br>\nTraceback (most recent call last):<br>\nFile \u201c\/home\/sjoshi\/anaconda3\/envs\/clip\/.guild\/runs\/894a22b5465840858109bc4504a0295c\/.guild\/sourcecode\/simclr.py\u201d, line 46, in <br>\nargs.git_hash = subprocess.check_output([\u2018git\u2019, \u2018rev-parse\u2019, \u2018HEAD\u2019])<br>\nFile \u201c\/home\/sjoshi\/anaconda3\/envs\/clip\/lib\/python3.10\/subprocess.py\u201d, line 420, in check_output<br>\nreturn run(*popenargs, stdout=PIPE, timeout=timeout, check=True,<br>\nFile \u201c\/home\/sjoshi\/anaconda3\/envs\/clip\/lib\/python3.10\/subprocess.py\u201d, line 524, in run<br>\nraise CalledProcessError(retcode, process.args,<br>\nsubprocess.CalledProcessError: Command \u2018[\u2018git\u2019, \u2018rev-parse\u2019, \u2018HEAD\u2019]\u2019 returned non-zero exit status 128.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Loading custom event-file summaires",
        "Question_creation_time":1606949313280,
        "Question_link":"https:\/\/my.guild.ai\/t\/loading-custom-event-file-summaires\/489",
        "Question_upvote_count":0.0,
        "Question_view_count":217.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using tensorboardX to store event files to disk into a folder that\u2019s different for every run.<br>\nHow do I configure guild where to find these event files?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Specifying guild home through -H doesn't work",
        "Question_creation_time":1602276095982,
        "Question_link":"https:\/\/my.guild.ai\/t\/specifying-guild-home-through-h-doesnt-work\/369",
        "Question_upvote_count":0.0,
        "Question_view_count":376.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I want to compare runs in a different path by using the <code>-H<\/code> option. My new path is called <code>experiments<\/code>:<\/p>\n<pre><code>$ ls experiments\na26ffe4727584d359e16f2a28af5dfab  cache\n<\/code><\/pre>\n<p>Now If I do:<\/p>\n<pre><code>$ guild -H experiments runs\n<\/code><\/pre>\n<p>There is no output. Same with other commands. Am I using the command run?<\/p>\n<p>This is the output of <code>guild check<\/code>:<\/p>\n<pre><code>guild_version:             0.7.0\nguild_install_location:    \/mypath\/.training_venv\/lib\/python3.6\/site-packages\/guild\nguild_home:                \/mypath\/.training_venv\/.guild\nguild_resource_cache:      \/mypath\/.training_venv\/.guild\/cache\/resources\ninstalled_plugins:         cpu, disk, exec_script, gpu, keras, memory, perf, python_script, queue, skopt\npython_version:            3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0]\npython_exe:                \/mypath\/.training_venv\/bin\/python3\nplatform:                  Linux 5.4.0-48-generic x86_64\npsutil_version:            5.6.3\ntensorboard_version:       2.2.2\ncuda_version:              10.0.130\nnvidia_smi_version:        440.100\nlatest_guild_version:      0.7.0.post1\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom post-hoc analysis",
        "Question_creation_time":1603226512765,
        "Question_link":"https:\/\/my.guild.ai\/t\/custom-post-hoc-analysis\/425",
        "Question_upvote_count":1.0,
        "Question_view_count":234.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I would like to do some custom post-hoc analysis on various things I\u2019ve logged into tensorboard. I\u2019m doing bootstrapping and want to calculate mean and confidence interval. I know there\u2019s the <code>ipy<\/code> widget, and I\u2019ve gotten semi far with that as it gives you <code>avg_value<\/code> etc. But I\u2019d like to get access to all the values logged for, say, <code>precision<\/code> so that I can calculate the confidence intervals. I know this data is logged in tensorboard and tensorflow has some guides but they do not work with the version of tensorboard that comes  with guild. How do you advise I would do something like this?<\/p>\n<p>I  can log this separately, sure but it ends up being a pain and cluttering up all my logs. I cannot visualize them the way I\u2019d like in tensorboard, so I\u2019d rather do this analysis ad-hoc when comparing runs myself.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Timeout error",
        "Question_creation_time":1592656643969,
        "Question_link":"https:\/\/my.guild.ai\/t\/timeout-error\/196",
        "Question_upvote_count":1.0,
        "Question_view_count":688.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>I can\u2019t figure out why sometimes I get timeout error when I run experiments. Here is my running command:<\/p>\n<pre><code>guild run train tb_volatility_lookback=range[30:300:10]\n<\/code><\/pre>\n<p>it hangs after some time and when I click ctr + c to abort it the below error shows up.<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\runpy.py\u201d, line 193, in _run_module_as_main<br>\nINFO: [numexpr.utils] NumExpr defaulting to 8 threads.<br>\n\u201c<strong>main<\/strong>\u201d, mod_spec)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\runpy.py\u201d, line 85, in _run_code<br>\nexec(code, run_globals)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_main.py\u201d, line 38, in <br>\nmain()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_main.py\u201d, line 26, in main<br>\nbatch_util.handle_trials(batch_run, trials)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 54, in handle_trials<br>\n_run_trials(batch_run, trials)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 79, in _run_trials<br>\n_start_trial_run(run, stage)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 117, in _start_trial_run<br>\nrun_impl.run(restart=run.id, stage=stage)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1940, in run<br>\nmain(args)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1017, in main<br>\n_dispatch_op(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1101, in _dispatch_op<br>\n_dispatch_op_cmd(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1286, in _dispatch_op_cmd<br>\n_confirm_and_run(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1354, in _confirm_and_run<br>\n_run(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1544, in _run<br>\n_run_local(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1575, in _run_local<br>\n_run_op(op, S.args)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1683, in _run_op<br>\nextra_env=extra_env,<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 160, in run<br>\nexit_status = _run(run, op, quiet, stop_after, extra_env)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 195, in _run<br>\nexit_status = _op_wait_for_proc(op, proc, run, quiet, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 230, in _op_wait_for_proc<br>\nreturn _op_watch_proc(op, proc, run, quiet, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 238, in _op_watch_proc<br>\nreturn _proc_wait(proc, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 259, in <strong>exit<\/strong><br>\nself._output.wait_and_close()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 254, in wait_and_close<br>\nself.close()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 219, in close<br>\nlock = self._acquire_output_lock()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 232, in _acquire_output_lock<br>\nraise RuntimeError(\u201ctimeout\u201d)<br>\nRuntimeError: timeout<\/p>\n<\/blockquote>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging scalars when running ray[tune] tuning fails in a guild run",
        "Question_creation_time":1615255994962,
        "Question_link":"https:\/\/my.guild.ai\/t\/logging-scalars-when-running-ray-tune-tuning-fails-in-a-guild-run\/557",
        "Question_upvote_count":0.0,
        "Question_view_count":353.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In my project I have a bit of automatic tuning of my pytorch-lightning models using ray and then I also automatically apply the model. The logging that ray[tune] uses is a SummaryWriter from tensorboardX package. I am also using tensorboardX SummaryWriter for logging other things in my project. For my own logging, there is no issue with this, but for some reason guild fails with the calls to <code>add_scalar()<\/code> when it\u2019s called from the tune library.<\/p>\n<p>The trace:<\/p>\n<pre><code>3\/8\/2021 5:40:52 PM\nTraceback (most recent call last):\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 594, in _process_trial\n3\/8\/2021 5:40:52 PM\ndecision = self._process_trial_result(trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 666, in _process_trial_result\n3\/8\/2021 5:40:52 PM\nself._callbacks.on_trial_result(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/callback.py\", line 192, in on_trial_result\n3\/8\/2021 5:40:52 PM\ncallback.on_trial_result(**info)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 393, in on_trial_result\n3\/8\/2021 5:40:52 PM\nself.log_trial_result(iteration, trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 631, in log_trial_result\n3\/8\/2021 5:40:52 PM\nself._trial_writer[trial].add_scalar(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/guild\/python_util.py\", line 239, in wrapper\n3\/8\/2021 5:40:52 PM\ncb(wrapped_bound, *args, **kw)\n3\/8\/2021 5:40:52 PM\nTypeError: _handle_scalar() got an unexpected keyword argument 'global_step'\n<\/code><\/pre>\n<p>The line from tune in question in full is <code>self._trial_writer[trial].add_scalar(full_attr, value, global_step=step)<\/code>. This fails.<\/p>\n<p>In my own project I have the following line: <code> logger.add_scalar(f\"{prefix}\/{tag}\", scalar_value, global_step, walltime)<\/code> and this does not fail.<\/p>\n<p>So I went into the ray.tune library and I changed the call to <code>self._trial_writer[trial].add_scalar(full_attr, value, step)<\/code> and reran it. The failure went away.<\/p>\n<p>I dug into the <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/e9271824141583b96a6de7d1d5cebd44a04e43fe\/guild\/plugins\/summary_util.py#L181\" rel=\"noopener nofollow ugc\">github<\/a> source, and it looks like <code>_handle_scalar()<\/code> is expecting <code>step<\/code> and not <code>global_step<\/code>.<\/p>\n<p>I originally needed help with this but as I wrote this I ended up figuring out the answer. Looks like there\u2019s a potential bug here?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Date\/Datetime flags in YAML file are not recognized",
        "Question_creation_time":1642615429696,
        "Question_link":"https:\/\/my.guild.ai\/t\/date-datetime-flags-in-yaml-file-are-not-recognized\/798",
        "Question_upvote_count":0.0,
        "Question_view_count":107.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have set up a YAML file with the flags for my single operation. When I run the guild run command, all flags get imported except for date and datetime flags. Will I have to write them as strings and parse them into dates myself on my main script?<\/p>\n<p>Contents of my YAML file:<\/p>\n<pre><code>N_LEADS: 30\nWINDOW_RANGE: [-395, -1]\nLAST_EOM_TRAIN: 2019-12-31\nLEAD_RANGE: [-60, -50]\nN_ESTIMATORS: 2\nEOM_PRED_BENCH: 2021-07-31 00:00:00\nAS_OF_PRED_BENCH: 2021-06-01 00:00:00\n<\/code><\/pre>\n<p>What shows up when running the guild run command:<\/p>\n<pre><code>You are about to run main\n  LEAD_RANGE: -60 -50\n  N_ESTIMATORS: 2\n  N_LEADS: 30\n  WINDOW_RANGE: -395 -1\n<\/code><\/pre>\n<p>Thank you for your help!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use parameters with include flags",
        "Question_creation_time":1667442665957,
        "Question_link":"https:\/\/my.guild.ai\/t\/use-parameters-with-include-flags\/952",
        "Question_upvote_count":0.0,
        "Question_view_count":37.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m trying to use parameters to set portions of flags, but it seems that the parameters don\u2019t affect flags which have been imported. Here\u2019s a simple case:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">- config: config\n  flags:\n    flag-key: \"{{param-key}}\"\n\n- model: model\n  params:\n    param-key: param-value\n  operations:\n    op:\n      main: path\n      flags:\n        $include: config\n<\/code><\/pre>\n<p>This results in flag:<br>\nflag-key: \u2018{{param-key}}\u2019<br>\nwhereas I\u2019d want it to be  the same result as if I declared the flag directly in the operation config:<br>\nflag-key: param-value<\/p>\n<p>In my specific use-case I\u2019m also getting the parameters from a config which this model extends, in case that affects the issue. Essentially the setup is that we have all parameters in a single config for organization, and there\u2019s many models\/operations which each use one of a few different sets of flags, and all flag sets reference the parameters.  Any recommendations or workarounds would be appreciated, thanks!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where to look for error logs",
        "Question_creation_time":1601350696211,
        "Question_link":"https:\/\/my.guild.ai\/t\/where-to-look-for-error-logs\/357",
        "Question_upvote_count":0.0,
        "Question_view_count":886.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>After defining a guild operation, when I try running, I am getting the following error<br>\n<code>ERROR: [guild] trial &lt;RUNHASH&gt; exited with an error (see log for details)<\/code>.<\/p>\n<p>Presumably something went wrong in the run but I have no idea what. And I am not sure where to look for the log. If I open <code>guild view<\/code> and look for the log there, there is nothing there. Which might be expected becasue that\u2019s supposed to be the log of that the program outputs on the terminal. But then, I am still not sure where to look for the actual error log that the error message is talking about.<\/p>\n<p>How I can troubleshoot this?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"TensorBoard HPARAM not working",
        "Question_creation_time":1647539306680,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-hparam-not-working\/835",
        "Question_upvote_count":0.0,
        "Question_view_count":278.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to use the HPARAM tab in tensorboard together with guild<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild --debug tensorboard 1\n<\/code><\/pre>\n<p>When I navigate to the HPARAM tab this is what I get:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b.png\" data-download-href=\"\/uploads\/short-url\/hTCwjWGkWGpnber9a2bv5j8RbaX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_690x356.png\" alt=\"image\" data-base62-sha1=\"hTCwjWGkWGpnber9a2bv5j8RbaX\" width=\"690\" height=\"356\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_690x356.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_1035x534.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_1380x712.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2505\u00d71296 153 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The runs are not actually showing up. Any idea why this is happening? There are no errors in the debug output from guild.<\/p>\n<p><strong>Versions<\/strong><br>\nguildai==0.8.0rc1<br>\ntensorboard==2.8.0<br>\ntensorboard-data-server==0.6.1<br>\ntensorboard-plugin-wit==1.8.1<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Specifying different target types for the same operation",
        "Question_creation_time":1631302538330,
        "Question_link":"https:\/\/my.guild.ai\/t\/specifying-different-target-types-for-the-same-operation\/761",
        "Question_upvote_count":1.0,
        "Question_view_count":266.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to link and copy different files from the same operation resource in my guild stage.  It seems that specifying an operation multiple times only results in the last operation being used.  Here is an example that matches what I\u2019m trying to do where a operation train is used as a resource.  The file model.pth is linked while config.json is copied.<\/p>\n<pre><code>test:\n  exec: \"python test.py\"\n  requires:\n    - operation: train\n      select:\n        - file: model.pth\n          target-type: link\n        - file: config.json\n          target-type: copy\n<\/code><\/pre>\n<p>Is there any way to achieve this kind of functionality? Thanks<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hyperopt TPE vs Skopt Gaussian Processes",
        "Question_creation_time":1594425943757,
        "Question_link":"https:\/\/my.guild.ai\/t\/hyperopt-tpe-vs-skopt-gaussian-processes\/226",
        "Question_upvote_count":0.0,
        "Question_view_count":1015.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In developing the <a href=\"\/examples\/hyperopt\">Hyperopt example<\/a> I wanted to compare its performance to Scikit Optimize \u2014 specifically the gp optimizer.<\/p>\n<p>I ran 50 trials for each optimizer, minimizing loss for the <a href=\"\/start\">Get Started<\/a> mock training script.<\/p>\n<p>Scatterplot results for <code>tpe<\/code>:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8ecf41f9c817b7f95164b7b316e22df93557e59f.png\" alt=\"Screenshot from 2020-07-10 18-54-25\" data-base62-sha1=\"knlQBjfmWovvTNYOIf57tOMzfRB\" width=\"617\" height=\"483\"><\/p>\n<p>Scatterplot results for <code>gp<\/code>:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/d53d68bff805218b9b3d82c06020af617d5b30f8.png\" alt=\"Screenshot from 2020-07-10 18-53-32\" data-base62-sha1=\"uqpgIYD2oMdbIxugfwf1HAXQWdq\" width=\"627\" height=\"475\"><\/p>\n<p>Notice the concentration of trials around the minimum for <code>gp<\/code>. I would have expected this for <code>tpe<\/code>. I wonder if the example is implemented incorrectly.<\/p>\n<h3>Steps to reproduce<\/h3>\n<p>From the <a href=\"https:\/\/github.com\/guildai\/guildai\/tree\/master\/examples\/hyperopt\">example dir<\/a> generate 50 trials using <code>tpe<\/code>:<\/p>\n<pre><code class=\"lang-command\">guild run train -o tpe -m50 x=[-2.0:2.0] -t tpe\n<\/code><\/pre>\n<p>Next generate 50 trials using <code>gp<\/code>:<\/p>\n<pre><code class=\"lang-command\">guild run train -o gp -m50 x=[-2.0:2.0] -t gp\n<\/code><\/pre>\n<p>View the tpe trials in TensorBoard:<\/p>\n<pre><code class=\"lang-command\">guild tensorboard -l tpe\n<\/code><\/pre>\n<p>Click <strong>HPARAMS<\/strong> and then the scatterplot tab. Deselect all flags and metrics except x and loss.<\/p>\n<p>Do the same for the gp trials:<\/p>\n<pre><code class=\"lang-command\">guild tensorboard -l gp\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Nb-replace with boolean value",
        "Question_creation_time":1646940828346,
        "Question_link":"https:\/\/my.guild.ai\/t\/nb-replace-with-boolean-value\/830",
        "Question_upvote_count":1.0,
        "Question_view_count":119.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I want to use guild to run a notebook as an experiment. I also want to parameterize the notebook by using guilds search and replace option with notebooks.<\/p>\n<p>One of my parameters is a boolean value. How should I define the <code>nb-replace<\/code> to work with booleans?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Module 'yaml' has no attribute 'encode_val'",
        "Question_creation_time":1608144416901,
        "Question_link":"https:\/\/my.guild.ai\/t\/module-yaml-has-no-attribute-encode-val\/503",
        "Question_upvote_count":0.0,
        "Question_view_count":359.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I have just  installed guildai on a new server in a conda env (py 3.7) and get this error. I have tried reinstalling  PyYAML and pip.<br>\nLet me know if you have suggestions!<br>\nThanks, Shannon<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot open Tensorboard with Guild - unhashable type 'Dict'",
        "Question_creation_time":1615470431294,
        "Question_link":"https:\/\/my.guild.ai\/t\/cannot-open-tensorboard-with-guild-unhashable-type-dict\/563",
        "Question_upvote_count":4.0,
        "Question_view_count":543.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nFirst of all, great job, loving Guild so far.<\/p>\n<p>I have successfully created a queue, staged runs and completed them. I can run \u2018guild view\u2019 or \u2018tensorboard\u2019 standalone and see the logs.<\/p>\n<p>However, I cannot run \u2018guild tensorboard\u2019 or similary start tensorboard from guild view. In both cases I get an error with the following traceback:<\/p>\n<pre><code>(anomaly_detection) E:\\source\\repos\\anomaly_simulation\\Zoo&gt;guild -H E:\/source\/repos\/anomaly_simulation\/Results tensorboard\nPreparing runs for TensorBoard\nTraceback (most recent call last):\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\blepo\\Anaconda3\\envs\\anomaly_detection\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 94, in _run_tensorboard\n    monitor.run_once(exit_on_error=True)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\run_util.py\", line 80, in run_once\n    runs = self.list_runs_cb()\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\tensorboard.py\", line 119, in f\n    _ensure_hparam_experiment(runs, state)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\tensorboard.py\", line 134, in _ensure_hparam_experiment\n    hparams = _experiment_hparams(runs)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\tensorboard.py\", line 146, in _experiment_hparams\n    hparams.setdefault(name, set()).add(val)\nTypeError: unhashable type: 'dict'\n<\/code><\/pre>\n<p>Any idea what may be causing it and what is the solution?<\/p>\n<p>My system is Windows 10, Guild version is 0.7.2, and Python is 3.8.5.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dask scheduler not using multiple gpus on remote",
        "Question_creation_time":1617036688225,
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-not-using-multiple-gpus-on-remote\/583",
        "Question_upvote_count":0.0,
        "Question_view_count":410.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nIt\u2019s a fresh feature and most likely I\u2019m doing something wrong, but I cannot get the Dask scheduler to work properly on a remote with 4 gpus.<\/p>\n<p>This is what I do, and apart from the remote part it is basically a copy of the steps in the How To guide:<\/p>\n<ol>\n<li>\n<p>Connect to remote. I have successfully staged runs on remote, run them directly, and also used multiple gpus by assigning runs to gpus manually using --gpus flag. So the remote works corectly.<\/p>\n<\/li>\n<li>\n<p>Start the Dask scheduler on the remote.<\/p>\n<\/li>\n<li>\n<p>Then I stage trials on remote, let\u2019s say 4, with different parameters, using a single command.<\/p>\n<\/li>\n<\/ol>\n<pre><code>guild run TABL:train window=[100,200,300,400] --remote cerberus --stage-trials\n<\/code><\/pre>\n<ol start=\"4\">\n<li>The trials are sent to remote, and the scheduler starts them. If workers is set to 4, it starts correctly 4 processes of loading data etc.<\/li>\n<\/ol>\n<p>And this is where something goes wrong. After doing the pre-procesing stage and creating 4 models concurrently, the scheduler places all 4 models and training processses on all 4 gpus (which can be seen on the screenshot - all gpus have allocated memory). And then only begins the training on 1 gpu.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png\" data-download-href=\"\/uploads\/short-url\/2PTbvO3vWfJgJUE57EqoCMII9Jc.png?dl=1\" title=\"Screenshot from 2021-03-29 18-26-25\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png\" alt=\"Screenshot from 2021-03-29 18-26-25\" data-base62-sha1=\"2PTbvO3vWfJgJUE57EqoCMII9Jc\" width=\"490\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2021-03-29 18-26-25<\/span><span class=\"informations\">731\u00d7745 103 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><strong>Expected<\/strong><br>\nI would expect the scheduler to assign the trials to available gpus and train them concurrently. Furthermore, when the number of trials is bigger than the number of gpus, I would expect the scheduler to automatically run the pending operation when a gpus becomes free.<\/p>\n<p>Did I understand what the scheduler is capable of correctly? Is there maybe some manual step somewhere that I missed?<\/p>\n<p>Thanks in advance.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild does not catch scalar outputs when formatting values",
        "Question_creation_time":1655380881913,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-does-not-catch-scalar-outputs-when-formatting-values\/890",
        "Question_upvote_count":1.0,
        "Question_view_count":116.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Thank you very much for making this fantastic experiment manager and making it free!<\/p>\n<p>I noticed an unexpected behavior of Guild\u2019s <a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197#output-scalars-15\">output scalar<\/a>, so I report it here. Shortly, when the output strings are formatted with \u2018.e\u2019, they can not be caught by Guild. Below is a script to reproduce with Guild 0.8.1<\/p>\n<p>In scalar.py, I had<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">import argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--a\", type=float, default=1.0)\n    args = parser.parse_args()\n    print(f\"step: {100}\")\n    print(f\"value: {args.a:.2e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n<\/code><\/pre>\n<p>and in guild.yml, I had<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">scalar:\n  main: scalar\n  flags-dest: args\n  flags-import: all\n<\/code><\/pre>\n<p>Then in console, I got<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">(.venv) \u279c  gscalar guild run scalar\nYou are about to run scalar\n  a: 1.0\nContinue? (Y\/n)\nstep: 100\nvalue: 1.00e+00\n(.venv) \u279c  gscalar guild runs info\nid: 0b6abb0dc8bc4bb2914c01e504a5865e\noperation: scalar\nfrom: ~\/project\/gscalar\/guild.yml\nstatus: completed\nstarted: 2022-06-16 19:58:39\nstopped: 2022-06-16 19:58:39\nmarked: no\nlabel: a=1.0\nsourcecode_digest: 59509472e567014859244cbc4ce14ccd\nvcs_commit:\nrun_dir: ~\/project\/gscalar\/guild_home\/runs\/0b6abb0dc8bc4bb2914c01e504a5865e\ncommand: \/Users\/kyika\/project\/gscalar\/.venv\/bin\/python -um guild.op_main scalar -- --a 1.0\nexit_status: 0\npid:\ntags:\nflags:\n  a: 1.0\nscalars:\n<\/code><\/pre>\n<p>The last line above shown that Guild records no scalar. I guess this is because values was formatted with \u201ce+00\u201d, which was not recognized by Guild.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I tag\/label multiple runs?",
        "Question_creation_time":1661527216949,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-do-i-tag-label-multiple-runs\/910",
        "Question_upvote_count":2.0,
        "Question_view_count":52.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m struggling to achieve the following:<br>\nIn TensorBoard I can only differenciate multiple runs by ID, as the comment is truncated, so the hyperparameters are mostly not shown.<br>\nFor example:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">42afef82 my_model:train 2022-08-23 02:02:04 architecture=DenseNet-121 batch_size=24 classes=14 comment='DenseNet\/experiments\/lightning_logs\/version_0\n<\/code><\/pre>\n<p>In this example, the hyper-parameter dropout is not visible.<br>\nNow, what I would love to do is to compare specific runs, e.g. all runs with <code>dropout=0.1<\/code>, in TensorBoard.<\/p>\n<p>Based on the docs AFAIK the only possibility is to filter the runs by tag and then run TensorBoard specifically for these runs, instead of using guild view \u2192 click on TensorBoard \u2192 filter inside TensorBoard.<\/p>\n<p>Therefore, my question is: how do I tag\/label multiple runs?<\/p>\n<p>I discovered, that I can display all <code>dropout=0.1<\/code> runs using:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild runs list --list-all -Fl=dropout=0.1\n<\/code><\/pre>\n<p>However, running <code>guild runs tag -Fl=dropout=0.1 -a dropout=0.1<\/code> tags only one run.<\/p>\n<p>Furthermore, I also want to tag runs where <code>dropout<\/code> wasn\u2019t a hyper-parameter but hardcoded. How do I select those? Is there something like a negative lookaround?<\/p>\n<p>Thanks!<\/p>\n<p>Cheers,<br>\nAlessandro<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dask Scheduler not utilizing all available resources",
        "Question_creation_time":1646331949841,
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-not-utilizing-all-available-resources\/822",
        "Question_upvote_count":0.0,
        "Question_view_count":153.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey all,<\/p>\n<p>I\u2019ve been trying to get the dask scheduler to work with my guild runs. Let\u2019s say I have 2 GPUs and I\u2019d like to put at max, 2 runs on each GPU.<\/p>\n<p>According to the guides (<a href=\"https:\/\/my.guild.ai\/t\/parallel-processing-with-dask-scheduler\/550\" class=\"inline-onebox\">Parallel processing with Dask scheduler<\/a>) I\u2019ve implemented two sets of commands. The first set is two commands to stage the set of runs across the 2 gpus. These commands look something like this:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run model:train   param1_to_sweep=[10, 20, 30]\n                        param2_to_sweep=[1,2,3,4,5]\n                        --label my_hp_runs\n                        --optimizer random\n                        --trials 5\n                        --tag dask:GPU0=1\n                        --stage-trials\n                        --gpus 0\n\nguild run model:train   param1_to_sweep=[10, 20, 30]\n                        param2_to_sweep=[1,2,3,4,5]\n                        --label my_hp_runs\n                        --optimizer random\n                        --trials 5\n                        --tag dask:GPU1=1\n                        --stage-trials\n                        --gpus 1\n<\/code><\/pre>\n<p>After the runs are staged, I spin up the scheduler with the associated resources that would allow 2 runs per GPU.<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run dask:scheduler run-once=yes\n                         workers=10\n                         resources='GPU0=2 GPU1=2'\n                         dashboard-address=8890\n<\/code><\/pre>\n<p>I\u2019m able to open up the dashboard and can see GPU0 and GPU1 resources avalible. The trouble happens when I see dask only puts 2 runs of GPU0 and none on GPU1. To be more clear, about the schedule of runs that actually happen I\u2019ll label each of the staged runs below:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">run id  0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n----------------------------------------------\nGPU     0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1\n<\/code><\/pre>\n<p>Assuming all the runs take the same amount of time the runs get ran in the following \u2018sets\u2019<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">Set  | run ids\n---------------\n0    | 0,1 - The initial two runs get put on GPU0\n1    | 2,3 - GPU0 runs 0 and 1 ended, so 2 more get put on\n2    | 4, 5, 6 - GPU0 runs 2,3 end and there is only one GPU0 run left that gets executed (4). Then 2 runs go on GPU2\n3    | 7, 8 - Similar behavior to set 1\n4    | 9   - The remaining runs in the GPU2 queue. \n<\/code><\/pre>\n<p>The expected behavior would be the following sets<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">Set  | run ids\n---------------\n0    | 0,1,5,6 - 2 runs on each gpu\n1    | 2,3,7,8 - 2 runs on each gpu\n2    | 4,9 - 1 run on each gpu, queue finishes\n<\/code><\/pre>\n<p>Does anyone know why this could be happening or have I just implemented the dask scheduler wrong?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"`guild export` and underlying shututil dead-slow for samba drives",
        "Question_creation_time":1604534963382,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-export-and-underlying-shututil-dead-slow-for-samba-drives\/442",
        "Question_upvote_count":0.0,
        "Question_view_count":383.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>This is a very specific issue, but I wanted to raise it either way.<\/p>\n<p>I am using <code>guild export<\/code> to export runs to a Samba share mounted in Linux. This is incredibly slow. I see <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/9ddeb46a11e8c37a8ecb452b3b32f5c245297f2c\/guild\/commands\/runs_impl.py#L1193\" rel=\"noopener nofollow ugc\">here<\/a> that <code>guild<\/code> uses <code>shututil<\/code>. This is a known issue for <code>shututil<\/code> and they suggest a monkey patch in this issue <a href=\"https:\/\/github.com\/SickChill\/SickChill\/issues\/3292#issuecomment-289728670\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>I think I am going to experiment with ways to patch <code>guild<\/code>. Since we are using Azure, I want to see if I can utilize <code>azcopy<\/code>, which is orders of magnitude faster.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild runs showing as error instead of pending",
        "Question_creation_time":1610486130764,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-showing-as-error-instead-of-pending\/518",
        "Question_upvote_count":2.0,
        "Question_view_count":350.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve been using my guild.yml file with no issue so far. I\u2019m not sure what happened but suddenly my pending runs are showing up as errors (no logs, no error messages, but the environment somehow hasn\u2019t been resolved either).<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c780081e0276246cae200732bc38d932d3a3adef.png\" alt=\"Screen Shot 2021-01-12 at 1.07.45 PM\" data-base62-sha1=\"ssRk2IH4fTBNzGvi7VQNIuHVTL9\" width=\"357\" height=\"195\"><br>\nThis behavior also shows on <code>guild runs<\/code>:<\/p>\n<pre><code>[1:1609e5a4]   imputer   2021-01-12 13:14:13  error      none ckd\n[2:9ceadd83]   imputer   2021-01-12 13:14:12  running    none ckd\n[3:323f949e]   imputer+  2021-01-12 13:14:12  running    \n[4:b9c5ba05]   fo        2021-01-12 13:14:09  running \n<\/code><\/pre>\n<p>The pending run eventually ran and succeeded, and the error went away. But it\u2019s unclear why there was an error in the first place. I took a look at guild view and I see this, which looks strange:<\/p>\n<pre><code>ERROR: 128.97.25.29 - - [12\/Jan\/2021 05:11:03] code 400, message Bad request syntax ('\\t\\x00\\x00\\x00\\x01\\x00\\x00\\x00\u00ff\u00fe\u00ff\\x0fQ\\x00u\\x00a\\x00l\\x00y\\x00s\\x00P\\x00r\\x00o\\x00b\\x00e\\x00T\\x00e\\x00s\\x00t\\x00\\x02\\x00\\x00\\x00')\nERROR: 128.97.25.29 - - [12\/Jan\/2021 05:11:03] code 400, message Bad request syntax ('\u00ff\u00ff\u00ff\u00ff')\nERROR: 128.97.25.29 - - [12\/Jan\/2021 05:11:48] code 400, message Bad request syntax ('\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\nERROR: 128.97.25.29 - - [12\/Jan\/2021 05:12:33] code 400, message Bad request syntax ('gqw7')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:06:18] code 400, message Bad request syntax ('\\t\\x00\\x00\\x00\\x01\\x00\\x00\\x00\u00ff\u00fe\u00ff\\x0fQ\\x00u\\x00a\\x00l\\x00y\\x00s\\x00P\\x00r\\x00o\\x00b\\x00e\\x00T\\x00e\\x00s\\x00t\\x00\\x02\\x00\\x00\\x00')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:06:18] code 400, message Bad request syntax ('\u00ff\u00ff\u00ff\u00ff')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:07:03] code 400, message Bad request syntax ('\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:07:48] code 400, message Bad request syntax ('gqw7')\n<\/code><\/pre>\n<p>However, when I do a longer run or try to use queues, they never run. They just show as errors.<\/p>\n<p>Not sure how to debug.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I test my Guild file?",
        "Question_creation_time":1603822545054,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-do-i-test-my-guild-file\/434",
        "Question_upvote_count":0.0,
        "Question_view_count":376.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I know guild has support for some testing and checks of the guild file setup. I just can\u2019t find the documentation for it.<\/p>\n<p>Ideally I would like to have some unit tests running that tests that the <code>guild.yml<\/code> is correctly setup and that some of the <code>operations<\/code> and <code>models<\/code> works as expected.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild pull no matching run found",
        "Question_creation_time":1592832356985,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-pull-no-matching-run-found\/198",
        "Question_upvote_count":1.0,
        "Question_view_count":323.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am having problems with pulling runs from a remote server.<\/p>\n<pre><code>guild runs --remote remote\n\n[1:5c07e941]  srl\/state:train-rl  2020-06-22 14:45:06  completed  env=SingleGoal-v01\n<\/code><\/pre>\n<p>When I try to pull the run it tells me there is no matching run was found:<\/p>\n<pre><code>guild pull remote 5c07e941 \n\nGetting remote run info \nguild: could not find a run matching '5c07e941'\n<\/code><\/pre>\n<p>My <code>config.yml<\/code> looks as follows:<\/p>\n<pre><code>remotes:\n  remote:\n    type: ssh\n    host: remote\n    user: username\n    guild-home: \/home\/username\/guild\n    guild-env: \/home\/username\/guild\/venv\n<\/code><\/pre>\n<p>The guild version I use is <code>0.7.0.rc11<\/code>.<br>\nPushing a run to the remote works fin.<\/p>\n<p>Have I done something wrong or is it a bug?<\/p>\n<p>Kind regards,<br>\nBart<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Installation problems",
        "Question_creation_time":1597999937041,
        "Question_link":"https:\/\/my.guild.ai\/t\/installation-problems\/295",
        "Question_upvote_count":1.0,
        "Question_view_count":484.0,
        "Question_answer_count":16,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have reopened the issue on GitHub. I am not sure if you see it, so I just wanted to forward the link here\u010c<br>\n<aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/133\" target=\"_blank\" rel=\"nofollow noopener\">github.com\/guildai\/guildai<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/133\" target=\"_blank\" rel=\"nofollow noopener\"> pip install guildai on macos x fails<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-03-01\" data-time=\"14:37:48\" data-timezone=\"UTC\">02:37PM - 01 Mar 20 UTC<\/span>\n      <\/div>\n\n        <div class=\"date\">\n          closed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-03-01\" data-time=\"15:15:51\" data-timezone=\"UTC\">03:15PM - 01 Mar 20 UTC<\/span>\n        <\/div>\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/mdiephuis\" target=\"_blank\" rel=\"nofollow noopener\">\n          <img alt=\"mdiephuis\" src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/ebde3a729b99195a01fadd847aa66b1763a50c8b.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          mdiephuis\n        <\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n<div class=\"github-row\">\n  <p class=\"github-content\">Runnning\npip install guildai\nERROR: Could not find a version that satisfies the requirement guildai (from versions: none)\nERROR: No matching distribution found for...<\/p>\n<\/div>\n\n<div class=\"labels\">\n<\/div>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<\/p>\n<p>For some reason, I can\u0107t install guildai on windows, after reinstalling anaconda, but I was able to install it before.<\/p>\n<p>Before update, I was able to install it.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Opening source file causes File not found exception",
        "Question_creation_time":1659175674039,
        "Question_link":"https:\/\/my.guild.ai\/t\/opening-source-file-causes-file-not-found-exception\/903",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I just started using guild and I\u2019m trying to integrate it with an existing project.<br>\nI have the following directory structure:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">project\/\n\u251c\u2500 configs\/\n\u2502  \u251c\u2500 __init__.py\n\u2502  \u251c\u2500 cfg1.py\n\u2502  \u251c\u2500 cfg2.py\n\u251c\u2500 utils\/\n\u2502  \u251c\u2500 base_config.py\n<\/code><\/pre>\n<p>Where the config files contain something similar to<\/p>\n<pre><code class=\"lang-python\">from utils.base_config import BaseConfig\n\nclass Config(BaseConfig):\n    class Model:\n        architecture: MLP\n        class Parameters:\n             pass\n\nif __name__ == \"__main__\":\n    Config.init()\n    train.main()\n<\/code><\/pre>\n<p>and <code>BaseConfig.py<\/code> among other things does this<\/p>\n<pre><code class=\"lang-python\">import __main__\n\nclass BaseConfig:\n    @classmethod\n    def init(cls):\n        with Path('configs\/__init__.py').open('w+') as f:\n            f.writelines([f'from .{Path(str(__main__)).stem } import Config'])\n            f.flush()\n<\/code><\/pre>\n<p>Now, when I run <code>guild run configs\/cfg1.py<\/code> I get the following error:<\/p>\n<pre><code class=\"lang-python\">Traceback (most recent call last):\n  File \"C:\\Users\\username\\PycharmProjects\\project\\configs\\cfg1.py\", line 56, in &lt;module&gt;\n    Config.init()\n  File \"C:\\Users\\username\\PycharmProjects\\project\\utils\\base_config.py\", line 13, in init\n    with Path('configs\/__init__.py').open('w+') as f:\n  File \"C:\\Users\\username\\.conda\\envs\\project_env\\lib\\pathlib.py\", line 1252, in open\n    return io.open(self, mode, buffering, encoding, errors, newline,\n  File \"C:\\Users\\username\\.conda\\envs\\project_env\\lib\\pathlib.py\", line 1120, in _opener\n    return self._accessor.open(self, flags, mode)\nFileNotFoundError: [Errno 2] No such file or directory: 'configs\\\\__init__.py'\n<\/code><\/pre>\n<p>Then, when I look into the <code>sourcecode<\/code> directory of the run I see that only <code>__init__.py<\/code>, <code>cfg1.py<\/code> and <code>cfg2.py<\/code>are present and the directory structure is not preserved (i.e. the three files are not under <code>configs<\/code>).  Any idea on how to solve this?<\/p>\n<p>Also, I was wondering how I could log the configuration file. I tried playing with <code>flag-dest:config<\/code> but couldn\u2019t manage to make it work.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild run hangs \/ very slow",
        "Question_creation_time":1601685221068,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362",
        "Question_upvote_count":1.0,
        "Question_view_count":370.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have successfully used guild in some older project, but in this new project I am having a hard time debugging what is going on.<\/p>\n<p>I am trying to do:<\/p>\n<pre><code>guild run model:train -y\n<\/code><\/pre>\n<p>But the operation hangs without any output to the console.<\/p>\n<p>If I do<\/p>\n<pre><code>guild run model:train -y --print-cmd\n<\/code><\/pre>\n<p>And execute it directly with python and exact same arguments it gets executed right away.<\/p>\n<p>How do I debug this behavior?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to combine flags from multiple operations in pipelines to enable parameter searches across multiple operations?",
        "Question_creation_time":1598537665027,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-combine-flags-from-multiple-operations-in-pipelines-to-enable-parameter-searches-across-multiple-operations\/328",
        "Question_upvote_count":2.0,
        "Question_view_count":690.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Let say I have 2 operations preprocessing and training.<br>\nEach has a series of flags that I want to do a hyper parameter search over.<br>\nHere is a rough idea of what my operations would look like<\/p>\n<pre><code>preprocessing:\n  flags:\n    a:\n    b:\ntraining:\n  flags:\n    c:\n    d:\n<\/code><\/pre>\n<p>Neglecting the practicality of this scenario, I want to run my preprocessing operation every time I run train.  Is there a way to achieve this with pipelines or a similar method?  I know I could do<\/p>\n<pre><code>mypipeline:\n  flags:\n    a: [1,2,3]\n    b: [4,5,6]\n    c: [12,13,14]\n    d: [15,16,17]\n  steps:\n    - preprocessing a=${a} b=${b}\n    - train c=${c} d=${d}\n<\/code><\/pre>\n<p>and this would solve my problem, but when both operations have a lot of parameters it would end up requiring a lot of copying and pasting between the pipeline and operation flags.  Is there a better\/more efficient way to achieve this?  I basically want the pipline to \u201cinherit\u201d the flags of the steps its performing if possible.  The documentation seems to hint at something like this<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"197\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197\/1\">Guild File Reference<\/a>\n<\/div>\n<blockquote>\n<p>You can include references to step flag values as needed to pass through user-specified values.<\/p>\n<\/blockquote>\n<\/aside>\n<p>But when I create a pipeline with no flags and attempt to pass a flag which is defined by one of the steps I get an error saying that the flag does not exists.  Is what I am trying to do possible with Guild?<\/p>\n<p>Thanks.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild Init Errors on Windows 10",
        "Question_creation_time":1628886345302,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-init-errors-on-windows-10\/751",
        "Question_upvote_count":0.0,
        "Question_view_count":241.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am working my way through the example files and have run into a bug where the guild init command fails.<\/p>\n<pre><code>You are about to initialize a Guild environment:\n  Location: C:\\Users\\{USER}\\repos\\ml_framework_testing\\guildai\\iris-svm\\venv\n  Name: iris-svm\n  Python interpreter: default\n  Use system site packages: no\n  Guild: 0.7.3\n  Python requirements:\n    .\\requirements.txt\n  Resource cache: shared\nContinue? (Y\/n) y\nInitializing Guild environment in C:\\Users\\{USER}\\repos\\ml_framework_testing\\guildai\\iris-svm\\venv\nCreating virtual environment\ncreated virtual environment CPython3.7.4.final.0-64 in 1341ms\n  creator CPython3Windows(dest=C:\\Users\\{USER}\\repos\\ml_framework_testing\\guildai\\iris-svm\\venv, clear=False, no_vcs_ignore=False, global=False)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=C:\\Users\\joeyr\\AppData\\Local\\pypa\\virtualenv)\n    added seed packages: pip==21.2.3, setuptools==57.4.0, wheel==0.37.0\n  activators BashActivator,BatchActivator,FishActivator,PowerShellActivator,PythonActivator\nUpgrading pip\nInstalling Guild guildai==0.7.3\nFatal Python error: _Py_HashRandomization_Init: failed to get random numbers to initialize Python\n\nguild: Command '['C:\\\\Users\\\\{USER}\\\\repos\\\\ml_framework_testing\\\\guildai\\\\iris-svm\\\\venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--no-warn-script-location', 'guildai==0.7.3']' returned non-zero exit status 1.\n<\/code><\/pre>\n<p>Any idea what is going on here or how to fix it? I was getting an access denied error on the Upgrading pip line for a bit until I upgraded manually ahead of time with \u201cpython -m pip install --upgrade pip\u201d.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Strange out-of-memory behavior on Guild with XGBoost",
        "Question_creation_time":1665986502533,
        "Question_link":"https:\/\/my.guild.ai\/t\/strange-out-of-memory-behavior-on-guild-with-xgboost\/931",
        "Question_upvote_count":0.0,
        "Question_view_count":55.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all, I\u2019m using Guild to manage and tune XGBoost models for a binary classification problem. My dataset is around 2MB and has around 20K rows of 15 features. My XGBClassifier has around 100 estimators, the max depth is 6, and the tree method is <code>gpu-hist<\/code>.<\/p>\n<p>When I run my program in VS Code it executes with no problems. When I run the same program in the command line with Guild, it also finishes without throwing any error. But when I look at the run in <code>guild view<\/code> or <code>guild runs<\/code>, it says that the run exited with an error status <code>3221226505<\/code>.<\/p>\n<p>Online sources say this generic python error is some form of out of memory error. However, this can\u2019t be the case as I monitored the RAM and VRAM usage while executing my program and they were both very low.<\/p>\n<p>When I switch the tree method to just <code>hist<\/code> (cpu-only training) and re-run the program, <code>guild view<\/code> now shows the run as <code>completed<\/code>.<\/p>\n<p>May I know if this is a bug? My GPU is a Quadro T1000, my XGBoost version is <code>1.6.2<\/code>, and here is part of the <code>guild check<\/code> output:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild_version:             0.8.1\npython_version:            3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]                            \nplatform:                  Windows 10 AMD64                                                                               \npsutil_version:            5.9.2\ntensorboard_version:       2.10.1\ncuda_version:              11.7\nnvidia_smi_version:        516.69\nlatest_guild_version:      0.8.1   \n<\/code><\/pre>\n<p>Thank you!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running multiple batches of an experiment with different hyperparameter flag values",
        "Question_creation_time":1624051984492,
        "Question_link":"https:\/\/my.guild.ai\/t\/running-multiple-batches-of-an-experiment-with-different-hyperparameter-flag-values\/724",
        "Question_upvote_count":1.0,
        "Question_view_count":332.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to utilize the grid search capability of guild to run an experiment multiple times with different hyperparameter flag values in a python file for SVM on the iris dataset.<\/p>\n<p>My hyperparameter flags are defined as a dictionary:<br>\nhyperparam_dict = {\u2018kernel\u2019: \u2018linear\u2019, \u2018test_split\u2019: 0.1, \u2018random_seed\u2019: 2, \u2018degree\u2019: 4, \u2018gamma\u2019: 50} and I want to be able to run the experiment with various values for each hyperparameter to test the accuracy value of every combination of flag values\u2026 e.g \u2018test_split\u2019 = [0.1, 0.2, 0.3], \u2018degree\u2019= [1, 2, 3, 4], etc.<\/p>\n<p>When I follow the example from the get-started.ipynb:<br>\n_ = guild.run(train, x=[-0.5,-0.4,-0.3,-0.2,-0.1])<br>\nwith my code:<br>\nguild.run(main, hyperparam_dict[\u2018test_split\u2019] =  [0.1, 0.2, 0.3],) I am getting an error.<\/p>\n<p>Is there any way what I am trying to achieve, be done??<\/p>\n<p>Looking for any help to try and resolve this issue!<\/p>\n<p>Thank you.<\/p>\n<p>Original guild.yml file:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/a15c24e93e41c11d4bd4960f978ec4c8b2a9631e.png\" alt=\"image\" data-base62-sha1=\"n1si1CVnP83aIsR6893PR5wXRVQ\" width=\"536\" height=\"425\"><\/p>\n<p>Notebook commands where I am trying to achieve running multiple runs of an experiment with three different  'test_split\" values to be tested.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c.png\" data-download-href=\"\/uploads\/short-url\/sjq2ufe4bGzrQn5kM5Fk1zyT0q8.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c.png\" alt=\"image\" data-base62-sha1=\"sjq2ufe4bGzrQn5kM5Fk1zyT0q8\" width=\"690\" height=\"197\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">727\u00d7208 9.91 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Torch.multiprocessing.spawn fails",
        "Question_creation_time":1665950939224,
        "Question_link":"https:\/\/my.guild.ai\/t\/torch-multiprocessing-spawn-fails\/929",
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have some code that works standalone, but fails when run from guild.  The offending line is:<\/p>\n<pre><code>torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args))\n<\/code><\/pre>\n<p>and the complaint is:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">  [...] \n  File \"\/usr\/lib\/python3.10\/multiprocessing\/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/context.py\", line 288, in _Popen\n    return Popen(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/spawn.py\", line 183, in get_preparation_data\n    main_mod_name = getattr(main_module.__spec__, \"name\", None)\nAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>Does anyone have any tips?  I\u2019m not sure I really understand what\u2019s failing in the spawn call\u2026  Thanks!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild doesn't copy module to new source code location",
        "Question_creation_time":1619015850647,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-doesnt-copy-module-to-new-source-code-location\/690",
        "Question_upvote_count":0.0,
        "Question_view_count":534.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When I use the command <code>guild run train,py<\/code> I get the error <code>Error: can't important module_1<\/code>. When I look in the folder to which guildai copies the source code after I call guild run, I can see that <code>module_1<\/code> was indeed not copied. There is nothing special about that module_1 (normal code files). How can I debug this issue further?<\/p>\n<p>Here is my folder structure:<\/p>\n<p>Folder structure:<\/p>\n<pre><code>guild.yml\ntrain.py\nmodule_1\nmodule_2\nmodule_3\n<\/code><\/pre>\n<p>guild.yml<\/p>\n<pre><code>train:\n  description: Training script\n  main: train\n  # sourcecode:\n  #   - '*.py'\n  flags-dest: global:params\n  flags-import: all\n  flags:\n    ## general\n    gpu:\n      description:\n      default: 0\n    seed:\n      description:\n      default: 0\n...\n<\/code><\/pre>\n<p>Thanks for your help!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Parallel batch trial pipeline",
        "Question_creation_time":1591810114957,
        "Question_link":"https:\/\/my.guild.ai\/t\/parallel-batch-trial-pipeline\/142",
        "Question_upvote_count":0.0,
        "Question_view_count":332.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In a batched trial pipeline see <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/196\" rel=\"nofollow noopener\">196<\/a>, how can we run as fast as possible parallelizing everything we can ?<\/p>\n<p>Example Pipeline:<\/p>\n<pre><code>operation_1: # has no dependencies\n\noperation_2: \n    requires:\n        - operation: operation_1\n    flags-dest: globals\n    flags-import:\n        - some_param\n\noperation_3: \n    requires:\n        - operation: operation_2\n    flags-dest: globals\n    flags-import:\n        - some_other_param\n\npipeline:\n  steps:\n    - run: operation_1\n    - run: operation_2\n      flags:\n        some_param: [a, b]\n    - run: operation_3\n      flags:\n        some_other_param: [1, 2, 3]\n<\/code><\/pre>\n<pre><code># create 6 queues as we have 6 batch trials that can be done in parallel (a1, a2, a3, b1, b2, b3, c1, c2, c3)\n# run this command 6 times\nguild run queue --background \n\nguild run pipeline --stage\n<\/code><\/pre>\n<p>For some reason this leads to out of sequence events happening, like operation 2 being run before operation 1 resulting in an error. Is this the correct way to do this?<\/p>\n<p>Further, is there a good way of timing this to sanity check parallel works faster i.e time batched trials run?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Getting full run id",
        "Question_creation_time":1600093581539,
        "Question_link":"https:\/\/my.guild.ai\/t\/getting-full-run-id\/343",
        "Question_upvote_count":0.0,
        "Question_view_count":563.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is there a way to print out the full run id when using guild compare?<\/p>\n<p>I wanted to run through the results and then inspect the folder containing the output of the run in .guild, but the output of guild compare is the shortened run id so I can\u2019t directly find the folder.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Passing arguments, defined in guild.yml file when using Python's argparse",
        "Question_creation_time":1598602229021,
        "Question_link":"https:\/\/my.guild.ai\/t\/passing-arguments-defined-in-guild-yml-file-when-using-pythons-argparse\/329",
        "Question_upvote_count":0.0,
        "Question_view_count":806.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Guild version 0.7.0<\/p>\n<p>I noticed that flags, defined in guild.yml are not passed normally as arguments to config.yml if I use argparse in my Python code. It seems it wants to pass them python arguments even though flags-dest and flags-import are off.<\/p>\n<p>My workflow usually consists of defining main hyperparameters in the config.yml file, but for choosing which gpu to run it on (in my testing phase) I use argparse. Is there a way to combine those two?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tensorboard version conflict",
        "Question_creation_time":1600941341725,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-version-conflict\/353",
        "Question_upvote_count":0.0,
        "Question_view_count":974.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Guildai tensorboard functionallity don\u0107t work for me for some time. Now I would like to make it work.<\/p>\n<p>If I execute command <code>guild tensorboard --started 'last 1 hour'<\/code>  I get:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\trademl&gt; guild tensorboard --started 'last 1 hour'\nPreparing runs for TensorBoard\n2020-09-24 11:32:59.909813: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\nTraceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    guild.main.main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    _main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 40, in _main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\click_util.py\", line 201, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 98, in _run_tensorboard\n    tensorboard.serve_forever(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\tensorboard.py\", line 607, in serve_forever\n    app = create_app(logdir, reload_interval, tensorboard_options=tensorboard_options)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\tensorboard.py\", line 508, in create_app\n    plugins = _tensorboard_plugins(disabled_plugins)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\tensorboard.py\", line 522, in _tensorboard_plugins\n    base_plugins = tensorboard.base_plugins()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\tensorboard.py\", line 232, in base_plugins\n    return list(set(default.get_plugins() + default.get_dynamic_plugins()))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\default.py\", line 122, in get_dynamic_plugins\n    return [\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\default.py\", line 123, in &lt;listcomp&gt;\n    entry_point.load()\n  File \"C:\\Users\\Mislav\\AppData\\Roaming\\Python\\Python38\\site-packages\\pkg_resources\\__init__.py\", line 2471, in load\n    self.require(*args, **kwargs)\n  File \"C:\\Users\\Mislav\\AppData\\Roaming\\Python\\Python38\\site-packages\\pkg_resources\\__init__.py\", line 2494, in require\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n  File \"C:\\Users\\Mislav\\AppData\\Roaming\\Python\\Python38\\site-packages\\pkg_resources\\__init__.py\", line 790, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.VersionConflict: (requests 2.18.4 (c:\\users\\mislav\\appdata\\roaming\\python\\python38\\site-packages), Requirement.parse('requests&lt;3,&gt;=2.21.0'))\n<\/code><\/pre>\n<p><code>Conda list requests<\/code> gives:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\trademl&gt; conda list requests\n# packages in environment at C:\\ProgramData\\Anaconda3:\n#\n# Name                    Version                   Build  Channel\nrequests                  2.24.0             pyh9f0ad1d_0    conda-forge\nrequests-oauthlib         1.3.0                    pypi_0    pypi\n<\/code><\/pre>\n<p>so it is greater than 2.21 and lower than 3?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Data Filepath Flag",
        "Question_creation_time":1602795008595,
        "Question_link":"https:\/\/my.guild.ai\/t\/data-filepath-flag\/376",
        "Question_upvote_count":1.0,
        "Question_view_count":305.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I\u2019m converting a project to guild and I have a question about setting a filepath as a flag.<\/p>\n<p>At the top of my training script I have a variable data_fp = \u201c\u2026\/\u2026\/data\/dtype2\/processed_data.npy\u201d<br>\nbefore this was a guild project I was just cutting and pasting different filepaths when I want to train the model on new data but now I want to be able to set it as a flag.<\/p>\n<p>When I try to run this with guild I get an error because it doesn\u2019t see that path. Is there a way to do set this up so that the script can see the data from the \/run directory?<\/p>\n<p>Also, currently my folder structure is like this and my guild file is :<\/p>\n<pre><code>proj\/\n  data\/\n    dtype1\/\n      raw_data.npy\n    dtype2\/\n      processed_data.npy\n  scripts\/\n    guild.yml\n    data_processing\/\n      process_data.py\n    model\/\n      train.py\n      ...\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to view runs on all remotes?",
        "Question_creation_time":1614500077203,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-view-runs-on-all-remotes\/554",
        "Question_upvote_count":0.0,
        "Question_view_count":347.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I have concurrent runs on multiple remotes (say 4 runs on 4 different remote). Is there a way to view the status of all runs from a single interface? I would imagine that <code>guild runs<\/code> should show them, but for some reason the remote runs appear as \u201cterminated\u201d (while in fact running) and switch to \u201ccompleted\u201d when they are finished.<br>\nI am working with a cluster that has shared drives if that could be taken advantage of.<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tracking source code that is a python package",
        "Question_creation_time":1645779758913,
        "Question_link":"https:\/\/my.guild.ai\/t\/tracking-source-code-that-is-a-python-package\/816",
        "Question_upvote_count":1.0,
        "Question_view_count":123.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m very sorry if this is already documented.<br>\nthe scripts I run are all part of a python package I\u2019m working on, the packages are installed using conda with.<\/p>\n<p>pip install -e<\/p>\n<p>which makes a symbolic link from conda to my package\u2019s directory.<\/p>\n<p>When I run the experiment with guild it stores the source code of all the files in the package\u2013which is good. However, when I try to restart the run, the program starts by using the main function in my<br>\nguild sourcecode folder\u2013which is what I want-- but the moment it tries to use a module imported from its parent package it starts using code in the packages main directory instead of the version of that file saved by Guild. How could i get guild to use the files saved by guild?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Process flag value in guild.yml for pipelines",
        "Question_creation_time":1650387338552,
        "Question_link":"https:\/\/my.guild.ai\/t\/process-flag-value-in-guild-yml-for-pipelines\/860",
        "Question_upvote_count":0.0,
        "Question_view_count":84.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Say I have a pipeline that takes as input a path as a flag:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">preprocess_bag:\n  flags:\n    bag_path: \"path\/to\/bag.bag\"\n  steps:\n    - decode_bag bag_path=${bag_path}\n    - dump_bag bag_bag=${bag_path_but_with_some_processed} # i.e. strip entire path down to only bag.bag\n<\/code><\/pre>\n<p>What I am essentially looking for is something like <a href=\"https:\/\/www.gnu.org\/software\/make\/manual\/html_node\/File-Name-Functions.html\" rel=\"noopener nofollow ugc\">Make File Name Functions<\/a> that I can use to manipulate the flags.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild runs very slow",
        "Question_creation_time":1663441155671,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-very-slow\/919",
        "Question_upvote_count":1.0,
        "Question_view_count":69.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Similarly to what has been described in <a href=\"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362\">this thread<\/a>, running the following command results in hanging for a while before we see any results:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run train\n<\/code><\/pre>\n<p>The Guild file contains the following:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  main: script\n<\/code><\/pre>\n<p>The script is simply:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">print(\"Hello, world!\")\n<\/code><\/pre>\n<p>I believe the hanging is related to the fact that I have a data folder which contains thousands of files each corresponding to a sample of the dataset I\u2019m using in my experiments. This is because:<\/p>\n<ol>\n<li>When using the <code>--debug<\/code> flag, Guild hangs just after copying one <code>.csv<\/code> file within the data folder (none of the thousands of sample files are copied to the target directory, though).<\/li>\n<li>When I remove the data folder, the hanging stops.<\/li>\n<\/ol>\n<hr>\n<p>My questions are:<\/p>\n<ol>\n<li>Is this a bug? If so, how can I circumvent this issue?<\/li>\n<li>How am I supposed to make my dataset accessible by the training script, since it is run from a totally different directory and there are thousands of files it needs to access?<\/li>\n<\/ol>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Detect SimpleNamespace as flag",
        "Question_creation_time":1616438128886,
        "Question_link":"https:\/\/my.guild.ai\/t\/detect-simplenamespace-as-flag\/567",
        "Question_upvote_count":0.0,
        "Question_view_count":239.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I\u2019m trying to tune hyperparameters using guild. These are defined as follows:<\/p>\n<pre><code>from types import SimpleNamespace\nparams = SimpleNamespace(\n    embedding_dim = 256,\n    window_size = 5,\n    batch_size = 2048,\n    epochs = 2,\n    preprocessed = f'{DATASET_ROOT}\/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}\/{DATASET_PREFIX}',\n    modelname = f'{WORKING_ROOT}\/{DATASET_VERSION}.pt',\n    train = True\n)\n<\/code><\/pre>\n<p>Guild won\u2019t find them by default and I am having a hard time working around this. I am  new to guild so maybe there is an answer in the DOCS i haven\u2019t found.<\/p>\n<p>Thanks in advance<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dependecies Problem",
        "Question_creation_time":1611065190742,
        "Question_link":"https:\/\/my.guild.ai\/t\/dependecies-problem\/523",
        "Question_upvote_count":0.0,
        "Question_view_count":479.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear all,<br>\nI am new to Guild.ai<br>\nI have a train file called train.py that needs as input a parameter (using argparse) that is called config_filepath .<br>\nI am trying over and over to run:<br>\nguild run train.py config_filepath=config_thyroid_segnet_multi_h5.json<\/p>\n<p>but nothing works. Everytime I get the following message:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018config_thyroid_segnet_multi_h5.json\u2019<\/p>\n<p>The funny part is that if I check in the runs directory (performing guild ls) the file is there. What am I doing wrong? I tried also to create a guild.yml file like following:<br>\ntrain.py:<br>\ndescription: Say hello to my friends<br>\nmain: train<br>\ndefault: yes<br>\nflags-import:<br>\n- config_filepath<\/p>\n<p>And alternatively to create a guild.yml with the tag requires.<\/p>\n<p>Nothing works\u2026<\/p>\n<p>Thanks in advance for your support<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Docs using -o option outdated",
        "Question_creation_time":1616319728869,
        "Question_link":"https:\/\/my.guild.ai\/t\/docs-using-o-option-outdated\/568",
        "Question_upvote_count":0.0,
        "Question_view_count":214.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In restart-batch-to-continue-optimization the \u201cguild select -o train.py+gp\u201d command actually doesn\u2019t work. I tried with \u201cguild select -Fo gp\u201d and it seemed to work.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remove accidentally recorded flags from runs",
        "Question_creation_time":1606745426615,
        "Question_link":"https:\/\/my.guild.ai\/t\/remove-accidentally-recorded-flags-from-runs\/470",
        "Question_upvote_count":1.0,
        "Question_view_count":276.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi the loss of every epoch has been recorded in my experiments. I dont want to delete the runs, but can these unwanted flags be removed from the overview shown in guild compare?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Operations as dependencies during checks",
        "Question_creation_time":1620166635120,
        "Question_link":"https:\/\/my.guild.ai\/t\/operations-as-dependencies-during-checks\/701",
        "Question_upvote_count":1.0,
        "Question_view_count":243.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a <code>guild<\/code> model that looks something like this:<\/p>\n<pre><code>- include: source_code_config.yml\n- model: _check\n  extends:\n    - source_code_config\n  operations:\n    _test_segmentation_training:\n      steps:\n        - run: segmentation:train dryrun=yes num_epochs=1 input_database=\"x.csv\"\n          expect:\n            - file: experiments\/best_model.pt\n    _test_segmentation_testing:\n      steps:\n        - run:segmentation:test input_database=\"x.csv\"\n          expect:\n            - output: \"Testing done.\"\n    _all:\n      steps:\n        - _test_segmentation_training\n        - _test_segmentation_testing\n<\/code><\/pre>\n<p>I use this for integration testing my training and tests scripts.<\/p>\n<p>The <code>segmentation<\/code> model looks something like this:<\/p>\n<pre><code>- model: segmentation\n  extends:\n    - source_code_config\n  operations:\n    train:\n      main: scripts\/training\/train_segmentation\n      flags:\n        $include:\n          - segmentation_flags\n          - train_flags\n          - common_flags\n      requires:\n        - prepared_data\n    test:\n      main: scripts\/training\/test_segmentation\n      flags:\n        batch_size: 1\n        $include:\n          - test_flags\n          - common_flags\n      requires:\n        - prepared_data\n        - trained_model\n  resources:\n    trained_model:\n      sources:\n        - operation: train\n          select:\n            - experiments\n            - .guild\/attrs\/flags\n          target-type: copy\n          rename:\n            - flags training-flags.yml # See https:\/\/github.com\/guildai\/guildai\/blob\/0.7.2\/examples\/upstream-flags\/guild.yml\n<\/code><\/pre>\n<p>Now when I run<\/p>\n<pre><code>guild run segmentation:train\n<\/code><\/pre>\n<p>Followed by:<\/p>\n<pre><code>guild run segmentation:test\n<\/code><\/pre>\n<p><code>guild<\/code> will automatically resolve the <code>trained_model<\/code> dependency and find the latest <code>segmentation:train<\/code> run.<\/p>\n<p>If I instead run:<\/p>\n<p><code>guild _check:_all<\/code><\/p>\n<p>The <code>_test_segmentation_training<\/code> runs successfully, but the <code>_test_segmentation_testing<\/code> operation is not able to resolve the <code>trained_model<\/code> resource.<\/p>\n<p>Ideally I would be able to run this pipeline as a step in my integration testing.<\/p>\n<p>EDIT:<\/p>\n<p>Using <code>guild<\/code> version <code>0.7.3<\/code><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"__main__ has no attribute __spec__ pytorch-lightning multiGPU",
        "Question_creation_time":1666401932704,
        "Question_link":"https:\/\/my.guild.ai\/t\/main-has-no-attribute-spec-pytorch-lightning-multigpu\/946",
        "Question_upvote_count":1.0,
        "Question_view_count":49.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When using guild to run experiments on multiple GPUs with pytorch-lightning I get the following error:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/main.py\", line 219, in &lt;module&gt;\n10\/21\/2022 5:54:52 PM\tmain()\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/main.py\", line 95, in main\n10\/21\/2022 5:54:52 PM\timputed_data = get_imputation_logic(args)(args, data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/task_logic\/ae_imputation.py\", line 119, in ae_imputation_logic\n10\/21\/2022 5:54:52 PM\tae_imputer = create_autoencoder(args, data, settings)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/task_logic\/tuner.py\", line 69, in create_autoencoder\n10\/21\/2022 5:54:52 PM\tae_imputer.fit(data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 148, in fit\n10\/21\/2022 5:54:52 PM\tself._fit(data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 166, in _fit\n10\/21\/2022 5:54:52 PM\tself.trainer.fit(self.ae, datamodule=data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 770, in fit\n10\/21\/2022 5:54:52 PM\tself._call_and_handle_interrupt(\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 721, in _call_and_handle_interrupt\n10\/21\/2022 5:54:52 PM\treturn self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 92, in launch\n10\/21\/2022 5:54:52 PM\tself._call_children_scripts()\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 109, in _call_children_scripts\n10\/21\/2022 5:54:52 PM\tif __main__.__spec__ is None: # pragma: no-cover\n10\/21\/2022 5:54:52 PM\tAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>So I went into the file <code>\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 109, in _call_children_scripts<\/code> and changed <code>if __main__.__spec__ is None<\/code> to <code>if True<\/code> as a stopgap, and it seemed to work. For some reason <code>__main__<\/code> is a dictionary.<\/p>\n<p>This isn\u2019t the exact same problem, but I happened to find a somewhat relevant problem <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3859\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tensorboard taking long to startup",
        "Question_creation_time":1593771427991,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-taking-long-to-startup\/212",
        "Question_upvote_count":2.0,
        "Question_view_count":868.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m getting this warning:<\/p>\n<p>WARNING: Guild took 9.56 seconds to prepare runs. To reduce startup time, try running with \u2018\u2013skip-images\u2019 or \u2018\u2013skip-hparams\u2019 options or reduce the number of runs with filters. Try \u2018guild tensorboard --help\u2019 for filter options.<\/p>\n<p>Adding skip-images or skip-hparams does not help.<\/p>\n<p>I suspect this happens because I have a resource (symbolic link) to a directory with a lot of files. Is it possible to configure guild tensorboard to ignore this directory?<\/p>\n<p>I found the -O option in: <a href=\"https:\/\/my.guild.ai\/t\/command-tensorboard\/127\" class=\"inline-onebox\">Command: tensorboard<\/a><br>\nbut adding -O logdir=tb did not work (had to abort because it never finished).<\/p>\n<p>Any ideas?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Import flags from dependency",
        "Question_creation_time":1631895678247,
        "Question_link":"https:\/\/my.guild.ai\/t\/import-flags-from-dependency\/765",
        "Question_upvote_count":1.0,
        "Question_view_count":216.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have the following guild yaml structure:<\/p>\n<pre><code>prepare-config:\n    main: model.utils.prepare-config\n    sourcecode: \n        root: \/home\/\n        dest: .\n        select: '*.py'\n\ntrain:\n    main: model.utils.run\n    sourcecode: \n        root: \/home\/\n        dest: .\n        select: '*.py'\n    requires:\n      - operation: prepare-config\n        select: prepare-config-output.json\n        target-type: copy\n        target-path: .\n    flags-dest: config:prepare-config-output.json\n    flags-import: all\n<\/code><\/pre>\n<p>In the run directory for the \u201ctrain\u201d operation I can see that the \u201cprepare-config-output.json\u201d file has been copied successfully. However when I perform \u201cguild run train --help-op\u201d I don\u2019t see any of the parameters in \u201cprepare-config-output.json\u201d. I am even able to run \u201cguild run train\u201d successfully using the parameters in \u201cprepare-config-output.json\u201d, but it seems as though the flags are not imported and I cant use something like \u201cguild run train.py lr=[0.1, 0.01, 0.001]\u201d for example.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Redirecting Standard Output",
        "Question_creation_time":1635368459517,
        "Question_link":"https:\/\/my.guild.ai\/t\/redirecting-standard-output\/774",
        "Question_upvote_count":2.0,
        "Question_view_count":397.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a guild file with the following lines:<\/p>\n<pre><code>train:\n  description: Train a model\n  exec: python -m model.utils.run &gt; logfile\n<\/code><\/pre>\n<p>The train command works correctly, however it does not redirect the standard output to \u201clogfile\u201d. I am able to see the standard output in the \u201cLog\u201d tab of the guild view server, but this also contains the output of the tqdm progress bar used in the code, which is not ideal.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pytorch Lightning, Distributed Data Parallel & remote",
        "Question_creation_time":1667510518376,
        "Question_link":"https:\/\/my.guild.ai\/t\/pytorch-lightning-distributed-data-parallel-remote\/953",
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI\u2019ve setup my model with Pytorch Lightning, and want to train it on a remote workstation (single machine, multiple GPUs) using multiple GPUs. The recommended strategy to use is DDP.<\/p>\n<p>I\u2019ve successfully run the model locally on 1 GPU, remotely on 1 GPU using DDP and DP strategies, and on multiple GPUs using DP strategy.<\/p>\n<p>However, when running DDP strategy with multiple GPUs on the remote, the processs hangs indefinitely on the first GPU initialization. The only indication in the Guild output that something is not correct is this:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">Installing package and its dependencies\nProcessing .\/gpkg.my_package-0.1-py2.py3-none-any.whl\nInstalling collected packages: gpkg.my_package\nSuccessfully installed gpkg.my_package-0.1\nStarting my_model:train on my_remote as 65f7f7f516f54c78900bcd313a6f906c\n[some file resolves]\n[some PLT info]\n \n**guild.op_main: missing required arg**\n\nINFO: [pytorch_lightning.utilities.distributed] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1\/2\n<\/code><\/pre>\n<p>The process never progresses from this stage. Running <code>nvidia-smi<\/code> shows no processes on any GPUs, and running <code>htop<\/code> shows no processes on the CPU.<\/p>\n<p>Below the training part of the script:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># Define the Pytorch Lightning trainer\ntrainer = pl.Trainer(\n        # auto_scale_batch_size='binsearch',\n        auto_lr_find=config.auto_lr,\n        fast_dev_run=config.fast_dev_run,\n        max_epochs=config.epochs,\n        accelerator=\"gpu\",\n        strategy='ddp',\n        devices=config.gpus,\n        precision=16,\n        callbacks=[\n            # pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-2),\n            pl.callbacks.EarlyStopping(monitor='val_auc', mode='max'),\n            pl.callbacks.LearningRateMonitor()\n        ]\n)\n\n# Tune the training parameters\ntrainer.tune(model)\n# Train\ntrainer.fit(model=model)\n# Test\ntrainer.test(model=model)\n<\/code><\/pre>\n<p>And the guild.yml file:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">- package: gpkg.my_package\n  description: My package\n  version: 0.1\n  data-files:\n    - '..\/datasets\/'\n    - 'my_model\/models\/model_definition.py'\n    - 'my_model\/train.py'\n    - 'my_model\/models\/loss.py'\n    - 'my_model\/models\/memory.py'\n    - 'my_model\/config.yaml'\n    - '.\/training_utils.py'\n    - '.\/model_utils.py'\n\n- model: my_model\n  description: \n  operations:\n    train:\n      description: Train my model\n      label: \"my_model:train - dataset: ${dataset_name}\"\n      sourcecode:\n        - my_model\/train.py\n        - my_model\/models\/model_definition.py\n        - my_model\/models\/loss.py\n        - my_model\/models\/memory.py\n        - training_utils.py\n        - model_utils.py\n      requires:\n        - config: my_model\/config.yaml\n        - file: ..\/datasets\/\n      main: my_model\/train\n      flags-dest: config:my_model\/config.yaml\n      flags-import: all\n      flags:\n        # Training parameters\n        auto_lr: True\n        epochs: 2\n        fast_dev_run: 100\n        gpus: (0,)\n        av: False\n        # Dataset parameters\n        dataset_name: my_dataset\n        batch_size: 2\n        num_workers: 6\n        model_input_size: (256,256)\n        data_path: '..\/..\/..\/data\/use_case\/my_dataset'\n        fraction: 1.0\n        crop_params: False\n        win_len: 16\n        # Model parameters\n        channels: 3\n        mem_dim: 2000\n        thresh: 0.0025\n        loss_weight: 0.0002\n        lr: 10e-5\n        wd: 10e-4\n<\/code><\/pre>\n<p>Perhaps there is a mistake in the package part? It\u2019s not entirely clear to me if that\u2019s the correct way to do this, and perhaps I\u2019m doing something redundant and\/or incorrect that prevents Guild from properly initializing my scripts?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline depending on multiple of the same operation",
        "Question_creation_time":1655386885562,
        "Question_link":"https:\/\/my.guild.ai\/t\/pipeline-depending-on-multiple-of-the-same-operation\/891",
        "Question_upvote_count":2.0,
        "Question_view_count":86.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have the following setup:<\/p>\n<pre><code class=\"lang-yaml\">- model: my_model\n  operations:\n    train: \n      main: scripts.train\n      flags:\n        a: 1\n    evaluate: \n      main: scripts.evaluate\n      requires:\n          - operation: train\n    train_evaluate:\n      flags:\n        a: 1\n      steps:\n      - run: train a=${a}\n      - run: evaluate\n    compare_evaluate:\n      main: scripts.compare\n      requires:\n        - train_evaluate_run_1\n        - train_evaluate_run_2\n    compare:\n      steps:\n      - run: train_evaluate a=1\n      - run: train_evaluate a=2\n      - run: compare_evaluate # HERE\nresources:\n  train_evaluate_run_1:\n    - operation: train_evaluate\n      name: train_evaluate_run_1\n  train_evaluate_run_2:\n    - operation: train_evaluate\n      name: train_evaluate_run_2\n<\/code><\/pre>\n<p>How to tell guild to resolve two different resources in the <code>compare_evaluate<\/code> run (with the # HERE tag)?<\/p>\n<p>The goal is to have a script\/notebook that takes as input two <code>train_evaluate<\/code> runs and compares them using custom plotting etc. and have all that specified in a single pipeline.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Specify guild home location on SSH remote",
        "Question_creation_time":1651083462913,
        "Question_link":"https:\/\/my.guild.ai\/t\/specify-guild-home-location-on-ssh-remote\/866",
        "Question_upvote_count":1.0,
        "Question_view_count":78.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a remote SSH with a OS disk and a data disk. I want to put my guild runs on the data disk, so on my ssh machine I have set my <code>GUILD_HOME=\/mnt\/guild_runs<\/code>.<\/p>\n<p>When I want to run from my local machine like <code>guild runs --remote ssh-machine<\/code> it cannot find the runs on the remote. When I run with additional debug information I see that the ssh command guild issues overwrites my <code>GUILD_HOME<\/code> env variable.<\/p>\n<p>Is there a way to tell guild where my <code>GUILD_HOME<\/code> is on my remote? I know I can do <code>venv-path<\/code>, but since my venv path and <code>GUILD_HOME<\/code> directory aren\u2019t the same, I don\u2019t think this solves it.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"OperatorNotAllowedInGraphError: Error while using Guild run",
        "Question_creation_time":1619977129009,
        "Question_link":"https:\/\/my.guild.ai\/t\/operatornotallowedingrapherror-error-while-using-guild-run\/697",
        "Question_upvote_count":2.0,
        "Question_view_count":826.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use a code with guild. I checked the code using python  and the code runs successfully. However, when I try to use the code using guild run  it gives me the following error. On looking at the error closely, I found that the code gives me an error in the model.fit function of keras. I am not sure what is happening when we use guild run in this function that is triggering the error?<\/p>\n<p>Any help or suggestion in this matter would be greatly appreciated.<\/p>\n<p>Thanks in advance,<br>\nVishal<\/p>\n<p>Full error below.<\/p>\n<pre><code>File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py\", line 108, in _method_wrapper\n    return method(self, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py\", line 1098, in fit\n    tmp_logs = train_function(iterator)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 780, in __call__\n    result = self._call(*args, **kwds)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 823, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 697, in _initialize\n    *args, **kwds))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/function.py\", line 3213, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/function.py\", line 3075, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/func_graph.py\", line 986, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 600, in wrapped_fn\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/func_graph.py\", line 973, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:\n\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py:806 train_function  *\n        return step_function(self, iterator)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py:799 step_function  **\n        write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py:2757 write_scalar_summaries\n        summary_ops_v2.scalar('batch_' + name, value, step=step)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/python_util.py:321 wrapper\n        cb(self._func, *args, **kw)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/summary_util.py:198 _handle_scalar_ops_v2\n        vals = self._summary_values(step)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/summary_util.py:179 _summary_values\n        return self._summary_cache.for_step(global_step)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/summary_util.py:246 for_step\n        return self._val if step == self._step else None\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py:877 __bool__\n        self._disallow_bool_casting()\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py:487 _disallow_bool_casting\n        \"using a `tf.Tensor` as a Python `bool`\")\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py:474 _disallow_when_autograph_enabled\n        \" indicate you are trying to use an unsupported feature.\".format(task))\n\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed",
        "Question_creation_time":1627001084083,
        "Question_link":"https:\/\/my.guild.ai\/t\/operatornotallowedingrapherror-using-a-tf-tensor-as-a-python-bool-is-not-allowed\/739",
        "Question_upvote_count":3.0,
        "Question_view_count":1672.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When I try to run the mnist example I get the following:<\/p>\n<p>OperatorNotAllowedInGraphError: using a <code>tf.Tensor<\/code> as a Python <code>bool<\/code> is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Flag not recognized: No module named --batch_size",
        "Question_creation_time":1633451275980,
        "Question_link":"https:\/\/my.guild.ai\/t\/flag-not-recognized-no-module-named-batch-size\/770",
        "Question_upvote_count":1.0,
        "Question_view_count":493.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<br>\nout of the blue my guild script now claims:<\/p>\n<blockquote>\n<p>guild: No module named --batch_size<\/p>\n<\/blockquote>\n<p>Batch size is a flag, as defined in the guild.yml<\/p>\n<pre><code>flags:\n        batch_size:\n          default: 32\n<\/code><\/pre>\n<p>The batch size is an argument of the arg parser<\/p>\n<pre><code>        parser.add_argument(\"--batch_size\", type=int, default=8)\n<\/code><\/pre>\n<p>I don\u2019t understand the error message, as it worked before and it is not a syntactic error:<\/p>\n<pre><code>You are about to run model:train\n  batch_size: 32\n<\/code><\/pre>\n<p>Do you have an idea what could have gone wrong?<br>\nI did not change the code and call guild with <code>guild run train<\/code>.<\/p>\n<p>Previously I had a similar error, where the <code>accelerator<\/code> flag of the PyTorch Lightning Trainer wouldn\u2019t be recognized (same error message).<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dependencies problem",
        "Question_creation_time":1666248030613,
        "Question_link":"https:\/\/my.guild.ai\/t\/dependencies-problem\/943",
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! Pretty new to guild.ai and trying to use it on my recent project. I tried to run my code with guild.ai wrapper, but it does not run due to a dependencies issue (\u201cbpe_simple_vocab_16e6.txt.gz\u201d is missing from the running folder, despite every other file in the same folder with it being included). I tried to solve it by including requires in my guild.yml:<\/p>\n<p>requires:<\/p>\n<ul>\n<li>file: pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz<\/li>\n<\/ul>\n<p>but despite the relative path is correct, the error pops up saying<\/p>\n<p>ERROR: resolving required source \u2018file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz\u2019 in file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz resource<br>\nTraceback (most recent call last):<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/op_dep.py\u201d, line 236, in resolve_source<br>\nsource_paths = _resolve_source_for_location(<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/op_dep.py\u201d, line 267, in _resolve_source_for_location<br>\nreturn resolver.resolve(resolve_context)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 114, in resolve<br>\nresolved = self._resolve_source_files(source_path, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 138, in _resolve_source_files<br>\nreturn resolve_source_files(source_path, self.source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 553, in resolve_source_files<br>\nreturn _resolve_source_file_or_archive_files(source_path, source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 618, in _resolve_source_file_or_archive_files<br>\nreturn _resolve_archive_files(source_path, archive_type, source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 674, in _resolve_archive_files<br>\nunpacked = _ensure_unpacked(source_path, archive_type, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 686, in _ensure_unpacked<br>\nunpacked = _unpack(source_path, archive_type, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 711, in _unpack<br>\nreturn _gunzip(source_path, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 749, in _gunzip<br>\nreturn _gen_unpack(<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 806, in _gen_unpack<br>\nextract(unpack_dir, to_extract)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 781, in extract<br>\nwith open(dest, \u201cwb\u201d) as f_out:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018\/home\/hyang\/deadclip\/CyCLIP\/env\/.guild\/cache\/resources\/6d8d1935c2ed1916a4210b8f6391e63d846c5f02d88deced8f8fa21b\/bpe_simple_vocab_16e6.txt\u2019<br>\nguild: run failed because a dependency was not met: unexpected error resolving \u2018file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz\u2019 in file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz resource: FileNotFoundError(2, \u2018No such file or directory\u2019)<\/p>\n<p>I also tried to toy with the relative path with \u201c\u2026\u201d or use the absolute path, but it still didn\u2019t work<\/p>\n<p>Any help would be appreciated :))<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dynamically generated parameters in pipeline",
        "Question_creation_time":1620135807746,
        "Question_link":"https:\/\/my.guild.ai\/t\/dynamically-generated-parameters-in-pipeline\/699",
        "Question_upvote_count":0.0,
        "Question_view_count":249.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Conceptually I have a two stage pipeline. Where the first stage generates a set of flags (\u201chyper-hyper parameters\u201d). Then in the second stage I want to combine those with a set of hyper parameters to optimize. The challenge is that since they\u2019re created dynamically\u2026 I can\u2019t know ahead of time how many there are.<\/p>\n<p>I can do it manually like this<\/p>\n<p><code>guild run train x='[1,2,3] y='[1,2,3]'' @bigbatch.csv<\/code><\/p>\n<p>What I would like to do is for the train step to use a generated bigbatch.csv from the upstream pipeline<\/p>\n<p>I\u2019ve attached what I think it should look like at the guild.yml level<\/p>\n<pre><code>train:\n  description: Sample training script\n  flags-import: all\n  requires:\n    - operation: bigbatch\nbigbatch:\n  description: make file bigbatch.csv\n<\/code><\/pre>\n<p>This gives me  a symlink to the correct file called bigbatch.csv in the \u201ctrain folder\u201d after the <code>guild train<\/code> operation. However when I use the \u201c@\u201d batch notation the bigbatch.csv is taken from my cwd. Is there any way to reference batch parameters in the guild.yml?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Random search unexpectedly failling with TypeError",
        "Question_creation_time":1595019412387,
        "Question_link":"https:\/\/my.guild.ai\/t\/random-search-unexpectedly-failling-with-typeerror\/240",
        "Question_upvote_count":1.0,
        "Question_view_count":376.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I was unable to replicate this in a minimalistic example so I didn\u2019t create an issue. I\u2019m guessing my guild.yml file is very incorrect or something. Might also be some package versions that are incorrect in this venv?<\/p>\n<p>Command:<\/p>\n<pre><code>guild run train batch_size=[16,32] --optimizer random\n<\/code><\/pre>\n<p>guild.yml snippet:<\/p>\n<pre><code>    train:\n      main: train\n      flags-import: all\n      output-scalars: off\n      requires:\n        - database\n<\/code><\/pre>\n<p>train.py snippet:<\/p>\n<pre><code>parser.add_argument('--batch_size', type=int, default=256)\n<\/code><\/pre>\n<p>Stacktrace:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 192, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/guild\/plugins\/random_main.py\", li\nne 23, in &lt;module&gt;\n    from . import skopt_util\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/guild\/plugins\/skopt_util.py\", line 27, in &lt;module&gt;\n    import skopt\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/skopt\/__init__.py\", line 44, in &lt;module&gt;\n    from . import callbacks\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/skopt\/callbacks.py\", line 17, in &lt;module&gt;\n    from skopt.utils import dump\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/skopt\/utils.py\", line 3, in &lt;module&gt;\n    from sklearn.utils import check_random_state\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/__init__.py\", line 64, in &lt;module&gt;\n    from .base import clone\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/base.py\", line 14, in &lt;module&gt;\n    from .utils.fixes import signature\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/utils\/__init__.py\", line 14, in &lt;module&gt;\n    from . import _joblib\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/utils\/_joblib.py\", line 22, in &lt;module&gt;\n    from ..externals import joblib\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/__init__.py\", line 119, in &lt;module&gt;\n    from .parallel import Parallel\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/parallel.py\", line 28, in &lt;module&gt;\n    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/_parallel_backends.py\", line 22, in &lt;module&gt;\n    from .executor import get_memmapping_executor\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/executor.py\", line 14, in &lt;module&gt;\n    from .externals.loky.reusable_executor import get_reusable_executor\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/loky\/__init__.py\", line 12, in &lt;module&gt;\n    from .backend.reduction import set_loky_pickler\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/loky\/backend\/reduction.py\", line 125, in &lt;module&gt;\n    from sklearn.externals.joblib.externals import cloudpickle  # noqa: F401\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/cloudpickle\/__init__.py\", line 3, in &lt;module&gt;\n    from .cloudpickle import *\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/cloudpickle\/cloudpickle.py\", line 152, in &lt;module&gt;\n    _cell_set_template_code = _make_cell_set_template_code()\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/cloudpickle\/cloudpickle.py\", line 133, in _make_cell_set_template_code\n    return types.CodeType(\nTypeError: an integer is required (got type bytes)\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tensorboard logging twice + is slow",
        "Question_creation_time":1603173353724,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-logging-twice-is-slow\/423",
        "Question_upvote_count":1.0,
        "Question_view_count":1225.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I  have an operation, let\u2019s call it <code>a<\/code> that is kinda the \u201cbase operation\u201d that I could run a number ways with different flags.  Then I have another operation <code>b<\/code> that has one step  that runs operation <code>a<\/code> with the proper flags.<\/p>\n<p>What\u2019s strange to me is that I seem to get multiple of everything I logged with tensorboard. Additionally, <code>guild view<\/code>  is pretty  slow to open  my runs,  and viewing in tensorboard from  there is much slower (takes 10+s) . I saw someone had an issue with symlinks but I don\u2019t think that\u2019s the issue here since I don\u2019t have any set up. I feel like these issues are probably linked, considering I wasn\u2019t having this problem before. If I run just 1 configuration I ended up with:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png\" data-download-href=\"\/uploads\/short-url\/ppPT2noEZFMHbXQsJsQEYjoFJoF.png?dl=1\" title=\"Screen Shot 2020-10-19 at 10.35.07 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_690x162.png\" alt=\"Screen Shot 2020-10-19 at 10.35.07 PM\" data-base62-sha1=\"ppPT2noEZFMHbXQsJsQEYjoFJoF\" width=\"690\" height=\"162\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_690x162.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2020-10-19 at 10.35.07 PM<\/span><span class=\"informations\">779\u00d7184 45.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg><\/div><\/a><\/div><\/p>\n<p>Not sure how to go about fixing this, I probably  did something wrong.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About the Troubleshooting category",
        "Question_creation_time":1591308511920,
        "Question_link":"https:\/\/my.guild.ai\/t\/about-the-troubleshooting-category\/13",
        "Question_upvote_count":0.0,
        "Question_view_count":363.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p><a href=\"\/new-topic?category=troubleshooting\">Get help<\/a> with a problem you\u2019re facing. If you have a general question, use <a href=\"\/c\/general\">General<\/a>.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"TensorBoard Hparm Parallel Coordinate View Problem",
        "Question_creation_time":1629491848267,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-hparm-parallel-coordinate-view-problem\/753",
        "Question_upvote_count":0.0,
        "Question_view_count":219.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m having an issue with the TensorBoard Hparm Parallel Coordinate View. It seems to be duplicated the axes on the lower half.<\/p>\n<p>This happens when I make a new project and just follow the get-started.ipynb file.<\/p>\n<p>I\u2019m using a Mac and Safari browser, and here are the versions info:<\/p>\n<pre><code>guild_version:             0.7.3\ntensorboard_version:       2.6.0\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb.jpeg\" data-download-href=\"\/uploads\/short-url\/oDDDcjhGMAurXRar09EHL2ltw3p.jpeg?dl=1\" title=\"Screen Shot 2021-08-20 at 4.24.55 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg\" alt=\"Screen Shot 2021-08-20 at 4.24.55 PM\" data-base62-sha1=\"oDDDcjhGMAurXRar09EHL2ltw3p\" width=\"517\" height=\"336\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_775x504.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_1034x672.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2021-08-20 at 4.24.55 PM<\/span><span class=\"informations\">1920\u00d71249 181 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Additionally, when I remove one of the variables, the entire plot grows at least 30% in size. Is there a way I can keep this constant.<\/p>\n<p>Lastly, runs in a notebook don\u2019t seem to save an output.index file and guild view throws an error from this.<\/p>\n<p>Thanks for the help! Let me know if I need to share anything else.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remote connection error with jump host",
        "Question_creation_time":1644268231237,
        "Question_link":"https:\/\/my.guild.ai\/t\/remote-connection-error-with-jump-host\/807",
        "Question_upvote_count":0.0,
        "Question_view_count":239.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nmy institution has recently changed the configuration of our remote workstations.<\/p>\n<p>Now the connection goes through a jump host, and we cannot use a ssh pair here. I have a proxy configured, so manually I connect to the workstation with \u2018ssh [workstation]\u2019. The jump host requires a password on every connection, followed by an app authentication. The workstation has a ssh pairing with my local machine, so I only have to login to the jump host. That\u2019s the policy, and cannot be changed.<\/p>\n<p>I have successfully manged to run a guild check on that remote. I have configured a training script, config files etc. so that it all runs smoothly locally.<\/p>\n<p>However, when I try to run the train operation on the remote, I get the following errors:<\/p>\n<pre><code>Initializing remote run\nPassword: \nCopying package\nPassword: \nConnection timed out during banner exchange\nrsync: connection unexpectedly closed (0 bytes received so far) [sender]\nrsync error: unexplained error (code 255) at io.c(235) [sender=3.1.3]\nTraceback (most recent call last):\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/bin\/guild\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/main_bootstrap.py\", line 40, in main\n    _main()\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1659, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\n    return __callback(*args, **kwargs)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run.py\", line 649, in run\n    run_impl.main(args)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1514, in main\n    _dispatch_op(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1610, in _dispatch_op\n    _dispatch_op_cmd(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1797, in _dispatch_op_cmd\n    _confirm_and_run(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1874, in _confirm_and_run\n    _run(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 2075, in _run\n    _run_remote(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 2082, in _run_remote\n    remote_impl_support.run(_remote_args(S))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/remote_impl_support.py\", line 125, in run\n    run_id = remote.run_op(**_run_kw(args))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh.py\", line 243, in run_op\n    remote_run_dir = self._init_remote_run(tmp.path, opspec, restart)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh.py\", line 265, in _init_remote_run\n    self._copy_package_dist(package_dist_dir, remote_run_dir)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh.py\", line 330, in _copy_package_dist\n    ssh_util.rsync_copy_to(\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh_util.py\", line 129, in rsync_copy_to\n    subprocess.check_call(cmd)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/subprocess.py\", line 364, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['rsync', '-vr', '-e', \"ssh -oConnectTimeout=10 -o 'ProxyCommand ssh -oConnectTimeout=100  -W %h:%p [user]@[jumphost]'\", '\/tmp\/guild-remote-stage-ahx9az7p\/', '[user]@[workstation]:~\/anaconda3\/envs\/time-gop\/.guild\/runs\/5d5d24d410c648f897630ef102538a1e\/.guild\/job-packages\/']' returned non-zero exit status 255.\n<\/code><\/pre>\n<p>I\u2019m curious about two things:<\/p>\n<ul>\n<li>Why do I have to login twice, once after \u2018Initializing remote run\u2019 log, and then again after \u2018Copying package\u2019 log?<\/li>\n<li>I have set up my remotes in the guild\/config.yml to have a timeout of 100 seconds for both the jump host and the second step connection. However, from the trace it seems that the guild\/config.yml timeout is not properly read?<\/li>\n<\/ul>\n<p>This is the guild\/config.yml:<\/p>\n<pre><code>remotes:\n [remote-name]:\n  type: ssh\n  host: [workstation]\n  proxy: ssh -oConnectTimeout=100 -W %h:%p [user]@[jump host]\n  connect-time: 100\n  user: [user]\n  conda-env: ~\/anaconda3\/envs\/time-gop  \n  init: source ~\/anaconda3\/etc\/profile.d\/conda.sh | guild -H ~\/projects\/protime-gop\n<\/code><\/pre>\n<p>the obvious reason would be that the connection times out, as per the error log. However, the config timeout value doesn\u2019t seem to actually change the value invoked with the remote command.<\/p>\n<p>Have I made a mistake while creating my guild\/config.yml? Or it is a bug? Or maybe some other reason?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"sqlite3.OperationalError: disk I\/O error when using the scratch drive on Linux cluster for storage of guild runs",
        "Question_creation_time":1618280824715,
        "Question_link":"https:\/\/my.guild.ai\/t\/sqlite3-operationalerror-disk-i-o-error-when-using-the-scratch-drive-on-linux-cluster-for-storage-of-guild-runs\/684",
        "Question_upvote_count":0.0,
        "Question_view_count":255.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use Guild for hyperparameter optimization. I am running max-trials of 50 and want to store these temporary models in the \/scratch drive on the Linux cluster. I checked that the drive is mounted corrected and I am able to read and write properly in the drive. However, when i submit my guild run, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"\/usr\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.6\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_gp_main.py\", line 77, in &lt;module&gt;\n    main()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_gp_main.py\", line 30, in main\n    skopt_util.handle_seq_trials(batch_run, _suggest_x)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 210, in handle_seq_trials\n    _run_seq_trials(batch_run, suggest_x_cb)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 234, in _run_seq_trials\n    batch_flag_vals,\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 266, in _iter_seq_trials\n    prev_trials = prev_trials_cb()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 224, in &lt;lambda&gt;\n    prev_trials_cb = lambda: batch_util.trial_results(batch_run, [objective_scalar])\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/batch_util.py\", line 404, in trial_results\n    return trial_results_for_runs(trial_runs(batch_run), scalars)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/batch_util.py\", line 408, in trial_results_for_runs\n    index = _run_index_for_scalars(runs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/batch_util.py\", line 423, in _run_index_for_scalars\n    index = indexlib.RunIndex()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/index.py\", line 314, in __init__\n    self._db = self._init_db()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/index.py\", line 323, in _init_db\n    self._init_tables(db)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/index.py\", line 349, in _init_tables\n    \"\"\"\nsqlite3.OperationalError: disk I\/O error\n<\/code><\/pre>\n<p>I checked my \/scratch drive and found that the runs and cache folders are created. Also found that one folder was created inside runs. But this folder was empty.<\/p>\n<p>The same command works perfectly when I run using my GUILD_HOME as a different drive. I am not sure if I am missing anything here.<\/p>\n<p>I would appreciate any help from your side.<\/p>\n<p>Thanks,<br>\nVishal<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to specify flags-import: all with remote runs",
        "Question_creation_time":1628617693656,
        "Question_link":"https:\/\/my.guild.ai\/t\/unable-to-specify-flags-import-all-with-remote-runs\/749",
        "Question_upvote_count":0.0,
        "Question_view_count":291.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have an operation in that looks like this<\/p>\n<pre><code>  operations:\n    prepare:\n      main: operations.prepare\n      flags-import: all\n      output-scalars: off\n<\/code><\/pre>\n<p>which has a number of different flags specified with argparse in <code>operations\/prepare.py<\/code>. I can run it just fine locally, but when I try run it remote with any flag set I get <code>guild: unsupported flag &lt;flag&gt;<\/code> for every flag. I can run it remote as long as I don\u2019t set flags. Also, if I specify the flags explicitly in <code>guild.yml<\/code> like this it works fine:<\/p>\n<pre><code>    prepare:\n      main: operations.prepare\n      flags:\n        &lt;flag1&gt;:\n        &lt;flag2&gt;:\n      output-scalars: off\n<\/code><\/pre>\n<p>Any idea what might be wrong here?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best way to specify boolean argparse flags",
        "Question_creation_time":1647534360656,
        "Question_link":"https:\/\/my.guild.ai\/t\/best-way-to-specify-boolean-argparse-flags\/834",
        "Question_upvote_count":0.0,
        "Question_view_count":122.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>What is the best way to combine argparse and guild with a boolean flag?<\/p>\n<p>I currently do something like this:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># In python argparse\ndata.add_argument('--enable_augmentation', dest='enable_augmentation', action='store_true',\n                    help=\"Whether or not apply augmentation on training dataset\")\n\n<\/code><\/pre>\n<p>And<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># guild.yml\n  enable_augmentation:\n    default: no\n    arg-switch: yes\n    type: boolean\n<\/code><\/pre>\n<p>Is this the preferred way?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Stage trials error",
        "Question_creation_time":1627609081354,
        "Question_link":"https:\/\/my.guild.ai\/t\/stage-trials-error\/741",
        "Question_upvote_count":1.0,
        "Question_view_count":241.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve tested the <code>--stage-trials<\/code> feature with <code>queues<\/code> today, all worked great. But now I call <code>guild run op--staged-trials<\/code>, and check with <code>guild runs<\/code>, some runs are staged, others show errors<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c.png\" data-download-href=\"\/uploads\/short-url\/kxgoN75AKZNvWS4wd90nbgU9u0I.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c.png\" alt=\"image\" data-base62-sha1=\"kxgoN75AKZNvWS4wd90nbgU9u0I\" width=\"690\" height=\"175\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">753\u00d7192 5.41 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>A minute later I try <code>guild runs<\/code> again, and all runs show as <code>error<\/code> without me having done anything. What could cause the staged trials to flip to errors all of a sudden? This always happens now.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241.png\" data-download-href=\"\/uploads\/short-url\/8VBYFfkCYF3cwNgAMfGh4A2J561.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241.png\" alt=\"image\" data-base62-sha1=\"8VBYFfkCYF3cwNgAMfGh4A2J561\" width=\"690\" height=\"176\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">757\u00d7194 5.21 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Rearranging columns in guild compare",
        "Question_creation_time":1607455711413,
        "Question_link":"https:\/\/my.guild.ai\/t\/rearranging-columns-in-guild-compare\/496",
        "Question_upvote_count":1.0,
        "Question_view_count":219.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there!<\/p>\n<p>First off, thanks a lot for making Guild!<\/p>\n<p>I was wondering if there is a way to rearrange the columns in <code>guild compare<\/code>? It seems like by default the scalars come after the flags whereas I\u2019d like for it to be the other way round.<\/p>\n<p>Second, I\u2019m noticing that if I use <code>guild compare -cc<\/code> followed by a comma-separated list of column names which include both flags and scalars, only the scalars get outputted correctly; the values for the flag columns are empty.<\/p>\n<p>FWIW I\u2019m on Guild version 0.7.1:<\/p>\n<pre><code>% pip freeze | grep guildai                                                                                                                               20-12-08 - 14:26:45\nguildai==0.7.1\n<\/code><\/pre>\n<p>EDIT: here is a quick example to reproduce things.<\/p>\n<p>Here\u2019s my toy <code>guild.yml<\/code> file:<\/p>\n<pre><code>- operations:\n    quick_example:\n      description: \"Quick example\"\n      exec: \"echo \\\"Loss: 0.123\\\"\"\n      flags:\n        log_file:\n          type: string\n          default: \"example.log\"\n      output-scalars:\n        - loss: 'Loss: (\\value)'\n<\/code><\/pre>\n<p>If I run <code>guild run quick_example<\/code> and then <code>guild_compare<\/code>, I see<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8.png\" data-download-href=\"\/uploads\/short-url\/vezbGnqgINBlCyo0RrLJKtjFQLK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_517x42.png\" alt=\"image\" data-base62-sha1=\"vezbGnqgINBlCyo0RrLJKtjFQLK\" width=\"517\" height=\"42\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_517x42.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_775x63.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_1034x84.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1350\u00d7110 5.76 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg><\/div><\/a><\/div><\/p>\n<p>However, if I run <code>guild compare -cc log_file,loss<\/code> I get<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/30bf182e19fbc50a4b89c31a42ed85cc964b4039.png\" alt=\"image\" data-base62-sha1=\"6XekRJDBHwfTbaXShdnSESkYNMR\" width=\"377\" height=\"99\"><\/p>\n<p>Thanks a lot in advance!<br>\njks<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remote stop not working",
        "Question_creation_time":1617118654840,
        "Question_link":"https:\/\/my.guild.ai\/t\/remote-stop-not-working\/584",
        "Question_upvote_count":0.0,
        "Question_view_count":264.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>After running <code>guild runs stop X -r server<\/code>, the processes are still running on the remote and GPU memory has not been released even though guild reports the run as terminated.<\/p>\n<p>I think it may be pytorch\u2019s data loader worker processes that are still running but I\u2019m not sure. I think I saw something about this subject before but I couldn\u2019t find it here or on github. Has anyone else experienced this issue?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error with gp: TypeError: '<' not supported between instances of 'Version' and 'tuple'",
        "Question_creation_time":1600932087603,
        "Question_link":"https:\/\/my.guild.ai\/t\/error-with-gp-typeerror-not-supported-between-instances-of-version-and-tuple\/352",
        "Question_upvote_count":0.0,
        "Question_view_count":1203.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>When I run the execute run as grid serach it works as expected (there are lots of flags):<\/p>\n<pre><code>guild run --max-trials 2 lightgbm:train num_leaves=range[40:150:10] max_depth=range[2:7:1] learning_rate=range[0.01:0.1:0.01] boosting_type=[gbdt] colsample_bytree=[0.75,0.8,0.85,0.9,0.95] subsample=[0.75,0.8,0.85,0.9,0.95] min_child_samples=[0.5,1.0,2.0,3.0,4.0,5.0,10.0] n_estimators=[100,250,500,1000]\n<\/code><\/pre>\n<p>But when I specify the gp optimizer:<\/p>\n<pre><code>guild run --max-trials 2 lightgbm:train num_leaves=range[40:150:10] max_depth=range[2:7:1] learning_rate=range[0.01:0.1:0.01] boosting_type=[gbdt] colsample_bytree=[0.75,0.8,0.85,0.9,0.95] subsample=[0.75,0.8,0.85,0.9,0.95] min_child_samples=[0.5,1.0,2.0,3.0,4.0,5.0,10.0] n_estimators=[100,250,500,1000] --maximize mean_score --optimizer gp\n<\/code><\/pre>\n<p>I get an error:<\/p>\n<pre><code>Continue? (Y\/n)\nTraceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_gp_main.py\", line 83, in &lt;module&gt;\n    main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_gp_main.py\", line 36, in main\n    skopt_util.handle_seq_trials(batch_run, _suggest_x)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 209, in handle_seq_trials\n    _run_seq_trials(batch_run, suggest_x_cb)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 225, in _run_seq_trials\n    for trial_flag_vals, is_trial_random_start, prev_trials, x0 in _iter_seq_trials(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 263, in _iter_seq_trials\n    suggested_x, random_state = _suggest_x(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 370, in _suggest_x\n    return suggest_x_cb(dims, x0, y0, is_random_start, random_state, suggest_opts)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_gp_main.py\", line 40, in _suggest_x\n    res = skopt.gp_minimize(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\gp.py\", line 264, in gp_minimize\n    return base_minimize(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\base.py\", line 271, in base_minimize\n    next_x = optimizer.ask()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\", line 332, in ask\n    return self._ask()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\", line 398, in _ask\n    return self.space.rvs(random_state=self.rng)[0]\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\space\\space.py\", line 764, in rvs\n    if sp_version &lt; (0, 16):\nTypeError: '&lt;' not supported between instances of 'Version' and 'tuple'\n<\/code><\/pre>\n<p>Here is the train script: <a href=\"https:\/\/github.com\/MislavSag\/trademl\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/MislavSag\/trademl<\/a><br>\nThe operation I run is the <code>- model: lightgbm<\/code><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to view logs from trials\/batches",
        "Question_creation_time":1599571635016,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-view-logs-from-trials-batches\/334",
        "Question_upvote_count":1.0,
        "Question_view_count":255.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I usually use <code>guild cat --output 89549e30 | less +G<\/code> to view the stdout logs, but it seems like there is a separate logging channel for trial-level operations. How would one access these logs?<\/p>\n<p><code>ERROR: [guild] trial efb7377f334b4b5da8411e2437771e91 exited with an error (see log for details)<\/code><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running jobs show as errors on cluster",
        "Question_creation_time":1628278406504,
        "Question_link":"https:\/\/my.guild.ai\/t\/running-jobs-show-as-errors-on-cluster\/746",
        "Question_upvote_count":0.0,
        "Question_view_count":233.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Not sure if this is a bug or a feature request, but here is the issue: I stage a bunch of jobs on computer 1, part of a cluster with a shared file system. When I call <code>guild runs<\/code> on any computer in the cluster, it will properly show all staged files. If I start a queue on computer 1 it will promptly launch the first staged job:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd.png\" data-download-href=\"\/uploads\/short-url\/anGnutFiNKGSAB3mEKYpbp9fAgR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd.png\" alt=\"image\" data-base62-sha1=\"anGnutFiNKGSAB3mEKYpbp9fAgR\" width=\"690\" height=\"15\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1095\u00d725 1.94 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>If I then log into computer 2, the running job will show as error:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f.png\" data-download-href=\"\/uploads\/short-url\/laL2L6avIgccXVOeb1x78OHbgIf.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f.png\" alt=\"image\" data-base62-sha1=\"laL2L6avIgccXVOeb1x78OHbgIf\" width=\"690\" height=\"25\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1158\u00d743 3.29 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Why would it show as error instead of just \u201crunning\u201d?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Scalar not saved if pipeline is used",
        "Question_creation_time":1599736313219,
        "Question_link":"https:\/\/my.guild.ai\/t\/scalar-not-saved-if-pipeline-is-used\/339",
        "Question_upvote_count":0.0,
        "Question_view_count":333.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Here is my guild file:<\/p>\n<aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/guild.yml\" target=\"_blank\" rel=\"nofollow noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/guild.yml\" target=\"_blank\" rel=\"nofollow noopener\">MislavSag\/trademl\/blob\/master\/guild.yml<\/a><\/h4>\n<pre><code class=\"lang-yml\">- config: model-base\n  resources:\n    prepared-data:\n      - operation: prepare-data\n\n- operations:\n    prepare-data:\n      main: trademl.modeling.prepare\n      flags-import: all\n      flags:\n        input_path:\n          description: Path to read data from. \n          arg_name: input_path\n          type: string\n          default: D:\/market_data\/usa\/ohlcv_features\n        output_path:\n          description: Main path where to save output\n          arg_name: output_path\n          type: string\n          default: D:\/algo_trading_files\n<\/code><\/pre>\n\n  This file has been truncated. <a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/guild.yml\" target=\"_blank\" rel=\"nofollow noopener\">show original<\/a>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>If I run prepare or random-forest operation it saves the scalars.<\/p>\n<p>But if I run the pipeline <code>pipeline-rf-opt<\/code> that includes prepare and random-forst as step, it doesn\u2019t save scalars. I call it like this:<\/p>\n<pre><code class=\"lang-command\">guild run pipeline-rf-opt \\\n  data-include_ta=1 \\\n  data-label_tuning=0 \\\n  data-label=[day_5] \\\n  data-pca=0 \\\n  data-tb_volatility_lookback=[50] \\\n  data-tb_volatility_scaler=1.0 \\\n  data-correlation_threshold=0.95 \\\n  data-scaling='none' \\\n  random-input_data_path='D:\/algo_trading_files' \\\n  random-forest-depth=4 \\\n  random-forest-maxf=10 \\\n  random-n_estimators=350 \\\n  random-min_weight_fraction_leaf=0.1\n<\/code><\/pre>\n<p>It is just one run.<\/p>\n<p>What could be the reason it doesn\u2019t save scalars?<\/p>\n<p>But it saves flags.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Distributed training hanging",
        "Question_creation_time":1651096364646,
        "Question_link":"https:\/\/my.guild.ai\/t\/distributed-training-hanging\/867",
        "Question_upvote_count":1.0,
        "Question_view_count":1079.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Python: 3.9.12<br>\nPytorch: \u20181.11.0+cu102\u2019<br>\nPytorch-lightning: 1.6.1<br>\nguildai: 0.8.0<\/p>\n<p>I\u2019m trying to run distributed training (DDP) on my 1 machine with 4 GPUs. It works fine if I just run it normally with python. However, when I run with guild, it hangs.<\/p>\n<pre><code class=\"lang-python\">INFO: [pytorch_lightning.utilities.distributed] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1\/4\nINFO: [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:1 to store for rank: 0\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\n...\nTraceback (most recent call last):\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/main.py\", line 223, in &lt;module&gt;\n    main()\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/main.py\", line 81, in main\n    results = get_imputation_logic(args)(args, data)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/task_logic\/ae_imputation.py\", line 101, in ae_imputation_logic\n    ae_imputer = create_autoencoder_with_tuning(args, data, settings)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/utils\/tuner.py\", line 43, in create_autoencoder_with_tuning\n    ae_imputer.fit(data)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 128, in fit\n    self._fit(data.longitudinal, \"longitudinal\")\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 139, in _fit\n    self.trainer[longitudinal_or_static].fit(ae, datamodule=data)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 768, in fit\n    self._call_and_handle_interrupt(\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 719, in _call_and_handle_interrupt\n    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 93, in launch\n    return function(*args, **kwargs)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 809, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1169, in _run\n    self.strategy.setup_environment()\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/ddp.py\", line 151, in setup_environment\n    self.setup_distributed()\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/ddp.py\", line 191, in setup_distributed\n    init_dist_connection(self.cluster_environment, self._process_group_backend)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 354, in init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/torch\/distributed\/distributed_c10d.py\", line 627, in init_process_group\n    _store_based_barrier(rank, store, timeout)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/torch\/distributed\/distributed_c10d.py\", line 255, in _store_based_barrier\n    raise RuntimeError(\nRuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\n<\/code><\/pre>\n<p>Some related errors:<\/p>\n<ul>\n<li><a href=\"https:\/\/discuss.pytorch.org\/t\/timed-out-initializing-process-group-in-store-based-barrier\/119803\/9\" rel=\"noopener nofollow ugc\">Timed out initializing process group in store based barrier<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/pytorch\/pytorch\/issues\/52848\" rel=\"noopener nofollow ugc\">Pytorch Issue<\/a><\/li>\n<\/ul>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Storing the guild artifacts on s3 using `guild run -r s3-dev`",
        "Question_creation_time":1605200901861,
        "Question_link":"https:\/\/my.guild.ai\/t\/storing-the-guild-artifacts-on-s3-using-guild-run-r-s3-dev\/449",
        "Question_upvote_count":4.0,
        "Question_view_count":483.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I wanted to store all my runs in s3 bucket directly instead of the local guildai home directory. So,  I configured my <code>~\/.guild\/config.yml<\/code> in the following way which I thought will setup my s3 remote location and I can automatically save the sourcecode and artifacts that I save locally into s3 bucket.<\/p>\n<pre><code>remotes:\n  s3-dev:\n    type: s3\n    description: Production runs\n    bucket: cortex-model-data\n    region: eu-central-1\n<\/code><\/pre>\n<p>and I have my <code>guild.yml<\/code>as follows<\/p>\n<pre><code>- model: AlwaysPredictMean\n  description: A dummy model which always predicts mean\n  operations:\n    train:\n      description: Training Pipeline Sample Code\n      main: training\/train\n      flags-import: all\n      output-scalars: '(\\key): (\\value)'\n<\/code><\/pre>\n<p>when I run the script using <code>guild run --remote s3-dev<\/code> , I get the following error message<\/p>\n<pre><code>\u00b1 |feature\/guildai U:1 \u2717| \u2192 guild run -r s3-dev\nYou are about to run AlwaysPredictMean:train on s3-dev\n  comment: Description for a given training run\n  config: training\/config\/example.yml\n  data: tests\/test_df.csv\n  epochs: 10\n  model_class: training.example_model::AlwaysPredictMean\n  use_case: example_use_case\nContinue? (Y\/n) y\nguild: remote 's3-dev' does not support this operation\n<\/code><\/pre>\n<p>Can someone let me know what exactly is the problem and why can\u2019t I use guild.ai to store in the specified s3. Thanks<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get run ID before\/during the corresponding run",
        "Question_creation_time":1648140890873,
        "Question_link":"https:\/\/my.guild.ai\/t\/get-run-id-before-during-the-corresponding-run\/844",
        "Question_upvote_count":0.0,
        "Question_view_count":121.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey, my issue is the following:<br>\nDuring my run I am training several models and save their checkpoint. In the end I need to load them all for the final testing procedure. However, since the models are saved in  <code>.guild\/runs\/ID\/my_saved_model.ckpt<\/code> and I don\u2019t know the current <code>ID<\/code>, there is no way to re-load the models into my training script. Is there a way to pass the current\/future <code>ID<\/code> into my python script?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"WinError 1314 when trying to do a grid search",
        "Question_creation_time":1642757463278,
        "Question_link":"https:\/\/my.guild.ai\/t\/winerror-1314-when-trying-to-do-a-grid-search\/800",
        "Question_upvote_count":0.0,
        "Question_view_count":448.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI\u2019m having trouble when trying to do a search with guild on a Windows 10 machine.<\/p>\n<p>I took the example code fromt the tutorial<\/p>\n<pre><code>import numpy as np\n\n# Hyperparameters\nx = 0.1\nnoise = 0.1\n\n# Simulated training loss\nloss = (np.sin(5 * x) * (1 - np.tanh(x ** 2)) + np.random.randn() * noise)\n\nprint(\"loss: %f\" % loss)\n<\/code><\/pre>\n<p>I\u2019m running it from console with<\/p>\n<pre><code>guild run test.py x=\"[1,0,-1]\"\n<\/code><\/pre>\n<p>When executing i get an error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"c:\\users\\x\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\users\\x\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_main.py\", line 41, in &lt;module&gt;\n    main()\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_main.py\", line 27, in main\n    batch_util.handle_trials(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 73, in handle_trials\n    _run_trials(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 135, in _run_trials\n    trial_runs = _init_trial_runs(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 144, in _init_trial_runs\n    return [init_trial_run(batch_run, trial) for trial in trials]\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 144, in &lt;listcomp&gt;\n    return [init_trial_run(batch_run, trial) for trial in trials]\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 149, in init_trial_run\n    _link_to_trial(batch_run, run)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 168, in _link_to_trial\n    os.symlink(rel_trial_path, trial_link)\nOSError: [WinError 1314] A required privilege is not held by the client.\n\n '..\\\\749cba372c474a1598aac11e2d4fb902' -&gt; 'C:\\\\Users\\\\x\\\\Anaconda3\\\\.guild\\\\runs\\\\045f73f210fb463f814f3334fd30825d\\\\749cba372c474a1598aac11e2d4fb902'\n<\/code><\/pre>\n<p>I don\u2019t have administrator permissions on the machine and I\u2019m probably not going to get them.<\/p>\n<p>Installing it via pip with --User flag doesnt fix it, either.<\/p>\n<p>Can I fix this somehow without having to run as admin?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Flags are not being tracked\/captured during a guild run inside a docker container",
        "Question_creation_time":1622846349934,
        "Question_link":"https:\/\/my.guild.ai\/t\/flags-are-not-being-tracked-captured-during-a-guild-run-inside-a-docker-container\/717",
        "Question_upvote_count":0.0,
        "Question_view_count":289.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to execute an experiment.py file that is called upon in guild.yml. The guild.yml file contains 5 flags for an svm experiment on the iris dataset. When I execute \u2018guild run model:train\u2019 on my local machine in a terminal, there are no issues and the <code>flags<\/code> are properly captured in the \/.guild directory. However, when I execute the experiment in a Docker container, the flags are no longer being tracked by guild. Is there any way to debug this??<br>\nOr where in the source code are the flags picked up by guild to create the Flags file in \/.guild directory.<br>\nPlease help!<\/p>\n<p>guild.yml file:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9917a5497d13a1a0217004c62eaee40ae9117b5f.png\" alt=\"image\" data-base62-sha1=\"lQjIeUjvD969jrZdz9AUmi1LCwL\" width=\"542\" height=\"321\"><\/p>\n<p>terminal output display of what in inside the \/.guild directory: (notice flags is just an empty dictionary)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b0b0d9e9a6511673b88d4aa093dc95ca7a280c8d.png\" data-download-href=\"\/uploads\/short-url\/pd4W5283z3KfZvKgLj5qtTat1Pn.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b0b0d9e9a6511673b88d4aa093dc95ca7a280c8d.png\" alt=\"image\" data-base62-sha1=\"pd4W5283z3KfZvKgLj5qtTat1Pn\" width=\"690\" height=\"75\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b0b0d9e9a6511673b88d4aa093dc95ca7a280c8d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">960\u00d7105 4.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ValueError: empty body on If",
        "Question_creation_time":1592405849984,
        "Question_link":"https:\/\/my.guild.ai\/t\/valueerror-empty-body-on-if\/187",
        "Question_upvote_count":1.0,
        "Question_view_count":343.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>Using Guild AI version 0.6.6 inside a virtual environment, I receive the following error:<\/p>\n<pre><code>$ guild run main.py\nYou are about to run \/mnt\/volume1\/Dropbox\/X\/Y\/W\/main.py\n  DEBUG: yes\n  TRAIN: yes\n  batch_norm: yes\n  cudanum: 0\n  ...\n  \nContinue? (Y\/n) Y\nTraceback (most recent call last):\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 290, in &lt;module&gt;\n    main()\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 57, in main\n    _main()\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 84, in _main\n    _try_module(arg1, rest_args)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 134, in _try_module\n    _dispatch_module_exec(_flags_dest(args), module_info)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 215, in _dispatch_module_exec\n    _exec_module_with_globals(module_info, flags, args)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 271, in _exec_module_with_globals\n    _module_with_globals(module_info, globals)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 278, in _module_with_globals\n    _gen_exec(module_info, exec_script)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 251, in _gen_exec\n    exec_cb()\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 277, in exec_script\n    python_util.exec_script(path, globals)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/python_util.py\", line 353, in exec_script\n    code = _compile_script(src, filename, _node_filter(globals))\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/python_util.py\", line 384, in _compile_script\n    dont_inherit=True)\nValueError: empty body on If\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Precision-recall dashboard not showing",
        "Question_creation_time":1591888269332,
        "Question_link":"https:\/\/my.guild.ai\/t\/precision-recall-dashboard-not-showing\/155",
        "Question_upvote_count":0.0,
        "Question_view_count":394.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi Garrett,<\/p>\n<p>I think this should be fixed in rc10 (<a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/173\" rel=\"nofollow noopener\">https:\/\/github.com\/guildai\/guildai\/issues\/173<\/a>) but unfortunately I am still not seeing the PR dashboard when viewing results through Guild. I can try to make an MWE if needed but is there an obvious gotcha\u2019?<\/p>\n<p>To be clear, if I navigate to <code>...\/.guild\/runs\/{RUN_HASH}<\/code> and run <code>tensorboard --logdir .<\/code> then the PR dashboard shows up. If I call <code>guild tensorboard<\/code> there is no sign of that dashboard.<\/p>\n<p>I am explicitly saving the precision-recall event files in a scalars folder in the run directory. I can send\/upload one of these event files if that would help debugging (the example one is only 400B)? N.b. the info in event files that don\u2019t contain the PR curves do show up in <code>guild tensorboard<\/code>.<\/p>\n<p>Thanks again for the hard work.<\/p>\n<p>Chris<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild.ipy incorrect home?",
        "Question_creation_time":1606912176799,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-ipy-incorrect-home\/484",
        "Question_upvote_count":0.0,
        "Question_view_count":181.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>How does <code>guild.ipy<\/code> home get defined?<\/p>\n<p>When I run<\/p>\n<pre><code>import guild.ipy as guild\n\nguild.runs()\n<\/code><\/pre>\n<p>I get different results from the following (which feels like it should be default behavior)<\/p>\n<pre><code>import sys\nguild_home = '{}\/.guild'.format(sys.exec_prefix)\nguild.set_guild_home(guild_home)\n\nguild.runs()\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sharing sourcecode attribute between models",
        "Question_creation_time":1611158818181,
        "Question_link":"https:\/\/my.guild.ai\/t\/sharing-sourcecode-attribute-between-models\/525",
        "Question_upvote_count":1.0,
        "Question_view_count":224.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a <code>guild.yml<\/code> that looks like this:<\/p>\n<pre><code>- include: guild\/model1.yml\n- include: guild\/model2.yml\n- include: guild\/model3.yml\n<\/code><\/pre>\n<p>where <code>guild\/model1.yml<\/code> and <code>guild\/model2.yml<\/code> looks something like this:<\/p>\n<pre><code>- model: model1\/model2\n  sourcecode:\n    - exclude:\n        dir:\n          - data\n          - build\n          - libs\n    - include:\n        dir:\n          - scripts\n          - module\n<\/code><\/pre>\n<p>As you notice, I have to specify the <code>sourcecode<\/code> attribute for each model, which gets a bit tedious. I am wondering if there is a better way to do this. Any suggestion?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild check error",
        "Question_creation_time":1606470797572,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-check-error\/466",
        "Question_upvote_count":0.0,
        "Question_view_count":259.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>I am using guild after some time\u2026<\/p>\n<p>I have installed the guild using <code>pip install guild<\/code> and execute <code>guild check<\/code>. I got the following error:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\alphar&gt; guild check\nguild_version:             0.7.1.dev3\nguild_install_location:    c:\\programdata\\anaconda3\\lib\\site-packages\\guild\nguild_home:                C:\\ProgramData\\Anaconda3\\.guild\nguild_resource_cache:      C:\\ProgramData\\Anaconda3\\.guild\\cache\\resources\nTraceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    guild.main.main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    _main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 40, in _main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\click_util.py\", line 201, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check.py\", line 104, in check\n    check_impl.main(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 87, in main\n    _check(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 92, in _check\n    _check_impl(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 107, in _check_impl\n    _print_info(check)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 156, in _print_info\n    _print_guild_info()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 179, in _print_guild_info\n    cli.out(\"installed_plugins:         %s\" % _format_plugins())\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 191, in _format_plugins\n    return \", \".join([name for name, _ in sorted(plugin.iter_plugins())])\nTypeError: '&lt;' not supported between instances of 'CPUPlugin' and 'CPUPlugin'\n<\/code><\/pre>\n<p>I get the same error with <code>pip install --pre --user guildai<\/code>.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remotes config doesn't work for Gist",
        "Question_creation_time":1647929797879,
        "Question_link":"https:\/\/my.guild.ai\/t\/remotes-config-doesnt-work-for-gist\/840",
        "Question_upvote_count":1.0,
        "Question_view_count":108.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  description: train\n  main: tests\/train\n  sourcecode:\n    - exclude: '*.json'\n\nremotes:\n  remotename:\n    type: gist\n    user: username\n    gist-name: results.md\n<\/code><\/pre>\n<p>This remote doesn\u2019t show up when I run <code>guild remotes<\/code><\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issues with Guild file - output-scalars and sourcecode",
        "Question_creation_time":1593895791379,
        "Question_link":"https:\/\/my.guild.ai\/t\/issues-with-guild-file-output-scalars-and-sourcecode\/218",
        "Question_upvote_count":1.0,
        "Question_view_count":515.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>This looks like it would solve the issue but I\u2019m getting new issues with my guild file.<\/p>\n<p>It\u2019s complaining about <code>output-scalars: off<\/code><\/p>\n<pre><code>  operations:\n    search_lr:\n      main: search_lr\n      flags-import: all\n      output-scalars: off\n      requires:\n        - database\n    train:\n<\/code><\/pre>\n<pre><code>ERROR: error loading guildfile from .: error in \/home\/richard\/Documents\/league\/guild.yml: invalid value for output-scalars: False\n<\/code><\/pre>\n<p>I checked here: <a href=\"https:\/\/my.guild.ai\/t\/scalars\/160\" class=\"inline-onebox\">Scalars<\/a> and it seems like I\u2019m doing it correctly?<\/p>\n<p>If I remove that then it starts complaining about my sourcecode:<\/p>\n<pre><code>  sourcecode:\n    - '*.py'\n    - '*.json'\n    - guild.yml\n    - exclude:\n        file:\n          - riot_api_key.py\n        dir:\n          - tb\n          - checkpoints\n          - matches\n          - match_histories\n<\/code><\/pre>\n<pre><code>ERROR: error loading guildfile from .: error in \/home\/richard\/Documents\/league\/guild.yml: invalid exclude value: {'file': ['riot_api_key.py'], 'dir': ['tb', 'checkpoints', 'matches', 'match_histories']}\n<\/code><\/pre>\n<p>The list probably continues after fixing these issues so I think it\u2019s better to wait with this.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow Release 0.4.2",
        "Question_creation_time":1533678749000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/NH0x9ch4MW0",
        "Question_upvote_count":null,
        "Question_view_count":16.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi mlflow-users,\n\nMLflow Release 0.4.2 is ready, released 2018-08-07. The release is available on\u00a0PyPI\u00a0and docs are\u00a0updated. Here are the release notes (also available\u00a0on GitHub):\n\n\n\nBreaking changes: None\n\nFeatures:\n\nMLflow experiments REST API and\u00a0mlflow experiments create\u00a0now support providing\u00a0--artifact-location\u00a0(#232,\u00a0@aarondav)\n[UI] Runs can now be sorted by columns, and added a Select All button (#227,\u00a0@ToonKBC)\nDatabricks File System (DBFS) artifactory support added (#226,\u00a0@andrewmchen)\ndatabricks-cli version upgraded to >= 0.8.0 to support new DatabricksConfigProvider interface (#257,\u00a0@aarondav)\n\nBug fixes:\n\nMLflow client sends REST API calls using snake_case instead of camelCase field names (#232,\u00a0@aarondav)\nMinor bug fixes (#243,\u00a0#242,\u00a0@aarondav;\u00a0#251,\u00a0@javierluraschi;\u00a0#245,\u00a0@smurching;\u00a0#252,\u00a0@mateiz)",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GPT-3 integration?",
        "Question_creation_time":1661173489000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/m36yrKpx87c",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"OpenAI has put out a powerful model called GPT-3. This model allows for custom model generation, complete with fine-tuning, metrics measurement, etc. I was wondering whether it would be possible to track GPT-3 runs using MLflow?\n\n\nThey do have a provision to track using Weights and Biases.\n\n\nHere's the link to the documentation (specific bookmark links to the chapter that provides github links to jupyter notebooks).\n\n\nThanks!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"custom database for mlflow",
        "Question_creation_time":1603121427000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/5zHcAO36wXY",
        "Question_upvote_count":null,
        "Question_view_count":20.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"I believe I have heard you can configure mlflow to use your own custom db.\u00a0\u00a0 Any one have any info on that?\n\n\n\nDan",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"log parameters in databricks",
        "Question_creation_time":1620152303000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/M5-z8doznpo",
        "Question_upvote_count":null,
        "Question_view_count":38.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello everyone , can someone please tell me how can i log the best parameters of crossvalid and paramgrid using mlflow , pyspark in databricks\u00a0\nthank you",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GOCD Plugin for MLflow",
        "Question_creation_time":1541984493000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CIihDdCS014",
        "Question_upvote_count":null,
        "Question_view_count":22.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi All,\n\n\nWe have built a new GoCD plugin[1] which works with mlflow and helps us in continuous delivery of models.\n\nHope Y'all find this useful.\u00a0\n\n\n[1] https:\/\/github.com\/indix\/mlflow-gocd\/\n\n\nBest,\nKrishna Sangeeth",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 0.8.2 Released!",
        "Question_creation_time":1548873019000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/pciLszBL2ig",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow 0.8.2 has been released!\n\n\n\nMLflow 0.8.2 is a patch release on top of 0.8.1 containing bug fixes and documentation updates. Please see the release change log\u00a0for more information\u00a0about the fixes and updates introduced in this release. Also, check out the latest documentation on mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model staging",
        "Question_creation_time":1622101738000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/OpF4sawraY0",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello Community\n\n\n1.Is there a way to get all the versions of\u00a0 a particular stage of a registered model\nlatest_version_info = client.get_latest_versions(model_name, stages=[\"Archived\"]). This code fetches only the latest version not all the versions of the staged model\n\n\n2.Is there a way to delete all the versions of a staged version say I want to delete all the versions staged \"Archived\"\n\n\nRegards,\nAarthi",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.1 released!",
        "Question_creation_time":1563888860000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/VDLdh-HzhlU",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nMLflow 1.1 has been released! The release contains many exciting new features:\n* Automatic logging from TensorFlow and Keras\n* Parallel coordinate plots in the tracking UI\n* Pandas DataFrame based search API\n* Java Fluent API\n* Kubernetes execution backend for MLflow projects\n* Search Pagination\n\nSee the CHANGELOG for details about the new features:\nhttps:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.1.0\n\nAnd see the blog post for some examples of these features in action: https:\/\/databricks.com\/blog\/2019\/07\/23\/announcing-the-mlflow-1-1-release.html\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Autologging metrics",
        "Question_creation_time":1629676470000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/dwowhoJAG-M",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello everyone,\nI have a question about autologging. I use it and I want to record another metrics.\nCan I change the recorded metrics for autologging?\n\nThanks,\nIrina",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow UI issue when running in docker",
        "Question_creation_time":1575115725000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/7cezJGzfnic",
        "Question_upvote_count":null,
        "Question_view_count":557.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I installed mlflow and started the ui with no issues on my windows 10 machine in an anaconda python 3.7 environment and am able to access the UI via http:\/\/localhost:5000\nHowever, when doing the exact same thing within the anacanda3 docker container the UI doesn't appear to be rendering\/responding.\nMy docker run command includes the proper port and I am able to exec into the container, install mlflow and start the ui (>mlflow ui) without any errors.\n\n\ndocker run --name conda3 -d -t -v \/\/c\/\/develop:\/develop -p 5000:5000 continuumio\/anaconda3\\\n\nWhen I try to access\u00a0http:\/\/localhost:5000\u00a0(or\u00a0http:\/\/127.0.0.1:5000\/), the response in the browser is \"ERR_EMPTY_RESPONSE\".\nI tried to access the UI within the contain via lynx, to confirm that it is running, however, the response is just a warning that this site is javascript which can't be rendered in lynx.\n\nI also tried the above docker approach using python 3.6 and also utilizing the docker container I normally develop with but in both cases I still get \"ERR_EMPTY_RESPONSE\".\n\nI am pretty much stuck at this point so any suggestions will be appreciated. Thanks.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow demonstration with Docker containers",
        "Question_creation_time":1553370857000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/SsVBNB_7zEw",
        "Question_upvote_count":null,
        "Question_view_count":30.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"This\u00a0github repo\u00a0demonstrates the use of mlflow in a set of Docker container with Python and R.\u00a0 This is the high-level design for the demonstration environment and software stack.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow usage and contribution",
        "Question_creation_time":1633257951000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/pSnbo3Mvd0w",
        "Question_upvote_count":null,
        "Question_view_count":38.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nIntertec is considering using MLFlow as the primary MLOps tool. Please let me know what are the criteria to get our logo on your site. We can also try to contribute to the product.\n\n\nKind regards\n\n\n\u00a0 \u00a0 \u00a0\nVelimir Graorkoski\nSoftware engineer\n\n\n\n\nslack\u00a0| velimir.graorkoski\n\nmail\u00a0|\u00a0velimir.g...@intertec.io\n\n\nweb\u00a0|\u00a0\u00a0https:\/\/www.intertec.io",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Back up experiments on a new Mlflow server",
        "Question_creation_time":1575893101000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Utw2OAnyW7A",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI am just wondering if there is a clean way to backup experiment(s) from one Mlflow server to another?\n\n\ncheers,\nAmin",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow model deployment as spark transform in a spark java job",
        "Question_creation_time":1631758302000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/zTot8BDmGOA",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi everyone,\nIs there a method which i can use to convert the mlflow model as spark UDF which i can use in spark java job? if Yes please help me with the approach to achieve it. If No is there any Workaround using which I can achieve it.\n\n\n\n\nThanks & Regards,\nSujay",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I'm presenting 'HIPAA Compliance for AI Data Using InfinStor MLflow' tomorrow at 11 AM Pacific",
        "Question_creation_time":1658850724000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CJBo2p1jddc",
        "Question_upvote_count":null,
        "Question_view_count":4.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"If this topic is of interest to you, please sign up at the following link:\n\n\nhttps:\/\/us06web.zoom.us\/webinar\/register\/2316563487439\/WN_etMx8m8oR--VhQBRmzuCfg\n\n\nWe look forward to exchanging ideas with you tomorrow.\n\nBest\nJagane",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.10 released!",
        "Question_creation_time":1595431094000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/XiJyetdLrdg",
        "Question_upvote_count":null,
        "Question_view_count":22.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of\u00a0MLflow 1.10.0!\n\nIn addition to bug and documentation fixes, MLflow 1.10.0 includes the following features and improvements:\n\nMlflowClient.transition_model_version_stage\u00a0now supports an\narchive_existing_versions\u00a0argument for archiving existing staging or production model\nversions when transitioning a new model version to staging or production (#3095,\u00a0@harupy)\nAdded\u00a0set_registry_uri,\u00a0get_registry_uri\u00a0APIs. Setting the model registry URI causes\nfluent APIs like\u00a0mlflow.register_model\u00a0to communicate with the model registry at the specified\nURI (#3072,\u00a0@sueann)\nAdded paginated\u00a0MlflowClient.search_registered_models\u00a0API (#2939,\u00a0#3023,\u00a0#3027\u00a0@ankitmathur-db;\u00a0#2966,\u00a0@mparkhe)\nAdded syntax highlighting when viewing text files (YAML etc) in the MLflow runs UI (#3041,\u00a0@harupy)\n\nFor a comprehensive list of changes, see the\u00a0release change log.\u00a0The latest documentation will be available soon on mlflow.org\n\nThanks!\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What are the advantages of Database usage vs. file based storage with MLflow?",
        "Question_creation_time":1583945229000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ubaYYaDROns",
        "Question_upvote_count":null,
        "Question_view_count":37.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\nwhat are the advantages of database usage vs. file based storage with MLflow?\nIs the DB faster on large experiments?\nI mean backup for example would be easier when it is all files. Artifacts and other data.\nThanks\nPhilip",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.3 released!",
        "Question_creation_time":1569934906000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/GjvwSSCX_e4",
        "Question_upvote_count":null,
        "Question_view_count":9.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all, MLflow 1.3 has been released! In addition to bug and documentation fixes, MLflow 1.3.0 includes several major features and improvements - to mention a few:\n\n* The Python client now supports logging & loading models using TensorFlow 2.0\n* Significant performance improvements when fetching runs and experiments in MLflow servers that use SQL database-backed storage\n* New `GetExperimentByName` REST API endpoint, used in the Python client to speed up `set_experiment` and `get_experiment_by_name`\n* New `mlflow.delete_run`, `mlflow.delete_experiment` fluent APIs in the Python client\n* New CLI command (`mlflow experiments csv`) to export runs of an experiment into a CSV\n\nSee the CHANGELOG for details about the new features:\nhttps:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.3.0\n\n\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Organizations using MLflow - Cabify",
        "Question_creation_time":1655977519000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Ji1EvN2DRGk",
        "Question_upvote_count":null,
        "Question_view_count":25.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey there,\n\nAt Cabify, we use MLflow since 2019 as part of our in-house ML platform named Lykeion. It has brought a lot of good things.\n\nWe would love to appear in the website as a company using MLflow. Please find attached our logo. \ud83d\ude00\n\nThanks,",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[RFC][Breaking] Runs URI for specifying artifact locations",
        "Question_creation_time":1555465978000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/59srnQ6Cjsk",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"The proposal is described in more detail in this Google Document. Please use comments in the google doc to give feedback. Thank you!\n\n\n\n\n\nTL;DR \u00a0We propose a new runs:\/ URI scheme to represent artifact locations associated with a Run. This allows us to:\n\nClean up artifact APIs where the Run ID is optional by replacing the run_id and artifact_path arguments with a single artifact_uri argument, unifying the representation of the artifact location in such APIs. \n\nRefer to artifacts by Run in places where currently only URIs are allowed; for example, for specifying artifact dependencies in Projects or pyfunc models.\n\n\n\n\nProposed Changes\n\n\n\n\nThe representation and support for artifact locations in MLflow is varied:\n\nIn most MLflow APIs, namely those in Tracking, the artifact location is represented as a tuple of (run_id,\n artifact_path = relative path with respect to the Run\u2019s base artifact URI).\n\nIn a few places the location is an absolute URI such as s3:\/\/<bucket>\/path\/to\/file.\n\nSome APIs take both representations, where artifact_path is relative if run_id is specified, and absolute otherwise. For absolute URIs, some such APIs support remote URIs (e.g. S3), and some only support local paths.\n\n\n\n\nTo unify the representation of artifact locations, we propose that for user-facing APIs in components outside of MLflow Tracking, such as MLflow Models, the artifact location be specified as a single URI argument:\n\n\n\n\nIn the scope of MLflow Tracking, there is always a contextual Run ID whenever we work with an artifact, and in the fluent APIs the user need not know the Run ID. The current API is convenient as the user doesn\u2019t need to think about the Run ID in most places.\n\n\n\n\nOutside of Tracking, artifacts and models may or may not be associated with an MLflow Tracking Run. It is in the philosophy of MLflow for the different components to be stand-alone, allowing the user to only use the components they want.\n\n\n\n\nThis means in the changed APIs,\n\n\n\n\nIf an artifact has no Run association, the user can give the absolute URI of the artifact location.\n\nIf the artifact has a Run associated, it is convenient to be able to specify the (Run ID, relative path) tuple (assuming there is a tracking server available in the context) instead of the underlying absolute path that has to be carefully constructed from the Run\u2019s RunInfo and the relative path. To support this Run ID short-cut while keeping the APIs succinct, we propose an absolute URI format:\n\n\t\n\nruns:\/<run_id>\/<relative_path>\n\n\n\n\nNote that affected APIs are all reading artifacts as they are in the Models component. To make the implementation simple, we can expand the scope of data.download_uri to support all URI types including runs:\/; then data.download_uri also becomes a full-functional utility method to download files from any artifact store backed by an Artifact Repository.\n\n\n\n\nBreaking Changes\n\n\n\n\nMany of the APIs proposed to be changed will lose the run_id argument and have the artifact_path argument renamed to artifact_uri. This means any usage of the said APIs specifying the run_id argument (which defaults to None) or using the artifact_path argument by name will break.\n\n\n\n\nThe most notable APIs to be affected are:\n\n<model-type>.load_model\n\npyfunc.load_pyfunc\n\npyfunc.spark_udf\n\nas they may already be used in deployments in production.\n\n\n\n\nFor the list of all APIs that would be affected, please see the google doc.\n\n\n\n\nRequest for Comment\n\n\nAgain, the full RFC is available in this Google Document. Please use comments in the Google Doc to give feedback. Thank you!\n\n\n\n\n\n\n--\n\n\n\n\nSue Ann Hong\n\n\nSoftware Engineer - Machine Learning\n\nDatabricks, Inc.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"pattern for ML model versioning?",
        "Question_creation_time":1567622906000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/d2g7oAa6Hy4",
        "Question_upvote_count":null,
        "Question_view_count":46.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nIs there a standard pattern for tracking different versions of a model in mlflow?\n\n\nIdeally, I would like to use a model id and version to determine a mlflow URI that can be used to load that model.\u00a0\n\n\nI could use mlflow tags, but tags wouldn't enforce version uniqueness for a given model id.\n\n\nrun_id can't really be used as a version number as a model run can be resumed from an existing run_id.\n\n\nThanks for any pointers.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Facing Problems in mlflow deployment on windows server",
        "Question_creation_time":1568860989000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lovl7Ns6x0Y",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"I am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ADFS Authentication to MLflow",
        "Question_creation_time":1603122246000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/l3udtZwwdbU",
        "Question_upvote_count":null,
        "Question_view_count":95.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\u00a0\n\n\nI was just wondering if there is any simpler way to apply Authentication to mlflow dashboards using active directory apart from nginx,\u00a0 or if any plugin is available to serve the purpose.\u00a0\n\n\nThanks\nIsha",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[ANNOUNCEMENT] CFP for Data + AI Conference",
        "Question_creation_time":1612536907000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/SrwmTCHd4vw",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow users:\n\n\nWe accept CfP on MLflow for this conference: how you use it in production; use MLOps best practices; with popular ML frameworks for experiment tracking; integrations and extensions with MLflow; and the model registry\u00a0for discovering, sharing, and deploying models, etc.\u00a0\n\n\nShare your bright ideas with the larger data community on how MLflow helps you manage your model lifecycle.\n\n\nhttps:\/\/databricks.com\/dataaisummit\/north-america-2021\/call-for-presentations\n\n\n\nCheers\nJules\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"windows compatibility",
        "Question_creation_time":1562794385000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mNJwqIkvMR8",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI am not sure if this is the right place to ask this:\nI have seen the PR of the mlflow which replaces Gunicorn by Waitress has been approved and merged in master, but when will this versioned released on Pypi?\n\n\nThanks a lot!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Failed to call Java API: MlflowClient.listArtifacts in docker",
        "Question_creation_time":1560123243000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/SrdkbCtKmgs",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"HI,\n\n\nmy Dockerfile as follows:\n\n\nFROM python:3.6\n\nENV MLFLOW_VERSION 0.8.2\n\nWORKDIR \/\nRUN apt-get update \\\n   && apt-get install -y ca-certificates wget openjdk-8-jdk\n\nRUN pip install mlflow==${MLFLOW_VERSION}\n\nRUN wget -O dd-java-agent.jar \\\n         'https:\/\/search.maven.org\/classic\/remote_content?g=com.datadoghq&a=dd-java-agent&v=LATEST'\n\n\nCOPY target\/xxxxxx.jar app.jar\nCOPY pipeline\/ci\/entrypoint.sh .\nCOPY pipeline\/ci\/sysctl.conf \/etc\/sysctl.d\/00-alpine.conf\n\nRUN chmod +x entrypoint.sh\n\nEXPOSE 8080\nENTRYPOINT [\"\/entrypoint.sh\"]\n\n\nmlflow is deployed in localhost outer docker\nartifact storage is s3 implemented by minio in localhost\n\n\nAfter started docker container, MlflowClient.listArtifacts is called by my own rest API, but it failed\n\n\n\n[http-nio-8080-exec-9] - [ERROR] - [com.xxx.dsd.aop.ApiExceptionHandler.internalErrorHandler(line:26)] - org.mlflow.tracking.MlflowClientException: Failed to exec 'python -m mlflow.cli', needed to access artifacts within the non-Java-native artifact store at 's3:\/\/mlflow\/artifacts\/1\/a34faddc96a544ce8e41f6049189a351\/artifacts'. Please make sure mlflow is available on your local system path (e.g., from 'pip install mlflow')\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.checkMlflowAccessible(CliBasedArtifactRepository.java:181)\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.listArtifacts(CliBasedArtifactRepository.java:127)\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.listArtifacts(CliBasedArtifactRepository.java:137)\n\nat org.mlflow.tracking.MlflowClient.listArtifacts(MlflowClient.java:438)\n\nat com.rakuten.dsd.service.ModelService.getArtifacts(ModelService.java:97)\n\nat com.rakuten.dsd.controller.ModelController.getArtifacts(ModelController.java:117)\n\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\nat java.lang.reflect.Method.invoke(Method.java:498)\n\nat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:209)\n\nat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136)\n\nat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102)\n\nat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:870)\n\nat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:776)\n\nat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)\n\nat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:991)\n\nat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:925)\n\nat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:978)\n\nat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:870)\n\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:635)\n\nat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:855)\n\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:742)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:158)\n\nat org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:126)\n\nat org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:111)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:84)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:199)\n\nat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)\n\nat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:496)\n\nat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140)\n\nat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81)\n\nat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87)\n\nat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)\n\nat org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:803)\n\nat org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)\n\nat org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:790)\n\nat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1459)\n\nat org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)\n\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\nat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\n\nat java.lang.Thread.run(Thread.java:748)\n\nCaused by: org.mlflow.tracking.MlflowClientException: Failed to get mlflow version. Error: Traceback (most recent call last):\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\n\n\u00a0 \u00a0 \"__main__\", mod_spec)\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/runpy.py\", line 85, in _run_code\n\n\u00a0 \u00a0 exec(code, run_globals)\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/cli.py\", line 223, in <module>\n\n\u00a0 \u00a0 cli()\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n\n\u00a0 \u00a0 return self.main(*args, **kwargs)\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/click\/core.py\", line 696, in main\n\n\u00a0 \u00a0 _verify_python3_env()\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/click\/_unicodefun.py\", line 124, in _verify_python3_env\n\n\u00a0 \u00a0 ' mitigation steps.' + extra\n\nRuntimeError: Click will abort further execution because Python 3 was configured to use ASCII as encoding for the environment. Consult https:\/\/click.palletsprojects.com\/en\/7.x\/python3\/ for mitigation steps.\n\n\n\n\nThis system supports the C.UTF-8 locale which is recommended.\n\nYou might be able to resolve your issue by exporting the\n\nfollowing environment variables:\n\n\n\n\n\u00a0 \u00a0 export LC_ALL=C.UTF-8\n\n\u00a0 \u00a0 export LANG=C.UTF-8\n\n\n\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.forkMlflowProcess(CliBasedArtifactRepository.java:209)\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.checkMlflowAccessible(CliBasedArtifactRepository.java:173)\n\n... 61 more\n\n\n\nBut, if I use 'exec' command to enter into the container, and use python API of mlflow, everything looks OK.\n\n\nSo, I'm not suer\u00a0 if there's anything wrong in my docker image or any other things wrong ?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Python] MLflow 0.9.0.1 released",
        "Question_creation_time":1554849336000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/OVyxnXd0Jac",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nWe've released an updated MLflow Python package (MLflow 0.9.0.1) on PyPI, which fixes an issue raised in https:\/\/github.com\/mlflow\/mlflow\/issues\/1113 & https:\/\/github.com\/mlflow\/mlflow\/issues\/1056 where form input (e.g. searching runs) was broken in the MLflow 0.9.0 UI.\n\n\nYou can upgrade to the new version via \"pip install --upgrade mlflow\".\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Spark-related tests are failing",
        "Question_creation_time":1553709289000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/BgU1p2juiBo",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nI'm trying to set up the dev environment for contributing to MLflow. I followed the instructions\u00a0here but I have some issues running the tests.\u00a0\nI am using pytest and see that 9 tests fail. Most of them are part of the:\u00a0tests\/spark\/test_spark_model_export.py file. The error description is the following:\n\n\ntests\/spark\/test_spark_model_export.py:61:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\/anaconda2\/lib\/python2.7\/site-packages\/pyspark\/context.py:115: in __init__\n\u00a0\u00a0\u00a0 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n\n\n\u00a0ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local-cluster[2, 1, 1024]) created by getOrCreate at \/home\/mlflow\/mlflow\/tests\/pyfunc\/test_spark.py:27\n..\/anaconda2\/lib\/python2.7\/site-packages\/pyspark\/context.py:314: ValueError\n\n\n************************************************\n\n\nIt seems that the SparkContext cannot be initialized because of an existing one. Any idea what might be the problem here?\n\n\nThanks,\nAvrilia",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Organizations using MLflow - Emerton Data",
        "Question_creation_time":1593271817000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9WHVX1YBK_A",
        "Question_upvote_count":null,
        "Question_view_count":30.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi !\n\n\nAt Emerton Data, we are big fans of MLflow and are using it in our project to industrialize AI models and data projects.\n\n\nHappy to be one of the mlflow supporter and would be glad to appear on your website as an organization using MLFlow.\u00a0\n\n\nCheers,\n\n\n\nYannick LEO\nDirector Data Science\n\n16 avenue Hoche\u00a0\u00a0\n75008 Paris\u00a0\nM + 33 6 38 21 33 99\nT + 33 1 53 75 38 75\nyanni...@emerton-data.com\u00a0|\u00a0http:\/\/www.emerton-data.com",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow load_model - no host supplied",
        "Question_creation_time":1663324709000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/jHikb5beQ9I",
        "Question_upvote_count":null,
        "Question_view_count":5.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"In the below image, I mention tracking URI and trying to load the model but facing an error in no host supplied.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow 1.29.0 release",
        "Question_creation_time":1663584167000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/RJYpm6WZZQc",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of MLflow 1.29.0!\n\nMLflow 1.29.0 includes several major features and improvements:\n\nFeatures:\n[Pipelines] Improve performance and fidelity of dataset profiling in the scikit-learn regression Pipeline (#6792, @sunishsheth2009)\n[Pipelines] Add an mlflow pipelines get-artifact CLI for retrieving Pipeline artifacts (#6517, @prithvikannan)\n[Pipelines] Introduce an option for skipping dataset profiling to the scikit-learn regression Pipeline (#6456, @apurva-koti)\n[Pipelines \/ UI] Display an mlflow pipelines CLI command for reproducing a Pipeline run in the MLflow UI (#6376, @hubertzub-db)\n[Tracking] Automatically generate friendly names for Runs if not supplied by the user (#6736, @BenWilson2)\n[Tracking] Add load_text(), load_image() and load_dict() fluent APIs for convenient artifact loading (#6475, @subramaniam02)\n[Tracking] Add creation_time and last_update_time attributes to the Experiment class (#6756, @subramaniam02)\n[Tracking] Add official MLflow Tracking Server Dockerfiles to the MLflow repository (#6731, @oojo12)\n[Tracking] Add searchExperiments API to Java client and deprecate listExperiments (#6561, @dbczumar)\n[Tracking] Add mlflow_search_experiments API to R client and deprecate mlflow_list_experiments (#6576, @dbczumar)\n[UI] Make URLs clickable in the MLflow Tracking UI (#6526, @marijncv)\n[UI] Introduce support for csv data preview within the artifact viewer pane (#6567, @nnethery)\n[Model Registry \/ Models] Introduce mlflow.models.add_libraries_to_model() API for adding libraries to an MLflow Model (#6586, @arjundc-db)\n[Models] Add model validation support to mlflow.evaluate() (#6582, @jerrylian-db)\n[Models] Introduce sample_weights support to mlflow.evaluate() (#6806, @dbczumar)\n[Models] Add pos_label support to mlflow.evaluate() for identifying the positive class (#6696, @harupy)\n[Models] Make the metric name prefix and dataset info configurable in mlflow.evaluate() (#6593, @dbczumar)\n[Models] Add utility for validating the compatibility of a dataset with a model signature (#6494, @serena-ruan)\n[Models] Add predict_proba() support to the pyfunc representation of scikit-learn models (#6631, @skylarbpayne)\n[Models] Add support for Decimal type inference to MLflow Model schemas (#6600, @shitaoli-db)\n[Models] Add new CLI command for generating Dockerfiles for model serving (#6591, @anuarkaliyev23)\n[Scoring] Add \/health endpoint to scoring server (#6574, @gabriel-milan)\n[Scoring] Support specifying a variant_name during Sagemaker deployment (#6486, @nfarley-soaren)\n[Scoring] Support specifying a data_capture_config during SageMaker deployment (#6423, @jonwiggins)\nBug fixes:\n[Tracking] Make Run and Experiment deletion and restoration idempotent (#6641, @dbczumar)\n[UI] Fix an alignment bug affecting the Experiments list in the MLflow UI (#6569, @sunishsheth2009)\n[Models] Fix a regression in the directory path structure of logged Spark Models that occurred in MLflow 1.28.0 (#6683, @gwy1995)\n[Models] No longer reload the __main__ module when loading model code (#6647, @Jooakim)\n[Artifacts] Fix an mlflow server compatibility issue with HDFS when running in --serve-artifacts mode (#6482, @shidianshifen)\n[Scoring] Fix an inference failure with 1-dimensional tensor inputs in TensorFlow and Keras (#6796, @LiamConnell)\nDocumentation updates:\n[Tracking] Mark the SearchExperiments API as stable (#6551, @dbczumar)\n[Tracking \/ Model Registry] Deprecate the ListExperiments, ListRegisteredModels, and list_run_infos() APIs (#6550, @dbczumar)\n[Scoring] Deprecate mlflow.sagemaker.deploy() in favor of SageMakerDeploymentClient.create() (#6651, @dbczumar)\n\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model Branching",
        "Question_creation_time":1621251740000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/W3FwEcd31VI",
        "Question_upvote_count":null,
        "Question_view_count":16.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I am wondering if anyone here explored idea of tracking model branching in GIT branching sense.\n\n\n\n\nI.e.\u00a0 as model is been worked on and modified\/updated\/retrained it gets different versioning, but then someone decided to \"extend\" model to say adopt model for similar data.\u00a0 At this point we can \"branch\" model and keep versioning of each branch separate.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Organizations using MLFlow and Integrations",
        "Question_creation_time":1621792677000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/j9vx2xoFfxA",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nAt Composable Analytics (https:\/\/composable.ai) we develop an end-to-end DataOps Platform that has direct integrations with MLFlow. Further, we are enabling our customers in using MLFlow as their in house ML platform.\n\n\nCan you please add Composable's logo in both Integrations and Organizations on the main site? Logo attached.\n\n\nBest,\nAndy",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"java api serve model",
        "Question_creation_time":1588824381000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Cl2cPlQhy98",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey. is there a possibility to serve a model with java api",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does MLFLOW supports CNNs ike yolo and unet running on NVIDIA tao platform",
        "Question_creation_time":1663470424000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/BjkpwrXLXPM",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi, we are a company working in the field of computer vision and would like to make sure that the MLFLOW experiment management platform fits our needs and workflow.\n\nWe work with image processing\u00a0 CNNs like Yolo, UNET, and RetinaNet based on an NVIDIA TAO framework.\u00a0\n\nWhat we actually need is a tool that concentrates on one place (in a nice and representative way comfortable for comparison) at least the three following things for each experiment:\n\n\na- chosen by user typical meta parameters that were used to train a network (such as batches, subdivisions, max batches, etc)\n\nb- a link to the dataset the network was trained on, located on our cloud storage (such as one-drive, google drive or google cloud) or a list of filenames or a link to a file storage cloud or online drive suggested by your service if there is such a thing.\n\nc- a result of running the trained network - the number of detected objects\n\nThank you very much",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Organization - NetObjex uses MLFlow",
        "Question_creation_time":1614645394000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/oqewetS7cuY",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Attached is our logo\nhttp:\/\/www.netobjex.com",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.0 released!",
        "Question_creation_time":1559827816000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/4N9tJToclI0",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow 1.0 has been released! In addition to some exciting new features (much-improved metric visualizations, metric X coordinates, improved search functionality and HDFS support), MLflow 1.0 offers Python, Java, R, and REST API stability. Note that in order to stabilize the APIs, we had to make several breaking changes.\n\nSee the CHANGELOG for details about the new features and breaking changes, and see the blog post for some examples of these features in action!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to either memorize column selection, or use sql to query mlflow?",
        "Question_creation_time":1605300530000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ZOzitFyJHaA",
        "Question_upvote_count":null,
        "Question_view_count":9.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"MLFlow looks super nice :)\n\n\nI have a lot of parameters and metrics (like dozens of each), and they won't all fit on the main mlflow page, that shows the list of experiments. I'd like to only show a subset. Now I can do this by clicking on 'columns' and choosing them, but every time I refresh the page or similar, I have to re-select the columns, which takes ~minutes each time.\n\n\nI'd like to be able to do one of the following things ideally:\n1. somehow save, memorize, persist, the choice of columns. Happy to use a bookmarklet for this, though I'm not clear how to write one, or\n2. have a sql interface for this page, so in the sql i will choose the values\/columns I want to display. i can then just copy\/paste the sql to a plain-text file somewhere, and just paste it back in after loading the page.\n\n\nIs there something I need to change to my workflow so that I don't face this issue with having to reselect the columns? Alternatively, is there some way to achieve either of the options 1 or 2 above? Or is there some way for me to achieve my high-level goal of only showing specific columns, and choosing those relatively quickly (currently takes minutes, because have to open columns, unslelect everything, then scroll down the lloonnng list of paramters and metrics).\n\n\nHugh",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Evaluating MLFlow on Kubernetes",
        "Question_creation_time":1534951986000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/vx1uqw2GsFk",
        "Question_upvote_count":null,
        "Question_view_count":411.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI meet Matei and Mani during spark summit. We are currently evaluating MLFlow on Kubernetes and had some questions about it.\u00a0\n\n\nScenario: We have MLFlow Tracking server deployed in Kubernetes we also have a Jupyter notebook to run MLFlow Training.\u00a0\n\n\nHowever, if we don't provide s3 credentials in Jupyter Notebook container it sends an error. Is it required for both Jupyter Notebook and MLFlow Tracking Server to have s3 credentials in the container?\u00a0\n\n\nApache Spark allows for us to specify an non s3 endpoint other than aws. That way we can use systems like Ceph to store our models. Is this something that will work for MLFlow.\u00a0\n\n\n\n\nThanks,\u00a0\n\nZak Hassan\n\nEngineer - Artificial Intelligence -\u00a0 Center Of Excellence, CTO Office\nhttp:\/\/radanalytics.io\/ - Machine Learning On OpenShift",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.20 released!",
        "Question_creation_time":1629931897000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/S36kTwPILeM",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of MLflow 1.20.0!\n\nNote: The MLflow R package for 1.20.0 is not yet available but will be in a week because CRAN's submission system will be offline until September 1st.\n\nIn addition to bug and documentation fixes, MLflow 1.20.0 includes the following features and improvements:\n\n- Autologging for scikit-learn now records post training metrics when scikit-learn evaluation APIs, such as `sklearn.metrics.mean_squared_error`, are called (#4491, #4628 #4638, @WeichenXu123)\n- Autologging for PySpark ML now records post training metrics when model evaluation APIs, such as `Evaluator.evaluate()`, are called (#4686, @WeichenXu123)\n- Add `pip_requirements` and `extra_pip_requirements` to `mlflow.*.log_model` and `mlflow.*.save_model` for directly specifying the pip requirements of the model to log \/ save (#4519, #4577, #4602, @harupy)\n- Added `stdMetrics` entries to the training metrics recorded during PySpark CrossValidator autologging (#4672, @WeichenXu123)\n- MLflow UI updates:\n1. Improved scalability of the parallel coordinates plot for run performance comparison,\n2. Added support for filtering runs based on their start time on the experiment page,\n3. Added a dropdown for runs table column sorting on the experiment page,\n4. Upgraded the AG Grid plugin, which is used for runs table loading on the experiment page, to version 25.0.0,\n5. Fixed a bug on the experiment page that caused the metrics section of the runs table to collapse when selecting columns from other table sections (#4712, @dbczumar)\n- Added support for distributed execution to autologging for PyTorch Lightning (#4717, @dbczumar)\n- Expanded R support for Model Registry functionality (#4527, @bramrodenburg)\n- Added model scoring server support for defining custom prediction response wrappers (#4611, @Ark-kun)\n- `mlflow.*.log_model` and `mlflow.*.save_model` now automatically infer the pip requirements of the model to log \/ save based on the current software environment (#4518, @harupy)\n- Introduced support for running Sagemaker Batch Transform jobs with MLflow Models (#4410, #4589, @YQ-Wang)\n\nFor a comprehensive list of changes, see the release change log, and check out the latest documentation on mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.11.0 released!",
        "Question_creation_time":1598845265000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Cf8RLtdN5A4",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of\u00a0MLflow 1.11.0!\n\nMLflow 1.11.0 includes a number of major features and improvements, in addition to bug fixes and documentation updates:\n\nNew\u00a0mlflow.sklearn.autolog()\u00a0API for automatic logging of metrics, params, and models from scikit-learn model training (#3287,\u00a0@harupy;\u00a0#3323,\u00a0#3358\u00a0@dbczumar)\nRegistered model & model version creation APIs now support specifying an initial\u00a0description\u00a0(#3271,\u00a0@sueann)\nThe R\u00a0mlflow_log_model\u00a0and\u00a0mlflow_load_model\u00a0APIs now support XGBoost models (#3085,\u00a0@lorenzwalthert)\nNew\u00a0mlflow.list_run_infos\u00a0fluent API for listing run metadata (#3183,\u00a0@trangevi)\nAdded section for visualizing and comparing model schemas to model version and model-version-comparison UIs (#3209,\u00a0@zhidongqu-db)\nEnhanced support for using the model registry across Databricks workspaces: support for registering models to a Databricks workspace from outside the workspace (#3119,\u00a0@sueann), tracking run-lineage of these models (#3128,\u00a0#3164,\u00a0@ankitmathur-db;\u00a0#3187,\u00a0@harupy), and calling\u00a0mlflow.<flavor>.load_model\u00a0against remote Databricks model registries (#3330,\u00a0@sueann)\nUI support for setting\/deleting registered model and model version tags (#3187,\u00a0@harupy)\nUI support for archiving existing staging\/production versions of a model when transitioning a new model version to staging\/production (#3134,\u00a0@harupy)\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No module named pandas when \" mlflow run\"",
        "Question_creation_time":1529404855000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QnASq7ITAoI",
        "Question_upvote_count":null,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\u00a0\nI have pandas 0.22.0 installed in ubuntu16.04, it successfully run\u00a0python example\/tutorial\/train.py:\npython example\/tutorial\/train.py\u00a0\nElasticnet model (alpha=0.500000, l1_ratio=0.500000):\n\u00a0 RMSE: 0.82224284976\n\u00a0 MAE: 0.627876141016\n\u00a0 R2: 0.126787219728\n\n\nbut failed as below:\n\u00a0mlflow run example\/tutorial -P alpha=0.5 --no-conda\n\n=== Fetching project from example\/tutorial ===\n=== Work directory for this run: example\/tutorial ===\n=== Created directory \/tmp\/tmpigdg385u for downloading remote URIs passed to arguments of type 'path' ===\n=== Running command: python train.py 0.5 0.1 ===\nTraceback (most recent call last):\n\u00a0 File \"train.py\", line 9, in <module>\n\u00a0 \u00a0 import pandas as pd\nImportError: No module named pandas\n=== Run failed ===\n\n\n\n\ndouble checked that pandas installed:\n$ python\nPython 3.5.1+ (default, Mar 30 2016, 22:46:26)\u00a0\n[GCC 5.3.1 20160330] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas as pd\n>>>\u00a0\n\n\n\n\nAny advice or work around?\n\n\nThanks,\nForest",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Set Github as Artifacts location",
        "Question_creation_time":1561645149000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QKRO33wr3hM",
        "Question_upvote_count":null,
        "Question_view_count":20.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":null,
        "Question_body":"Im testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dagster integration",
        "Question_creation_time":1575964582000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/t0evBl17ZvE",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Just a thought --\u00a0\n\n\nI like ML Flow, but the Projects aspects of it seem a bit weak & cumbersome (no clear docs for sophisticated pipelines, such as DAGs, etc.)\n\n\nCould it be possible to integrate with\u00a0Dagster? This way we can enjoy the best of both worlds...",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow UI extensibility",
        "Question_creation_time":1640149922000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ORiCle7j4NM",
        "Question_upvote_count":null,
        "Question_view_count":33.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello everyone,\n\n\nso I recently chose mlflow as our current solution for logging long-term research results in our company and one significant pain point I encountered is the UI extensibility. I would love to be able to display whatever I want but even a small amount of flexibility\/customization would go a long way.\n\n\nIs this something currently on the roadmap?\n\n\nIf I were to fork mlflow and implement my own UI modules or even whole UI API module for extensibility:\nwould you be interested in a later pull request with such contribution to the project?\nif I wanted to get a feel for how difficult it would be to make this happen, could you point me to some relevant parts of the codebase which I should see first and start with?\nCheers,\nAdam",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow gui - displaying csv artifact.",
        "Question_creation_time":1544526691000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/OgApYabrvPE",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n First of all great job with mlflow, it is an amazing tool.\nI have a question\/request though, is there a way to display a csv file (i.e. confusion matrix data) saved in mlflow artifacts repo as a table or graph? \nIf not,  it would be nice if we could add some plugin into ML Flow to display CSV data in a table view.  \n\nCheers",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow Release 0.4.1",
        "Question_creation_time":1533324652000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/2Diy6_Sflr0",
        "Question_upvote_count":null,
        "Question_view_count":28.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi mlflow-users,\n\nMLflow Release 0.4.1 is ready, released 2018-08-03. The release is available on\u00a0PyPI\u00a0and docs are\u00a0updated. Here are the release notes (also available\u00a0on GitHub):\n\n\n\nBreaking changes: None\n\nFeatures:\n\n[Projects] MLflow will use the conda installation directory given by the $MLFLOW_CONDA_HOME if specified (e.g. running conda commands by invoking \"$MLFLOW_CONDA_HOME\/bin\/conda\"), defaulting to running \"conda\" otherwise. (#231,\u00a0@smurching)\n[UI] Show GitHub links in the UI for projects run from http(s):\/\/ GitHub URLs (#235,\u00a0@smurching)\n\nBug fixes:\n\nFix GCSArtifactRepository issue when calling list_artifacts on a path containing nested directories (#233,\u00a0@jakeret)\nFix Spark model support when saving\/loading models to\/from distributed filesystems (#180,\u00a0@tomasatdatabricks)\nAdd missing mlflow.version import to sagemaker module (#229,\u00a0@dbczumar)\nValidate metric, parameter and run IDs in file store and Python client (#224,\u00a0@mateiz)\nValidate that the tracking URI is a remote URI for Databricks project runs (#234,\u00a0@smurching)\nFix bug where we'd fetch git projects at SSH URIs into a local directory with the same name as the URI, instead of into a temporary directory (#236,\u00a0@smurching)\n\n\n\n\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy problems",
        "Question_creation_time":1557760602000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mZCOZ5FCQaM",
        "Question_upvote_count":null,
        "Question_view_count":27.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi guys!\n\n\nI'm new with MLFlow and I would like to deploy a simple model on SageMaker but I got some problems.\n\n\n\nI have trained a model locally and successfully generated a docker image and uploaded it to AWS ECR but, when I called deploy method I got the following error:\u00a0MlflowException: Run '6b33f54c1b2541da8ec95152ab5f566b' not found\n\n\nI'm using MLFlow\u00a00.9.1 and I'm following this tutorial:\u00a0https:\/\/docs.databricks.com\/_static\/notebooks\/mlflow\/mlflow-quick-start-deployment-aws.html\n\n\nCould someone help me please =D S2\u00a0\n\n\nThanks a lot!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"benchmark issue",
        "Question_creation_time":1545628226000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/WZFtu_8SK_Q",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"https:\/\/databricks.com\/blog\/2018\/09\/13\/whats-new-in-mlflow-v0-6-0.html\n\nIn the above article, the team presented a comparison between sprak-ml and mlflow time consumption, I am\ncurious about those data.\nIs there anyone knows more about the machine information? Such us memory, CPU ...",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFLOW project MLFLOW_TRACKING_URI to postgres in docker",
        "Question_creation_time":1562003506000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/HOstT6RINbA",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"I want to test MLFLOW projects running in docker container, I use examples\/docker from mlflow. My\u00a0 MLFLOW runs in docker container, backend store is in Postgres ( runs in its own docker container). I have started mlfow container and postgres container using docker-compose. I can connect to postgres using command:\n\npsql -p 5432 -d mlflow -U mlflow -h localhost\n\n\n\n\nHowever, when I am trying to run mlflow project (export MLFLOW_TRACKING_URI=postgresql:\/\/mlflow:mlflow@localhost\/mlflow)\n\n\u00a0 \u00a0\u00a0mlflow run . -P alpha=0.55\n\nI have an error message:\u00a0sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused\n\nIs the server running on host \"localhost\" (::1) and accepting\n\nTCP\/IP connections on port 5432?\n\ncould not connect to server: Connection refused\n\nIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\nTCP\/IP connections on port 5432?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"adding pyspark.ml.recommendation.ALS to allowlist for mlflow.pyspark.ml.autolog?",
        "Question_creation_time":1656061490000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/gOopnnYcaas",
        "Question_upvote_count":null,
        "Question_view_count":15.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi, so\u00a0 mlflow.pyspark.ml.autolog by default excludes\u00a0pyspark.ml.recommendation.ALS from the allowlist.\u00a0\n\n\nI have two questions:\nIf I add it to a custom allowlist, is there a maximum model size that will cause a problem?\nIf I cannot do an autolog, how am I supposed to log and use the model with mlflow on databricks?\n\n\nThanks!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow usage clarification",
        "Question_creation_time":1635816831000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Zqzr2Sib7CA",
        "Question_upvote_count":null,
        "Question_view_count":30.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI am new to MLflow and I am glad to find this group.\n\n\nI would appreciate if someone can give me some advice on how proceed in the scenario described below.\u00a0\u00a0\n\n\nI run mlflow in a Conda env (say mlflow-dev). I also have a few ML models in another Conda env (say models-dev).\u00a0 The models-dev has a few older dependencies that I have to keep it separate at all cost. How can I access and run models from models-dev in mlflow?\n\n\nAny suggestions, links or pointers would be very much appreciated.\n\n\nThanks\nRoy",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow scaling issue",
        "Question_creation_time":1549379818000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/MfgmfilrJsI",
        "Question_upvote_count":null,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\nwhen running below script, we notice a visible performance degradation as number of experiments increases.\nimport mlflow\n  \nfor i in range(5000):\n\n    mlflow.set_experiment('exp%d' % i)\n\n    for j in range(5):\n\n        with mlflow.start_run() as run:\n\n            mlflow.log_param('n', '%d-%d' % (i, j))\n            mlflow.log_metric('fscore', 0.9242)\n            mlflow.set_tag('some tag', '%d-%d' % (i, j))\n            with open('test.json', 'w') as f:\n                f.write('{ \"i\": %d, \"j\": %d }' % (i, j))\n            mlflow.log_artifact('test.json')\n            print(mlflow.active_run())\n\nabove script creates 5000 experiments, with 5 dummy runs in each experiment.  After about 18 hours, it has only created 2500 experiments.  At this point, it is taking about a minute to create a new experiment, and 5 seconds to create a run.\n\nwhat is the bottleneck. \nThanks in advance ..",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Kedro vs MLFlow",
        "Question_creation_time":1562004548000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/NKeIrAaBpV8",
        "Question_upvote_count":null,
        "Question_view_count":96.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Newbie question: Is anyone familiar with Kedro?\u00a0 If so, how does it compare with MLFlow?\u00a0 Could they be used together effectively, if one has strengths the other lacks?\u00a0 As I start trying to move my organization in this direction, I would like to have any experienced honest feedback available.\u00a0 I know Kedro is pretty new, so I understand if noone has any experience with it.\n\nKedro Ref:\u00a0https:\/\/medium.com\/@QuantumBlack\/introducing-kedro-the-open-source-library-for-production-ready-machine-learning-code-d1c6d26ce2cf",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Org Using MLflow - BRIDGEi2i Analytics",
        "Question_creation_time":1611101585000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/1ptDCEaHLZ0",
        "Question_upvote_count":null,
        "Question_view_count":15.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nWe at BRIDGEi2i Analytics Solutions\u00a0are extensively using MLFlow in most of our analytical products.\nWe would like to support and contribute to MLFlow, Please add our organization to the list on MLFlow website.\n\n\n\n\nThanks!!\n\n\n\n\nRegards,\n\nSaddam |\u00a0Manager |\u00a0AI Actions\u00a0\n\nBRIDGEi2i Analytics Solutions\n\nCell:\u00a0+91 - 8147171826\n\n\n\n\n\n\n\nThis e-mail (and any attachments), is confidential and may be privileged. It may be read, copied and used only by intended recipients. Unauthorized access to this e-mail (or attachments) and disclosure or copying of its contents or any action taken in reliance on it is unlawful. Unintended recipients must notify the sender immediately by e-mail or phone and delete it from their system without making any copies or disclosing it to a third person. BRIDGEi2i Analytics Solutions reserves the right to store, monitor and review the content of all messages sent to or from this email address.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"sanic python - instead of flask",
        "Question_creation_time":1560415286000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/KjKwpgi8EjI",
        "Question_upvote_count":null,
        "Question_view_count":6.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey,\n\n\nWe are not able to use the mlflow server in our production API due to the fact it's using flask.\nWe have create our own API which is taking the pickel from mlflow and uses\u00a0sanic python for the API which is much faster.\n\n\nCan you please change your flask to\u00a0sanic python?\n\n\nIt will help us a lot.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.26.1 released!",
        "Question_creation_time":1653686824000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/OkDlyI-k6Ss",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow 1.26.1!\n\n\n\nMLflow 1.26.1 is a patch release containing the following bug fixes:\n\n[Installation] Fix compatibility issue with\u00a0protobuf >= 4.21.0\u00a0(#5945,\u00a0@harupy)\n[Models] Fix\u00a0get_model_dependencies\u00a0behavior for\u00a0models:\u00a0URIs containing artifact paths (#5921,\u00a0@harupy)\n[Models] Revert a problematic change to\u00a0artifacts\u00a0persistence in\u00a0mlflow.pyfunc.log_model()\u00a0that was introduced in MLflow 1.25.0 (#5891,\u00a0@kyle-jarvis)\n[Models] Close associated image files when\u00a0EvaluationArtifact\u00a0outputs from\u00a0mlflow.evaluate()\u00a0are garbage collected (#5900,\u00a0@WeichenXu123)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.\n\n\n\nNote: Version 1.26.1 of the MLflow R package has not yet been released. It will be available on CRAN within the next week.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot save Spark Pipeline model as pyfunc",
        "Question_creation_time":1582551638000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/IRaESID1o5g",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"I have a Spark Pipeline model saved using mlflow.spark.save_model().Then load it back as a Pyfunc model (mlflow.pyfunc.load_pyfunc). But it is not getting saved on my local machine using the mlflow.pyfunc.save_model command. Why not able to save using mlflow.pyfunc.save_model () ?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow system environment in windows",
        "Question_creation_time":1604455776000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/aGVwpa-0IC4",
        "Question_upvote_count":null,
        "Question_view_count":13.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nIs it possible to create and implement ml model using mlflow without conda installed in the system environment(python environment) ?\n\n\nThanks\nSafuvan",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trying to add the log_param() function",
        "Question_creation_time":1539895945000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CxuBUiu1gys",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"with mlflow.start_run(nested =True):\n### instantiate the RNN model object\u00a0\n\u00a0 \u00a0 regr = Sequential()\u00a0\n\n\n### add the input and LSTM layers\u00a0\nregr.add(LSTM(units =4, activation ='sigmoid', input_shape =(None, 1)))\u00a0 \u00a0\u00a0\n\n\n### add the output layer\nregr.add(Dense(units =1))\n\n\n### compile the RNN\u00a0\noptimizer = 'adam'\nloss = 'mean_squared_error'\nmetrics = ['accuracy']\u00a0\nregr.compile(optimizer, loss , metrics)\u00a0\n\n\n### fit the model on the training set\u00a0\nbatch_size = 5\nepochs = 1\nregr.fit(X_train, y_train, batch_size, epochs)\u00a0\n\n\nlog_param(\"loss\", loss)\u00a0\nlog_param(\"epochs\", epochs)\nlog_param(\"optimizer\", optimizer)\nlog_param(\"batch_size\", batch_size)\nlog_param(\"metrics\", metrics)",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow on CDSW",
        "Question_creation_time":1552816967000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CYyStbjSTts",
        "Question_upvote_count":null,
        "Question_view_count":13.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi there!\n\n\nI had been able to integrate mlflow into my model on local and track the metrics.\n\n\nHowever, now am carrying out a task to integrate mlflow into Cloudera workbench. Facing either SSL or https connection issues when trying to trigger the tracking URI.\nDo you please have any documentation on working with mlflow on CDSW? Any suggestions are\u00a0welcome.\n\n\nThanks!\nBest,\nAsis",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.14.1 released!",
        "Question_creation_time":1614632185000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lSEtCZf7CrI",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all, we are happy to announce the availability of\u00a0MLflow 1.14.1!\n\n\nMLflow 1.14.1 is a patch release containing the following bug fix:\nFix issues in handling flexible numpy datatypes in TensorSpec (#4147,\u00a0@arjundc-db)\nThis information is also available in the\u00a0release change log, and you can check out the latest documentation for MLflow 1.14.1 on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Mlflow integration with Google's Vertex AI",
        "Question_creation_time":1645165866000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/q9759ED2C2A",
        "Question_upvote_count":null,
        "Question_view_count":82.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\n\u00a0 Is there any way through which we can integrate mlflow with vertex AI ?\u00a0\nAny articles or resources I can go through ?\n\nThanks\nAvinash",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to filter on Date?",
        "Question_creation_time":1563493900000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/EKDhKa3UrbM",
        "Question_upvote_count":null,
        "Question_view_count":5.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"There is a Date column in the run's field. Can I do filtering on this field? I haven't find a way to do this.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model serve",
        "Question_creation_time":1570070825000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ATDsXS83w9o",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi ,\u00a0\nI am trying to use model serve, REST API ser er is created and now I am trying to pass CSV file as POST input to the \/invocations path.\u00a0\nCan someone please guide on how to pass the csv file as POST input and save the output to a file.\u00a0\n\n\nRegards\nBinay",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.5.0 released!",
        "Question_creation_time":1576787046000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/aLPUvzEx2KY",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nWe are happy to announce the availability of\u00a0MLflow 1.5.0!^\u00a0 In addition to bug and documentation fixes, MLflow 1.5.0 includes several major features and improvements, including:\n\n\n* New support for a LightGBM flavor (#2136, @harupy)\n* New support for a XGBoost flavor (#2124, @harupy)\n* New support for a Gluon flavor and autologging (#1973, @cosmincatalin)\n* Runs created by mlflow.tensorflow.autolog() and mlflow.keras.autolog() (#2088) are now automatically ended after training (#2094, @juntai-zheng)\n\n* When using the mlflow server CLI command, you can now expose metrics on \/metrics for Prometheus via the optional --activate-parameter argument (#2097, @t-henri)\n\n\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.\n\n\nThanks,\n\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLProject with Docker not producing MLModel artifact",
        "Question_creation_time":1566902067000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/54mVBwanIVQ",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI've been trying to run an MLProject (using the command mlflow run ...) with a Docker image to generate an MLmodel file with sklearn and the project runs but the artifacts directory created in the newly created mlruns directory is empty. If I run the exact same python script as a standalone (without the MLProject and Docker image), it works perfectly fine and correctly populates the artifacts directory. I'm not sure if this is an actual bug or if I am just doing something incorrectly.\n\n\nHere is the workflow I am using:\ncreated script (sklearn_mlflow_train.py) that uses mlflow.sklearn.log_model() python api call to create model log\nsnippet of python script:\n# Run the experiments using set parameters and log the resulting model\nwith mlflow.start_run() as run:\n\u00a0 \u00a0 clf = DecisionTreeClassifier(criterion=params.criterion, random_state=params.trand)\n\u00a0 \u00a0 model = clf.fit(X_train, y_train)\n\u00a0 \u00a0 mlflow.log_metric(\"Accuracy\", model.score(X_test, y_test))\n\n\n\u00a0 \u00a0 mlflow.log_param('train_size', params.train_size)\n\u00a0 \u00a0 mlflow.log_param('srand', params.srand)\n\u00a0 \u00a0 mlflow.log_param('trand', params.trand)\n\u00a0 \u00a0 mlflow.log_param('criterion', params.criterion)\n\n\n\u00a0 \u00a0 sk_model_path = \"sk_models\"\n\u00a0 \u00a0 mlflow.sklearn.log_model(model, sk_model_path)\ncreate docker image (standard docker image creation)\ncreate MLproject file that references the docker image\n\nname: mlflowproject-sklearn-test\n\n\ndocker_env:\n\u00a0 \u00a0 image: mlflow-sklearn-test\n\n\nentry_points:\n\u00a0 \u00a0 main:\n\u00a0 \u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_size: {type: float, default: 0.2}\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 trand: {type: int, default: 123}\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 srand: {type: int, default: 12}\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 criterion: {type: string, default: gini}\n\u00a0 \u00a0 \u00a0 \u00a0 command: \"python36 sklearn_mlflow_train.py -train_size {train_size} -trand {trand} -srand {srand} -criterion {criterion}\"\nrun the mlflow project in the working directory that has both the python script and the MLproject file: mlflow run .\nafter running the project, an mlruns directory is created and all of the params and metrics are correctly logged, but the artifacts directory is empty\nThanks in advance for the help!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[RFC] Fluent API for MLflow Java SDK",
        "Question_creation_time":1561654678000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QdB_DBMGPhU",
        "Question_upvote_count":null,
        "Question_view_count":2.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi MLflow users,\n\n\nWe're planning on adding a \"fluent\" API to the MLflow Java SDK. This API allows users to manage their MLflow runs similarly to how they would with the Python SDK. Instead of\n    RunInfo runCreated = client.createRun(expId);\n    System.out.println(\"CreateRun: \" + runCreated);\n    String runId = runCreated.getRunUuid();\n\n    \/\/ Log parameters\n    client.logParam(runId, \"min_samples_leaf\", \"2\");\n    client.logParam(runId, \"max_depth\", \"3\");\n\n    \/\/ Log metrics\n    client.logMetric(runId, \"auc\", 2.12F);\n    client.logMetric(runId, \"accuracy_score\", 3.12F);\n    client.logMetric(runId, \"zero_one_loss\", 4.12F);\n\n    \/\/ Update finished run\n    client.setTerminated(runId, RunStatus.FINISHED);\nthe code would look something like\n    MlflowContext mlflow = MlflowContext.getOrCreate();\n    ActiveRun run = mlflow.startRun(\"run\");\n    run.logParam(\"alpha\", \"0.0\");\n    run.logMetric(\"MSE\", 0.0);\n    run.setTags(ImmutableMap.of(\n      \"company\", \"databricks\",\n      \"org\", \"engineering\"\n    ));\n    run.endRun();\nWe'd appreciate any feedback on the proposed APIs on the Github PR.\u00a0https:\/\/github.com\/mlflow\/mlflow\/pull\/1508\n\n\nThanks!\nAndrew",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow ui throws exception when using sqlite as backend_store_uri",
        "Question_creation_time":1580038416000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/qeRbVnClqSI",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I try to start the mlflow ui service using SQLite as a backend store and cannot get rid of this exception:\n\n\nTypeError: Invalid argument(s) 'pool_pre_ping' sent to create_engine(), using configuration SQLiteDialect_pysqlite\/NullPool\/Engine.\u00a0 Please check that the keyword arguments are appropriate for this combination of components.\n\n\n\ncommand:\n(mlflow_demo) netanel@netpy:~\/git\/mlflow\/backend_store$ mlflow ui --backend-store-uri 'sqlite:\/\/\/home\/netanel\/git\/mlflow\/backend_store\/mlflow_tracker.db'\n\n\n\nI created the SQLite DB using the sqlite3 command line and initialize the schema using the latest_schema.sql script.\n\n\npython version:\u00a0Python 3.6.0 :: Anaconda 4.3.1 (64-bit)\nmlflow version: 1.5.0\n\n\nThanks.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"posting to managed mlflow (azure databricks) from remote",
        "Question_creation_time":1569029585000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/jFvYBFUTIAc",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"If we have mlflow via a managed mlflow instance such as Azure Databricks - can we track, post experiments from remote like my machine's Jupyter notebook instead of running the code in databricks workspace?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Admin Tool",
        "Question_creation_time":1589950750000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/KHc7WI97kJE",
        "Question_upvote_count":null,
        "Question_view_count":36.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi ,\n\n\nI hope you are good.\n\n\nIs there any governance or admin tool in mlflow by which we can give only access to certain people to register a model or change a model status?\n\n\nThanks\u00a0\nSonalee",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Artifacts not shown in mlflow tracking ui",
        "Question_creation_time":1545216558000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/_wStTxLPmRg",
        "Question_upvote_count":null,
        "Question_view_count":80.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI am running the following command: mlflow server -h 0.0.0.0\u00a0--file-store .\/mlruns\u00a0--default-artifact-root .\/artifacts\n\n\nI then run the example from here:\u00a0https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/examples\/quickstart\/mlflow_tracking.py\n\n\nEverything seems to be running fine, but the artifacts are not shown in the ui.\n\n\nShmuel",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Bay Area MLflow Meetup @ Databricks, San Francisco",
        "Question_creation_time":1574271617000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/HFC-t4OCzL8",
        "Question_upvote_count":null,
        "Question_view_count":6.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello Everyone,\n\n\nWe hosting\u00a0a Bay Area MLflow Meetup with talks from Microsoft, Google, and Databricks on machine\u00a0learning, model management, and model data analytics\/validation. if you\u00a0are interested and live in the San Francisco Bay Area, please do join us for an evening of technical talks.\n\n\nRSVP here:\u00a0https:\/\/www.meetup.com\/Bay-Area-MLflow\/events\/266614106\/\n\n\nSee you all there!\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.9.0 released!",
        "Question_creation_time":1592573520000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/M525psdvxds",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of MLflow 1.9.0! Some highlights from this release:\n\n\nlog_model and save_model APIs now support saving model signatures (the model's input and output schema) and example input along with the model itself \u00a0(#2698, #2775, @tomasatdatabricks). Model signatures are used to reorder and validate input fields when scoring\/serving models using the pyfunc flavor, mlflow models CLI commands, or mlflow.pyfunc.spark_udf (#2920, @tomasatdatabricks and @aarondav)\nIntroduce fastai model persistence and autologging APIs under mlflow.fastai (#2619, #2689 @antoniomdk)\nAdd pluggable mlflow.deployments API and CLI for deploying models to custom serving tools, e.g. RedisAI (#2327, @hhsecond)\nAdd plugin interface for executing MLflow projects against custom backends (#2566, @jdlesage)\nEnable viewing PDFs logged as artifacts from the runs UI \u00a0(#2859, @ankmathur96)\nSignificant performance and scalability improvements to metric comparison and scatter plots in the UI (#2447, @mjlbach)\nFor a comprehensive list of changes, see the\u00a0release change log (https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.9.0), and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.1 Release Candidate Process",
        "Question_creation_time":1563311564000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/VIc0iMz1sSM",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe\u2019re excited to announce that we\u2019re moving towards MLflow 1.1, with a tentative release date of July 19. The release contains a number of major features, including a pandas-based runs search API, autologging from TensorFlow to MLflow, a high-level Java fluent API, support for running MLflow projects on Kubernetes, search pagination, and a parallel coordinates plot in the runs UI.\n\n\nWe\u2019ve published an RC of MLflow 1.1, which you can use to try out all the latest features. We\u2019d love to get your feedback and fix any issues that arise before the 1.1 release. Please report issues at\u00a0https:\/\/github.com\/mlflow\/mlflow\/issues\u00a0with [MLflow 1.1] in the issue title.\n\n\nPlease see this document for instructions on how to try out the latest RC:\nhttps:\/\/docs.google.com\/document\/d\/1n3rAQwf9ldkcnOY7WOLxNeyJafq3br0N_gLSPgxxjhQ\/edit\n\n\n\nThanks!\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"grpc Endpoint for Databricks MLFlow server",
        "Question_creation_time":1641355940000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/sB9lZIvPu0E",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello Everyone,\n\n\nDoes MLFlow server (Azure managed) provide gRPC endpoints?\nIf so, I couldn't find any doc to use client libraries to communicate with server with gRPC endpoints.\nAny guidance would be highly appreciated.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Predicting H2O-models stored as pyfunc-models",
        "Question_creation_time":1557995579000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/y8hhL4ecb7k",
        "Question_upvote_count":null,
        "Question_view_count":29.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\u00a0\n\nMe and my team are trying to run H2O models (stored as pyfunc-models) on our Hadoop-cluster using spark.\nBasically not much more than this: \u00a0\n\nimport mlflow\n\nimport mlflow.pyfunc\n\n\u00a0\n\nfeatures = spark.read.parquet(\u201c\u2026\/features.parquet\u201d) \\\n\npredict_udf = mlflow.pyfunc.spark_udf(spark,\"..\/mlruns\/0\/f4ff13d1d93f4baab67dea545effca9b\/artifacts\/model_pyfunc\/\")\n\n\u00a0\n\npredictions = features.withColumn(\"prediction\", predict_udf('FML_Saved_vs_AUM_Ratio','TOT_IVT_AM','IND_Saved_vs_AUM_Ratio','SOL_SMT_CD','FML_PSN_FUND_AM')\n\npredictions.show(100,False)\n\n\u00a0\n\n\u00a0\n\nWe get the same prediction for each row.\n\n\n\n\nAfter taking a closer look at what happens during the prediction I see that the column names on which the data is trained are not recognized within the dataset that is being predicted.\n\n\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'FML_Saved_vs_AUM_Ratio': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'TOT_IVT_AM': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'IND_Saved_vs_AUM_Ratio': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'SOL_SMT_CD': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\/hadoop\/sdj1\/yarn\/local\/usercache\/jd05953\/appcache\/application_1557817794126_12227\/container_e312_1557817794126_12227_01_000002\/env\/mlflow_h2o\/lib\/python2.7\/site-packages\/h2o\/job.py:69: UserWarning: Test\/Validation dataset is missing column 'FML_PSN_FUND_AM': substituting in a column of NaN\n\n\u00a0 warnings.warn(w)\n\n\u00a0\n\n\u00a0\n\nWhen I take a look at the dataframe which is actually being used for the prediction (generated within the pandas UDF of the pyfunc-module)\u00a0 \u2026\n\n\u00a0\u00a0\u00a0 def predict(*args):\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 model = SparkModelCache.get_or_load(archive_path)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 schema = {str(i): arg for i, arg in enumerate(args)}\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Explicitly pass order of columns to avoid lexicographic ordering (i.e., 10 < 2)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 columns = [str(i) for i, _ in enumerate(args)]\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 pdf = pandas.DataFrame(schema, columns=columns)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 result = model.predict(pdf)\n\n\u00a0\n\n\u2026 I do indeed see that the column names are replaced by numeric values:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4\u00a0\u00a0\u00a0\u00a0\u00a0\n\n0\u00a0\u00a0\u00a0\u00a0 0.826852\u00a0\u00a0\u00a0 2891.36\u00a0 0.825862\u00a0\u00a0\u00a0\u00a0\u00a0 123.62\u00a0\u00a0 0.00\u00a0\u00a0\u00a0\n\n1\u00a0\u00a0\u00a0\u00a0 0.656996\u00a0\u00a0\u00a0 7227.10\u00a0 0.000000\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.00\u00a0\u00a0\u00a0 0.00\u00a0\u00a0\u00a0\u00a0\u00a0\n\n2\u00a0\u00a0\u00a0\u00a0 0.011852\u00a0\u00a0\u00a0 1498.00\u00a0 0.000000\u00a0 \u00a0\u00a0\u00a065.25\u00a0\u00a0\u00a0\u00a0 0.00\n\n\u00a0\n\n\u00a0\n\nSo it seems I\u2019m getting the same prediction because everything is predicted on NaN-values.\n\n\u00a0\n\nIn Scikit-learn the order of the column is used so this should not be a problem. However, in H2O, the column names are used.\n\nSo I don\u2019t see how I can score my H2O-model properly? Am I missing something?\u00a0\n\n\n\n\nkind regards\n\nTomasz",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Feedback Welcome: dropping Python 2 support in MLflow 1.8.0",
        "Question_creation_time":1583092647000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/fMRrlEdZHos",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nThis github issue\u00a0contains a proposal for dropping Python 2 support in MLflow 1.8.0, with a target release date of end-of-March, 2020.\n\n\nThis move will help reduce MLflow's ongoing maintenance burden, and we believe it makes sense given that Python 2 is EOL & many other ML libraries (pandas, numpy, sklearn, tensorflow) have already dropped support.\n\n\nPlease review the issue & leave questions or comments - barring major concerns, we'll publish a formal announcement the week of March 8 - thanks!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"System Tags",
        "Question_creation_time":1604981734000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/kecHfSKU8eY",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey there,\n\n\nThe documentation includes reference to a list of system tags that are said to be automatically set.\nhttps:\/\/www.mlflow.org\/docs\/latest\/tracking.html#system-tags\n\n\n\nHowever, in my flows I see no trace of these tags in the UI as well as in the programmatic interface. Is there a flag I need to set to have this automatic tagging? Or are these values available for me to tag my flows manually?\n\n\nThanks,\nAvi",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow project error",
        "Question_creation_time":1633590226000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9yeEpSWsoFc",
        "Question_upvote_count":null,
        "Question_view_count":25.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi Team, there is an error although all my tries, I could not resolve it. I also posted the problem in StackOverflow and our slack channel.\n\nThe description of the error:\n\u00a0\nI want to implement the MLFlow project on my own ML model. However, I am getting \"Could not find main among entry points\"\n\nThe full problem with source files published on StackOverflow.\n\nhttps:\/\/stackoverflow.com\/questions\/69479488\/hi-i-am-very-new-to-mlflow-and-want-to-implement-mlflow-project-on-my-own-ml-m\n\nAny comments are more than welcome.\nBest Regards,\nNihad Shukur",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to load Spark model",
        "Question_creation_time":1574980477000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/IjZxr890xlY",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi, I have saved my pyspark pipelines with mlflow. The pipelines includes several custom transformers.\u00a0\n\n\nWhen i tried to load the pipeline with\u00a0model=mlflow.spark.load_model(\"location of model on local\"), it is giving me the error as below:\n```AttributeError: module '__main__' has no attribute 'AgeCalc'``` and 'AgeCalc' is one of the name of my custom transformer.\u00a0\n\n\nAny idea how to fix this?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.27.0 released, including new MLflow Pipelines component!",
        "Question_creation_time":1656460642000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/7H5WdbTpuYk",
        "Question_upvote_count":null,
        "Question_view_count":25.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are very excited to announce the availability of\u00a0MLflow\u00a01.27.0!\n\n\n\n\nMLflow 1.27.0 includes several major features and improvements:\n\n[Pipelines] With MLflow 1.27.0, we are excited to announce the release of\nMLflow Pipelines, an opinionated framework for\nstructuring MLOps workflows that simplifies and standardizes machine learning application development\nand productionization. MLflow Pipelines makes it easy for data scientists to follow best practices\nfor creating production-ready ML deliverables, allowing them to focus on developing excellent models.\nMLflow Pipelines also enables ML engineers and DevOps teams to seamlessly deploy models to production\nand incorporate them into applications. To get started with MLflow Pipelines, check out the documentation at\nhttps:\/\/mlflow.org\/docs\/latest\/pipelines.html. (#6115)\n\n[UI] Introduce UI support for searching and comparing runs across multiple Experiments (#5971,\u00a0@r3stl355)\n\nMore features:\n\n[Tracking] When using batch logging APIs, automatically split large sets of metrics, tags, and params into multiple requests (#6052,\u00a0@nzw0301)\n[Tracking] When an Experiment is deleted, SQL-based backends also move the associate Runs to the \"deleted\" lifecycle stage (#6064,\u00a0@AdityaIyengar27)\n[Tracking] Add support for logging single-element\u00a0ndarray\u00a0and tensor instances as metrics via the\u00a0mlflow.log_metric()\u00a0API (#5756,\u00a0@ntakouris)\n[Models] Add support for\u00a0CatBoostRanker\u00a0models to the\u00a0mlflow.catboost\u00a0flavor (#6032,\u00a0@danielgafni)\n[Models] Integrate SHAP's\u00a0KernelExplainer\u00a0with\u00a0mlflow.evaluate(), enabling model explanations on categorical data (#6044,\u00a0#5920,\u00a0@WeichenXu123)\n[Models] Extend\u00a0mlflow.evaluate()\u00a0to automatically log the\u00a0score()\u00a0outputs of scikit-learn models as metrics (#5935,\u00a0#5903,\u00a0@WeichenXu123)\n\nBug fixes and documentation updates:\n\n[UI] Fix broken model links in the Runs table on the MLflow Experiment Page (#6014,\u00a0@hctpbl)\n[Tracking\/Installation] Require\u00a0sqlalchemy>=1.4.0\u00a0upon MLflow installation, which is necessary for usage of SQL-based MLflow Tracking backends (#6024,\u00a0@sniafas)\n[Tracking] Fix a regression that caused\u00a0mlflow server\u00a0to reject\u00a0LogParam\u00a0API requests containing empty string values (#6031,\u00a0@harupy)\n[Tracking] Fix a failure in scikit-learn autologging that occurred when\u00a0matplotlib\u00a0was not installed on the host system (#5995,\u00a0@fa9r)\n[Tracking] Fix a failure in TensorFlow autologging that occurred when training models on\u00a0tf.data.Dataset\u00a0inputs (#6061,\u00a0@dbczumar)\n[Artifacts] Address artifact download failures from SFTP locations that occurred due to mismanaged concurrency (#5840,\u00a0@rsundqvist)\n[Models] Fix a bug where MLflow Models did not restore bundled code properly if multiple models use the same code module name (#5926,\u00a0@BFAnas)\n[Models] Address an issue where\u00a0mlflow.sklearn.model()\u00a0did not properly restore bundled model code (#6037,\u00a0@WeichenXu123)\n[Models] Fix a bug in\u00a0mlflow.evaluate()\u00a0that caused input data objects to be mutated when evaluating certain scikit-learn models (#6141,\u00a0@dbczumar)\n[Models] Fix a failure in\u00a0mlflow.pyfunc.spark_udf\u00a0that occurred when the UDF was invoked on an empty RDD partition (#6063,\u00a0@WeichenXu123)\n[Models] Fix a failure in\u00a0mlflow models build-docker\u00a0that occurred when\u00a0env-manager=local\u00a0was specified (#6046,\u00a0@bneijt)\n[Projects] Improve robustness of the git repository check that occurs prior to MLflow Project execution (#6000,\u00a0@dkapur17)\n[Projects] Address a failure that arose when running a Project that does not have a\u00a0master\u00a0branch (#5889,\u00a0@harupy)\n[Docs] Correct several typos throughout the MLflow docs (#5959,\u00a0@ryanrussell)\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Load any model.pkl file into mlflow format",
        "Question_creation_time":1570689982000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/fgEmXcVn6Kg",
        "Question_upvote_count":null,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I have some models pre trained that I want to deploy using mlflow. My models come from different libraries scikit-learn, boostedTrees, XGBoost etc. I have model.pkl files for these models and I want to load these models and write a wrapper that will save my model in Mflow format. My question is how can I load any model saved in .pkl file format and save it in mlflow format??",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Adding ONNX flavor -- issue with python version during CI",
        "Question_creation_time":1555343851000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/baS8WpeJ2JQ",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\u00a0\n\n\nI submitted a PR for supporting ONNX flavor in Mlflow (source code: https:\/\/github.com\/avflor\/mlflow\/tree\/onnx).\u00a0\n\n\nONNX only supports Python 3 and not 2.\u00a0\nMy PR consists on an mlflow\/onnx.py file and tests\/onnx\/test_onnx_model_export.py.\u00a0\n\n\nI have some issues when testing through Travis.\nI have managed to successfully exclude the tests when testing with Python 2.* version but I have trouble with the main onnx.py file.\nIn this file, I import onnx and onnxmltools packages, but these imports fail on Python 2 (as expected).\n\n\nI added the following lines in the file:\n\n\nimport sys\nif sys.version_info <= (3, 6):\n\u00a0\u00a0\u00a0 print('Requires Python 3.6')\n\u00a0\u00a0\u00a0 sys.exit(1)\nimport os\nimport yaml\u00a0\n\n\n...more imports...\n\n\n\n\nbut then lint.sh fails with the following errors:\u00a0\nmlflow\/onnx.py:9:1: E402 module level import not at top of file\n\nWhat is the best way to exclude a module in the main code based on python version?\n\nThanks,\nAvrilia",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get artifact with authentication",
        "Question_creation_time":1543546297000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/1RdBm4j3GMU",
        "Question_upvote_count":null,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I have setup Authentication for MLflow v0.8 by NGinx. But when I access artifact page of model in MLflow UI, I received error:\n\n\n\n\n\nI have seen that mlflow get artifact by \"\/get-artifact\" request followed by:\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/v0.8.0\/mlflow\/server\/js\/src\/components\/artifact-view-components\/ShowArtifactPage.js#L42\n\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/v0.8.0\/mlflow\/server\/js\/src\/components\/artifact-view-components\/ShowArtifactTextView.js#L60\n\n\n\nHow could I get artifact with authentication ?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using mlflow.models.FlavorBackend(config, **kwargs)",
        "Question_creation_time":1560305537000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mjlgu0lDAJM",
        "Question_upvote_count":null,
        "Question_view_count":13.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\nI am trying to use\u00a0mlflow.models.FlavorBackend(config,\u00a0**kwargs) to serve the model from python. What should be the values of config and what all params do we need to specify? Any kind of help would be appreciated.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow version",
        "Question_creation_time":1622417662000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/cIO3sb6oQx0",
        "Question_upvote_count":null,
        "Question_view_count":16.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"can someone tell me why i'm getting this error\nand when i tried to upgrade mlflw i still get the same error",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"purging deleted runs (from db and NFS artifacts store)",
        "Question_creation_time":1573693167000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/EtF0x3BuCR8",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Before I write one, I wanted to see if anyone was aware of a script floating around that could be used to permanently remove deleted runs\/experients, both in terms of their records in the DB and any artifacts in the NFS store?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Configuring the docker swarm\/cluster in MLflow project",
        "Question_creation_time":1558369442000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/nTDNBIE3g2g",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"I am developing an MLflow project. The running environment is docker and needs to be a cluster\/swarm due to data and computation needs.\u00a0\n\nWhat is the recommended way to configure and manage the cluster\/swarm? Specifically, do we need Kubeflow or Kubernetes to manage the docker cluster? Where(at which file in the MLflow project structure) should we configure in MLflow project? Currently, in\u00a0the MLflow docker example on GithHub, there is a place that we can define\n\ndocker_env:\n  image:  mlflow-docker-example\n\n\nShould we define the cluster\/swarm outside MLflow framework, or let MLflow control the cluster or docker swarm?\u00a0\n\n\n\n\nThanks,\n\nBin",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I build an Azure ML Container Image for an MLflow model which is trained and stored locally on my machine.",
        "Question_creation_time":1568964184000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/k0GuAjkC24k",
        "Question_upvote_count":null,
        "Question_view_count":4.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Can I build an Azure ML Container Image for an MLflow model which is trained and stored locally on my machine.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow models deployment\/serving",
        "Question_creation_time":1531881520000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/sI7yw-eZxnE",
        "Question_upvote_count":null,
        "Question_view_count":56.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi folks,\n\n\nFew people have checked on easy model deployment method. And Matei also mentioned on Slide 9 in last MLflow meetup.\n\n\nWhat do you folks think of this \"one click\" solution to get your endpoint? See on site: http:\/\/dockai.com\u00a0\nYou can try wine quality example there.\u00a0\n\n\n\nBased on the feedback, we would like to extend and open it up. Feel free to reach out directly to me.\n\n\nThanks,\nHenry",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Shared Deployments of MLFlow",
        "Question_creation_time":1569414037000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mLqqkTBBel0",
        "Question_upvote_count":null,
        "Question_view_count":13.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Good afternoon,\n\n\n\nAre there any resources available to centralize MLFlow to a dedicated server which can:\n\n\n1) Archive models\n\n2) Host MLFlow web UI\n3) Host deployed model REST API\n\n\n\nIdeally, each model could be secured by groups or individual users, and shared with other groups \/ users.\n\n\n\nDo any existing example deployments exist? I see Databricks appears to have a hosted version of MLFlow per this announcement: https:\/\/mlflow.org\/news\/2019\/09\/10\/MLflow-Community-Edition\/index.html\n\n\nThanks!\n\n\n-Rob",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Depploy Mlflow with python wheel and egg file on dat bricks",
        "Question_creation_time":1648690892000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/bKm7IdEuARg",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi team,\nI have to deploy the model built on data bricks with mlflow\u00a0\nas python egg and wheel file on data bricks. please let me know the process and share me the resources.\n\n\n\nThanks in advance.\n\n\nRegards,\nNaveen",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.16.0 Released!",
        "Question_creation_time":1619476512000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/iCtI31h79PI",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow 1.16.0!\n\n\nIn addition to bug and documentation fixes, MLflow 1.16.0 includes the following features and improvements:\nAdd\u00a0mlflow.pyspark.ml.autolog()\u00a0API for autologging of\u00a0pyspark.ml\u00a0estimators (#4228,\u00a0@WeichenXu123)\nAdd\u00a0mlflow.catboost.log_model,\u00a0mlflow.catboost.save_model,\u00a0mlflow.catboost.load_model\u00a0APIs for CatBoost model persistence (#2417,\u00a0@harupy)\nEnable\u00a0mlflow.pyfunc.spark_udf\u00a0to use column names from model signature by default (#4236,\u00a0@Loquats)\nAdd\u00a0datetime\u00a0data type for model signatures (#4241,\u00a0@vperiyasamy)\nAdd\u00a0mlflow.sklearn.eval_and_log_metrics\u00a0API that computes and logs metrics for the given scikit-learn model and labeled dataset. (#4218,\u00a0@alkispoly-db)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.\n\n\nThanks,\nHaru",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow Serving",
        "Question_creation_time":1603082631000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/5QBHlQ90OuY",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all, I'm wondering if there is a limit for the curl json requests sent to the served model or not when using Sklearn's Linear Regression within mlflow's pyfunc. I receive an output for messages of about 75 records of input. However, for larger messages, I receive error 52, empty reply from server and I wonder why is that. For other served models I deployed, everything works normally such as custom pyfunc, and randomforests on R.\u00a0\n\n\nThanks a lot for any hints,\nCheers\u00a0\nKarim",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Shapash and MLFlow",
        "Question_creation_time":1631381970000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/D4ttV9ONWRA",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Dear all,\nI have a Model which is using shapash LIbray. Do I need any special precompilation to its input values, in order to serve it through MLFlow?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow workflows",
        "Question_creation_time":1581588566000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/w4BGI5qcFzE",
        "Question_upvote_count":null,
        "Question_view_count":26.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\u00a0\n\nI am new to mlflow and I'd like to ask for MLflow workflows capabilities. In April 2019 (SPARK+AI) two new components were announced for feature releases: MLflow workflows and MLflow Model registry. Afterwards, MLflow Model registry has been added (ver.1.4) but MLflow workflows has not till now (as far as I know). Does anyone know if MLflow workflows will be released and when (in which release)?\n\n\u00a0\n\nThanks,\n\n\u00a0\n\nDimitris",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"log_artifact not working",
        "Question_creation_time":1535031783000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/vyq2eJqDU0k",
        "Question_upvote_count":null,
        "Question_view_count":2390.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey folks,\n\n\nI got the mlflow ui server set up, and its saving everything except log_artifact.\n\n\nThe server is running mlflow in a docker container with continuum\/miniconda3 as the base image.\n\n\nFrom the command line, I'm launching the server like this:\n\n\n$ mlflow ui -h 0.0.0.0 -p 5000\n\n\nI can see and interact with the data.\n\n\nWhen I try to save the params, metrics and artifacts, I do this:\n\n\n# neumann\nmlflow_server = '52.89....'\n\n# Tracking URI\nmlflow_tracking_URI = 'http:\/\/' + mlflow_server + ':5000'\nprint (\"MLflow Tracking URI: %s\" % (mlflow_tracking_URI))\n\n\n# set tracking URI\nmlflow.set_tracking_uri(mlflow_tracking_URI)\n\n\nwith mlflow.start_run(experiment_id=3):\n\u00a0 \u00a0 mlflow.log_param(\"depth\", 5)\n\u00a0 \u00a0 mlflow.log_metric(\"roc_auc\", 0.8)\n\u00a0 \u00a0 mlflow.log_artifact(local_path='curve.png')\n\nThis is my FileNotFoundError error message:\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n<ipython-input-113-f5870bc80ffe> in <module>()\n      2     mlflow.log_param(\"depth\", 5)\n      3     mlflow.log_metric(\"roc_auc\", 0.8)\n----> 4     mlflow.log_artifact(local_path='curve.png')\n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py in log_artifact(local_path, artifact_path)\n    131     \"\"\"Log a local file or directory as an artifact of the currently active run.\"\"\"\n    132     artifact_uri = _get_or_start_run().info.artifact_uri\n--> 133     get_service().log_artifact(artifact_uri, local_path, artifact_path)\n    134 \n    135 \n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/tracking\/service.py in log_artifact(self, artifact_uri, local_path, artifact_path)\n    105         :param artifact_path: If provided, will be directory in artifact_uri to write to\"\"\"\n    106         artifact_repo = ArtifactRepository.from_artifact_uri(artifact_uri, self.store)\n--> 107         artifact_repo.log_artifact(local_path, artifact_path)\n    108 \n    109     def log_artifacts(self, artifact_uri, local_dir, artifact_path=None):\n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/store\/local_artifact_repo.py in log_artifact(self, local_file, artifact_path)\n     14             if artifact_path else self.artifact_uri\n     15         if not exists(artifact_dir):\n---> 16             mkdir(artifact_dir)\n     17         shutil.copy(local_file, artifact_dir)\n     18 \n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/utils\/file_utils.py in mkdir(root, name)\n     99             return target\n    100     except OSError as e:\n--> 101         raise e\n    102 \n    103 \n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/utils\/file_utils.py in mkdir(root, name)\n     96     try:\n     97         if not exists(target):\n---> 98             os.mkdir(target)\n     99             return target\n    100     except OSError as e:\n\nFileNotFoundError: [Errno 2] No such file or directory: '\/mlruns\/3\/1053b732c0a14d6cb8c07ee4320fd781\/artifacts'\n\n\n\nWhen I look at the filesystem, everything is saving except artifacts:\n\n\n~\/mlruns\/3$ tree\n.\n\u251c\u2500\u2500 5dcd18160aa74e6e8e405a6257a13177\n\u2502 \u00a0 \u251c\u2500\u2500 artifacts\n\u2502 \u00a0 \u251c\u2500\u2500 meta.yaml\n\u2502 \u00a0 \u251c\u2500\u2500 metrics\n\u2502 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 roc_auc\n\u2502 \u00a0 \u2514\u2500\u2500 params\n\u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 depth\n\n\nAny suggestions?\n\n\nThanks!\nFranklin",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to delete previously logged metrics?",
        "Question_creation_time":1655988394000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/e35ItjNDE20",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"It am aware its quite easy to add metrics to an old experiment with\n\n\n```\nwith mlflow.start_run(run_id=some_old_run_id) as run:\n\u00a0 \u00a0 mlflow.log_metric(k,v)\n```\nHowever, I was wondering if we could delete metrics as well?\n\n\nWe have some old experiments that used the incorrect key convention and we want to delete old keys\/value pairs from the jsonb column.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 0.8.0 released!",
        "Question_creation_time":1542041230000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/TgR6lSe4gfA",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow 0.8.0 has been released: https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v0.8.0\n\n\nMLflow 0.8.0 introduces several major features:\n\n\n- Dramatically improved UI for comparing experiment run results (grouping columns, showing nested runs, using run names instead of ids, and persisting table state)\n- Support for deploying models as Docker containers directly to Azure Machine Learning Service Workspace (as opposed to the previously-recommended solution of Azure ML Workbench)\n\n\nIn addition to these features, there are a host of improvements and bugfixes to the REST API, R API, Python API, tracking UI, and documentation.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Train and deploy H2O model using MLFlow spark",
        "Question_creation_time":1572311154000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/RJBzJjxPl0w",
        "Question_upvote_count":null,
        "Question_view_count":9.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI want to train and deploy a h2o model using mlfow spark as mentioned in the diagram:\u00a0https:\/\/res.infoq.com\/presentations\/mlflow-databricks\/en\/slides\/sl21-1566324281761.jpg\nI am training the model using below link:\nhttps:\/\/docs.databricks.com\/_static\/notebooks\/h2o-sparkling-water-python.html\nThen after training when I try to deploy the model using mlflow spark, it throws an error \"MLFlow can only save descendants of pyspark.ml.Model which implement MLReadable and MLWritable\".\nCan anyone help and let me know what I am doing wrong.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using MLflow and Sagemaker with preprocessing steps",
        "Question_creation_time":1658304744000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/I7LdK_7KCIw",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the\u00a0mlflow_pyfunc\u00a0is closer to what I want, but I'm not sure if it is compatible with Sagemaker.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Tags to MLflow registry",
        "Question_creation_time":1633057625000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ADNSwAemjAw",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi Team,\n\n\nI am looking to add custom tags to a model registered in mlflow registry via python api. I am able to add tags from the UI but unable to find an api to do it programmatically.\u00a0\u00a0\nThe set_tag api only allows to add tags to a run and those tags are not being taken forward when the model is registered. Please help.\n\n\nThank you,\nJagdeesh R",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tracking URI",
        "Question_creation_time":1621583202000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/kJOp3qmaqdI",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello Community\n\n\nI am trying to save and register only the last 5 runs and one older run among the 20 runs registered is there a way to delete it all at once instead of using the ui\n\n\n\n\nRegards,\nAarthi",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[RFC] Extended Search and Pagination functionality",
        "Question_creation_time":1557323729000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/IdVF99MAgyM",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Overview\n\nSearch is one of the most used APIs in MLflow to read logged experiment and run data. As organizations and projects scale and an increasing number of ML runs are logged, users have a need to severely limit the search results returned. In addition to selecting specific rows, there may be needs to limit the number of parameters, metrics, tags, and run attributes returned with searches. In cases where a large number of runs are produced from a search, there is a need to paginate the results at server-side and return only a limited set of runs at a time.\n\nIn this RFC, we present proposals to extend search functionality for advanced use cases along with some optimizations. We also discuss proposed solution for server-side pagination of results.\n\n\n\nRequest for comment\n\nFull RFC for these 2 features is in this google doc\u00a0(which has comment access). We are looking forward to your feedback directly in this document.Thank you!\n\n\n\n\nMani Parkhe\n\nma...@databricks.com",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using SQL to log params",
        "Question_creation_time":1579190481000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/7dnPPx5tXSk",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nThis is probably a stupid question, but as far as I have seen mlflow does not allow to log params in a SQL databse. Am I right?\nIf so, what is the main reason for that? Is that due to the fact that through the project the number of parameters might increase largely?\n\n\nAlso, I'm currently running computations on a compute server that I access through ssh from my laptop. The loggings happen on that machine. I then download to my laptop the mlruns folder and I have to run a script I wrote to update the yaml files so that they point to the right artifact location on my laptop. That's not much of a hassle but is there a better way to do that? I cannot run the http server through mlflow serve as I am not allowed to do that on the compute server I use.\n\n\nThanks.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFLOW project run from artifactory (s3)",
        "Question_creation_time":1561715227000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CJQN4zxO7ZE",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"I am testing MLFLOW; MLFLOW runs in docker container, artifactory is in MINIO ( runs in its own docker container); backend store is in Postgres ( runs in its own docker container). I run my experiments in Jupiter notebook ( runs in its own docker container). After I run an experiment, I can see artifacts located in MINIO S3 bucket - MLModel, conda.yml and model.pkl files. How do I create a project and run it? I have to created MLProject file, do I create it locally and specify full path to the artifacts (s3 full path)?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Reproducible runs with Mlflow Projects Problem Authenticating databrics",
        "Question_creation_time":1569824545000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/GzWFU13yNoc",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I am running the tutorial\u00a0https:\/\/docs.databricks.com\/applications\/mlflow\/projects.html#prerequisites\nto create an experiement in databrics and run an MLflow project from my local machine. I have setup the databrics cli authentication and MLFLOW_TRACKING_URI=databrics\n\n\nWhen I run the command\u00a0mlflow\u00a0run\u00a0examples\/sklearn_elasticnet_wine\/\u00a0-b\u00a0databricks\u00a0-c\u00a0clusterspec.json\u00a0--experiment-id\u00a03914263189079640\u00a0-P\u00a0alpha=0.5\n\n\nI get the following error\n2019\/09\/30\u00a015:15:36\u00a0ERROR\u00a0mlflow.utils.rest_utils:\u00a0API\u00a0request\u00a0to\u00a0https:\/\/westeurope.azuredatabricks.net\/?o=7680777384243007\/api\/2.0\/mlflow\/runs\/create\u00a0failed\u00a0with\u00a0code\u00a0500\u00a0!=\u00a0200,\u00a0retrying\u00a0up\u00a0to\u00a02\u00a0more\u00a0times.\u00a0API\u00a0response\u00a0body:\u00a0<html>\n<head>\n<meta\u00a0http-equiv=\"Content-Type\"\u00a0content=\"text\/html;charset=ISO-8859-1\"\/>\n<title>Error\u00a0500\u00a0<\/title>\n<\/head>\n<body>\n<h2>HTTP\u00a0ERROR:\u00a0500<\/h2>\n<p>Problem\u00a0accessing\u00a0\/login.html.\u00a0Reason:\n<pre>\u00a0\u00a0\u00a0\u00a0java.lang.NumberFormatException:\u00a0For\u00a0input\u00a0string:\u00a0&quot;7680777384243007\/api\/2.0\/mlflow\/runs\/create&quot;<\/pre><\/p>\n<hr\u00a0\/>\n<\/body>\n<\/html>\n\n\n2019\/09\/30\u00a015:15:40\u00a0ERROR\u00a0mlflow.utils.rest_utils:\u00a0API\u00a0request\u00a0to\u00a0https:\/\/westeurope.azuredatabricks.net\/?o=7680777384243007\/api\/2.0\/mlflow\/runs\/create\u00a0failed\u00a0with\u00a0code\u00a0500\u00a0!=\u00a0200,\u00a0retrying\u00a0up\u00a0to\u00a01\u00a0more\u00a0times.\u00a0API\u00a0response\u00a0body:\u00a0<html>\n<head>\n<meta\u00a0http-equiv=\"Content-Type\"\u00a0content=\"text\/html;charset=ISO-8859-1\"\/>\n<title>Error\u00a0500\u00a0<\/title>\n<\/head>\n<body>\n<h2>HTTP\u00a0ERROR:\u00a0500<\/h2>\n<p>Problem\u00a0accessing\u00a0\/login.html.\u00a0Reason:\n<pre>\u00a0\u00a0\u00a0\u00a0java.lang.NumberFormatException:\u00a0For\u00a0input\u00a0string:\u00a0&quot;7680777384243007\/api\/2.0\/mlflow\/runs\/create&quot;<\/pre><\/p>\n<hr\u00a0\/>\n<\/body>\n<\/html>\n\n\n2019\/09\/30\u00a015:15:44\u00a0ERROR\u00a0mlflow.utils.rest_utils:\u00a0API\u00a0request\u00a0to\u00a0https:\/\/westeurope.azuredatabricks.net\/?o=7680777384243007\/api\/2.0\/mlflow\/runs\/create\u00a0failed\u00a0with\u00a0code\u00a0500\u00a0!=\u00a0200,\u00a0retrying\u00a0up\u00a0to\u00a00\u00a0more\u00a0times.\u00a0API\u00a0response\u00a0body:\u00a0<html>\n<head>\n<meta\u00a0http-equiv=\"Content-Type\"\u00a0content=\"text\/html;charset=ISO-8859-1\"\/>\n<title>Error\u00a0500\u00a0<\/title>\n<\/head>\n<body>\n<h2>HTTP\u00a0ERROR:\u00a0500<\/h2>\n<p>Problem\u00a0accessing\u00a0\/login.html.\u00a0Reason:\n<pre>\u00a0\u00a0\u00a0\u00a0java.lang.NumberFormatException:\u00a0For\u00a0input\u00a0string:\u00a0&quot;7680777384243007\/api\/2.0\/mlflow\/runs\/create&quot;<\/pre><\/p>\n<hr\u00a0\/>\n<\/body>\n<\/html>\n\n\nTraceback\u00a0(most\u00a0recent\u00a0call\u00a0last):\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/bin\/mlflow\",\u00a0line\u00a010,\u00a0in\u00a0<module>\n\u00a0\u00a0\u00a0\u00a0sys.exit(cli())\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/click\/core.py\",\u00a0line\u00a0764,\u00a0in\u00a0__call__\n\u00a0\u00a0\u00a0\u00a0return\u00a0self.main(*args,\u00a0**kwargs)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/click\/core.py\",\u00a0line\u00a0717,\u00a0in\u00a0main\n\u00a0\u00a0\u00a0\u00a0rv\u00a0=\u00a0self.invoke(ctx)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/click\/core.py\",\u00a0line\u00a01137,\u00a0in\u00a0invoke\n\u00a0\u00a0\u00a0\u00a0return\u00a0_process_result(sub_ctx.command.invoke(sub_ctx))\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/click\/core.py\",\u00a0line\u00a0956,\u00a0in\u00a0invoke\n\u00a0\u00a0\u00a0\u00a0return\u00a0ctx.invoke(self.callback,\u00a0**ctx.params)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/click\/core.py\",\u00a0line\u00a0555,\u00a0in\u00a0invoke\n\u00a0\u00a0\u00a0\u00a0return\u00a0callback(*args,\u00a0**kwargs)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/cli.py\",\u00a0line\u00a0137,\u00a0in\u00a0run\n\u00a0\u00a0\u00a0\u00a0run_id=run_id\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/projects\/__init__.py\",\u00a0line\u00a0266,\u00a0in\u00a0run\n\u00a0\u00a0\u00a0\u00a0use_conda=use_conda,\u00a0storage_dir=storage_dir,\u00a0synchronous=synchronous,\u00a0run_id=run_id)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/projects\/__init__.py\",\u00a0line\u00a094,\u00a0in\u00a0_run\n\u00a0\u00a0\u00a0\u00a0active_run\u00a0=\u00a0_create_run(uri,\u00a0experiment_id,\u00a0work_dir,\u00a0entry_point)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/projects\/__init__.py\",\u00a0line\u00a0622,\u00a0in\u00a0_create_run\n\u00a0\u00a0\u00a0\u00a0active_run\u00a0=\u00a0tracking.MlflowClient().create_run(experiment_id=experiment_id,\u00a0tags=tags)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/tracking\/client.py\",\u00a0line\u00a088,\u00a0in\u00a0create_run\n\u00a0\u00a0\u00a0\u00a0tags=[RunTag(key,\u00a0value)\u00a0for\u00a0(key,\u00a0value)\u00a0in\u00a0iteritems(tags)]\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/store\/rest_store.py\",\u00a0line\u00a0153,\u00a0in\u00a0create_run\n\u00a0\u00a0\u00a0\u00a0response_proto\u00a0=\u00a0self._call_endpoint(CreateRun,\u00a0req_body)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/store\/rest_store.py\",\u00a0line\u00a062,\u00a0in\u00a0_call_endpoint\n\u00a0\u00a0\u00a0\u00a0host_creds=host_creds,\u00a0endpoint=endpoint,\u00a0method=method,\u00a0json=json_body)\n\u00a0\u00a0File\u00a0\"\/root\/mlflow\/new-venv\/lib\/python3.5\/site-packages\/mlflow\/utils\/rest_utils.py\",\u00a0line\u00a059,\u00a0in\u00a0http_request\n\u00a0\u00a0\u00a0\u00a0(url,\u00a0retries))\nmlflow.exceptions.MlflowException:\u00a0API\u00a0request\u00a0to\u00a0https:\/\/westeurope.azuredatabricks.net\/?o=7680777384243007\/api\/2.0\/mlflow\/runs\/create\u00a0failed\u00a0to\u00a0return\u00a0code\u00a0200\u00a0after\u00a03\u00a0tries",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.30.0 release",
        "Question_creation_time":1666290242000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/h0AobjGBZ6Q",
        "Question_upvote_count":null,
        "Question_view_count":9.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of MLflow 1.30.0!\n\nMLflow 1.30.0 includes several major features and improvements\n\nFeatures:\n[Pipelines] Introduce hyperparameter tuning support to MLflow Pipelines (#6859, @prithvikannan)\n[Pipelines] Introduce support for prediction outlier comparison to training data set (#6991, @jinzhang21)\n[Pipelines] Introduce support for recording all training parameters for reproducibility (#7026, #7094, @prithvikannan)\n[Pipelines] Add support for Delta tables as a datasource in the ingest step (#7010, @sunishsheth2009)\n[Pipelines] Add expanded support for data profiling up to 10,000 columns (#7035, @prithvikanna)\n[Pipelines] Add support for AutoML in MLflow Pipelines using FLAML (#6959, @mshtelma)\n[Pipelines] Add support for simplified transform step execution by allowing for unspecified configuration (#6909, @Apurva Koti)\n[Pipelines] Introduce a data preview tab to the transform step card (#7033, @prithvikannan)\n[Tracking] Introduce run_name attribute for create_run, get_run and update_run APIs (#6782, #6798 @Apurva Koti)\n[Tracking] Add support for searching by creation_time and last_update_time for the search_experiments API (#6979, @harupy)\n[Tracking] Add support for search terms run_id IN and run ID NOT IN for the search_runs API (#6945, @harupy)\n[Tracking] Add support for searching by user_id and end_time for the search_runs API (#6881, #6880 @subramaniam02)\n[Tracking] Add support for searching by run_name and run_id for the search_runs API (#6899, @harupy; #6952, @alexacole)\n[Tracking] Add support for synchronizing run name attribute and mlflow.runName tag (#6971, @BenWilson2)\n[Tracking] Add support for signed tracking server requests using AWSSigv4 and AWS IAM (#7044, @pdifranc)\n[Tracking] Introduce the update_run() API for modifying the status and name attributes of existing runs (#7013, @gabrielfu)\n[Tracking] Add support for experiment deletion in the mlflow gc cli API (#6977, @shaikmoeed)\n[Models] Add support for environment restoration in the evaluate() API (#6728, @jerrylian-db)\n[Models] Remove restrictions on binary classification labels in the evaluate() API (#7077, @dbczumar)\n[Scoring] Add support for BooleanType to mlflow.pyfunc.spark_udf() (#6913, @BenWilson2)\n[SQLAlchemy] Add support for configurable Pool class options for SqlAlchemyStore (#6883, @mingyu89)\n\nBug fixes:\n[Pipelines] Enable Pipeline subprocess commands to create a new SparkSession if one does not exist (#6846, @prithvikannan)\n[Pipelines] Fix a rendering issue with bool column types in Step Card data profiles (#6907, @sunishsheth2009)\n[Pipelines] Add validation and an exception if required step files are missing (#7067, @mingyu89)\n[Pipelines] Change step configuration validation to only be performed during runtime execution of a step (#6967, @prithvikannan)\n[Tracking] Fix infinite recursion bug when inferring the model schema in mlflow.pyspark.ml.autolog() (#6831, @harupy)\n[UI] Remove the browser error notification when failing to fetch artifacts (#7001, @kevingreer)\n[Models] Allow mlflow-skinny package to serve as base requirement in MLmodel requirements (#6974, @BenWilson2)\n[Models] Fix an issue with code path resolution for loading SparkML models (#6968, @dbczumar)\n[Models] Fix an issue with dependency inference in logging SparkML models (#6912, @BenWilson2)\n[Models] Fix an issue involving potential duplicate downloads for SparkML models (#6903, @Serena Ruan)\n[Models] Add missing pos_label to sklearn.metrics.precision_recall_curve in mlflow.evaluate() (#6854, @dbczumar)\n[SQLAlchemy] Fix a bug in SqlAlchemyStore where set_tag() updates the incorrect tags (#7027, @gabrielfu)\n\nDocumentation updates:\n[Models] Update details regarding the default Keras serialization format (#7022, @balvisio)\n\nFor a comprehensive list of changes, see the release change log, and check out the latest documentation on mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to backup all MLFlow Tracking and Model Registry data from a Databricks Workspace",
        "Question_creation_time":1611072809000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Kttrnxcw3C4",
        "Question_upvote_count":null,
        "Question_view_count":486.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Dear all,\n\n\nI have a few questions, so please bear with me.\n\n\n\nThe MLFlow documentation does not provide any information regarding backups, except here. If I managed my own tracking server, I could make a backup, but how to I achieve this within Databricks?\n\n\npprint([dict(e)[\"artifact_location\"] for e in MlflowClient().list_experiments()])\n\n\nshows that the artifact locations are in dbfs:\/databricks\/mlflow-tracking\/, but these are accessible only with the client ( https:\/\/docs.microsoft.com\/de-de\/azure\/databricks\/security\/access-control\/workspace-acl#mlflow-artifact-permissions ) I had thought about a blunt rsync copy all artifacts, but these would probably be problematic for the model lineage stored in the Registry.\n\n\n\nI was hoping to get an idea of the tracking server inside a Databricks Notebook, with the env variable MLFLOW_TRACKING_URI, but its value is only:\n\nMLFLOW_TRACKING_URI=databricks\n\n3. In short, if I would delete a Databricks Workspace and wanted to have the full Tracking and Model Registry in a new one, how would I go about it?\n\n\nThank you for any help!\n\n\n-Alec",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to use MLFlow logo on our landing page?",
        "Question_creation_time":1576476836000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/F1YiBxZa1dc",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\u00a0\n\n\nAt NBT AG, we are currently developing\u00a0a\u00a0deployment and orchestration platform for MLOps. We are planning to make an integration of MLflow for deploying models on edge devices through our platform. Could we use MLflow logo on our landing page already until we actually make that integration?\n\n\nThank you!\n\n\nSimon Bernard",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.20.2 released!",
        "Question_creation_time":1630702851000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/LO5wYh97kk4",
        "Question_upvote_count":null,
        "Question_view_count":15.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow\u00a01.20.2!\n\nMLflow 1.20.2 is a patch release containing the following features and bug fixes:\n\nFeatures:\n\n- Enabled auto dependency inference in spark flavor in autologging (#4759, @harupy)\n\nBug fixes and documentation updates:\n\n- Increased MLflow client HTTP request timeout from 10s to 120s (#4764, @jinzhang21)\n- Fixed autologging compatibility bugs with TensorFlow and Keras version 2.6.0 (#4766, @dbczumar)\n\nSmall bug fixes and doc updates (#4770, @WeichenXu123)\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Website section \"organizations using MLflow\"",
        "Question_creation_time":1604036380000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/AL3eITG_MP8",
        "Question_upvote_count":null,
        "Question_view_count":24.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI am from https:\/\/mlcertific.com which provides the certification on machine learning\u00a0 we are using mlflow in our organisation and want to contribute to it. Please add our organization\u00a0in the list\u00a0 .\n\n\nThanks,\nhttps:\/\/mlcertific.com\/",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Upgrade mlflow",
        "Question_creation_time":1624850016000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CX4PiIGYZiE",
        "Question_upvote_count":null,
        "Question_view_count":114.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"The following error is thrown when I upgrade mlflow from 1.13.1 to 1.18.0\n\n\nalembic.util.exc.CommandError: Can't locate revision identified by 'a8c4a736bde6' mlflow\n\n\n\nWould deleting the alembic version fix the issue or is there any alternate solution?\n\n\nThank you and Warm Regards",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Support for Oracle dialect for backend-store-uri",
        "Question_creation_time":1636675432000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/zB-1xLJ6IJk",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\nDoes anyone knows any way for Oracle database as backend-store-uri\nCurrently, only mysql,\u00a0mssql,\u00a0sqlite, and\u00a0PostgreSQL are supported.\n\n\nRegards,\nNikhil",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow-Docker Artifacts Model Not Found",
        "Question_creation_time":1624881919000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/JFdrWyG9vIA",
        "Question_upvote_count":null,
        "Question_view_count":28.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"https:\/\/github.com\/mlflow\/mlflow\/issues\/4487\n\n\n\nHi, I have posted this issue in github, please help me to solve this issue",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Doubt regarding MLFlow",
        "Question_creation_time":1654009019000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/s717aOcKwQ0",
        "Question_upvote_count":null,
        "Question_view_count":15.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Can we do training datasets and feature storing , recurrent model training and deployment pipelines and integration of ML models using MLFlow ?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.0 Release Candidate Process",
        "Question_creation_time":1558522998000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/HIlqey_t2qU",
        "Question_upvote_count":null,
        "Question_view_count":36.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We\u2019re excited to announce that we\u2019re moving towards MLflow 1.0, with a tentative release date of May 29. In addition to some exciting new features (dramatically improved metric visualizations, metric x coordinates, improved search functionality), MLflow 1.0 offers Python, Java, R, and REST API stability.\n\n\nTo that end, we\u2019ve published an RC of MLflow 1.0, which you can use to try out all the latest 1.0 features. We\u2019d love to get your feedback and fix any issues that arise before the 1.0 release. Please report issues at https:\/\/github.com\/mlflow\/mlflow\/issues.\n\n\nPlease see this document for instructions on how to try out the latest RC:\nhttps:\/\/docs.google.com\/document\/d\/1Hu1y73aR21uDPbuUTBlTfSN7e5tR-9txqk_VHlCnupk\/edit#\n\n\nThanks!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow integration with Kubeflow",
        "Question_creation_time":1567801317000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/b8WLoMLFtmo",
        "Question_upvote_count":null,
        "Question_view_count":30.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\nCan anyone point me to docs that talks about integration of how MLFlow can be integrated to use Kubeflow. As per my understanding by looking at the code of MLFlow, code has to be written to integrate it with Kubeflow. By default support does not exist.\n\n\nPlease advise.\n\n\n-Sid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi user on one DB ok?",
        "Question_creation_time":1583597894000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/VQc6nWUNjYM",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"When I use tracking directly on a Database (PostgreSQL) would it cause problems when two computer do tracking into the same DB at the same time?\n\n\n\nSame question with MLFlow UI: Is it ok when we connect two UIs from two different computers to one (the same) DB?\n\n\nMy guess is that it is no problem but just want to make sure.\n\n\nThanks\nPhilip",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 0.6.0 released!",
        "Question_creation_time":1536613157000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/kJz7T4052RM",
        "Question_upvote_count":null,
        "Question_view_count":29.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow 0.6.0 has been released: https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v0.6.0\n\n\nMLflow 0.6.0 introduces several major features:\n\n\n- A Java client API (to be published on Maven within the next day or two)\n- Support for saving and serving SparkML models as MLeap for low-latency serving\n- Support for tagging runs with metadata, during and after the run completion\n- Support for deleting (and restoring deleted) experiments\n\n\nIn addition to these features, there are a host of improvements and bugfixes to the REST API, Python API, tracking UI, and documentation.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow Lightweight Python Client",
        "Question_creation_time":1644037406000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/_QoyNVy60Bs",
        "Question_upvote_count":null,
        "Question_view_count":84.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Dear MLflow users,\n\nWe actively use MLFlow at MTS as a part of our MLOps platform for experiment tracking and model registry.\n\nFurthermore, I\u2019m happy to say that we started to post on GitHub under Apache 2.0 license some parts of our inhouse development around MLFlow.\n\nLightweight python client for MLflow REST API:\n\nhttps:\/\/github.com\/MobileTeleSystems\/mlflow-rest-client\n\nMain features:\n\nMinimal dependencies\nAll REST API methods and params are exposed to user\nAll methods and classes are documented\n\nFeel free to contribute and\/or open issues on GitHub.\n\nP.S. It would be nice if you add our logo to the \u201cOrganizations using and contributing\u201d section. Website:\u00a0https:\/\/mts.ru\/\n\n\u00a0\n\nBest Regards,\n\nMax Bartenev, CTO\n\nMTS BigData department",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Bay Area MLflow Meetup @ Mesoshere",
        "Question_creation_time":1537454038000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/h8MFLcRq11M",
        "Question_upvote_count":null,
        "Question_view_count":30.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"If you can't flow with us in person, do so remotely:\n\n\nMEETUP @ Mesosphere :\u00a0 https:\/\/www.meetup.com\/Bay-Area-MLflow\/events\/254124344\/\n\n\nZOOM:\u00a0https:\/\/mesosphere.zoom.us\/j\/823248629\n\n\nCheers\nJules",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.28.0 released!",
        "Question_creation_time":1660199398000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/2BtF8qwt0Zc",
        "Question_upvote_count":null,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of\u00a0MLflow\u00a01.28.0\u00a0!\n\n\nMLflow\u00a01.28.0 includes several major features and improvements:\n\nFeatures:\n\n[Pipelines] Log the full Pipeline runtime configuration to MLflow Tracking during Pipeline execution (#6359,\u00a0@jinzhang21)\n[Pipelines] Add\u00a0pipeline.yaml\u00a0configurations to specify the Model Registry backend used for model registration (#6284,\u00a0@sunishsheth2009)\n[Pipelines] Support optionally skipping the\u00a0transform\u00a0step of the scikit-learn regression pipeline (#6362,\u00a0@sunishsheth2009)\n[Pipelines] Add UI links to Runs and Models in Pipeline Step Cards on Databricks (#6294,\u00a0@dbczumar)\n[Tracking] Introduce\u00a0mlflow.search_experiments()\u00a0API for searching experiments by name and by tags (#6333,\u00a0@WeichenXu123;\u00a0#6227,\u00a0#6172,\u00a0#6154,\u00a0@harupy)\n[Tracking] Increase the maximum parameter value length supported by File and SQL backends to 500 characters (#6358,\u00a0@johnyNJ)\n[Tracking] Introduce an\u00a0--older-than\u00a0flag to\u00a0mlflow gc\u00a0for removing runs based on deletion time (#6354,\u00a0@Jason-CKY)\n[Tracking] Add\u00a0MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE\u00a0environment variable for recycling SQLAlchemy connections (#6344,\u00a0@postrational)\n[UI] Display deeply nested runs in the Runs Table on the Experiment Page (#6065,\u00a0@tospe)\n[UI] Add box plot visualization for metrics to the Compare Runs page (#6308,\u00a0@ahlag)\n[UI] Display tags on the Compare Runs page (#6164,\u00a0@CaioCavalcanti)\n[UI] Use scientific notation for axes when viewing metric plots in log scale (#6176,\u00a0@RajezMariner)\n[UI] Add button to Metrics page for downloading metrics as CSV (#6048,\u00a0@rafaelvp-db)\n[UI] Include NaN and +\/- infinity values in plots on the Metrics page (#6422,\u00a0@hubertzub-db)\n[Tracking \/ Model Registry] Introduce environment variables to control retry behavior and timeouts for REST API requests (#5745,\u00a0@peterdhansen)\n[Tracking \/ Model Registry] Make\u00a0MlflowClient\u00a0importable as\u00a0mlflow.MlflowClient\u00a0(#6085,\u00a0@subramaniam02)\n[Model Registry] Add support for searching registered models and model versions by tags (#6413,\u00a0#6411,\u00a0#6320,\u00a0@WeichenXu123)\n[Model Registry] Add\u00a0stage\u00a0parameter to\u00a0set_model_version_tag()\u00a0(#6185,\u00a0@subramaniam02)\n[Model Registry] Add\u00a0--registry-store-uri\u00a0flag to\u00a0mlflow server\u00a0for specifying the Model Registry backend URI (#6142,\u00a0@Secbone)\n[Models] Improve performance of Spark Model logging on Databricks (#6282,\u00a0@bbarnes52)\n[Models] Include Pandas Series names in inferred model schemas (#6361,\u00a0@RynoXLI)\n[Scoring] Make\u00a0model_uri\u00a0optional in\u00a0mlflow models build-docker\u00a0to support building generic model serving images (#6302,\u00a0@harupy)\n[R] Support logging of NA and NaN parameter values (#6263,\u00a0@nathaneastwood)\n\nBug fixes and documentation updates:\n\n[Pipelines] Improve scikit-learn regression pipeline latency by limiting dataset profiling to the first 100 columns (#6297,\u00a0@sunishsheth2009)\n[Pipelines] Use\u00a0xdg-open\u00a0instead of\u00a0open\u00a0for viewing Pipeline results on Linux systems (#6326,\u00a0@strangiato)\n[Pipelines] Fix a bug that skipped Step Card rendering in Jupyter Notebooks (#6378,\u00a0@apurva-koti)\n[Tracking] Use the 401 HTTP response code in authorization failure REST API responses, instead of 500 (#6106,\u00a0@balvisio)\n[Tracking] Correctly classify artifacts as files and directories when using Azure Blob Storage (#6237,\u00a0@nerdinand)\n[Tracking] Fix a bug in the File backend that caused run metadata to be lost in the event of a failed write (#6388,\u00a0@dbczumar)\n[Tracking] Adjust\u00a0mlflow.pyspark.ml.autolog()\u00a0to only log model signatures for supported input \/ output data types (#6365,\u00a0@harupy)\n[Tracking] Adjust\u00a0mlflow.tensorflow.autolog()\u00a0to log TensorFlow early stopping callback info when\u00a0log_models=False\u00a0is specified (#6170,\u00a0@WeichenXu123)\n[Tracking] Fix signature and input example logging errors in\u00a0mlflow.sklearn.autolog()\u00a0for models containing transformers (#6230,\u00a0@dbczumar)\n[Tracking] Fix a failure in\u00a0mlflow gc\u00a0that occurred when removing a run whose artifacts had been previously deleted (#6165,\u00a0@dbczumar)\n[Tracking] Add missing\u00a0sqlparse\u00a0library to MLflow Skinny client, which is required for search support (#6174,\u00a0@dbczumar)\n[Tracking \/ Model Registry] Fix an\u00a0mlflow server\u00a0bug that rejected parameters and tags with empty string values (#6179,\u00a0@dbczumar)\n[Model Registry] Fix a failure preventing model version schemas from being downloaded with\u00a0--serve-arifacts\u00a0enabled (#6355,\u00a0@abbas123456)\n[Scoring] Patch the Java Model Server to support MLflow Models logged on recent versions of the Databricks Runtime (#6337,\u00a0@dbczumar)\n[Scoring] Verify that either the deployment name or endpoint is specified when invoking the\u00a0mlflow deployments predict\u00a0CLI (#6323,\u00a0@dbczumar)\n[Scoring] Properly encode datetime columns when performing batch inference with\u00a0mlflow.pyfunc.spark_udf()\u00a0(#6244,\u00a0@harupy)\n[Projects] Fix an issue where local directory paths were misclassified as Git URIs when running Projects (#6218,\u00a0@ElefHead)\n[R] Fix metric logging behavior for +\/- infinity values (#6271,\u00a0@nathaneastwood)\n[Docs] Move Python API docs for\u00a0MlflowClient\u00a0from\u00a0mlflow.tracking\u00a0to\u00a0mlflow.client\u00a0(#6405,\u00a0@dbczumar)\n[Docs] Document that MLflow Pipelines requires Make (#6216,\u00a0@dbczumar)\n[Docs] Improve documentation for developing and testing MLflow JS changes in\u00a0CONTRIBUTING.rst\u00a0(#6330,\u00a0@ahlag)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"We would like to be listed in the MLflow users\/contributors list",
        "Question_creation_time":1604586050000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/RXIK8zSzZPU",
        "Question_upvote_count":null,
        "Question_view_count":22.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nAt InfinStor, we are users of MLflow, we have a free MLflow SaaS offering in the cloud, and we hope to contribute to the MLflow open source project.\n\n\nWe would like to be listed in the users\/contributors list.\n\n\nThanks to the MLflow contributors for creating such an awesome project. Kudos to the architects of MLflow - it is a truly extensible piece of software.\n\n\nBest\nJagane\n\n\nPS: Included with this email is a logo of InfinStor",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow ui is not running in windows 10",
        "Question_creation_time":1574260355000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/YMa_sVOspgw",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"MLFLOW UI is not running to any of my installed browser while I tried to run it through terminal.Please help with a solution.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow Tracking server scalability",
        "Question_creation_time":1529642287000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/TQe7ATr8Wqw",
        "Question_upvote_count":null,
        "Question_view_count":107.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey Guys,\n\n\nFirst of all, great job with this effort. It's certainly something a lot of people are waiting for (or have tried to create themselves).\n\n\nI was wondering about the scalability of the tracking server. I see in the code there is an abstraction of for the tracking Store, which is currently a FileStore if I'm correct. What are the plans to support other stores for this (ElasticSearch, Kafka, S3, ...?)\n\n\nCheers,\nD.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 2.0.0rc0 release",
        "Question_creation_time":1667264453000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/khwIVANuAKI",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We're happy to announce a release candidate for MLflow 2.0:\nhttps:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v2.0.0rc0\n\nInstallation:\n===================================\n# Make sure python version is >=3.8\npip install mlflow==2.0.0rc0\n===================================\n\n\nDocumentation:\nMLflow 2.0.0rc0 documentation\n\nPlease report any issues with the release candidate in the issue tracker.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Not able to get expected output while serving the model using \"mlflow model serve\" while using sklearn-crfsuite",
        "Question_creation_time":1568339640000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/l7sPjsCx9dQ",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I am not able to get the expected response when calling api served with \"mlflow model serve\" command. It is working with other sklearn libraries but not with sklearn-crfsuite. Need help.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to connect to HDFS",
        "Question_creation_time":1590988383000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/8olskwFCHJY",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\nI'm installing MLFlow 1.6 on a production environment with a PostgreSQL database for --backend-store-uri and HDFS for --default-artifact-root.\u00a0When I launch an experiment it writes perfectly in PostgreSQL, but it cannot connect to HDFS to save the artifacts. El error is \"HDFS connection failed\".\u00a0HDFS is on a Cloudera cluster.I also tried MLFlow 1.8 and the error is the same.\n\n\nThanks",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Track training loss to Azure when running file locally",
        "Question_creation_time":1655382924000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QXD8azgicVI",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all :)\n\n\nI have followed the official microsoft tutorial (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-mlflow-cli-runs?tabs=mlflow) to log parameters or metrics in the Azure Workspace when I run something locally. That works just fine.\n\n\nHowever, what I want to do is run a train.py file by using the os.system command like\nos.system(f\"python .\/src\/train.py --data_path .\/data) instead of the os.system command used in the tutorial.\u00a0\n\n\nThis works fine in general. However, inside the train.py file, I am not able to get access to the Azure Machine Learning experiment that I created in the file which runs the os.system command. Therefore, I am not able to track the loss of the train.py file to Azure.\n\n\n\nDo you have any suggestions how I could solve that?\n\n\nThanks a lot already!\n\n\nBest regards",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Step by step instructions for running MLflow Projects in EKS",
        "Question_creation_time":1650997219000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/r-XDGcqb4Qg",
        "Question_upvote_count":null,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Folks - as you might know, MLflow includes preliminary support for running MLflow Projects in kubernetes. if you are interested in doing so, specifically on EKS, I wrote a medium article with step by step instructions.\n\n\nhttps:\/\/medium.com\/infinstor\/run-mlflow-project-in-eks-b0906e04c273\n\n\n\nCheers!\nJagane",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"databricks to power bi",
        "Question_creation_time":1621248613000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/GvkoTw0sL4U",
        "Question_upvote_count":null,
        "Question_view_count":9.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello community , i'm trying to save stream data in delta lake and i want to push this data to power bi for real time insights , however when i try to connect databricks to power bi i get an empty table , someone can help me please ??\u00a0\nif there's another alternative it would be grateful\u00a0\nthank you",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"runtime timeout?",
        "Question_creation_time":1537200910000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/JN9X_yUJ1bY",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\nIs there a way to set timeout value when mlflow.project.run(...) is called?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy a Shap explainer model with mlflow",
        "Question_creation_time":1570589759000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/d6FOZhHqTOg",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I have a shap explainer model and I want to deploy it using mlflow to azure AKS how can I do that. The model is available in model.pk file.\n\n\nimport shap\nX_train_summary = shap.kmeans(X_train,5)\nk_explainer = shap.KernelExplainer(model_bag_clf.predict_proba, X_train_summary)\nk_shap_values = k_explainer.shap_values(data_for_prediction)",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DATA Pill - knowledge-sharing project",
        "Question_creation_time":1656344537000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/hm7VvOcNIuY",
        "Question_upvote_count":null,
        "Question_view_count":16.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\nAs we are a community focused around MLFlow I thought I would send you some information about the new DATA Pill project.\u00a0\n\nI hope you will be interested because it covers our area and the project is focused on highly selected content for specialists.\n\nIt is also a community-driven project.\n\nDATA Pill is a weekly newsletter with an overview of the best Big Data, Cloud and AI\/ML content.\n\nfiltered content from over 200 sources\n\nextracts from articles, tutorials, podcasts, youtube, etc\n\na simple mail with a condensed form that you can skim through in just 10 minutes\u00a0\n\nreminders of upcoming meetups and events\n\nHere you can see examples of previous mails: DATA PiIl, so you can decide if it's something for you.\n\nIt started from the internal slack channel where we shared interesting links.\n\nOver time, the idea arose to organize this content more and gather it in one place\u00a0\n\nso that it would not get lost amongst hundreds of notifications.\u00a0\n\nSince we started doing this, browsing through even more sources of information,\u00a0\n\nsomeone threw in a thought: why not share it and allow everyone to subscribe?\n\n\n\n\nWe also want to involve everyone who is interested in creating this newsletter.\u00a0\n\nWe are in ongoing communication by mail and we are looking for a place where we could interact more.\n\nAny ideas are welcomed.\n\n\n\n\n\n\nCheers!\nSylwia from\u00a0GetInData",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"RFC: Add a \"step\" axis for logging metrics in MLflow",
        "Question_creation_time":1555358346000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/8zlfjQHnhgk",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Motivation \/ Overview\n\nTraining machine learning models is a process of iterative improvement. Models are successively exposed to sections of training data and adjusted based on their performance. Therefore, ML developers often need to observe how model performance changes throughout the training process. There are multiple ways to quantify the progress of the training process; here are several:\n\nThe amount of time, in seconds (wall clock time), that the training process has been running\nThe number of times that the model has been exposed to the entire training dataset. This is the number of training epochs.\nModels often make multiple passes through the training dataset.\nThe number of batches (fixed-size subsets of training data) to which the model has been exposed. This is the number of training iterations.\n\nMLflow currently provides an API for tracking model performance metrics; however, MLflow metrics can only be specified as a function of wall clock time. To paraphrase, wall clock time is the only supported x-axis on which metrics can recorded and visualized; wall clock time is the only supported x-coordinate for a metric entry.\n\nThe purpose of this RFC is to introduce support for recording and visualizing performance metrics against different types of axes, enabling ML developers to answer questions such as:\n\n\u201cWhat was the training accuracy of my model after the 50th training epoch?\u201d\n\u201cWhat was the precision of my model when evaluating the test data set after the 20th training iteration?\u201d\n\nSummary of Proposed Solution\nAt a high level, we propose to introduce a\u00a0step\u00a0attribute to MLflow metrics via the following high-level tasks:\n\nIntroduce a step attribute to MLflow\u2019s Metric entity definition\nUpdate backend store definitions to handle the new attribute\nUpdate the metric write path to create Metric entities that supply values for the step attribute\nThis includes client APIs, protos, the backend store write path, etc.\nExpand plotting capabilities of the MLflow UI to allow the user to toggle between the step axis and the \u201cwall clock time\u201d axis.\n\nFor more information, please see the full RFC below.\n\n\n\n\nRFC link\nThe full RFC is available as a Google Document at:\u00a0https:\/\/docs.google.com\/document\/d\/17yHR_xOvoEJQBT-D-y4QGfc3OvHqP-eGWwfMcGBIH2k\/edit?usp=sharing\n\nPlease leave comments and feedback within Google Docs!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow Meetup 1.0 @ Microsoft",
        "Question_creation_time":1561656475000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Xf5Bs8xootI",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello!\n\n\n\nIf you missed the last\u00a0MLflow meetup,\u00a0here are the slides and the video presentation.\n\nTalks for the meetup are on the SlideShare:\n1. Talk 1:\u00a0https:\/\/www.slideshare.net\/databricks\/flock-data-science-platform-cisl\n2. Talk-2:\u00a0https:\/\/www.slideshare.net\/databricks\/mlflow-10-meetup\n\nAnd the video is now available:\u00a0\u00a0https:\/\/youtu.be\/ILIllCMDEgc\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow.sagemaker.deploy",
        "Question_creation_time":1530872605000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/M51fi08Drk4",
        "Question_upvote_count":null,
        "Question_view_count":45.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi Databricks,\n\n\nI was successfully able to push image to ecr -\n\n\n\u00a0mlflow.sagemaker.push_image_to_ecr(image='mlflow_sage')\n\n\nI was trying to deploy the model using -\n\n\nmlflow.sagemaker.deploy(app_name,\u00a0model_path,\u00a0execution_role_arn,\u00a0bucket,\u00a0run_id=None,\u00a0image='mlflow_sage',\u00a0region_name='us-west-2')\n\n\napp_name\u00a0\u2013 Name of the deployed app.\n\n\nI am not sure what is deployed app. Could help here.\n\n\nThanks,\nSunil",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"request for help",
        "Question_creation_time":1620331635000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/k0kgHG1Qbm8",
        "Question_upvote_count":null,
        "Question_view_count":45.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"hello everyone , how can i solve this probblem plase\u00a0\nMlflowException: Failed to download an \"MLmodel\" model file from \"\/dbfs\/databricks\/mlflow-tracking\/4026011258138627\/e1e6a5f1819e426b81587a5ea2124228\/artifacts\/spark-model\": No such file or directory: '\/dbfs\/databricks\/mlflow-tracking\/4026011258138627\/e1e6a5f1819e426b81587a5ea2124228\/artifacts\/spark-model\/MLmodel'\n\nit's urgent , please help",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Request to add TVS in Organizations using & contributing to MLflow",
        "Question_creation_time":1627294324000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ReyGdDjmCeo",
        "Question_upvote_count":null,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi Team,\n\nAt TVS Motor we use MLflow extensively for ML model tracking, projects and registry. In the past, I was a contributor to MLflow as well.\n\nSo, can we get the TVS Motor listed in the \u201cOrganizations using and contributing to MLflow\u201d section, please?\n\n\n\n\n\u00a0\n\n\u00a0Thanks,\nNaga",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow for beginners",
        "Question_creation_time":1667821452000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/px-W1aq-dE8",
        "Question_upvote_count":null,
        "Question_view_count":9.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello everyone, I would like to use MLflow but I don't know how to register on the platform. Also, I would like to know if there are any step-by-step instructions on how to use this tool (for example, Run an MLflow project).\n\n\nthanks in advance,\n\n\n--\n\nOlga Ximena Giraldo Pasmin\nOrcid ID: orcid.org\/0000-0003-2978-8922\nTwiter: @olgaxgiraldo\nSkype:olgaximenagiraldo\nWebsite: http:\/\/oxgiraldo.wordpress.com",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does MLflow compare to ModelDB (A system to manage machine learning models)?",
        "Question_creation_time":1530686529000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/NW5GxrD9NQ0",
        "Question_upvote_count":null,
        "Question_view_count":701.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI wonder how MLflow compares to ModelDB (A system to manage machine learning models).\n\n\nIn ModelDB web page, I've seen Matei Zaharia as one of the contributors:\n\n\n\u00a0\u00a0\u00a0 https:\/\/mitdbg.github.io\/modeldb\/\n\n\nAny ideas?\n\n\n\n--\nEmre",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.12.0 released!",
        "Question_creation_time":1605122926000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/txoBu2XyQO8",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nIn addition to bug and documentation fixes,\u00a0MLflow 1.12.0\u00a0includes several major features and improvements, in particular a number of improvements to MLflow\u2019s Pytorch integrations and autologging:\n\nPyTorch:\nmlflow.pytorch.log_model,\u00a0mlflow.pytorch.load_model\u00a0now support logging\/loading TorchScript models (#3557,\u00a0@shrinath-suresh)\nmlflow.pytorch.log_model\u00a0supports passing\u00a0requirements_file\u00a0&\u00a0extra_files\u00a0arguments to log additional artifacts along with a model (#3436,\u00a0@shrinath-suresh)\nAutologging:\nAdd universal\u00a0mlflow.autolog\u00a0which enables autologging for all supported integrations (#3561,\u00a0#3590,\u00a0@andrewnitu)\nAdd\u00a0mlflow.pytorch.autolog\u00a0API for automatic logging of metrics, params, and models from Pytorch Lightning training (#3601,\u00a0@shrinath-suresh,\u00a0#3636,\u00a0@karthik-77). This API is also enabled by\u00a0mlflow.autolog.\nScikit-learn, XGBoost, and LightGBM autologging now support logging model signatures and input examples (#3386,\u00a0#3403,\u00a0#3449,\u00a0@andrewnitu)\nmlflow.sklearn.autolog\u00a0now supports logging metrics (e.g. accuracy) and plots (e.g. confusion matrix heat map) (#3423,\u00a0#3327,\u00a0@willzhan-db,\u00a0@harupy)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLProject",
        "Question_creation_time":1573712963000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/wfNFPYbJo6M",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"The sample MLProject file copied below contains entry_points for running train.py and validate.py. Kindly let me know the mlflow run command for running the validate.py script.\n\n\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 name: My Project\n\n\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 conda_env: my_env.yaml\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # Can have a docker_env instead of a conda_env, e.g.\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # docker_env:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 #\u00a0\u00a0\u00a0 image:\u00a0 mlflow-docker-example\n\n\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 entry_points:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 main:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 parameters:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 data_file: path\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 regularization: {type: float, default: 0.1}\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 command: \"python train.py -r {regularization} {data_file}\"\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 validate:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 parameters:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 data_file: path\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 command: \"python validate.py {data_file}\"\n\n\u00a0\n\nRegards\n\nBinay",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.15.0 Released!",
        "Question_creation_time":1616776948000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/hxFNU7qIBRw",
        "Question_upvote_count":null,
        "Question_view_count":13.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow 1.15.0!\nIn addition to bug and documentation fixes, MLflow 1.15.0 includes the following features and improvements:\nAdd\u00a0silent=False\u00a0option to all autologging APIs, to allow suppressing MLflow warnings and logging statements during autologging setup and training (#4173, @dbczumar)\nAdd\u00a0disable_for_unsupported_versions=False\u00a0option to all autologging APIs, to disable autologging for versions of ML frameworks that have not been explicitly tested against the current version of the MLflow client (#4119, @WeichenXu123)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.\n\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow hangs during training -- no error message",
        "Question_creation_time":1661292855000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/7-1JNBwU1aQ",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI'm training a large model (a TCN -- Temporal Convolutional Network) in TensorFlow 2.8.1 with Keras 2.8.0 and mlflow 1.27.0 on Ubuntu 18.04.\n\n\n\nI have a loop to do multiple mlflow training runs... I'm getting a \"random\" hang in the middle of training my 150 epochs.\u00a0 Sometimes 1-3 runs (out of 13) will complete ok, but there's always (somewhere within the first 3 runs) a hang or \"total freeze\" of output in my terminal with no error message or anything else... no output at all.\u00a0 The terminal just freezes on some random epoch during 1-3 runs.\n\nI checked and the GPU memory upper limit has not been hit... also system RAM has not been hit.\u00a0 Neither is close.\n\nAny ideas?\u00a0 Does MLflow ever freeze like this?\n\nThanks for any ideas :).",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to log_artifact using HDFS",
        "Question_creation_time":1560396586000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/3WYutmO54ao",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\nI am trying to use an HDFS for the artifact repository.\nI used the following command to run the mlflow server :\nmlflow server --host 0.0.0.0 --default-artifact-root hdfs:\/\/--.--.--.---:8020\/mlruns\n\nIn my python code :\n\nmlflow.create_experiment(\"NewTest21\",\"hdfs:\/\/--.--.--.---:8020\/mlruns\")\nmlflow.set_experiment(\"NewTest21\")\nwith mlflow.start_run():\n\tprint(mlflow.get_artifact_uri())\n\tlr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n\tlr.fit(train_x, train_y)\n\tpredicted_qualities = lr.predict(test_x)\n\t\n\t(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\tmlflow.log_param(\"alpha\", alpha)\n\tmlflow.log_param(\"l1_ratio\", l1_ratio)\n\tmlflow.log_metric(\"rmse\", rmse)\n\tmlflow.log_metric(\"r2\", r2)\n\tmlflow.log_metric(\"mae\", mae)\n\u00a0 \u00a0 \u00a0 \u00a0 with open(\"output.txt\", \"w\") as f:\n\u00a0 \u00a0 \t\tf.write(\"Hello world! Its a new day\")\n\u00a0 \u00a0 \t\tmlflow.log_artifact(\"output.txt\")\n\tmlflow.sklearn.log_model(lr,\"hdfs:\/\/--.--.--.---::8020\")\n\nHowever I am getting the following error :\n\nTraceback (most recent call last):\n\u00a0 File \"C:\\Program Files\\KNIME\\plugins\\org.knime.python2_3.7.1.v201901281201\\py\\PythonKernelBase.py\", line 278, in execute\n\u00a0 \u00a0 exec(source_code, self._exec_env, self._exec_env)\n\u00a0 File \"<string>\", line 59, in <module>\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 244, in log_artifact\n\u00a0 \u00a0 MlflowClient().log_artifact(run_id, local_path, artifact_path)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 210, in log_artifact\n\u00a0 \u00a0 artifact_repo.log_artifact(local_path, artifact_path)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\mlflow\\store\\hdfs_artifact_repo.py\", line 34, in log_artifact\n\u00a0 \u00a0 with hdfs_system(host=self.host, port=self.port) as hdfs:\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\contextlib.py\", line 59, in __enter__\n\u00a0 \u00a0 return next(self.gen)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\mlflow\\store\\hdfs_artifact_repo.py\", line 168, in hdfs_system\n\u00a0 \u00a0 extra_conf=extra_conf)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\pyarrow\\hdfs.py\", line 211, in connect\n\u00a0 \u00a0 extra_conf=extra_conf)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\pyarrow\\hdfs.py\", line 36, in __init__\n\u00a0 \u00a0 _maybe_set_hadoop_classpath()\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\pyarrow\\hdfs.py\", line 134, in _maybe_set_hadoop_classpath\n\u00a0 \u00a0 classpath = _hadoop_classpath_glob(hadoop_bin)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\pyarrow\\hdfs.py\", line 161, in _hadoop_classpath_glob\n\u00a0 \u00a0 return subprocess.check_output(hadoop_classpath_args)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\subprocess.py\", line 316, in check_output\n\u00a0 \u00a0 **kwargs).stdout\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\subprocess.py\", line 383, in run\n\u00a0 \u00a0 with Popen(*popenargs, **kwargs) as process:\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\subprocess.py\", line 676, in __init__\n\u00a0 \u00a0 restore_signals, start_new_session)\n\u00a0 File \"C:\\Users\\gs-2024\\AppData\\Local\\Programs\\Python\\Python35\\lib\\subprocess.py\", line 957, in _execute_child\n\u00a0 \u00a0 startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n\nCan anyone help me with this?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"HOW LOG IN PRODUCTION?",
        "Question_creation_time":1632499834000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/xvXSBl-2V6c",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey everyone!\n\n\nI'l planning put MLFlow server into production and I'm worried about logging.\nIs there an way about how to tail logs? Is there something such a verbose mode ?\nAs far as I could see on documentation there is no way to set this thing.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.23.1 released!",
        "Question_creation_time":1643243510000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/VBcXL-yx3WQ",
        "Question_upvote_count":null,
        "Question_view_count":25.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow\u00a01.23.1!\n\nMLflow 1.23.1 is a patch release containing the following bug fixes:\n\n[Models] Fix a directory creation failure when loading PySpark ML models (#5299,\u00a0@arjundc-db)\n[Model Registry] Revert to using case-insensitive validation logic for stage names in\u00a0models:\/\u00a0URIs (#5312,\u00a0@lichenran1234)\n[Projects] Fix a race condition during Project tar file creation (#5303,\u00a0@dbczumar)\n\nNote: Version 1.23.1 of the MLflow R package has not yet been released. It will be available on CRAN within the next week.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to display artifacts in AWS S3?",
        "Question_creation_time":1659662931000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/D3K4imRGS20",
        "Question_upvote_count":null,
        "Question_view_count":15.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi there,\n\n\nI'm using Mlflow 1.27.0 tracking server with docker, log db (MySQL), Artifacts storage (AWS S3). I'm trying to make model with log_params, metrics, artifacts and checked the functions worked correctly. Also, I checked my artifacts storage(AWS S3) whether artifacts are stored or not and there was no problem.\n\n\nBut, if i access tracking server and select specific log in experiment, there is no display about artifacts with tracking server's error.\n\u00a0\nSo, I want to know display artifacts in aws s3 is impossible or just environment error.\n\n\nHere are links refer to solve this problem.\n\n\nhttps:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html\n\nhttps:\/\/stackoverflow.com\/questions\/72280328\/mlflow-artifacts-on-s3-but-not-in-ui\n\n\n\nBest regards,\nKwon",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"security setup - username\/password experiment level",
        "Question_creation_time":1542243319000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/E9QW4HdS8a8",
        "Question_upvote_count":null,
        "Question_view_count":4154.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi\n\n\nCurrently the way mlflow works is that we can set an experiment ID for a MLFLOW tracker server URL and call it using our training model code. We would like to see if we can add security to this - say setup username\/password and also assign users with permissions for certain experiments\/projects. Is it possible to do that?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model Deployment Issues",
        "Question_creation_time":1567508364000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QPGFSApJ4Io",
        "Question_upvote_count":null,
        "Question_view_count":41.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI've been trying to deploy a scikit-learn model (a super simple classification using the Iris data set) that I built and trained using the standard MLproject protocol. I have been trying to deploy in two different manners and have been getting errors both ways:\n\n\nServe the model using a local REST server\n\nTo serve the model, I have been using the following command:\nmlflow models serve -m mlruns\/0\/f7cad9db15134c2abaa6d2a8b208c505\/artifacts\/sk_models -h **.***.**.** -p 1234 --no-conda\nNOTE: The host flag is the correct IP, I just masked it for this post\n\nWhen I run this command, I get the following output:\n2019\/09\/03 14:36:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/09\/03 14:36:16 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b **.***.**.**:1234 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\nbash: gunicorn: command not found\n\nI have ensured that gunicorn is in fact installed:\nsudo gunicorn --version\ngunicorn (version 19.9.0)\n\nIs this an issue that anyone else has run into?\n\n\nDeploy the model to Sagemaker\n\nTo deploy onto Sagemaker, I built the model using MLProject. The docker image that I have uploaded to ECR is the image I used to run the project and generate the model. When I attempt to deploy the model, I am using the following python script (the XXXX are personal info I removed for the post):\n\nimport mlflow.sagemaker as mfs\n\n\nrun_id = '0'\nexperiment_id = 'f7cad9db15134c2abaa6d2a8b208c505'\nregion = 'us-east-1'\naws_id = 'XXXXXXX'\narn = 'XXXXXXXXXX'\nimage_url = 'XXXXXXX\/mlflow-sklearn-test:latest'\napp_name = 'iris-dt-1'\nmodel_uri = 'mlruns\/%s\/%s\/artifacts\/sk_models' % (run_id, experiment_id)\n\n\nmfs.deploy(app_name=app_name, model_uri=model_uri, region_name=region, mode='create', execution_role_arn=arn, image_url=image_url)\n\n\nWhen I run the script, I get the following error:\n\u00a0\n2019\/09\/03 14:50:53 INFO mlflow.sagemaker: Creating new endpoint with name: iris-dt-1 ...\nTraceback (most recent call last):\n\u00a0 File \"sagemaker_deployment.py\", line 12, in <module>\n\u00a0 \u00a0 mfs.deploy(app_name=app_name, model_uri=model_uri, region_name=region, mode='create', execution_role_arn=arn, image_url=image_url)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/sagemaker\/__init__.py\", line 325, in deploy\n\u00a0 \u00a0 role=execution_role_arn, sage_client=sage_client)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/sagemaker\/__init__.py\", line 628, in _create_sagemaker_endpoint\n\u00a0 \u00a0 sage_client=sage_client)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/sagemaker\/__init__.py\", line 840, in _create_sagemaker_model\n\u00a0 \u00a0 model_response = sage_client.create_model(**create_model_args)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 357, in _api_call\n\u00a0 \u00a0 return self._make_api_call(operation_name, kwargs)\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\n\u00a0 \u00a0 raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: ECR image \"XXXXXXX\/mlflow-sklearn-test:latest\" is invalid.\n\n\nHas anyone experienced this error before? I have tried googling the answer and the only answer I could find was to add the\u00a0:latest\u00a0tag to the image URI, but I have already done this and I still get the error.\n\n\n\n\nThank you so much!!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Artifacts are not shown in UI",
        "Question_creation_time":1560965321000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-WkRc1NoH5g",
        "Question_upvote_count":null,
        "Question_view_count":432.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"I run mlflow server with Postgres backend and local directory as default artifact root. When I run my experiments\/runs I do see them in mlflow uI, however, artifacts are not shown. Artifacts are created in the local directory ( subdirectory from which I train model), but they are not showing in UI.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.9.1 released!",
        "Question_creation_time":1593055257000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/z--UqHehim0",
        "Question_upvote_count":null,
        "Question_view_count":10.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of MLflow 1.9.1! It's a patch release containing a number of bug-fixes & improvements:\n\n\nFixes AttributeError when pickling an instance of the Python MlflowClient class (#2955, @Polyphenolx)\nFixes bug that prevented updating model-version descriptions in the model registry UI (#2969, @AnastasiaKol)\u00a0\nFixes bug where credentials were not properly propagated to artifact CLI commands when logging artifacts from Java to the DatabricksArtifactRepository (#3001, @dbczumar)\nRemoves use of new Pandas API in new MLflow model-schema functionality, so that it can be used with older Pandas versions (#2988, @aarondav)\nFor a comprehensive list of changes, see the release change log, and check out the latest documentation on https:\/\/mlflow.org\/\n\nThanks,\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.20.1 released!",
        "Question_creation_time":1629950772000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/OxR3LPYxfgU",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow 1.20.1!\n\nNote: The MLflow R package for 1.20.1 is not yet available but will be in a week because CRAN's submission system will be offline until September 1st.\n\nMLflow 1.20.1 is a patch release for the MLflow Python and R packages containing the following bug fixes:\n\n-\u00a0Avoid calling\u00a0`importlib_metadata.packages_distributions`\u00a0upon\u00a0`mlflow.utils.requirements_utils`\u00a0import (#4741, @dbczumar)\n-\u00a0Avoid depending on\u00a0`importlib_metadata==4.7.0`\u00a0(#4740, @dbczumar)\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a specification document for server sizing?",
        "Question_creation_time":1530070766000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/f0hQ6k4w8dA",
        "Question_upvote_count":null,
        "Question_view_count":42.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI'll give MLflow a try within my team: that'll be a few data scientists trying out some use cases to evaluate MLflow in our environment.\n\n\n\nI couldn't find a specification document for sizing servers for MLflow installation. Is there some kind of guideline that could tell me amount of CPU, RAM, etc. for the server on which MLflow runs? It's of course not very important in this early stage, but nevertheless I think having such a guideline would be good.\n\n\nCheers,\nEmre",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Help us with the MLflow Features",
        "Question_creation_time":1548070719000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QQ7ycCYpeGE",
        "Question_upvote_count":null,
        "Question_view_count":28.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\n\n\nWe would like to continue our momentum of building new MLflow features at the same pace as we did in the past six months. We need your help. Give us your feedback by taking\n\nthis user\u00a0feedback survey.\n\n\n\n\nYou can also read Matei's blog: Kicking off 2019 with an MLflow Survey.\u00a0\n\n\n\n\nThank you for help.\n\n\n\n\nCheers,\n\nJules\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\ndatabricks.com",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Organizations using and contributing to MLflow - we are using it! :)",
        "Question_creation_time":1588070017000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/n8GI4uhm8No",
        "Question_upvote_count":null,
        "Question_view_count":40.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello MLflow organization,\n\nAt\u00a0Stratio\u00a0- a leading Big Data and Artificial Intelligence company - we've been using MLflow and integrating it to our product. We want MLFlow to be one of the key elements for the development of ML Algorithms\u00a0in our product and we've already been using it for quite some time.\u00a0\n\n\nWe would really like to be included in your website's list of contributors.\u00a0Attached you can find our logo. Please let us know if you need anything else.\n\nThanks in advance,\n\nStratio Team",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"connecting vertica to mlflow for recording runs",
        "Question_creation_time":1568345943000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Fg7JEwWlbTA",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\u00a0\n\nHas anyone tried connecting vertica with mlflow? It's an\u00a0SQLAlchemy compatible database which means it should work with mlflow.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow Release 0.5.2",
        "Question_creation_time":1535403393000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/k9oG1jeZxQM",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow Release 0.5.2 is ready, released 2018-08-27. The release is available on\u00a0PyPI\u00a0and docs are\u00a0updated. Here are the release notes (also available\u00a0on GitHub):\n\n\nBreaking changes: None\n\n\n\nChange log:\n\n\n0.5.2 (2018-08-24)\n------------------\n\n\nMLflow 0.5.2 is a patch release on top of 0.5.1 containing only bug fixes and no breaking changes or features.\n\n\nBug fixes:\n\n\n- Fix a bug with ECR client creation that caused ``mlflow.sagemaker.deploy()`` to fail when searching for a deployment Docker image (#366, @dbczumar)",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Regarding pushing managed MLflow models to sagemaker endpoints",
        "Question_creation_time":1641363690000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/oPRZpS35XRw",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi All,\n\n\nDoes anybody know anything about the below procedure.\n\n\nIf we have a registered model on managed AWS Databricks then how can we publish it as sagemaker endpoint.\u00a0\n\n\nThe mlflow sagemaker API doesn\u2019t work because it requires an image to be present in ECR. This seems like a manual step. Also the model registry path is not accessible on databricks. Only accessible via mlflow API.\u00a0\n\n\nIs there any way where we can directly push our managed mlflow models to sagemaker endpoint with \u00a0all the imgae creation and push steps are automatically handled by the API itself.\n\n\nThanks\nSaurabh\n\n\n--\n\nThanks & Regards\nSaurabh Verma",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"RFC: Batched logging APIs in MLflow",
        "Question_creation_time":1551116940000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/0M2BIceZirQ",
        "Question_upvote_count":null,
        "Question_view_count":13.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nThis GitHub issue contains a proposal for adding new REST, fluent, and client APIs for logging batches of metrics\/params\/tags in MLflow, which we expect will improve performance by reducing the number of API requests needed to log data.\n\n\n\nWe may deprecate the existing set of REST APIs in favor of the new batched API endpoint in MLflow 1.0. Any feedback is welcome - we'd like to start implementation work on this by early next week.\n\nThanks!\nSid",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does Community UI has Admin dashboard?",
        "Question_creation_time":1648187728000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1460",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"I have setup a community self-hosted polyaxon, and the config abourt ui is\n\nui:\n  enabled: true\n  offline: false\n  adminEnabled: true\n\n\naccording to the documentation, if I set adminEnabled to true, there should be an admin dashboard. But I did not find it, there is no difference to turn it on\/off",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":1458.0
    },
    {
        "Question_title":"Improve integrations with data sources",
        "Question_creation_time":1619251910000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1297",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"This discussion was started on slack and was moved to github for future reference.\n\nOriginal content:\n\nThese are actually good questions (both the \u201cis there an integration for DVC\u201d and \u201chow do you want to use DVC\u201d), it was also asked a couple of times in the past, and I also had such discussions in demos or feedback sessions and shared some info with some users about upcoming work.\n\nI can share some ideas and some features we are planning, I think some of them will materialize in the next few months.\n\nFirst, I think it depends on the kind of data, how it\u2019s stored, how it changes, and how frequently it changes, and who changes that data. But we can look at the integration from the point of view of Polyaxon as it provides the orchestration and scheduling mechanisms that should work with different external systems and libraries.\n\nUsing DVC, DBT, Feast, \u2026 are all good candidates for integration in addition to the direct access with boto, GCS, Azure client, mounted paths, git, \u2026 that we currently provide.\n\nWe provide an abstraction called connection , that\u2019s how we integrate Polyaxon with an external system, some systems support a versioning mechanism like git, docker registries, data\/volumes\/buckets (by calculating a hash and storing the path)\nFor the current systems that we integrate with, we provide 3 mechanisms, an optional way to fetch the data automatically (an initializer), an optional way to collect some outputs automatically, and a custom container (main) that runs the user\u2019s logic with some accompanying tracking methods to log the version summary, and some predefined logic (i.e. tracking the commit, calculating the hash\/path, storing the image hash, \u2026) that the users can use or provide their own summary (json blob) about such versions.\n\nAdding an integration as initializer for DVC, DBT, Feast, \u2026 is a matter of extending the connection schema to allow the user to define the default access method, when multiple options are possible, like for datasets a user can use the native client or DVC, for a database the user can use the native library or DBT ...\nLinking and exploring artifacts and lineage for each connection. In Polyaxon EE\/Cloud we have a connections catalog metadata layer, this layer will be promoted to the same level as projects\/component hub\/model registry in the next coming releases. Users can explore all artifacts related to a connection, e.g. all container images related to a registry, all commits used in Polyaxon related to a git connection, all metrics references in an artifacts store. They can also see the runs that requested those git commits or artifact versions under a specific connection as well as the profile of the runs that interacted with those connections (e.g. duration, resources GPU\/CPU, ...).\nAdding an integration to the logging system, this is actually coming rather soon we are still thinking how best to handle CE. The tracking code will allow to specify the the connection for log_artifact_ref , log_code_ref , log_data_ref , log_file_ref , log_dir_ref and the generic log_artifact_lineage to provide the user with the tools to create such rich metadata.\nIf anyone want to discuss such features or would like share more ideas, feel free to comment. (edited)",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Serverless Airflow Operators on Polyaxon",
        "Question_creation_time":1649327667000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1469",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Release the components library based on Airflow operators:\n\nRun in serverless way\nRun in containers\nLeverage the connection catalog based on the Kubernetes\/Vault secrets (no storing secrets in a database)",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to patch a multi-run downstream operation, for example tensorboard:multi-run",
        "Question_creation_time":1651747273000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1502",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nHi, simple Q. I want to launch a tensorboard with the tensorboard profiler pip installed from the GUI. At the moment I am using this:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"uuid: XX\"\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nHowever I want to runPatch it so that it installs:\n\npip install -U tensorboard-plugin-profile\n\nIs there a way to easily do this?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to retrieve all the artifacts lineage for a set of runs, based on both the name of the run, and the name of the artifact?",
        "Question_creation_time":1649410426000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1486",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI have several runs with the named RUN_NAME each run is logging an artifact named ARTIFACT_NAME. I would like to query all these artifacts from all this subset of runs.\n\nWhat I\u2019m using to try to achieve this is calling RunClient.client.runs_v1.get_runs_artifacts_lineage, but in this function I get all artifacts from all runs ever independently from the run name.\nAlso, when listing the artifacts from these runs, for some reason the path is always None, even though if I use the function you suggested last time, get_artifacts_lineage , for a specific run, I do get values on the path field.\nSo my two main issues are:\n\nHow do I get all artifacts from a set of runs with the same run_name?\nWhy the path is empty in case this is the correct function to use?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Batch deletion of archived runs",
        "Question_creation_time":1649327972000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1471",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI\u2019m new to Polyaxon and still learning how to use the UI. I would like to delete all archived jobs from All Runs but I can\u2019t see the option to filter them. We are running v1.17.0. Can you advise how to do it please? Thanks!",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Welcome to Discussions!",
        "Question_creation_time":1619177543000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1290",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey everyone,\n\nWe're opening up Discussions as a place for our community users to interact with each other and Polyaxon' contributors.\n\nWe've historically used Slack as the place to ask questions, and we would like to try a new experiment with discussion as the new default place to ask repeatable questions about how to use a specific feature.\n\nQ&A, Ideation, and feature requests\n\nThis is a great place to:\n\nAsk questions about Polyaxon\nRequest features from our team\nShare ideas and projects you are working on\nBug Reports\n\nIf you've found a bug, it is best to file an issue.\n\nJoin us on Slack\n\nYou are welcome to join our Slack community by signing up here\n\nWe will still be using Slack as a place to provide private help and make announcements about events and releases. We encourage all of our users to still join our community. If you have a question that contains sensitive information that needs to be asked in private, it is best to do it on Slack.\n\nCode of Conduct\n\nIn addition to the Code of Conduct guide line, we have a couple of rules we would like you to follow:\n\nBe nice, always!\nBe respectful; we assume positive intent from you and we ask the same in return\nAvoid posting sensitive information\nDon\u2019t abuse tagging other users\nDon\u2019t advertise material unrelated to Polyaxon\nExplicit is better than implicit: be as precise as you can.\nIt\u2019s OK to disagree but disagree politely and constructively.\nAvoid absolutes: absolute statements do not provide room for a conversation to grow\nNever attack: being defensive about your own ideas, or running offense on another person\u2019s ideas will always result in shutting down collaboration\nStay humble: disagreements are learning opportunities\nBefore creating a new topic, search if someone posted a similar issue before to avoid duplication.\nEach topic should be well-scoped (focused on one thing), non-duplicated and specific to problematic we are solving in one of our repos or with our solution.\nOrganizing FAQs and discussions\nTitle: try to specify the problem rather than the feature or solution you tried.\nIf you have multiple issues, please break them down to multiple discussions.\nUse labels to categorize the new discussion.\nProvide as much information as you can to help us identify the root cause of the issue:\nWhat environment are you using, e.g. Python version when for CLI\/Library issues?\nWhat infrastructure are you using, e.g. an on-premise deployment, a specific cloud provider (AWS, GCP, Azure).\nWhat version are you using?\nDid you check the docs and search bar?\nDid you check the discussions and github search?\nDid you try upgrading before asking?\nIf the issue is related to operations on k8s, can you please use the inpsection button and copy\/paste the YAML results as an attachement (you can replace sensitive info with xxx).\nCan you share logs or code snippets that can make it easier to reproduce your issue?\nAs a discussion creator, you will have a button that allows you to accept an answer. If some user responds to your question with a great answer, mark it as the accepted solution. Doing that helps others quickly navigate to the answer that helped you solve the problem.\nCategories\nAnnouncements: a category for general-purpose category for announcing new releases and blog posts.\nFAQ: a category for frequently asked questions not strictly falling into any other mentioned category.\nQ&A: a category to ask for help.\nGeneral: a category for discussing anything related to Polyaxon.\nShow and tell: a category for links to various resources that can help you get started and learn about Polyaxon as we as a place for sharing blog posts, walkthroughs, or simply sharing how to solve a specific problem.\nIdeas: a category to share ideas for new features.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to get model references logged by a specific run?",
        "Question_creation_time":1649410139000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1485",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"We are trying to save a model using log_model_ref and add a name to it, i.e. best_auc. Then we want to be able to retrieve this model from the latest run.\nHowever, if we use RunClient.client.runs_v1.get_runs_artifacts_lineage this returns all the artifacts ever generated for that project. And if we use RunClient.get_artifacts_tree, we do have more control about which run we are looking at, but we lose the name information we set when using log_model_ref?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Push image error: Get \"https:\/\/https\/v2\/\": dial tcp: lookup https on SOME-IP: no such host",
        "Question_creation_time":1619178414000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1292",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"This is question\/issue from Polyaxon Slack that was resolved. I am posting it for visibility if someone stumbles on the same issue with answer down below.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd Kaniko configuration:\nconnections:\n  - name: docker-registry\n    kind: registry\n    description: \"aws docker repository\"\n    schema:\n      url: https:\/\/ID.dkr.ecr.SOME-REGION.amazonaws.com\n secret:\n      name: aws-secret\n      mountPath: \/root\/.aws\/\n    configMap:\n      name: docker-config\n      mountPath: \/kaniko\/.docker\nAnd polyaxonfile for the build:\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: docker-registry\n    value: polyaxon\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"tensorflow\/tensorflow:2.0.1-py3\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\",\"polytune\"]'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\nThen raised error:\nConditions:\nTYPE    STATUS    REASON                MESSAGE                                      LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n------  --------  --------------------  -------------------------------------------  ------------------  ----------------------\nfailed  True      BackoffLimitExceeded  Job has reached the specified backoff limit  a few seconds ago   a few seconds ago\n2021-04-19 08:18:08.805683+02:00 | error checking push permissions -- make sure you entered the correct tag name, and that you are authenticated correctly,\n and try again: checking push permission for \"https:\/\/ID.dkr.ecr.REGION.amazonaws.com\/SOME-NAME:SOME-TAG\": creating push check transport for https: failed: Get \"https:\/\/https\/v2\/\": dial tcp: lookup https on SOME-IP: no such host",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Support for spark DF?",
        "Question_creation_time":1644430572000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1497",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":1496.0
    },
    {
        "Question_title":"Create video tutorials for the new users",
        "Question_creation_time":1649007252000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1459",
        "Question_upvote_count":3.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"We have been using Polyaxon for years as the ML Platform team. One feedback we got from ML engineers is the onboarding cost. One idea to help the onboarding is to create video tutorials for new users.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Programmatic upload and start an operation without CLI",
        "Question_creation_time":1649330161000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1476",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI am using the python run client and am trying to use 'pending' correctly. Here is an example of my code:\n\nclient = RunClient(owner=\"owner\", project=\"project\")\nclient.create(content=operation, pending='upload')\nclient.upload_artifacts_dir('.\/')\n\nI can see in the UI a new job is created, the artifacts have been uploaded correctly but in the info panel pending is still in the 'upload' state and the status remains as 'created'.How do I progress the job \/ remove the pending status after the upload has occurred?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Advanced filters for the comparison table",
        "Question_creation_time":1619177274000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1287",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"We are in the process of thinking about some new ways to simplify filtering runs in the comparison table.\nSome of these filters are:\n\nA filter based on parallel coordinate chart\nA filter based on some other graphs like line chart or a scatter plot\nA filter based on a calendar\nA filter based on a histogram of runs",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to using client to upload data\/artifacts to s3 bucket?",
        "Question_creation_time":1665738037000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1516",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"How to use the client to upload data\/artifacts to the s3 bucket?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Allow annotations in connections",
        "Question_creation_time":1593440974000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1304",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"Use case\n\nWe would like to use Vault for managing some connection's secrets.\n\nEDIT: Different use case where node scheduling can be also linked directly to a connection:\n\nWhen I need to iterate multiple times over the input data I would like to copy the data from the network storage to the SSD of the experiment machine as first step in my polyaxon file and then use that local copy to iterate on.\n\nIn this case we might want to always schedule a connection with a specific node or nodes(s).\n\nFeature description\nAllow annotations in connections.\nAllow node scheduling in connections.\nAlternatives\n\nI think these use-cases can be manged right now using RunProfile.\n\nN.B. the second issue was raised by a CE user, where the RunProfile is not accessible.\n\nConsiderations\n\nIf we provide annotations\/node-scheduling and other meta-data in connections we need to think about:\n\nThe overlap between the RunProfile and Connection concepts. RunProfile concept still handles more logic, but we should not duplicate efforts\nIf both concepts provide overlapping logic it can result in unexpected behaviour.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":865.0
    },
    {
        "Question_title":"Update jobs status from CLI or Client",
        "Question_creation_time":1649332613000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1478",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI was wondering if there is CLI option to \"Update the Job status\" similar to:\n\nI have found polyaxon ops update --help\n\nUsage: polyaxon ops update [OPTIONS]\n\n  Update run.\n\n  Uses \/docs\/core\/cli\/#caching\n\n  Examples:\n\n  $ polyaxon ops update --uid 8aac02e3a62a4f0aaa257c59da5eab80\n  --description=\"new description for my runs\"\n\n  $ polyaxon ops update --project=cats-vs-dogs -uid 8aac02e3a62a4f0aaa257c59da5eab80 --tags=\"foo, bar\" --name=\"unique-name\"\n\nOptions:\n  -p, --project TEXT  The project name, e.g. 'mnist' or 'acme\/mnist'.\n  -uid, --uid TEXT    The run uuid.\n  -n, --name TEXT     Name of the run (optional).\n  --description TEXT  Description of the run (optional).\n  --tags TEXT         Tags of the run (comma separated values).\n  --help              Show this message and exit.\n\nLong story short is that lot of my jobs finished working with status \"Running\". Don't know why, but in logs I see they finished.\nSo I decided to stop them manually, and I see \"Stopping\".\nSome jobs are being stopped but did not change status\nSo I want to clean that and \"update the status\" manually --> let's mark them as stopped.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How can I send Slack alert on failure ?",
        "Question_creation_time":1649334677000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1481",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Auto-resume for deep learning training is not working",
        "Question_creation_time":1649329501000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1474",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI'm trying to run a training job and make it resume automatically whenever it is preempted or it encounters an issue.\nI'm using for this the \"termination\" and \"maxRetries\" field to restart the job.\nAfter a problem happens, the job is restarted automatically starting from where the problem has happened if I look at the logs. However, nothing is being saved to the artifacts and any call to tracking.log_metric doesn't seem to have an effect. If I look at the logs, the job then continues until it reaches the end. However instead of just ending, it just keeps restarting (from the point where the problem occurred) until all the \"maxRetries\" are used and fails with the warning \"Underlying job has an issue\" at the status page.\nAny idea what could cause such a problem and if there is anything I could do to avoid it?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to configured S3 connection to upload\/download artifacts programmatically",
        "Question_creation_time":1649333190000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1480",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"We\u2019re implementing a few things and I\u2019ve got a quick question.\nI see the example here https:\/\/github.com\/polyaxon\/polyaxon\/blob\/faec6649ed6a09ad29365f17795a404cc714c22e\/site\/integrations\/data-on-s3.md but I don\u2019t quite understand how to setup that S3Service(...) object. what would I pass in? a connection? how do I create the s3 connection object to pass in? I\u2019ve got the connection defined in polyaxon\u2019s config\/polyaxonfile but I\u2019m not sure what to create in python there.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Able to push image to ECR with Kaniko but pull stuck on ContainersNotReady containers with unready status: [polyaxon-main polyaxon-sidecar]",
        "Question_creation_time":1619180115000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1294",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"This question was resolved and discussed on Polyaxon Slack. Posting for visibility if someone stumbles upon the same issue.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd built image at:\n\nID.dkr.ecr.REGION.amazonaws.com\/VALID-DIRECTORY\/VALID-REPO:polyaxon-1\n\nAnd polyaxonfile based on xgboost\/boston example :\n\nyou can see that run.environment was added to expose aws-secret and run.container.image was substituted with above\n\nversion: 1.1\nkind: component\ntags: [examples, xgboost]\ninputs:\n  - {name: max_depth, type: int, isOptional: true, value: 5}\n  - {name: eta, type: float, isOptional: true, value: 0.5}\n  - {name: gamma, type: float, isOptional: true, value: 0.1}\n  - {name: subsample, type: int, isOptional: true, value: 1}\n  - {name: lambda, type: int, isOptional: true, value: 1}\n  - {name: alpha, type: float, isOptional: true, value: 0.35}\n  - {name: objective, type: str, isOptional: true, value: 'reg:squarederror'}\n  - {name: cross_validate, type: bool, isOptional: true, value: false}\nrun:\n  kind: job\n  connections: [docker-registry]\n  environment:\n    imagePullSecrets:\n     - aws-secret\n  init:\n  - git: {\"url\": \"https:\/\/github.com\/polyaxon\/polyaxon-examples\"}\n  container:\n    image: ID.dkr.ecr.REGION.amazonaws.com\/VALID-DIRECTORY\/VALID-REPO:polyaxon-1\n    workingDir: \"{{ globals.artifacts_path }}\/polyaxon-examples\/in_cluster\/xgboost\/boston\"\n    command: [\"python\", \"-u\", \"model.py\"]\n    args: [\n      \"--max_depth={{ max_depth }}\",\n      \"--eta={{ eta }}\",\n      \"--gamma={{ gamma }}\",\n      \"--subsample={{ subsample }}\",\n      \"--lambda={{ lambda }}\",\n      \"--alpha={{ alpha }}\",\n      \"--objective={{ objective }}\",\n      \"--cross_validate={{ cross_validate }}\",\n    ]\nThen experiment would start but would be stuck on status indifinetely:\nConditions:\nTYPE      STATUS    REASON                MESSAGE                                                           LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n--------  --------  --------------------  ----------------------------------------------------------------  ------------------  ----------------------\ncreated   True      OperationServiceInit  Run is created                                                    a few seconds ago   a few seconds ago\ncompiled  True      SchedulerPrepare      Run is compiled                                                   a few seconds ago   a few seconds ago\nqueued    True      SchedulerStart        Run is queued                                                     a few seconds ago   a few seconds ago\nstarting  True      OperatorController    Operation is starting                                             a few seconds ago   a few seconds ago\nwarning   True      ContainersNotReady    containers with unready status: [polyaxon-main polyaxon-sidecar]  a few seconds ago   a few seconds ago\n\nAnd manually creating dummy pod with helm:\n\nwas also not able to be built due to error with pulling the image.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  namespace: POLYAXON_NAMESPACE\nspec:\n  containers:\n  - name: test\n    image: ECR_IMAGE_TO_TEST\n    command:\n    - '\/bin\/bash'\n    - '-c'\n    args:\n    - echo\n    - \"works\"",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Pickle support?",
        "Question_creation_time":1548256656000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1493",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Would it be reasonable to have a custom pickling magic method? Something like this:\n\nclass DataFrameSummary:\n    ...\n\n    def __getstate__(self):\n         return {'length': self.length, \n                 'column_stats': self.column_stats,\n                 'corr': self.corr}\n   \n    def __setstate__(self, state):\n        self.length = state['length']\n        self.column_stats = state['column_stats']\n        self.corr = state['corr']\n\nI guess that usually, you don't need to store df property because it is probably already stored somewhere. Or it could be an optional behavior depending on __init__ parameters.\n\nWhat do you think?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":1492.0
    },
    {
        "Question_title":"How can I start a Tensorboard for the top 5 experiments",
        "Question_creation_time":1649335986000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1483",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI would like to compare the top 4 experiments based on a specific metrics.\n\nCurrently I query the top experiment using the cli:\n\npolyaxon ops ls -q \"name: GROUP_NAME, metrics.loss:<0.002\"  -s \"metrics.loss\" -l 5\n\nAnd then I copy\/paste the run UUIDs to:\n\npolyaxon run --hub tensorboard:mulit-run -P uuids=UUID1,UUID2,UUID3,UUID4,UUID5",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to stop all runs attached to a specific queue",
        "Question_creation_time":1650719473000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1500",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nHi team, I am trying to reconfigure queues for one of our agents, and one queue (used by several projects) is full and has several runs queued. Is there a feature to drain a queue, like a button or an API to stop all runs for a specific queue in all projects? (edited)",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to show all active runs without selecting the status manually",
        "Question_creation_time":1649332873000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1479",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nThere used to be a switch to show\/hide all active runs (multiple states at once) under the flags dropdown, I can't find it anymore. Was it removed from the UI?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Re-introduce a per-operation automatic build process",
        "Question_creation_time":1619176829000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1286",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"This discussion was started on slack and was moved to github for future reference.\n\nOriginal content:\n\nAfter some thinking about the impact and the size of this feature request https:\/\/polyaxon.slack.com\/archives\/C6QBND2SV\/p1618316341162200\nI think we will probably deploy an experimental version next release (v1.9) or the one after (v1.10) to Polyaxon Cloud. Once we think the feature is stable and does not require any major migrations (both on the spec and db level) we will move it to CE & EE.\n\nThis feature will not replace the ad-hoc build operations, users can still create independent polyaxonfiles with a kaniko\/dockerize hub ref.\nThe no-build requirement that the platform provides at the moment will stay the same, so users who have stable pipelines that do not require frequent changes to their images can safely ignore this feature.\nA new section build will be introduced, where users can signal to the platform that a build is required prior to starting the main operation, the build section will provide the necessary fields to provide stuff like, queue, preset, resources, node selectors, ... specific to the build.\nCache (and invalidation) requires some improvements on the commercial offering, but they are planned anyway. I am not sure yet how it would work for CE, but we will work on it when we get to that point and will add docs around edge cases.\nBy providing a build section, Polyaxon will take care of generating the image based on the project and the uuid, project:build-uuid, and will set that image automatically on the container of the main operation. The registry to use will be still configured via a connection.\nA new status building will be added to show that a build is progressing before compiling the main manifest (without the build). And a build icon in the UI to redirect to the build run for viewing info, logs, \u2026 about the build operation.\nWhen the build and matrix sections are used together, a single build operation will be scheduled and will be used for all runs.\nTo make the process predictable and easy, when the build section is used with -u\/--upload flag of the run command, the uploaded artifacts will automatically be injected in the build operation and not the main operation, which means users have to copy any necessary artifacts to their generated image.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Are Tensorboad services preventing CPU nodes to scale down?",
        "Question_creation_time":1649335208000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1482",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nWhen I schedule a tensorboard from the UI does it always schedule on the polyaxon \"core nodes\", or does it schedule on any cpu machine that is available?\n\nWe have an issue sometimes with cpu nodes not scaling down, and I am wondering if it is because tensorboards are running on them?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Private github repo fail for using log_model and log_metric",
        "Question_creation_time":1619492179000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1302",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Describe the bug\n\nI have two version for using component. One is directly uploading local code to each pod, this work fine that we could see the models artifact and metric curve. The other one use the same code as the first one, except for using private github to load the code. In this case, the \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally.\n\nTo reproduce\nAdd private code connection\nconnections:\n  - name: my-repo\n    kind: git\n    schema:\n      url: https:\/\/github.com\/xxx\/my-repo\n    secret:\n      name: \"github-secret-my-repo\"\n\nruning job config:\nrun:\n  kind: job\n  init:\n    - connection: my-repo\n\nHow we use log_metric and log_model\n# log metric\ntracking.log_metric(\"val_loss\", val_loss, step=epoch)\ntracking.log_metric(\"val_precision\", precision, step=epoch)\ntracking.log_metric(\"val_recall\", recall, step=epoch)\n\n# log model\nmodel_output_dir = tracking.get_outputs_path(\"models\", is_dir=True)\nckpt_file = os.path.join(model_output_dir, 'checkpoint.pth.tar')\ntorch.save({xxx}, ckpt_file)\ntracking.log_model(name=\"checkpoint\", path=ckpt_file, framework=\"pytorch\")\n\nExpected behavior\n\nShowing metric curve and saving models normally.\n\nEnvironment\n\nminikube: v1.15.1\npolyaxon ce: 1.7.5",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":1301.0
    },
    {
        "Question_title":"Artifacts lineage is tracked but the Dashboard and Artifacts tabs are empty",
        "Question_creation_time":1650441558000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1498",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI'm currently facing a problem regarding the polyaxon tracking module. When running a job, I can see the logged metrics, parameters, artifacts in the lineage tab, although on the Dashboard tab, I can see only the chart's name and no values being tracked.\n\nHave anyone encountered this issue before? Any help will be much appreciated.\n\nI have tried a basic example of MNIST, when training it on CPU it tracks the metrics, but on GPU it doesn't.\n\nAlso, when I'm running a training job (MNIST) on the CPU it tracks the metrics and I can see the charts in the Dashboard, but when I configure the polyaxonfile to use GPU, the logs are indeed saying that the GPU is being used, but the metrics charts are empty.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Best practices of project management for multiple users on Polyaxon v1",
        "Question_creation_time":1619642544000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1306",
        "Question_upvote_count":6.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello all, I\u2019m curious about the best practices of project management for multiple users on Polyaxon v1. Polyaxon v0 allows multiple users to have the same name project, but a project seems to be global under an organization on Polyaxon v1. The ideas I came up with are the following. Do we have other approaches?\n\nAdd username as a prefix or suffix to a project name\nUse username as a tag in a project",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Can I add Component Tags via Input Params?",
        "Question_creation_time":1667585195000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1518",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"I have no idea if this is possible, but I found myself wanting to do the following.\n\nIn words, I want to add component tags based on matrix parameters. The following yaml doesn't work, but I think it illustrates what I want.\n\nversion: 1.1\nkind: operation\nmatrix:\n  kind: grid\n  params:\n    sota:\n      kind: choice\n      value:\n        - model: ModelA\n          max_epochs: 200\n          dataset_hash: ae43ff\n        - model: ModelB\n          max_epochs: 150\n          dataset_hash: 33fba2\ncomponent:\n  name: model-trainer\n  tags:\n    - dataset_update_retrain\n    - \"{{ sota.model }}\"  #<- I want to set this dynamically based on the job input\n  inputs:\n    - name: sota\n      type: dict\n      isOptional: false\n  run:\n    kind: job\n    container:\n      args: >\n        set -x;\n       train \\\n          hydra.run.dir={{ globals.run_outputs_path }} \\\n          model={{ sota.model }} \\\n            model.datamodule.dataset_hash={{ sota.dataset_hash }}\/ \\\n            model.datamodule.data_directory=\/data\/{{ sota.model }}\/ \\\n          trainer=gpu \\\n            trainer.max_epochs={{ sota.max_epochs }} \\\n      ...\n\nAs the above pattern doesn't work, I am either doing it wrong or it isn't possible in the config. Could I potentially set ENV vars that could get picked up by the tracking client?\n\nThanks a ton for the help!",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Is there a way to have the logs from a polyaxon run viewable via the cloud UI?",
        "Question_creation_time":1649327817000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1470",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nIs there some way we could save the output logs to have them be accessible?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"I would like to configure Polyaxon in a way to avoid asking data-scientists to configure pre-emptible node-pools or request TPUs on their own",
        "Question_creation_time":1649336966000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1484",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI am currently defining some machines configuration using machine-env1.yaml, machine-env2.yaml which basically contains node selectors and CPU, GPU, and TPU requests configuration, and then running:\n\npolyaxon run -f polyaxonfile.yaml -f machine-env1.yaml\n\nI have two problems with this approach:\n\nI need to copy the env files to all our git repos, which means if I make a change I need to perform several pull requests\nI need to tell the data-scientits to pull the last commit, sometimes that's not possible because they can not merge\/rebase the changes.\n\nBased on those two issues, in the end we tell data-scientists to just use:\n\nenvironment:\n  nodeSelector:\n    nodes: large-pool\n...\nrun:\n  ...\n  container:\n      resources:\n        limits:\n          cpu: 3000m\n          memory: 6000Mi\n        requests:\n          cpu: 2000m\n          memory: 4000Mi\n\nWhich is error prone and confusing for them, and make the files bigger and difficult to change.\n\nAny elegant way to abstract this type of configuration from the data-scientists?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"(How) Can I pass path to Dockerfile using Kaniko?",
        "Question_creation_time":1620659662000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1315",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Context\n\nLet's say I have following structure\n\nroot\n\u251c\u2500\u2500model_files\n\u251c\u2500\u2500polyaxonfiles\n\u2502      \u251c\u2500\u2500build.yml\n\u2502      \u2514\u2500\u2500run.yml\n\u251c\u2500\u2500utils\n\u2514\u2500\u2500dockerfiles\n         \u251c\u2500\u2500Dockerfile.0\n         \u2514\u2500\u2500Dockerfile.1\n\n\nmy build looks as simple as:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  context:\n    value: \"{{ globals.run_artifacts_path }}\/uploads\"\n  destination:\n    connection: docker-registry\n    value: machine-learning\/polyaxon-tutorial:1\n\nhubRef: kaniko\n\nRunning this with command\n\npolyaxon run -f polyaxonfiles\/build.yml -u\n\nGives me very much expected error:\n\nError: error resolving dockerfile path: please provide a valid path to a Dockerfile within the build context with --dockerfile\n\nSo the Kaniko expects Dockerfile to exist in root of the context by default. Does the polyaxonfile specification exposes a way to overwrite the default as suggested with error message?",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to debug my init git container",
        "Question_creation_time":1649328334000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1472",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nMy job is stack with a warning status, I configured a private bitbucket connection and the cloning fails.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to persist custom job runs table",
        "Question_creation_time":1650636682000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1499",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nHi everyone. Is there a way to save display preferences in the job runs search UI? My main interest is in saving (1) which columns are displayed, and (2) the order in which they are displayed. Thank you.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Polyaxon CE fresh deployment never finishes",
        "Question_creation_time":1649328918000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1473",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nHi, I\u2019m trying to deploy Polyaxon CE with helm\/argocd and the polyaxon-api keeps logging in loop:\n\nSystem check identified some issues:\n\nPreparing...\n\n\nand the pod never becomes ready.\nAny idea what could cause that?\n\nMore\n\nIt seems the polyaxon helm chart installation always fails with the message:\n\nError: timed out waiting for the condition\nThis is what I see:\n\npolyaxon-polyaxon-api-d8d7c8b5f-spsb2           1\/1     Running   0          3d5h\npolyaxon-polyaxon-api-dbb84b79c-6jqm9           0\/1     Running   2          13m\n\n\nThe polyaxon api pods take a long time to become ready. It fails liveness probe:\n\nEvents:\n  Type     Reason     Age                   From               Message\n  ----     ------     ----                  ----               -------\n  Normal   Scheduled  15m                   default-scheduler  Successfully assigned polyaxon\/polyaxon-polyaxon-api-dbb84b79c-6jqm9 to ip-192-168-165-30.us-east-2.compute.internal\n  Normal   Pulling    14m                   kubelet            Pulling image \"polyaxon\/polyaxon-api:xx\"\n  Normal   Pulled     14m                   kubelet            Successfully pulled image \"polyaxon\/polyaxon-api:xx\" in 33.387487209s\n  Normal   Created    14m                   kubelet            Created container polyaxon-api\n  Normal   Started    14m                   kubelet            Started container polyaxon-api\n  Normal   Killing    8m49s                 kubelet            Container polyaxon-api failed liveness probe, will be restarted\n  Warning  Unhealthy  8m37s (x10 over 13m)  kubelet            Readiness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused\n  Normal   Pulled     8m19s                 kubelet            Container image \"polyaxon\/polyaxon-api:1.9.5\" already present on machine\n  Warning  Unhealthy  4m49s (x16 over 13m)  kubelet            Liveness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Make references dynamic in DAGs",
        "Question_creation_time":1649330065000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1475",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":0,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI would like to use a parameter with a ref to a run whose uuid is read from a parameter. For example, inside a DAG I can do the following:\n\nparams:\n  my_arg:\n    value: artifacts.my_artifact\n    ref: ops.my_op_in_the_same_dag\n\nI would also like to do\n\nparams:\n  my_arg:\n    value: artifacts.my_artifact\n    ref: runs.{{ params.upstream_run_uuid_not_in_this_dag }}\n\nThis is not possible at the moment because params object isn't ready in this context. But I guess with some dependency resolution, it could be made to work? I have a workaround for this myself (I'm generating polyaxonfiles on the fly based on some input arguments, so I can inject the UUID at compile time to the polyaxonfile), but IMO such a feature would be useful.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to use boolean params as string values without converting to a boolean values",
        "Question_creation_time":1649330562000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1477",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nI have a problem with plx CLI, a param of type str with value False that gets converted automatically, but I want to have a string parameter:\n\n- {name: add_noise,       type: str,   isOptional: true, value: \"False\"}\n\nPolyaxon compiles the YAML to\n\ninputs:\n  - ...\n  - name: add_noise\n    type: str\n    value: false  <-----\n    isOptional: true\n\nrun:\n  ...\n  container:\n    args:\n      - ...\n      - '--add_noise=false' <----",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"How to filter all jobs that accessed a connection, a dataset, or an artifact",
        "Question_creation_time":1649676773000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1487",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nHi, I have a quick question, is it possible to filter all jobs that requested\/accessed a specific connection?\n\nTo explain my use-case, we detected an issue with some data, and we would like to assess how many jobs and how far in the past that data was used in our training jobs.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"About the General Discussion category",
        "Question_creation_time":1633570856312,
        "Question_link":"https:\/\/community.sigopt.com\/t\/about-the-general-discussion-category\/22",
        "Question_upvote_count":0.0,
        "Question_view_count":189.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>This category is here for you to post any general questions or topics you\u2019d like to discuss with the community. Please observe community rules, and enjoy!<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SigOpt best parameters",
        "Question_creation_time":1645558581666,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-best-parameters\/95",
        "Question_upvote_count":0.0,
        "Question_view_count":108.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>How do I know that the parameters picked by SigOpt are the best ones?<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Suppress output during SigOpt runs optimization",
        "Question_creation_time":1644202800217,
        "Question_link":"https:\/\/community.sigopt.com\/t\/suppress-output-during-sigopt-runs-optimization\/84",
        "Question_upvote_count":0.0,
        "Question_view_count":118.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Right now, when I run with<\/p>\n<pre><code class=\"lang-python\">while not experiment.is_finished():\n    with experiment.create_run() as run:\n<\/code><\/pre>\n<p>I get <code>Run started<\/code> and <code>Run finished<\/code> calls.  How can I suppress these if I don\u2019t want to see this output?<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I use SigOpt together with any cloud provider?",
        "Question_creation_time":1643278813027,
        "Question_link":"https:\/\/community.sigopt.com\/t\/can-i-use-sigopt-together-with-any-cloud-provider\/77",
        "Question_upvote_count":0.0,
        "Question_view_count":121.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>The short answer it Yes!<\/p>\n<p>The SigOpt backend is communicating with your environment using a lightweight API. So as long as you have access to the internet you can use SigOpt for your modeling project.<\/p>\n<p>Similarly you can use SigOpt together with any infrastructure available (your laptop, company wide cluster etc.) as long as you can connect to the internet.<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Debug and Troubleshoot",
        "Question_creation_time":1638997918942,
        "Question_link":"https:\/\/community.sigopt.com\/t\/debug-and-troubleshoot\/49",
        "Question_upvote_count":0.0,
        "Question_view_count":104.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Think you\u2019ve found a bug? Let us know here!<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SigOpt run improvement",
        "Question_creation_time":1645558490644,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-run-improvement\/94",
        "Question_upvote_count":1.0,
        "Question_view_count":117.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In the \u201ctrials vs best value\u201d visualization I see a lot of points with the same value. Does this mean that SigOpt is not finding any improvement for those runs?<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sigopt for RL DQN",
        "Question_creation_time":1664314630467,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-for-rl-dqn\/120",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I found an old example on DQN on the github page. But it was using TF v1 and the old sigopt. It would have been nice to get a new tutorial replacing it.<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Python Client Discussion",
        "Question_creation_time":1638998126760,
        "Question_link":"https:\/\/community.sigopt.com\/t\/python-client-discussion\/50",
        "Question_upvote_count":0.0,
        "Question_view_count":123.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Got a question or want to discuss how to best use our Python client? Post here!<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SigOpt for Neural Architecture Search",
        "Question_creation_time":1642760439531,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-for-neural-architecture-search\/72",
        "Question_upvote_count":1.0,
        "Question_view_count":110.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Neural Architecture Search is certainly a challenge that faces the modeling and optimization community today. What are some examples of efficient or effective NAS experiments that you\u2019ve come across lately?<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What Compute Infrastructure do I have to use?",
        "Question_creation_time":1644274389975,
        "Question_link":"https:\/\/community.sigopt.com\/t\/what-compute-infrastructure-do-i-have-to-use\/86",
        "Question_upvote_count":2.0,
        "Question_view_count":126.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Where can you choose the device you\u2019d like to execute a SigOpt run or optimization on? Is there a default device?<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Has SigOpt been used for NLP tasks?",
        "Question_creation_time":1642760029892,
        "Question_link":"https:\/\/community.sigopt.com\/t\/has-sigopt-been-used-for-nlp-tasks\/71",
        "Question_upvote_count":0.0,
        "Question_view_count":114.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Yes, SigOpt can and has been applied to a variety of applications, NLP included. Check out this <a href=\"https:\/\/sigopt.com\/blog\/efficient-bert-overview\/\" rel=\"noopener nofollow ugc\">blog post<\/a> to see how we tackled the task of question answering by tuning the architecture of BERT.<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"Early Stop\" feature and API call for terminating running experiment",
        "Question_creation_time":1659438278081,
        "Question_link":"https:\/\/community.sigopt.com\/t\/early-stop-feature-and-api-call-for-terminating-running-experiment\/118",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I was wondering if there is an \u201cearly stop\u201d feature for the SigOpt optimization: something that would stop the Experiment once a user provided value for a given metric is satisfied (ie. if our metric is \u2018time\u2019, then the experiment would stop before it runs out of budget, as soon as any of it\u2019s run achieves a \u2018time\u2019 metric below a defined value.)<br>\nI looked at the API Reference documentation and didn\u2019t find any similar feature so I started working on a separate python script that sends a query to the API and pulls the best runs from the given experiment (via the use of sigopt.get_experiment(experiment_id).get_best_runs() method).<br>\nFrom there it is easy to write custom logic based on the pulled data, however I cannot find an elegant way to close the Experiment (if I find the best run data to be good enough). The only way I was able to pull it off is via the use of sigopt.archive_experiment([experiment ID]). This will cause an error in the currently running optimization job, as the sigopt won\u2019t be able to log any run data to an archived experiment (that\u2019s ok - this is the behaviour I\u2019m looking for), however it also means that the Experiment is moved to the archived section (not great, would prefer to keep it in the main view). If there was an API call to just close the given experiment (without moving it to archive) it would be great!<br>\nIs there any missing API call that I missed that would enable such functionality?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sign up for SigOpt for free!",
        "Question_creation_time":1641897567877,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sign-up-for-sigopt-for-free\/68",
        "Question_upvote_count":0.0,
        "Question_view_count":143.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Interested in trying the SigOpt platform? Sign up for your free account <a href=\"https:\/\/app.sigopt.com\/signup\">here<\/a>, and get optimizing today!<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"XGBoost, cross-validation",
        "Question_creation_time":1644433403173,
        "Question_link":"https:\/\/community.sigopt.com\/t\/xgboost-cross-validation\/92",
        "Question_upvote_count":4.0,
        "Question_view_count":172.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am checking out the <code>sigopt.xgboost<\/code> API.  How do I perform cross-validation with it?<\/p>\n<p>I think I see how to do it using <code>sigopt.create_experiment<\/code>, because that expects the training to happen external of the API call, and I can use a cross-validation technique that I am familiar with (e.g. <code>sklearn.model_selection.cross_val_score<\/code>).  However <code>sigopt.create_experiment<\/code> also requires me to do more work setting up parameter search ranges compare to <code>sigopt.xgboost<\/code>.<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SigOpt Optimization",
        "Question_creation_time":1638998831270,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-optimization\/51",
        "Question_upvote_count":0.0,
        "Question_view_count":106.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>General discussion board for anything optimization!<\/p>",
        "Tool":"SigOpt",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does it work underhood: Predictions of multiple instances (Batch) to Vertex AI online serving",
        "Question_creation_time":1661981100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-does-it-work-underhood-Predictions-of-multiple-instances\/td-p\/462022\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":59.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,Vertex AI online serving:When multiple instances are passed for prediction to an endpoint, Does prepackaged container serve the inferences in the same manner as TFX Serving does with enable_batching.  If so how do we optimize batching parameters with multiple instances sent to Vertex AI online.If multi_instances prediction is different from TFX serving batching, how do we gain GPU resources efficient usage optimization with prepackaged serving container.On a general note, how to handle efficient GPU usage for both prepackaged container and custom container using a custom trained model.Please guide.Thank you.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI Datasets & batch predictions",
        "Question_creation_time":1658670780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Datasets-amp-batch-predictions\/td-p\/446346\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":134.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all,I just started to play with Vartex AI. I am working with \"Tabular\" - \"Forecasting\" and currently struggling with few things and i hope you can help me in order i can continue. I tried to organize my questions to three categories:1) Datasets for training:      a) \"series identifier\" define to which time series data are belonging ... lets assume that i have two series identifier - one is called \"A\" and one is called \"B\". Does this means that AI treats them as completely separate and noncorrelated - this means any data that belongs to series A don't have any correlation to B, right? This give me possibility to train different dataset with one shot right? Otherwise i would need to make (in my case) two trainings - one for A and one for B.2) Training new model --> Model detailsa) Is possible to predict more then one target column? b) lets assume that my dataset data granularity is 1 day. Can i use data granularity of \"5min\" for Forecast configuration or can this setup decrease quality of my forecast? Should it be more correct to use already at beginning lets say dataset granularity of 5 minute and afterwards it could be more flexible when setting data granularity for forecast configuration without influencing forecast quality?c) If I set Forecast horizon of 7 and context window 30, does this means that this setting limit my forecast to maximum 7 time steps and requesting always exactly 30 time steps of historical data as input when forecasting on existing trained model?3) Batch predictionsa) Batch Source fileLets assume that i have data with 15 columns from which one is \"serial identifier\" , one is \"time step\" - actually date and one of those columns is target column. Rest of 12 columns are used as influencer and used to train my model. I know that i need to have same structure for batch source file - i read that i can use same file as i used for training, but i just need to add in my case a 2x7 new rows with adding 7 dates and serial identification (in my case 7x A and 7xB) and target column need to be empty. But what should i do with data of rest 12 columns? Do i need manually to enter data for those new rows (2x7) of those 12 columns which values should be for future? But what if i don't have those data? Does this means i cannot do prediction?I tried to make prediction without those future data and i got following message:\"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\"I hope you can help me with above questions. Tnx in advance! Regards, Arny ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I explicitly authenticate to the ai-platform using the java PredictionServiceClient",
        "Question_creation_time":1669041720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-explicitly-authenticate-to-the-ai-platform-using-the\/td-p\/491537\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a model hosted on a Google Cloud endpoint and I would like to access it via the Java client.  I've created a service account and a key for that service account with the , when I run my client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, I am able to call the service.  When I try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.  The code is as follows`````` ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How should the input JSONL look for a batch prediction job?",
        "Question_creation_time":1649068920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-should-the-input-JSONL-look-for-a-batch-prediction-job\/td-p\/410193\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":109.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I can't find any examples online of how an input jsonl is supposed to look for a batch training job. When I tried with this:  I got an error email saying  Error Messages: BatchPrediction could not start because no valid instances \nwere found in the input file. Is there some other way this should look for it to work? Maybe like      ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to get textStyle in JSON response with Document ai",
        "Question_creation_time":1659591600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-get-textStyle-in-JSON-response-with-Document-ai\/td-p\/450434\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I was trying to get text style or Font style with document ai but was getting null list..This is the file I wanted text style to be extractedThis was the response I received.can someone help me with this?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Too many pages",
        "Question_creation_time":1665984720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Too-many-pages\/td-p\/478806\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I sent a 13 page pdf thru a document ai parser.  GCP, instead of populating the errors collection of the result with an error indicating too many pages, instead throws a runtime error causing a crash.Is try...except... really the best solution for this as I've not seen use of try...except in any of Google parser examples.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Cloud Platform - Vertex AI - Workbench JupyterLab - Spark\/Hadoop - JAVA_HOME is not set error",
        "Question_creation_time":1649862720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Platform-Vertex-AI-Workbench-JupyterLab-Spark\/td-p\/413482\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":382.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi All,I am trying to connect to a SparkSession on Vertex AI's Workbench JupyterLab, but receive this error. Locally, my JAVA_HOME system environments and path environments are already set, and can work when I run Jupyter locally. But only on Vertex AI's Workbench JupyterLab I get this error. Code: \n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.appName('Jupyter BigQuery Storage')\\\n.config('spark.jars', 'gs:\/\/spark-lib\/bigquery\/spark-bigquery-latest_2.12.jar') \\\n.getOrCreate()Full Error:Do let me know if you have advice or help, thank you!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can i show alias instead of voice name?",
        "Question_creation_time":1655940660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-i-show-alias-instead-of-voice-name\/td-p\/434016\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":114.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"[ ko-KR-Wavenet-A ] This name is so awkward for me.Can i show alias instead of that voice name?like this. [ ko-KR-Wavenet-A ] -> [ Jinsung ]------------------------------------------------------I'm developing a web service that can edit videos on the web.I will provide Google TTS on that web service.I show the Google (source of the voice) on the side, just wanna alias.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"The new languages are missing",
        "Question_creation_time":1652751780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-new-languages-are-missing\/td-p\/423648\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":89.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Google cloud translation have added new languages. About 24 new languages has been added to Google Translate. Very good job, well done. But they are not listed on this link.\nhttps:\/\/cloud.google.com\/translate\/docs\/languages\n\nI tried to access it using basic v2 API code, but no response came to my translation request. When will this new languages be available to be accessed by v2 APIs? ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Translating streaming audio into text",
        "Question_creation_time":1649687580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translating-streaming-audio-into-text\/td-p\/412679\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I'm using @Google-cloud\/media-translation in node with express js server. I want to translate media file (\".wav\" format) with media-translation. At first, i got an error because of authentication and I fixed it with env variable as specified in documentation, I followed each and every step exactly told in the documentation but I'm getting no response from server. When i looked into APIs & Services tab it only recorded my failed auth attempts no other API calls are recorded. Please help because there is no help available online about this product and it doesn't even send error responses so i can debug. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Meetup on machine translation for low-resource languages this Friday!",
        "Question_creation_time":1666181580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Meetup-on-machine-translation-for-low-resource-languages-this\/td-p\/479955\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":19.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.The next machine translation meetup is all about low-resource machine translation and it'll be online.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to organize intents\/pages for a non-service\/support application",
        "Question_creation_time":1669003380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-organize-intents-pages-for-a-non-service-support\/td-p\/491306\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am a new Dialogflow user and I need to create an agent for an application that is not service or support oriented.  The application is educational and has a large collection of questions and answers (1,000s) with no concrete conclusion (for example,  to renew a driver's license).  For the POC I did in Watson I was able to use folders to organize sub-topics.  What is the best way to group intents and responses by topic and sub-topic (for example, President Lincoln's early life President Lincoln's career)?  I expect it would be difficult to manage a list of 1,000s of pages in the left pane.  Thank you.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"is there any list of brands\/logos supported by google vision api?",
        "Question_creation_time":1636578060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/is-there-any-list-of-brands-logos-supported-by-google-vision-api\/td-p\/175425\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":233.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"is there anyplace i can see the list of brands\/logos that are currently supported by the google vision api's logo recognition service?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Billing & Cloud Vision API issue with \"Recognize Text\" on Android system",
        "Question_creation_time":1663802520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Billing-amp-Cloud-Vision-API-issue-with-quot-Recognize-Text-quot\/td-p\/469422\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":29.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,Here I got a billing problem with Cloud Vision API.First, I follow this link to setup my Firebase project to enable the feature of \"Recognize Text\".https:\/\/firebase.google.com\/docs\/ml\/android\/recognize-textThen all the functions used are normal. I call the function of \"annotateImage\" in Cloud Functions to invoke the Cloud Vision API, then can also used successful.Absolutely, I have trace the flow and requests on Cloud Vision API, it is just looks reasonable.But the issue I encountered is, \"it still charges when I'm not using it\", also when it has no any flow and requests! Billing, September 1-22, 2022 (the project has billing alerts set up now) :Cloud Vision API, 30 days to 9\/22\/2022 : It would be so gratefull if there any good suggestions !",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DocAI - Response in a single json file",
        "Question_creation_time":1668727800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Response-in-a-single-json-file\/td-p\/490702\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":59.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello Experts,\nI'm doing BatchProcessDocument. I have 18 pages of a PDF file and tried to process this using DocumentProcessorServiceClient API. After the process, Im getting response in json file. This is perfect.\nBut the json output file is created only for the 5 pages of the source PDF file. Each 5 pages of the content are converted into a separate json file.My question here is, is it possible to have a single output json file for a PDF source file? ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"hello custom training tutorial failed on cloud function deploy",
        "Question_creation_time":1646927880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/hello-custom-training-tutorial-failed-on-cloud-function-deploy\/td-p\/402689\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":385.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi guys I'm following this tutorial to get my had around Vertex AI - https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/image-recognition-custom\/ On step  - https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/image-recognition-custom\/serving#2_deploy_a   since I'm new on GCP anyone tried this tutorial and have the same error? any tips on how to fix this?thank you very much guys",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Manage Labeling Assignments on DataCompute",
        "Question_creation_time":1636661640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Manage-Labeling-Assignments-on-DataCompute\/td-p\/175499\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":416.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Our team has started to use the DataCompute console to assign labelers to labeling tasks created in Vertex AI. Currently, the Assignments tab requires the Labeling Manager to Populate the Specialists Column and Populate the Tasks Column I wanted to highlight some issues we are facing and ask if there's any plan to implement fixes.Issues: 1. The dropdown for task selection does not order the tasks alphabetically so it is difficult to find a specific task.2. There's no \"Select All\" option, instead, the manager must select each task individually.3. There is no drop down for the specialist emails even though they are available under the Specialists tab.Generally, it would be nice to see the entire assignment table by default rather than nothing on this page.Let me know if some of these issues can be addressed! ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"BigQueryML Explainability Apparently Not Working",
        "Question_creation_time":1640165100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQueryML-Explainability-Apparently-Not-Working\/td-p\/181036\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":319.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm using BigQueryML to train an XGBoost model on some of my data. When I create the model, I set the ENABLE_GLOBAL_EXPLAIN flag to TRUE, the model then trains properly and I can evaluate it. However there is no Interpretability tab on the model's page, and when I try to query the model with the ML.GLOBAL_EXPLAIN command, I get an error that says: Is this a bug or am I doing something wrong?Here's my create model code: ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"BatchPredict could not start due to empty input CSV file",
        "Question_creation_time":1640922780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BatchPredict-could-not-start-due-to-empty-input-CSV-file\/td-p\/181685\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\nI have a problem with batch prediction for text classification.\nAccording to the documentation I have created a csv file in which every single line refers to a PDF in my bucket. However I get the error message \"InvalidArgument: 400 BatchPredict could not start due to empty input CSV file\".I would be infinitely grateful for help in this case....",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Translate javascript API",
        "Question_creation_time":1659749160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-javascript-API\/td-p\/451250\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":94.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What's the order for the labels in txt file after I have exported my tflite model from Vertex AI",
        "Question_creation_time":1666354380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-order-for-the-labels-in-txt-file-after-I-have\/td-p\/480804\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":116.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have exported my trained tflite model. But I noticed the order of the labels in the txt file matters. I'm using image classification models. The ones with only two labels, it's an easy fix. I just switch the two. But when I have more than two labels, I notice the predictions are way off. Does it say in Vertex AI or is there a general rule to what label should go first, second, third..etc in the txt file that we create on our own? ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Question_creation_time":1661561100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/td-p\/460565\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":144.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.  I have enabled aiplatform.googleapis.com.Command:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifierUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.Please suggest what am I missing.   ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Make deep learning VM JupyterLab publicly available?",
        "Question_creation_time":1643281320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/td-p\/386576\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":96.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Create an instance of TextToSpeechClient() and ApplicationDefaultCredentials ...",
        "Question_creation_time":1651421220000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-an-instance-of-TextToSpeechClient-and\/td-p\/418964\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":146.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Folks,My first post here. This was posted on stackoverflow without much feedback - it is a little specific to the TextToSpeechClient and using ApplicationDefaultCredentials.  The link to the stackoverflow article is below just for reference.https:\/\/stackoverflow.com\/questions\/72074724\/trying-to-create-an-instance-of-googles-class-texttospe...I'm attempting to Create an instance of TextToSpeechClient() and an getting an exception - Could not construct ApplicationDefaultCredentials. I was able to get the php sample code provided on your github site running from the command line. I'm now executing in a browser session on an apache server. I have added the putenv() function to set the GOOGLE_APPLICATION_CREDENTIALS value.Below is the code sample <?php\nheader(\"Content-Type: application\/json; charset=UTF-8\");\nheader(\"Access-Control-Allow-Methods: POST\");\nheader(\"Access-Control-Max-Age: 3600\");\nheader(\"Access-Control-Allow-Headers: Content-Type, Access-Control-Allow- Headers, Authorization, X-Requested-With\");require_once '\/home\/macgowan\/vendor\/autoload.php';\/\/ [START tts_synthesize_text]\nuse Google\\Cloud\\TextToSpeech\\V1\\AudioConfig;\nuse Google\\Cloud\\TextToSpeech\\V1\\AudioEncoding;\nuse Google\\Cloud\\TextToSpeech\\V1\\SsmlVoiceGender;\nuse Google\\Cloud\\TextToSpeech\\V1\\SynthesisInput;\nuse Google\\Cloud\\TextToSpeech\\V1\\TextToSpeechClient;\nuse Google\\Cloud\\TextToSpeech\\V1\\VoiceSelectionParams;putenv('GOOGLE_APPLICATION_CREDENTIALS=\/Users\/macgowan\/google_cloud\/service-account-text-to-speech-test-00.json');try\n{putenv('GOOGLE_APPLICATION_CREDENTIALS=\/Users\/macgowan\/google_cloud\/service-account-text-to-speech-test-00.json');\n\/\/ $client->useApplicationDefaultCredentials();$ip = getenv('GOOGLE_APPLICATION_CREDENTIALS');\nprintf(\"Get env var - GOOGLE_APPLICATION_CREDENTIALS: %s<br \/>\", $ip);$ip = getenv('APACHE_RUN_USER');\nprintf(\"Get env var - APACHE_RUN_USER: %s<br \/>\", $ip);\/\/ *** FAILS HERE ***\n$client = new TextToSpeechClient();$text = \"Hello Joe\";print('Set input text using the SynthesisInput() object' . PHP_EOL);\n$input_text = (new SynthesisInput())\n->setText($text);$voice = (new VoiceSelectionParams())\n->setLanguageCode('en-US')\n->setSsmlGender(SsmlVoiceGender::FEMALE);$audioConfig = (new AudioConfig())\n->setAudioEncoding(AudioEncoding::MP3);$response = $client->synthesizeSpeech($input_text, $voice, $audioConfig);\n$audioContent = $response->getAudioContent();file_put_contents('\/home\/macgowan\/output.mp3', $audioContent);\n$client->close();\n}\ncatch (Exception $e)\n{\nprintf(\"Caught exception: %s<br \/>\", $e->getMessage());\n}\n?>Thanks for your help - Chris   ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AI Augmented Sensory Headset",
        "Question_creation_time":1665173880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-Augmented-Sensory-Headset\/td-p\/475836\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":20.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Wondering when Google will develop olfactory sensor addition to VR headsets and technology. In laymens terms, adding the sense of smell to VR headsets using an add on similar to a printer ink cartridge, but designed specifically for the sense of smell. Theoretically, it is possible, but to manufacture it in a large scale. It can change the way programs, especially helping boost the food and hospitality industry as well as giving everyday people a very good reason to smell fresh food and drink... from their phone! Where and how can we further this research for this wonderful idea?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vocal emojis in Speech-to-Text",
        "Question_creation_time":1667461260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vocal-emojis-in-Speech-to-Text\/td-p\/485418\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello! I am majoring in Theoretical Linguistics this year and I would like to write my dissertation on Google Cloud API and the vocal emojis supported, delving into the neural network to find out how they are translated. I have seen that my native language is missing and could build a dataset of spoken forms. Following the tutorial for using the Speech-to-Text API with Phyton I found out that very little information on this project are public.Should I contact some specific person\/service via my institutional account to receive material for a study case?Thank you!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AI\/ML",
        "Question_creation_time":1624022940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-ML\/td-p\/31\/jump-to\/first-unread-message",
        "Question_upvote_count":10.0,
        "Question_view_count":532.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"This is the discussion space to talk about all things AI\/ML related.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Translate API",
        "Question_creation_time":1640861160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API\/td-p\/181620\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":272.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,I would like to use Google Translate API in plain javascript.As far as I understand from this guide, the supported languages are : ... and some additional languages :Does translate API is supported for Javascript as well? If so, where is the guide?Thanks in advance.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google cloud text to speech",
        "Question_creation_time":1632636660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-text-to-speech\/td-p\/171231\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":771.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I registered myself for the google cloud text-to-speech service recently. Speech Studio worked just fine for the first few days, but today, to my dismay, there is distortion in the text reader's voice.What can I do about it?Thanks.  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vonage Smart Number",
        "Question_creation_time":1666659540000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vonage-Smart-Number\/td-p\/481819\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":113.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Trying to find a way to link a Vonage Smart Number (Vonage Communications, not API) to Dialogflow ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What's the training corpus of models behind GCP Natural Language APIs?",
        "Question_creation_time":1668695760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-training-corpus-of-models-behind-GCP-Natural-Language\/td-p\/490614\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, where can I find some information about which datasets are used for training models that power the natural language APIs for sentiment analysis, entity extraction, etc.? Thanks!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Feature Engineering Vertex AI\/AutoML",
        "Question_creation_time":1658999880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Engineering-Vertex-AI-AutoML\/td-p\/447814\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":88.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey There,I am writing my Master Thesis at the moment. I am comparing AutoML products for image classification. There I compare the product Vertex AI with Azure from Microsoft. However, I can't find the concrete methods of feature engineering and model selection from the documentation. Does anybody know these methodes used for Google AutoML for image classification?Thanks a lot!Arndt",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\u062a\u062d\u0633\u064a\u0646 \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0628\u062d\u062b",
        "Question_creation_time":1638393540000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/%D8%AA%D8%AD%D8%B3%D9%8A%D9%86-%D8%B9%D9%85%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A8%D8%AD%D8%AB\/td-p\/176885\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":37.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"\u0627\u0633\u0639\u062f \u0627\u0644\u0644\u0647 \u0635\u0628\u0627\u062d\u0643\u0645\u0641\u064a \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0628\u062d\u062b \u0641\u064a \u0627\u0644\u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u0634\u0627\u0626\u0639\u0629 \u0648 \u0627\u0644\u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u062a\u064a \u062a\u062c\u0639\u0644 \u0627\u0644\u0634\u062e\u0635 \u0644\u0627 \u064a\u0639\u0628\u0631 \u0639\u0646 \u0633\u0624\u0627\u0644\u0647 \u0623\u0648 \u0645\u0648\u0636\u0648\u0639\u0647 \u0647\u0648 \u0639\u062f\u0645 \u0627\u0644\u0648\u0635\u0648\u0644 \u0625\u0644\u0649 \u0627\u0644\u0643\u062a\u0627\u0628\u0647 \u0627\u0644\u062e\u0637\u064a\u0629 \u0628\u0634\u0643\u0644 \u0635\u062d\u064a\u062d \u0623\u0648 \u0639\u0646\u062f\u0645\u0627 \u064a\u0633\u0623\u0644 \u0633\u0624\u0627\u0644 \u0644\u0627 \u064a\u0633\u062a\u0637\u064a\u0639 \u0634\u0631\u062d\u0647\u0627 \u0639\u0646 \u0637\u0631\u064a\u0642 \u0627\u0644\u0643\u0644\u0627\u0645)\u0627\u0642\u062a\u0631\u062d \u0639\u0646\u062f\u0645\u0627 \u064a\u062a\u0643\u0644\u0645 \u0627\u0644\u0628\u0627\u062d\u062b \u0639\u0646 \u0645\u0639\u0644\u0648\u0645\u0629 \u0623\u0648 \u0633\u0624\u0627\u0644 \u064a\u062a\u0643\u0644\u0645\u0647\u0627 \u0627\u0644\u0628\u0627\u062d\u062b \u0628\u0627\u0644\u0635\u0648\u062a \u0648\u0639\u0644\u0649 \u0637\u0631\u064a\u0642\u062a\u0647 \u0627\u0644\u0639\u0627\u0645\u064a\u0629 \u0648\u0627\u0644\u0643\u0644\u0627\u0645 \u0627\u0644\u0645\u062a\u062f\u0627\u0648\u0644 \u0639\u0644\u064a\u0647 \u0641\u064a \u0645\u0646\u0637\u0642\u062a\u0647 \u0648\u064a\u0643\u0648\u0646 \u0647\u0646\u0627\u0644\u0643 \u0627\u0634\u062e\u0627\u0635  \u0645\u0646 \u0646\u0641\u0633 \u0627\u0644\u0645\u0646\u0637\u0642\u0629 \u064a\u0641\u0647\u0645 \u0644\u063a\u0629 \u0627\u0644\u0645\u062a\u0643\u0644\u0645 \u0648\u064a\u062c\u064a\u0628\u0647 \u0639\u0644\u0649 \u0627\u0633\u0627\u0633\u0647\u0627 \u0648\u064a\u0648\u062c\u062f \u0627\u0634\u062e\u0627\u0635 \u0643\u062b\u0631 \u0645\u062a\u0637\u0648\u0639\u064a\u0646 \u0641\u064a \u0646\u0634\u0631 \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0629 \u0648\u0627\u0644\u062e\u064a\u0631 \u0628\u0627\u0644\u0645\u062c\u0627\u0646 \u0633\u0648\u0627\u0621 \u0643\u0627\u0646\u062a \u0627\u0644\u0637\u0628 \u0627\u0648 \u0627\u0644\u0647\u0646\u062f\u0633\u0629 \u0627\u0648 \u0639\u0644\u0645 \u0645\u0639\u064a\u0646 \u0627\u0648 \u0627\u064a \u0639\u0644\u0645 \u0648\u0645\u0639\u0644\u0648\u0645\u0629\u0627\u0644\u0645\u062e\u062a\u0635\u0631 \u0639\u0646\u062f\u0645\u0627 \u0627\u062a\u0643\u0644\u0645 \u0645\u0646 \u0627\u0644\u062e\u0627\u062f\u0645 \u062d\u0648\u062c\u0644 \u0635\u0648\u062a \u0644\u0627 \u064a\u062a\u0643\u0644\u0645 \u0628\u0627\u0644\u0644\u063a\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0641\u0635\u062d\u0649 \u0648\u0644\u0643\u0646 \u064a\u0648\u062c\u062f \u0627\u0634\u062e\u0627\u0635 \u0643\u0627\u062f\u0645 \u062d\u0648\u062c\u0644 \u064a\u062d\u0644\u0644\u0648\u0646 \u0627\u0644\u0643\u0644\u0627\u0645 \u0628\u0644\u063a\u0629 \u0627\u0644\u0634\u062e\u0635 \u0627\u0644\u0645\u062a\u0643\u0644\u0645 \u0643\u0644\u064b \u062d\u0633\u0628 \u0645\u0646\u0637\u0642\u062a\u0647..\u0627\u062a\u0645\u0646\u0627 \u0648\u0635\u0644\u062a \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0629 \u0648\u0627\u0630\u0627 \u0628\u062f\u0643\u0645 \u0627\u062d\u0643\u064a\u0647\u0627 \u0635\u0648\u062a \u0648\u0627\u0634\u0631\u062d\u0647\u0627 \u0627\u0641\u0636\u0644 \u064a\u0627\u0631\u064a\u062a \u062a\u062e\u0628\u0631\u0648\u0646\u064a \u0648\u0627\u062a\u0648\u0627\u0635\u0644 \u0645\u0639\u0643\u0645 ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Retail API predict call saves the userEvent. It should NOT!",
        "Question_creation_time":1658079900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Retail-API-predict-call-saves-the-userEvent-It-should-NOT\/td-p\/443911\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"According to document, the userEvent sent as part of the predict body is not recorded.  https:\/\/cloud.google.com\/retail\/docs\/predict#recommendHowever, I noticed this was not TRUE.  Here is how to reproduce thisBecause both \"FAKE_SESSION_ID_1\" and \"FAKE_SESSION_ID_2\" are never used before this experient.   The recommendation result for the same sku should be same or very similar.  But they diff a lot.  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Create TPU Node - Malformed Name",
        "Question_creation_time":1645851780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-TPU-Node-Malformed-Name\/td-p\/397566\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":180.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi! Im trying to create a Google Cloud TPU node using TPU client API and I cannot figure out the parent resource name of a TPU node in Google Cloud.I tried all the possible combinations, for example:And I always get the same error (google.api_core.exceptions.InvalidArgument: 400 Malformed name) :        Below you can find the full code I'm using to create the node. Im using Python 3.8, google-cloud-tpu v1.2.1, on a Conda virtualenv. Any help would be much apprecciated!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Node hours vs actual time",
        "Question_creation_time":1655341800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Node-hours-vs-actual-time\/td-p\/431897\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":153.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"What is meant by node hours in VertexAI?I set the budget in VertexAI AUtoML to a 1 node hour but my model has been training for 1 hr and 30+ minutes. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"EntityAnalysis, Version 2 model in natural language API",
        "Question_creation_time":1668688080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/EntityAnalysis-Version-2-model-in-natural-language-API\/td-p\/490557\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":162.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,could anyone share the python code on how to get natural language API to use version 2 for Entity  Sentiment Analysis?\nThe Demo can be run for that, but it seems like in the docs this part is missing:\nhttps:\/\/cloud.google.com\/natural-language\/docs\/reference\/rest\/v1\/documents\/analyzeEntitySentiment\n\nHowever, for the classification, it is possible, as it is described here: \nhttps:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Version-2-model-in-natural-language-API\/m-p\/484641\n\nThanks ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI workbench and Google cloud storage problems accesing files",
        "Question_creation_time":1644211620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-workbench-and-Google-cloud-storage-problems-accesing\/td-p\/390712\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":353.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to create a ML project in which the job is a classification task of videos, so I upload those videos in Google cloud storage, and then I create a notebook on the workbench of vertex AI, for making data balancing, and then train my respective ML algorithm. But I have this problem:1. I want to use the video files from GCS without the need of downloading them again(that was the purpose of uploading them in GCS), but I don't know how can i do this?.I also try uploading de videos into the dataset space of the vertex AI workbench but still don't know how to acces to this files without downloading them again.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to use audio to text transcribe",
        "Question_creation_time":1647737040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-use-audio-to-text-transcribe\/td-p\/405132\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am new to this Google Audio transcription and I have set up the whole Google Free Trial thing and I have tried to use the function of Google's Audio to Speech transcript and well so far my customer experience has been so hard.  I have two files and *.mpa and a *.mp4 file and no matter what i do i keep getting an error that it cannot transcribe.Can someone  please help me with this.  Here are the errors I am getting.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying AutoML tabular model changes feature column types to text",
        "Question_creation_time":1647314460000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deploying-AutoML-tabular-model-changes-feature-column-types-to\/td-p\/403658\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":49.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I\u2019ve trained an AutoML tabular model using a pretty simple CSV file of numeric data. When I ran the training I ensured each feature column was set as numeric. When viewing the column meta data of the trained model, all columns show as numeric. However, when I deploy the model they all show as text and will only accept strings. What am I doing wrong?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I avoid being charged for Tensorboard?",
        "Question_creation_time":1640089320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-avoid-being-charged-for-Tensorboard\/td-p\/180658\/jump-to\/first-unread-message",
        "Question_upvote_count":2.0,
        "Question_view_count":331.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Today I received an email from GCP saying that my account will be charged for using Vertex AI Tensorboard from February. It is quite expensive and I want to stop using the service and avoid being charged.How can I do that? There is no option for Tensorboard in the API dashboard (just one for Vertex AI generally). I only have \"basic support\" so I cannot contact technical support, and I am not the billing administrator so I cannot contact billing support. Is there any way I can disable Tensorboard?Thank you.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Feature Store Calculations",
        "Question_creation_time":1652363340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-Calculations\/td-p\/422580\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,I have a Google Colab notebook with some functions (Python) that been used to calculate the features for a model.The functions use as inputs data from an API.The question is if I can or should calculate the features inside a Features Store and feed the results to the Model?Or in which Instance do I need to make the calculations and then feed the results into the model?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML",
        "Question_creation_time":1636692600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ML\/td-p\/175533\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":311.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"How to start my journey for being an ML\/AI or data science engineer?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vision AI labels",
        "Question_creation_time":1652361420000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-AI-labels\/td-p\/422564\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Where can I find list of all labels what could be detected in Vision AI ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Translation issue",
        "Question_creation_time":1666945620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translation-issue\/td-p\/483154\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":205.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello Team,I am trying to document translation using Google cloud translate V3.I found some issue in below-1. Text Overlapping from German to English2.Some text position was not correct3.table column name show in bottom of pages.4.Some pages were not being Translate. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI endpoint deployment",
        "Question_creation_time":1667537580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-endpoint-deployment\/td-p\/485783\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"How can I utilize the mega GPU during endpoint deployment  for vertex ai work? Are there any model for examples or other resources that I can use to better grasp this?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Short polish inputs recognized but not returned by ASR.",
        "Question_creation_time":1663809540000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Short-polish-inputs-recognized-but-not-returned-by-ASR\/td-p\/469439\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where is Visual Inspection AI?",
        "Question_creation_time":1641277380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Where-is-Visual-Inspection-AI\/td-p\/181914\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":179.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Does anybody know where to get started with the Visual Inspection AI?  It has been advertised for more than 6 months, but I cannot find where it is available.   The landing page is here: https:\/\/cloud.google.com\/solutions\/visual-inspection-ai  However, it has never shown up in my Google Cloud Platform Console. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run Colab with Mulitple GPUs using Drive Files OR workaround",
        "Question_creation_time":1658158200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Run-Colab-with-Mulitple-GPUs-using-Drive-Files-OR-workaround\/td-p\/444346\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":312.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,I am trying to run my ML model in Colab utilizing a custom VM with multiple GPUs. I can successfully spin up a 2 GPU DeepLearning VM and connect to a Colab notebook via port-forwarding to a locally-hosted connection (Jupyter Notebook), as shown here.Although I can connect to custom runtimes directly WITHOUT port-forwarding to a locally-hosted connection, I can only access 1 of the 2 GPUs this way (i.e. I can successfully connect to a locally-hosted, port-forwarded runtime and verify that the notebook can access the 2 GPUs; however I am running into issues when trying to mount my Google Drive.I know that ocamlfuse was offered as a suggestion to this Drive issue,  however, none of the download options work. Specifically, it seems like a locally-hosted port-forwarded runtime doesn't allow terminal inputs, so I can't \"Press [ENTER]\" to allow the download, as shown below:User import cursor shows up for a direct connection to a custom or hosted runtime:User import cursor fails to show up\/accept inputs in a locally-hosted, port-forwarded custom VM.In general, it seems like terminal commands don't work in Colab in a locally-hosted runtime.  Another option is PyDrive, which I've used in the past. However, since PyDrive relies on authentication through a local port, I can't get it to work on my locally-hosted custom VM.In short I'm looking for tips\/suggestions for any of the following issues:1) An alternative workflow to run my ML model using multiple GPUs (i.e. that's not through port-forwarding to a locally-hosted connection)2) How to get that user cursor to show up (enabling me to download ocamlfuse)3) How to authenticate in PyDrive, given I'm already using a local port connection to host my runtime.4) Alternatives to accessing my Drive\/Drive files.  Thank you so much!     ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Getting started DialogFlow CX",
        "Question_creation_time":1664023260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-started-DialogFlow-CX\/td-p\/470603\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":51.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Looking into starting a project using DialogFlow CX. Seems rather promising but have one issue I cannot seem to find an answer for. The agent will be connected to via IVR (from Flex\/Callcenter). I need to gather some information on start so that I can identify the hotel\/property that will be referenced in the conversation.  I found session parameters but those are isolated to the session from start to finish but not passed to the start of a session. We are starting with about 60 properties and when the agent starts, it needs to \"know\" what property it is dealing with. Another quick question - will I need a separate telephony integration number to run multiple concurrent instances? I am really new to all this so my language may be off. Thanks in advance!!Robert ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get Cloud Vision API as good as Google Lens",
        "Question_creation_time":1656077040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Get-Cloud-Vision-API-as-good-as-Google-Lens\/td-p\/434624\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":375.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"As part of a student team, I am building a system to classify used shoes.I know that Google Lens is doing a really good job here.I came across Google Cloud Vision API (which should be a similar thing) and implemented this in python.For clean, well-angled images like this Air Force One:  I am getting really promising results:If however, i input real-world images like this old used Nike Tanjun:  Things fall apart:But if I upload the image to google lens, I could still figure out the right label:  Logo detection (Nike) almost always works. And using this, I could for example search after the most often occurring word after the Logo (Tanjun) to figure out the model.It must be mentioned that the data of our system will be better than that, there will be multiple images taken from different angles and very good lighting conditions.Now i am trying to figure out how toEITHER: Get Vision API working in the same way as Google LensOR: Acces Google Lens data in a somehow convenient way (should in the best case run from a raspberry pi)   ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issues with Handover Protocols - Facebook & Dialogflow",
        "Question_creation_time":1632938520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-Handover-Protocols-Facebook-amp-Dialogflow\/td-p\/171609\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":343.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoML Tables for model where comparison is required?",
        "Question_creation_time":1632891900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Tables-for-model-where-comparison-is-required\/td-p\/171520\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":361.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,I have used GCP for a while now, and have trained quite a few models using AutoML Tables - all of these have been fairly simple datasets with probably a maximum of 20 columns. I now have a problem that I would like to solve, but the dataset is a lot more complicated. I want to be able to predict the results of Greyhound Racing, or at least the % chances of each Greyhound winning a given race, compared to the other greyhounds running in that same race. To be able to do this I need to feed multiple pieces of data for each Greyhound in each given race, to be able to predict the winning chance of that greyhound in that day's race.However, I am very stuck on how to structure my data. Using AutoML Tables - would I need to structure the data in a tabular form with many columns? Or is there a better way to tackle this problem.Here is an example of the data I would be using:Race:Example data for each Greyhound in the race: Does anyone please have any advice of how to tackle this kind of problem, and how best to structure the data to attempt to predict the winning chance of each Greyhound for that day's race, based on that greyhound's previous data, compared to the other greyhounds in that day's race? Thanks,\nRob",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflox cx conflict: \"intents matching\" and \"parameters form\" at the same page.",
        "Question_creation_time":1668491700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflox-cx-conflict-quot-intents-matching-quot-and-quot\/td-p\/489582\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":67.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello everyone,I found an unexpected behavior with the following page-level configuration.CONFLICT: After the question \"What do you want? \", if the user input is not clear there will not be any intent matching, but if the user include in the sentence the word \"Value1\" which is synonym of \"Entity1\", then the parameter \"intent_param\" (entity type \"@intent\") will be collected with value \"Entity1\". When this happens I was expecting \"sys.no-match-1\" to be activated, but this did not occurred and the page state status is \"PROCESSING_FORM\" (FormFilled: false).Does anyone knows why this happens and if is there a way to avoid this behavior? In this situation I would like to continue the workflow with the  parameter collected and no intents matched.Thank you,\nMiguel.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Confirmation page \/ custom text",
        "Question_creation_time":1666408980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Confirmation-page-custom-text\/td-p\/480915\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi All,I'm just migrating over from AWS to CX, and so far I think its great. However - the tutorial section I'm working through kind of hit a 'draw the rest of the owl' meme - if you don't know it, look it up.The difficult bit is where it gets to 'confirmation page' in the quick start - here: ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Recover deleted Vertex AI resources",
        "Question_creation_time":1647987780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Recover-deleted-Vertex-AI-resources\/td-p\/405934\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":210.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,In order to save on billing, I deleted most of the resources in the data sources, workbench, pipelines in Vertex AI.Is there a way I can recover them??",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Being told to contact Vertex AI support but we don't have a support contract?!",
        "Question_creation_time":1646739240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Being-told-to-contact-Vertex-AI-support-but-we-don-t-have-a\/td-p\/401449\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":51.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Getting an internal error when training a model on Vertex AI.I have gotten repeated emails from Google telling me to contact Vertex AI support about this.We don't pay for a support contract.It seems odd that there is no way to report issues like this to Vertex AI without a support contract.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vision API - Text detection training",
        "Question_creation_time":1641355140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-Text-detection-training\/td-p\/182002\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":75.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there,I'd like to train my Vision Project to improve document text detection for manuscript books. I couldn't find the solution anywhere. The current result is awful. The language is Portuguese.Please advise. Thanks.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflow cx caller abandoned call event",
        "Question_creation_time":1666761180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-caller-abandoned-call-event\/td-p\/482361\/jump-to\/first-unread-message",
        "Question_upvote_count":2.0,
        "Question_view_count":98.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is there a way to trigger a webhook, as soon as the caller abandons the call?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Voice\/language options during conversion of long text files to speech",
        "Question_creation_time":1627934100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Voice-language-options-during-conversion-of-long-text-files-to\/td-p\/165947\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":375.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"The voice\/language options during conversion of long text files to speech. Can anyone help with the doc\/sample for the same.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI dataset permissions",
        "Question_creation_time":1632122100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-dataset-permissions\/td-p\/170536\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":432.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is there a way to assign IAM roles to datasets in Vertex AI so only certain people have access to certain datasets?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use Recommendations AI(Retail API) for multiple stores?",
        "Question_creation_time":1665550260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-Recommendations-AI-Retail-API-for-multiple-stores\/td-p\/477225\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi Guys,We are trying to build an e-commerce personalized recommendation system. we want to make it worthwhile for our clients.  We are trying to use the recommendations API(retail API) for multiple e-commerce stores. But in the retail API, it seems we can use retail API under one project per store. Importing catalogs, creating models, and getting recommendations are only applicable to a single store under one project.One solution is to create separate projects per store only to use retail API, which is not the right way for numerous customers.So, is there any way to do this or any other GCP service that we can go for? Please suggest. Thanks in advance.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to disable TLS 1.0 and 1.1 in dialogflow?",
        "Question_creation_time":1663317120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-disable-TLS-1-0-and-1-1-in-dialogflow\/td-p\/467528\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Need to disable TLS 1.0 and 1.1 for oauth api and events api in the dialogflow. We get those apis while integrating with slack.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Document AI fails for one particular image, else works great",
        "Question_creation_time":1651497900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-fails-for-one-particular-image-else-works-great\/td-p\/419233\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":84.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We are delivering a platform to a customer based on Document AI. The use case it to send a lottery ticket via API and return the structure information using Document AI. We tried for several hundred images and the Document AI OCR worked great (95%+ times captured right string, only errors were line feeds and Q turning into O etc. that we could resolve using a post-processor). But for one set of images (from DC), the OCR fails miserably.  This is a corner case that seems to throw the Document AI engine off the mark.I will appreciate greatly if anyone can help explain it.See one particular image which is the most problematic.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Speech-to-Text for many langages",
        "Question_creation_time":1648449180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-for-many-langages\/td-p\/407723\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":53.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I'm testing bunch of variety of the Speech-to-Text API to transcribe audio from microphone but I'm going through two issues:Note: I didn't see anywhere how to use utf-8 for transcriptionHow can I fix it I used the code found here https:\/\/github.com\/googleapis\/python-speech\/blob\/main\/samples\/microphone\/transcribe_streaming_infini...",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Vertex AI Automl Model ID is invalid. It should start with 3 letters Error",
        "Question_creation_time":1668912000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vertex-AI-Automl-Model-ID-is-invalid-It-should-start-with\/td-p\/491126\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":30.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Authentication errors running vaictl in container",
        "Question_creation_time":1667833740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authentication-errors-running-vaictl-in-container\/td-p\/486888\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":37.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to run vaictl on OSX inside a docker container based on these Vertex AI Vision instructions, but hitting the following auth error:  I've run gcloud auth login in the container and saved the authorization code.Are there any extra steps needed to make this work?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI data lost on VM stop",
        "Question_creation_time":1658479200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-data-lost-on-VM-stop\/td-p\/445990\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":131.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am new to Vertex AI and wanted to try it out for a Kaggle competition. I was able to get a GPU machine up and running, as well as download the data to the machine. The download script was automatically generated when uploading my notebook to Vertex AI. I ran the script and 5 hours later all of the data was there successfully (to the boot disk -  standard persistent disk with 1000 GB). I then ran a first iteration of my model and everything worked great. When I was done, I went back to GCP and stopped my VM, assuming all of my data would be saved. It was not!I then started over and once the data was on the machine I took a snapshot so I wouldn't have to redownload the data a third time. I then made some edits to my model and ran it again. After I was done, I again stopped my VM to not leave it running. All of the data was lost again, but less surprisingly this time. I thought a snapshot could be used as a backup to the original machine, but the documentation makes it seem like it is only for creating a new VM from the boot disk. I then made a new machine but cannot figure out how to use it. I also tried looking for a way to make a new notebook on Vertex with the disk snapshot, but it did not look possible. Questions: ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Vertex AI] Bug - Failed to create endpoint due to the error: INTERNAL",
        "Question_creation_time":1647326400000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Bug-Failed-to-create-endpoint-due-to-the-error\/td-p\/403711\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":338.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"When attempting to create a new Vertex AI endpoint in us-central1 using a healthy model, I keep getting the error: \"Failed to create endpoint \"NAME\" due to the error: INTERNAL\"\n\nI expected the endpoint to get deployed successfully.  In fact, up to about 7 days ago, this operation worked perfectly.\n\nSteps to reproduce:\nAttempt to deploy a health Vertex AI model to a new endpoint in us-central1\n\nI'm currently trying to figure out if this INTERNAL error is specific to a region (or not), but it will take me hours before I can determine if the region is a factor.  I suspect there's some other global issue that's the problem.Has anyone else encountered this problem?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"load a .h5 trained model directly from GCS ?",
        "Question_creation_time":1645415820000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/load-a-h5-trained-model-directly-from-GCS\/td-p\/395442\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":242.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, it's the fist time I actually try to put in a production environment a locally trained .h5 model. I have a website hosted on a cloud run container and I'm trying to run an Image processing pipeline every-time a file is uploaded to GCS via the website (that's why I want to use a cloud function that triggers when a new file is created).my issue:I have found a way to load my .h5 model from GCS but It's taking way too mush time and I'm sure there's surely a better way to do what i'm trying to do:almost 1 minute to load on my local machine. Do you have any recommendation on how to trigger the prediction of my trained model + (pre\/post processing) easily upon file upload from my website (in a serverless context) ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"timeSegments vs timeSegmentAnnotations",
        "Question_creation_time":1668585720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/timeSegments-vs-timeSegmentAnnotations\/td-p\/490092\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":47.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"timeSegments vs timeSegmentAnnotationsCan anyone explain what's the difference between these 2 fields described here? https:\/\/storage.cloud.google.com\/google-cloud-aiplatform\/schema\/dataset\/ioformat\/video_action_recogn...why would I want to tag timeSegments? what's the objective of this? associate a label to a time segment?  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Action Needed | OAuth Google Cloud platform | multiple unique domains",
        "Question_creation_time":1649040900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Action-Needed-OAuth-Google-Cloud-platform-multiple-unique\/td-p\/410024\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":93.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,I am carrying out the OAuth verification in Google Cloud Platform, I received an email that said:\n\"Thanks for your patience while we reviewed your project.Your project pc-api-XXXXXXXXXXXXXXX-XX has multiple unique domains in the redirect URI and origin URLs, many of which have unrelated applications. This is in direct violation of the Google API Services: User Data Policy, which requires that projects accurately represent their identity and intent to Google and to our users when they request access to Google user data.Please follow the instructions on the Google API Console to:You can find more information in the OAuth Application Verification FAQ.  To make sure we don't miss your messages, respond directly to this email to continue with the verification process.\"I have a web server, which checks the validity (domain-1.com) in-app purchases, and I also have a site with a different domain containing: privacy-policy and terms-of-service (domain-2.com).My settings are as follows:OAuth consent screen:\n- Home page application: https:\/\/www.domain-2.com\/\n- Privacy Policy: https:\/\/www.domain-2.com\/privacy-policy\/\n- Terms of Service: https:\/\/www.domain-2.com\/terms-of-service\/\n\nAuthorized domains:\n- domain-2.com\n- domain-1.comID client OAuth 2.0 -> Authorized Redirect URIs:\n- https:\/\/game.domain-1.com:8443I have a working service account.\nI have successfully verified all 2 domains.Where is the mistake?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cloud Vision model change",
        "Question_creation_time":1639371660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-model-change\/td-p\/178017\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":95.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, The changes for the Vision API's model from latest -> stable and stable -> legacy are scheduled around Dec 30th\/Jan 1st. Is there a more concrete date and time for this planned change?I'd like to use the currently stable model for the time being, which would involve switching from \"stable\" to \"legacy\". Since this could have an impact on some environments, I will need to make this swap shortly after the model references are being changed. Thanks!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Eliminacion de fondos personalizados",
        "Question_creation_time":1668229260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Eliminacion-de-fondos-personalizados\/td-p\/488739\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hay documentos que tienen un fondo personalizado ya sea con logos o texto referencial a la empresa o al proceso que se lleva a cabo, es o ser\u00e1 posible eliminar estas caracter\u00edsticas y as\u00ed poder hacer m\u00e1s eficiente el nivel de eficiencia del OCR",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Vision API pricing",
        "Question_creation_time":1654853700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vision-API-pricing\/td-p\/430454\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":185.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I'm currently using the service of the Google Cloud Vision API. On the website it says that the first 1000 Request are for free every month. But for that I need a Billing account which is not for free if I understand correctly.  So basically you can't use the Cloud Vision API completely for free. Am I right or can you use the service without any costs?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Streaming Ingestion into Vertex AI Feature Store",
        "Question_creation_time":1657617960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Streaming-Ingestion-into-Vertex-AI-Feature-Store\/td-p\/441577\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":256.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm just wondering if Vertex AI Feature Store supports streaming ingestions rather than just batch ingestion as seen here (https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/ingesting-batch). I figured that the presence of an online store (https:\/\/cloud.google.com\/vertex-ai\/pricing) means that there is a way to store the most up-to-date data and serve them.Thanks!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Text to speech data residency",
        "Question_creation_time":1664972700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-speech-data-residency\/td-p\/474727\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":84.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Compliance team is asking for Data residency for text to speech data. On Google website, it says,\u201cText-to-Speech is both stateless and resourceless. This means Data Access and System Event data don't apply. As a result, Text-to-Speech is out of the scope of Client Access Licenses (CAL). Google does not log any customer Text-to-Speech text or audio data.\u201dDoes logging here refer to data storage. We do not want to store any data in the cloud. Regards,",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to package custom prediction code and serve it using an Endpoint in Vertex AI ?",
        "Question_creation_time":1635161760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-package-custom-prediction-code-and-serve-it-using-an\/td-p\/173876\/jump-to\/first-unread-message",
        "Question_upvote_count":2.0,
        "Question_view_count":204.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Goal: serve prediction request from a Vertex AI Endpoint by executing custom prediction logic.Expected Workflow:1. Upload a pretrained image_quality.pb model (developed in a non vertex-ai pythonic environment) in a gcs bucket2. Port existing image inference logic into a container and serve the prediction functionality through a vertex AI endpoint. 3. Use Vertex AI api for logging and capturing metrics inside the  custom inference logic.4. Finally we want to pass a list of images (stored in another gcs bucket) to that endpoint.5. We also want to see the logs and metrics in tensorboard.Existing Vertex AI code samples provide examples for custom training , invoking model.batch_predict \/ endpoint.predict , but don't mention how to execute custom prediction code.It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.Thanks  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"503 on translations",
        "Question_creation_time":1658295960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/503-on-translations\/td-p\/445069\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I started to see this error on multiple clusters in America. But there is nothing in the status page. I don't think we had any updates to our code.google.api_core.exceptions.ServiceUnavailable: 503 POST https:\/\/translation.googleapis.com\/language\/translate\/v2?prettyPrint=false: The service is unavailable at this time.I guess I need to wait, but posting here just to raise it",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Steps to get Real life data into the features section",
        "Question_creation_time":1651246740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Steps-to-get-Real-life-data-into-the-features-section\/td-p\/418695\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I already create a tabular classification model using AutoML. All ready have the features created in a Goolge colab and now I need to get the real life data from a Public API to feed the features, then pass the features into the Model and finally get the classifications.The question is are the steps and which tools should I use in order to connect to the API in order to receive the real time data? ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Speech changes to a more robotic feel if I use certain phonemes",
        "Question_creation_time":1642378740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-changes-to-a-more-robotic-feel-if-I-use-certain-phonemes\/td-p\/184068\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":47.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using phonemes for some speaks, to make them sound more natural, but some phonemes change the rest of the sentence feel. I have made a demo here.Using these settings:In the speak below I have two identical speaks, where one word is replaced with a phoneme. Notice how the end of the sentence changes to a more robotic feel in the first. How do I avoid this?<speak>\nIn this training, you will learn more about how you sell <phoneme alphabet=\"ipa\" ph=\"k\u0251\u02d0d\">placeholder<\/phoneme> as a solution for companies that want to minimize out-of-pocket spending and have better control of company spending by employees.In this training, you will learn more about how you sell card as a solution for companies that want to minimize out-of-pocket spending and have better control of company spending by employees.\n<\/speak>",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI integration with mlflow ?",
        "Question_creation_time":1645156620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-integration-with-mlflow\/td-p\/394738\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":918.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is there any way to integrate vertex AI with mlflow ? \nAny articles or resources I can go through ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Poor OCR results from PDF files compared to TIFFs",
        "Question_creation_time":1665717480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Poor-OCR-results-from-PDF-files-compared-to-TIFFs\/td-p\/478060\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,We're using DOCUMENT_TEXT_DETECTION in production to perform OCR on documents. We've found  the quality of OCR of PDF documents compared to the exact same TIFF to be very poor (with missing characters, extra whitespace etc).I've attached an example test image in both PDF and TIFF formats. You can see the text is very legible and the OCR from the TIFF is 100% correct. The OCR from the PDF has multiple missing characters.This leads me to believe that the internal rendering of PDFs performed by the cloud vision API is buggy.Can anyone shed any light?Correct OCR results from TIFF:Poor read from PDF:See missing hyphen, missing 'ME' from 'PAYMENT', and various lost hash\/pound characters with extra newlines.The pdf and tiff can be found in this shared gdrive: https:\/\/drive.google.com\/drive\/folders\/1M4VZ3cT3YDoEn5o565fdWP6_47Y_KISL?usp=sharingHere's a screenshot of the PDF for ease: ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I deploy a pretrained fasttext model?",
        "Question_creation_time":1647135060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-deploy-a-pretrained-fasttext-model\/td-p\/403114\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":235.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I have this code : \"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Vertex AI] Bug - Failed to download file",
        "Question_creation_time":1657179300000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Bug-Failed-to-download-file\/td-p\/439222\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":69.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Vertex AI recently fails to download any file greater than 30M. Any downloaded file will be trimmed at 30M. The download speed is also way slower recently (200k\/s). It was working a few days ago. (downloads files of 100+M at 5M\/s) Any ideas?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoML tables - sample size of an average",
        "Question_creation_time":1645771440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-tables-sample-size-of-an-average\/td-p\/397276\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi everyonesI'm new to google automl tables and have a basic question about which data is worthwhile including in the training of my model.I have a dataset of golfers and will be looking at the averages of scores over different periods. For example, average over the past 3 months, 6 months, 1 year etc.My question is, is it worthwhile also including the sample size for each date range for each player. For example, over the past 3 months, some players will have a sample size of 28 while some will only have 2. Those players that have 28 rounds will have more accurate averages than those with 2. However, I didn't know whether google automl tables would pick up this link automatically, whether I could create a different weighting\/reliability variable, or whether there's a way to specify a link between columns? Or if this automated type of automl isn't really suitable or just leave out that sample size variable?Thanks in advance",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does google cloud speech to text api deals with invalid inputs ?",
        "Question_creation_time":1664101920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-does-google-cloud-speech-to-text-api-deals-with-invalid\/td-p\/470810\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":59.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am using google cloud stream(AsyncStreamingRecognize) for speech to text conversion in my applications. I have gone through the below link to understand the structure of response returned by the apis :StreamingRecognizeResponse  I can have various scenarios where I can end up with various invalid scenarios and I do not understand what could be the responses. I can invalid scenarios like :- User speaks in a different language than what is passed in configuration .- User does not speak anything \/ no input- Only noise gets passed \/ Data lossIs there any parameter inside my response which can point to above scenarios ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GCP Idle Model Charging",
        "Question_creation_time":1667794200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-Idle-Model-Charging\/td-p\/486604\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":113.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello!\nI would like to deploy the ML Model into GCP.Most of the time the model will be sleeping. Sometimes I should use it through Endpoint for some seconds.\nI don't want to pay for full-time GPU instance and I need fast responses at the same time, without deployment from scratch everytime I need it.Is this possible in GCP ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I use this specific voice? (English (en-gb-x-gbg-network)",
        "Question_creation_time":1642719360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-use-this-specific-voice-English-en-gb-x-gbg-network\/td-p\/184984\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Goal: Given text, generate mp3 files using Google Cloud tts servicesProblem: Unable to find specific voice I am used to hearing English (en-gb-x-gbg-network). Other info: I've been using this tts app on android in which I can select the aforementioned Voice Type from the Google TTS engine on android. I have since created a Google Cloud account, and followed the tutorial to setup a project to which I can use their selection of voices. However, when I went through the list of voice that I can use, the en-gb-x-gbg-network was not available to use. AFAIK, en-gb-x-gbg-network is not a premium WaveNet voice type. I suspect it has something to do with android but I can't not see why I can't use this voice on the Google Cloud Platform. Many thanks for any helpful info or any nudge that can point me to the right directionCheers, Welp",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Authentication for the Document AI",
        "Question_creation_time":1654252140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authentication-for-the-Document-AI\/td-p\/428530\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"First off, please be kind, as I'm not a developer and may struggle with some basics concepts.\n\nI'm trying to build a AI Invoice reader to collect invoice data in a spreadsheet, using Integromat \/ make.com (no-code platform) and Google Cloud Services.\nUsually, there are integrations for what I need in Integromat or I use simple REST calls. \n\nWith the Document AI, afaik I have to use OAuth. I have my \"processor\" and the I've been searching the Google Cloud documentation for a while, but for a non-dev it's quite confusing. Where can I find the two URLs needed?\n\nThank you very much for your help!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"A100 ram limitations",
        "Question_creation_time":1657189320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/A100-ram-limitations\/td-p\/439296\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Can someone please help me understand why the best GPUs google offer (A100) have a fixed CPU RAM of 85GB (only 2x that of the GRAM) and all the other poorer GPU options can go over 300GB. It's terribly frustrating for large dataset training pipelines. Especially when mmdetection libraries don't work well on mutiple GPUs and would rather just use the 1",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Vision release notes",
        "Question_creation_time":1655165100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vision-release-notes\/td-p\/431130\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"From here https:\/\/cloud.google.com\/vision\/docs\/release-notes it says that there was an upgrade on OCR model for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION. What is the recent model improvement compare to legacy model (is there some metrics used)?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI image classification models lose accuracy when being placed in a python dictionary",
        "Question_creation_time":1666779180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-image-classification-models-lose-accuracy-when-being\/td-p\/482475\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":106.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have made a  model using vertex AI's image classification. Exported as EdgeTPU tflite model to my Raspberry pi 4 with Coral USB accelerator. When I used the Pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py  to run my model, I get a perfect prediction result. But when I passed them to a python dictionary in my script, the prediction accuracy is way off. https:\/\/github.com\/hillyuyichu\/Pycoral-python-API\/blob\/main\/pycoral_classification.py   Here is a screenshot of the prediction results on my python classification.py:The label in row 1 is always the most active. The one in the last rows are the least active and most inaccurate.ex: In picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Join us on August 4! Machine Learning Day on Google Open Source Live",
        "Question_creation_time":1658991180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Join-us-on-August-4-Machine-Learning-Day-on-Google-Open-Source\/td-p\/447714\/jump-to\/first-unread-message",
        "Question_upvote_count":3.0,
        "Question_view_count":93.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI deploy custom model error - Model server terminated: model server container terminated:",
        "Question_creation_time":1668750420000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-deploy-custom-model-error-Model-server-terminated\/td-p\/490796\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":50.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I'm stuck at following error message when I try to deploy custom model to vertex-ai endpoint.Command:  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Imbalance DataSet for Tabular AutoML",
        "Question_creation_time":1650277560000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Imbalance-DataSet-for-Tabular-AutoML\/td-p\/414630\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":182.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I would like to know if in case of having a tabular database,  with binary data (class 0 and Class 1), that has an imbalance between class 0 and class 1, as it occurs in scenarios of fraud in financial transactions.Does AutoML solves automatically the imbalance situation? Or is it possible to add SMOTE or ADASYN to the AutoML model?  Any comments to advice more than appreciated",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DTMF working in dialogflow cx telephony gateway",
        "Question_creation_time":1641423360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DTMF-working-in-dialogflow-cx-telephony-gateway\/td-p\/182110\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":235.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,\nI want to know that whether we can integrate  bot to Dialogflow CX Phone Gateway and then we can make something like press 1 for this and press 2 for this? and then that bot should work on numbers entered by user? Actually i have done a research regarding this and found that we have dtmf option which let us take input from the user but that is not working , so can you please let me know if something like this is supported?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Batch prediction on custom model",
        "Question_creation_time":1657712340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Batch-prediction-on-custom-model\/td-p\/442147\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":439.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,I used custom containers for training and prediction to create a model on Vertex AI. Now I want to run batch prediction against it but get error message that says \"Unable to start batch prediction job due to the following error: A model using a third-party image must specify PredictRoute and HealthRoute in ContainerSpec.\"I checked documentation, AIP_HEALTH_ROUTE = \/v1\/endpoints\/ENDPOINT\/deployedModels\/DEPLOYED_MODELDoes this mean that the model has to be deployed to an endpoint in order to generate the value of the AIP_ENDPOINT_ID variable?However, the documentation \u201cGet batch predictions\u201d says \u201cRequesting a batch prediction is an asynchronous request (as opposed to online prediction, which is a synchronous request). You request batch predictions directly from the model resource; you don't need to deploy the model to an endpoint.I am confused whether in my situation, the model has to be deployed first. Also, is there any resources regarding hosting custom models for batch predictions?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Need help for compute engine pricing",
        "Question_creation_time":1662441060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Need-help-for-compute-engine-pricing\/td-p\/463295\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"GPU: nvidia-a100-80gb has no pricing but  nvidia-tesla-a100 has",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Working with Context for different intent",
        "Question_creation_time":1663583040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Working-with-Context-for-different-intent\/td-p\/468253\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":138.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\ni'm really new in DialogFlow and would like to dig deep my knowledge about this topic.\nMy question right now is about context. So for my case, i build a Conversational Bot for a Fitness Center and trying to create intent related to an individual's program goal\n\nSo for the training phrases would be\"i want to get ideal weight\"\n\"i want to build muscle\"\n\"i want to be more healthy\"\n\"i want to lose weight\"\n\"i want to gain weight\"\n\nI Called the entity \"Individual-goal\"\nTha Output Context for this is \"Fitness-Goal\"Now for my question:\ni would like to segment OR create the \"Sub-Context\" for this Fitness Goal, in following category:\nHealth - Gain Weight\nHealth - Lose Weight\nHealth - General\nFitness - Muscle Building\nFitness - General\n\nFor this case:\n1. Is it better for me to create multiple Intent ?\n2. Is there a way to put a context based on the response, like \"IF Individual goal contain 'gain weight' then Output Context set to \"Health - Gain Weight\"\n\nWhats the best scenario for this ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB",
        "Question_creation_time":1659595920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-DAGScheduler-Broadcasting-large-task-binary-with-size-2-2\/td-p\/450466\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":794.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi there.\nThere were quite a number of such warnings as the model was getting trained.May I know if we are safe to ignore them?\nWhat does it mean actually?\nThanks in advance.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"BatchAnnotateImagesResponse images info (context support) for web based images",
        "Question_creation_time":1640222460000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BatchAnnotateImagesResponse-images-info-context-support-for-web\/td-p\/181129\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":128.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi all. How could one know in what exact web based image the text was detected when multiple web based images are sent to the cloud vision api in a single request using BatchAnnotateImagesRequest? BatchAnnotateImagesResponse doesn't return that information which is kinda odd... It has ImageAnnotationContext, which holds image details, but it's reserved only for files and not web based images.\n\nIs there some way to do this? Maybe like preserving order of images in request \/ response or something down that line.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI Custom Training Job Container not finding my module: Error while finding module for '...'",
        "Question_creation_time":1650854880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Custom-Training-Job-Container-not-finding-my-module\/td-p\/416620\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":152.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I have a PyTorch training job that I am packaging in a Python software distribution (.tar.gz file). I upload the sdist to a GCS bucket and run it in a container using the gcloud ai custom-jobs create CLI.Up until a couple of weeks ago this worked fine but in recent days my jobs consistently fail with messages like these appearing in their logs:Running command: python3 -m MyPackage.MyModule --job-dir=gs:\/\/my-bucket\/my-job\/model --model-name=my-model\n\n\/opt\/conda\/bin\/python3: Error while finding module specification for 'MyPackage.MyModule' (ModuleNotFoundError: No module named 'MyPackage.MyModule') MyPackage.MyModule is my module where my training code runs, naturally.As I've mentioned above the same procedure worked until recently. There have not been any changes to it and I can clearly see that MyModule.py is located under MyPackage in my .tar.gz file.The container image that I am using is us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-9:latest and from what I can tell it has not changed since the time I successfully used it before.Why is the Vertex AI container not finding my training module? How can I further debug and fix this?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Artificial Intelligence - list of APIs",
        "Question_creation_time":1627065000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Artificial-Intelligence-list-of-APIs\/td-p\/164618\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":485.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Good day.Is it possible to list the Google APIs in the Artificial Intelligence, especially in the Voice recognitions, comparison, analytics etc? Thank you.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline failed to deploy model: \"service_account cannot be specified for deploying AutoML models\"",
        "Question_creation_time":1649678700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Pipeline-failed-to-deploy-model-quot-service-account-cannot-be\/td-p\/412645\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":92.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I made a pipeline that almost mirrors step 6 of Intro to Vertex Pipelines which has managed to get past every step up until the model deployment side of things. The code snippet for my model deploy op is here:   And the associated error message in the logs for the deployment part of the pipeline was:RuntimeError: Failed to create the resource. Error: {'code': 400, 'message': 'service_account cannot be specified for deploying AutoML Models.', 'status': 'FAILED_PRECONDITION'} Does it have to do with a specific permission I need to give my service account? I don't know how to interpret this error.  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Getting Error Deadline Exceeded when deploying model from Cloud Firestoer functions",
        "Question_creation_time":1638530220000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-Error-Deadline-Exceeded-when-deploying-model-from-Cloud\/td-p\/177128\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":860.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I currently have a firebase function that is set to deploy my AutoML tables model everyday at 5am. This has been working fine for the past month, up until the last week. I have been getting the following error below when the function attempts to deploy the model.I watched a google tutorial and it recommend to return a promise from my cloud function. That seemed to work for 1 day, but I received the error again this morning.I am going to try to implement a retry function, but I figured I would ask on here as well. Also, I am thinking that moving from autoML to VertexAI might help alleviate my issues. Any guidance here is helpful.See below for my deploy model code:  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI",
        "Question_creation_time":1658799480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-while-trying-to-get-explanation-from-custom-container\/td-p\/446817\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":166.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,I created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get explainable predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.Please take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.https:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cloud Translation Permission",
        "Question_creation_time":1668497880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Translation-Permission\/td-p\/489632\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":53.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:I have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org. I did verify that the service account has the permission from the https:\/\/console.cloud.google.com\/iam-admin\/troubleshooter so and that my organization's admin sees that the service account is granted access through ancestor policies.  So what else can we check? ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflow should have its own official facebook app for integration",
        "Question_creation_time":1665696420000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-should-have-its-own-official-facebook-app-for\/td-p\/477998\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflowI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the settings for Webhooks channels it needI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experienceps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"computational instances at the tool Vertex AI",
        "Question_creation_time":1660795320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/computational-instances-at-the-tool-Vertex-AI\/td-p\/455213\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":37.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,\nI have a question about the computational instances at the tool Vertex AI in the field of image classification. What are the underlying instances of the process or where can I find out? I'm looking for Information comparable to these syntax for example: Virtual Machine (CPU: Intel Xeon E5-2690 v3, 6 vCPUs, GPU: NVIDIA Tesla K80, 56 GB RAM, 380 GB SSD)\nThanks\nArndt",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"deploying model on vertex ai deploymentResourcePool to an endpoint located in another project.",
        "Question_creation_time":1666743120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/deploying-model-on-vertex-ai-deploymentResourcePool-to-an\/td-p\/482309\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":91.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'am trying to deploy a custom trained model to a deployment resource pool that is located in project-1 to an endpoint located in project-2 , I have granted the editor role for project-1 to user account (u1) which also has editor role in project-2. when I try to deploy the model from user account (u1) ,I get the following error:grpc_message:\"DeploymentResourcePool 'projects\/{project-1}\/locations\/us-central1\/deploymentResourcePools\/drlpool' does not exist.*the deployment resource pool (drlpool) exists and also deploys successfully if the endpoint and the deployment Resource Pool are in the same project.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Strange behaviour of ARIMA model",
        "Question_creation_time":1658478600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Strange-behaviour-of-ARIMA-model\/td-p\/445988\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi guys, I'm working with ARIMA Model and I found a strange behaviour.I have two dataset called Then I create two model in this waysample_10_arimasample_11_arimaThen I call the ML.FORECAST function for both in that wayResult for sample_10:Result for sample_11:In the first case sample_10_arima the standard_error is low (around 2.8) but in the sample_11_arima the standard_error is high (between 60 and 101). Why this difference occour? The time series are very similarThanks, Marcello",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"NLP is hard",
        "Question_creation_time":1629067440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NLP-is-hard\/td-p\/167242\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":328.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Who else thinks NLP is the hardest subset of AI to build?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Translate service error - Unsupported language pair",
        "Question_creation_time":1661402280000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translate-service-error-Unsupported-language-pair\/td-p\/459774\/jump-to\/first-unread-message",
        "Question_upvote_count":3.0,
        "Question_view_count":526.0,
        "Question_answer_count":13,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Our application started to have some strange error from 25th of August which was working properly until today. Some very basic translation requests get the \"Status(StatusCode=\"InvalidArgument\", Detail=\"Unsupported language pair.\" error. For example the words \"loan\", \"excellent\", \"wonderful\" get the errors from service. I checked the release notes of the service but found nothing. Could you please help about the issue?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Changing the service account for an endpoint",
        "Question_creation_time":1668994620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Changing-the-service-account-for-an-endpoint\/td-p\/491278\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I have deployed a Vertex AI model that was created using a custom image. However, when I tried deploying to an endpoint, it fails when it tries to run this line:In the logs, there was an error message that readsIt appears that the endpoint has been assigned a service account that is not associated with my account. From this documentation (https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/custom-service-account), it appears that a service account managed by AI Platform Prediction is being used when a custom container is being used. However, my account does not have the permissions to create another custom service account, as it is being managed by my client. I came across this solution (https:\/\/stackoverflow.com\/questions\/68456262\/gcp-vertex-ai-model-access-gcs-failed) where the user had the exact same problem and solved it by changing the service account used at the endpoint. As such, I would like to ask how it will be possible for me to change the service account used by the endpoint without having to create a new service account?Thank you.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflow CX Intergration with Messenger from Facebook",
        "Question_creation_time":1668544140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-Intergration-with-Messenger-from-Facebook\/td-p\/489873\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I have some issue when I integrate dialogflow cx with messenger. \nThe issue is I didn't get response from my bot, I have a suspect that dialogflow failed to send message to user on messenger.\nSo, my problem now is how to check sending message process in dialogflow to get the detail of error?Thanks",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google assistant and cloud speech API not working",
        "Question_creation_time":1649221140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-assistant-and-cloud-speech-API-not-working\/td-p\/410871\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":94.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am having problem using Google cloud platform.I bought Google AIY voice kit from AliExpress.com. I discovered it was an old version. Two weeks ago, I used Etcher to flash aiyprojects-2021-04-02.img.xz from GitHub on an SD card and set up my voice kit. Hardware testing was good. I then created a project, named \u201cVoice Kit\u201d, on google cloud following directions given on \"aiyprojects.withgoogle.com\/voice-v1\/\". I had the following experience:It would be appreciated if I could be educated on the following:",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Cloud AutoML Vision annotation stopped working",
        "Question_creation_time":1668777360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-AutoML-Vision-annotation-stopped-working\/td-p\/490920\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":55.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Has anyone encountered the issue where the AutoML Vision annotations for datasets stopped working. This includes not being able to change labels anymore, not being able delete created labels or not save create labels. The annotations were working as expected last week, but for some reason they stopped working this week.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Auto ML edge training failure",
        "Question_creation_time":1667146440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Auto-ML-edge-training-failure\/td-p\/483746\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am training an edge model in vertex AI. It is failing after a few hours. Details in the screenshot below. Tried 4 times and failed all 4 times. I cannot see to see any detail at all on the error. Can someone from Vertex AI please help? Training fails after about 3 hours if I pick the highest accuracy option but seems to process if I pick the 'best trade-off' options.  Screenshot of the of the failed jobs below.Screenshot upload fails just like getting any support from Google.  The training pipeline id is 2116799302125748224",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No results with is_final true for single utterance set to true",
        "Question_creation_time":1665469080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/No-results-with-is-final-true-for-single-utterance-set-to-true\/td-p\/476809\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":72.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using below configuration to identify my voice input stream (Hindi language)  :",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Commercial usage of Google Cloud TTS",
        "Question_creation_time":1661107680000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Commercial-usage-of-Google-Cloud-TTS\/td-p\/458420\/jump-to\/first-unread-message",
        "Question_upvote_count":2.0,
        "Question_view_count":117.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,I wish to use Google cloud's Wavenet TTS (TextToSpeech) for commercial use for my company. Can anyone please confirm whether it is allowed or not?RegardsUtkarsh Dubey",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model Selection \/ Feature Engineering",
        "Question_creation_time":1659418020000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-Selection-Feature-Engineering\/td-p\/449387\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hey There,I am writing my Master Thesis at the moment. I am comparing AutoML products for image classification. There I compare the product Vertex AI with to Azure. However, I can't find the concrete methods of feature engineering and model selection from the documentation.Thanks a lot!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Cloud Translation language support for bcp-47",
        "Question_creation_time":1658348100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translation-language-support-for-bcp-47\/td-p\/445359\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":73.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Google Speech to Text supports languages using bcp 47 codes like es-MX for mexican spanish and pt-BR for Brazilian Portugese.I am using transcription and translation in a pipeline.Is there any support for bcp 47 languages in Google Cloud Translation. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Translate Cloud API",
        "Question_creation_time":1638766980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-Cloud-API\/td-p\/177308\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":208.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi All,I have recently started using Google cloud translate API with python. M having trouble converting this word in the Telugu language which is written in English \"parishkaram chesamu\".  In general internet or mobile the application google translate which we use is giving correctly. But API is returning the same word again.Google Cloud translate API:Input text: parishkaram chesamuOutput text: parishkaram chesamuparameters : text ='''parishkaram chesamu'''\ntarget = \"en\"\noutput = translate_client.translate(text)print(output) --> {'translatedText': 'parishkaram chesamu', 'detectedSourceLanguage': 'te', 'input': 'parishkaram chesamu'}================================Mobile or Internet google translate:Input text : parishkaram chesamuOutput text: We have solved",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google translator is free or has any kind of pricing?",
        "Question_creation_time":1662421920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-translator-is-free-or-has-any-kind-of-pricing\/td-p\/463225\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":64.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm using this code for translating my website in my angular project. I'm not using translate API provided by google cloud. So, I just need to confirm that the source I'm using is paid for publicly available (free)?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error uploading csv file to Vertex DataSets",
        "Question_creation_time":1655457780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-uploading-csv-file-to-Vertex-DataSets\/td-p\/432499\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi to allTrying to upload a .csv file to AutoMl for training.Not sure what Im doing wrong, I save the file as csv encode utf 8 and values separated by comma and with both cases getting the error that you will find in the next image.Do I need to upload the files to Cloud Storage or Google BigQuery before using them for training? When trying to create and train the model got the warning from the next image:",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cloud Vision API in Vertex AI?",
        "Question_creation_time":1669604340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-API-in-Vertex-AI\/td-p\/493648\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":22.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"  Hi,I am a newbie in Google Cloud and i have an elementary conceptual question about the dependency between Cloud Vision API and Vertex AI or the recently launched Vertex Vision AI.I have an app that makes predictions on images using Google Vision AI API ImageAnnotatorClient() Is this API going to be part of  Vertex AI  or Vertex Vision AI?Or in other words, should I modify the below code to make it part of Vertex AI\/Vertex Vision AI?          ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Mount gcsfuse in gcloud ai custom-jobs local-run",
        "Question_creation_time":1664326260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Mount-gcsfuse-in-gcloud-ai-custom-jobs-local-run\/td-p\/471834\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":96.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When locally testing my custom-job through \"gcloud ai custom-jobs local-run\" command, I would like to have access to a bucket mounted though gcsFuse as it happens when I launch the same containerized job from GCloud console. Is there the option to have the same access locally?Thank you for helping",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"speech-to-text improvements",
        "Question_creation_time":1635759360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/speech-to-text-improvements\/td-p\/174445\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":331.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Good afternoon!I am a user of Speech-to-Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually.So, in Russian it works fine, however, 30-40 percents of the words are incorrect. Moreover, there are no Russian punctuation at all.  So I get the speech-to-text transcript, then I create the perfect transcript out of this with correct words and punctuations.All I want to know is how I can improve Speech-to-Text by using the perfect transcript I have already corrected? Where I can send that data to?P.S. Sorry for my English. I hope You can understand me",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflow cx v3 DetectIntentRequest returning no-match",
        "Question_creation_time":1666792500000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-v3-DetectIntentRequest-returning-no-match\/td-p\/482545\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to create and manage agents using exclusively the api.  I have created an agent with one intent only:name: \"projects\/???\/locations\/global\/agents\/1828e34b-78bc-48f5-9212-6dd83497d409\/intents\/b9e91883-f358-46ea-9661-a8a39c7d2557\"\ndisplay_name: \"test-age\"\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 23 \"\nparameter_id: \"p0\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" my age is \"\n}\nparts {\ntext: \" 68 \"\nparameter_id: \"p1\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 44 \"\nparameter_id: \"p2\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age \"\n}\nparts {\ntext: \" 81 \"\nparameter_id: \"p3\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age is \"\n}\nparts {\ntext: \" 35 \"\nparameter_id: \"p4\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" the age is \"\n}\nparts {\ntext: \" 29 \"\nparameter_id: \"p5\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 37 \"\nparameter_id: \"p6\"\n}\nparts {\ntext: \" years of age \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 45 \"\nparameter_id: \"p7\"\n}\nparts {\ntext: \" years \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 52 \"\nparameter_id: \"p8\"\n}\nparts {\ntext: \" years old \"\n}\nrepeat_count: 1\n}\nparameters {\nid: \"p0\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p1\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p2\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p3\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p4\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p5\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p6\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p7\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p8\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\npriority: 500000However, when I try to detect that intent using the DetectIntentRequest as shown in the github samples I keep getting no-match results:====================\nQuery Text: ' I am 55 '\nDetected Intent: text: \" I am 55 \"\nlanguage_code: \"en\"\nresponse_messages {\ntext {\ntext: \"Sorry, could you say that again?\"\n}\n}\ncurrent_page {\nname: \"projects\/???\/locations\/global\/agents\/1828e34b-78bc-48f5-9212-6dd83497d409\/flows\/00000000-0000-0000-0000-000000000000\/pages\/START_PAGE\"\ndisplay_name: \"Start Page\"\n}\nintent_detection_confidence: 0.3\ndiagnostic_info {\nfields {\nkey: \"Alternative Matched Intents\"\nvalue {\nlist_value {\n}\n}\n}I did train the agent in advance. Appreciate any help",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Will Google provide MTQP in Cloud Translation API?",
        "Question_creation_time":1666700820000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Will-Google-provide-MTQP-in-Cloud-Translation-API\/td-p\/482115\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, I discovered with interest that your Google Translation Hub advanced tier offers document post-editing features, and, as part of that, includes an MTQP quality prediction score on a segment by segment basis. \n\nThis would be a very interesting feature to include in Cloud Translation API, particularly for TMS and CAT tools like Trados\/MemoQ\/Memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional Translation Memory technology.\n\nIt could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.So my question is whether Google is looking at making this available in the API, or whether Google is treating that as proprietary information that you guys do not want to make available outside of your Google Translation Hub?  I really hope the answer is the former, not the latter...\n\nThank you.\n\nMichel Farhi\nPrincipal Localization Engineer\nNI (formerly National Instruments)",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google ML kit",
        "Question_creation_time":1660119120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-ML-kit\/td-p\/452579\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":106.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I know Google provides an ML kit supported by android that we can integrate into an app. The ML Kit provides many Vision and NLP APIs that can help us make our own Google-like Lens.Anyone can give me more information on how to get the ML kit?I am the CEO and I am looking for a CTO to my company, must be good in Python, A.I., Machine Learning, IoT and Robotics.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Total Novice",
        "Question_creation_time":1652211720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Total-Novice\/td-p\/421957\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Friends,Can I submit a file for conversion from speech to text without having to learn computer coding - even if it is at a very simple level? Can I just submit the file somewhere for transcription?Thank You,Just, simply, a consumer",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI explain with a custom trained scikit-learn classification model",
        "Question_creation_time":1656581520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-explain-with-a-custom-trained-scikit-learn\/td-p\/436711\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":282.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi Google Community,I was wondering, has anyone been able to successfully train and deploy a custom trained scikit-learn classification model and deploy it to a vertex endpoint with the feature attribution through the explain endpoint working?Every time i define my instances, predictions and explanation_spec while uploading my model, i get errors on the endpoint for the :explain method. Specifically, i get '400 bad request' with no information on why it was a bad request.I am using the v1beta1 ai platform python SDK and also am using a custom basic serving container. The custom container works for :predict but :explain does not work. Is there some example code out there? Is scikit-learn not supported for feature attribution? Thanks! Ryan",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model adaptation - Speech-to-Text - GA?",
        "Question_creation_time":1634633040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-adaptation-Speech-to-Text-GA\/td-p\/173368\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":365.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I'd like to know if Model adaptation feature is ready to use in production (custom classes, phrase sets, etc). Official web documentation (https:\/\/cloud.google.com\/speech-to-text\/docs\/model-adaptation) says it is a preview feature (Pre-GA). Also, REST resources are inside namespace v1p1beta1 (https:\/\/cloud.google.com\/speech-to-text\/docs\/reference\/rest).On the other hand, release notes web page (https:\/\/cloud.google.com\/speech-to-text\/docs\/release-notes#May_07_2021) says \"The Speech-to-Text model adaptation  feature is now a GA feature\".Thank you very much,Pablo Gomez",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error in GCP Doc AI project",
        "Question_creation_time":1667966340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-in-GCP-Doc-AI-project\/td-p\/487561\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Good evening . My peer while try to access Document AI page is getting the below error . Facing this issue from 2 PM yesterday. We are working for a POC project from LTI organization. Basically, it should show some processors or specialized processors. Please can you guide us.Regards,Vamsi ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Idle shutdown for user-managed notebook (vertex-AI)",
        "Question_creation_time":1654153800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Idle-shutdown-for-user-managed-notebook-vertex-AI\/td-p\/428171\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":188.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"There are two types of notebooks in Vertex-AI1) managed notebook: https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/introduction2) user-managed notebook: https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/introductionI see that the former has a useful function called \"idle shutdown\" that help manage costs: managed notebooks instances shut down after being idle for a specific time period by default.Why we didn't make it available for user-managed notebook as well? Thanks",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How would you model a list of an unknown number of items in DialogFlow CX?",
        "Question_creation_time":1669058700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-would-you-model-a-list-of-an-unknown-number-of-items-in\/td-p\/491605\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi,Taking from the example at Dialogflow CX: Build a retail virtual agent , if you were to build a shopping cart where users could add unlimited items to purchase. How would you model a solution for this?That is, instead of having:Can we have something equivalent to:How?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"vertex AI Workbench is hanging with error \"Opening notebook with JupyterLab\" for more than a day",
        "Question_creation_time":1662624720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/vertex-AI-Workbench-is-hanging-with-error-quot-Opening-notebook\/td-p\/464300\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":112.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to follow instructions in https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/jupyter-notebooks (vertex AI Jupyter Notebooks tutorials). Steps done1. For the first notebook \"Text Classification model\" I have clicked on \"Vertex AI Workbench\". It takes me to GCP console & workbench.2. I am supposed to click on the \"Create\" button, which I did.3. THen the message \"Opening notebook with JupyterLab\" will come. But it is there for past 1 day, and still it hasn't finished creating. So I canceled the same. I tried once more the same thing happens. Not sure why?I have screen shots, but can't see anywhere to attach.Have anyone tried this tutorial, especially in workbench? Thanks,",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Fast Start GPU for AI training",
        "Question_creation_time":1630550460000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Fast-Start-GPU-for-AI-training\/td-p\/168768\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":341.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Is there a way to fast start-up GPU (like Cloud RUN) if there is training request come-in?Due to GPU cost is high, turn-on 24 hours\/day does not make sense.Pre-empted GPU cloud be another option but offer only 1st minute free.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"The kernel for MyTest.ipynb appears to have died. It will restart automatically.",
        "Question_creation_time":1669287660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-kernel-for-MyTest-ipynb-appears-to-have-died-It-will-restart\/td-p\/492715\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I'm trying to run a test jupyter notebook of a LSTM model running tensorflow. I have tried setting the GPU memory limit like suggested here. But still the I get the error mentioned above. I can not find anything realted to GC vertex AI and everyone suggest setting the gpu memory in case of such errors. For reference I have tried to run this as well on my Vertex AI jupyter lab and it crashes as well. The only thing I added was this:gpus = tf.config.list_physical_devices('GPU')\nif gpus:\ntf.config.set_logical_device_configuration(\ngpus[0],\n[tf.config.LogicalDeviceConfiguration(memory_limit=12288)]\n)logical_gpus = tf.config.list_logical_devices('GPU')\nprint(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")On my personal computer it runs just fine, but it would take 13 hours to train which is not a option for me at the moment.Any help would be appriciated. Barnabas.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"VertexAI- Auto ML training model failed without giving the reason",
        "Question_creation_time":1658362380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-Auto-ML-training-model-failed-without-giving-the-reason\/td-p\/445439\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":110.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"After an hour of training Auto ML with Vertex AI, it failed without mentioning the reason. I have received the following email;\n\"Due to an error, Vertex AI was unable to train model \"some_model\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \nprojects\/xxxxxxxxxxxxxxx\/locations\/region\/trainingPipelines\/xxxxxxxxxxxxxxxxxxxxxxxx\nError Messages: Internal error occurred. Please retry in a few minutes. If \nyou still experience errors, contact Vertex AI.\"Would you please help me with it?\nThanks",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Translation of MySQL data in 6 different language",
        "Question_creation_time":1660701600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translation-of-MySQL-data-in-6-different-language\/td-p\/454758\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have 20K record in 1 Table of MySQL DBThis table is having a Description column and 6 different columns as TLang1, TLang2, Tlang3....I have to translate the data in Description column in 6 Different Languages and insert them in TLang1, TLang2, Tlang3.... columns in the same row.What approach I can use to do this since the current approach is taking too long.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Importing to Vertex dataset does not import labels.",
        "Question_creation_time":1667362320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Importing-to-Vertex-dataset-does-not-import-labels\/td-p\/484912\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":220.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In Vertex AI I am updating an image dataset, thus:the images are uploaded to the dataset but their labels are ignored and they are classed as Unlabeled. What am I doing wrong? TIA!\n\nPS they are in a csv, like:which worked fine for the dataset creation. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Question_creation_time":1668498960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/td-p\/489639\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000. Thanks! ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Triton on Vertex AI does not support multiple models?",
        "Question_creation_time":1661411700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Triton-on-Vertex-AI-does-not-support-multiple-models\/td-p\/459822\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":107.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Currently, I want to deploy a Triton server to Vertex AI endpoint. However I received this error message.\"failed to start Vertex AI service: Invalid argument - Expect the model repository contains only a single model if default model is not specified\"Is this mean that the Triton server deploy only support one model? It is different from what I have read in this document about concurrent model executionhttps:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-nvidia-triton",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does Vertex AI support labels for counting?",
        "Question_creation_time":1635151620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-labels-for-counting\/td-p\/173840\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":404.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have an image where I have to do a multi-label classification and additionally count the number of a specific item in each image. I'm trying to setup a labeling task so I can enter a continuous number (0-100 for example), but there doesn't seem to be support for it.  Additionally, does the labeling have capabilities to pre-choose a \"default\"  value? Does anyone have an idea?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Python Code example to transcribe 2 audio inputs into speech at the same time",
        "Question_creation_time":1640045220000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-Code-example-to-transcribe-2-audio-inputs-into-speech-at\/td-p\/180446\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":105.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm trying to create a piece of python code that can take 2 audio inputs, 1. from my microphone2. virtual input from zoomat the same timehowever, i am not sure how to transcribe them simultaneously.any help would be appreciated, thank you!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Authenticating to Vertex AI deployed endpoints",
        "Question_creation_time":1668088620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authenticating-to-Vertex-AI-deployed-endpoints\/td-p\/488229\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I am a new user of Vertex AI.  I have trained and deployed a tabular data categorization model to an Vertex AI hosted endpoint.  I have successfully called it from a program running on my laptop where the \"gcloud\" cli is installed.  If I want to run this not from my desktop but have it called from another service, how do I authenticate ?  I have created a service account but I am not sure 1) what roles would need to be attached to that account and 2) how I would provide the service account credentials given that I don't have much control over how the service that will call my model is started (i.e. I can't control its environment vars).  Any help would be appreciated! ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can Google Cloud Vision work as fast as Google lens for OCR?",
        "Question_creation_time":1640779500000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-Google-Cloud-Vision-work-as-fast-as-Google-lens-for-OCR\/td-p\/181569\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":474.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello, I am using Google Cloud Vision for text recognition, but the processing speed is quite slow (5 to 15 seconds). I would like to know how does Google lens work so fast and if there's any way to make Google Vision as fast.Edit: My photos that go through Vision are stored in Firebase Storage. (As I've read in some posts this is the quickest way to process them). Thanks in advance!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Save audio file from speech to text stream",
        "Question_creation_time":1646292900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Save-audio-file-from-speech-to-text-stream\/td-p\/398993\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using @Google-cloud\/speech for streaming audio from the browser to my nodejs backend.\nI would like to save the recorded audio.\nI see no option to do so. Any suggestions? Thanks.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"3D object detection using mobile camera or 3D scanner using cloud vision",
        "Question_creation_time":1637628240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/3D-object-detection-using-mobile-camera-or-3D-scanner-using\/td-p\/176320\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"The use case is, I want detect the 3d model through mobile camera or 3d scanner with dimensions to verify the scanned model is available or not in cloud storage. If model is not available it should list the model with approx model with percentage. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to set multiple series identifier columns on tabular forecast?",
        "Question_creation_time":1628721840000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-set-multiple-series-identifier-columns-on-tabular\/td-p\/166959\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":108.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I tried BigQuery ML's ARIMA+ to predict sales data, but the results were not particularly good.So, I wanted to try adding weather as a feature to the dataset. This requires the use of Vertex AI Tabular forecast (AutoML).The dataset looks like this.When using ARIMA+, multiple columns can be specified by using the following statement. How to set multiple series identifier columns on AutoML? Should I consider merging the store and product columns into one column(eg: tokyo_pixel6)?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using other API to translate PDF documents",
        "Question_creation_time":1667344620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-other-API-to-translate-PDF-documents\/td-p\/484845\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello all, \n\nI am trying to find a way to translate English PDF documents to a target language(Korean) without messing up the original PDF page format(pictures, headers, tables, etc.)\n\nThe only problem with the default google translation is that many of the words that appear in the document are very industry-specific and need to be translated accordingly through AutoML translation. \n\nHowever, we'd like to use our own language model (i.e. fine-tuned GPT3) to translate just the text and feed the translated text to the output stream to get the final pdf output.\n\nI'm yet to see any other company that maintains PDF formatting as well as Google while translating, so I'd really like to use Cloud Translation API with our own translation module for optimal accuracy.\n\nIs there a way to do this? I've tried reaching out to the local Google branch to no avail. Please help!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflow CX logs sink to BigQuery. sink error - field: value is not a record",
        "Question_creation_time":1668584880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-logs-sink-to-BigQuery-sink-error-field-value-is\/td-p\/490079\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":260.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using google cloud logging to sink Dialogflow CX requests data to big query. BigQuery tables are auto generated when you create the sink via Google Logging.We keep getting a sink error - field: value is not a record.This is because pageInfo\/formInfo\/parameterInfo\/value is of type String in BigQuery BUT there are values that are records, not strings. One example is @sys.date-timeHow do we fix this?We have not tried anything at this point since the BigQuery dataset is auto created via a Logging Filter. We cannot modify the logs and if we could modify the table schema, what would we change it to since most of the time \"Value\" is a String but other times it is a Record",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use GCP Endpoints as reverse proxy for Vertex Ai Endpoint",
        "Question_creation_time":1645692900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Use-GCP-Endpoints-as-reverse-proxy-for-Vertex-Ai-Endpoint\/td-p\/396912\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":343.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am using GCP Endpoints to work as a reverse proxy to a Vertex Ai Endpoint. I can authenticate to GCP Endpoints with api keys, service account... but I get the following error code. Yet, am able to get a successful response from Vertex Ai Endpoint directly. # Error code when requesting to GCP Endpoints (API is authenticated)   Even using the flag \"--allow-unauthenticated\" when setting up ESPv2 still fails. The request   openapi.json (host and address removed for privacy)   Any help would be greatly appreciated",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use ARIMA coefficients from BigQuery",
        "Question_creation_time":1663590000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-ARIMA-coefficients-from-BigQuery\/td-p\/468311\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I am trying to use Auto ARIMA from BigQuery and I just want to understand the results. That's what BigQuery is giving me:Store ACoeficients from Store AI trained the model using weekly incomeHow to fit this information in an equation?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom container in vertex workbench",
        "Question_creation_time":1654856700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-container-in-vertex-workbench\/td-p\/430464\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":236.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I noticed that you can use a custom container from the container registry when creating user-managed notebook, but I couldn't find any documentation on the required configuration\/dockerfile specs for it to work with jupyterlab in a similar fashion to launcing a regular workbench environment (e.g. python 3). Should I open default jupyter lab port? anything else?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tabular Forecasting Model in Vertex AI - Cannot deploy model to endpoint",
        "Question_creation_time":1665200280000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Tabular-Forecasting-Model-in-Vertex-AI-Cannot-deploy-model-to\/td-p\/475893\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm getting started with Vertex AI, and the model I'd like to use is a Tabular Forecasting Model. After spending hours tweaking the model that I wanted to deploy, I came across this error message. \"The default version cannot be deployed\". I can deploy a normal Tabular model to an endpoint, but not the Tabular Forecasting model. Does anyone know if there is a way to deploy a Tabular Forecasting Model? If not, is Google planning on adding this functionality anytime soon? Thanks in advance.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI video action recognition - can it return action timeframes instead of a timestamps?",
        "Question_creation_time":1668952020000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-video-action-recognition-can-it-return-action\/td-p\/491200\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":131.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"My problem is that my usecase requires the AI engine I use to provide predictions with the entire duration of the action. It seems to me that vertex AI picks a random frame in the span of the action and return it as the same start\/end values. Here's an excerpt from an actual response Can I make it work the way I need it to? Maybe I'm annotating in a wrong manner?Here's a mockup of what I need. Notice how timeSegmentStart and timeSegmentEnd represent a duration now:",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"When will Hebrew language be available in Text-To-Speech API?",
        "Question_creation_time":1652854440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Hebrew-language-be-available-in-Text-To-Speech-API\/td-p\/424088\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":111.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI quota policy exceed when training custom model",
        "Question_creation_time":1664159880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-quota-policy-exceed-when-training-custom-model\/td-p\/470907\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello team,Can anyone please help me with this,I have been trying to run the custom model training in vertex ai and gives an error saying\"Training pipeline failed with error message: The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus\"Followed the below steps to solve it but didn't help me at all,1. Changed the region (As it mentioned in one comment of Stack Overflow for this error)2. Increased CPU instances in the work pool as well as notebooks but didn't help me at all.I have gone through the IAM & API Services, and then when I checked the quotas for the Vertex AI API for all resources in it, none of them had exceeded the quota limit. I'm still confused as to why it was showing a quota exceed error when I was training the custom model.Please help me on this issue, how to solve it.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Vertex AI Batch Predictions: Bigquery format must be used as input and output simultaneously",
        "Question_creation_time":1667318280000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Batch-Predictions-Bigquery-format-must-be-used-as\/td-p\/484734\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I'm encountering an error when I try to create a batch prediction job with a bigquery table as my input, and a JSONL output in a GCS bucket. The documentation for batch predictions seems to indicate that I can do so, but I still see an error.I'm trying to create a batch prediction job on the Vertex AI console, and I see this error. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Glossary not found.; Failed to initialize a glossary.",
        "Question_creation_time":1668717000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Glossary-not-found-Failed-to-initialize-a-glossary\/td-p\/490677\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"I have created glossary to translate text using Cloud Translation API. it shows me status as running-\n\"name\": \"projects\/xxx\/locations\/us-central1\/operations\/xxx\",\n\"metadata\": {\n\"@type\": \"type.googleapis.com\/google.cloud.translation.v3.CreateGlossaryMetadata\",\n\"name\": \"projects\/xxx\/locations\/us-central1\/glossaries\/xxx\",\n\"state\": \"RUNNING\",\n\"submitTime\": \"2022-11-18T03:59:51.876209069Z\"\n}\n}but when I am trying to use this Glossary for translation api, it shows me error as-\n\"Glossary not found.; Failed to initialize a glossary\".\nEven when I tried listing my Glossary, it doesn't show.Not sure what is the issue. Console activity dashboard shows activity as created Glossary.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"VM Ram vs Google Colab Ram",
        "Question_creation_time":1658926260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VM-Ram-vs-Google-Colab-Ram\/td-p\/447466\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hi, @Eduardo_Ortiz  @josegutierrez sorry to bother but I`m completely lostDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.VM Configuration : GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)Ram obtained in Google Colab from the VM: 1.31 Gb \/ 51.01 Gb Disc 43.79 \/ 186.52As you realized,  althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....What could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?In the next screen shot the ram and disck that I got from Google Colab:Thanks a lot for any help ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Endpoint in GCP",
        "Question_creation_time":1665751020000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Endpoint-in-GCP\/td-p\/478360\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"In GCP I was deploying a model which obtain from training a dataset and after success full Vertex AI Model Registry. It takes too much time around 10 min to create endpoint for model. How can I reduce creation time when creating endpoints on GCP? What factors affected endpoint creation ?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Removed voices from German standard text to speech (tts)",
        "Question_creation_time":1645077900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Removed-voices-from-German-standard-text-to-speech-tts\/td-p\/394397\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":124.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"We have a problem related to the Cloud text to speech API.\nWe develop an AI based chatbot system, and we have lot of different chatbot which speak in English and German also.\nWe are using two different voices 'de-DE-Standard-B' (male) and 'de-DE-Standard-C' (female) in the case of German bots, but both bots speak in same vois at now.\nWe detected the problem at 2022-02-16.Could you give me some information about this problem?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dialogflow quota reset",
        "Question_creation_time":1646183700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-quota-reset\/td-p\/398513\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"Hello,I have a Dialogflow ES agent, and I'm sending a few thousand (~5k) DetectIntent requests asynchronously. Our quota is 9k requests per minute, so it shouldn't be a problem. However what I'm seeing is that even after waiting for 10 minutes or more, when I run another batch (also ~5k), I get a resource exhausted error. If the quota is 9k per minute, why is the resource still exhausted after 10 minutes? And is there a way to know by what time I should try again?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep creation down? (Resolved)",
        "Question_creation_time":1655321334078,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-creation-down-resolved\/2615",
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I thought you\u2019d like to know that creating a sweep seems to be broken for me. I get a new sweep ID, but going to the URL in question gets a 404, and it doesn\u2019t appear in the sweep list either. I\u2019ve tried via CLI and browser.<\/p>\n<p>Edit: Seems to be back up!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Filter GPU curves in the system panel",
        "Question_creation_time":1663566341242,
        "Question_link":"https:\/\/community.wandb.ai\/t\/filter-gpu-curves-in-the-system-panel\/3152",
        "Question_upvote_count":0.0,
        "Question_view_count":149.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi,<br>\nMe and a colleague are sharing a remote server with 8 GPUs. We split them, 4 GPUs each. In the system panel at the WANDB page I currently see data of all 8 GPUs. Is it possible to filter some of those curves, so I\u2019ll only see the GPUs I\u2019m using?<\/p>\n<p>Many thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AttributeError: 'EnumTypeWrapper' object has no attribute 'NOW'",
        "Question_creation_time":1646095129098,
        "Question_link":"https:\/\/community.wandb.ai\/t\/attributeerror-enumtypewrapper-object-has-no-attribute-now\/1993",
        "Question_upvote_count":0.0,
        "Question_view_count":113.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>what I do can reslove this problem?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What are your requirements for a cloud server?",
        "Question_creation_time":1659034492111,
        "Question_link":"https:\/\/community.wandb.ai\/t\/what-are-your-requirements-for-a-cloud-server\/2816",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Amazon Web Services (AWS) presently services over 7,500 government entities and 5000 educational institutions.<\/p>\n<p>If that isn\u2019t an endorsement, we don\u2019t know what is! AWS, known as one of the world\u2019s premier IT corporations, is now one of the top four public cloud computing companies in the world.<\/p>\n<p>Source : <a href=\"https:\/\/www.sevenmentor.com\/amazon-web-services-training-institute-in-pune.php\" rel=\"noopener nofollow ugc\">AWS Course In Pune<\/a><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error on torch.load for a run's saved file",
        "Question_creation_time":1645746538986,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-on-torch-load-for-a-runs-saved-file\/1979",
        "Question_upvote_count":0.0,
        "Question_view_count":118.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI want to load a pt file of a run which is downloaded using  WandB api. but this error is raised:<br>\n<code>'utf-8' codec can't decode byte 0xaa in position 4: invalid start byte<\/code><\/p>\n<p>My code is:<\/p>\n<pre><code class=\"lang-python\">api = wandb.Api()\nruns = api.runs('USERNAME\/PROJ')\nmodel_path = list(list(runs)[0].files())[1].download()\nmodel = torch.load(model_path)\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Add_reference() with nested folders",
        "Question_creation_time":1662827790113,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-reference-with-nested-folders\/3092",
        "Question_upvote_count":0.0,
        "Question_view_count":177.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>We need to set a dataset folder in S3 as an artifact.  The folder has many sub-directories (only one layer though).<br>\nWhen I use the a <code>add_reference()<\/code> command it only stores the directory names of the top-level.<br>\nOf course, I could loop across it, but I\u2019m wondering if there is a command option to make the operation recursive?<\/p>\n<pre><code class=\"lang-auto\">run  = wandb.init(project=WB_PROJECT)\nart = wandb.Artifact(WB_ENTITY, type=WB_DATASET)\nart.add_reference(s3_full, max_objects=WB_MAX_OBJECTS_TO_UPLOAD)\nrun.log_artifact(art)\nwandb.finish()\n<\/code><\/pre>\n<p>EDIT 1: I conclude that the all files are not being added because the <code>Num Files<\/code> in the Artifact Overview shows only <code>5<\/code>.  If I click on the directories, it seems I can see the files, but I assume they are not actually there because of the <code>5<\/code> being reported for the number of files.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"WandB not using user PID when updating",
        "Question_creation_time":1635882593147,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Question_upvote_count":2.0,
        "Question_view_count":258.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Forcing Pre-emption in a sweep",
        "Question_creation_time":1667929194104,
        "Question_link":"https:\/\/community.wandb.ai\/t\/forcing-pre-emption-in-a-sweep\/3391",
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>This is a bit of a weird one\u2026<br>\nMy lab has a cluster that usually runs using slurm, but slurm is down (and may be not up for a while). We would like to still use wandb and still maintain the priority levels that slurm gives (i.e. we want to make sure if there is some critical jobs that need to get run, we can easily pre-empt existing jobs that are running in sweeps and still get them to requeue later when the critical jobs are over (essentially we are trying to do manual pre-emption)<\/p>\n<p>I have been trying to set this up with a dummy sweep that just sends a single \u201cmagic number\u201d to each run, however no matter what I do I cannot seem to get the run to be pre-empted. If I try killing via <code>ctrl+C<\/code>, the wandb process shuts down normally and it marks the run as finished. Is there any way I can get around this to force the pre-emption? Thank you so much for your help!<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport time\nfrom random import randint\nimport os\nimport sys\nfrom tqdm.auto import tqdm\nwandb.init()\nmagic_number = wandb.config.magic_number\ntry:\n    print(f'Hello, world! Magic number is {magic_number}')\n    print('My PID is', os.getpid())\n    size = 1_000_000_000\n    for count in tqdm(range(size)):\n        if count % (size \/\/ 10) == 0:\n            print(f'On count {count}')\nexcept (Exception, KeyboardInterrupt, SystemExit) as e:\n    print('Keyboard interrupt!')\n    # I cannot reach this piece of code no matter when I do\n    # I have tried ctrl+c, killing the process corresponding to this python script, killing the wandb agent process\n    wandb.mark_preempting()\n    print('Preempted!')\n    sys.exit(999)\nprint('Done!')\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Image resolution with log()",
        "Question_creation_time":1666200922619,
        "Question_link":"https:\/\/community.wandb.ai\/t\/image-resolution-with-log\/3289",
        "Question_upvote_count":0.0,
        "Question_view_count":221.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We are generating some intricate images and using log() to store those with the run (not to mention trying to add them to a Report).  It has recently become apparent that WandB is downscaling the images when they are logged.  For example, our images are 200DPI, but on viewing them after logging they are 72DPI.<br>\nIs there a way to override this?  I have checked the source for <code>wandb.Image()<\/code> but there is no mention of DPI or resolution.  Its not clear if this is being done by <code>log()<\/code> or by <code>Image()<\/code>.<br>\nThank you.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Collab example for building an \"evaluation\" table using wandb.log()",
        "Question_creation_time":1636129969253,
        "Question_link":"https:\/\/community.wandb.ai\/t\/collab-example-for-building-an-evaluation-table-using-wandb-log\/1232",
        "Question_upvote_count":2.0,
        "Question_view_count":233.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi - in the <a href=\"https:\/\/wandb.ai\/_scott\/wandb_example?workspace=\">wandb_example Workspace<\/a>, there is a table visualisation showing \u201cevaluation\u201d results logged from a run:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f7f77fb07316f11f2805765bd0e420843cb50185.png\" data-download-href=\"\/uploads\/short-url\/znCckAyMsQCwsr3hGddC1h58tN3.png?dl=1\" title=\"Screenshot 2021-11-05 at 16.21.56\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f7f77fb07316f11f2805765bd0e420843cb50185_2_689x426.png\" alt=\"Screenshot 2021-11-05 at 16.21.56\" data-base62-sha1=\"znCckAyMsQCwsr3hGddC1h58tN3\" width=\"689\" height=\"426\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f7f77fb07316f11f2805765bd0e420843cb50185_2_689x426.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f7f77fb07316f11f2805765bd0e420843cb50185_2_1033x639.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f7f77fb07316f11f2805765bd0e420843cb50185.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f7f77fb07316f11f2805765bd0e420843cb50185_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2021-11-05 at 16.21.56<\/span><span class=\"informations\">1212\u00d7749 52.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>My understanding is that this table has to be created using wandb.log() in the training code, but I can\u2019t access the notebook used for the run in the example workspace.<\/p>\n<p>I want to create a similar evaluation table using one of your examples:  <a href=\"https:\/\/github.com\/wandb\/examples\/examples\/pytorch\/pytorch-cnn-fashion\/train.py\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/wandb\/examples\/examples\/pytorch\/pytorch-cnn-fashion\/train.py<\/a><\/p>\n<p>Is there a code snippet or collab example for doing this? I\u2019ve seen the docs for logging a table, but I need a specific example like the one in the example workspace that shows clothing images, and metrics e.g. \u2018guess\u2019. \u2018truth\u2019 etc.<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use W&B in a Jupyter notebook to load a dataset",
        "Question_creation_time":1630334022499,
        "Question_link":"https:\/\/community.wandb.ai\/t\/use-w-b-in-a-jupyter-notebook-to-load-a-dataset\/358",
        "Question_upvote_count":6.0,
        "Question_view_count":328.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>after a few years working in the field and suggesting people to try W&amp;B, I\u2019m excited to finally get to use it myself <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/grinning.png?v=12\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>As part of a remote team, I\u2019m doing an EDA (Exploratory Data Analysis) in Jupyter. We\u2019re storing the dataset as a W&amp;B artifact, and I need my notebook to download the dataset locally,  so I wrote something like:<\/p>\n<pre><code class=\"lang-python\">import wandb\n\nartifact_file = \"my_entity\/my_project\/my_dataset:v0\"\ndata_dir = Path('.').parent \/ 'data'\n\n# Download data from W&amp;B\ndata = wandb.use_artifact(artifact_file)\ndata.download(root=data_dir)\n<\/code><\/pre>\n<p>However,  when I run the cells I get the error:<\/p>\n<p><code>Error: You must call wandb.init() before wandb.use_artifact()<\/code><\/p>\n<p>Two questions:<\/p>\n<ol>\n<li>how do I fix this? Would something like this suffice?<\/li>\n<\/ol>\n<pre><code class=\"lang-python\">run = wandb.init(\n        reinit=True,\n        project=\"my_project\",\n        entity=\"my_entity\",\n        group=\"eda\",\n    )\n\n# Download data from W&amp;B\ndata = wandb.use_artifact(artifact_file)\ndata.download(root=data_dir)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Since I called <code>wandb.init()<\/code>, I guess I should call  <code>run.finish()<\/code>at the end of my EDA, otherwise the background process will run forever (or more realistically until some timeout). Now, in the usual training script, where all the code has been written and debugged before I launch the <code>wandb<\/code> background process, this would be easy: I would just add the <code>run.finish()<\/code> line at the end of the script. Here however I edit and add code while I continue with the analysis (it\u2019s Jupyter). So what\u2019s the best practice? Do I just go on with my analysis and add a <code>run.finish()<\/code> line in the last cell? Or do I call <code>run.finish()<\/code> immediately after downloading the data to the <code>data_dir<\/code>? In other words, I know the standard workflow for using W&amp;B logger and artifacts in non-interactive mode (Python scripts), but I\u2019m not so familiar with the W&amp;B workflow for interactive analyses (Jupyter notebook). Can you help me? Thanks,<\/li>\n<\/ol>\n<p>Andrea<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Forgot password in local",
        "Question_creation_time":1645448025579,
        "Question_link":"https:\/\/community.wandb.ai\/t\/forgot-password-in-local\/1959",
        "Question_upvote_count":0.0,
        "Question_view_count":158.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>What can I do when I forget my password in the local wandb?<br>\nIt seems that deleting or uninstalling  doesn\u2019t work.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Chrome Inline JavaScript Issue when Rendering Molecules",
        "Question_creation_time":1635974407326,
        "Question_link":"https:\/\/community.wandb.ai\/t\/chrome-inline-javascript-issue-when-rendering-molecules\/1217",
        "Question_upvote_count":0.0,
        "Question_view_count":248.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, everyone.<br>\nI recently have been trying to log wandb.Molecule objects via PyTorch Lightning. However, when I view the logged Molecules in my web browser (Google Chrome, Version 95.0.4638.69, 64-Bit), I see the following error, preventing my Molecules from being rendered.<\/p>\n<p><em>[Report Only] Refused to apply inline style because it violates the following Content Security Policy directive: \u201cdefault-src \u2018none\u2019\u201d. Either the \u2018unsafe-inline\u2019 keyword, a hash (\u2018sha256-NFPvvJTeausaOnuU9syzBhm5OjQ9MGcbA9SexsBrsF4=\u2019), or a nonce (\u2018nonce-\u2026\u2019) is required to enable inline execution. Note also that \u2018style-src\u2019 was not explicitly set, so \u2018default-src\u2019 is used as a fallback.<\/em><\/p>\n<p>It looks to me like Chrome is preventing WandB\u2019s web site from rendering inline JavaScript. For reference, I have disabled any browser extension that may be affecting this, and that didn\u2019t seem to help.<br>\nAny ideas as to how to get around this issue? Without of course using a new browser <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=10\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get a link to share project",
        "Question_creation_time":1644130321779,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-a-link-to-share-project\/1873",
        "Question_upvote_count":0.0,
        "Question_view_count":163.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have an existing project. I want to be able to make it public and share it to someone so that they can take a look at the graphs<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Show only columns with different values in experiments table",
        "Question_creation_time":1661183885637,
        "Question_link":"https:\/\/community.wandb.ai\/t\/show-only-columns-with-different-values-in-experiments-table\/2972",
        "Question_upvote_count":1.0,
        "Question_view_count":42.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>In the table view of a project, is it possible to show only the columns that have different values among runs? This would be very useful to quickly explore how changing parameters affect the model.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Wandb with HParams on TF",
        "Question_creation_time":1636160879946,
        "Question_link":"https:\/\/community.wandb.ai\/t\/using-wandb-with-hparams-on-tf\/1233",
        "Question_upvote_count":1.0,
        "Question_view_count":332.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I think I may have got confused with this one. I had to code up a custom model using TF. It is training and running but I want to do some hyper parameter tuning so been working on getting HParms integrated.<\/p>\n<p>But I\u2019m trying to link up Wandb to keep track of things.<\/p>\n<p>Currently, since I\u2019m using hparms, when I initialize wandb with wandb.init(), it seems to initialize it for the whole process and it doesn\u2019t change when it is a new parameter set.<\/p>\n<p>I am calling the wandb.init() and logging after each parameter run, but still it doesn\u2019t create a unique job.<\/p>\n<p>This the function I call,<\/p>\n<pre><code class=\"lang-auto\">def write_to_wandb(ldl_model_params, KLi, f1_macro):\n    wandb.init(project=\"newjob1\", entity=\"demou\")\n    wandb.config = ldl_model_params\n\n    wandb_log = {\n        \"train KL\": KLi,\n        \"train F1\": f1_macro,\n        }\n\n    # logging accuracy\n    wandb.log(wandb_log)   \n<\/code><\/pre>\n<p>This is called from this train function (a high-level version of it). This <code>train_model<\/code> function is repeated again through another hyperparamter function with different hyper-parameter.<\/p>\n<pre><code class=\"lang-auto\">\ndef train_model(ldl_model_params,X,Y):\n    model = new_model(ldl_model_params)\n    model.fit(X,Y)\n    predict = model.transform(X)\n    KLi,F1 = model.evaluate(predict,Y)\n    write_to_wandb(ldl_model_params,KLi,F1)\n<\/code><\/pre>\n<p>So how do I fix this? I want each call to train_model to be recorded in a new run.<\/p>\n<p>I\u2019m new to wandb so I have a feeling that I am not using it as it should be. Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"White screen crash",
        "Question_creation_time":1660020515949,
        "Question_link":"https:\/\/community.wandb.ai\/t\/white-screen-crash\/2862",
        "Question_upvote_count":1.0,
        "Question_view_count":33.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>If I login <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a>, the website does not respond and just show white screen\u2026<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/0fd3ec49a644267b723ce3a0df7d09acd227e45f.png\" data-download-href=\"\/uploads\/short-url\/2g1cHYijVLuymAZWY8CC8f6Kqon.png?dl=1\" title=\"\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-08-09 \u110b\u1169\u1112\u116e 1.46.50\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0fd3ec49a644267b723ce3a0df7d09acd227e45f_2_690x388.png\" alt=\"\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-08-09 \u110b\u1169\u1112\u116e 1.46.50\" data-base62-sha1=\"2g1cHYijVLuymAZWY8CC8f6Kqon\" width=\"690\" height=\"388\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0fd3ec49a644267b723ce3a0df7d09acd227e45f_2_690x388.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0fd3ec49a644267b723ce3a0df7d09acd227e45f_2_1035x582.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0fd3ec49a644267b723ce3a0df7d09acd227e45f_2_1380x776.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0fd3ec49a644267b723ce3a0df7d09acd227e45f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-08-09 \u110b\u1169\u1112\u116e 1.46.50<\/span><span class=\"informations\">5120\u00d72880 363 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any help is greatly appreciated.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging Date Objects",
        "Question_creation_time":1650387630533,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-date-objects\/2263",
        "Question_upvote_count":1.0,
        "Question_view_count":179.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey everyone!<\/p>\n<p>I\u2019m looking for a way to log Dates. I want to show the evolution of our labeled data over time over non-uniform time steps. To make it more clear, let\u2019s say I want to display the amount of data on arbitrary days. If I ran my W&amp;B run on those days, I could distinguish them by <code>run:createdAt<\/code>. The plots could display them as dates as one would expect and everything is fine.  Now, this fails as soon as I want to have a starting date AND an end date. Therefore I\u2019m looking to log date data.<\/p>\n<p>The functionality should be there, as it is for <code>createdAt<\/code>,  but I can\u2019t figure out how to log my own. I couldn\u2019t find a suitable object in the docs and neither POSIX timestamp nor iso format\/datetime objects work out of the box.<\/p>\n<p>Is there no way to do this, or did I overlook something?<\/p>\n<p>As a workaround I could just use the POSIX timestamp as scale, but I guess we all agree that\u2019s a little unwieldy.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hypyerparameter optimization with k folds on each iteration",
        "Question_creation_time":1668614113531,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hypyerparameter-optimization-with-k-folds-on-each-iteration\/3429",
        "Question_upvote_count":0.0,
        "Question_view_count":13.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to perform hyperparameter optimization with wandb and for each iteration I would like to get the average performance across 3 different folds of my dataset.<\/p>\n<p>I have defined a function optimize that i pass to wandb.agent:<\/p>\n<pre><code class=\"lang-auto\">def optimize(config):\n    for fold in range(1, 4):    \n        dataset_artifact = f'fold-{fold}:latest'\n        config['dataset_artifact'] = dataset_artifact \n        with wandb.init(config=config, group=group_name, job_type=f'train-fold-{fold}', name=f'train-fold-{fold}', reinit=True) as run:   \n            train_and_log(config, run)  \n            run.finish()\n<\/code><\/pre>\n<p>I would expect this to creat a seperate run for each fold (since I have specified a different job type and run name as well as passing init=True) so that I would end up with:<\/p>\n<p>Group: param_combo_1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-2<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-2<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-3<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-3<\/p>\n<p>However each run for a given hyperparameter iteration overwrites the previous fold so I in fact end up with<\/p>\n<p>Group: param_combo_1<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0&gt; Job Type: train-fold-3<\/p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&gt; train-fold-3<\/p>\n<p>How can I resolve this issue?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hiding runs in a sweep chart export",
        "Question_creation_time":1642467866195,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hiding-runs-in-a-sweep-chart-export\/1778",
        "Question_upvote_count":0.0,
        "Question_view_count":175.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I have a large sweep (~ 150 runs) and I want to hide some runs (these are not crashed runs) in the final sweep chart that I export. However, even if I turn off these runs in my visualization, I still see them with very faint lines (low opacity) in the chart that I see in my workspace and the PNG\/PDF that I export. Is there a way to completely turn off the visualization of completed runs in my sweep plots without deleting the runs? Thank you!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How often to log to avoid slow down of code?",
        "Question_creation_time":1631380514205,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-often-to-log-to-avoid-slow-down-of-code\/516",
        "Question_upvote_count":4.0,
        "Question_view_count":962.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Just curious, do ppl sync artifacts with wandb\u2019s cloud stuff every time they log or less often? I was curious to know if calling <code>wandb.log<\/code> or logging artifacts was slow or does it use a different process and thus slow down is minimum? \u2026 (in the past I discovered that the slowest part of my code was model checkpointing)<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Visualizations, metrics, etc. keep randomly appearing and disappearing",
        "Question_creation_time":1650483251598,
        "Question_link":"https:\/\/community.wandb.ai\/t\/visualizations-metrics-etc-keep-randomly-appearing-and-disappearing\/2283",
        "Question_upvote_count":4.0,
        "Question_view_count":117.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>This morning we were looking at the visualizations and charts on an active training run and everything was fine. After about 11am PDT, all of the visualizations started randomly disappearing. Sometimes only the loss charts would be visible, other times the losses and metrics and statistics would all be visible.<\/p>\n<p>The best I can tell is that the site is only showing charts for whatever things were in the most recent step. If you send some things less frequently, then their charts\/visualizations disappear until they\u2019re in the step data again.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does one have high disk utilization in pytorch?",
        "Question_creation_time":1631575114171,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-one-have-high-disk-utilization-in-pytorch\/553",
        "Question_upvote_count":4.0,
        "Question_view_count":242.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I saw that being mentioned here <a href=\"https:\/\/youtu.be\/G7GH0SeNBMA?t=1141\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">\ud83d\udd25 Integrate Weights &amp; Biases with PyTorch - YouTube<\/a> so I was curious - how do we have the data loaders in pytorch to have high disk utilization e.g. is increasing the batch size, num_workers the way to go or something else?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep track loss from Tensorboard",
        "Question_creation_time":1636052247640,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-track-loss-from-tensorboard\/1226",
        "Question_upvote_count":0.0,
        "Question_view_count":195.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nIs it possible to use sweep with a metric that is only visible in Tensorboard?<br>\nIt does show up on WandB when  sync_tensorboard=True<\/p>\n<p>But sweep does not seem to work properly when I use the name showing up in the GUI to be tracked and minimized.<\/p>\n<p>Thanks<br>\nBen<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cant see the team after accepting invite",
        "Question_creation_time":1659088178684,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cant-see-the-team-after-accepting-invite\/2823",
        "Question_upvote_count":0.0,
        "Question_view_count":271.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi! I can\u2019t see my team after accepting the invite, it says \u201cReceived an invite but still can\u2019t see the team? Make sure you are logged in with the email where you received the invite.\u201d but it does not tell you what to do when this is already checked <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/stuck_out_tongue.png?v=12\" title=\":stuck_out_tongue:\" class=\"emoji\" alt=\":stuck_out_tongue:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>regards<br>\nR<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Configuration for the Replication for sweep",
        "Question_creation_time":1649996855854,
        "Question_link":"https:\/\/community.wandb.ai\/t\/configuration-for-the-replication-for-sweep\/2246",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is there a replication option for the sweep config?<br>\nI want to compare the average performance of my experiments more than one time due to variance in my experiments.<\/p>\n<p>Thaks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem with wandb.plot.pr_curve",
        "Question_creation_time":1643736812021,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problem-with-wandb-plot-pr-curve\/1857",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello. My code for each training epoch:<\/p>\n<pre><code class=\"lang-auto\">wandb.log({\n    \"PR_curve\":  wandb.plot.pr_curve(y_true, [(x, 1 - x) for x in y_predict])\n})\n<\/code><\/pre>\n<p>But I had plots like that(the left part):<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/44df6eaaba224cfd4133696ef68c21a9de1f33d5.jpeg\" data-download-href=\"\/uploads\/short-url\/9PhavrmCKrRlTKNxaRThfK8TGZv.jpeg?dl=1\" title=\"imgonline-com-ua-2to1-fyiDuqxQ397nx579\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/44df6eaaba224cfd4133696ef68c21a9de1f33d5_2_690x210.jpeg\" alt=\"imgonline-com-ua-2to1-fyiDuqxQ397nx579\" data-base62-sha1=\"9PhavrmCKrRlTKNxaRThfK8TGZv\" width=\"690\" height=\"210\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/44df6eaaba224cfd4133696ef68c21a9de1f33d5_2_690x210.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/44df6eaaba224cfd4133696ef68c21a9de1f33d5_2_1035x315.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/44df6eaaba224cfd4133696ef68c21a9de1f33d5_2_1380x420.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/44df6eaaba224cfd4133696ef68c21a9de1f33d5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">imgonline-com-ua-2to1-fyiDuqxQ397nx579<\/span><span class=\"informations\">1565\u00d7477 91.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhen I used pyplot I got the right part for some epoche:<br>\nAs I understand curve for class \u201c1\u201d must be equal to one of curves from second picture.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Artifacts logged with run_id",
        "Question_creation_time":1657984287757,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifacts-logged-with-run-id\/2759",
        "Question_upvote_count":0.0,
        "Question_view_count":99.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello everyone,<br>\nI am new to w&amp;b, so this might be a beginner question, but I was wondering why when I run<br>\n<code> wandb.log_artifact(file_path, name='dataset', type='dataset')<\/code><br>\nI am able to log artifacts correctly without many issues,  whereas if I use the example provided <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/wandb-artifacts\/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=Mb8GiolzPgUU\" rel=\"noopener nofollow ugc\">here<\/a><\/p>\n<pre><code class=\"lang-auto\">def load_and_log():\n\n    # \ud83d\ude80 start a run, with a type to label it and a project it can call home\n    with wandb.init(project=\"artifacts-example\", job_type=\"load-data\") as run:\n        \n        datasets = load()  # separate code for loading the datasets\n        names = [\"training\", \"validation\", \"test\"]\n\n        # \ud83c\udffa create our Artifact\n        raw_data = wandb.Artifact(\n            \"mnist-raw\", type=\"dataset\",\n            description=\"Raw MNIST dataset, split into train\/val\/test\",\n            metadata={\"source\": \"torchvision.datasets.MNIST\",\n                      \"sizes\": [len(dataset) for dataset in datasets]})\n\n        for name, data in zip(names, datasets):\n            # \ud83d\udc23 Store a new file in the artifact, and write something into its contents.\n            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n                x, y = data.tensors\n                torch.save((x, y), file)\n\n        # \u270d\ufe0f Save the artifact to W&amp;B.\n        run.log_artifact(raw_data)\n\nload_and_log()\n<\/code><\/pre>\n<p>I get the artifacts stored in a run_table, and it makes versioning impossible.<br>\nAm I doing something wrong? Below you can find the same function as I modified it for my project, in case I might have missed something<\/p>\n<pre><code class=\"lang-auto\">from wandb.sdk import wandb_init\ndef load_and_log():\n\n    # \ud83d\ude80 start a run, with a type to label it and a project it can call home\n    with wandb.init(project=\"project\", job_type=\"load-data\", resume=\"allow\") as run:\n        \n        dataset = my_function(dir_path + '\/datas', MAX_SAMPLES, MAX_LENGTH) #returns a tuple of lists\n        datasets = dataset.load()  # separate code for loading the datasets\n        names = [\"questions\", \"answers\"]\n\n        # \ud83c\udffa create our Artifact\n        raw_data = wandb.Artifact(\n            \"dataset\", type=\"dataset\",\n            description=\"json of the preprocessed dataset - not split\",\n            metadata={\"source\": \"https:\/\/source.php\",\n                      \"sizes\": [len(dataset) for dataset in datasets]})\n\n        # transfer lists into table\n        table = wandb.Table(columns=[], data=[])\n        for name, dataset in zip(names, datasets):\n          table.add_column(name=f\"{name}\", data=dataset)\n\n        # \u270d\ufe0f Save the artifact to W&amp;B.\n        wandb.log({f\"dataset_{MAX_SAMPLES}_{MAX_LENGTH}\": table})\n\nload_and_log()\n<\/code><\/pre>\n<p>Thank you in advance if you have an answer!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"App UI Bug: Can't see project's artifacts when it has no runs",
        "Question_creation_time":1638473023932,
        "Question_link":"https:\/\/community.wandb.ai\/t\/app-ui-bug-cant-see-projects-artifacts-when-it-has-no-runs\/1461",
        "Question_upvote_count":1.0,
        "Question_view_count":248.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>First, you may call it <em>a flaw<\/em> instead of <em>a bug<\/em> if you like. Second, I am not sure if this is a bug in the App UI or a flaw in your data model (I am new to WandB). Third, I am not sure if I am supposed to report bugs here, on github, or elsewhere. I could not find a non-public means of communicating this.<\/p>\n<p><strong>Expected behaviour.<\/strong><br>\nI should be able to see all artifacts created and\/or used within a given project via App UI (my browser) at all times. If the project has no artifacts, then I should see an explicit indication of that fact, just as on the first screenshot.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fd930d039ffbcbc6df6018430d9da2148658814a.png\" data-download-href=\"\/uploads\/short-url\/AbdR0zySJTKb4nfew90jfroE6W6.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd930d039ffbcbc6df6018430d9da2148658814a_2_345x227.png\" alt=\"image\" data-base62-sha1=\"AbdR0zySJTKb4nfew90jfroE6W6\" width=\"345\" height=\"227\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd930d039ffbcbc6df6018430d9da2148658814a_2_345x227.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd930d039ffbcbc6df6018430d9da2148658814a_2_517x340.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd930d039ffbcbc6df6018430d9da2148658814a_2_690x454.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd930d039ffbcbc6df6018430d9da2148658814a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">741\u00d7489 27.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><strong>Observed behaviour.<\/strong><br>\nThe \u201cArtifacts\u201d icon on the side panel disappears when a project has no runs, <em>whether there are artifacts or not<\/em>.  As depicted on the second screenshot.<\/p>\n<p><strong>What I did.<\/strong><br>\nAll I did between the first and the second screenshots was deleting the only remaining (and empty) run within this project.<\/p>\n<p><strong>Why the observed behaviour is problematic:<\/strong><\/p>\n<ul>\n<li>If there are no artifacts in the project, I do not get an <em>explicit<\/em> confirmation of that.<\/li>\n<li>I cannot see the remaining artifacts if they still exist.<\/li>\n<li>Basically, you are forcing the user to keep a dummy run in a project, so that the bloody icon stays in place. (Figuratively speaking, because it\u2019s not just an icon issue: if you type the <code>\/entity\/project\/artifacts\/<\/code> in the address bar, you still can\u2019t get there)<\/li>\n<\/ul>\n<p>Now, you may argue that artifacts are attached to runs, and not to projects, therefore all artifacts created in a project that has become run-less are doomed to be orphaned (i.e., neither \u201cused\u201d, nor \u201clogged\u201d by any run), therefore they will be garbage-collected sooner or later, therefore there is no reason for the Artifacts tab. But this does not make sense for two reasons. First, why not keep the Artifact tab anyways? Second, if the user creates a dummy run so that the Artifact tab comes back, he\/she will be able to see the orphaned artifacts, as shown on the third screenshot.<br>\nHere\u2019s how I produced the third screenshot. I created a run and \u201cused\u201d a new artifact in this run, all via Python interface. Then I deleted the run, leaving the artifact orphaned. Then I created a new run within the same project, went back to the Artifact tab and found the orphaned artifact from the first run, as on the screenshot. If artifacts are attached to runs and not to projects, why does WandB show the orphaned artifact in this project\u2019s tab?<\/p>\n<hr>\n<p>P.S. Many thanks for all your work and, of course, for making your product available for free. But I can\u2019t help but point out that to facilitate neat and well-organised data science, WandB should be transparent, understandable, and predictable. So far, I have been having a hard time understanding your model and figuring out things like the unexpected difference between <code>wandb.sdk.wandb_run.Run<\/code> and <code>wandb.apis.public.Run<\/code>. I can\u2019t understand why any deviation from the simplest use cases and \u201cBest Practices\u201d must be so painful that it may be easier to re-run all experiments in order to correct how they are logged in WandB than to correct the record directly.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does one change how wandb logs\/prints to my screen so that it always shows it?",
        "Question_creation_time":1637982355385,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-one-change-how-wandb-logs-prints-to-my-screen-so-that-it-always-shows-it\/1401",
        "Question_upvote_count":0.0,
        "Question_view_count":195.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I think wandb might be logging with some <code>WARN<\/code> flag or something because I can\u2019t always see the printing when I <code>tail -f <\/code> my scripts output.<\/p>\n<p><strong>How do I change wandb\u2019s settings so that it prints to stdout?<\/strong><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging values to ongoing run from a different process",
        "Question_creation_time":1656391372182,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-values-to-ongoing-run-from-a-different-process\/2671",
        "Question_upvote_count":0.0,
        "Question_view_count":51.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there!<br>\nIn my use case I\u2019m running a training loop and storing a model at a regular interval. During training, I also want to evaluate metrics such as the CIDEr score for image captioning. The problem is, computing these metrics takes a lot of time (~40 minutes), and the training is running on a cluster where I can\u2019t evaluate the metrics for several reasons.<\/p>\n<p>So my plan is to load the stored models on a separate machine after every update, and evaluate the metrics there. Once done, I would like to log the metrics to the ongoing training runs, with a step parameter set to the time when the model was stored. So by the time the evaluation is finished, the training runs will have progressed in steps.<\/p>\n<p>Is this possible using the wandb api, without getting concurrency problems?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SOLVED: Runs are not logged separately",
        "Question_creation_time":1635269222662,
        "Question_link":"https:\/\/community.wandb.ai\/t\/solved-runs-are-not-logged-separately\/1103",
        "Question_upvote_count":4.0,
        "Question_view_count":349.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I\u2019m trying to run several studies and have them log into separate runs but be grouped un der the name \u201cScenario Eval\u201d as part of the project \u201cScenario\u201d. I have one python script running a for-loop with multiple calls of the following lines:<\/p>\n<pre><code class=\"lang-auto\">for STUDY_NAME, study ...:\n    wandb_logger = WandbLogger(project=\"Scenario\",  name=STUDY_NAME+'_EVAL_'+str(study.best_params['lr']),\ngroup='Scenario Eval',\n log_model=\"all\",\n reinit=True)\n            trainer = pl.Trainer(\n                stochastic_weight_avg=False,\n                logger=wandb_logger,\n                checkpoint_callback=False,\n                log_every_n_steps=10,\n                default_root_dir = cache_path,\n                max_epochs=N_EPOCHS_EVAL,\n                gpus=1 if torch.cuda.is_available() else None,\n            )\n            wandb_logger.watch(model)\n            trainer.fit(model, datamodule=datamodule)\n            trainer.logger.log_hyperparams(hyperparameters)\n<\/code><\/pre>\n<p>In Wandb everything shows up as a single run, despite multiple loggers being created in python with explicit \u201creinit=True\u201d.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d1e8a5e923f3641d8ba225411a83542eece543f7.png\" data-download-href=\"\/uploads\/short-url\/tWWeJa1q9uMmt5B1QFE3PPNWdHV.png?dl=1\" title=\"W&amp;amp;B Chart 26.10.2021, 19_30_41\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1e8a5e923f3641d8ba225411a83542eece543f7_2_690x345.png\" alt=\"W&amp;B Chart 26.10.2021, 19_30_41\" data-base62-sha1=\"tWWeJa1q9uMmt5B1QFE3PPNWdHV\" width=\"690\" height=\"345\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1e8a5e923f3641d8ba225411a83542eece543f7_2_690x345.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1e8a5e923f3641d8ba225411a83542eece543f7_2_1035x517.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1e8a5e923f3641d8ba225411a83542eece543f7_2_1380x690.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1e8a5e923f3641d8ba225411a83542eece543f7_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 26.10.2021, 19_30_41<\/span><span class=\"informations\">2400\u00d71200 225 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nDo I need to do something else to separate these runs?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Uploading basic data only once with wandb",
        "Question_creation_time":1642410731020,
        "Question_link":"https:\/\/community.wandb.ai\/t\/uploading-basic-data-only-once-with-wandb\/1770",
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi guys,<br>\nnot sure if this is the right place to ask, but i\u2019m trying to figure out how i can upload an \u201cartifact\u201d only once for my project.<br>\nI would like to upload a plot of my data to visualize some basic information. Adding it with wandb.log will upload this information on each run. Artifacts seem to be for data that should be versioned, which also doesn\u2019t seem to fit my usecase very well.<\/p>\n<p>I\u2019ve also tried uploading the dataset and then plotting it using the table information, but wandb seems to struggle with pandas datetimeindex, so i\u2019m not very happy with that solution either.<\/p>\n<p>Whats the proper way to go about doing this?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Only a subset of the artifacts exist issue",
        "Question_creation_time":1639917855017,
        "Question_link":"https:\/\/community.wandb.ai\/t\/only-a-subset-of-the-artifacts-exist-issue\/1569",
        "Question_upvote_count":0.0,
        "Question_view_count":256.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI generated different 7 cells datasets.<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/chen-brestel\/cells_dataset\/runs\/z5lo24hn\/overview?workspace=user-chen-brestel\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/chen-brestel\/cells_dataset\/runs\/z5lo24hn\/overview?workspace=user-chen-brestel\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/s.gravatar.com\/avatar\/984d35ad02a6fb4388e6f91a95a0e805?s=480&amp;r=pg&amp;d=https%3A%2F%2Fcdn.auth0.com%2Favatars%2Fch.png\" class=\"thumbnail onebox-avatar\" width=\"120\" height=\"120\">\n\n<h3><a href=\"https:\/\/wandb.ai\/chen-brestel\/cells_dataset\/runs\/z5lo24hn\/overview?workspace=user-chen-brestel\" target=\"_blank\" rel=\"noopener\">chen-brestel<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>The pkl data of <em>all<\/em> the seven datasets do exist.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a8c17350da9dea9b06094ba90955d0690f3d8bf0.png\" data-download-href=\"\/uploads\/short-url\/o4SGcGfGg8KdPBsRnNdwBIRYPVC.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a8c17350da9dea9b06094ba90955d0690f3d8bf0_2_690x487.png\" alt=\"image\" data-base62-sha1=\"o4SGcGfGg8KdPBsRnNdwBIRYPVC\" width=\"690\" height=\"487\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a8c17350da9dea9b06094ba90955d0690f3d8bf0_2_690x487.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a8c17350da9dea9b06094ba90955d0690f3d8bf0_2_1035x730.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a8c17350da9dea9b06094ba90955d0690f3d8bf0_2_1380x974.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a8c17350da9dea9b06094ba90955d0690f3d8bf0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1389\u00d7981 52.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>However, There exist artifacts for <em>only<\/em> 3\/7.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/bb3ef6d248c7add270d8c59a4932fe6ae2229787.png\" data-download-href=\"\/uploads\/short-url\/qIsbAXGhKr2J7Gt2hOwV2tE1l9J.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/bb3ef6d248c7add270d8c59a4932fe6ae2229787_2_690x489.png\" alt=\"image\" data-base62-sha1=\"qIsbAXGhKr2J7Gt2hOwV2tE1l9J\" width=\"690\" height=\"489\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/bb3ef6d248c7add270d8c59a4932fe6ae2229787_2_690x489.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/bb3ef6d248c7add270d8c59a4932fe6ae2229787_2_1035x733.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/bb3ef6d248c7add270d8c59a4932fe6ae2229787.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/bb3ef6d248c7add270d8c59a4932fe6ae2229787_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1360\u00d7965 59.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Screenshots attached.<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Runs not listed in workspace",
        "Question_creation_time":1654867264003,
        "Question_link":"https:\/\/community.wandb.ai\/t\/runs-not-listed-in-workspace\/2593",
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In my workspace (on the left) runs are not listed, although they appear in the plots.<\/p>\n<p>I tried turning filters on and off but this didn\u2019t change anything.<\/p>\n<p>When I switch to the table view all the runs are there.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/01aa72c7785a58ce3e6d6471225787ec7e235db8.jpeg\" data-download-href=\"\/uploads\/short-url\/eJF0nB5WgI7JkvFgmKGP6rnRJ6.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/01aa72c7785a58ce3e6d6471225787ec7e235db8_2_690x268.jpeg\" alt=\"image\" data-base62-sha1=\"eJF0nB5WgI7JkvFgmKGP6rnRJ6\" width=\"690\" height=\"268\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/01aa72c7785a58ce3e6d6471225787ec7e235db8_2_690x268.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/01aa72c7785a58ce3e6d6471225787ec7e235db8_2_1035x402.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/01aa72c7785a58ce3e6d6471225787ec7e235db8_2_1380x536.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/01aa72c7785a58ce3e6d6471225787ec7e235db8_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1920\u00d7746 70.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Resources on how to use wandb docker",
        "Question_creation_time":1640217844715,
        "Question_link":"https:\/\/community.wandb.ai\/t\/resources-on-how-to-use-wandb-docker\/1596",
        "Question_upvote_count":2.0,
        "Question_view_count":410.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Greetings:<\/p>\n<p>After lots of research, I decided to go with wandb as the solution to several of the project management organization currently in place (or lack thereof).<\/p>\n<p>With that, I am looking to acquire the most effective workflow using <code>wandb docker<\/code> and <code>wandb local<\/code>.<\/p>\n<p>There is a page on this in the documents, and another in Github, but both are brief, and do not provide much information. All other documentation appears quite impressive (a major factor to me choosing this over the many other solutions). Whether featured by the company or a blog done by a third party, any good references to set up docker with wandb? Eventually, we will be spanning many parts of the data science pipeline (i.e., this is me doing a trial for a group at a company). So figured best practices and an efficient work environment should be set up first. Then, to start playing around with the outputs from the container runs to the project space (dashboard).<\/p>\n<p>Any pointers, references, samples projects, or any other material that might be out there?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot join team via invite",
        "Question_creation_time":1660223458030,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-join-team-via-invite\/2905",
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>I can\u2019t join team after accepting the invite, only receive message \u201cReceived an invite but still can\u2019t see the team? Make sure you are logged in with the email where you received the invite.\u201d, but I already logged with that email.<\/p>\n<p>Best regards,<br>\nBohdan<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb process not getting terminated properly",
        "Question_creation_time":1635600298677,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-process-not-getting-terminated-properly\/1166",
        "Question_upvote_count":0.0,
        "Question_view_count":264.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>My process is not getting terminated properly (running in a multi-GPU setting). It is trying to upload information but gets stuck for some reason. I am facing this problem since yesterday, and haven\u2019t made any changes to the version of the library (although this didn\u2019t get resolved after upgrading the library to the latest version). Any help will be highly appreciated. I can disable wandb completely by passing <code>mode = \"disabled\"<\/code> in the test setting, but need it while running sweeps or logging training metrics.<br>\nP.S.: Same code was running just fine till yesterday.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d.png\" data-download-href=\"\/uploads\/short-url\/7rETgzwUz7nuCquITds6NyYKXVj.png?dl=1\" title=\"Screenshot 2021-10-30 at 6.52.11 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_690x69.png\" alt=\"Screenshot 2021-10-30 at 6.52.11 PM\" data-base62-sha1=\"7rETgzwUz7nuCquITds6NyYKXVj\" width=\"690\" height=\"69\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_690x69.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_1035x103.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/342fa335656c0535a9ca21307507f30ef53feb1d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2021-10-30 at 6.52.11 PM<\/span><span class=\"informations\">1048\u00d7106 27.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to reenable automatic synchronisation",
        "Question_creation_time":1662254971968,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-reenable-automatic-synchronisation\/3061",
        "Question_upvote_count":0.0,
        "Question_view_count":72.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I made a change to my script and now I have to manually synchronise my runs, my script contains<\/p>\n<pre><code>if args.dry_run:\n    os.environ['WANDB_MODE'] = 'dryrun'\n\nwandb.init(project=args.project_name, notes=args.notes)\n\n# log all experimental args to wandb\nwandb.config.update(args)\n<\/code><\/pre>\n<p>The change I made was the first line, setting <code>WANDB_MODE=dryrun<\/code>. From that point on I cannot re-enable automatic synchronisation.<\/p>\n<p>I\u2019ve run <code>wandb online<\/code> and run my script with <code>dryrun=False<\/code>. I also realised that this doesn\u2019t unset WANDB_MODE so I tried setting it to \u2018online\u2019 when <code>dryrun==False<\/code>. But it always ends up logging to <code>wandb\/offline-run-*<\/code> and I have to manually sync it.<\/p>\n<p>Is there another step to re-enable sync\u2019ing?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to plot multiline in one plot with smoothing features?",
        "Question_creation_time":1639275305880,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-plot-multiline-in-one-plot-with-smoothing-features\/1512",
        "Question_upvote_count":0.0,
        "Question_view_count":239.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I\u2019m trying to plot the figure as in [W&amp;B Smoothing Features], but it didn\u2019t provide any code:<\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" width=\"690\" height=\"301\">\n          <\/a>\n\n<\/p>\n<p>Tutorials I could find by searching <code>wandb multiline in one plot<\/code> is [Custom Multi-Line Plots] which introduces <code>wandb.plot.line_series()<\/code>.  So I tried the code following<\/p>\n<pre><code class=\"lang-python\">import wandb\nimport numpy as np\n\n\nwandb.init(project=\"test\", entity=\"xxxx\")\n\nwandb.log({\"my_custom_id\":\n           wandb.plot.line_series(\n               xs=range(100),\n               ys=[range(100), np.random.randint(100, size=100)],\n               keys=[\"y1\", \"y2\"],\n               title=\"Multiline\",\n               xname=\"steps\"\n           )})\n<\/code><\/pre>\n<p>It gives me the following pic after choosing <code>Edit panel<\/code><\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" width=\"690\" height=\"351\">\n          <\/a>\n\n<\/p>\n<p>Unlike the first picture:<\/p>\n<ol>\n<li>It <strong>doesn\u2019t<\/strong> have <code>Data<\/code>, <code>Group<\/code> etc tabs.<\/li>\n<li>There are <strong>two types<\/strong> of legend <code>name<\/code> and <code>lineKey<\/code> rather than one type.<\/li>\n<\/ol>\n<p>My question is how to plot exactly the same as the first picture with same function supported in wandb web?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"W&B Outage? 11\/1\/2022",
        "Question_creation_time":1667333754242,
        "Question_link":"https:\/\/community.wandb.ai\/t\/w-b-outage-11-1-2022\/3360",
        "Question_upvote_count":3.0,
        "Question_view_count":48.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I was wondering if anybody from the W&amp;B team can confirm that there is an outage at the moment.<\/p>\n<p>I\u2019ve been having issues starting runs and it seems like other folks are having issues syncing runs with a network time out error (<a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4424\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: canno't sync my runs \u00b7 Issue #4424 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>). It\u2019s been ongoing for about 2 hours now.<\/p>\n<p>The status page is saying everything is fine - <a href=\"https:\/\/status.wandb.com\" rel=\"noopener nofollow ugc\">https:\/\/status.wandb.com<\/a><\/p>\n<p>All the best,<br>\nAlexey<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Save multiindex dataframes",
        "Question_creation_time":1660296123520,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-multiindex-dataframes\/2913",
        "Question_upvote_count":0.0,
        "Question_view_count":86.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is it possible to log a multiindex pandas dataframe?<\/p>\n<p>In addition, is it possible to save a pandas dataframe with the names of the rows? Even though my dataframe has names in the rows, in the UI I see a linear index.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Help Adding To Tacotron Model",
        "Question_creation_time":1641081887756,
        "Question_link":"https:\/\/community.wandb.ai\/t\/help-adding-to-tacotron-model\/1660",
        "Question_upvote_count":0.0,
        "Question_view_count":193.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p><strong>Hello, can someone please help Me get this set up?<\/strong><br>\nI am running this in <strong>Google Colab<\/strong><\/p>\n<p>I am unable to get any data from my tacotron model.  I was able to login\/create multiple wandb runs and it tracks usage(log is connected, Utilization updates) but no data has been entered and the way it trains it runs one line forever so i dont even know how to begin.<\/p>\n<p>This is the Script that currently runs the training<\/p>\n<pre><code class=\"lang-auto\">print('FP16 Run:', hparams.fp16_run)\nprint('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\nprint('Distributed Run:', hparams.distributed_run)\nprint('cuDNN Enabled:', hparams.cudnn_enabled)\nprint('cuDNN Benchmark:', hparams.cudnn_benchmark)\n\nfrom IPython.display import Javascript\ndisplay(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n#for i in range(200):\n#  print(i)\n\ntrain(output_directory, log_directory, checkpoint_path,\n      warm_start, n_gpus, rank, group_name, hparams, log_directory2)\n\n<\/code><\/pre>\n<p><strong>I tried doing this but it didnt work either. here is my code to start the training with wandb<\/strong><\/p>\n<p>Install and login<\/p>\n<pre><code class=\"lang-auto\">#@markdown Login and start a new run\nprint('Installing wandb')\n!pip -q install wandb\nimport wandb\nprint('Login To wanb!!!\\n')\n!wandb login\n<\/code><\/pre>\n<p>Capture a dictionary of hyperparameters<\/p>\n<pre><code class=\"lang-auto\">#@markdown Capture a dictionary of hyperparameters\nwandb.config.p_attention_dropout=hparams.p_attention_dropout\nwandb.config.p_decoder_dropout=hparams.p_decoder_dropout\nwandb.config.decay_start=hparams.decay_start\nwandb.config.A_=hparams.A_\nwandb.config.B_=hparams.B_\nwandb.config.C_=hparams.C_\nwandb.config.min_learning_rate=hparams.min_learning_rate\nwandb.config.batch_size=hparams.batch_size\nwandb.config.epochs=hparams.epochs\nwandb.config.generate_mels=generate_mels\nwandb.config.show_alignments=hparams.show_alignments\nwandb.config.alignment_graph_height=alignment_graph_height\nwandb.config.alignment_graph_width=alignment_graph_width\nwandb.config.load_mel_from_disk=hparams.load_mel_from_disk\nwandb.config.ignore_layers=hparams.ignore_layers\nwandb.config.checkpoint_path=checkpoint_path\n<\/code><\/pre>\n<p>Start wanb and get runID<br>\n<code>wandb.init(project=\"tacotron\", entity=\"gmirsky2\")<\/code><\/p>\n<p><strong>start wandb run then Train<\/strong><\/p>\n<pre><code class=\"lang-auto\">#Run\napi = wandb.Api()\nrun = api.run(\"gmirsky2\/tacotron\/\" + wandb.run.id)\n\n#train\ntrain(output_directory, log_directory, checkpoint_path,\n      warm_start, n_gpus, rank, group_name, hparams, log_directory2)\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.history()\nmetrics_dataframe.to_csv(\"metrics.csv\")\n<\/code><\/pre>\n<p>When The Training Runs it Just goes to the train line and then never finishes.<br>\nI am looking for help with how to incorporate wandb with the tacotron train script\u2026<\/p>\n<pre><code class=\"lang-auto\">train(output_directory, log_directory, checkpoint_path,\n      warm_start, n_gpus, rank, group_name, hparams, log_directory2)\n<\/code><\/pre>\n<p>I thought that was what the hyperparameters were for but i guess im wrong.<\/p>\n<p>Any help would be welcome. Thanks a bunch!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Log_code not working with hydra",
        "Question_creation_time":1666091221890,
        "Question_link":"https:\/\/community.wandb.ai\/t\/log-code-not-working-with-hydra\/3276",
        "Question_upvote_count":0.0,
        "Question_view_count":641.0,
        "Question_answer_count":13,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello to all,<\/p>\n<p>I am new here.  I would like to  save my files to the wandb experiment .<br>\nThis was also working before I used hydra. Since Hydra is changing the run dir.<br>\nI also adapted the  wandb.run.log_code(root=) to the where files are.<br>\nStill not working.<br>\nHas someone an Idea How to fix it<\/p>\n<p>Thank you very much<br>\nBest regards<br>\nChris<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Axis scales",
        "Question_creation_time":1660153441068,
        "Question_link":"https:\/\/community.wandb.ai\/t\/axis-scales\/2892",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I have created 100 runs, and would like to make two scatter plots. The first scatter plot involves the first 50 simulations, with axis limits [0,10] in both directions. The second scatter plot uses simulations 51 to 100, with different axis limits, say [10,20]. So far, I created a new panel, for both these plots. But wandb does not like that. Whatever I set the axis limits will be the same for both subsets (1-50, and 51-100). What is the recommended approach to have a plot for each of the data subsets? Must I create two different panels? If so, that means that one panel 2 might have to be turned off for the first batch of data experiments, and panel 1 would be turned off for the second batch of experiments. Is this the recommended approach? Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweeps while using MPI and SLURM",
        "Question_creation_time":1652695453529,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweeps-while-using-mpi-and-slurm\/2427",
        "Question_upvote_count":0.0,
        "Question_view_count":400.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! I am attempting to perform a hyperparameter search on my project, which uses MPI under the hood to aggregate the results of multiple agents. I have 63 agents that run an episode, returning a total reward at the end. At the end, each worker node sends their results to the main node, which logs the total reward of every 5th training run.<\/p>\n<p>I have tried to create a sweep with a custom command to use <code>mpirun<\/code>(as seen below) and running <code>wandb agent sweepid --count 1<\/code> in the SLURM script.  This results in using all the cores of the machine to start a sweep, effectively blocking my other agents from training.<\/p>\n<pre><code class=\"lang-auto\">program: src.sweep_mpi \ncommand:\n  - mpirun\n  - \"--mca\" \n  - opal_warn_on_missing_libcuda\n  - 0\n  - python\n  - \"-m\"\n  - ${program}\n  - ${args}\n<\/code><\/pre>\n<p>Next, I have tried setting up the sweeping agent inside the python code with a local controller, but this also led to issues regarding the parallelization. Currently, I need to initialize wandb using <code>settings=wandb.Settings(start_method=\"fork\")<\/code>, but I cannot find any way to specify this as a sweep parameter. Therefore, each  run crashes since it is not using the correct parallelization procedure.<\/p>\n<p>Is there anything I can do in this case? Or should I implement my own parameter search?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Radar Plot",
        "Question_creation_time":1653015157992,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-radar-plot\/2455",
        "Question_upvote_count":0.0,
        "Question_view_count":47.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019ve been trying to set up a radar plot using the custom charts feature, but I\u2019m stuck!<\/p>\n<p>Ideally, I want to be able to display seven different summary metrics on the radar plot, and overlay multiple runs.<br>\nI\u2019ve found an example vega chart that would work perfectly (<a href=\"https:\/\/vega.github.io\/vega\/examples\/radar-chart\/\" rel=\"noopener nofollow ugc\">https:\/\/vega.github.io\/vega\/examples\/radar-chart\/<\/a>), however I cannot figure out how to integrate the summary metrics into it.<\/p>\n<p>I\u2019m sure this is just coming from a lack of experience, so any help would be much appreciated!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR Abnormal program exit",
        "Question_creation_time":1669091571015,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-abnormal-program-exit\/3448",
        "Question_upvote_count":0.0,
        "Question_view_count":5.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am using wandb version 0.13.5 on python 3.6.9 (Linux kernel version: 4.14.281-212.502.amzn2.x86_64). I have a problem running <code>wandb.init()<\/code> with the following error.<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py\u201d, line 1075, in init<br>\nwi.setup(kwargs)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py\u201d, line 165, in setup<br>\nself._wl = wandb_setup.setup()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 312, in setup<br>\nret = _setup(settings=settings)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 307, in _setup<br>\nwl = _WandbSetup(settings=settings)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 293, in <strong>init<\/strong><br>\n_WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 106, in <strong>init<\/strong><br>\nself._setup()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 234, in _setup<br>\nself._setup_manager()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py\u201d, line 266, in _setup_manager<br>\n_use_grpc=use_grpc, settings=self._settings<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_manager.py\u201d, line 108, in <strong>init<\/strong><br>\nself._service.start()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py\u201d, line 112, in start<br>\nself._launch_server()<br>\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py\u201d, line 108, in _launch_server<br>\nassert ports_found<br>\nAssertionError<br>\nwandb: ERROR Abnormal program exit<br>\nproc exited with 1<\/p>\n<\/blockquote>\n<blockquote>\n<hr>\n<p>AssertionError                            Traceback (most recent call last)<br>\n\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)<br>\n1074         wi = _WandbInit()<br>\n \u2192 1075         wi.setup(kwargs)<br>\n1076         except_exit = wi.settings._except_exit<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py in setup(self, kwargs)<br>\n164<br>\n \u2192 165         self._wl = wandb_setup.setup()<br>\n166         # Make sure we have a logger setup (might be an early logger)<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in setup(settings)<br>\n311 def setup(settings=None) \u2192 Optional[\u201c_WandbSetup\u201d]:<br>\n \u2192 312     ret = _setup(settings=settings)<br>\n313     return ret<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in _setup(settings, _reset)<br>\n306         return<br>\n \u2192 307     wl = _WandbSetup(settings=settings)<br>\n308     return wl<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in <strong>init<\/strong>(self, settings)<br>\n292             return<br>\n \u2192 293         _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)<br>\n294<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in <strong>init<\/strong>(self, pid, settings, environ)<br>\n105         self._check()<br>\n \u2192 106         self._setup()<br>\n107<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in _setup(self)<br>\n233     def _setup(self):<br>\n \u2192 234         self._setup_manager()<br>\n235<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_setup.py in _setup_manager(self)<br>\n265         self._manager = wandb_manager._Manager(<br>\n \u2192 266             _use_grpc=use_grpc, settings=self._settings<br>\n267         )<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_manager.py in <strong>init<\/strong>(self, settings, _use_grpc)<br>\n107         if not token:<br>\n \u2192 108             self._service.start()<br>\n109             host = \u201clocalhost\u201d<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py in start(self)<br>\n111     def start(self) \u2192 None:<br>\n \u2192 112         self._launch_server()<br>\n113<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/service\/service.py in _launch_server(self)<br>\n107             ports_found = self._wait_for_ports(fname, proc=internal_proc)<br>\n \u2192 108             assert ports_found<br>\n109             self._internal_proc = internal_proc<\/p>\n<p>AssertionError:<\/p>\n<p>The above exception was the direct cause of the following exception:<\/p>\n<p>Exception                                 Traceback (most recent call last)<br>\n in <br>\n1 import wandb<br>\n----&gt; 2 wandb.init()<\/p>\n<p>\/usr\/local\/lib\/python3.6\/dist-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)<br>\n1114             if except_exit:<br>\n1115                 os._exit(-1)<br>\n \u2192 1116             raise Exception(\u201cproblem\u201d) from error_seen<br>\n1117     return run<\/p>\n<p>Exception: problem<\/p>\n<\/blockquote>\n<p>I tried downgrading the wandb version to 0.9.7 but the problem still the same. Could you please help me solve this error?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"View-only report will still expose whole project",
        "Question_creation_time":1630504156357,
        "Question_link":"https:\/\/community.wandb.ai\/t\/view-only-report-will-still-expose-whole-project\/387",
        "Question_upvote_count":6.0,
        "Question_view_count":313.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I add a report to a private project named \u201csomething\u201d and create a view-only link to this report. By clicking on the link, you can see a report. This is expected. However, by clicking the project name displayed in the navigator, you will see all information about the project, even some runs that are not expected to expose.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to make select group use a different x axis",
        "Question_creation_time":1643048912031,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-make-select-group-use-a-different-x-axis\/1813",
        "Question_upvote_count":0.0,
        "Question_view_count":111.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In my project, one of the network is larger than the others and has to use a smaller batch size. To keep the comparison fair, I used a 2x smaller batch size but accumulate gradients for 2 batches, which is theoretically the same amount of gradient updates as long as I keep the number of epochs 2x larger than the normal models. However, in the plots that are being tracked, the bigger model will look like it learns twice as slow as the normal model. This is expected, but to simulate the effect training with the same batch size, I need to shrink the x axis of the big model\u2019s learning curve.<\/p>\n<p>How would I do that? There is an expression column that I can use when editing the panels, but this seems to only work for all the curves in the plot, not selectively.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging with Tensorboard",
        "Question_creation_time":1665946757447,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-with-tensorboard\/3265",
        "Question_upvote_count":0.0,
        "Question_view_count":47.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I am trying to run the demo code from <a href=\"https:\/\/colab.research.google.com\/gist\/sayakpaul\/5b31ed03725cc6ae2af41848d4acee45\/demo_tensorboard.ipynb\" rel=\"noopener nofollow ugc\">Demo_tensorboard.ipynb<\/a> so that I can learn more about the use of Tensorboard in combination with W&amp;B. Unfortunately this code throws this warning:<\/p>\n<p>WARNING When using several event log directories, please call <code>wandb.tensorboard.patch(root_logdir=\"...\")<\/code> before <code>wandb.init<\/code><\/p>\n<p>When I implement the suggested change with:<\/p>\n<p><code>wandb.tensorboard.patch(root_logdir=\".\/logs\/debug\")<\/code><\/p>\n<p>I get the following warning:<br>\nFound log directory outside of given root_logdir, dropping given root_logdir for event file in i:\\tinyml\\tiny_cnn\\wandb\\run-20221016_205607-22b9tlzf\\files\\train<\/p>\n<p>So my questions is: What is a suitable root_logdir for Tensorboard?<\/p>\n<p>Thanks for your support.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ConnectionRefusedError",
        "Question_creation_time":1664460602042,
        "Question_link":"https:\/\/community.wandb.ai\/t\/connectionrefusederror\/3200",
        "Question_upvote_count":0.0,
        "Question_view_count":236.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI used colab to run the code,  it succeeded the first time, but failed the third time, why did it like this? what should I do? Thanks!<br>\nException has occurred: ConnectionRefusedError<br>\n[Errno 111] Connection refused<br>\nFile \u201c\/content\/OPTIM\/0_template_graph_node_classify copy\/agent_sweep.py\u201d, line 75, in <br>\nsweep_id = wandb.sweep(sweep_config, project=\u201cvscode_debug\u201d)<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Separating Training From Testing",
        "Question_creation_time":1630322356926,
        "Question_link":"https:\/\/community.wandb.ai\/t\/separating-training-from-testing\/353",
        "Question_upvote_count":0.0,
        "Question_view_count":279.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>I\u2019ve got a sort of lopsided ML workflow where most of my time is spent producing plots and metrics, and my set of trained models is rarely updated. Is there a good way to separate my training code from all of my evaluation scripts?<\/p>\n<p>I\u2019d like my experiments to have the logging information from when the model was trained, but not to have to retrain it every time.<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Saving Model Class Source, including forward() method",
        "Question_creation_time":1640171653390,
        "Question_link":"https:\/\/community.wandb.ai\/t\/saving-model-class-source-including-forward-method\/1592",
        "Question_upvote_count":6.0,
        "Question_view_count":194.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey folks,<\/p>\n<p>I couldn\u2019t find the best practice when it comes to saving the model definition to <code>wandb<\/code>, including the forward call.<\/p>\n<p>Most of my research is done by changing the forward function, so it is an important piece of data I want to track.<\/p>\n<p>I tried using <code>inspect.getsource(class)<\/code> however, there seems to be an issue with using it in IPython.<\/p>\n<p>I am aware that I can save the whole notebook \/ file, but this means a lot of auxiliary information is also saved which makes it hard to compare just the models.<\/p>\n<p>Please let me know how you would approach this issue.<\/p>\n<p>thank you very much and enjoy life,<br>\nbatu<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Account deletion request",
        "Question_creation_time":1650983296869,
        "Question_link":"https:\/\/community.wandb.ai\/t\/account-deletion-request\/2322",
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Can you delete my account please? username realdionysus<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom settings for wandb.Object3D",
        "Question_creation_time":1651459164070,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-settings-for-wandb-object3d\/2351",
        "Question_upvote_count":0.0,
        "Question_view_count":130.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I wonder if <a class=\"mention-group notify\" href=\"\/groups\/team\">@team<\/a> can add some custom seetings for wandb.Object3D.<br>\nAf first I tried to use Plotly to achieve custom 3D point cloud visualization, but I saw team says Plotly is not supported now in github issue.<br>\nFor example, point size, backgorund color, etc.<br>\nIt would be really nice for 3D task.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I add a table to a run after it has completed via the API?",
        "Question_creation_time":1651088864988,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-add-a-table-to-a-run-after-it-has-completed-via-the-api\/2334",
        "Question_upvote_count":0.0,
        "Question_view_count":89.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I would like to log a table to a wandb run, like shown here: <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/tables-quickstart\">https:\/\/docs.wandb.ai\/guides\/data-vis\/tables-quickstart<\/a><\/p>\n<p>The table will contain information about the performance of an RL agent in environments which differ from its training environment. I want to add the table to the wandb created during the training of the RL agent. Is this possible?<\/p>\n<p>Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb login issue",
        "Question_creation_time":1661773970926,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-login-issue\/3023",
        "Question_upvote_count":0.0,
        "Question_view_count":124.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, everytime I run my experiment in kubernetes, wandb is asking for options wandb:<br>\n(1) Create a W&amp;B account<br>\nwandb: (2) Use an existing W&amp;B account<br>\nwandb: (3) Don\u2019t visualize my results<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Calling run.history(samples=n_samples) returns a sample size different from n_samples",
        "Question_creation_time":1668436042567,
        "Question_link":"https:\/\/community.wandb.ai\/t\/calling-run-history-samples-n-samples-returns-a-sample-size-different-from-n-samples\/3414",
        "Question_upvote_count":0.0,
        "Question_view_count":22.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi everyone!<\/p>\n<p>I am experiencing weird behaviour of the run.history() function in Python:<\/p>\n<p>Calling <code>run.history(samples = 100)<\/code> gives me sample sizes different to 100 and the sample size varies for each call. E.g. executing it 5 times gave me sample sizes 98, 90, 88, 110, 104.<br>\nHowever, when I execute <code>run.history(keys=['my_key'], samples=100)<\/code>, I get a sample size of exactly 100 for every call. Why is this the case?<\/p>\n<p>After investigating this further, I found more strange behaviour: Calling <code>run.history(keys=['my_key'], samples=n_samples)<\/code> yields a sample size of exactly n_samples, as long as n_samples &lt;= 12493 (at least for my test run). If n_samples &gt; 12493, smaller sample sizes (varying roughly  between 12400 and 12490) are returned.<\/p>\n<p>Am I understanding something wrong or are these functions behaving in a way that they shouldn\u2019t?<\/p>\n<p>Thanks in advance!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb not logging git commit\/hash",
        "Question_creation_time":1649357269284,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-logging-git-commit-hash\/2197",
        "Question_upvote_count":0.0,
        "Question_view_count":141.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>When I first used wandb, I did find that wandb automatically logs the git commit hash when I ran experiments. However, recently, I found that it stops doing so. Namely, I cannot find any git information on the \u201coverview\u201d page of each experiment. Does anyone have some guesses on what might be the cause? Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb.Html() not displaying",
        "Question_creation_time":1666068919749,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-html-not-displaying\/3271",
        "Question_upvote_count":0.0,
        "Question_view_count":163.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m trying to display a block of text (a confusion matrix actually, since they do not display in WB) using HTML.<br>\nThe code runs but it will not show up on run panel.  How can i see it?  It seems only images will display.<\/p>\n<pre><code class=\"lang-auto\">wandb.log({f\"ConfMatrix\" : wandb.Html(\"&lt;tt&gt;\"+my_confusion_matrix.ai2_confusion_matrix(y_true, y_pred...).replace(\"\\n\", \"&lt;P&gt;\").replace(\" \", \"&amp;nbsp;\"))})\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"WandB icevision not showing prediction",
        "Question_creation_time":1658141609792,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-icevision-not-showing-prediction\/2767",
        "Question_upvote_count":0.0,
        "Question_view_count":108.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello everyone,<\/p>\n<p>I am using WanbB in IceVision through the fastai integration, you could have a look here for <a href=\"https:\/\/airctic.com\/0.12.0\/wandb_efficientdet\/\" rel=\"noopener nofollow ugc\">IceVision WandB<\/a> but i get the following message:<\/p>\n<pre><code class=\"lang-auto\">Could not gather input dimensions\nWandbCallback was not able to prepare a DataLoader for logging prediction samples -&gt; 'Dataset' object has no attribute 'items'\n<\/code><\/pre>\n<p>Any help appreciated.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Point Clouds no longer 3D-viewable?",
        "Question_creation_time":1658187293727,
        "Question_link":"https:\/\/community.wandb.ai\/t\/point-clouds-no-longer-3d-viewable\/2770",
        "Question_upvote_count":1.0,
        "Question_view_count":71.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve been using the WandB 3D point cloud feature for a few months now to view my model\u2019s embeddings in 3D.  But recently my collaborators and I have notice that this feature seems to have been disabled\u2026?<br>\nNow when we mouse over a point cloud image, instead of getting the \u201cX\u201d in the top right in order to expand and going into 3D mode\u2026 nothing happens.<br>\n(How) Can we get 3D viz back for point clouds? This was an important feature.<\/p>\n<p>To reproduce: go to any WandB page for point cloud that used to be viewable in 3D, whether in documentation or in your own runs.  You will see that there is no longer a way to make it 3D.<\/p>\n<p>UPDATE: I notice that also <a href=\"https:\/\/wandb.ai\/stacey\/alphafold?workspace=user-stacey\">\u201c3D Molecules\u201d<\/a> are also no longer offered in 3D, rather only as static images.<\/p>\n<p>UPDATE 2: Yea I\u2019m thinking this is an unintentional bug.  Your <a href=\"https:\/\/wandb.ai\/nbaryd\/SparseConvNet-examples_3d_segmentation\/reports\/Point-Clouds--Vmlldzo4ODcyMA\">\u201c3D Objects Live Example\u201d<\/a> also no longer is 3D-viewable.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to build a sweeps model for different numbers of hidden layers?",
        "Question_creation_time":1651579869475,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-build-a-sweeps-model-for-different-numbers-of-hidden-layers\/2363",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am making a simple network in PyTorch with linear units as a practise project. I\u2019d like to use sweeps to find the best hyper parameters for the network. Some of these hyperparameters include batch_norm, dropout value, number of hidden layers, number of units in each hidden layer.<\/p>\n<p>I can\u2019t figure out how to set up the model and sweep config so that two different model structures can be swept without being confusing. For example, I want to use batch_norm OR have dropout values of <code>[0, 0.2, 0.4, 0.5]<\/code>. I never want <code>batch_norm<\/code> AND <code>dropout<\/code> to be used. If I use random search with wandb, it may choose both <code>0.4<\/code> dropout AND <code>batch_norm<\/code> which I don\u2019t want.<\/p>\n<p>I know how to set up the network class with simple if statements so it adds either <code>batch_norm<\/code> or <code>dropout<\/code>, but the <code>wandb.config<\/code> would still select a value for <code>dropout<\/code> and a boolean for <code>batch_norm<\/code>, and I don\u2019t want the sweep report to show both these parameters if the network only uses one.<\/p>\n<p>Another example is, I\u2019d like 2, 3, 4 or 5 hidden layers. I\u2019d also like each layer to have a randomly selected  number of neurons from the range <code>[64, 128, 256, 512]<\/code>.<\/p>\n<p>I can forsee a problem where wandb will select the model to have 3 hidden layers but also pick say, 256 neurons for the 4th or 5th layer which will be misleading on the sweep parameter graph.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb not compatible with Torch Script",
        "Question_creation_time":1646168187010,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-compatible-with-torch-script\/1997",
        "Question_upvote_count":0.0,
        "Question_view_count":88.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m using wandb logging in conjunction with pytorch + pytorch lightning, and it seems like some of the code in wandb makes it so I cannot JIT my model into torchscript. Here\u2019s the error<\/p>\n<pre><code class=\"lang-auto\">torch.jit.frontend.UnsupportedNodeError: Set aren't supported:\n  File \"\/home\/peter\/catkin_ws\/src\/venv\/lib\/python3.8\/site-packages\/wandb\/wandb_torch.py\", line 355\n        \n            # hook has been processed\n            self._graph_hooks -= {id(module)}\n                                 ~ &lt;--- HERE\n        \n            if not self._graph_hooks:\n<\/code><\/pre>\n<p>is there a workaround for this?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Permanently delete Artifact Versions",
        "Question_creation_time":1662366506516,
        "Question_link":"https:\/\/community.wandb.ai\/t\/permanently-delete-artifact-versions\/3065",
        "Question_upvote_count":1.0,
        "Question_view_count":105.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to delete Artifact Versions using the API, for instance with something like this:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api(overrides={\"entity\": entity, \"project\": project})\nartifact = api.artifact(name, type)\nartifact.delete()\n<\/code><\/pre>\n<p>The artifact is indeed deleted from the web UI, but then when I call <code>api.artifact_versions(type, name)<\/code> it is still present in the iterator.<\/p>\n<p>So, in order to get the list of all available artifacts, what I\u2019m currently doing is call the <code>api.artifact_versions()<\/code> method and then check if a specific version really exists by usiing <code>api.artifact<\/code> inside a try\/except clause (if the artifact was deleted, it will raise a <code>wandb.CommError<\/code> exception), but of course this solution is more expensive.<\/p>\n<p>My question is: is there a way to permanently delete artifact versions so that they are not shown in <code>api.artifact_versions()<\/code> anymore? This would also help keep the version number from growing too high<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep on remote cluster GPUs",
        "Question_creation_time":1657388456681,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-on-remote-cluster-gpus\/2731",
        "Question_upvote_count":1.0,
        "Question_view_count":108.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey. I\u2019m trying to run a sweep on a cluster GPUs by submitting it as a new job.<br>\nThe problem is that the job runs, but keep logging a network error:<\/p>\n<blockquote>\n<p>wandb: Network error (ConnectionError), entering retry loop.<\/p>\n<\/blockquote>\n<p>The script works fine if I\u2019m trying to run it \u201clocally\u201d in the cluster i.e. without submitting a GPU job.<br>\nMy intuition is that W&amp;B doesn\u2019t find my creds (.netrc file) on the node its running. So I was wondering if there is a way to directly pass my API key to the wandb.agent function, so that the script is independent of its execution environment?<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep - starting with a small project",
        "Question_creation_time":1647273883323,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-starting-with-a-small-project\/2075",
        "Question_upvote_count":0.0,
        "Question_view_count":95.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<br>\nI\u2019m new to W&amp;B and try to use sweep to find best parameters for MNIST with tf2.<\/p>\n<p>First, I ran sweep agent and I\u2019ve got this issue  that I don\u2019t understand where it comes from\u2026<br>\nAttributeError: module \u2018wandb\u2019 has no attribute \u2018init\u2019<br>\nIt doesn\u2019t appear when I 'm not using any agent.<\/p>\n<p>Second it\u2019s not clear to me if it\u2019s mandatory to put the hyperparameters as command line arguments. I\u2019m using a json file to fill the default values. I thought I would use this kind of file to configure the sweep.<\/p>\n<p>Where exactly do we have to run the agent? My script train.py is in a folder, source code in another, and my experiment in a third one. I would have like to put the sweep.yaml with my experiments.  Is there a way to put the script and the yaml file in a different folder?<\/p>\n<p>Thanks for your help<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Feature Request] W&B badge or shield for GitHub repositories",
        "Question_creation_time":1648914623994,
        "Question_link":"https:\/\/community.wandb.ai\/t\/feature-request-w-b-badge-or-shield-for-github-repositories\/2181",
        "Question_upvote_count":4.0,
        "Question_view_count":157.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I was wondering if it would be possible to have a simple W&amp;B\/wandb badge to display on GitHub repositories, meaning: \u201cThis repository supports experiment tracking with wandb\u201d.<\/p>\n<p>By badge, I mean like below. The official wandb client repository for example uses pypi, codecov and circleci badges.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png\" data-download-href=\"\/uploads\/short-url\/64tMj9Dw36m9P2OBKPlPRcyIuBq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_690x151.png\" alt=\"image\" data-base62-sha1=\"64tMj9Dw36m9P2OBKPlPRcyIuBq\" width=\"690\" height=\"151\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_690x151.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">775\u00d7170 36 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why is only the final logged value counted towards min or max of a metric?",
        "Question_creation_time":1654959967646,
        "Question_link":"https:\/\/community.wandb.ai\/t\/why-is-only-the-final-logged-value-counted-towards-min-or-max-of-a-metric\/2601",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I wanted to find the run with the best validation accuracy, however, I noticed that to calculate the maximum only the  last logged values are used.<\/p>\n<p>You can see this on the screenshot below - it says the run with the maximum validation accuracy is run \u201821-\u2026\u2019 (the one that stopped earlier), even though run \u201818-\u2026\u2019 had a higher value one epoch after the other ones end. The problem is that one epoch later run 18 dropped in accuracy again, so it doesn\u2019t have \u2018final maximum\u2019 accuracy\u2026<\/p>\n<p>This is a bit problematic for me - I\u2019m saving my models after each epoch, so I don\u2019t really care only about the last model - precisely to prevent such a problem where the accuracy would suddenly drop at the end.<\/p>\n<p>I think this is a bit similar to this post: <a href=\"https:\/\/community.wandb.ai\/t\/can-i-plot-the-value-of-a-metric-at-a-single-step\/1971\" class=\"inline-onebox\">Can I plot the value of a metric at a single step?<\/a><\/p>\n<p>Is this a feature, or is this a bug?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c54fbc738774c965e52b5c6cea75de46d5e889ff.png\" data-download-href=\"\/uploads\/short-url\/s9uTwyqB1GUIxqL7kJT4rjYgf8P.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c54fbc738774c965e52b5c6cea75de46d5e889ff_2_478x500.png\" alt=\"image\" data-base62-sha1=\"s9uTwyqB1GUIxqL7kJT4rjYgf8P\" width=\"478\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c54fbc738774c965e52b5c6cea75de46d5e889ff_2_478x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c54fbc738774c965e52b5c6cea75de46d5e889ff_2_717x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c54fbc738774c965e52b5c6cea75de46d5e889ff_2_956x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c54fbc738774c965e52b5c6cea75de46d5e889ff_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1094\u00d71144 56.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error while calling W&B API iinternal database error (<Response [500]>)",
        "Question_creation_time":1655361075016,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-while-calling-w-b-api-iinternal-database-error-response-500\/2624",
        "Question_upvote_count":0.0,
        "Question_view_count":409.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am running four experiments from the same system (a google cloud VM) and while one is running fine: three have frozen (no progress but program still active\/has not errored out). Curious if anyone knows how to fix this?<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/19876a837127852045cb70c484da66f5a3c28d52.png\" alt=\"image\" data-base62-sha1=\"3DQ3Zmd1Ly9oF4hn1gpJ6jJ9bB8\" width=\"681\" height=\"130\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get all the artifacts of a project from the API",
        "Question_creation_time":1635175139226,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-all-the-artifacts-of-a-project-from-the-api\/1088",
        "Question_upvote_count":4.0,
        "Question_view_count":250.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>How can I use the API to get all the artifacts of a project, possibly of a given type?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging temporal data \/ overriding single image",
        "Question_creation_time":1642965201882,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-temporal-data-overriding-single-image\/1809",
        "Question_upvote_count":0.0,
        "Question_view_count":147.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey, I have 1D data that I\u2019d like to show it at every step (think hidden state of a model) as a 2D image (x-axis would correspond to time\/epoch and y axis to # of neurons). Can I do this?<\/p>\n<p>If not, I can maintain the image myself and simply resend the data as an image, but then I\u2019d have to overwrite the image on the server. Can I do that?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training hangs with GPU Utilization 100% and wandb trying to sync",
        "Question_creation_time":1667517148231,
        "Question_link":"https:\/\/community.wandb.ai\/t\/training-hangs-with-gpu-utilization-100-and-wandb-trying-to-sync\/3376",
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve been trying to get wandb to work with pytorch lightning on multiple gpus, it works fine, in the sense that the model is being trained, and metrics are being reported properly to the dashboard; however, only after a couple of hours and sometimes 20 mins, the system is maxed to use all the resources, causing the whole training process to freeze without any progress. I used <code>py-spy<\/code> to generate the following dumps<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b7f9aeb6b00a64f0949160291dd29702c3f2a805.png\" data-download-href=\"\/uploads\/short-url\/qfwjNuMRxJcCiiUOe6uuHTM1BLT.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b7f9aeb6b00a64f0949160291dd29702c3f2a805_2_690x260.png\" alt=\"image\" data-base62-sha1=\"qfwjNuMRxJcCiiUOe6uuHTM1BLT\" width=\"690\" height=\"260\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b7f9aeb6b00a64f0949160291dd29702c3f2a805_2_690x260.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b7f9aeb6b00a64f0949160291dd29702c3f2a805_2_1035x390.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b7f9aeb6b00a64f0949160291dd29702c3f2a805.png 2x\" data-dominant-color=\"EAE7D7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1282\u00d7484 70.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Hopefully, they\u2019d be helpful to figure out where is the issue.<\/p>\n<p>Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does wandb charges for data transfer as s3 does(apart from data storage cost)?",
        "Question_creation_time":1638962523480,
        "Question_link":"https:\/\/community.wandb.ai\/t\/does-wandb-charges-for-data-transfer-as-s3-does-apart-from-data-storage-cost\/1487",
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>The pricing on the wandb website states cost of data storage.Does wandb also charges downloading and uploading of artifacts like S3 does for data transfer?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Login error! init error + broken pipeline",
        "Question_creation_time":1660494944419,
        "Question_link":"https:\/\/community.wandb.ai\/t\/login-error-init-error-broken-pipeline\/2926",
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there, I am facing this issue while using wandb.init().<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4de690d919b9d13b072bd6968c7c5a6d8c4061af.png\" data-download-href=\"\/uploads\/short-url\/b78KvLFKhAaWKkyNhJ2WPEjnkBN.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4de690d919b9d13b072bd6968c7c5a6d8c4061af.png\" alt=\"image\" data-base62-sha1=\"b78KvLFKhAaWKkyNhJ2WPEjnkBN\" width=\"690\" height=\"364\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/4de690d919b9d13b072bd6968c7c5a6d8c4061af_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1211\u00d7640 24.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried running following instructions:<\/p>\n<pre><code class=\"lang-auto\">wandb.init(settings=wandb.Settings(start_method='fork'))\nor\nwandb.init(settings=wandb.Settings(start_method='thread'))\n<\/code><\/pre>\n<p>is not working.<\/p>\n<p>Help would be appreciated, thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Empty group name",
        "Question_creation_time":1653003921104,
        "Question_link":"https:\/\/community.wandb.ai\/t\/empty-group-name\/2452",
        "Question_upvote_count":0.0,
        "Question_view_count":75.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>By my mistake, group name is changed to empty.<br>\nWhen I click the group with empty name, it always redirect to project workspace.<br>\nI can\u2019t find any options make me enable to change the empty group name.<br>\nHow can I change the empty group name.<br>\nIt\u2019s so inconvenient.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Launching More Agents in HP Sweep",
        "Question_creation_time":1632250752251,
        "Question_link":"https:\/\/community.wandb.ai\/t\/launching-more-agents-in-hp-sweep\/733",
        "Question_upvote_count":5.0,
        "Question_view_count":293.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>The documentation shows how, once the sweep is running, <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project\">we can spawn off more agents<\/a>.<\/p>\n<p>But I can\u2019t figure out a way to get to that actual page and do it.<\/p>\n<p>If I open the \u201cSweeps\u201d tab, I see the button \u201cCreate Sweep\u201d, but I\u2019m not sure that\u2019s what I\u2019m looking to do (right?). I want to launch a new agent of this sweep, not an entirely new sweep.<\/p>\n<p>Can I simply run the same code on multiple machines, and will it automatically know (through the same project name?) to combine all those runs in the same overall sweep?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to include citation of wandb easily with zotero or mendeley?",
        "Question_creation_time":1632596067622,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-include-citation-of-wandb-easily-with-zotero-or-mendeley\/772",
        "Question_upvote_count":2.0,
        "Question_view_count":238.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019d like to help support wandb and I think its important to make it as trivial as possible for user to cite wandb as thats valueble - in the spirit of how amazon makes it trivial to buy things with one click. I believe most of us researchers use zotero or mendeley. I usually go to a webpage and then download the citation automatically that I use for all my future papers - with mendeley or zotero. I suggest something like that is done for wandb. Perhaps with a whitepaper report (like tensorflow has) and then we can download the citation for it.<\/p>\n<p>For now I am just copy pasting this<\/p>\n<pre><code class=\"lang-auto\">@misc{wandb,\ntitle = {Experiment Tracking with Weights and Biases},\nyear = {2020},\nnote = {Software available from wandb.com},\nurl={https:\/\/www.wandb.com\/},\nauthor = {Biewald, Lukas},\n}\n<\/code><\/pre>\n<p>but I don\u2019t think copy pasting this is a good long term solution to promote\/support wanbd.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Continuing an artifact",
        "Question_creation_time":1664425707593,
        "Question_link":"https:\/\/community.wandb.ai\/t\/continuing-an-artifact\/3198",
        "Question_upvote_count":1.0,
        "Question_view_count":337.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>We are running long data preparation run (30+ hours) to pre-build source files for training.  However, part of the dataset was not ready and was excluding from the current run (which is 20+ hours into the run).  I would like to process the remaining data and ADD it to this current artifact.<br>\nI note that whenever I run this code is creates a new version of the artifact.<br>\nHow can I append new data to an existing artifact?<\/p>\n<p>Second question: Can I add new data in-parallel with the original job.  That is, can two different processes add data to the same artifact at the same time?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Manually ask sweep agents to start new run",
        "Question_creation_time":1668783209156,
        "Question_link":"https:\/\/community.wandb.ai\/t\/manually-ask-sweep-agents-to-start-new-run\/3441",
        "Question_upvote_count":0.0,
        "Question_view_count":10.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Sorry, it may be easy question or I was unable to find the answer \u2013 how do I manually ask one (or more) agents of a running sweep to stop the current run and move to a new one ?<\/p>\n<p>I am running a sweep with 48 possible configurations with 10 agents. I noticed (from sweep dashboard) some of configurations are poor from the very beginning. So I would like to terminate them and explore the unexplored configurations. The poor ones are just wasting my resources. I don\u2019t want to terminate all since some of them are doing quite good,<\/p>\n<p>How do we do this is wandb ?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I check whether an artifact is available?",
        "Question_creation_time":1643300292817,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-check-whether-an-artifact-is-available\/1826",
        "Question_upvote_count":0.0,
        "Question_view_count":147.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi, just started to use W&amp;B and managed to refactor some code to use artifact versioning today. What I could not find is (and sorry if this is very basic): during the first run of the program I would like to check if there is already some artifact (raw data) f\u00fcr that project \/ artifact name \/ type available: If yes, use it. If no, prepare it (might take a while). I am looking for the equivalent of <code>&lt;filename&gt;.is_file()<\/code> but for artifacts. I could use\/download the artifact in a <code>try, except<\/code> clause but that\u2019s not very pretty (throwing errors on the console, not sure what the correct Exception is). The API does not seem to provide such a functionality?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"HP Sweep: Conditional Sampling",
        "Question_creation_time":1632243031093,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hp-sweep-conditional-sampling\/726",
        "Question_upvote_count":0.0,
        "Question_view_count":252.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Suppose I have a somewhat complicated hyper parameter distribution I\u2019d like to sample:<\/p>\n<p>For example, I have a hyperparameter called HP1 controlling normalization applied my dataset. If I sample HP1 \u2190 maximum-eigenvalue-norm, then maybe I have another hyperparameter I must sample; in this case that could be how to compute maximum eigenvalue which could be in the set {fancy-eigenvalue-computation, torch-built-in-symeig}.<\/p>\n<p>But suppose if normalization technique was sampled as HP1 \u2190 Frobenius-norm, then I have no other hyper parameters to sample.<\/p>\n<p>Optuna handles this nicely, and I was hoping W&amp;B\u2019s had a similar way of auto-magically handling it.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to make parametric plots?",
        "Question_creation_time":1656316710393,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-make-parametric-plots\/2662",
        "Question_upvote_count":1.0,
        "Question_view_count":75.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, I would like to make plots such as the ones that can be seen in this video at this timestamp (59:59): <a href=\"https:\/\/youtu.be\/XL07WEc2TRI?t=3599\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Stanford Seminar - Information Theory of Deep Learning - YouTube<\/a><\/p>\n<p>Basically, I have two variables (let\u2019s say X and Y) that are measured at each layer and epoch, and I would like to have a unified plot where each layer is represented as a parametric curve. Connecting points in a same epoch by neighboring layers would be a plus but that\u2019s optional.<br>\nSo for each epoch and layer, I would like to plot a point at coordinates (X,Y) connected to the corresponding previous point of the previous epoch. If possible, I would like to color each point according to the epoch so that we can see the progression.<\/p>\n<p>I tried to plot a line series like this:<\/p>\n<pre><code class=\"lang-python\">wandb.log({\"XY\": wandb.plot.line_series(self.layers_x, self.layers_y, self.layer_names,\n                                        \"XY by layer and epoch\", \"X\")}, step=step)\n<\/code><\/pre>\n<p>But there are three issues with this:<\/p>\n<ol>\n<li>The points of the curves aren\u2019t connected in the correct order, it seems they are implicitly connected according to their sorted X values. So the resulting curves are incorrect, even if I can guess the true shapes they should have.<\/li>\n<li>I haven\u2019t managed to get point coloring according to the epoch number, and I had to manually modify the plot in the dashboard so that I had all curves correctly displayed in the same plot. I had to use custom plots but I am not familiar with these. I also don\u2019t know how to set the display name of the y axis which is \u201cy\u201d by default.<\/li>\n<li>I have to manually keep track of the table values, if possible I would like to log the values for each step normally, like any other value like the accuracy at each epoch.<\/li>\n<\/ol>\n<p>So, is it possible to make such plots? Thank you.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About hyperparameters sweeping for DDP program",
        "Question_creation_time":1652109385291,
        "Question_link":"https:\/\/community.wandb.ai\/t\/about-hyperparameters-sweeping-for-ddp-program\/2384",
        "Question_upvote_count":0.0,
        "Question_view_count":67.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I have a program which needs multiple GPUs to run at the same time, currently I use DDP to launch the program. I wonder how can I do the sweeping , the program will still be launched  in DDP mode (using all GPUs) at each trial. Thanks!<\/p>\n<p>Best<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Show single lines in groups",
        "Question_creation_time":1649968185659,
        "Question_link":"https:\/\/community.wandb.ai\/t\/show-single-lines-in-groups\/2245",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<br>\nI want to have a way to organise my runs so that I can know some parameters of the runs already by the name. I can do that by using group_by and then I can see the different parameter for each run<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7aefbd3b28fbf0ac496352f40ceb75e0975e1f00.png\" alt=\"image\" data-base62-sha1=\"hxxTc29jObSNMn9oZg3e64ZOA4U\" width=\"269\" height=\"195\"><\/p>\n<p>But this also means all the runs inside a group, is it possible to group but still see the different runs (optionally all runs in a group with the same color?)<br>\nThanks,<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Visual Bug in Documentation",
        "Question_creation_time":1654672339920,
        "Question_link":"https:\/\/community.wandb.ai\/t\/visual-bug-in-documentation\/2572",
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fd0f258d7049524defde72d94701022480abd440.png\" data-download-href=\"\/uploads\/short-url\/A6FfBmGoIRNds6FR4YCdptOz7Fe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png\" alt=\"image\" data-base62-sha1=\"A6FfBmGoIRNds6FR4YCdptOz7Fe\" width=\"690\" height=\"264\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1035x396.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1380x528.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2560\u00d7981 120 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I think there is a missing ``` to finish the code block<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to compute FID score for different checkpoints",
        "Question_creation_time":1653800462853,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-compute-fid-score-for-different-checkpoints\/2506",
        "Question_upvote_count":0.0,
        "Question_view_count":96.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a hyper spectral image of 170 bands.  I have used auto-encoder to reconstruct the image. Now I want to plot the FID score for 100 epochs like we can do for MSE plot.<\/p>\n<p>like we do for MSE polt:<\/p>\n<p>model.compile(optimizer=\u2018adam\u2019, loss=\u2018mean_absolute_error\u2019, metrics=[\u2018accuracy\u2019])<\/p>\n<p>history = model.fit(img, img,<br>\nepochs=100, batch_size=1, verbose=1,<br>\nvalidation_split=0.33, shuffle=True)<\/p>\n<h1>\n<a name=\"list-all-data-in-history-1\" class=\"anchor\" href=\"#list-all-data-in-history-1\"><\/a>list all data in history<\/h1>\n<p>print(history.history.keys())<\/p>\n<h1>\n<a name=\"summarize-history-for-accuracy-2\" class=\"anchor\" href=\"#summarize-history-for-accuracy-2\"><\/a>summarize history for accuracy<\/h1>\n<p>plt.plot(history.history[\u2018accuracy\u2019])<br>\nplt.plot(history.history[\u2018val_accuracy\u2019])<br>\nplt.title(\u2018model accuracy\u2019)<br>\nplt.ylabel(\u2018accuracy\u2019)<br>\nplt.xlabel(\u2018epoch\u2019)<br>\nplt.legend([\u2018train\u2019, \u2018test\u2019], loc=\u2018upper left\u2019)<br>\nplt.show()<\/p>\n<p>Is there a simple way to do that? If so can anyone assist me with a demo code is possible\u2026<\/p>\n<p>Thanks in advance\u2026<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I think that W&B is having some connection issues since yesterday night",
        "Question_creation_time":1635001804843,
        "Question_link":"https:\/\/community.wandb.ai\/t\/i-think-that-w-b-is-having-some-connection-issues-since-yesterday-night\/1073",
        "Question_upvote_count":0.0,
        "Question_view_count":282.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Although it\u2019s still possible to update data to W&amp;B, since yesterday night no data can be downloaded with the error:<\/p>\n<pre><code class=\"lang-auto\">\/requests\/models.py\", line 953, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https:\/\/api.wandb.ai\/graphql\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Add a default sweep plot?",
        "Question_creation_time":1659658946380,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-a-default-sweep-plot\/2854",
        "Question_upvote_count":0.0,
        "Question_view_count":150.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p><strong>Context<\/strong>: In my project, I am performing benchmarking on a lot of datasets and therefore need to perform a lot of sweeps.<\/p>\n<p><strong>Problem<\/strong>:  I have some custom metrics that I wish to plot together to quickly review the performance of the different runs in the sweep in a specific way. This means for each sweep I need to recreate the plot which is frustrating.<\/p>\n<p><strong>Question<\/strong>: Is there a way of adding a custom default plot setup in the sweep workspace?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the recommended way to use the name value in wandb.init?",
        "Question_creation_time":1632176195961,
        "Question_link":"https:\/\/community.wandb.ai\/t\/what-is-the-recommended-way-to-use-the-name-value-in-wandb-init\/721",
        "Question_upvote_count":4.0,
        "Question_view_count":266.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I usually track runs based on the job id of the HPC. So I was thinking I wanted to tack that value to the end to the nice 2 word name that wandb gives. How do I do that?<\/p>\n<p>Also, I am interested in knowing the recommended way to name runs or the common way wandb users, developers etc use this.<\/p>\n<p>note: I am already using the config to track the hyperparams and the group name to group similar experiments. I don\u2019t usually use jobtype actually.<\/p>\n<hr>\n<p>current script:<\/p>\n<pre><code class=\"lang-auto\">    if hasattr(args, 'log_to_wandb'):\n        if args.log_to_wandb:\n            # os.environ['WANDB_MODE'] = 'offline'\n            import wandb\n\n            # - experiment name\n            experiment_name = args.wandb_group\n            # - set run name\n            run_name = None\n            if hasattr(args, 'jobid'):\n                if args.jobid is not None:\n                    run_name: str = f'jobid={str(args.jobid)}'\n            # - initialize wandb\n            wandb.init(project=args.wandb_project,\n                       entity=args.wandb_entity,\n                       # job_type=\"job_type\",\n                       name=run_name,\n                       group=experiment_name\n                       )\n            wandb.config.update(args)\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Set x axis expression for multiple plots at once",
        "Question_creation_time":1634216056421,
        "Question_link":"https:\/\/community.wandb.ai\/t\/set-x-axis-expression-for-multiple-plots-at-once\/978",
        "Question_upvote_count":1.0,
        "Question_view_count":245.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9382546d321c088f70d9f648e6c6f42f91eff90f.png\" data-download-href=\"\/uploads\/short-url\/l2VpWvmsWouis0P14ZE8w2DtEWr.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9382546d321c088f70d9f648e6c6f42f91eff90f_2_638x500.png\" alt=\"image\" data-base62-sha1=\"l2VpWvmsWouis0P14ZE8w2DtEWr\" width=\"638\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9382546d321c088f70d9f648e6c6f42f91eff90f_2_638x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9382546d321c088f70d9f648e6c6f42f91eff90f.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9382546d321c088f70d9f648e6c6f42f91eff90f.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9382546d321c088f70d9f648e6c6f42f91eff90f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">693\u00d7543 28.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nI use this feature to show Epoch instead of Iter on the X axis.<br>\nIt\u2019s a bit laborious to go trough all my plots to set this expression manually.<\/p>\n<p>Is there a workaround ?<br>\nOtherwise, please flag this with [feature request]<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Stored artifact is not h5 file",
        "Question_creation_time":1666706847698,
        "Question_link":"https:\/\/community.wandb.ai\/t\/stored-artifact-is-not-h5-file\/3322",
        "Question_upvote_count":0.0,
        "Question_view_count":172.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using tensorflow with the python callback. The callback stores several files automatically for each run:<\/p>\n<ul>\n<li>saved_model.pb<\/li>\n<li>keras_metadata.pb<\/li>\n<\/ul>\n<p>I would like to automatically store the weights h5 file as well. Is there an option for this? Do I have to do it manually?<br>\nNote that I can find best-model.h5 in the wand folder of each run, but for some reason it is not uploaded to the server.<\/p>\n<p>Thank you,<br>\nArnaud<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remove from a team",
        "Question_creation_time":1664267118870,
        "Question_link":"https:\/\/community.wandb.ai\/t\/remove-from-a-team\/3193",
        "Question_upvote_count":0.0,
        "Question_view_count":275.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to remove myself from a tesm. How can I do it?<\/p>\n<p>Thanks,<br>\nP<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logged value available in graph panel, but not in columns",
        "Question_creation_time":1647509310562,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logged-value-available-in-graph-panel-but-not-in-columns\/2100",
        "Question_upvote_count":0.0,
        "Question_view_count":344.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I log values which have names in the form of <code>test\/temp_top-k.---1<\/code> (I want the dashes for sorting reasons). I can create graph panels with these values, but they do not show up in the column view. When I <code>Manage Columns<\/code> they are not listed in the <code>Hidden Columns<\/code>. When I search for them, it gives me no (an empty) result. Even when I select <code>Show All<\/code> they don\u2019t show up in the column view. A bug?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Config and summary logs gone after initialising a run again",
        "Question_creation_time":1654399014146,
        "Question_link":"https:\/\/community.wandb.ai\/t\/config-and-summary-logs-gone-after-initialising-a-run-again\/2556",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>After I finished a training run I wanted to upload a file, like shown here <a href=\"https:\/\/community.wandb.ai\/t\/add-files-to-run\/1066\/2\" class=\"inline-onebox\">Add files to run - #2 by _scott<\/a><br>\nThis worked fine, but after that all the config and summary logs were gone.<\/p>\n<p>Is there a way to restore them?<\/p>\n<p>If no, how can I save this file without loosing the logs?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Legend ordering",
        "Question_creation_time":1661988930181,
        "Question_link":"https:\/\/community.wandb.ai\/t\/legend-ordering\/3041",
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey all,<br>\nI was just wondering if its possible to rearrange\/order elements in the legend of a chart? I have different data series that each go by a number and right now, the legend displays these numbers in random order\u2026<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Instant crash",
        "Question_creation_time":1653081865643,
        "Question_link":"https:\/\/community.wandb.ai\/t\/instant-crash\/2468",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey Guys, if i wand.init my pc instantly blue screens after inpputing my Key. I have no idea how to figure out what the issue is here and if some of you have any idea please let me know.<br>\nThanks in advance.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb.watch not logging parameters",
        "Question_creation_time":1635863467980,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-not-logging-parameters\/1197",
        "Question_upvote_count":1.0,
        "Question_view_count":326.0,
        "Question_answer_count":19,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I just started to use w&amp;b to monitor the training of my few-shot learning NNs in Pytorch. I use wandb.watch(model, log=\u2018all\u2019) but it only logs the gradients. Any idea what could be causing this? Also, is there an easy way to log the activation histograms of the different layers for pytorch?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Change logger handler",
        "Question_creation_time":1635881450395,
        "Question_link":"https:\/\/community.wandb.ai\/t\/change-logger-handler\/1203",
        "Question_upvote_count":2.0,
        "Question_view_count":248.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I use <a href=\"https:\/\/rich.readthedocs.io\/en\/stable\/logging.html\" rel=\"noopener nofollow ugc\">RichHandler<\/a> everywhere in my code as well as <a href=\"https:\/\/rich.readthedocs.io\/en\/stable\/traceback.html\" rel=\"noopener nofollow ugc\">Rich Tracebacks<\/a> as they\u2019re just useful beautiful and really convenient.<\/p>\n<p>I was wondering how I could integrate the handler with WandB to have my logs in the same format.<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb sweeps running on Kaggle GPU or Colab GPU are much slower than on my local CPU",
        "Question_creation_time":1632736952286,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sweeps-running-on-kaggle-gpu-or-colab-gpu-are-much-slower-than-on-my-local-cpu\/794",
        "Question_upvote_count":4.0,
        "Question_view_count":379.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<\/p>\n<p>i have run a few sweeps on my local computer and the same sweeps on Kaggle and Colab<\/p>\n<p>i have an i7 (10th gen) CPU in my home computer but no GPU<br>\ni measured around 50secs for 100 epochs (1 run)<\/p>\n<p>on Kaggle and Colab the same 100 epochs took 2mins 30secs (Colab) and ~3mins  (Kaggle) <em>using GPU<\/em><\/p>\n<p>how is that possible? am i doing something wrong?<\/p>\n<p>i observed this extreme slowdown only when using W&amp;B Sweeps<br>\nno slowdown when running single experiments<\/p>\n<p>please help, any idea appreciated!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"W&B Sweeps w\/ Self-Supervised Learning",
        "Question_creation_time":1654719691416,
        "Question_link":"https:\/\/community.wandb.ai\/t\/w-b-sweeps-w-self-supervised-learning\/2579",
        "Question_upvote_count":1.0,
        "Question_view_count":403.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m reaching out to get some thoughts on integrating W&amp;B Sweeps with some of the code we\u2019re interested in using. An example of the code we\u2019re running is linked <a href=\"https:\/\/streaklinks.com\/BEDYVhxBgS02lmUdIwa1e2xV\/https%3A%2F%2Fgithub.com%2Fspaceml-org%2FSelf-Supervised-Learner%2Fblob%2Fmain%2Ftutorials%2FPythonColabTutorial_Merced.ipynb\" rel=\"noopener nofollow ugc\">here<\/a>. Note the 2 key sections \u2018Training Self-Supervised Learning Model\u2019 and \u2018Fine Tuning Model\u2019 which contain the !python commands we\u2019re interested in tuning (model, technique, learning rate, etc.)<\/p>\n<p>Based on <a href=\"https:\/\/streaklinks.com\/BEDYVh1Fxefx8VZVJQh5ZZi-\/https%3A%2F%2Fdocs.wandb.ai%2Fguides%2Fsweeps%2Fpython-api\" rel=\"noopener nofollow ugc\">this documentation<\/a>, I\u2019ve set up sweep_config but I\u2019m unsure how to incorporate the 2 !python commands in train() when running an agent. Do you have any input on how to integrate a wandb sweep with these 2 commands?<\/p>\n<p>An additional point I wanted to discuss was the strategy for a Sweep. The SSL code we\u2019re running requires training 2 sequential models (the SSL and the final classification Model) where the output SSL model is the input to the final classification model (see the linked code above). We\u2019re interested in doing hyperparameter tuning for both of the models - should we set up 2 independent sweeps for each? Or should we run a sweep on the first SSL model, pick the best performing model and use that as the input into the second classification model where we run a second sweep?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging linear evaluation results asynchronously in SimCLR",
        "Question_creation_time":1655919425378,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-linear-evaluation-results-asynchronously-in-simclr\/2646",
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! I\u2019m currently investigating SimCLR, which consists of a pretraining and fine-tuning\/linear evaluation step. I can log pretraining loss and linear eval accuracy metrics in the same W&amp;B run by running eval after every pretraining epoch, but the pretraining script has to wait until eval is done before continuing with the next epoch. Is there any way to run linear eval <em>after<\/em> pretraining is done (e.g., in a separate <code>eval.py<\/code> script, and log the results to the same run_id?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/75c2e2de900389d008ad3df1a0c3f242bed1606d.png\" data-download-href=\"\/uploads\/short-url\/gNLp9XWDciUbXa3tmRjGtU7axqZ.png?dl=1\" title=\"wandb_fig\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75c2e2de900389d008ad3df1a0c3f242bed1606d_2_690x456.png\" alt=\"wandb_fig\" data-base62-sha1=\"gNLp9XWDciUbXa3tmRjGtU7axqZ\" width=\"690\" height=\"456\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75c2e2de900389d008ad3df1a0c3f242bed1606d_2_690x456.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/75c2e2de900389d008ad3df1a0c3f242bed1606d.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/75c2e2de900389d008ad3df1a0c3f242bed1606d.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75c2e2de900389d008ad3df1a0c3f242bed1606d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wandb_fig<\/span><span class=\"informations\">835\u00d7553 56.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep over pre-training, then sweep over finetuning",
        "Question_creation_time":1646399505042,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-over-pre-training-then-sweep-over-finetuning\/2015",
        "Question_upvote_count":0.0,
        "Question_view_count":129.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019d like to pre-train a model under several different conditions, then finetune each of those resulting models. The simple way to do this would be two separate sweeps. But then I need to manually start the second one. Is there a way to combine these into one single sweep?<\/p>\n<p>With a bash script, I can simply call pre-train and finetune in sequence, passing the respective arguments via a config file. Is it somehow possible to tell the respective scripts that they are part of the same sweep and should therefore use certain parameters?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"HP sweep - correct way to stop a specific agent (and not the entire sweep)",
        "Question_creation_time":1635686596678,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hp-sweep-correct-way-to-stop-a-specific-agent-and-not-the-entire-sweep\/1173",
        "Question_upvote_count":2.0,
        "Question_view_count":417.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am conducting a parameter sweep, and I use my dev machine during the night for extra compute. My problem is that I don\u2019t know how to correctly stop the local runs. Any suggestions or best practices would be appreciated.<\/p>\n<p>A related question - if I stop an agent run forcefully (for instance, close the process running the agent) how would the sweep controller handle the run data? would it remove it from the dashboard? would it be indicated in any way?<\/p>\n<p>Thanks,<br>\nTom<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error while hyperparameter search",
        "Question_creation_time":1657819842704,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-while-hyperparameter-search\/2751",
        "Question_upvote_count":0.0,
        "Question_view_count":92.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When I try wandb.sweep, it gives following error:  wandb.errors.CommError: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.<\/p>\n<p>Following this, is my sweep config<br>\n{\u2018method\u2019: \u2018random\u2019,<br>\n\u2018metric\u2019: {\u2018goal\u2019: \u2018minimize\u2019, \u2018name\u2019: \u2018loss\u2019},<br>\n\u2018parameters\u2019: {\u2018batch_size\u2019: {\u2018distribution\u2019: \u2018q_log_uniform_values\u2019,<br>\n\u2018max\u2019: 256,<br>\n\u2018min\u2019: 32,<br>\n\u2018q\u2019: 8},<br>\n\u2018epochs\u2019: {\u2018value\u2019: 10},<br>\n\u2018fc_layer_size\u2019: {\u2018values\u2019: [16, 32, 64]},<br>\n\u2018learning_rate\u2019: {\u2018distribution\u2019: \u2018uniform\u2019,<br>\n\u2018max\u2019: 0.1,<br>\n\u2018min\u2019: 0},<br>\n\u2018optimizer\u2019: {\u2018values\u2019: [\u2018adam\u2019, \u2018sgd\u2019]},<br>\n\u2018training_snr\u2019: {\u2018values\u2019: [0.3981071705534972,<br>\n0.44668359215096315,<br>\n0.5011872336272722,<br>\n0.5623413251903491,<br>\n0.6309573444801932,<br>\n0.7079457843841379,<br>\n0.7943282347242815,<br>\n0.8912509381337456,<br>\n1.0,<br>\n1.1220184543019633,<br>\n1.2589254117941673,<br>\n1.4125375446227544,<br>\n1.5848931924611136,<br>\n1.7782794100389228,<br>\n1.9952623149688795,<br>\n2.2387211385683394,<br>\n2.51188643150958,<br>\n2.8183829312644537,<br>\n3.1622776601683795,<br>\n3.548133892335755,<br>\n3.9810717055349722,<br>\n4.466835921509632,<br>\n5.011872336272722,<br>\n5.623413251903491,<br>\n6.309573444801933,<br>\n7.079457843841379,<br>\n7.943282347242816,<br>\n8.912509381337454,<br>\n10.0]}}}<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Confusion Matrix generates report not plot",
        "Question_creation_time":1666065014660,
        "Question_link":"https:\/\/community.wandb.ai\/t\/confusion-matrix-generates-report-not-plot\/3270",
        "Question_upvote_count":0.0,
        "Question_view_count":310.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve recently attempted to build a Confusion Matrix using the <code>wandb.plot.confusion_matrix()<\/code> command.<br>\nMuch to my surprise, I get a Table object, not a plot.<br>\nImagine my surprise when I discovered a comment (<a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\">here<\/a>) saying \u201cLogging the Table also is expected behaviour\u201d.<br>\nReally?  If I wanted a Table, i would call <code>wand.table.confusion_matrix()<\/code>, not the <code>plot<\/code> command.<br>\nI really do want a plot and and do NOT want to \u201cinteractively explore\u201d it.  I want to see it along with the twenty other plots that I generate with this run.  Clicking each and every CM to view it kinda defeats the purpose.<br>\nI would like to recommend that this choice be rethought, or at least put in the <code>table<\/code> namespace.<br>\nIs there a way to generate a real CM plot?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Having problems with LaTeX reports",
        "Question_creation_time":1639306013787,
        "Question_link":"https:\/\/community.wandb.ai\/t\/having-problems-with-latex-reports\/1514",
        "Question_upvote_count":0.0,
        "Question_view_count":163.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When I try to download a LaTeX report the download spinning wheel starts, but never stops.<br>\nDoes it take so long or is this an issue with my setup?<\/p>\n<p>I\u2019m working on a MacBook accessing wandb from Apple Safari browser.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Set random seed for sweep initialization",
        "Question_creation_time":1668541381663,
        "Question_link":"https:\/\/community.wandb.ai\/t\/set-random-seed-for-sweep-initialization\/3418",
        "Question_upvote_count":0.0,
        "Question_view_count":14.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>is there a way to set the initial random seed for a random or bayesian hyperparameter sweep  so that  it goes through a reproducible  sequence of hyperparameter configurations?<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run crashed at end of epoch due to invalid name",
        "Question_creation_time":1658234073854,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-crashed-at-end-of-epoch-due-to-invalid-name\/2776",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I walked away from my run, only to come back and see it stopped after 1 epoch with:<\/p>\n<pre><code class=\"lang-auto\">  File \"\/home\/ubuntu\/src\/polez\/conda\/polez\/lib\/python3.9\/site-packages\/wandb\/integration\/keras\/keras.py\", line 1019, in _save_model_as_artifact\n    model_artifact = wandb.Artifact(f\"model-{wandb.run.name}\", type=\"model\")\n  File \"\/home\/ubuntu\/src\/polez\/conda\/polez\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_artifacts.py\", line 137, in __init__\n    raise ValueError(\nValueError: Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: \"model-point-tall-fine,temp=0.2,batch=1024,custom_sched=false\"\n<\/code><\/pre>\n<p>I would much rather have found this out at the beginning when I called <code>wandb.init<\/code><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Make user admin for trial",
        "Question_creation_time":1633048801825,
        "Question_link":"https:\/\/community.wandb.ai\/t\/make-user-admin-for-trial\/826",
        "Question_upvote_count":6.0,
        "Question_view_count":283.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>I\u2019m trialing wanb for my company to see if its something we\u2019re interested in using.  My account is a \u201cmember\u201d account even though i\u2019m the one who created the Team.  I\u2019d like to invite other team members to my team but because my user isn\u2019t \u201cadmin\u201d status I cannot.  How can I get my team members added to the wand team I created?<\/p>\n<p>Thank you,<br>\nBlake<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Workspace Bug",
        "Question_creation_time":1634197748869,
        "Question_link":"https:\/\/community.wandb.ai\/t\/workspace-bug\/974",
        "Question_upvote_count":0.0,
        "Question_view_count":249.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear WandB hello.<\/p>\n<p>When I\u2019m trying to work in my workspace and looking at the graph I could not see the model\u2019s menu, is there a way to fix it at my level?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Compare different architectures for a same task",
        "Question_creation_time":1657202004620,
        "Question_link":"https:\/\/community.wandb.ai\/t\/compare-different-architectures-for-a-same-task\/2724",
        "Question_upvote_count":0.0,
        "Question_view_count":217.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey there.<\/p>\n<p>I discovered W&amp;B recently and decided to use it for my current research project. The thing is that I have a specific task to solve and would like to evaluate a bunch of completely different model architectures, having different sets of hyper-parameters.<\/p>\n<p>Most of the online resources and tutorials I\u2019ve found only shows examples of W&amp;B usage to evaluate different experiments with different params selections (e.g. optimized using sweeps). However none of the examples I found explained how to optimally organize a W&amp;B project including different architectures to solve the same task, and thus being able to compare in a glimpse the different performances in a single view \/ report.<\/p>\n<p>My idea was to make use of the job_type flag and group every architecture instances together under a same job_type flag. But still seems like not the best solution and was wondering if there is some special feature or built-in tool that I\u2019ve not noticed yet (or even good practices?).<\/p>\n<p>(Other than that, W&amp;B looks really insane).<\/p>\n<p>Many thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smile.png?v=12\" title=\":smile:\" class=\"emoji\" alt=\":smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using wandb sweep with torch.distributed.launch",
        "Question_creation_time":1653309050747,
        "Question_link":"https:\/\/community.wandb.ai\/t\/using-wandb-sweep-with-torch-distributed-launch\/2483",
        "Question_upvote_count":0.0,
        "Question_view_count":82.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello<\/p>\n<p>I am using wandb sweep to perform hyperparameter tuning.<\/p>\n<p>Basically when I launch wandb agent with \u201cwandb agent &lt;USERNAME\/PROJECTNAME\/SWEEPID&gt;\u201d,<\/p>\n<p>It will automatically run  \u201c\/usr\/bin\/env python train.py --param1=value1 --param2=value2\u201d according to the configurations.<\/p>\n<p>However my code is based on torch distributed data parallel and it has to be launched with torch.distributed.launch   train.py  rather than just  python train.py.<\/p>\n<p>How can I tackle this problem?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Upload and Syncing of Artifacts are too slow using WSL - MainThread and HandlerThread hanging",
        "Question_creation_time":1662381199984,
        "Question_link":"https:\/\/community.wandb.ai\/t\/upload-and-syncing-of-artifacts-are-too-slow-using-wsl-mainthread-and-handlerthread-hanging\/3068",
        "Question_upvote_count":0.0,
        "Question_view_count":172.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello everyone, hope you can help me with this issue.<\/p>\n<p>I am very new to the W&amp;B interface and python library. I am tryingo to incorporate the dataset versioning and experiment tracking issues into my training procedure for now.<\/p>\n<p>In the dataset versioning section of my code, I am logging the raw dataset and the cleaned ones via wandb.Artifact \u2192 wandb.add_file \u2192 wandb.log_artifact workflow, as show in documentation.<\/p>\n<p>The problem is that a simple <strong>upload and syncing takes around 10 minutes to finish!<\/strong> The datasets sizes are small (approx. 2MB) and I don\u2019t have any connection constraints or issues that I\u2019m aware of.<br>\nI\u2019m using a JupyterNotebook in a WSL2 environment (distro Ubuntu 20.04)<\/p>\n<p>The output of code shows: <code>Wating for W&amp;B process to finish (sucess) ...<\/code> for the whole time, and the upload and syncing bar stucks during the whole time of waiting.<\/p>\n<p>The debug log for the <code>latest-run<\/code> show this over and over:<\/p>\n<pre><code class=\"lang-auto\">2022-09-05 01:29:54.344 INFO    MainThread:10293 [jupyter.py:save_history():447] not saving jupyter history\n2022-09-05 01:29:54.344 INFO    MainThread:10293 [jupyter.py:save_ipynb():377] not saving jupyter notebook\n2022-09-05 01:29:54.344 INFO    MainThread:10293 [wandb_init.py:_jupyter_teardown():393] cleaning up jupyter logic\n2022-09-05 01:29:54.344 INFO    MainThread:10293 [wandb_run.py:_atexit_cleanup():1931] got exitcode: 0\n2022-09-05 01:29:54.344 INFO    MainThread:10293 [wandb_run.py:_restore():1914] restore\n2022-09-05 01:29:54.345 INFO    MainThread:10293 [wandb_run.py:_restore():1920] restore done\n...\n2022-09-05 01:29:57.481 INFO    MainThread:10293 [wandb_run.py:_on_finish():2221] got exit ret: file_counts {\n  wandb_count: 5\n}\npusher_stats {\n  uploaded_bytes: 397\n  total_bytes: 4431\n}\n<\/code><\/pre>\n<p>And the <code>debug-intenal<\/code> log shows the following:<\/p>\n<pre><code class=\"lang-auto\">2022-09-05 01:29:57.789 DEBUG   SenderThread:10324 [sender.py:send_request():316] send_request: poll_exit\n2022-09-05 01:29:57.892 DEBUG   HandlerThread:10324 [handler.py:handle_request():141] handle_request: poll_exit\n2022-09-05 01:29:57.892 DEBUG   SenderThread:10324 [sender.py:send_request():316] send_request: poll_exit\n2022-09-05 01:29:57.993 DEBUG   HandlerThread:10324 [handler.py:handle_request():141] handle_request: poll_exit\n2022-09-05 01:29:57.994 DEBUG   SenderThread:10324 [sender.py:send_request():316] send_request: poll_exit\n<\/code><\/pre>\n<p>And in the end of the running cell, the <code>debug-internal<\/code> log shows (sensible info omitted):<\/p>\n<pre><code class=\"lang-auto\">2022-09-05 01:33:09,176 DEBUG   HandlerThread:10324 [handler.py:handle_request():141] handle_request: poll_exit\n2022-09-05 01:33:09,176 DEBUG   SenderThread:10324 [sender.py:send_request():316] send_request: poll_exit\n2022-09-05 01:33:10,268 INFO    WriterThread:10324 [datastore.py:close():279] close [...]\n2022-09-05 01:33:11,177 INFO    SenderThread:10324 [sender.py:finish():1312] shutting down sender\n2022-09-05 01:33:11,177 INFO    SenderThread:10324 [file_pusher.py:finish():171] shutting down file pusher\n2022-09-05 01:33:11,177 INFO    SenderThread:10324 [file_pusher.py:join():176] waiting for file pusher\n<\/code><\/pre>\n<p>I tried setting <code>WANDB_START_METHOD=thread<\/code> as mentioned in a Github issue, but didn\u2019t reduce overall time that the cell takes to finish. I have made the login through CLI and the cell recognizes my user.<\/p>\n<p>The raw data are in JSON format, and the cleaned data is a Pandas dataframe where of 30-60 rows, where each row contains an array of data (temporal analysis) around 2000 items.<\/p>\n<p><strong>Is there something I forgot  to setup? Is this the average time taken to upload files, even when they are small? I am missing something in the code workflow?<\/strong><\/p>\n<p>Any help would be much appreciated!<\/p>\n<p>Regards<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to run wandb online after running offline",
        "Question_creation_time":1636505640810,
        "Question_link":"https:\/\/community.wandb.ai\/t\/unable-to-run-wandb-online-after-running-offline\/1252",
        "Question_upvote_count":2.0,
        "Question_view_count":269.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I have a compute environment where I was running wandb offline for quite a while. I am now hoping to use it online (to get automatic syncing), however I seem to be unable to set this up now. The following is a minimal reproducible example:<\/p>\n<pre><code class=\"lang-auto\">&gt;&gt; import wandb\n&gt;&gt; test = wandb.init(mode='online')\nTraceback (most recent call last):\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 867, in init\n    wi.setup(kwargs)\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 182, in setup\n    user_settings = self._wl._load_user_settings()\n  File \"[path]\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_setup.py\", line 183, in _load_user_settings\n    flags = self._server._flags\nAttributeError: 'NoneType' object has no attribute '_flags'\nwandb: ERROR Abnormal program exit\n<\/code><\/pre>\n<p>I have tried<\/p>\n<ul>\n<li>running wandb online in the terminal<\/li>\n<li>setting the wandb mode environment variable to be online<\/li>\n<li>uninstalling and reinstalling wandb<\/li>\n<\/ul>\n<p>Is there any way I can run this online?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging dictionary from Pytorch Lightning Logger",
        "Question_creation_time":1632242203176,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-dictionary-from-pytorch-lightning-logger\/725",
        "Question_upvote_count":0.0,
        "Question_view_count":266.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When logging using the W&amp;B Callback, a call to <code>self.log()<\/code> will only take scalars (the call won\u2019t fail, it just won\u2019t show up in the W&amp;B plots). I\u2019d like to to something like the following:<\/p>\n<p>Using the standard PL log call:<br>\n<code>self.log(name='my_metrics', value={ 'a': 1, 'b':2} ).<\/code><br>\nand have nice plots show up auto-magically with all these plots on the same axis or in the same tab.<\/p>\n<p>Currently I am simply looping through my dict and logging them as scalars manually<\/p>\n<p>Note I would like to avoid unpacking the the W&amp;B logger from the PL Trainer, and calling it directly.<br>\n<code>wandb = self.logger.experiment<\/code><\/p>\n<p>I haven\u2019t tested this, but I assume it would work. But it would make my code messy and dependent on knowledge of whether I was using W&amp;B for a particular run.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pytorch sync_tensorboard help",
        "Question_creation_time":1634500284534,
        "Question_link":"https:\/\/community.wandb.ai\/t\/pytorch-sync-tensorboard-help\/1017",
        "Question_upvote_count":1.0,
        "Question_view_count":524.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m having some issues getting W&amp;B to sync with Tensorboard in PyTorch. According to this <a href=\"https:\/\/github.com\/wandb\/client\/issues\/493\" rel=\"noopener nofollow ugc\">issue<\/a> and the docs, I should initializing <code>SummaryWriter<\/code> after W&amp;B  <code>init<\/code> possibly using <code>wandb.tensorboard.patch<\/code>. So far I haven\u2019t been able to get this to work with either <code>torch.utils.tensorboard<\/code> or <code>tensorboardX<\/code> and with or without the patch. Not sure if this is a bug or I\u2019m missing something. Thanks.<\/p>\n<p>Windows 11<br>\nPython 3.8.8<br>\nwandb 0.12.4<br>\ntorch 1.9.1<\/p>\n<pre><code class=\"lang-auto\"> wandb.tensorboard.patch(root_logdir=\"logs\")\n wandb.init(config=hyperparameter_defaults, project=f\"ppo_{env_name}_torch\", sync_tensorboard=True, save_code=True, name=run_name)\n config = wandb.config\n writer = SummaryWriter(f\"logs\")\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to override a Table",
        "Question_creation_time":1642451101000,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-override-a-table\/1776",
        "Question_upvote_count":0.0,
        "Question_view_count":191.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using a Table as it\u2019s the simplest to log text (Input output expected).<\/p>\n<p>However, I would like to keep only the last logged item in the Table, else I have thousand of rows or hundred of tables if I create a new table instead.<\/p>\n<p>Is there any fix for that ?<\/p>\n<p>Thanks in advance,<br>\nHave a great day <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why is min and max causing errors when logging gradients for biases in model?",
        "Question_creation_time":1632175724071,
        "Question_link":"https:\/\/community.wandb.ai\/t\/why-is-min-and-max-causing-errors-when-logging-gradients-for-biases-in-model\/720",
        "Question_upvote_count":3.0,
        "Question_view_count":434.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Why is this error happening when wandb is logging the grad.data field?<\/p>\n<pre><code class=\"lang-auto\">  File \"\/home\/miranda9\/automl-meta-learning\/automl-proj-src\/experiments\/meta_learning\/main_metalearning.py\", line 360, in &lt;module&gt;\n    main(args)\n  File \"\/home\/miranda9\/automl-meta-learning\/automl-proj-src\/experiments\/meta_learning\/main_metalearning.py\", line 333, in main\n    meta_train_fixed_iterations_full_epoch_possible(args)\n  File \"\/home\/miranda9\/automl-meta-learning\/automl-proj-src\/meta_learning\/training\/meta_training.py\", line 216, in meta_train_fixed_iterations_full_epoch_possible\n    log_train_val_stats(args, args.it, train_loss, train_acc, valid=meta_eval, bar=bar_it,\n  File \"\/home\/miranda9\/automl-meta-learning\/automl-proj-src\/meta_learning\/training\/meta_training.py\", line 129, in log_train_val_stats\n    val_loss, val_acc = valid(args, save_val_ckpt=save_val_ckpt)\n  File \"\/home\/miranda9\/automl-meta-learning\/automl-proj-src\/meta_learning\/training\/meta_training.py\", line 274, in meta_eval\n    eval_loss, eval_acc = args.meta_learner(spt_x, spt_y, qry_x, qry_y)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_cpu\/lib\/python3.9\/site-packages\/torch\/nn\/modules\/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"\/home\/miranda9\/automl-meta-learning\/automl-proj-src\/meta_learning\/meta_learners\/maml_meta_learner.py\", line 159, in forward\n    (qry_loss_t \/ meta_batch_size).backward()  # note this is more memory efficient (as it removes intermediate data that used to be needed since backward has already been called)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_cpu\/lib\/python3.9\/site-packages\/torch\/_tensor.py\", line 255, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_cpu\/lib\/python3.9\/site-packages\/torch\/autograd\/__init__.py\", line 147, in backward\n    Variable._execution_engine.run_backward(\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_cpu\/lib\/python3.9\/site-packages\/wandb\/wandb_torch.py\", line 285, in &lt;lambda&gt;\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_cpu\/lib\/python3.9\/site-packages\/wandb\/wandb_torch.py\", line 283, in _callback\n    self.log_tensor_stats(grad.data, name)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_cpu\/lib\/python3.9\/site-packages\/wandb\/wandb_torch.py\", line 235, in log_tensor_stats\n    tensor = flat.histc(bins=self._num_bins, min=tmin, max=tmax)\nRuntimeError: max must be larger than min\n<\/code><\/pre>\n<p>I am not doing anything myself so I am unsure how I can fix this\u2026<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Question_creation_time":1640245639181,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Question_upvote_count":0.0,
        "Question_view_count":166.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I create a custom metric for bayesian sweeps?",
        "Question_creation_time":1655333611703,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-create-a-custom-metric-for-bayesian-sweeps\/2622",
        "Question_upvote_count":0.0,
        "Question_view_count":129.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am trying to figure out how to integrate a custom metric for sweeps. It should be a composite of the number of clusters created as well as the number of outliers. I\u2019m just getting started and the answer doesn\u2019t jump out from the documentation. Thanks in advance.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tensorboard sync shows incorrect number of steps",
        "Question_creation_time":1633617442784,
        "Question_link":"https:\/\/community.wandb.ai\/t\/tensorboard-sync-shows-incorrect-number-of-steps\/881",
        "Question_upvote_count":0.0,
        "Question_view_count":257.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello!<\/p>\n<p>I have observed a strange behavior when synchronizing tensorboard runs. Two runs have different lengths in steps when uploaded on wandb. And both are wrong. They are probably different due to multiprocessing. Although, if I open the tensorboard tab in the wandb interface it shows both results correctly.<\/p>\n<p>I can provide the files if I figure out how to attach them here. Or should I upload it somewhere else?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MatPlotLib into WandB",
        "Question_creation_time":1664770989932,
        "Question_link":"https:\/\/community.wandb.ai\/t\/matplotlib-into-wandb\/3212",
        "Question_upvote_count":0.0,
        "Question_view_count":296.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I need to import MatPlotLib images into WandB.  On the surface, this seems simple, since the documentation clearly shows how to ingest a <code>plt<\/code> or <code>fig<\/code> object.  However, WandB is making a mess of the plots and I don\u2019t want to recode them in plotly.<br>\nSo I next want to use MatPlotLib to save a PNG and ingest that.  Again seems easy, but I would prefer to do it using an in-memory buffer object (this avoids messing with local paths and temp directories on various instances).  Apparently I\u2019m not the first one to do this either (<a href=\"https:\/\/stackoverflow.com\/questions\/35999020\/convert-pyplot-figure-into-wand-image-image\" rel=\"noopener nofollow ugc\">link<\/a>). The instructions are clear and show someone has already done this.  But it fails when I try it:<\/p>\n<pre><code class=\"lang-auto\">fig, (ax1, ax2) = plt.subplots(2, 1, dpi=300, figsize=(10, 5))\n...\nbuf = io.BytesIO()\nplt.savefig(buf, format='png')\nbuf.seek(0)\nwandb.log(({\"chart\": wandb.Image(file=buf)}))\n<\/code><\/pre>\n<p>The error seems to be with <code>wandb.Image()<\/code>.  It returns:<br>\n<code>{TypeError}__init__() got an unexpected keyword argument 'file'<\/code><\/p>\n<p>I can remove the <code>file=<\/code> parameter so that the command is:<\/p>\n<pre><code class=\"lang-auto\">wandb.Image(buf)\n<\/code><\/pre>\n<p>And I get: <code>{AttributeError}'_io.BytesIO' object has no attribute 'ndim'<\/code><\/p>\n<p>Any recommendations?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Plotting array-like values in a parallel coordinates plot",
        "Question_creation_time":1637668401807,
        "Question_link":"https:\/\/community.wandb.ai\/t\/plotting-array-like-values-in-a-parallel-coordinates-plot\/1371",
        "Question_upvote_count":0.0,
        "Question_view_count":243.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I have set up a sweep where one of the parameters possible values (arrays) are [0, 1, 3, 5], [0, 1, 5, 8], [0, 1, 8, 11], etc. The sweep is working fine and the model receives the correct value from the agent, but when visualizing the sweep using the parallel coordinates plot, every possible value of this hyperparameter is plotted at zero, since it is the first element of every array.<\/p>\n<p>I have thought of two solutions:<\/p>\n<ul>\n<li>Download all the data using the API, modify each value with an alias, and reupload the data.<\/li>\n<li>Write a custom plot to modify how that axe is plotted.<\/li>\n<\/ul>\n<p>However, I would like to know if there is a more straightforward solution so that each experiment line correctly passes through its value of this hyperparameter.<\/p>\n<p>Thank you in advance!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb login issue on git bash",
        "Question_creation_time":1646203887453,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-login-issue-on-git-bash\/2000",
        "Question_upvote_count":0.0,
        "Question_view_count":331.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In the login process, this error occurs. It  says install \u201cMkl-service\u201d but the service has already installed and I had tried to fix this error but I was unable to do so<\/p>\n<p>$ wandb login<br>\nc:\\users\\great\\anaconda3\\lib\\site-packages\\numpy_<em>init<\/em>_.py:143: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see <a href=\"http:\/\/github.com\/IntelPython\/mkl-service\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - IntelPython\/mkl-service: Python hooks for Intel(R) Math Kernel Library runtime control settings.<\/a><br>\nfrom . import _distributor_init<br>\nwandb: Appending key for <a href=\"http:\/\/api.wandb.ai\">api.wandb.ai<\/a> to your netrc file: C:\\Users\\great\/.netrc<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Recording videos of custom gym environments",
        "Question_creation_time":1663105781075,
        "Question_link":"https:\/\/community.wandb.ai\/t\/recording-videos-of-custom-gym-environments\/3110",
        "Question_upvote_count":0.0,
        "Question_view_count":207.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I am try to use VecVideoRecorder to log videos of my custom environment. The observation come from a camera, though I don\u2019t think that is the issue.  The error is:<\/p>\n<p>AttributeError(\u201c\u2018VideoRecorder\u2019 object has no attribute \u2018path\u2019\u201d)<\/p>\n<p>I\u2019m not directly setting nor accessing an attribute \u2018path\u2019 so I\u2019m having identifying where this is coming from. My code looks like this:<\/p>\n<pre><code>def make_env():\n    env = DummyVecEnv([lambda:    Monitor(ReacherFiveJointsImageSpace(random_start=wandb.config.random_start,\n                                                        log_state_actions=True,\n                                                        shape_reward=wandb.config.shape_reward,\n                                                        file_name_prefix=wandb.config.rl_name,\n                                                        env_type=wandb.config.env_type,\n                                                        seed=(wandb.config.seed+wandb.config.run)),\n                                       log_dir)])\n    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=5.)\n    env = VecVideoRecorder(env, video_folder=log_dir,\n                           record_video_trigger=lambda x: x % 100 == 0, video_length=10)  # record videos\n    stats_path = os.path.join(log_dir,\n                              \"run\" + str(wandb.config.run) + \"_vec_normalize_\" + run.id + \".pkl\")\n    env.save(stats_path)\n    return env\n<\/code><\/pre>\n<p>Can anyone point me in the right direction?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb for Huggingface Trainer saves only first model",
        "Question_creation_time":1650439111790,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Question_upvote_count":1.0,
        "Question_view_count":123.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Delete artifact in s3",
        "Question_creation_time":1636056284051,
        "Question_link":"https:\/\/community.wandb.ai\/t\/delete-artifact-in-s3\/1228",
        "Question_upvote_count":1.0,
        "Question_view_count":257.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I was wondering if it\u2019s possible to delete an artifact that is a reference to an S3 object.<br>\nI mean deleting it within weight and biases but also from S3.<\/p>\n<p>If it\u2019s not built-in, is there any webhook that I can use to delete my model in S3 when deleting it from W&amp;B?<\/p>\n<p>The flow would be :<\/p>\n<ol>\n<li>Train =&gt; Push metrics to W&amp;B, push model to S3, add the S3 reference of the model as an artifact in W&amp;B.<\/li>\n<li>Evaluate, do some comparison, ML magic, etc\u2026<\/li>\n<li>Deciding to delete the run as another was better. Delete in W&amp;B, which will remove all the run and all the files in S3 associated with this run.<\/li>\n<\/ol>\n<p>Thanks in advance for any help.<\/p>\n<p>Have a great day <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Boolean variables",
        "Question_creation_time":1660076747423,
        "Question_link":"https:\/\/community.wandb.ai\/t\/boolean-variables\/2874",
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am performing a hyperparameter sweep, and all is going well, except for an issue involving Boolean variables.<br>\nVariables that are True or False, do not appear as such in the run tables. Here are some images to demonstrate.<br>\nInstead, there are little horizontal dashes. So my question is whether one can include \u201cTrue\/False\u201d values for variables in configurations, and if so, how do they appear in the run tables displayed on <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a>?  Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Metric data exceeds maximum size",
        "Question_creation_time":1662622216472,
        "Question_link":"https:\/\/community.wandb.ai\/t\/metric-data-exceeds-maximum-size\/3082",
        "Question_upvote_count":0.0,
        "Question_view_count":407.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When I use wandb.log online, the following error will be reported: \u201cMetric data exceeds maximum size of 10.4MB\u201d. Now if I don\u2019t want to run again, how can I fix this mistake?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Upload model weights to the Artifacts of a finished run",
        "Question_creation_time":1654215967487,
        "Question_link":"https:\/\/community.wandb.ai\/t\/upload-model-weights-to-the-artifacts-of-a-finished-run\/2540",
        "Question_upvote_count":2.0,
        "Question_view_count":104.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I was training a yolov5 model, using the pre-configured wandb settings. But the weights weren\u2019t uploaded because the session was killed. I tried <code>wandb sync path\/to\/run<\/code> but the model file didn\u2019t get synced.<\/p>\n<p>I want to upload the resulting <code>best.pt<\/code> file to the artifacts regardless without messing up with the current summary and results of the finished run. I looked up in the documentation and tried multiple guides but couldn\u2019t manage to do that.<\/p>\n<p>TL;DR: I have a finished run and a weights file. I need to upload the weights file as a model artifact to that finished run using the run path.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where are artifacts stored locally?",
        "Question_creation_time":1641674779166,
        "Question_link":"https:\/\/community.wandb.ai\/t\/where-are-artifacts-stored-locally\/1733",
        "Question_upvote_count":0.0,
        "Question_view_count":268.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>As the title says, where are the artifacts stored locally when I save an artifact? How can I change its default location? It seems that changing <code>WANDB_DIR<\/code> does not change where artifacts are stored. On the other hand, I found many folders in <code>...\/wandb\/artifacts\/obj\/md5<\/code>,  what are these folders? Can I change its default saving location? Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to delete files like images and tables logged in the Files section",
        "Question_creation_time":1654356859657,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-delete-files-like-images-and-tables-logged-in-the-files-section\/2552",
        "Question_upvote_count":0.0,
        "Question_view_count":170.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey all,<br>\nHope everyone is doing well.<br>\nI was wondering if it\u2019s possible to delete files saved in the Files section of a run. It seems that the only option available is to download them locally.<br>\nI\u2019ve already checked out the docs and other posts here in the forum, but the few things I found referred specifically to artifacts like model checkpoints, whereas I\u2019m looking for a way to remove unwanted media files like images, tables and so on.<\/p>\n<p>Thanks in advance for your help!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"BUG: Parallel coordinates Panel in Sweep",
        "Question_creation_time":1666661845342,
        "Question_link":"https:\/\/community.wandb.ai\/t\/bug-parallel-coordinates-panel-in-sweep\/3318",
        "Question_upvote_count":0.0,
        "Question_view_count":104.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>My Sweep config looks like this:<\/p>\n<pre><code class=\"lang-yaml\">parameters:\n  pretraining_run_name:\n    values: [ns4bedao, 6cnr8gb9, lo4f6pma, qiha6oci]\n<\/code><\/pre>\n<p>As you can see, some of their names start with decimal.<br>\nThis makes the Parallel coordinates Panel broken.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/6\/671538bef4b0651502815871345bf48d5af62323.png\" data-download-href=\"\/uploads\/short-url\/eHUE5Lsun9I9dgN8ekmT7ipoEvN.png?dl=1\" title=\"\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-10-25 \u110b\u1169\u110c\u1165\u11ab 10.36.11\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/6\/671538bef4b0651502815871345bf48d5af62323_2_176x500.png\" alt=\"\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-10-25 \u110b\u1169\u110c\u1165\u11ab 10.36.11\" data-base62-sha1=\"eHUE5Lsun9I9dgN8ekmT7ipoEvN\" width=\"176\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/6\/671538bef4b0651502815871345bf48d5af62323_2_176x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/6\/671538bef4b0651502815871345bf48d5af62323_2_264x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/6\/671538bef4b0651502815871345bf48d5af62323.png 2x\" data-dominant-color=\"FAF9FB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2022-10-25 \u110b\u1169\u110c\u1165\u11ab 10.36.11<\/span><span class=\"informations\">320\u00d7906 34.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Is there any workaround that I can do immediately?<br>\nOr not, I wish this would be fixed as soon as possible.<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to find out if wandb service is down",
        "Question_creation_time":1635795445251,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-find-out-if-wandb-service-is-down\/1188",
        "Question_upvote_count":2.0,
        "Question_view_count":276.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Yesterday, wandb initialization was working for me both from my local desktop and on a remote EC2 instance. Today, I get the following error:<\/p>\n<pre><code class=\"lang-auto\">**wandb** : Network error (ReadTimeout), entering retry loop. See wandb\/debug-internal.log for full traceback.\n<\/code><\/pre>\n<p>I haven\u2019t changed anything about the settings, so this makes me think this is a wandb service issue. Is there a page on the website that tells the status of wandb services?<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to retrieve the `group` and `job_type` of a resumed run?",
        "Question_creation_time":1661874784218,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-retrieve-the-group-and-job-type-of-a-resumed-run\/3031",
        "Question_upvote_count":1.0,
        "Question_view_count":61.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I am inspecting and analysing my best runs. I expected that <code>group<\/code> and <code>job_type<\/code> would be populated with the resumed run\u2019s values after running the code below.<\/p>\n<pre><code class=\"lang-python\">run_id = input(\"id=\")\nwith wandb.init(entity=wandb_entity, project=wandb_project, id=run_id, resume=\"must\") as wandb_r:\n    config = wandb_r.config\n    group = wandb_r.group\n    job_type = wandb_r.job_type\n<\/code><\/pre>\n<p>Even though <code>config<\/code> is successfully recovered, <code>group<\/code> and <code>job_type<\/code> are just empty strings. How do I retrieve group and job_type values from WandB? Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can you edit config of a run after it finishes?",
        "Question_creation_time":1650054158436,
        "Question_link":"https:\/\/community.wandb.ai\/t\/can-you-edit-config-of-a-run-after-it-finishes\/2247",
        "Question_upvote_count":1.0,
        "Question_view_count":87.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>A very bad practice, I know. But for a part of my experiment, the main file wasn\u2019t updated so it ignored some configs. Is it possible to manually add them into the runs, now that the runs are finished?<\/p>\n<p>If it helps, I don\u2019t need to add new entries to the config. I just need to add to an existing string in the config.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Invalid apollo response An application error occurred",
        "Question_creation_time":1649792556780,
        "Question_link":"https:\/\/community.wandb.ai\/t\/invalid-apollo-response-an-application-error-occurred\/2228",
        "Question_upvote_count":1.0,
        "Question_view_count":82.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am getting \u201cinvalid apollo response An application error occurred.\u201d when accessing the any sweep via the UI. The rest seems to work, e.g. I can access all my runs in the UI. What is going on here, how can i fix this?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About the W&B Help category",
        "Question_creation_time":1631550250999,
        "Question_link":"https:\/\/community.wandb.ai\/t\/about-the-w-b-help-category\/539",
        "Question_upvote_count":1.0,
        "Question_view_count":720.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Ask your W&amp;B questions here. Let us know if you have an issue and one of our engineers or community experts will get back to you ASAP. If you have an urgent W&amp;B issue, please contact our support team at <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Get best model from artifacts",
        "Question_creation_time":1634323325709,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-best-model-from-artifacts\/992",
        "Question_upvote_count":5.0,
        "Question_view_count":303.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I guess this use-case is common but I cannot figure it out\u2026<\/p>\n<p>I would like to log one model per run, and in the end, be able to load the best overall model for production.<\/p>\n<p>So like :<br>\nRun 1,2,3,4,5\u2026<\/p>\n<pre><code class=\"lang-auto\">run.log_artifact(my_model_artifact)\n<\/code><\/pre>\n<p>Production:<\/p>\n<pre><code class=\"lang-auto\">artifact = api.artifact.get_best_of_all_my_runs()\n<\/code><\/pre>\n<p>For now my solution is :<\/p>\n<pre><code class=\"lang-auto\">Runs : \nartifact.save() # with the same name so only one artifact for all runs\n\nProduction\nartifact = api.artifact(\"entity\/project\/artifact:alias\") # Get the only model (which also should be the best)\n<\/code><\/pre>\n<p>Thanks in advance for any help.<br>\nhave a great day<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Some clarification about W&B starter-plan pricing",
        "Question_creation_time":1665161481915,
        "Question_link":"https:\/\/community.wandb.ai\/t\/some-clarification-about-w-b-starter-plan-pricing\/3227",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello everyone! I\u2019m currently evaluating W&amp;B as experiment tracking solution for the company I\u2019m currently working. So far, we found that W&amp;B cover all our basic needs so we would like to buy a starter plan. However before to move on,  I need some clarifications about the pricing that are not so clear from the info available on the website:<\/p>\n<ul>\n<li>\n<p>Tracked Hours:  for tier 1 plan there are 250 to 5000 cumulative tracked hours. These are counted per-user or shared by all the user of the team? In other words, can each user track up to 5000 hours of experiments each month? It seems to be a little bit confusing since the the bill is specified per user.<\/p>\n<\/li>\n<li>\n<p><a href=\"https:\/\/wandb.ai\/site\/pricing#lineage-tracking\">Storage\/Artifacts<\/a>: these 100GB of storage should be intended as shared between Storage &amp; Artifacts or there are a total of 200gb (100+100) of storage for each objects category. By the way, what is considered \u201cfiles saved to W&amp;B\u201d? Tracked metrics\/distributions\/parameters are included in this category?<\/p>\n<\/li>\n<\/ul>\n<p>Sorry for the silly questions but we want to be extra sure before to buy a license <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Change pointsizes, background, etc. in Object3D objects",
        "Question_creation_time":1631616541633,
        "Question_link":"https:\/\/community.wandb.ai\/t\/change-pointsizes-background-etc-in-object3d-objects\/564",
        "Question_upvote_count":1.0,
        "Question_view_count":257.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am logging pointclouds using numpy arrays following the shape [x,y,z,class], all integers.<br>\nLogging works, but the points are tiny and barely visible. Is it possible to make them larger?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb has no attribute login error",
        "Question_creation_time":1663548044682,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-has-no-attribute-login-error\/3146",
        "Question_upvote_count":0.0,
        "Question_view_count":182.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>For some reason, I\u2019m getting an error when I run wandb in a notebook that used to run fine. My error is:<\/p>\n<p>AttributeError                            Traceback (most recent call last)<br>\nInput In [11], in &lt;cell line: 1&gt;()<br>\n----&gt; 1 wandb.login(key=\u2018xxx\u2019)<\/p>\n<p>AttributeError: module \u2018wandb\u2019 has no attribute \u2018login\u2019<\/p>\n<p>Has anyone else encountered this?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Delete my account",
        "Question_creation_time":1647678914265,
        "Question_link":"https:\/\/community.wandb.ai\/t\/delete-my-account\/2116",
        "Question_upvote_count":0.0,
        "Question_view_count":102.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Can you delete my account please? Username is cengizk<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Lighting: Checkpoints silently fail to save",
        "Question_creation_time":1662073802656,
        "Question_link":"https:\/\/community.wandb.ai\/t\/lighting-checkpoints-silently-fail-to-save\/3048",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Every couple of hours I silently get this error during training then checkpoints silently fail to save from then on. I\u2019ve now lost &gt;30 hrs of training because of this weird issue. Does anyone know what\u2019s causing this and how to fix it?<\/p>\n<p>for future searchers:<br>\n<code>NVMLError_OperatingSystem: The operating system has blocked the request.<\/code><\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/40594953988cd414c66052042a88418ba4eda5a4.jpeg\" data-download-href=\"\/uploads\/short-url\/9bfQUuHL4Ek3Qjo5EjzA1z8ucRK.jpeg?dl=1\" title=\"IMG_0367\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/40594953988cd414c66052042a88418ba4eda5a4_2_690x497.jpeg\" alt=\"IMG_0367\" data-base62-sha1=\"9bfQUuHL4Ek3Qjo5EjzA1z8ucRK\" width=\"690\" height=\"497\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/40594953988cd414c66052042a88418ba4eda5a4_2_690x497.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/40594953988cd414c66052042a88418ba4eda5a4_2_1035x745.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/40594953988cd414c66052042a88418ba4eda5a4.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/40594953988cd414c66052042a88418ba4eda5a4_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">IMG_0367<\/span><span class=\"informations\">1284\u00d7925 205 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Runs log stops at 50",
        "Question_creation_time":1656936147013,
        "Question_link":"https:\/\/community.wandb.ai\/t\/runs-log-stops-at-50\/2696",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello, i am running wandb locally in my computer. I start a sweep and it runs smoothly, but when it reaches 50 runs it stops. Although kernel seems to be running it does not show any runs in the wandb site nor locally files in my computer. Does anyone know what seems to be the problem? I can provide any logs if requested, i don\u2019t know what to post and be helpful.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Delete files from a run",
        "Question_creation_time":1634631299024,
        "Question_link":"https:\/\/community.wandb.ai\/t\/delete-files-from-a-run\/1031",
        "Question_upvote_count":1.0,
        "Question_view_count":360.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I need to free up some space and want to delete an heavy file uploaded within a run.<br>\nI dont want to delete the whole logs of the runs, only this one heavy file<\/p>\n<p>How to ?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Agent bug? File not found error",
        "Question_creation_time":1647594182106,
        "Question_link":"https:\/\/community.wandb.ai\/t\/agent-bug-file-not-found-error\/2109",
        "Question_upvote_count":2.0,
        "Question_view_count":1996.0,
        "Question_answer_count":11,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi I\u2019m using kaggle with Pytorch and W&amp;B<\/p>\n<ul>\n<li>Weights and Biases version: 0.12.11<\/li>\n<li>Python version: 3.7.12<br>\n<strong>Description:<\/strong><br>\nWhen using the attached notebook I get the following error:<br>\n[<a href=\"https:\/\/www.kaggle.com\/code\/wojtekddl\/license-plate-w-b\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">License-plate-w&amp;b | Kaggle<\/a>]<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">wandb: Agent Starting Run: 9uvr1lj3 with config:\nwandb: \tbatch_size: 64\nwandb: \tdropout: 0.2\nwandb: \tdropout_lstm: 0.1\nwandb: \tepochs: 8\nwandb: \thidden_size: 32\nwandb: \tlinear_output: 64\nwandb: \tmodels: PlateLUX_2GRU\nwandb: \toptimizer: RMSprop\nwandb: \tscheduler: ReduceLROnPlateau\nwandb: Currently logged in as: wualas (use `wandb login --relogin` to force relogin)\nTracking run with wandb version 0.12.11\nRun data is saved locally in \/kaggle\/working\/wandb\/run-20220318_082708-9uvr1lj3\nSyncing run winter-sweep-1 to Weights &amp; Biases (docs)\nSweep page: https:\/\/wandb.ai\/wualas\/pytorch-sweeps-rejestracje_last\/sweeps\/7ioy5yu1\n\nWaiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nSynced winter-sweep-1: https:\/\/wandb.ai\/wualas\/pytorch-sweeps-rejestracje_last\/runs\/9uvr1lj3\nSynced 4 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nFind logs at: .\/wandb\/run-20220318_082708-9uvr1lj3\/logs\nRun 9uvr1lj3 errored: FileNotFoundError(2, 'No such file or directory')\nwandb: ERROR Run 9uvr1lj3 errored: FileNotFoundError(2, 'No such file or directory')\n<\/code><\/pre>\n<p>train_function:<\/p>\n<pre><code class=\"lang-auto\">def train(config=None):\n    with wandb.init(config=config):\n        config = wandb.config\n        df = pd.read_csv('\/content\/OCRdataset\/annotations_CRNN.csv')\n        df['filename'] = '\/content\/OCRdataset\/images\/' + df['filename'].astype(str)\n        image_files = df['filename'].tolist()\n        targets_orig = df['label'].tolist()\n        targets = [[c for c in x] for x in targets_orig]\n        targets_flat = [c for clist in targets for c in clist]\n\n        lbl_enc = preprocessing.LabelEncoder()\n        lbl_enc.fit(targets_flat)\n        targets_enc = [lbl_enc.transform(x) for x in targets]\n        targets_enc = np.array(targets_enc)\n        targets_enc = targets_enc + 1\n\n        (\n        train_imgs,\n        test_imgs,\n        train_targets,\n        test_targets,\n        _,\n        test_targets_orig,\n        ) = model_selection.train_test_split(\n        image_files, targets_enc, targets_orig, test_size=0.1, random_state=42\n        )\n        num_chars=len(lbl_enc.classes_)\n        train_loader, test_loader = build_loader(train_imgs, train_targets, test_imgs, test_targets, config.batch_size)\n        model = build_network(config.models, num_chars, config.linear_output, config.hidden_size, config.dropout, config.dropout_lstm)\n        optimizer = build_optimizer(model, config.optimizer)\n        scheduler = build_scheduler(optimizer, config.scheduler)\n\n        train_loss_tab = []\n        test_loss_tab = []\n        accuracy_tab = []\n        best_test_loss = 100000000000000\n        for epoch in range(config.epochs):\n            train_loss = train_fn(model, train_loader, optimizer)\n            valid_preds, test_loss = eval_fn(model, test_loader)\n            valid_captcha_preds = []\n            for vp in valid_preds:\n                current_preds = decode_predictions(vp, lbl_enc)\n                valid_captcha_preds.extend(current_preds)\n            combined = list(zip(test_targets_orig, valid_captcha_preds))\n            print(combined)\n            test_dup_rem = [remove_duplicates(c) for c in test_targets_orig]\n            accuracy = metrics.accuracy_score(test_dup_rem, valid_captcha_preds)\n            print(\n              f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy}\"\n            )\n            exloss = calculate_EXACTloss(combined)\n            scheduler.step(test_loss)\n            #torch.save(model.state_dict(), \"\/content\/epoch_save\/EPOCH_SAVER_CRNN_state_dict3{}.pt\".format(epoch))\n            # dopisac zapisywanie kazdego modelu\n            train_loss_tab.append(train_loss)\n            test_loss_tab.append(test_loss)\n            accuracy_tab.append(accuracy)\n            print(\"zapisuje\")\n            wandb.log({'epoch': epoch, 'loss_test': test_loss, 'loss_train': train_loss, 'accuracy': accuracy, 'EXACTacc' : exloss})\n<\/code><\/pre>\n<p>Is there an error how I\u2019m using wandb or is this a bug?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Violin plots are inaccurate",
        "Question_creation_time":1640853835684,
        "Question_link":"https:\/\/community.wandb.ai\/t\/violin-plots-are-inaccurate\/1644",
        "Question_upvote_count":0.0,
        "Question_view_count":195.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I really like the violin plots but there is something strange in my plots.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/19acb7ed3ad4d2c1f2fc8f2037098205c753e63a.png\" data-download-href=\"\/uploads\/short-url\/3F7Z1TdRo1hxg9aaZQGaFCcjA8i.png?dl=1\" title=\"screen_violin\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/19acb7ed3ad4d2c1f2fc8f2037098205c753e63a_2_690x358.png\" alt=\"screen_violin\" data-base62-sha1=\"3F7Z1TdRo1hxg9aaZQGaFCcjA8i\" width=\"690\" height=\"358\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/19acb7ed3ad4d2c1f2fc8f2037098205c753e63a_2_690x358.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/19acb7ed3ad4d2c1f2fc8f2037098205c753e63a_2_1035x537.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/19acb7ed3ad4d2c1f2fc8f2037098205c753e63a.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/19acb7ed3ad4d2c1f2fc8f2037098205c753e63a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">screen_violin<\/span><span class=\"informations\">1062\u00d7552 21.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nAll of these distributions have the lower bound at 0.0, however they appear to be \u2018randomly\u2019 translated of some shift. Moreover, I have realized that the maximum value that you can estimate by looking at the extremum of the plot is clearly not corresponding with the maximum value you get from the tabular data.<br>\nIs this intended or not?<\/p>\n<p>Thanks in advance<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Saving model's weights",
        "Question_creation_time":1644911065288,
        "Question_link":"https:\/\/community.wandb.ai\/t\/saving-models-weights\/1924",
        "Question_upvote_count":0.0,
        "Question_view_count":80.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI\u2019m training a BERT model and I\u2019m trying to save the weights to wandb\u2019s files tab in the end of the training.<br>\nHow can I accomplish that?<br>\nAlso - how can I load the weights from wandb\u2019s files tab?<br>\nI\u2019m using this code:<br>\n<a href=\"https:\/\/wandb.ai\/cayush\/bert-finetuning\/reports\/Sentence-classification-with-Huggingface-BERT-and-W&amp;B--Vmlldzo4MDMwNA\">https:\/\/wandb.ai\/cayush\/bert-finetuning\/reports\/Sentence-classification-with-Huggingface-BERT-and-W&amp;B\u2013Vmlldzo4MDMwNA<\/a><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Coordinate wandb local across two laptops",
        "Question_creation_time":1655327268687,
        "Question_link":"https:\/\/community.wandb.ai\/t\/coordinate-wandb-local-across-two-laptops\/2620",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi Wandb Community!<\/p>\n<p>I have a laptop in my office that I am able to run <code>wandb local<\/code> on and sync my ML experiments to my account email address. My company gave me another laptop to work from on the road and I would like to set up <code>wandb local<\/code> on that laptop to streamline ML experiments I do in office and on the road.<\/p>\n<p>On my laptop, I have <code>wandb<\/code> and <code>docker<\/code> installed successfully. I can also run <code>wandb local<\/code> successfully. However, I\u2019m not sure if I need to copy the same api key and license over for the single account to work on both machines. Is there a smart way to do this?<\/p>\n<p>Thanks in advance!<\/p>\n<p>wand: 0.12.18<br>\nOS: Ubuntu 22.04<br>\ndocker: 20.10.17<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Api key + entity verification",
        "Question_creation_time":1641324610203,
        "Question_link":"https:\/\/community.wandb.ai\/t\/api-key-entity-verification\/1687",
        "Question_upvote_count":0.0,
        "Question_view_count":212.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello Everyone,<\/p>\n<p>I\u2019ve been using this package for the past year to keep track of all my phd experiments (it is awesome!!!). I am in the process of developing an application (using streamlit) that makes use of my neural network framework more accessible to users.  For that reason, I want to provide users with the ability to use their wandb credentials to start logging results to their accounts. As far as I understand you need a valid API key and entity. Is there a way to verify that this API_key+entity combination exists?? Passing a random 40 character string in the key parameter of wandb.login() returns true, so I suspect that it only checks the length of the key and not if it actually exists. I guess I can try logging to a dummy project and then catch an exception (this means that the API key or the entity name is wrong) but I\u2019m looking for something more elegant.<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Rename username",
        "Question_creation_time":1668141278790,
        "Question_link":"https:\/\/community.wandb.ai\/t\/rename-username\/3409",
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello to all,<\/p>\n<p>I am new here. I thought I can change my username whenever I want when I signed up, so my username was informal. However, I couldn\u2019t find the place where I can change my username. I can only change my real name, institution, location, etc.<\/p>\n<p>Thank you very much<br>\nBest regards<br>\nJellerode<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hide Command from Overview Run Page Bug - Reopen",
        "Question_creation_time":1658864314475,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hide-command-from-overview-run-page-bug-reopen\/2802",
        "Question_upvote_count":0.0,
        "Question_view_count":51.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<aside class=\"quote\" data-post=\"1\" data-topic=\"2231\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"><\/div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/ed8c4c\/40.png\" class=\"avatar\">\n    <a href=\"https:\/\/community.wandb.ai\/t\/hide-command-from-overview-run-page\/2231\">Hide Command from Overview Run Page<\/a> <a class=\"badge-wrapper  bullet\" href=\"\/c\/w-b-support\/36\"><span class=\"badge-category-bg\" style=\"background-color: #0088CC;\"><\/span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"Ask your W&amp;B questions here. Let us know if you have an issue and one of our engineers or community experts will get back to you ASAP.\">W&amp;B Support<\/span><\/a>\n  <\/div>\n  <blockquote>\n    On the Run Page (<a href=\"https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page\">https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page<\/a>) it shows on the left incognito that it shouldn\u2019t show your command when the public is viewing your page. \nHowever, on my page, when public and I view as not-me, it still shows the command that launched it, and that includes my Windows username, which I\u2019d rather not. I can\u2019t find anything to override or hide this. What am I missing? \nThanks.\n  <\/blockquote>\n<\/aside>\n\n<p>Reopening the above, this still isn\u2019t fixed and it\u2019s still bothering me.<\/p>\n<p>Any updates?<\/p>\n<p><a class=\"mention\" href=\"\/u\/armanharutyunyan\">@armanharutyunyan<\/a><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweeps with multiple seeds for the same config values",
        "Question_creation_time":1635085861671,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweeps-with-multiple-seeds-for-the-same-config-values\/1077",
        "Question_upvote_count":6.0,
        "Question_view_count":721.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Sometimes (for example in RL) agents are very unstable and you only know how a config behaves if you tested it on 5-10 seeds. So I was wondering if there is a feature in wandb sweeps that allows the aggregation of a metric over multiple seeds (but the same config values)?<\/p>\n<p>I know one solution is to define a for loop in my own training script that repeats the same config, but I would like these runs to be executed in parallel, and possibly even on different machines.<\/p>\n<p>Thanks,<br>\nTom<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb in academic work as a PhD student with industry collaborations\/internship)?",
        "Question_creation_time":1654114178109,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-in-academic-work-as-a-phd-student-with-industry-collaborations-internship\/2529",
        "Question_upvote_count":2.0,
        "Question_view_count":123.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I was wondering on a use case most graduate students (PhD) in machine learning often come across and thought it would be better to open it up. We often do <strong>academic research<\/strong> (not for commercial) while doing an internship at a company while still affiliated to our university institution.<\/p>\n<p>In that case is it ok to use the academic teams we usually use during the semester for our internship?<\/p>\n<p>Does that fall this use:<\/p>\n<blockquote>\n<p>And guess what? W&amp;B is free for personal and academic use. (The latter is especially important for students and academics and something we\u2019ve championed since we started the company).<\/p>\n<\/blockquote>\n<p>from this site: <a href=\"https:\/\/wandb.ai\/ivangoncharov\/wandb-teams-for-students\/reports\/How-to-Use-W-B-Teams-For-Your-University-Machine-Learning-Projects-For-Free---VmlldzoxMjk1Mjkx\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>PS: I thought I had asked this already\u2026if yes link the question if it has an answer and my apologies before hand. If not I will remove this ps later.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Which stream is captured on Run Log page?",
        "Question_creation_time":1656344212897,
        "Question_link":"https:\/\/community.wandb.ai\/t\/which-stream-is-captured-on-run-log-page\/2669",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I noticed that there is a log page for each run. Which stream does the log page capture? In my case it seems only capture stderr but no stdout<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging multiline plots",
        "Question_creation_time":1661785923618,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-multiline-plots\/3025",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m looking for an elegant way to log multiline plots (e.g. showing AUROC and AUPRC on one plot) whilst training, and using the wandb.log() function. As far as I can tell, the only way is to keep track of the values from the start of training up until the current iteration, then log a wandb.plot.line (according to <a href=\"https:\/\/wandb.ai\/wandb\/plots\/reports\/Custom-Line-Plots--VmlldzoyNjk5NTA\" class=\"inline-onebox\">Weights &amp; Biases<\/a>), however this is a different interface from the standard use of wandb.log(), wherein you only give the latest value.<\/p>\n<p>Is there some way to do what I\u2019m looking for (multiline plots that update each iteration of training, and only need to be given the latest value for each variable)?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[BUG] Config param name inclusing a dot",
        "Question_creation_time":1633963376319,
        "Question_link":"https:\/\/community.wandb.ai\/t\/bug-config-param-name-inclusing-a-dot\/916",
        "Question_upvote_count":0.0,
        "Question_view_count":285.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a config param with the following name:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555.png\" data-download-href=\"\/uploads\/short-url\/dqR4ex4NYNlpeVlIDbLPCuUMzPL.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555_2_690x152.png\" alt=\"image\" data-base62-sha1=\"dqR4ex4NYNlpeVlIDbLPCuUMzPL\" width=\"690\" height=\"152\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555_2_690x152.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">893\u00d7198 10.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>However, when I call him in my graph, there is a bug and the graph stays white:<\/p>            <div class=\"onebox imgur-album\">\n              <a href=\"https:\/\/imgur.com\/a\/EIML6VO\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n                <span class=\"outer-box\" style=\"width:600px\">\n                  <span class=\"inner-box\">\n                    <span class=\"album-title\">[Album] imgur.com<\/span>\n                  <\/span>\n                <\/span>\n                <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2f6fef7dba514e4297115fb6caaa0d05a941ddf4.jpeg\" title=\"imgur.com\" height=\"315\" width=\"600\">\n              <\/a>\n            <\/div>\n",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is wanbd giving me a multiprocessing error when my code is serially running?",
        "Question_creation_time":1631828085267,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-wanbd-giving-me-a-multiprocessing-error-when-my-code-is-serially-running\/655",
        "Question_upvote_count":0.0,
        "Question_view_count":570.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a multiprocessing error but my code is not multiprocessing (its running serially) - even the pytorch dataloader as <code>num_workers=0<\/code> but I get this error:<\/p>\n<pre><code class=\"lang-auto\">N\/A% (0 of 100) |         | Elapsed Time: 0:00:00 | ETA:  --:--:-- |   0.0 s\/itTraceback (most recent call last):\n  File \"\/Users\/brando\/anaconda3\/envs\/metalearning\/lib\/python3.9\/multiprocessing\/spawn.py\", line 126, in _main\n    self = reduction.pickle.load(from_parent)\n  File \"\/Users\/brando\/anaconda3\/envs\/metalearning\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 110, in __setstate__\n    self._semlock = _multiprocessing.SemLock._rebuild(*state)\nFileNotFoundError: [Errno 2] No such file or directory\npython-BaseException\nTraceback (most recent call last):\n  File \"\/Users\/brando\/anaconda3\/envs\/metalearning\/lib\/python3.9\/multiprocessing\/spawn.py\", line 126, in _main\n    self = reduction.pickle.load(from_parent)\n  File \"\/Users\/brando\/anaconda3\/envs\/metalearning\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 110, in __setstate__\n    self._semlock = _multiprocessing.SemLock._rebuild(*state)\nFileNotFoundError: [Errno 2] No such file or directory\npython-BaseException\n<\/code><\/pre>\n<p>How do I start debugging this?<\/p>\n<hr>\n<p>I am running this in pycharm. Not sure what else to say, will think about it\u2026<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Artifacts (local) caching - how does it really work?",
        "Question_creation_time":1650203452610,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifacts-local-caching-how-does-it-really-work\/2255",
        "Question_upvote_count":1.0,
        "Question_view_count":390.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m trying to figure out how does the caching  of artifacts work. Let\u2019s say I want to download a model artifact to run some evaluation on. I don\u2019t need the file on disk to persist rather I just want to load it into memory. What I do right now in my evaluation script is:<\/p>\n<pre><code class=\"lang-auto\">import tempfile\nimport wandb\n\nartifact = wandb.use_artifact(model_weights_uri)\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    artifact.download(tmpdirname)\n    model_weights = load_pickle(os.path.join(tmpdirname, \"model_weights.pickle\"))\n<\/code><\/pre>\n<p>And from that point on I use the <code>model_weights<\/code> as it was loaded into memory.<\/p>\n<p>My first question is: if I run the code twice (on the same machine), <strong>will the model-weights be downloaded again<\/strong> or are they cached somewhere? assuming the logged artifact wasn\u2019t changed of course. And if they are cached, where are they cached?<br>\nI\u2019m also not clear about the <code>artifact<\/code> directory (which is used if I run <code>artifact.download()<\/code> without any argument). Does that directory serve as cache? if so, what does the <code>.cache<\/code> directory used for?<\/p>\n<p>I would appreciate answers to my questions and perhaps a  general explanation of the artifact caching mechanism &amp; best practices.<\/p>\n<p>Thanks!<br>\nRan<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to log custom criterion function?",
        "Question_creation_time":1657061987579,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-custom-criterion-function\/2703",
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>We can use <code>wandb.watch(model, criterion, ...)<\/code> in order to log a model + a loss function.<br>\nBut my loss function is not something simple like: <code>criterion = nn.CrossEntropyLoss()<\/code>.<\/p>\n<p>Rather, here\u2019s how I calculate my loss:<\/p>\n<pre><code class=\"lang-auto\">            # `set_to_none=True` boosts performance\n            optimizer.zero_grad(set_to_none=True)\n            masks_pred = model(imgs)\n\n            probs = F.softmax(masks_pred, dim=1).float()\n            ground_truth = F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float()\n\n            loss = criterion(masks_pred, masks) + dice_loss(probs, ground_truth)\n            loss.backward()\n            optimizer.step()\n<\/code><\/pre>\n<p>As you can see, the loss is a composition of 2 functions: the criterion and the <code>dice_loss<\/code> function.<br>\nWhat should I pass to <code>wandb.watch<\/code> for the <code>criterion<\/code> argument?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb upload limit \/ request limit should not stop execution of script",
        "Question_creation_time":1650796195742,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-upload-limit-request-limit-should-not-stop-execution-of-script\/2311",
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>my scripts regularly slow down significantly because I run into the wandb upload limit\/ request limit.<br>\nE.g. getting <code>429 encountered (Filestream rate limit exceeded, retrying in 4.902817452929678 seconds), retrying request<\/code><\/p>\n<p>Is there a way to set wandb to \u201csoft uploads\u201d, i.e. uploading data whenever possible but never stopping\/pausing the execution of the main script?<\/p>\n<p>Thanks so much!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Population Based Training",
        "Question_creation_time":1658207885013,
        "Question_link":"https:\/\/community.wandb.ai\/t\/population-based-training\/2773",
        "Question_upvote_count":0.0,
        "Question_view_count":85.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m interested in using population-based training as a WandB Sweep methodology as referenced in the article <a href=\"https:\/\/wandb.ai\/wandb\/DistHyperOpt\/reports\/Modern-Scalable-Hyperparameter-Tuning-Methods--VmlldzoyMTQxODM\">here<\/a>.<\/p>\n<p>However, I can\u2019t find any mention or tutorials for PBT with WandB outside of this article. I\u2019m interested in using PBT with a <a href=\"https:\/\/github.com\/spaceml-org\/Self-Supervised-Learner\" rel=\"noopener nofollow ugc\">self-supervised learner<\/a> and configuring the sweep in a YAML. Does anyone have any advice on how to go about this? I\u2019m familiar with configuring sweeps in a YAML or jupyter notebook but, I\u2019m unsure how to make the PBT scheduler and tuner (as outlined in the article) compatible with the SSL code I\u2019m using which is run from the command line.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Reverse deleted projects",
        "Question_creation_time":1660069667533,
        "Question_link":"https:\/\/community.wandb.ai\/t\/reverse-deleted-projects\/2872",
        "Question_upvote_count":0.0,
        "Question_view_count":78.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey,<br>\nWondering is there anyway to restore deleted projects with artifacts ?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Export panel between sweeps",
        "Question_creation_time":1649060013695,
        "Question_link":"https:\/\/community.wandb.ai\/t\/export-panel-between-sweeps\/2186",
        "Question_upvote_count":1.0,
        "Question_view_count":112.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Is there a way to export custom plots I created in one sweep to the dashboard of another?<br>\nSpecifically, I used the \u201cadd section\u201d button to create a new section, and added custom plots to it. I would like to move this section \\ panel as a whole to another sweep.<\/p>\n<p>Is it possible somehow? It could save me a lot of time instead of setting it again and again for every sweep.<\/p>\n<p>Thanks,<br>\nTom<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Information in tables disappearing",
        "Question_creation_time":1649654781916,
        "Question_link":"https:\/\/community.wandb.ai\/t\/information-in-tables-disappearing\/2215",
        "Question_upvote_count":0.0,
        "Question_view_count":107.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello!<\/p>\n<p>I have 39 runs in an experiment, all correctly finished with the corresponding tables per run correctly uploaded and available (when clicking on the specific run).<\/p>\n<p>I have 4 different tables, and combine the runs depending on the table ID (runs .summary[\u201ca\u201d], runs .summary[\u201cb\u201d], \u2026).<\/p>\n<p>2 of the combined tables are correctly outputted, but in 1 of the tables there is half the data available, and in the other a message of \u201cno rows to display\u201d is shown. If I re-upload the data of the \u201cno rows to display\u201d table, it is shown properly, but another table becomes empty, with the \u201cno rows to display\u201d message.<\/p>\n<p>Probably is due to the amount of rows can be processed at the same time? The total number of rows per run is 42, so 39x42 = 1638, which shouldn\u2019t be that much?<\/p>\n<p>Thank you!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot find a specific column in sweep -> sweep table",
        "Question_creation_time":1660449640995,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-find-a-specific-column-in-sweep-sweep-table\/2925",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When searching, I first hide all the columns (because in total there are more than 500 entries, which cannot be fully visible), and then I search for my intended column, but I find that I cannot find it. What\u2019s weird is that I didn\u2019t change my code and i am pretty sure I can see them before. Also, I can see that entry (i.e., a bleu result) in the charts. What\u2019s happening?<\/p>\n<p>Actually, I can see that entry appears for a millisecond, and then disappears. So, I guess that the table can only show arguments instead of result from now on?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"UI : How to drag\/move in a plot?",
        "Question_creation_time":1634543420154,
        "Question_link":"https:\/\/community.wandb.ai\/t\/ui-how-to-drag-move-in-a-plot\/1021",
        "Question_upvote_count":0.0,
        "Question_view_count":262.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Let\u2019s say I\u2019m watching a plot.<br>\nThen I zoom in a specific section within the plot.<br>\nThen I want to look a bit more to the right.<br>\nWhat kind of key\/mouse button allows me to move my view to the right  ?<br>\nMiddle mouse button does not work <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb Tables to Latex Tables, anyway?",
        "Question_creation_time":1634821750155,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-tables-to-latex-tables-anyway\/1060",
        "Question_upvote_count":2.0,
        "Question_view_count":718.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I discovered Wandb few years back but I finally sat down to integrate everything with my experiments. I am loving this so much! Something I\u2019d like to know is if there is anyway to export tables to Latex from Wandb itself.<\/p>\n<p>I\u2019m in ML research and I have to include my results in Latex tables for papers. So it would be really cool if I could export them from Wandb, just like reports or graphs. I\u2019m open to suggestions you may have. thank you<\/p>\n<p>(willing to submit a feature request as well)<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Parallel coordinate plot doesn't work with nested groupings",
        "Question_creation_time":1663059079631,
        "Question_link":"https:\/\/community.wandb.ai\/t\/parallel-coordinate-plot-doesnt-work-with-nested-groupings\/3102",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>I have tried to use a parallel coordinate plot with a nested group, and it only shows the top-level group, whereas line plots correctly show multiple level groups. See screenshot below for example\u2014there are 3 lines on the parallel coordinates, but 6 on the line plot. Is this correct behaviour?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda.png\" data-download-href=\"\/uploads\/short-url\/oRjaIBHNsSCjMDTUurlQOHfluci.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_503x500.png\" alt=\"image\" data-base62-sha1=\"oRjaIBHNsSCjMDTUurlQOHfluci\" width=\"503\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_503x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_754x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_1006x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1042\u00d71034 127 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb: ERROR Failed to sample metric: psutil.NoSuchProcess process no longer exists (pid=453)",
        "Question_creation_time":1667941612934,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-error-failed-to-sample-metric-psutil-nosuchprocess-process-no-longer-exists-pid-453\/3393",
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am running some NLP models and simply using wandb to log the errors during these modelings. I am receiving  the following error while logging:<\/p>\n<p><code>wandb: ERROR Failed to sample metric: psutil.NoSuchProcess process no longer exists (pid=453)<\/code><\/p>\n<p>I appreciate your help in fixing it.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Histogram across steps with in a run",
        "Question_creation_time":1664946194952,
        "Question_link":"https:\/\/community.wandb.ai\/t\/histogram-across-steps-with-in-a-run\/3219",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>As we prepare our datasets we are using WandB to track results of different runs.  These data-prep runs last several days across 10M+ frames and we would like to log the ongoing results to histograms.  For example, we want to generate histograms of statistics metrics across all data frames.<br>\nThe simplest technique would seem to be to log each parameter step-by-step for each frame using <code>wandb.log()<\/code>.  But the workspace Panel (\u201cAdd Panel\u201d) does not not support histograms.<br>\nI\u2019ve read the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/media#histograms\">section on histograms<\/a> repeatedly and I can\u2019t help but conclude that continuous tracking of histograms is not supported.  Which seems odd.<br>\nThe documentation suggests that I should accumulate the elements in a local list, generate a histogram with each frame (locally) and to then log the histogram to WandB. Basically we are just plotting the histogram locally and pushing a figure. This seems inefficient for large data sets.<br>\nWandB would seem to be well suited for tracking statistics across steps and runs using histograms.<br>\nBut perhaps I have not yet found it in the documentation.  Can you provide some guidance?<\/p>\n<p>To reiterate, I want to log a several statistics every step across many (millions) of steps.  I wish to plot a histogram of these values across all steps.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Example code for how to set up logging processes (cross validation folds) and grouping them?",
        "Question_creation_time":1644680260014,
        "Question_link":"https:\/\/community.wandb.ai\/t\/example-code-for-how-to-set-up-logging-processes-cross-validation-folds-and-grouping-them\/1908",
        "Question_upvote_count":0.0,
        "Question_view_count":141.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am new to wandb and I am trying to figure out how to set up logging processes (based on cross validation folds) and to group them. What I would like to do is to plot\/visualise performances for each fold in a cross validation scheme.<\/p>\n<p>In this Colab example notebook<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/scikit\/Simple_Scikit_Integration.ipynb\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9d23677f636eedb4d570ae645da788122519566f.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/scikit\/Simple_Scikit_Integration.ipynb\" target=\"_blank\" rel=\"noopener nofollow ugc\">colab.research.google.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e2eb089e1834a0581da1d893b1624f376f01ad6a.png\" class=\"thumbnail onebox-avatar\" width=\"260\" height=\"260\">\n\n<h3><a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/scikit\/Simple_Scikit_Integration.ipynb\" target=\"_blank\" rel=\"noopener nofollow ugc\">Google Colaboratory<\/a><\/h3>\n\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>at the very bottom the section \u201cBasic Setup\u201d says in point 2: \"Groups: For multiple processes or cross validation folds, log each process as a runs and group them together. <code>wandb.init(group='experiment-1')<\/code>\". I am not quite sure how to do this. I searched the documentation, but I was not successful. Can anyone point me to some example code how to do this?<\/p>\n<p>Basically, what I am interested in is to visualise ROC, etc. for each fold and compare how much they differ.<\/p>\n<p>Thanks in advance!<br>\nOliver<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Grouping and comparing across two dimensions",
        "Question_creation_time":1654091611752,
        "Question_link":"https:\/\/community.wandb.ai\/t\/grouping-and-comparing-across-two-dimensions\/2525",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have two hyperparameters, lets say p1 with values {A, B,C} and p2 with values {0, 1} and a metric I want to compare. Now I want to  group my runs to visualize how p2 affects the metric for each value of p1 separately. Something like on the image below.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6fc197c9a83f96d5196805c315c0edb402521d2d.png\" alt=\"wandb_classes\" data-base62-sha1=\"fWDLKpRDMuRYnyft4eMiPsGXkbH\" width=\"371\" height=\"250\"><\/p>\n<p>When I group my runs by both p1 and p2 and do a bar plot I lose the information which pairs of bars I want to compare. Is there any way to achieve what I want on a report?<br>\nThe best I could think of is to have separate sets of runs each filtered on A, B, C and then do multiple plots with just the two bars. But I\u2019d much rather have all the information together.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom HTML with three.js for interactive 3D asset visualization",
        "Question_creation_time":1650291491993,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-html-with-three-js-for-interactive-3d-asset-visualization\/2258",
        "Question_upvote_count":0.0,
        "Question_view_count":265.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I am working on a 3D reconstruction task that returns multiple 3D meshes in my visualization. The custom W&amp;B 3D visualization is too bright and it is not helpful for tasks that require to visualize texture; also, it seems to reduce the triangle counts, but triangle counts are important for 3D vision.<\/p>\n<p>It seems that W&amp;B supports uploading custom HTML.  I am wondering if it is possible to:<\/p>\n<ol>\n<li>Upload my predicted 3D meshes for each step.<\/li>\n<li>Display my 3D meshes using a custom HTML that using <a href=\"https:\/\/threejs.org\/examples\/#webgl_animation_keyframes\" rel=\"noopener nofollow ugc\">three.js<\/a> to allow me interact with it.<\/li>\n<\/ol>\n<p>Potential blockers:<\/p>\n<ul>\n<li>I assume <code>wandb.Object3D<\/code> is used with <code>wandb.log<\/code> to upload my 3D prediction. In my HTML, how do I get access to the uploaded meshes?<\/li>\n<li>Similarly, how do I upload and get access to a js library in the HTML? (the paths)<\/li>\n<\/ul>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"W&B support vs help?",
        "Question_creation_time":1633115724242,
        "Question_link":"https:\/\/community.wandb.ai\/t\/w-b-support-vs-help\/844",
        "Question_upvote_count":2.0,
        "Question_view_count":280.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>what is the difference between the categories?<\/p>\n<p>which one to use?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Early Stopping",
        "Question_creation_time":1630705196042,
        "Question_link":"https:\/\/community.wandb.ai\/t\/early-stopping\/422",
        "Question_upvote_count":13.0,
        "Question_view_count":1204.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello All,<\/p>\n<p>I\u2019m configuring a hyper parameter sweep. I have training, validation, and test set.<\/p>\n<p>I\u2019d like to use the test_loss as the final metric to optimize and val_loss for early stopping.<\/p>\n<p>I don\u2019t see a place to specify a metric for early stopping. Does it default to the same metric specified for overall optimization (of hyper parameters)? If so, how can I change this?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to group runs (e.g., different random seeds) together on the wandb report function for plots?",
        "Question_creation_time":1655577992183,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-group-runs-e-g-different-random-seeds-together-on-the-wandb-report-function-for-plots\/2634",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am running some experiments where I have multiple random seeds per experiment setting, so I am trying to group the runs together to get their average and standard deviation (this is standard in reinforcement learning research these days). However, I can\u2019t seem to figure out how to get this to reliably work on wandb \u2013 sometimes it works and sometimes it doesn\u2019t.<\/p>\n<p>For reference, this is the kind of plot that I am trying to generate:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7dfb019e4801f1aa0f88fc7e4a50aa7df66f68eb.jpeg\" data-download-href=\"\/uploads\/short-url\/hYtsFlCoHHY8wJNPJseLDMnC29R.jpeg?dl=1\" title=\"Screen Shot 2022-06-18 at 2.32.22 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7dfb019e4801f1aa0f88fc7e4a50aa7df66f68eb_2_565x500.jpeg\" alt=\"Screen Shot 2022-06-18 at 2.32.22 PM\" data-base62-sha1=\"hYtsFlCoHHY8wJNPJseLDMnC29R\" width=\"565\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7dfb019e4801f1aa0f88fc7e4a50aa7df66f68eb_2_565x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7dfb019e4801f1aa0f88fc7e4a50aa7df66f68eb_2_847x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7dfb019e4801f1aa0f88fc7e4a50aa7df66f68eb.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7dfb019e4801f1aa0f88fc7e4a50aa7df66f68eb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-06-18 at 2.32.22 PM<\/span><span class=\"informations\">1070\u00d7946 97.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>There are two overall curves, but these are averaged among several runs, which is why you see a shaded region for standard deviation.<\/p>\n<p>I make this figure in a wandb report by going to the panel grid and assigning different runs together to a group manually. Here is a screen recording of the process of how I try to do this.<\/p>\n<aside class=\"onebox googledrive\" data-onebox-src=\"https:\/\/drive.google.com\/file\/d\/10mX_1EqWC5UUchSy2giluDv8lvKvvA2a\/view?usp=sharing\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/drive.google.com\/file\/d\/10mX_1EqWC5UUchSy2giluDv8lvKvvA2a\/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\">drive.google.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https:\/\/drive.google.com\/file\/d\/10mX_1EqWC5UUchSy2giluDv8lvKvvA2a\/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\"><span class=\"googledocs-onebox-logo g-drive-logo\"><\/span><\/a>\n\n\n\n<h3><a href=\"https:\/\/drive.google.com\/file\/d\/10mX_1EqWC5UUchSy2giluDv8lvKvvA2a\/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\">wandb_cannot_group.mov<\/a><\/h3>\n\n<p>Google Drive file.<\/p>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Here, I\u2019m showing another set of runs that I\u2019m trying to group together (it\u2019s about 15 total individual runs, but in 3-4 groups, so I\u2019m trying to group the curves). However, clicking on the \u201cRuns\u201d button means nothing changes! This is strange since it\u2019s how I do this to create my other grouping plots. Sometimes it works, sometimes it does not. Does anyone have suggestions on how to make this function work more reliably? Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to invert the logged images?",
        "Question_creation_time":1660223927393,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-invert-the-logged-images\/2907",
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>is it possible to invert my logged images ? Or is there maybe a way to do that in pytorch ?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Image plotting tells lies(samples are not from the same iteration when not trained)!",
        "Question_creation_time":1646782965898,
        "Question_link":"https:\/\/community.wandb.ai\/t\/image-plotting-tells-lies-samples-are-not-from-the-same-iteration-when-not-trained\/2047",
        "Question_upvote_count":0.0,
        "Question_view_count":138.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>As you can see, when I push the image slider to 13500, while the red model is not trained yet to that iteration, the wandb shows a sample for it. I guess it is the last sample of the model, but wandb I guess at least show exactly from which iteration this image is coming from because I think this way is misleading. I mean, if someone does not pay attention they may think they are comparing the quality of the samples at the same number of iterations.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f47d3c9714bbc13b8a32401fc97695379b5f804d.jpeg\" data-download-href=\"\/uploads\/short-url\/ySQOUszDVHpVqz8QnJLmnU2HItf.jpeg?dl=1\" title=\"Screen Shot 2022-03-08 at 6.27.10 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f47d3c9714bbc13b8a32401fc97695379b5f804d_2_690x447.jpeg\" alt=\"Screen Shot 2022-03-08 at 6.27.10 PM\" data-base62-sha1=\"ySQOUszDVHpVqz8QnJLmnU2HItf\" width=\"690\" height=\"447\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f47d3c9714bbc13b8a32401fc97695379b5f804d_2_690x447.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f47d3c9714bbc13b8a32401fc97695379b5f804d_2_1035x670.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f47d3c9714bbc13b8a32401fc97695379b5f804d_2_1380x894.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f47d3c9714bbc13b8a32401fc97695379b5f804d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-03-08 at 6.27.10 PM<\/span><span class=\"informations\">2332\u00d71512 177 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Artifact Referencing",
        "Question_creation_time":1651864251629,
        "Question_link":"https:\/\/community.wandb.ai\/t\/azure-artifact-referencing\/2376",
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Following doesn\u2019t work for us. Is their any way to solve the following problems for Artifact reference with  <strong>Azure Blob Storage<\/strong>:<\/p>\n<ol>\n<li>\n<p>How should we pass credentials to wandb ? As for Amazon S3 and GCS the priority and env variables are mentioned in docs.<\/p>\n<\/li>\n<li>\n<p>It was recommended that by passing <strong>az:\/\/<\/strong> as prefix will work similar to whats done s3 bucket and gcs.  However I didn\u2019t see any storage handler in wandb code for azure. I wonder how would it work just by passing a prefix ? Furthermore, unlike boto for s3 and google-cloud-storage sdk for gcs. I don\u2019t see any requirement of azure-storage in requirements.txt. Is their any Microsoft Azure Storage SDK for Python somewhere in code that I can not find ??<\/p>\n<\/li>\n<li>\n<p>Although it doesn\u2019t make any sense still I gave it a try, and as expected. Following are the results.<\/p>\n<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">run = wandb.init(project=\"Dummy_Training\", job_type=\"upload\")\nbucket = 'az:\/\/azurestorage.blob.core.windows.net\/container_name'\ndataset_at = wandb.Artifact('sample',type=\"raw_data\")\n\ndataset_at.add_reference(bucket)\nrun.log_artifact(dataset_at)\nrun.finish()\n<\/code><\/pre>\n<p>And I get the following Error<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d74321689bbd477953afb77691a5b5ee70505085.jpeg\" data-download-href=\"\/uploads\/short-url\/uIitV3rWGuO3hAvgAkyuvwPTnF3.jpeg?dl=1\" title=\"6853D58492F2404D8EAC586087E55373\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d74321689bbd477953afb77691a5b5ee70505085_2_682x500.jpeg\" alt=\"6853D58492F2404D8EAC586087E55373\" data-base62-sha1=\"uIitV3rWGuO3hAvgAkyuvwPTnF3\" width=\"682\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d74321689bbd477953afb77691a5b5ee70505085_2_682x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d74321689bbd477953afb77691a5b5ee70505085_2_1023x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d74321689bbd477953afb77691a5b5ee70505085.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d74321689bbd477953afb77691a5b5ee70505085_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">6853D58492F2404D8EAC586087E55373<\/span><span class=\"informations\">1202\u00d7881 264 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<ol start=\"4\">\n<li>Do I need to pass something in name parameter? What would be the entry name for azure?<br>\nSeems like az:\/\/ is defiantly not in your known handlers<\/li>\n<\/ol>\n<p><strong>Is their any way for Azure Artifact Referencing (azure blob storage) to work. And please let me know if their is any thing that I am missing. Any example for the resolution of this problem will be much appreciated.<\/strong><\/p>\n<p>Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to control grid sweep's parameters order?",
        "Question_creation_time":1640902333984,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-control-grid-sweeps-parameters-order\/1648",
        "Question_upvote_count":0.0,
        "Question_view_count":170.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have several parameters that I intend to grid sweep e.g. batch size, learning rate and optimizer. How can one specify the order in which the parameters are changed?<\/p>\n<p>For instance, suppose I think choice of optimizer will matter least and learning rate will matter most. I\u2019d like to try all possible learning rates on one optimizer before moving on to the next optimizer. How do I do this?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Fail to show images when logging images in a project created in teams mode",
        "Question_creation_time":1645005003760,
        "Question_link":"https:\/\/community.wandb.ai\/t\/fail-to-show-images-when-logging-images-in-a-project-created-in-teams-mode\/1936",
        "Question_upvote_count":0.0,
        "Question_view_count":95.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We created a team and then added a project. while everything works when working with our personal space separately, in the project we created in the team space, images are not shown. Any ideas on how to fix it?<\/p>\n<p>please let me know what type of information I should provide here.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb Agent - some runs fail",
        "Question_creation_time":1635197682205,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-agent-some-runs-fail\/1094",
        "Question_upvote_count":0.0,
        "Question_view_count":264.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve been using the Wandb agent to do some hyperparameter optimization with sweep config. Somehow approx. 70% of my runs fails.  This message is persisted:<\/p>\n<p>wandb: ERROR Run kyg8jl2m errored: InternalError()<\/p>\n<p>Is this a known issue?<\/p>\n<p>edit: I just did another 20 runs<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6fa137232597a0d819fa8d93bcea31af5fc491c8.png\" alt=\"image\" data-base62-sha1=\"fVwoTFqCbnNIdB63Atkf4ye1nm0\" width=\"585\" height=\"211\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Mutually exclusive parameters for sweeps",
        "Question_creation_time":1658961338889,
        "Question_link":"https:\/\/community.wandb.ai\/t\/mutually-exclusive-parameters-for-sweeps\/2808",
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Can I pass exclusive parameters for a sweep? E.g. for a particular pre-trained model, I want to try learning rate values of [0.1, 0.2]. For another model I want to use [0.3, 0.4].  if I use the sweep configuration below, then grid search will try all the four learning rate values for each model. However, for model 1 - I want to use a learning rate of 0.1, 0.2 whereas, for model2, I want to use 0.3, 0.4.<\/p>\n<p>project: my_project<br>\nprogram: main.py<br>\nname: grid_search<br>\nmethod: grid<br>\nparameters:<br>\nlearning_rate:<br>\nvalues: [0.1, 0.2, 0.3, 0.4]<br>\narch:<br>\nvalues: [\u2018model1\u2019, \u2018model2\u2019]<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Force Bayesian sweep to run certain variable tests",
        "Question_creation_time":1662970178525,
        "Question_link":"https:\/\/community.wandb.ai\/t\/force-bayesian-sweep-to-run-certain-variable-tests\/3098",
        "Question_upvote_count":0.0,
        "Question_view_count":47.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi!<br>\nI want to run a bayesian HP sweep with 5-fold CV. In other words I want the bayesian sweep to decide upon a configuration, run 5 runs with that configuration and log each run. The easiest way to do this would be to have a variable in the sweep, called e.g. fold_id which simply can take the values 1,2,3,4,5 and force the agent to always test all the fold_ids per configuration.<\/p>\n<p>Is there any way to make this possible? I.e force the sweep agent to always test a variable, even though running a bayesian sweep. In a way it would be like running a grid sweep over a bayesian sweep.<\/p>\n<p>One way I\u2019ve thought of is by making all parameters nested inside the fold_id variable but it still won\u2019t probably do what I\u2019m after.<\/p>\n<p>I\u2019ve seen the k-fold CV example code, but it\u2019s quite advanced and does not seem to work when running on CUDA and my understanding of multiprocessing is limited.<\/p>\n<p>Thank you!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Scaling in the parelles coordinates plot",
        "Question_creation_time":1666083248386,
        "Question_link":"https:\/\/community.wandb.ai\/t\/scaling-in-the-parelles-coordinates-plot\/3274",
        "Question_upvote_count":0.0,
        "Question_view_count":256.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I want to have custom value ranges for my hyperparameters, for example, for all GCN layers, I want it to range from 0 to 500.  I also want accuracy to range from 0-100. Would this be possible in the plot?<\/p>\n<p>Thanks.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/2fa3a6c300fee74cd3820b8960ba4c2c662641ce.jpeg\" data-download-href=\"\/uploads\/short-url\/6Nr3SOp7uDdsYf6W6V2DDTkiMsS.jpeg?dl=1\" title=\"Screen Shot 2022-10-18 at 7.52.00 pm\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/2\/2fa3a6c300fee74cd3820b8960ba4c2c662641ce_2_690x224.jpeg\" alt=\"Screen Shot 2022-10-18 at 7.52.00 pm\" data-base62-sha1=\"6Nr3SOp7uDdsYf6W6V2DDTkiMsS\" width=\"690\" height=\"224\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/2\/2fa3a6c300fee74cd3820b8960ba4c2c662641ce_2_690x224.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/2\/2fa3a6c300fee74cd3820b8960ba4c2c662641ce_2_1035x336.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/2\/2fa3a6c300fee74cd3820b8960ba4c2c662641ce_2_1380x448.jpeg 2x\" data-dominant-color=\"EABFC3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-10-18 at 7.52.00 pm<\/span><span class=\"informations\">1574\u00d7512 141 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Hide Command from Overview Run Page",
        "Question_creation_time":1649811239532,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hide-command-from-overview-run-page\/2231",
        "Question_upvote_count":0.0,
        "Question_view_count":104.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>On the Run Page (<a href=\"https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page\">https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page<\/a>) it shows on the left incognito that it shouldn\u2019t show your command when the public is viewing your page.<\/p>\n<p>However, on my page, when public and I view as not-me, it still shows the command that launched it, and that includes my Windows username, which I\u2019d rather not. I can\u2019t find anything to override or hide this. What am I missing?<\/p>\n<p>Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to visualize HTML run in Kubeflow Pipeline?",
        "Question_creation_time":1643806405410,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-visualize-html-run-in-kubeflow-pipeline\/1862",
        "Question_upvote_count":0.0,
        "Question_view_count":540.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<\/p>\n<p>We use Pytorch Lightning for training and we use Kubeflow Pipelines and are thinking about using wandb to track and visualize the training and test metrics.<\/p>\n<p>Kubeflow pipelines offers the possibility to view a static html page (see <a href=\"https:\/\/www.kubeflow.org\/docs\/components\/pipelines\/sdk\/output-viewer\/#single-html-file\" rel=\"noopener nofollow ugc\">this link<\/a> ).<br>\nI was wondering if it would be possible via the wandb python sdk to get a read-only embeded code (iframe) that I could then simply pass to Kubeflow pipeline sdk to show the html ?<\/p>\n<p>Thanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Support for ComplexFloat",
        "Question_creation_time":1666115047645,
        "Question_link":"https:\/\/community.wandb.ai\/t\/support-for-complexfloat\/3279",
        "Question_upvote_count":0.0,
        "Question_view_count":89.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Python 3.9.7<br>\nWandb 0.13.4<br>\nPytorch 1.12.0+cu116<\/p>\n<p>I am running a neural network over complex valued data and getting the following error:<\/p>\n<pre><code class=\"lang-python\">Traceback (most recent call last):\n  File \"\/home\/aclifton\/rf_fp\/run_training_w_evaluate.py\", line 523, in &lt;module&gt;\n    run_training_pipeline(tmp_dict)\n  File \"\/home\/aclifton\/rf_fp\/run_training_w_evaluate.py\", line 229, in run_training_pipeline\n    outputs = rffp_model(**batch)\n  File \"\/home\/aclifton\/anaconda3\/envs\/rffp\/lib\/python3.9\/site-packages\/torch\/nn\/modules\/module.py\", line 1151, in _call_impl\n    hook_result = hook(self, input, result)\n  File \"\/home\/aclifton\/anaconda3\/envs\/rffp\/lib\/python3.9\/site-packages\/wandb\/wandb_torch.py\", line 110, in &lt;lambda&gt;\n    lambda mod, inp, outp: parameter_log_hook(\n  File \"\/home\/aclifton\/anaconda3\/envs\/rffp\/lib\/python3.9\/site-packages\/wandb\/wandb_torch.py\", line 105, in parameter_log_hook\n    self.log_tensor_stats(data.cpu(), \"parameters\/\" + prefix + name)\n  File \"\/home\/aclifton\/anaconda3\/envs\/rffp\/lib\/python3.9\/site-packages\/wandb\/wandb_torch.py\", line 221, in log_tensor_stats\n    tmin = flat.min().item()\nRuntimeError: \"min_all\" not implemented for 'ComplexFloat'\n<\/code><\/pre>\n<p>I\u2019m not quite sure what to make of it and was wondering if anyone could offer some advice? Thanks in advance for your help!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to save\/restore model using artifact on servers that do not have internet access?",
        "Question_creation_time":1637040280243,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-save-restore-model-using-artifact-on-servers-that-do-not-have-internet-access\/1313",
        "Question_upvote_count":0.0,
        "Question_view_count":265.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I just started using wandb tools. According to the instruction <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/save\">here<\/a>, it suggests using Artifact for new code to save models. And I am able to save the model in the offline mode. However, I wonder how to restore the model from an artifact with a particular version (e.g., v3, not necessarily the latest version of the artifact) if I want to resume the training after it\u2019s interrupted?<\/p>\n<p>I am running code on compute nodes that do not have access to internet, so I have to use the offline mode. And in offline mode, I cannot run <code>use_artifact<\/code> command as it only works in online mode. However, I think all the artifact data is already saved locally using <code>log_artifact<\/code> command. So in theory, I should be able to restore a particular version of the artifact? How can I get that? Even though I know the model file location, it is the latest model file, not a particular history version of the model file (that\u2019s the point of using Artifact to track the model?).<\/p>\n<pre><code class=\"lang-auto\">    run = wandb.init(mode='offline', project='test')\n    artifact = run.use_artifact('hello-world:v3')\n    artifact_dir = artifact.download(root=run.dir)\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Local controller seems block",
        "Question_creation_time":1660803433186,
        "Question_link":"https:\/\/community.wandb.ai\/t\/local-controller-seems-block\/2955",
        "Question_upvote_count":0.0,
        "Question_view_count":140.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I make the following sweep (yaml) file:<\/p>\n<pre><code class=\"lang-auto\">program: train_mnist.py\nmethod: grid\nparameters:\n  lr_schedule:\n    values: [ step, cyclic ]\n  epoch_total:\n    values: [ 2, 4 ]\nmetric:\n  goal: maximize\n  name: test-result\/accuracy\nproject: my-mnist-test-project\nname: MNIST-Sweep-Test\ndescription: test sweep demo\n<\/code><\/pre>\n<p>and I use <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/advanced-sweeps\/local-controller#running-the-local-controller-from-the-command-line\">local controller<\/a> to perform sweep locally. However, it seems block here:<\/p>\n<pre><code class=\"lang-auto\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Created sweep with ID: o2mzl569\nwandb: View sweep at: https:\/\/wandb.ai\/geyao\/my-mnist-test-project\/sweeps\/o2mzl569\nwandb: Run sweep agent with: wandb agent geyao\/my-mnist-test-project\/o2mzl569\nwandb: Starting wandb controller...\nSweep: o2mzl569 (grid) | Runs: 0\n\n# ------blocked here!------\n<\/code><\/pre>\n<p>When I turn off the network, it will be:<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Network error (ConnectionError), entering retry loop.\n<\/code><\/pre>\n<p>Why local controller tries to connect the network? How can I perform local sweep with\/without network in the right way?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What should the .gitignore file be when using wandb?",
        "Question_creation_time":1632413740148,
        "Question_link":"https:\/\/community.wandb.ai\/t\/what-should-the-gitignore-file-be-when-using-wandb\/756",
        "Question_upvote_count":2.0,
        "Question_view_count":348.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I noticed that my pycharm suggests some wandb files that are created automatically\u2026to avoid pushing those (btw what are they?) what is the recommended .gitignore files contents?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I make a Confusion Matrix that does not overlap?",
        "Question_creation_time":1640677793568,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-make-a-confusion-matrix-that-does-not-overlap\/1634",
        "Question_upvote_count":0.0,
        "Question_view_count":225.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI just made a confusion matrix in validation step, but it overlapped like this during training.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ea61626463c01a256837c5b06f5f95a92b9ad9d2.png\" data-download-href=\"\/uploads\/short-url\/xrqn87xjrMZ9EDb39gqB8tmMu6C.png?dl=1\" title=\"dd\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ea61626463c01a256837c5b06f5f95a92b9ad9d2_2_690x70.png\" alt=\"dd\" data-base62-sha1=\"xrqn87xjrMZ9EDb39gqB8tmMu6C\" width=\"690\" height=\"70\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ea61626463c01a256837c5b06f5f95a92b9ad9d2_2_690x70.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ea61626463c01a256837c5b06f5f95a92b9ad9d2_2_1035x105.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ea61626463c01a256837c5b06f5f95a92b9ad9d2_2_1380x140.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ea61626463c01a256837c5b06f5f95a92b9ad9d2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">dd<\/span><span class=\"informations\">2046\u00d7208 15.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I want to make a confusion matrix or table that can be seen by epoch.<\/p>\n<pre><code class=\"lang-auto\">            wandb.log({\n                \"Confusion Matrix\": wandb.sklearn.plot_confusion_matrix(\n                                                                y_true=targets,\n                                                                y_pred=outputs,\n                                                                labels=['Normal','COVID','Others']\n                                                                )\n            })\n<\/code><\/pre>\n<p>Is there any way to solve this?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Publishing Graphs\/Visualizations",
        "Question_creation_time":1638456342347,
        "Question_link":"https:\/\/community.wandb.ai\/t\/publishing-graphs-visualizations\/1457",
        "Question_upvote_count":1.0,
        "Question_view_count":179.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I\u2019m soon going to start implementing W&amp;B for my neural network\u2019s hyperparameter tuning. This is in preparation for an academic paper I\u2019m writing on the subject. The software seems very pragmatic and well-polished, so I\u2019m quite excited to get started.<\/p>\n<p>Its visualizations in particular seem to be of a very high quality. Some present sophisticated functionality that other experiment trackers can\u2019t touch. With proper citation, can these be included for publication?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run best model off sweep?",
        "Question_creation_time":1652672302869,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-best-model-off-sweep\/2423",
        "Question_upvote_count":2.0,
        "Question_view_count":244.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi,<br>\nI am using Sweeps to run through different configuration models and I was told by the wandb chat support that to run the best model configuration off sweeps is to create a new sweep with the best performing parameter set and running off it.<\/p>\n<p>But this is lot of tedious work, is there any other elegant way of quering wandb project for the best model configuration and running off it?<\/p>\n<p>tldr: I run a sweep with different configuration, would like to run predictions off a specific set of parameters (or best performing set of parameters). How  to do it with the sweep API?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Show baseline score in plots",
        "Question_creation_time":1653476126544,
        "Question_link":"https:\/\/community.wandb.ai\/t\/show-baseline-score-in-plots\/2492",
        "Question_upvote_count":1.0,
        "Question_view_count":83.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m training N models and I\u2019m plotting on wandb their evaluation score. This results in having a run group where each panel has N plots, like in the figure below.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6bfd099e64d3d1327f97778eb9b5fd37605c3674.png\" alt=\"Screenshot from 2022-05-25 12-49-24\" data-base62-sha1=\"fpjdD3BHyP6hIa3uCWJUY9ZbVKk\" width=\"441\" height=\"210\"><\/p>\n<p>I want to also show a horizontal line that represents the baseline score that my model needs to beat. At the moment I\u2019m doing that manually by adding an expression like <code>baseline_value + 0 * ${evaluation}<\/code>. However, this is ugly since N lines will be created with the same name as the model runs.<\/p>\n<p>Is there a way to automate this, and only produce one line with a different name (e.g. <code>'baseline'<\/code>)?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run.history() returns different values on almost each call",
        "Question_creation_time":1652713229122,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-history-returns-different-values-on-almost-each-call\/2431",
        "Question_upvote_count":2.0,
        "Question_view_count":235.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I recently started using the <code>wandb.Api()<\/code> in order not to manually download all the Charts in <code>.csv<\/code> format.<\/p>\n<p>The problem is that I cannot get consistent results, most of the times that I call the API  in a jupyter-notebook I get different results.<\/p>\n<p>I have made public one of my dashboards to tackle this issue. Here is a screenshot with a reproducible example:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" data-download-href=\"\/uploads\/short-url\/8x7Rm9lNkSyg4pi6edKG0wNOxgE.png?dl=1\" title=\"2022-05-16-165542_647x517_scrot\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" alt=\"2022-05-16-165542_647x517_scrot\" data-base62-sha1=\"8x7Rm9lNkSyg4pi6edKG0wNOxgE\" width=\"625\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">2022-05-16-165542_647x517_scrot<\/span><span class=\"informations\">647\u00d7517 50.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>In order to obtain the <code>csv_val_f1<\/code> variable one just needs to download the <code>Val F1<\/code> chart. Two things can be seen here:<\/p>\n<ol>\n<li>Multiple runs of the same code produce different results<\/li>\n<li>The maximum value obtained by the API differs from the maximum value obtained by manually downloading the <code>.csv<\/code> version of the Chart.<\/li>\n<\/ol>\n<p>Any ideas on what I\u2019m missing?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Account storage not being freed?",
        "Question_creation_time":1637707347325,
        "Question_link":"https:\/\/community.wandb.ai\/t\/account-storage-not-being-freed\/1375",
        "Question_upvote_count":0.0,
        "Question_view_count":191.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I deleted all my projects in order to free up my account storage, but the usage dashboard page still shows it as being used. 36gb of the 100gb available to be precise. Isn\u2019t it supposed to be reclaimed after the projects\/artifacts are deleted?<\/p>\n<p>I used all this space just by uploading some Driverless AI (<a href=\"https:\/\/www.h2o.ai\/products\/h2o-driverless-ai\/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">H2O Driverless AI | H2O.ai<\/a>) models and temporary files I was experimenting with. But if I certainly won\u2019t continue doing this if this space is gone for good.<\/p>\n<p>I can\u2019t really complain as I\u2019ve a free account, but I wonder if you are charging your paying customers for deleted files too\u2026<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problems accessing web interface: \"rate limit exceeded\"",
        "Question_creation_time":1646737169636,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problems-accessing-web-interface-rate-limit-exceeded\/2041",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am having trouble accessing the <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> web interface. I very often, yet not always, get error messages like \u201crate limit exceeded\u201d or \u201cThere was a problem rendering these panels\u201d.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b365941060b6ae0dc38310d95e97379e66323316.png\" data-download-href=\"\/uploads\/short-url\/pB164ekBeFRf2vXOzJZpRNGSnBQ.png?dl=1\" title=\"Screenshot_google-chrome_20220308104954_crop\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b365941060b6ae0dc38310d95e97379e66323316_2_690x254.png\" alt=\"Screenshot_google-chrome_20220308104954_crop\" data-base62-sha1=\"pB164ekBeFRf2vXOzJZpRNGSnBQ\" width=\"690\" height=\"254\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b365941060b6ae0dc38310d95e97379e66323316_2_690x254.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b365941060b6ae0dc38310d95e97379e66323316.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b365941060b6ae0dc38310d95e97379e66323316.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b365941060b6ae0dc38310d95e97379e66323316_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot_google-chrome_20220308104954_crop<\/span><span class=\"informations\">790\u00d7291 10.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Yesterday I had no problems at all. What\u2019s the issue?<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"All records are lost in a project without any action",
        "Question_creation_time":1661403318517,
        "Question_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Question_upvote_count":5.0,
        "Question_view_count":120.0,
        "Question_answer_count":13,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to draw many images with a bar",
        "Question_creation_time":1652175740664,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-draw-many-images-with-a-bar\/2391",
        "Question_upvote_count":0.0,
        "Question_view_count":91.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I can only draw image with [wandb.Image(Numpy.array()),] like this<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b48aa9bfc94603e638f56ff8452ed88b900f00db.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"11:54AM - 20 November 2020\">AIcrowd Forum \u2013 20 Nov 20<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:600\/325;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e05a631c976b165047261523c356b3fa7e5eab41.gif\" class=\"thumbnail animated\" width=\"600\" height=\"325\"><\/div>\n\n<h3><a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\">MaskRCNN integrated with WandB and DIRECT SUBMIT FROM COLAB!<\/a><\/h3>\n\n  <p>Hi everyone!    @rohitmidha23 and me have been following this challenge for quite a while. We have written a starter notebook using MaskRCNN. We further integrate MaskRCNN with WandB which really helps to keep track of the various experiments that...<\/p>\n\n  <p>\n    <span class=\"label1\">Reading time: 1 mins \ud83d\udd51<\/span>\n      <span class=\"label2\">Likes: 17 \u2764<\/span>\n  <\/p>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p>\nBut how can I draw many images with a bar like this<br>\n<a href=\"https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp<\/a><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run to Run Logging",
        "Question_creation_time":1667283829357,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-to-run-logging\/3353",
        "Question_upvote_count":0.0,
        "Question_view_count":69.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We are interested in using WandB to track monitoring statistics for a Model that is going into production.  We have a process that runs known data against the model and records results to confirm operation and timing.<br>\nWe would like to show the statistics in a lineplot over many iterations from these regular monitoring runs.  Its important to be able see a plot tracking the values over time.<br>\nWhat is the recommended way to log run-to-run values?<br>\nOptions:<\/p>\n<ol>\n<li>Use the <code>resume<\/code> command to continue a RunID repeatedly for every monitoring run.<br>\n<code>wand.init(project=\"name\",id=\"my_run_id\", resume=True)<\/code><br>\nThis seems to work, but I have found that I can\u2019t look at old values. Only the most recent values are shown under the <code>Summary<\/code> panel.  It would be helpful to see a Table of all recorded values over time.<\/li>\n<li>Somehow use a Table, but can\u2019t find a way to tack on data to a table from a previous run<\/li>\n<li>Use a separate Run for each test, but find some way to track statistics across runs.  I\u2019ve tried to search on this, but can\u2019t find a way to do this across dozens to hundreds of runs.<br>\nThanks for the help.<\/li>\n<\/ol>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create a panel that reports number of successful runs per group",
        "Question_creation_time":1641293761794,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-create-a-panel-that-reports-number-of-successful-runs-per-group\/1683",
        "Question_upvote_count":3.0,
        "Question_view_count":185.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m currently performing a hyperparameter search (I\u2019m not using wandb\u2019s sweeep feature for that) on a GPU cluster.  I group the runs into different categories, let\u2019s say that\u2019s simply \u201cGroup A\u201d, \u201cGroup B\u201d, etc.<\/p>\n<p>Now, since jobs can crash for various reasons, I would love to have a panel that reports the number of successful runs <em>for each group<\/em>, so I know how each group is doing (\u201cGroup A has 200 successful runs, while Group B only has 50, so I need to start some more jobs for Group B\u201d). I know I can get there by using the general filter and group feature in my project\u2019s run-table, but this is rather tedious and it would be more convenient for me to have it as a panel for quick access (e.g., inside a report).<\/p>\n<p>I\u2019ve fiddled around with the \u201cscalar chart\u201d and the \u201cweave\u201d panel, but without success \u2013 any ideas?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Chat Application Integration",
        "Question_creation_time":1659529273332,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-chat-application-integration\/2844",
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I would like to get some references to integrate wandb alert to a custom chat application(discord bot or other slack like app for example). how do i go about achieving that?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Couldn't make Wandb run on a TPU",
        "Question_creation_time":1631757463951,
        "Question_link":"https:\/\/community.wandb.ai\/t\/couldnt-make-wandb-run-on-a-tpu\/633",
        "Question_upvote_count":0.0,
        "Question_view_count":406.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I tried a lot of things but was not able to run Wandb on TPU.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6ac7b539cf35cb8e5b4160099c1f51f226d94626.png\" data-download-href=\"\/uploads\/short-url\/feCu0Sg0RhG0KlIVQ3CeQYH2Tlk.png?dl=1\" title=\"Screenshot from 2021-09-15 21-32-25\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6ac7b539cf35cb8e5b4160099c1f51f226d94626_2_472x500.png\" alt=\"Screenshot from 2021-09-15 21-32-25\" data-base62-sha1=\"feCu0Sg0RhG0KlIVQ3CeQYH2Tlk\" width=\"472\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6ac7b539cf35cb8e5b4160099c1f51f226d94626_2_472x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6ac7b539cf35cb8e5b4160099c1f51f226d94626_2_708x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6ac7b539cf35cb8e5b4160099c1f51f226d94626.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6ac7b539cf35cb8e5b4160099c1f51f226d94626_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2021-09-15 21-32-25<\/span><span class=\"informations\">807\u00d7854 105 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><a href=\"https:\/\/www.kaggle.com\/harveenchadha\/chaii-tpu-train-nfold-xlm-hf-tf-data-extra?scriptVersionId=74865075\" rel=\"noopener nofollow ugc\">Kernel<\/a><\/p>\n<p>If there is something wrong in my code structure please let me know <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> . Have raised an issue on <a href=\"https:\/\/github.com\/wandb\/client\/issues\/2672\" rel=\"noopener nofollow ugc\">github<\/a>  as well.<\/p>\n<p>Cheers!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Summarize Complex Configuration Dictionaries",
        "Question_creation_time":1653291234729,
        "Question_link":"https:\/\/community.wandb.ai\/t\/summarize-complex-configuration-dictionaries\/2481",
        "Question_upvote_count":0.0,
        "Question_view_count":86.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<br>\nIs there a way I can access the nested dictionaries in the <code>run.config<\/code> object for custom panels, weaves and reports. Apart from sweeps, I am manipulating a variable space and logging them as an array to the config as below:<\/p>\n<pre><code class=\"lang-python\">wandb.config.update({'observation\/experiment': AN_ARRAY })\n<\/code><\/pre>\n<p>I would like to access and visualize\/summarize this variable in text or weave form. However when I call <code>run.config<\/code> in a weave, this variable doesn\u2019t show up even though I can see it in the run overview.<br>\nThank you for your support!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is the InvalidVersionSpec: Invalid version '1<2': invalid character(s) to wandb?",
        "Question_creation_time":1631575270538,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-the-invalidversionspec-invalid-version-1-2-invalid-character-s-to-wandb\/556",
        "Question_upvote_count":1.0,
        "Question_view_count":231.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I noticed that all my scripts that have wandb started to give this error:<\/p>\n<pre><code class=\"lang-auto\">InvalidVersionSpec: Invalid version '1&lt;2': invalid character(s)\n<\/code><\/pre>\n<p>and was wondering if this is something that others have experienced when incorporating wandb and how do you remove it?<\/p>\n<p>Thanks in advance! wandb is pretty cool <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error uploading dataset to colab",
        "Question_creation_time":1641645756058,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-uploading-dataset-to-colab\/1728",
        "Question_upvote_count":0.0,
        "Question_view_count":156.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am having issues when I try to upload my dataset. I don\u00b4t know why but when I upload just one image and label in each folder(train and val) everything works perfectly, but when I try to upload more than one image I get the next error:<\/p>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master\\utils\\loggers\\wandb\\log_dataset.py\u201d, line 27, in <br>\ncreate_dataset_artifact(opt)<br>\nFile \u201cC:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master\\utils\\loggers\\wandb\\log_dataset.py\u201d, line 11, in create_dataset_artifact<br>\nlogger = WandbLogger(opt, None, job_type=\u2018Dataset Creation\u2019)  # TODO: return value unused<br>\nFile \u201cC:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master\\utils\\loggers\\wandb\\wandb_utils.py\u201d, line 190, in <em>init<\/em><br>\nself.data_dict = self.check_and_upload_dataset(opt)<br>\nFile \u201cC:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master\\utils\\loggers\\wandb\\wandb_utils.py\u201d, line 203, in check_and_upload_dataset<br>\nconfig_path = self.log_dataset_artifact(opt.data,<br>\nFile \u201cC:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master\\utils\\loggers\\wandb\\wandb_utils.py\u201d, line 345, in log_dataset_artifact<br>\nself.train_artifact = self.create_dataset_table(LoadImagesAndLabels(<br>\nFile \u201cC:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master\\utils\\loggers\\wandb\\wandb_utils.py\u201d, line 428, in create_dataset_table<br>\nartifact.add(table, name)<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\sdk\\wandb_artifacts.py\u201d, line 500, in add<br>\nval = obj.to_json(self)<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\data_types.py\u201d, line 610, in to_json<br>\nmapped_row.append(_json_helper(v, artifact))<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\data_types.py\u201d, line 105, in _json_helper<br>\nreturn val.to_json(artifact)<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\sdk\\data_types.py\u201d, line 2303, in to_json<br>\nclasses_entry = artifact.add(self._classes, class_name)<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\sdk\\wandb_artifacts.py\u201d, line 520, in add<br>\nwith self.new_file(name) as f:<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\u201d, line 117, in <em>enter<\/em><br>\nreturn next(self.gen)<br>\nFile \u201cC:\\Users\\Robcib\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\sdk\\wandb_artifacts.py\u201d, line 359, in new_file<br>\nraise ValueError(<br>\nValueError: File with name \u201cmedia\\classes\\56699adf4321fa19e6264a528fad82c4_cls.classes.json\u201d already exists at \u201cC:\\Users\\Robcib\\AppData\\Local\\Temp\\tmph3c76g8x\\media\\classes\\56699adf4321fa19e6264a528fad82c4_cls.classes.json\u201d<\/p>\n<p>command line: C:\\Users\\Robcib\\Desktop\\Miguel\\YOLO_Miguel\\yolov5-master&gt;python utils\/loggers\/wandb\/log_dataset.py --project custom_yolov5 --data data\/custom_dataset.yaml<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to show max performance and average across trials",
        "Question_creation_time":1635663573755,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-show-max-performance-and-average-across-trials\/1172",
        "Question_upvote_count":5.0,
        "Question_view_count":347.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>So I\u2019ve been using the default curves to monitor my RL experiments for a while now. They are very handy and easier to manage than my old <code>.csv<\/code> workflow. In my experiments, I have multiple runs with the same hyperparameters and they are organized into groups. What I\u2019m trying to plot is this: create a bar plot of max performance averaged across trials for each group, versus the name of the group.<\/p>\n<p>I tried to create a panel of bar plot, and in general it looks like what I want: it lists all the groups of runs, and automatically calculates some aggregated value of them, e.g. mean or median. But it seems that the plot is taking the mean\/median of all the values from the whole group (like the concat of all trials) instead of giving me a choice of, e.g., averaging over the max return of each run. Here is what the plot looks like:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1b148885f64e24dbdf9496970c581de3a1c06a3e.png\" data-download-href=\"\/uploads\/short-url\/3RySOpsmjzfJzAkEHbaGE2ilpro.png?dl=1\" title=\"WX20211031-025712@2x\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1b148885f64e24dbdf9496970c581de3a1c06a3e_2_575x500.png\" alt=\"WX20211031-025712@2x\" data-base62-sha1=\"3RySOpsmjzfJzAkEHbaGE2ilpro\" width=\"575\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1b148885f64e24dbdf9496970c581de3a1c06a3e_2_575x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1b148885f64e24dbdf9496970c581de3a1c06a3e.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1b148885f64e24dbdf9496970c581de3a1c06a3e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1b148885f64e24dbdf9496970c581de3a1c06a3e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">WX20211031-025712@2x<\/span><span class=\"informations\">716\u00d7622 17.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I saw that there are custom tables, but I\u2019m not quite sure how to use them. If it\u2019s easy to write, can someone give some hints about how to get custom tables to do this for me? Lots of thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Graphs out of sync with each other",
        "Question_creation_time":1658864529811,
        "Question_link":"https:\/\/community.wandb.ai\/t\/graphs-out-of-sync-with-each-other\/2803",
        "Question_upvote_count":0.0,
        "Question_view_count":80.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>This happens to multiple users on other projects as well, not just me. If you look at the graphs here, as an example (<a href=\"https:\/\/wandb.ai\/kaiyotech\/KaiBumBot?workspace=user-kaiyotech\" class=\"inline-onebox\">Weights &amp; Biases<\/a>) you can see that if you move your mouse to the right side of the graph, they show different steps, so they\u2019re not in sync with each other. This makes it really complicated to actually nicely figure out what\u2019s happening with a run, and makes some graphs out of date with others. Occasionally the screen will refresh and some graphs will change to be more in date and others will move out of date, it seems random.<\/p>\n<p>Is there something I can do about this?<\/p>\n<p>Thanks,<br>\nKai<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to have wandb print how long experiment ran on console?",
        "Question_creation_time":1633115790901,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-have-wandb-print-how-long-experiment-ran-on-console\/845",
        "Question_upvote_count":0.0,
        "Question_view_count":240.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I always print this string:<\/p>\n<pre><code class=\"lang-auto\">wandb: Find logs at: .\/wandb\/run-20211001_135946-18loi2se\/logs\/debug.log\nwandb: \n-- wandb finished\ntime_passed_msg = time passed: hours:0.0714601335922877, minutes=4.287608015537262, seconds=257.2564809322357\n<\/code><\/pre>\n<p>but it\u2019s sort of annying if I am already running <code>wandb.finish<\/code>. Is it possible to have wandb print it for me? (especially to save it on my wandb log\/print std out file?)<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Waiting for W&B process to finish (success)",
        "Question_creation_time":1659034495533,
        "Question_link":"https:\/\/community.wandb.ai\/t\/waiting-for-w-b-process-to-finish-success\/2818",
        "Question_upvote_count":0.0,
        "Question_view_count":246.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>When using wandb I don\u2019t have any runs at the moment however it won\u2019t let me start any new runs. When the python script is run the bellow message is shown and the run is \u2018synced\u2019 doesn\u2019t appear to be running on the PC. I have checked the system processes and there is no instance of a wandb script running. On my wandb profile, all runs appear to be failed, finished or killed.<br>\nIs there a way to manually override this? I have tried running in offline mode and it still has no effect and I cannot test the changes to my code.<\/p>\n<p>EDIT: Elaborated description and provided console output:<\/p>\n<p>wandb: Currently logged in as: username. Use <code>wandb login --relogin<\/code> to force relogin<\/p>\n<p>wandb: Tracking run with wandb version 0.12.19<\/p>\n<p>wandb: Run data is saved locally in \/tmp\/tmpjjlzygaw\/wandb\/run-<br>\n20220627_121802-X_training18<\/p>\n<p>wandb: Run <code>wandb offline<\/code> to turn off syncing.<\/p>\n<p>wandb: Syncing run 88_DCAC_training18<br>\nwandb: <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/star.png?v=12\" title=\":star:\" class=\"emoji\" alt=\":star:\" loading=\"lazy\" width=\"20\" height=\"20\"> View project at <a href=\"https:\/\/wandb.ai\/username\/X\" class=\"inline-onebox\">Weights &amp; Biases<\/a><br>\nwandb: <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/rocket.png?v=12\" title=\":rocket:\" class=\"emoji\" alt=\":rocket:\" loading=\"lazy\" width=\"20\" height=\"20\"> View run at <a href=\"https:\/\/wandb.ai\/username\/X\/runs\/X_training18\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>continuing\u2026<\/p>\n<p>wandb: Waiting for W&amp;B process to finish\u2026 (success).<br>\nwandb:<br>\nwandb: Synced X_training18: <a href=\"https:\/\/wandb.ai\/lucmc\/DCAC%20L-Reacher\/runs\/X_training18\" class=\"inline-onebox\">Weights &amp; Biases<\/a><br>\nwandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)<br>\nwandb: Find logs at: \/tmp\/tmpjjlzygaw\/wandb\/run-20220627_121802-X_training18\/logs<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot add new members to a team I created",
        "Question_creation_time":1663580528669,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-add-new-members-to-a-team-i-created\/3156",
        "Question_upvote_count":0.0,
        "Question_view_count":99.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<br>\nI created a team, but apparently I\u2019m not an admin (and neither is anyone else for that matter).<br>\nSo we\u2019re all simple members and cannot add new ppl to our team.<\/p>\n<p>Any solutions?<br>\nThanks<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Traceback error",
        "Question_creation_time":1661465657943,
        "Question_link":"https:\/\/community.wandb.ai\/t\/traceback-error\/3008",
        "Question_upvote_count":1.0,
        "Question_view_count":585.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey guys,<\/p>\n<p>I am totally new to W&amp;B. I am getting a Traceback error when I want to run \u201cwandb.init(project=\u201d\u2026\u201c)\u201d. Last week it still did work. Any tips what to do?? Thank you so much.<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 999, in init\n    run = wi.init()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 651, in init\n    backend.cleanup()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/backend\/backend.py\", line 246, in cleanup\n    self.interface.join()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 475, in join\n    super().join()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 666, in join\n    _ = self._communicate_shutdown()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 472, in _communicate_shutdown\n    _ = self._communicate(record)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 226, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 231, in _communicate_async\n    raise Exception(\"The wandb backend process has shutdown\")\nException: The wandb backend process has shutdown\nwandb: ERROR Abnormal program exit\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    998         try:\n--&gt; 999             run = wi.init()\n   1000             except_exit = wi.settings._except_exit\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(self)\n    650                     # we don't need to do console cleanup at this point\n--&gt; 651                     backend.cleanup()\n    652                     self.teardown()\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/backend\/backend.py in cleanup(self)\n    245         if self.interface:\n--&gt; 246             self.interface.join()\n    247         if self.wandb_process:\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in join(self)\n    474     def join(self) -&gt; None:\n--&gt; 475         super().join()\n    476 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface.py in join(self)\n    665             return\n--&gt; 666         _ = self._communicate_shutdown()\n    667 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate_shutdown(self)\n    471         record = self._make_record(request=request)\n--&gt; 472         _ = self._communicate(record)\n    473 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate(self, rec, timeout, local)\n    225     ) -&gt; Optional[pb.Result]:\n--&gt; 226         return self._communicate_async(rec, local=local).get(timeout=timeout)\n    227 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate_async(self, rec, local)\n    230         if self._process_check and self._process and not self._process.is_alive():\n--&gt; 231             raise Exception(\"The wandb backend process has shutdown\")\n    232         future = self._router.send_and_receive(rec, local=local)\n\nException: The wandb backend process has shutdown\n\nThe above exception was the direct cause of the following exception:\n\nException                                 Traceback (most recent call last)\n&lt;ipython-input-49-e3734aa09c65&gt; in &lt;module&gt;\n      1 #Login to wandb\n      2 # #! wandb login config_dict[\"wandb_key\"]\n----&gt; 3 wandb.init()\n      4 #run_name = wandb.run.name\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n   1035             if except_exit:\n   1036                 os._exit(-1)\n-&gt; 1037             raise Exception(\"problem\") from error_seen\n   1038     return run\n\nException: problem\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Undelete runs no longer available?",
        "Question_creation_time":1668549759237,
        "Question_link":"https:\/\/community.wandb.ai\/t\/undelete-runs-no-longer-available\/3420",
        "Question_upvote_count":0.0,
        "Question_view_count":19.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi,<br>\nI deleted a run by accident and tried to recover it. However, I noticed there is undelete all runs option available from the dot menu on the overview page of the project.<br>\nIs there a way to recover a deleted run?<br>\nThank you.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Subset of list of values as hyperparameters",
        "Question_creation_time":1660571168207,
        "Question_link":"https:\/\/community.wandb.ai\/t\/subset-of-list-of-values-as-hyperparameters\/2934",
        "Question_upvote_count":0.0,
        "Question_view_count":288.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m currently training a object detection model and wanted to use sweeps to do some hyperparameter optimization. A few of the hyperparameters are in the form of lists. E.g: data_preprocessors: [\u201crandom-flip\u201d, \u201crandom-crop\u201d, \u201crandom-expand\u201d, etc.]<\/p>\n<p>I would like sweep to take a subset from these values and pass them to my training script. However I could not find how to do this easily without a lot of custom wrapping code.<br>\nMy current solution would be to have each value as a boolean and add some custom logic to convert that to the list I want, however this is not easily expandable\/reusable. Is there something I am missing?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"The wandb.log function does not treat nested dict as it describes in the document",
        "Question_creation_time":1666831371509,
        "Question_link":"https:\/\/community.wandb.ai\/t\/the-wandb-log-function-does-not-treat-nested-dict-as-it-describes-in-the-document\/3330",
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I find that using the <code>wandb.log()<\/code> does not work like it describe in the document that<\/p>\n<blockquote>\n<p>Logging nested metrics is encouraged and is supported in the W&amp;B UI. If you log with a nested dictionary like <code>wandb.log({\"train\": {\"acc\": 0.9}, \"val\": {\"acc\": 0.8}})<\/code> , the metrics will be organized into <code>train<\/code> and <code>val<\/code> sections in the W&amp;B UI.<\/p>\n<\/blockquote>\n<p>Instead, it seems like it will create separate plots with a dot to make different names. In the above sample, it would show me<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/7\/77fb781483d08f4abf148a2abd22caa5c42a5d2b.png\" data-download-href=\"\/uploads\/short-url\/h7pAvZ8ws18O7lJwodYZ0CdnF6X.png?dl=1\" title=\"img1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/7\/77fb781483d08f4abf148a2abd22caa5c42a5d2b_2_690x237.png\" alt=\"img1\" data-base62-sha1=\"h7pAvZ8ws18O7lJwodYZ0CdnF6X\" width=\"690\" height=\"237\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/7\/77fb781483d08f4abf148a2abd22caa5c42a5d2b_2_690x237.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/7\/77fb781483d08f4abf148a2abd22caa5c42a5d2b_2_1035x355.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/7\/77fb781483d08f4abf148a2abd22caa5c42a5d2b_2_1380x474.png 2x\" data-dominant-color=\"FDFCFC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">img1<\/span><span class=\"informations\">2136\u00d7736 31.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nHowever, what I expect is the UI should show me two sections (train and val) and separate \u201cacc\u201d named plots inside. BTW, I know if I use <code>wandb.log({\"train\/acc\": 0.9, \"val\/acc\": 0.9})<\/code> can achieve this functionality.<\/p>\n<p>Any helps would be appreciated!<br>\nThank you!<br>\nDylan<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Easiest way to load the best model checkpoint after training w\/ pytorch lightning",
        "Question_creation_time":1667348143889,
        "Question_link":"https:\/\/community.wandb.ai\/t\/easiest-way-to-load-the-best-model-checkpoint-after-training-w-pytorch-lightning\/3365",
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I have a notebook based on <a href=\"http:\/\/wandb.me\/lit-colab\" rel=\"noopener nofollow ugc\"> Supercharge your Training with PyTorch Lightning + Weights &amp; Biases<\/a> and I\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes.<\/p>\n<p>I\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc).<\/p>\n<p>Specifically I was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: <code>.\/MnistKaggle\/1vzsgin6\/checkpoints<\/code>, where <code>1vzsgin6<\/code> is the run id auto-generated by wandb.<\/p>\n<p>One (clunky) way to do it would be:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"MnistKaggle\")\ncheckpoint_dir_path = None\n\ndef my_after_save_checkpoint(checkpoint):\n  checkpoint_dir_path = checkpoint.dirpath\n\nwandb_logger.after_save_checkpoint = my_after_save_checkpoint\n\n# Now find the checkpoint file in the checkpoint_dir_path directory and load the model from that.\n<\/code><\/pre>\n<p>Is there an easier way?  I was sorta expecting the <code>WandbLogger<\/code> object to have an easy method like <code>get_save_checkpoint_dirpath()<\/code>, but I\u2019m not seeing anything.<\/p>\n<p>Thanks in advance for any help!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Adding tfRecords files to artifacts doesn't work?",
        "Question_creation_time":1645200359797,
        "Question_link":"https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948",
        "Question_upvote_count":0.0,
        "Question_view_count":174.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying logging my tfRecords files to artefact, but it seems to not be working (I get an error: \u201cwandb: Network error (TransientError), entering retry loop.\u201d).<\/p>\n<p>I am providing the code I use below. I am pretty sure it is something regarding the tfRecords file since I tried changing the contents of my folders to contain only .csv and .paqruet and it worked nicely. Do you have any ideas what could be happening here?<\/p>\n<pre><code class=\"lang-auto\">with wandb.init(project=\"----\", entity='----', job_type='saving_processed_files') as run:\n    train_data_art = wandb.Artifact(\n        name='train_data',\n        type='train_data'  \n    )\n\n    files_train = os.listdir(final_path_train)\n    files_train=[x  for x in files_train if x[0]!='.']\n\n    for file in files_train:\n        file_path = os.path.join(final_path_train, file)\n        train_data_art.add_file(file_path, name=file)\n\n    run.log_artifact(train_data_art)\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Workflow for running an ensemble of experiments with different initial conditions",
        "Question_creation_time":1662490156459,
        "Question_link":"https:\/\/community.wandb.ai\/t\/workflow-for-running-an-ensemble-of-experiments-with-different-initial-conditions\/3074",
        "Question_upvote_count":1.0,
        "Question_view_count":73.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Any ideas on the right workflow to run sweeps\/groups with a whole bunch of different variations on initial conditions to see an ensemble of results?  I think that the  <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\" class=\"inline-onebox\">Group Runs - Documentation<\/a>  seems a natural candidate for this but I am not sure the right approach or how it overlays with sweeps in this sort of usecase.<\/p>\n<p>To setup the scenario I have in mind: I have a script  I want to run hundred times on my local machine with pretty much all parameters fixed except the neural network initial conditions.  I can control that by doing things like incrementing a <code>--seed<\/code> argument  or just not establishing a default seed.  After running those experiments, it is nice to see pretty pictures of distribtions in wandb but I also want to be able to later collect the results\/assets as a group. and do things like plot a histogram of <code>val_loss<\/code> to put in a research paper.<\/p>\n<p>Is the way to do this with a combination of sweeps and run_groups?  Forr example, can I run a bunch of these in a sweep with after setting the <code>WANDB_RUN_GROUP<\/code> environment variable?  For example, maybe setup a sweep file like<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">program: train.py\nmethod: grid\nparameters:\n  seed:\n    min: 2\n    max: 102\n<\/code><\/pre>\n<p>Where <code>--seed<\/code> is used internally to set the seed for the experiment?  Any better approaches<\/p>\n<p>If that works, ,  then do I just need to set <code>WANDB_RUN_GROUP<\/code> environment variable on every machine that I will run an agent on and then it can be grouped?  Then I can pull down all of the assets for these with the <code>WAND_RUN_GROUP<\/code>?  I couldn\u2019t figure it out from the docs how to get all of the logged results (and the artifacts if there are any) for a group.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to download the learning curves of grouped runs",
        "Question_creation_time":1661531147920,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-download-the-learning-curves-of-grouped-runs\/3010",
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Is there a way to download the learning curves of grouped runs? For grouped runs, the learning curves have shaded area. Does that represent the standard deviation or the 95% confidence interval? And can we download them (the grouped curve, not the individual ones) in a python script so that we can customize the plot with matplotlib or seaborn? Thanks.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"A colleague accidentally deleted some projects, any chance to get them back?",
        "Question_creation_time":1642440642480,
        "Question_link":"https:\/\/community.wandb.ai\/t\/a-colleague-accidentally-deleted-some-projects-any-chance-to-get-them-back\/1774",
        "Question_upvote_count":0.0,
        "Question_view_count":117.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Some of our teams projects were erroneously deleted by a colleague since he wanted to clean up his account and probably thought it were his own projects. Is there any chance to get the data back (I assume not, but worth a try\u2026)?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About manually assign weights & biases",
        "Question_creation_time":1644363601355,
        "Question_link":"https:\/\/community.wandb.ai\/t\/about-manually-assign-weights-biases\/1884",
        "Question_upvote_count":0.0,
        "Question_view_count":102.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>How can we manually assign weights and biases rather than using randomly generated weights and biases at the beginning?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging all summary metrics based on the max of one",
        "Question_creation_time":1643411787613,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-all-summary-metrics-based-on-the-max-of-one\/1836",
        "Question_upvote_count":0.0,
        "Question_view_count":146.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>If you\u2019re doing something like early stopping, you might simply save the model with the best loss and let it run for 10 more epochs, but discard those model weights.  I see that for individual metrics I can tell it to store the \u201cmax\u201d or \u201cmin\u201d, but what if I want the entire summary to happen based on a single metric\u2019s max\/min?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb Project table value select algorithm",
        "Question_creation_time":1650599811340,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-project-table-value-select-algorithm\/2300",
        "Question_upvote_count":0.0,
        "Question_view_count":328.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a wondering how to select value in project table.<br>\nWhat is algorithm in wandb project table to select values? In images, these values is not minimum in each models.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5238ecb4f4a97cf42692282562cfbd48f3bb95da.png\" data-download-href=\"\/uploads\/short-url\/bJn70ylkOmBrf45BJzAwLO3FLIe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5238ecb4f4a97cf42692282562cfbd48f3bb95da_2_690x99.png\" alt=\"image\" data-base62-sha1=\"bJn70ylkOmBrf45BJzAwLO3FLIe\" width=\"690\" height=\"99\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5238ecb4f4a97cf42692282562cfbd48f3bb95da_2_690x99.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5238ecb4f4a97cf42692282562cfbd48f3bb95da_2_1035x148.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5238ecb4f4a97cf42692282562cfbd48f3bb95da_2_1380x198.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5238ecb4f4a97cf42692282562cfbd48f3bb95da_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2478\u00d7358 42.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to stop logging locally but only save to wandb's servers and have wandb work using soft links?",
        "Question_creation_time":1666566211012,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-stop-logging-locally-but-only-save-to-wandbs-servers-and-have-wandb-work-using-soft-links\/3305",
        "Question_upvote_count":0.0,
        "Question_view_count":82.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am having a weird issue where I change the location of all my code &amp; data to a different location with more disk space, then I soft link my projects &amp; data to those locations with more space. I assume there must be some file handle issue because wandb\u2019s logger is throwing me issues. So my questions:<\/p>\n<ol>\n<li>how do I have wandb only log  online and not locally? (e.g. stop trying to log anything to <code>.\/wandb<\/code>[or any secret place it might be logging to]  since it\u2019s creating issues). Note my code was running fine after I  stopped logging to wandb so I assume that was the issue. note that the <code>dir=None<\/code> is the default to wandb\u2019s param.<\/li>\n<li>how do I resolve this issue entirely so that it works seemlessly with all my projects softlinked somewhere else?<\/li>\n<\/ol>\n<hr>\n<h1>\n<a name=\"more-details-on-the-error-1\" class=\"anchor\" href=\"#more-details-on-the-error-1\"><\/a>More details on the error<\/h1>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1087, in emit\n    self.flush()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1067, in flush\n    self.stream.flush()\nOSError: [Errno 116] Stale file handle\nCall stack:\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 930, in _bootstrap\n    self._bootstrap_inner()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/vendor\/watchdog\/observers\/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/vendor\/watchdog\/observers\/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/vendor\/watchdog\/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/filesync\/dir_watcher.py\", line 275, in _on_file_created\n    logger.info(\"file\/dir created: %s\", event.src_path)\nMessage: 'file\/dir created: %s'\nArguments: ('\/shared\/rsaas\/miranda9\/diversity-for-predictive-success-of-meta-learning\/wandb\/run-20221023_170722-1tfzh49r\/files\/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1087, in emit\n    self.flush()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1067, in flush\n    self.stream.flush()\nOSError: [Errno 116] Stale file handle\nCall stack:\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 930, in _bootstrap\n    self._bootstrap_inner()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 263, in _process\n    self._hm.handle(record)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 130, in handle\n    handler(record)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 138, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: stop_status'\nArguments: ()\nN\/A% (0 of 100000) |      | Elapsed Time: 0:00:00 | ETA:  --:--:-- |   0.0 s\/it\n\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_dist_maml_l2l.py\", line 1814, in &lt;module&gt;\n    main()\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_dist_maml_l2l.py\", line 1747, in main\n    train(args=args)\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_dist_maml_l2l.py\", line 1794, in train\n    meta_train_iterations_ala_l2l(args, args.agent, args.opt, args.scheduler)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/torch_uu\/training\/meta_training.py\", line 167, in meta_train_iterations_ala_l2l\n    log_zeroth_step(args, meta_learner)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/meta_learning.py\", line 92, in log_zeroth_step\n    log_train_val_stats(args, args.it, step_name, train_loss, train_acc, training=True)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/supervised_learning.py\", line 55, in log_train_val_stats\n    _log_train_val_stats(args=args,\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/supervised_learning.py\", line 116, in _log_train_val_stats\n    args.logger.log('\\n')\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logger.py\", line 89, in log\n    print(msg, flush=flush)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/lib\/redirect.py\", line 640, in write\n    self._old_write(data)\nOSError: [Errno 116] Stale file handle\nwandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nwandb: Synced vit_mi Adam_rfs_cifarfs Adam_cosine_scheduler_rfs_cifarfs 0.001: args.jobid=101161: https:\/\/wandb.ai\/brando\/entire-diversity-spectrum\/runs\/1tfzh49r\nwandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: .\/wandb\/run-20221023_170722-1tfzh49r\/logs\n--- Logging error ---\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router_sock.py\", line 27, in _read_message\n    resp = self._sock_client.read_server_response(timeout=1)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 283, in read_server_response\n    data = self._read_packet_bytes(timeout=timeout)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 269, in _read_packet_bytes\n    raise SockClientClosedError()\nwandb.sdk.lib.sock_client.SockClientClosedError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router.py\", line 70, in message_loop\n    msg = self._read_message()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router_sock.py\", line 29, in _read_message\n    raise MessageRouterClosedError\nwandb.sdk.interface.router.MessageRouterClosedError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1087, in emit\n    self.flush()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/logging\/__init__.py\", line 1067, in flush\n    self.stream.flush()\nOSError: [Errno 116] Stale file handle\nCall stack:\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 930, in _bootstrap\n    self._bootstrap_inner()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 973, in _bootstrap_inner\n    self.run()\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/threading.py\", line 910, in run\n    self._target(*self._args, **self._kwargs)\n  File \"\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/router.py\", line 77, in message_loop\n    logger.warning(\"message_loop has been closed\")\nMessage: 'message_loop has been closed'\nArguments: ()\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmpmvf78q6owandb'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmpt5etqpw_wandb-artifacts'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmp55lzwviywandb-media'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n\/home\/miranda9\/miniconda3\/envs\/metalearning_gpu\/lib\/python3.9\/tempfile.py:817: ResourceWarning: Implicitly cleaning up &lt;TemporaryDirectory '\/srv\/condor\/execute\/dir_27749\/tmprmk7lnx4wandb-media'&gt;\n  _warnings.warn(warn_message, ResourceWarning)\n<\/code><\/pre>\n<p>cross: <a href=\"https:\/\/stackoverflow.com\/questions\/74175401\/how-to-stop-logging-locally-but-only-save-to-wandbs-servers-and-have-wandb-work\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">python - How to stop logging locally but only save to wandb's servers and have wandb work using soft links? - Stack Overflow<\/a><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Embed Plotly Graphs in HTML",
        "Question_creation_time":1661353640713,
        "Question_link":"https:\/\/community.wandb.ai\/t\/embed-plotly-graphs-in-html\/2986",
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I would like to use some of my wandb plots to <a href=\"https:\/\/plotly.com\/python\/embedding-plotly-graphs-in-HTML\/\" rel=\"noopener nofollow ugc\">Embed Plotly Graphs in HTML<\/a>. Is there anyway to get similar functionality? If it cannot be done directly on wandb I see the other option as downloading the graphs, and uploading them to Plotly Chart Studio where they can then be embedded in HTML.<\/p>\n<p>I do like reports, but this would allow for the inclusion of plots in a wider scope of documents. For example, papers for publication, books, or even a dissertation.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep agents sometimes become extremely slow",
        "Question_creation_time":1663834079543,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-agents-sometimes-become-extremely-slow\/3171",
        "Question_upvote_count":0.0,
        "Question_view_count":306.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I am running a hyperparameter grid search using sweeps. I launched 4 agents on the same machine but I noticed that after completing one run, one of the agents is struggling with the next run.<\/p>\n<p>This agent seems to be still communicating with <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a>, the updated variable is regularly updated to the current time on the active run, and the agent itself has the same heartbeat as the others.<\/p>\n<p>The agent should be in its 8th run by now, but is only at the second one and only computed two epochs. All runs should take the same amount of time as they have the same number of epochs and model architecture.<\/p>\n<p>The \u201cLogs\u201d panel is also completely blank for this agent. And now that I\u2019m writing this, it seems another agent is starting to slow down.<\/p>\n<p>Also, I had the same issue in the previous sweep, this is the second time I run it with the same configuration. Previously, the issue appeared in the first few runs so I quickly stopped it and ran everything again.<\/p>\n<p>Is it most likely an issue with the CPU resources being not well distributed (all threads are at 100% usage), or could it be a network issue? How can I investigate what\u2019s happening?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"TypeError when uploading pixel value bounding box",
        "Question_creation_time":1656620400089,
        "Question_link":"https:\/\/community.wandb.ai\/t\/typeerror-when-uploading-pixel-value-bounding-box\/2684",
        "Question_upvote_count":0.0,
        "Question_view_count":195.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi,<br>\nI was just trying to log a 2D bounding box with pixel coordinates, but I keep on running into this error:<br>\n<code>TypeError: Object of type int is not JSON serializable<\/code><\/p>\n<p>The code I used:<\/p>\n<pre><code class=\"lang-auto\">box_data = []\n\nclass_labels = {\n    0: \"face\"\n}\n\nfor (x,y,w,h) in face_rects:\n    frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n\n    midX = int(x+w\/2)\n    midY = int(y+h\/2) \n    box = {\n                \"position\": {\n                    \"middle\": [midX, midY],\n                    \"width\": w,\n                    \"height\": h\n                },\n                \"domain\" : \"pixel\",\n                \"class_id\" : 0\n            }\n    box_data.append(box)\n\npredictions = {\"predictions\": {\n        \"box_data\": box_data,\n        \"class_labels\": class_labels\n    }\n    }\n\nimg = wandb.Image(frame, boxes=predictions)\n<\/code><\/pre>\n<p>It works when I\u2019m using the relational notation instead of pixel values, but I\u2019d rather keep the pixel values for simplicity in the code.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to create endpoint",
        "Question_creation_time":1553516561000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":301.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:\n\nFailure reason\nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets\n\nI have tried using different instance types but always the same error\n\nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Canvas connect Redshift failed",
        "Question_creation_time":1641573857407,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvjatAJaQv-Ist96WT1IIw\/sage-maker-canvas-connect-redshift-failed",
        "Question_upvote_count":0.0,
        "Question_view_count":135.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Try to add the Redshift connection on SageMaker Canvas to import the data\n\nThe cluster identify: redshift-cluster-1\ndatabase name: dev\ndatabase user: awsuser\nunload IAM Role: my-reshift-role\nconnection name: redshift\ntype: IAM\n\nmy-reshift-role trust-relationship is trust the \"redshift.amazonaws.com\" and \"sagemaker.amazonaws.com\"\n\nExpectation: create connection successfully\n\nActually result: RedshiftCreateConnectionError Unable to validate connection. An error occurred when trying to list schema from Redshift",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to use the same HumanWorkflow within textract for more than 1 file\/call",
        "Question_creation_time":1639923120530,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDgM77ZgnTbWjW57_v9rCGw\/unable-to-use-the-same-human-workflow-within-textract-for-more-than-1-file-call",
        "Question_upvote_count":0.0,
        "Question_view_count":50.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I created the private team from the Amazon SageMaker console for labeling tasks followed by the creation of the human review workflow, which I later integrated with the Amazon Textract for Key-Value pair extraction.\n\nWhile I called the analyze_document (along with HumanLoop configuration) to extract key-value pairs for the first time it worked as expected and I was able to see the Job in the labeling project console. However, when I called it again (irrespective of the same or different file) the HumanLoop started giving the below error.\n\n\"[ERROR] InvalidParameterException: An error occurred (InvalidParameterException) when calling the AnalyzeDocument operation: HumanLoop 'textractworkflow1' already exists and it is associated with a different InputContent. Please use a new HumanLoopName and try your request again.\"\n\nDo we have to create a new Human Review Loop each time we trigger analyze_document with another file?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do you analyze Autopilot results in Amazon SageMaker Studio?",
        "Question_creation_time":1601332827000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7p9B6zcpSIeTG4MbzrSjKA\/how-do-you-analyze-autopilot-results-in-amazon-sage-maker-studio",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I launched an Autopilot job in SageMaker Studio, and now I'm trying to figure out how to compare autoML iterations. Is there a way to list them, see their metrics, and see the configuration of the best job?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pros and cons of restricting user access to certain regions",
        "Question_creation_time":1642700804560,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello, Are there any drawbacks I should be aware of if we restrict user access to only a single region?\n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different. It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"aws textract human review flow, failed to load image",
        "Question_creation_time":1665415509779,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo48ev4bTTvO-GjsezfAmuQ\/aws-textract-human-review-flow-failed-to-load-image",
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI`m using aws textract to extract key-value pairs from an pdf. Because sometimes the accucary is low i use augmented AI (human review worflows) to involve a human worker. That works fine with png files, but when I use pdf files (which textract supports), I get an \"Failed to load image\". How do I get around this? I tried using a custom template, but can't find a way to insert the file type.\n\nBest regards,\n\nPaul",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to deploy lifecicle configuration to Sagemaker Studio via Cloudformation",
        "Question_creation_time":1666655893373,
        "Question_link":"https:\/\/repost.aws\/questions\/QUb1jvjl80RCClOggimxYaTQ\/how-to-deploy-lifecicle-configuration-to-sagemaker-studio-via-cloudformation",
        "Question_upvote_count":0.0,
        "Question_view_count":23.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI need to deploy a new lifecicle configuration to Sagemaker Studio. I was searching in the Cloudformation Documentation and found nothing but this service.\n\nhttps:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-notebookinstancelifecycleconfig.html\n\nAs the name of the service says, it was deployed as a Lifecicle config for a Notebook Instance.\n\nIs there any way to deploy a Lifecicle configuration for Sagemaker studio via Cloudformation ?\n\nThanks. Anderson",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Question_creation_time":1586331915000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sage-maker-model-artifact-during-a-training-job",
        "Question_upvote_count":0.0,
        "Question_view_count":182.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Question_creation_time":1603285551000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sage-maker-sizing-and-cost",
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price comparison than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to test locally SageMaker Inference Pipelines?",
        "Question_creation_time":1600158011000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8R_MjbU1QPm66SCgld4spQ\/is-it-possible-to-test-locally-sage-maker-inference-pipelines",
        "Question_upvote_count":0.0,
        "Question_view_count":202.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to test locally SageMaker Inference Pipelines? I would like to be able to easily troubleshoot and find the appropriate serialization between containers",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Amazon SageMaker container registration and deployment tracking",
        "Question_creation_time":1607710961000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdJqWN_WJQeuYrkZMikkibQ\/custom-amazon-sage-maker-container-registration-and-deployment-tracking",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"My customer asks that:\n\nContainer images must be registered and deployments tracked\n\nContainers must be registered within a private customer-owned registry prior to deployment\n\nOnly registered containers are to be deployed.\n\nPart of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities.\n\nAn inventory of all deployed containers must be maintained at all times.\n\nThe inventory must include: Software installed within the container version of all software and patch level . Where the container has been deployed . Owner of the container\n\nDo we do any of these? Please provide documentation on AWS\/SageMaker vs custom container provider's responsibilities.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tracking the lineage between Amazon SageMaker endpoint model and Model Monitor captured data",
        "Question_creation_time":1602088286000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPWBH_xFoS4aq5i41Za5qaQ\/tracking-the-lineage-between-amazon-sage-maker-endpoint-model-and-model-monitor-captured-data",
        "Question_upvote_count":0.0,
        "Question_view_count":84.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have an Amazon SageMaker endpoint with A1, a model with data capture activated, and I want to update the endpoint with A2, a new model.\n\nHow do I track the Model Monitor Data Capture that captured data in Amazon S3, and identify which data referred to model A1 and which data referred to model A2?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker AutoPilot Regions",
        "Question_creation_time":1596620138000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_7jk19ozQSeyjTAR_D_hEA\/sage-maker-auto-pilot-regions",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there official documentation showing the regions in which SageMaker AutoPilot is supported? From my understanding, it should work with the SDK wherever SageMaker is supported, while in the no-code mode only where SageMaker Studio is available. Is this true?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Question_creation_time":1597251203000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sage-maker-algorithms-can-only-use-gpu-for-training",
        "Question_upvote_count":0.0,
        "Question_view_count":121.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can only be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Greengrass for data processing and ML model training",
        "Question_creation_time":1556295399000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to version step functions for ML?",
        "Question_creation_time":1549456954000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Question_upvote_count":0.0,
        "Question_view_count":230.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, Step Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers Olivier",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Question_creation_time":1658343315554,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/file-not-found-error-errno-2-no-such-file-or-directory-home-ec-2-user-anaconda-3-envs-python-3-lib-python-3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Question_upvote_count":0.0,
        "Question_view_count":87.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_creation_time":1592577511000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Question_creation_time":1652344291417,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sage-maker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Question_upvote_count":0.0,
        "Question_view_count":201.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi all,\n\nI am asking if it's possible to use framework processor inside a sagemaker pipeline.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep, there isn't an available argument to specify the source_dir.\n\nThank you! Best, Ruoy",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker XGBoost Parquet Example Code Fails and Errors out. Bug?",
        "Question_creation_time":1648146766576,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqqbIbodsT42efRxxi1FLzw\/sage-maker-xg-boost-parquet-example-code-fails-and-errors-out-bug",
        "Question_upvote_count":0.0,
        "Question_view_count":117.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I'm trying to run the SageMaker XGBoost Parquet example linked here. I followed the exact same steps but using my own data. I uploaded my data, converted it to a pandas df. The train_df shape is (15279798, 32) while the test_df shape is (150848, 32). I then converted it to parquet files and uploaded it to an S3 bucket - per example instructions.\n\nMy error is as follows:\n\nFailure reason\nAlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/data_utils.py\", line 422, in _get_parquet_dmatrix_pipe_mode data = np.vstack(examples) File \"<__array_function__ internals>\", line 6, in vstack File \"\/miniconda3\/lib\/python3.7\/site-packages\/numpy\/core\/shape_base.py\", line 283, in vstack return _nx.concatenate(arrs, 0) File \"<__array_function__ internals>\", line 6, in concatenate ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 32 and the array at index 1 has size 9 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py\", line 94, in main train(framework.tr\n\n\n\nBut I'm confused because the train and test are the same shape and I added no extra code. My code below:\n\n# requires PyArrow installed\ntrain.to_parquet(\"Xgb_train.parquet\")\ntest.to_parquet(\"Xgb_test.parquet\")\n\n%%time\nsagemaker.Session().upload_data(\n    \"Xgb_train.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptrain\"\n)\n\nsagemaker.Session().upload_data(\n    \"Xgb_test.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptest\"\n)\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\njob_name = \"xgboost-parquet-example-training-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"Training job\", job_name)\n\n# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n\ncreate_training_params = {\n    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"Pipe\"},\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\"S3OutputPath\": bucket_path + \"\/\" + prefix + \"\/single-xgboost\"},\n    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 20},\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"max_depth\": \"5\",\n        \"eta\": \"0.2\",\n        \"gamma\": \"4\",\n        \"min_child_weight\": \"6\",\n        \"subsample\": \"0.7\",\n        \"objective\": \"reg:linear\",\n        \"num_round\": \"10\",\n        \"verbosity\": \"2\",\n    },\n    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptrain\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptest\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n    ],\n}\n\n\nclient = boto3.client(\"sagemaker\", region_name=region)\nclient.create_training_job(**create_training_params)\nprint(client)\nstatus = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\nprint(status)\nwhile status != \"Completed\" and status != \"Failed\":\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n    print(status)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio encountered an error when creating your project(github and codepipeline template)",
        "Question_creation_time":1658949790257,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOCKdskABQumCC7OnzBZR4g\/sagemaker-studio-encountered-an-error-when-creating-your-project-github-and-codepipeline-template",
        "Question_upvote_count":0.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I trying tutorial on \"MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline\". But I am getting error as shown in image",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to check smdistributed-modelparallel version?",
        "Question_creation_time":1660805972479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Question_upvote_count":0.0,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of smdistributed-modelparallel module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"confusion about PIPE mode when using S3 shard key",
        "Question_creation_time":1589363811000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s-3-shard-key",
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker PIPE Mode vs FSx ?",
        "Question_creation_time":1579692074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sage-maker-pipe-mode-vs-f-sx",
        "Question_upvote_count":0.0,
        "Question_view_count":124.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, SageMaker supports training data streaming via PIPE mode, and also reading from FSx distributed file system. Those options seem to provide same value: low latency, high throughput.\n\nWhat are the reasons for using one or the other?\nDo we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Question_creation_time":1584346040000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sage-maker-spot-training",
        "Question_upvote_count":0.0,
        "Question_view_count":51.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job? Is there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Question_creation_time":1540384039000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Question_upvote_count":0.0,
        "Question_view_count":654.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download?\n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location, train_instance_count=1, train_instance_type='ml.p3.8xlarge', train_volume_size=300, train_max_run = 52460*60 , security_group_ids='sg-00f1529adc4076841')\n\nThe output was: 2018-10-18 23:27:15 Starting - Starting the training job... Launching requested ML instances...... Preparing the instances for training... 2018-10-18 23:29:15 Downloading - Downloading input data............ .................................................................... .................................................................... .................................................................... 2018-10-19 00:23:50 Training - Downloading the training image..\n\nDataset download took ~54mins",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_creation_time":1660135320930,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\nEndpoint Configuration\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a ml.m4.xlarge instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried s3:\/\/<bucket-name> as well as s3:\/\/<bucket-name>\/<some-other-path>. With the \"Capture content type\" I tried leaving everything blank, setting text\/csv in \"CSV\/Text\" and application\/json in \"JSON\".\n\nEndpoint Invokation\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n\n\nInternally, the endpoint uses a Flask API with an \/invocation path that returns the result.\n\nLogs\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Question_creation_time":1609954586000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Question_upvote_count":0.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Question_creation_time":1592823369000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":636.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI want to use awswrangler package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use Lifecycle configuration. I tried to do it using the following script:\n\n#!\/bin\/bash\n\npip install awswrangler==0.2.2\n\n\nBut when I import that package into my Notebook:\n\nimport boto3                                      # For executing native S3 APIs\nimport pandas as pd                               # For munging tabulara data\nimport numpy as np                                # For doing some calculation\nimport awswrangler as wr\nimport io\nfrom io import StringIO\n\n\nI still get the following error:\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-1-f3d85c7dd0f6> in <module>()\n      2 import pandas as pd                               # For munging tabulara data\n      3 import numpy as np                                # For doing some calculation\n----> 4 import awswrangler as wr\n      5 import io\n      6 from io import StringIO\n\nModuleNotFoundError: No module named 'awswrangler'\n\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Question_creation_time":1596055479000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sage-maker-ground-truth-interface-option-to-skip-a-task-and-then-return",
        "Question_upvote_count":1.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Should SageMaker Canvas region and S3 region be the same?",
        "Question_creation_time":1658221373656,
        "Question_link":"https:\/\/repost.aws\/questions\/QULHZtj6HwQReouXor72UuSg\/should-sage-maker-canvas-region-and-s-3-region-be-the-same",
        "Question_upvote_count":0.0,
        "Question_view_count":56.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I'm going to use the canvas by connecting to S3. When using sagemaker canvas, should the canvas region and S3 region be the same? Thank you.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Pipe Mode",
        "Question_creation_time":1590161458000,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sage-maker-pipe-mode",
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_creation_time":1603454458000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":426.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoPilot for Forecasting",
        "Question_creation_time":1595154416000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu_IodOxiQ6eTu010AYb8pQ\/auto-pilot-for-forecasting",
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi there,\n\nIHAC who asked a question regarding the possible use of AutoPilot for solving Forecasting problems. They don't have the knowledge to play with DeepAR and they are running tests in parallel with Amazon Forecast. Their questions are:\n\nIs it possible to use AutoPilot for Forecasting problems? (my answer would be yes, since regression problems can be solved by XGBoost, which also won a bunch of competitions on Forecasting)\nWhich kind of pre-processing should the customer do and which pre-processing is done by AutoPilot which could simplify transformation of data for solving forecasting? In particular: are there any transformation to be done on the timestamp column? Should we introduce lagged entries - or is it done by AutoPilot?\n\nThanks to those taking the time to answer these questions :)\n\nBest, Davide Gallitelli",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ask AWS SageMaker",
        "Question_creation_time":1660876307100,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sage-maker",
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Can we make a new code through the sagemaker studio?\nIn my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\nHow much GPU performance can you improve compared to before?\nI want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Notebook Instance Types for SageMaker Studio",
        "Question_creation_time":1593107198000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sage-maker-studio",
        "Question_upvote_count":0.0,
        "Question_view_count":343.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to feed seed code to GitHub Repository from Sagemaker Projects Organization Template created with Service Catalog?",
        "Question_creation_time":1657259337697,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_Y4T-A3aQySFeRr3feBscA\/how-to-feed-seed-code-to-git-hub-repository-from-sagemaker-projects-organization-template-created-with-service-catalog",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"The objective is to replicate \"MLOps template for model building, training, and deployment with third-party Git repositories using Jenkins\" builtin Sagemaker Project template. I want to feed custom seed code to the Github repository each time a project is created using my organization custom template instead of the default seed code that the builtin template feeds.\n\nI am able to create the custom template using service catalog but I could not find a solution for feeding the seed code to github repo. So, I decided to see how the built in project template is doing this and it is using resources from this bucket \"s3:\/\/sagemaker-servicecatalog-seedcode-us-east-1\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\" but I could not access it. I am not sure how to achieve the objective?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Experiments Deletion Help Needed",
        "Question_creation_time":1668037196721,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sage-maker-experiments-deletion-help-needed",
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does Ground Truth Support Circles?",
        "Question_creation_time":1595625000000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqVY0A3PIQsuJpcce1tjJcA\/does-ground-truth-support-circles",
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Customer has circular objects in their data. Does Ground Truth support drawing circles rather than boxes out of the box (no pun intended)? I know that it supports semantic segmentation, but that is overkill in this case.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Question_creation_time":1604477936000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sage-maker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Question_upvote_count":0.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that in EC2 it's possible, however I don't see it mentioned in Amazon SageMaker documentation.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to automate sagemaker batch transform?",
        "Question_creation_time":1649085888036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyENAstk3Q_--wYwScAIq-A\/how-to-automate-sagemaker-batch-transform",
        "Question_upvote_count":0.0,
        "Question_view_count":546.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"does cloudformation support sagemaker batch transform? if yes, can the jobs be triggered\/run automatically once the stack is created?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"adjusting sagemaker xgboost project to tensorflow (or even just different folder name)",
        "Question_creation_time":1664391665708,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name",
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have sagemaker xgboost project template \"build, train, deploy\" working, but I'd like to modify if to use tensorflow instead of xgboost. First up I was just trying to change the abalone folder to topic to reflect the data we are working with.\n\nI was experimenting with trying to change the topic\/pipeline.py file like so\n\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"tensorflow\",\n        region=region,\n        version=\"1.0-1\",\n        py_version=\"py3\",\n        instance_type=training_instance_type,\n    )\n\n\ni.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when I run the following from a notebook:\n\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n\n\nI get the following error\n\nValueError                                Traceback (most recent call last)\n<ipython-input-5-6343f00c3471> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    188         version=\"1.0-1\",\n    189         py_version=\"py3\",\n--> 190         instance_type=training_instance_type,\n    191     )\n    192     tf_train = Estimator(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    197                 logger.warning(warning_msg_template, arg_name, func_name, type(value))\n    198                 kwargs[arg_name] = value.default_value\n--> 199         return func(*args, **kwargs)\n    200 \n    201     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    152             if inference_tool == \"neuron\":\n    153                 _framework = f\"{framework}-{inference_tool}\"\n--> 154         config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type)\n    155 \n    156     original_version = version\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type)\n    277         image_scope = available_scopes[0]\n    278 \n--> 279     _validate_arg(image_scope, available_scopes, \"image scope\")\n    280     return config if \"scope\" in config else config[image_scope]\n    281 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    443             \"Unsupported {arg_name}: {arg}. You may need to upgrade your SDK version \"\n    444             \"(pip install -U sagemaker) for newer {arg_name}s. Supported {arg_name}(s): \"\n--> 445             \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options))\n    446         )\n    447 \n\nValueError: Unsupported image scope: None. You may need to upgrade your SDK version (pip install -U sagemaker) for newer image scopes. Supported image scope(s): eia, inference, training.\n\n\nI was skeptical that the upgrade suggested by the error message would fix this, but gave it a try:\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npipelines 0.0.1 requires sagemaker==2.93.0, but you have sagemaker 2.110.0 which is incompatible.\n\n\nSo that seems like I can't upgrade sagemaker without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries.\n\nBut so is it that the \"framework\" name should be different, e.g. \"tf\"? Or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...?\n\nHowever I find that if I use the existing abalone\/pipeline.py file I can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook.\n\nI've searched all the files in the project to try and find any dependency on the abalone folder name, and the closest I came was in codebuild-buildspec.yml but that hasn't helped.\n\nHas anyone else successfully changed the folder name from abalone to something else, or am I stuck with abalone if I want to make progress?\n\nMany thanks in advance\n\np.s. is there a slack community for sagemaker studio anywhere?\n\np.p.s. I have tried changing all instances of the term \"Abalone\" to \"Topic\" within the topic\/pipeline.py file (matching case as appropriate) to no avail\n\np.p.p.s. I discovered that I can get an error free run of getting the pipeline from a unit test:\n\nimport pytest\n\nfrom pipelines.topic.pipeline import *\n\nregion = 'eu-west-1'\nrole = 'arn:aws:iam::398371982844:role\/SageMakerExecutionRole'\ndefault_bucket = 'sagemaker-eu-west-1-398371982844'\nmodel_package_group_name = 'TopicModelPackageGroup-Example'\npipeline_name = 'TopicPipeline-Example'\n\ndef test_pipeline():\n    pipeline = get_pipeline(\n        region=region,\n        role=role,\n        default_bucket=default_bucket,\n        model_package_group_name=model_package_group_name,\n        pipeline_name=pipeline_name,\n    )\n\n\nand strangely if I go to a different copy of the notebook, everything runs fine, there ... so I have two seemingly identical ipynb notebooks, and in one of them when I switch to trying to get a topic pipeline I get the above error, and in the other, I get no error at all, very strange\n\np.p.p.p.s. I also notice that conda list returns very different results depending on whether I run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[AI\/ML] Data acquisition and preprocessing",
        "Question_creation_time":1607357917000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data. The e-bike sensor data are size about 4KB files each and posted in S3 buckets. The sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ timestamp2, sensorA, sensorB, sensorC, ..., sensorZ timestamp3, sensorA, sensorB, sensorC, ..., sensorZ ...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\nRead S3 objects\nParse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\nPut the data in DynamoDB with bike ID as primary key and timestamp as sort key.\nUse SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\nIf we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\nAlso, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\nMake inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently? Would you rather learn from S3 directly via Athena or direct S3 access? Or would you rather use Glue and Redshift? But the data about 100MB would be sufficient to train the model we have in mind. Glue and Redshift maybe overkill. Currently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker and Data on Databases",
        "Question_creation_time":1533317474000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Question_upvote_count":0.0,
        "Question_view_count":208.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"A customer has a question about data sources\n\n\u201cmost of our data is stored in SQL databases, while the SageMaker docs say that I have to put it all in S3. It\u2019s not obvious what the best way to do this is. I can think for example of splitting my analysis code in two; one pre-processing step to go from SQL queries to tabular data, and e.g. store that as Parquet files. For high-dimensional tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SparkR not working",
        "Question_creation_time":1591029652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/spark-r-not-working",
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/ and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:\n\nAn error was encountered:\n[1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Question_creation_time":1601671292000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sage-maker-autopilot-for-my-time-series-modeling",
        "Question_upvote_count":0.0,
        "Question_view_count":166.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case. I understand that Amazon SageMaker Autopilot doesn't work with time series. Is there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ClientError: object_detection_augmented_manifest_training template",
        "Question_creation_time":1547563664000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/client-error-object-detection-augmented-manifest-training-template",
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n\n\nI now have 2 output manifest files with many lines of this:\n\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.\n\nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to 'application\/x-image' with Record wrapper type:RecordIO : 'ClientError: train channel is not specified.'\n\nI then changed the channel to train_annotation instead of train and I receive this error message: \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property\n\nAdditional information can be provided if neccessary.\nAny help would be much apreciated! Thank you.\n\nEdited by: LuciA on Jan 16, 2019 1:12 PM\n\nEdited by: LuciA on Jan 16, 2019 1:18 PM\n\nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio - create domain error",
        "Question_creation_time":1586796156000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Question_upvote_count":1.0,
        "Question_view_count":792.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User: arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:::domain\/*\u201d. We confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking. And aws sagemaker list-user-profiles returns no user profiles.\n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker MXNet local mode not working",
        "Question_creation_time":1537534843000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sage-maker-mx-net-local-mode-not-working",
        "Question_upvote_count":0.0,
        "Question_view_count":186.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\nbucket = 'XXXXXXXXXXX'\nprefix = 'sagemaker\/cifar-bench\/data'\n\ninputs = sagemaker_session.upload_data(\n    path='data',\n    bucket=bucket, \n    key_prefix=prefix)\n\nprint('data sent to ' + inputs)\n\n\nInception = MXNet('gluon_cifar_net.py', \n          role=role, \n          train_instance_count=1, \n          train_instance_type='local_gpu',\n          framework_version='1.2.1',\n          base_job_name='cifar10-inception-',\n          hyperparameters={'batch_size': 256, \n                           'optimizer': 'sgd',\n                           'epochs': 100, \n                           'learning_rate': 0.1, \n                           'momentum': 0.9})\n\n\nInception.fit(inputs)\n\n\nwhich returns an OSError: [Errno 2] No such file or directory\n\nIn the error log I can see that there seems to be error at self.latest_training_job = _TrainingJob.start_new(self, inputs) and self.sagemaker_client.create_training_job(**train_request)\n\nHow can I make the local mode work?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Question_creation_time":1596036787000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sage-maker-autopilot-doing-when-in-state-in-progress-analyzing-data",
        "Question_upvote_count":0.0,
        "Question_view_count":24.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass data to an endpoint",
        "Question_creation_time":1625081705000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Question_upvote_count":0.0,
        "Question_view_count":277.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Amazon SageMaker Built-in algorithms and Spot checkpointing",
        "Question_creation_time":1593596016000,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbWeXcwDT8i4dvXKE4HZXg\/amazon-sage-maker-built-in-algorithms-and-spot-checkpointing",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Which Amazon SageMaker built-in algorithms support checkpointing? In the documentation it says that:\n\nSageMaker built-in algorithms and marketplace algorithms that do not checkpoint are currently limited to a MaxWaitTimeInSeconds of 3600 seconds (60 minutes).\n\nHowever, in the algorithms I don't find any pointer to \"checkpoint\" or \"spot\". Can you help me out?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Hyperparameter Tuning Jobs over Training and Preprocessing",
        "Question_creation_time":1610658074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3xcWKDPHR8ylWSaX83lNKQ\/using-hyperparameter-tuning-jobs-over-training-and-preprocessing",
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Some data science teams want to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.\n\nDoes AWS have a recommended approach to establish this using Sagemaker Hyperparameter tuning?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Glue Interactive vs SageMaker Processing?",
        "Question_creation_time":1663171688837,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7J5WaZe3Qzi2giJaOmBFDQ\/glue-interactive-vs-sage-maker-processing",
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Greetings! I'm a data scientist working in SageMaker notebooks. I'd appreciate an explanation about when should I use Glue Interactive and not SageMaker Processing jobs. To my understanding, they are very similar and I can't differentiate them.\n\nThank you!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker with multiple models",
        "Question_creation_time":1587366119000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sage-maker-with-multiple-models",
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\nCan one SageMaker host more than one model? Each model then share the same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\nUsing Multi-model Inference endpoints: Amazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found here. The sample code can be found here. Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\nUsing Bring your own algorithm on SageMaker You can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker here\n\nUsing Multi-model serving container by using multi-model archive file You can find a sample example here [4] for tensorflow serving\n\nIf models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint Sagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Question_creation_time":1660653174738,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s-3-that-is-created-in-a-sagemaker-processing-container",
        "Question_upvote_count":0.0,
        "Question_view_count":115.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Error message: \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\nBackground: I am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n\n\nThank you in advance!!!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS AI\/ML integration with Power BI",
        "Question_creation_time":1607495476000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Question_upvote_count":0.0,
        "Question_view_count":148.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Question_creation_time":1667961704440,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sage-maker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm following this guide for transitioning to Amazon Linux 2 provided by AWS\n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found here",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Question_creation_time":1666360814590,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sage-maker-endpoint-to-zero",
        "Question_upvote_count":1.0,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). For this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used.\n\nAre there any costs associated with having an endpoint with 0 instances?\n\nThanks!",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Question_creation_time":1611507553000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-model-bias-monitor-and-model-explainability-monitor",
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links. https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can you configure Amazon ECR containers to be immutable?",
        "Question_creation_time":1607710511000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n\nContainers can't be changed after their built.\nContainers can't receive updates.\nChanges in the containerized application must require the building and deployment of a new container image.\nRuntime data and configurations must be stored outside of the container environment.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Glue + SageMaker Pip Packages",
        "Question_creation_time":1591210245000,
        "Question_link":"https:\/\/repost.aws\/questions\/QULN3fro-LQ1umpDN831VlZg\/glue-sage-maker-pip-packages",
        "Question_upvote_count":0.0,
        "Question_view_count":89.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"My customer is looking to use Glue dev endpoints along with a SageMaker notebook. What I've noticed is that in Glue, a package, in this case scipy, will be listed as 1.4.1, but this will or won't match what you get in a sagemaker notebook dependent on kernel.\n\nconda_python3:\n\n!pip show scipy\nName: scipy\nVersion: 1.1.0\nSummary: SciPy: Scientific Library for Python\nHome-page: https:\/\/www.scipy.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: seaborn, scikit-learn, sagemaker\n\n\nconda_tensorflow_p36:\n\n!pip show scipy\nName: scipy\nVersion: 1.4.1\nSummary: SciPy: Scientific Library for Python\nHome-page: https:\/\/www.scipy.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\nRequires: numpy\nRequired-by: seaborn, scikit-learn, sagemaker, Keras\n\n\nIs there some sort of best practice to use a kernel that corresponds directly to what's installed on Glue?\n\nSeparate not very useful question. I wasn't able activate the venv that Jupyter notebooks do via shell. Is it using a venv? How come I can't find the right activate script?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to compile model to Neuron: no error message, no output",
        "Question_creation_time":1663166637969,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_oVwSPQReCt96QyX4cz-g\/unable-to-compile-model-to-neuron-no-error-message-no-output",
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi. We are trying to convert all our in-house pytorch models to aws-neuron on inferentia. We successfully converted one, but the second model we tried did not compile. Unfortunately, compilation did not generate any error message nor log of any kind, so we are stuck. The model is rather simple, but large, U-Net, with partial convolutions instead of regular ones, but otherwise no fancy operators. Conversion of this model to torchscript is ok on the same instance. Could it be a memory problem ?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass environment variables in sagemaker tuner job",
        "Question_creation_time":1669725280762,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Question_upvote_count":0.0,
        "Question_view_count":9.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Question_creation_time":1568641875000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Question_upvote_count":0.0,
        "Question_view_count":134.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi Team,\nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.\nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e.. deployment package size is 50 MB.\nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.\nsample code for this api :\n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.\n\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n\nAny reference would be of great help. Thank you.",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create a serverless endpoint configuration?",
        "Question_creation_time":1645067206226,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Question_upvote_count":0.0,
        "Question_view_count":129.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed unknown parameter in ProductVariants [ 0 ]: \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
        "Question_creation_time":1664215795645,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEQiFVzOhR5q1XYhjlltO7w\/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Question_creation_time":1599771185000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Question_upvote_count":0.0,
        "Question_view_count":199.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks. I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row. The result is that I'm not able to use the \"Join source\" feature with \"Input - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cancel all child runs in Azure ML",
        "Question_creation_time":1649253299717,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/802549\/cancel-all-child-runs-in-azure-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How to I properly cancel all child runs in an Azure ML experiment? When I use the code below as expected from documentation, I get an error. \"RunConfigurationException:\nMessage: Error in deserialization. dict fields don't have list element type information. field=output_data, list_element_type=<class 'azureml.core.runconfig.OutputData'>...} with exception init() missing 2 required positional arguments: 'datastore_name' and 'relative_path'\"\n\nrun = Run.get(ws, 'run-id-123456789')\n\nfor child in run.get_children():\nprint(child.get_details())\ntry:\nchild.cancel()\nexcept Exception as e:\nprint(e)\ncontinue\n\nThe datasets and runs were configured properly because they run just fine.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does the \"Estimated monthly costs\" for Azure Machine Learning in the Price Calculator include all other non-compute \"additional resources\" created in the workspace",
        "Question_creation_time":1609267518950,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/213635\/does-the-34estimated-monthly-costs34-for-azure-mac.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When trying to use the Azure Pricing estimate in the Azure Pricing Calculator, the \"Estimated monthly costs\" seems to include but also far exceeds the compute cost. Does this Estimated Monthly cost include the other resources that get created?\neg. Azure Container Registry Basic account, Azure Block Blob Storage (general purpose v1), Key Vault\n\nThank you\nPeter",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning - error during the creation Create a control script",
        "Question_creation_time":1643753509637,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/718873\/azure-machine-learning-error-during-the-creation-c.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello, I am reproducing this tutorial https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world \/ Create a control script. The next observations appear in the console.\n\nI will thank you if some ideas are shared with me to face this issue.\n\nRegards",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"VBA And Azure Machine Learning Excel Add In",
        "Question_creation_time":1610149745593,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/224491\/vba-and-azure-machine-learning-excel-add-in.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi! I wanted to see if VBA and Azure Machine Learning Excel Add In can be connected to each other. Are there any way to code VBA (use VBA) for controlling or altering Azure Machine Learning Excel Add In? I have used Azure Machine Learning to rate candidate feedback as negative or positive, but it has like a 75 -80% success rate - there are still a good chunk of comments that are rated wrong. However, it is still an amazing tool that I want to use v- I was just wondering if I can increase the accuracy of it somehow by creating a VBA code that connects it to Azure Machine Learning where I can add words related to negative responses or vice versa for positive response to increase the accuracy.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML pipeline designer export",
        "Question_creation_time":1668149568060,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1085047\/azure-ml-pipeline-designer-export.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hey,\nIs there any way to export the ML Pipeline as Template\/PNG\/Code ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"machine learning server",
        "Question_creation_time":1610574178773,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/229665\/machine-learning-server.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI am not able to use my Jupyterlab and Jupyter Notebook. It cannot connect to a Kernel.\n\nWill this problem be solved when I uninstall Anaconda and install Microsoft Machine Learning Server.\n\nThanks,\n\nNaveen",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the best way to deploy my machine learning model using GPUs, specifically as a web based API?",
        "Question_creation_time":1630916125883,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/541074\/what-is-a-the-best-way-to-deploy-my-machine-learni.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to find the best way to run my machine learning models on GPUs for inference as an http request. Do Azure functions support GPUs? if not, what are other options I can look into?\n\nnote: I also want to use packaged models, not necessarily ones of my own creation (such as easyOCR for python)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Preparing ML object detction dataset for deep learning in PyTorch or similar",
        "Question_creation_time":1607567444300,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/192973\/preparing-ml-object-detction-dataset-for-deep-lear.html",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"The intent of what I'm trying to achieve is:\n\nExport data labelling project as a Dataset\n\n\nConsume the Dataset in a notebook (converting to a Pandas dataframe)\n\n\nPerform a custom train \/ test split that maintains particular file groupings\n\n\nRegister the resulting training and testing dataframes as Datasets\n\n\nUse these Datasets to train and test a custom object detection model\n\n\n\n\nI need help in preparing the data for that final step. I'm familiar with different deep learning libraries, but have never implemented them in the Azure environment before. I've managed to complete 1 to 4. For step 4, I ended up writing the data to csv files and uploading these to the datastore.\n\n # define path for training data file and create new delimited file\n train_path = '.\/data\/train.csv'\n train_dataframe.to_csv(train_path, sep = ';', index = False)\n    \n # repeat for testing\n test_path = '.\/data\/test.csv'\n test_dataframe.to_csv(test_path, sep = ';', index = False)\n    \n # get the datastore to upload prepared data\n datastore = Datastore.get(ws, datastore_name='learningdata')\n    \n # upload the local files from src_dir to the target_path in datastore\n datastore.upload(src_dir='data', target_path='train-test', overwrite=True)\n    \n # create and register training dataset from datastore files\n training_ds = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-test\/train.csv')], separator=';')\n training_ds = training_ds.register(workspace=ws, name = 'train', description = 'training dataset sampled from labelled data', create_new_version=True)\n    \n # create and register testing dataset from datastore files\n testing_ds = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-test\/test.csv')], separator=';')\n testing_ds = testing_ds.register(workspace=ws, name = 'test', description = 'testing dataset sampled from labelled data', create_new_version=True)\n\n\n\nThe approach I was intending to use for step 5 was to use to_torchvision() to convert it into a Torchvision dataset. This doesn't work, I receive the following error:\n\n UserErrorException: UserErrorException:\n  Message: Cannot perform torchvision conversion on dataset without labeled columns defined\n  InnerException None\n  ErrorResponse \n {\n     \"error\": {\n         \"code\": \"UserError\",\n         \"message\": \"Cannot perform torchvision conversion on dataset without labeled columns defined\"\n     }\n }\n\n\n\nI suspect that the issue has to do with DataTypes. The original Dataset (exported from the data labelling project) has the DataTypes displayed below. By comparison, all column types in the train and test Datasets are parsed as strings. From my understanding, there's no way to convert to these data types.\n\nimage_url = Stream\n\n\nlabel = List\n\n\nlabel_confidence = List\n\nAny advice on how to prepare this dataset for use in PyTorch or recommendation for an alternative approach would be greatly appreciated.\n\n\n\n\n\n\nUpdate as per comment below:\n\nI'm currently mounting the dataframe rather than downloading it due to data size.\n\n\nI can view images from the originally mounted Dataset, but when loading the newly registered training Dataset I can't access images as '\/tmp\/tmpog809x4v\/[...].jpg' is no longer relevant.\n\n\nI can't perform random split because I'm using clustered sampling.\n\n\nI'm working on creating a class object to define the dataset, but I cannot currently create the PIL Image object as required by PyTorch (https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html#defining-the-dataset)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Descriptors cannot not be created",
        "Question_creation_time":1661977116463,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989409\/descriptors-cannot-not-be-created.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"This error message is super confusing, what does it mean?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to install python package in hardware accelerated GPU spark pool ?",
        "Question_creation_time":1654683175267,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/881432\/how-to-install-python-package-in-hardware-accelera.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I've a azure synapse analytics workspace in region North Europe, as the region has hardware Accelerated pools, GPU base pools so to say. But i don't see the packages setting.\nhere is the comparison for 2 workspace, 1 in north Europe and other one in West Europe.\n vs \n\nEven the package setting in the Workspace itself is disabled for me: here is the screenshot.\n\n\nI've 2 questions in this reagrd:\n- Am I missing any configuration for the GPU pool or this feature is not released?\n- Is there any alternate way to install a package? pip install or pip3 install are not working.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Convert web service output to a dataset Azure MLS classic",
        "Question_creation_time":1592408319737,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/37214\/convert-web-service-output-to-a-dataset-azure-mls.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to convert a web service output as a dataset or a csv file ? I want to consume this in another experiment.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to deploy R script web service via Azure CLI",
        "Question_creation_time":1635213453407,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/603664\/how-to-deploy-r-script-web-service-via-azure-cli.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello everyone,\n\nI am tring to deploy R script as a web service using Azure Machine Learning. I created pipeline as below.\n\nI can deploy the model and endpoint from [Deploy] button but I cannot control some properties: i.e. resource name, dns name.\n\nIt seems that the az ml model deploy command can be used to deploy the endpoint.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli\n\nI have no information for inferenceconfig.json. How to write score.py to execute R script? Is it any example?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I extend the waiting time of Azure speech-to-text API in Python?",
        "Question_creation_time":1649087644937,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/799565\/how-do-i-extend-the-waiting-time-of-azure-speech-t.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When using speech-to-text to transfer audio file to text, I found that the function would stop working if human voices haven't occurred for about 5 seconds. In my case, what I want to transfer is audios of interviews, which would often contain some advertisements or music in the middle of it, and when this happens, the speech-to-text would only transfer the first half of the whole audio, and report an error that \"No speech could be recognized\".\nIn this case, how can I extend the waiting time of that in order to transfer the whole file in Python codes?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Question_creation_time":1629008927050,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/514067\/no-data-being-exported-from-39export-data39-module.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.\n\n\"Not exporting to run RunHistory as the exporter is either stopped or there is no data\"\n\n\"Process exiting with code: 0\n\nThere is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.\n\nWould appreciate some assistance.\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Anaconda commercial use on Azure Data Science Virtual Machine",
        "Question_creation_time":1614757334137,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/296502\/anaconda-commercial-use-on-azure-data-science-virt.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I would like to know if there is any problem in terms of license if enterprise companies use Anaconda that is preinstalled in Azure Data Science Virtual Machine. In another inquiry, I saw an answer that Anaconda included in Azure Machine Learning service has no problem in terms of the license but I would like to confirm whether DSVM also has a problem or not. https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About creating a computing cluster with Azure Machine Learning",
        "Question_creation_time":1635431417297,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/607903\/about-creating-a-computing-cluster-with-azure-mach.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello.\nYou can only select up to one maximum node when Create an Azure Machine Learning compute cluster. How do I select multiple nodes?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deep learning training on Azure steps and tutorial",
        "Question_creation_time":1605040090287,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/158168\/deep-learning-training-on-azure-steps-and-tutorial.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi MSFT Community,\n\nI followed this guide to set up a GPU: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro\n\nVM: Standard NC12_Promo, 12 vCPUs, 112 Gib RAM\nOperating System: Linux\nOffer: Ubuntu-1804\n\nI am ready to start deep learning training but I am confused about what to do next. I am doing a medical image classification project. I have 1 millions images store in Azure blob now. Do I need to download them to my VM in order to train? Or is it a better way to access image efficiently?\n\nWhat are some good tutorials to set up the experiments? I've read a lot of documentation but still confused.\n\nThank you very much!\nBest Regards,\nClaire",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML for SAP ERP",
        "Question_creation_time":1664541861543,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1030800\/azure-ml-for-sap-erp.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to figure out about standard connectors between SAP ERP product and Azure ML especially for NLP scenarios. Can you please suggest on this.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deployment of Multiple Models to Container Instance Fails in Azure DevOps",
        "Question_creation_time":1592223294357,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/36104\/deployment-of-multiple-models-to-container-instanc.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi Team,\n\nI am trying to deploy 2 ML models ( which is registered in Model Registry ) to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension\n\nMy ACI Configuration is :\n\ncontainerResourceRequirements: cpu: 1 memoryInGB: 4 computeType: ACI\n\nInference Config :\n\nentryScript: score.py runtime: python condaFile: conda_dependencies.yml extraDockerfileSteps: schemaFile: sourceDirectory: enableGpu: False baseImage: baseImageRegistry:\n\nAll score.py, conda_dependencies.yml, aciDeploymentConfig.yml is placed in a flattened directory which is publised in to DevOps pipeline artifcat and looks like\n\n\n\n\n\nDevOps Deploy command looks like\n\naz ml model deploy -g $(ml.resourceGroup) -w $(ml.workspace) --name $(service.name.staging) -f .\/model.json -m \"GloVe:4\" --dc aciDeploymentConfig.yml --ic inferenceConfig.yml --overwrite --debug\n\nAlso i have set the working directory as the folder where all above files are placed. something like\n\n$(System.DefaultWorkingDirectory)\/_Symptom-Code-Indexing\/symptom_model\/a\n\nIts getting in to an exception as\n\n2020-06-08T12:50:27.9202657Z \"error\": {\n2020-06-08T12:50:27.9208361Z \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"BadRequest\\\",\\\"statusCode\\\":400,\\\"message\\\":\\\"The request is invalid.\\\",\\\"details\\\":[{\\\"code\\\":\\\"InvalidOverwriteRequest\\\",\\\"message\\\":\\\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\\\"}],\\\"correlation\\\":{\\\"RequestId\\\":\\\"823e8483923846b1958c08ffaba074ff\\\"}}'\"\n2020-06-08T12:50:27.9212109Z }\n2020-06-08T12:50:27.9212376Z }}\n2020-06-08T12:50:27.9213437Z {'Azure-cli-ml Version': '1.6.0', 'Error': WebserviceException:\n2020-06-08T12:50:27.9214158Z Message: Received bad response from Model Management Service:\n2020-06-08T12:50:27.9214688Z Response Code: 400\n2020-06-08T12:50:27.9217800Z Headers: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\n2020-06-08T12:50:27.9222115Z Content: b'{\"code\":\"BadRequest\",\"statusCode\":400,\"message\":\"The request is invalid.\",\"details\":[{\"code\":\"InvalidOverwriteRequest\",\"message\":\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\"}],\"correlation\":{\"RequestId\":\"823e8483923846b1958c08ffaba074ff\"}}'\n2020-06-08T12:50:27.9223705Z InnerException None\n2020-06-08T12:50:27.9224049Z ErrorResponse\n2020-06-08T12:50:27.9224320Z {\n2020-06-08T12:50:27.9224617Z \"error\": {\n2020-06-08T12:50:27.9229025Z \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"BadRequest\\\",\\\"statusCode\\\":400,\\\"message\\\":\\\"The request is invalid.\\\",\\\"details\\\":[{\\\"code\\\":\\\"InvalidOverwriteRequest\\\",\\\"message\\\":\\\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\\\"}],\\\"correlation\\\":{\\\"RequestId\\\":\\\"823e8483923846b1958c08ffaba074ff\\\"}}'\"\n2020-06-08T12:50:27.9230782Z }\n2020-06-08T12:50:27.9230908Z }}\n2020-06-08T12:50:27.9231134Z Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x7fea2ca1f730>]\n2020-06-08T12:50:27.9231431Z az_command_data_logger : exit code: 1\n2020-06-08T12:50:27.9275693Z telemetry.save : Save telemetry record of length 7390 in cache\n2020-06-08T12:50:27.9280735Z telemetry.check : Negative: The \/home\/vsts\/work\/_temp\/.azclitask\/telemetry.txt was modified at 2020-06-08 12:47:41.161160, which in less than 600.000000 s\n2020-06-08T12:50:27.9290480Z command ran in 55.735 seconds.\n2020-06-08T12:50:28.1525434Z ##[error]Script failed with exit code: 1\n2020-06-08T12:50:28.1536650Z [command]\/opt\/hostedtoolcache\/Python\/3.6.10\/x64\/bin\/az account clear\n2020-06-08T12:50:29.9078943Z ##[section]Finishing: Deploy Model to ACI\n\nBut when i tried to Deploy it using Python SDK it works as well. Is there any permission issues or login to be set before using DevOps Release. I have not done any sort of login in my DevOps Build pipeline.\n\nAny pointers on what is going wrong here ? It would be really helpful.\n\nThanks,\nSrijith",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Retrieve Notebooks Azure Files",
        "Question_creation_time":1615959743450,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/317875\/retrieve-notebooks-azure-files.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is it possible to retrieve notebooks that were hosted on notebooks.azure.com? If so, how? The service is now discontinued but I would like to retrieve files that were hosted on the service.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error invoking the azure ML pipeline from Azure Devops",
        "Question_creation_time":1658316045857,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/934296\/error-invoking-the-azure-ml-pipeline-from-azure-de.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When I tried invoking an Azure ML pipeline from an Azure DevOps pipeline, I keep running into errors, Can you please share any sample that works.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure machine learning",
        "Question_creation_time":1656618921300,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/909965\/azure-machine-learning-2.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there any way to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer)?I know there is an integration between CI and Azure Machine Learning studio (classic). Please help to integrate these two services.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ambiguous error in Azure Machine Learning Designer 'Evaluate Model' Module",
        "Question_creation_time":1622567822803,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/418016\/ambiguous-error-in-azure-machine-learning-designer.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am getting the following error from the Evaluate Model module in Azure Machine Learning Designer:\n\n\nWhen I open the Assigned Data to Clusters module everything seems fine. I downloaded the output for Assigned Data to Clusters and played with cluster number 31 and there doesn't seem to be any issue. Additionally, I am using Azure Modules, so I am confused as to why this is failing. Please provide some clarity into this issue. This is a part of my pipeline:\n\nAdditionally, it seems unless I successfully run the Evaluate Model module, I cannot create an inference pipeline. If this is untrue, please help me out here as well. There is no option for me to 'Create an Inference Pipeline' which shown in this tutorial; step 1.\n\nPlease let me know if you need any other information.\n\nThanks in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can i access azure ml pipeline parameters from a python script running in designer?",
        "Question_creation_time":1620522230407,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/387875\/how-can-i-access-azure-ml-pipeline-parameters-from.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I would like to perform some data transformations using the Python script module in Designer for which i would need to access some pipeline parameters. How can i get those values?\n\nWhat would be the equivalent for an R script?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to deploy ML Designer pipeline as real-time inference pipeline using N-Gram",
        "Question_creation_time":1606307286153,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/175242\/how-to-deploy-ml-designer-pipeline-as-real-time-in.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\ni deployed a real-time inference pipeline using ML Designer. Training and deploying works fine. But when I'm consuming\/testing my API it doesn't work. Postman gives me Errorcode 500 and \"Internal Server Error. Run: Server internal error is from Module Extract N-Gram Features from Text\".\n\nThis is my training pipeline:\n\n\nI read this: https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams\n\nBut I don't know how to achieve this.\n\nThanks in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Studio error while testing real-time endpoint - list index out of range",
        "Question_creation_time":1645577589217,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am new to the Azure ML Studio and just deployed the bike-rental regression model. When I tried to test it using the built in test tool in the studio, I am getting the attached error. Similar results running the Python code as well. Can someone please help me?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Migrate to portal studio",
        "Question_creation_time":1653988049527,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/871064\/migrate-to-portal-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Experts,\n\nI just try studio classic which is good but retired soon\n\nI am moving to the new studio in azure portal. Any guidance for newbie?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Estimate the cost for Machine learning SDK or UI portal",
        "Question_creation_time":1658729307217,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/940045\/estimate-the-cost-for-machine-learning-sdk-or-ui-p.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello experts, we are working on a medium size solution for our company and we are exploring basic estimate for SDK or studio decision. How I can know?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Clear Feature with Auto ML",
        "Question_creation_time":1627927876257,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/498759\/clear-feature-with-auto-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello, I am trying to add a user Id column to my dataset but I don't want the user Id to impact the results of the ML.\n\nI am using Auto ML on my dataset to generate a model and then deployed the model to an endpoint.\n\nCurrently I am calling the endpoint like:\n\n {\"data\":[\n        {\n           \"TEMP\":\"X\",\n         }\n     ]\n }\n\n\n\nand I would like to call it like:\n\n {\"data\":[\n     {\n       \"TEMP\":\"X\",\n       \"userID\": 5434643\n      }\n   ]}\n\n\n\nI'm wondering if there is a way I can do this? I've seen about using Clear Feature in Edit Metadata for the Designer but I'm wondering if something similar can be done for automated ML?\n\nThanks so much!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Designer: Export Code",
        "Question_creation_time":1646150939773,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755142\/azure-ml-designer-export-code.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there an option to export the Azure ML Designer to code so we can copy between workspaces?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Not able to consume predict-auto-price endpoint",
        "Question_creation_time":1628625065883,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/508671\/not-able-to-consume-predict-auto-price-endpoint.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\nI was following Create a Regression Model with Azure Machine Learning Designer\n\nI reached to deploy and created an endpoint for the service. But when I click consume, it keeps on loading and after sometime page become unresponsive. What can be the possible reasons for this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"From AMLS Deploying models in ACI in a vnet",
        "Question_creation_time":1601173833227,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/108659\/from-amls-deploying-models-in-aci-in-a-vnet.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am facing error when I deploy in ACI. Is there a way to deploy the models when AMLS and vnet are in different resource groups?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Question_creation_time":1645621539517,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiment39s.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.\nI cannot find log\/user folder with \"Logs generated when loading and running user's scripts.\"\n\nreadme.txt file states:\nParallelRunStep has two major parts:\n1. Scheduling, progress tracking and file concatenation for append_row.\n2. Processing mini batch by calling the entry script.\nThe agent manager on each node start agents.\nAn agent gets mini batch and calls the entry script against the mini batch.\n\n The \"logs\" folder has user, sys and perf sub folders.\n The user folder includes messages from the entry script in processing mini batches.\n The sys folder includes messages from #1 and non-entry script log from #2.\n The perf folder includes periodical checking result of resource usage.\n\n\n\nIn majority case, users can find the processing messages from the user folder.\nUsers need to check sys folder for messages beyond processing mini batches.\nlogs\/\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep\nuser\/ : Logs generated when loading and running user's scripts.\nerror\/ : Logs of errors encountered while loading and running entry script.\nstderr\/ : stderr output of user's scripts.\nstdout\/ : stdout output of user's scripts.\nentry_script_log\/ : Logs generated by loggers of EntryScript()\n<node seq> :\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Parallel computing with Python SDK V2",
        "Question_creation_time":1657198500433,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/918129\/parallel-computing-with-python-sdk-v2.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello :)\n\nDo you have any kind of idea when Azure Machine Learning Python SDK V2 could support parallel computing? We are testing things out with the machine learning studio and we are in a bit confusing stage that should we go with the SDK V1 or V2, but seemingly the V2 is not yet supporting multiple nodes in compute clusters.\n\nBest regards,\nTuomas",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Question_creation_time":1602855221470,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-34modu.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:\n\ncreated a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)\n\n\nregistered a custom environment in AML Workspace, based on the image in ACR\n\n\ndeployed R entry script (just a simple hello world script with init() and run() functions defined)\n\n\nthe inference configuration uses the custom AML environment\n\n\ndeployment is made with Azure ML R SDK\n\nThe container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:\n\n 2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n 2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n 2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n 2020-10-16T12:56:21,735291424+00:00 - nginx\/run \n EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n 2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n 2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\n Starting gunicorn 20.0.4\n Listening at: http:\/\/127.0.0.1:31311 (11)\n Using worker: sync\n worker timeout is set to 300\n Booting worker with pid: 38\n \/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\n SPARK_HOME not set. Skipping PySpark Initialization.\n Exception in worker process\n Traceback (most recent call last):\n   File \"\/var\/azureml-server\/app.py\", line 43, in <module>\n     from azureml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azureml.api'\n    \n During handling of the above exception, another exception occurred:\n    \n Traceback (most recent call last):\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker\n     worker.init_process()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 119, in init_process\n     self.load_wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 144, in load_wsgi\n     self.wsgi = self.app.wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\n     self.callable = self.load()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 49, in load\n     return self.load_wsgiapp()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 39, in load_wsgiapp\n     return util.import_app(self.app_uri)\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py\", line 383, in import_app\n     mod = importlib.import_module(module)\n   File \"\/usr\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\n     return _bootstrap._gcd_import(name[level:], package, level)\n   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n   File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n   File \"\/var\/azureml-server\/wsgi.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\n     from app import main\n   File \"\/var\/azureml-server\/app.py\", line 45, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n Worker exiting (pid: 38)\n Shutting down: Master\n Reason: Worker failed to boot.\n 2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n 2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n\n\n\nHow do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:\n\n RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\n RUN pip install azureml-core\n\n\n\nI also have Miniconda installed. Pip refers to Miniconda's pip.\n\nOr, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deployment from Designer fails in every possible way",
        "Question_creation_time":1632862175993,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569925\/deployment-from-designer-fails-in-every-possible-w.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I trained a model with Designer, created a real-time inference pipeline which was succesfully submitted. When deploying to either ACI or AKS it fails and I get the error \"ModuleNotFoundError: No module named 'azureml.api'\". I've had no problems deploying this model many times in the past and haven't changed anything. Even if I use one of the sample pipelines (automobiles basic), I get the same error when deploying to real-time.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"getting an error when trying to deploy azure ml model",
        "Question_creation_time":1639419111370,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/662007\/getting-an-error-when-trying-to-deploy-azure-ml-mo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm new to Azure ML so I have very little knowledge of this service..\nI've built a dummy regression model using automl package and now I'm trying to deploy it.\nI looked up some docs and followed a tutorial I found to deploy the model and I'm getting some errors..\n <- this is the error I'm currently getting\nI think there is a problem with my score.py so I'm attaching the photo here as well.\n\n\nand this is the output i need to print out through the model..\n\n\nI'd appreciate it much if somebody could give me some help\n\nthank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I export my project from Azure",
        "Question_creation_time":1654324217247,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/876500\/how-do-i-export-my-project-from-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi\n\nI would like to know how I keep the studio and all the elements I have done in Azure.\n\nI need them for a project but my subscription is expired and I'd like to keep what I've done.\n\nI can't afford paying for a new subscription.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning Jupyter Lab Git options",
        "Question_creation_time":1594373340610,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/45187\/azure-machine-learning-jupyter-lab-git-options.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\nI&#39;m trying to setup an integration between a GITHub repository and my Jupyter Lab but I&#39;m struggling to find the GIT options in my Jupyter Lab application.\n\nI was expecting to see a Git clone button, a Git option on the toolbar and also the same option on the left pane but there is nothing GIT related.\n\nI&#39;ve already installed successfully the following:\n\npip install jupyterlab-git\npip install --upgrade python-gitlab\n\nBut nothing happens.\nWhen I try to clone a GIT repository, I get the folders\/files but then I can&#39;t interact with it. It&#39;s just copying it into my space but then I can&#39;t push\/pull anything.\n\nCan you help me on this?\n\nThank you,\nCarla",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":30.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"azure cosmos db as a datastore in ml",
        "Question_creation_time":1597331768270,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/66297\/azure-cosmos-db-as-a-datastore-in-ml.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I'm wondering if I can register azure cosmos db as a datastore in azure machine learning?\nFrom your documentation, it seems not https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py\n\nDo you have a plan to implement the feature in near future?\n\n\nAny recommended alternative solutions for now?\n\nThanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training a TensorFlow model in Azure ML",
        "Question_creation_time":1649367124903,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/804968\/training-a-tensorflow-model-in-azure-ml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am following the link below for training a TensorFlow model in Azure ML:\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\n\nHowever, as my training dataset is in a container named \"sample-datasets\" in ADLS Gen2, I changed the following code (in the above link) to refer to the paths in my data lake. So I replaced code A (in the link above) with code B (my code)\n\nCode A:\n\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\n\n\n\nCode B:\n\nfrom azureml.core.dataset import Dataset\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\nBut I receive the following error:\n\nHTTPError: HTTP Error 401: Server failed to authenticate the request. Please refer to the information in the www-authenticate header.\n\nCan you please let me know how I can train the model using my data which are stored in the data lake? More precisely, how my Python code can copy the training dataset from my data lake into data_folder?\n\nPS: Please note that I have already granted the Blob Storage data Contributor role on my data lake storage account to my Azure ML workspace as a managed identity.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the difference between uri_file and uri_folder in components?",
        "Question_creation_time":1655434518570,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/892897\/what-is-the-difference-between-uri-file-and-uri-fo.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"What is the difference between uri_file and uri_folder in components?\n\nNo matter I specify uri_file or uri_folder in a component input\/output type, in Azure ML Studio jobs it is displayed as uri_folder and I still need to manually append a file name to the path derefernced by uri_file to access a single file. Is there any convenience or difference to specify uri_file if I only intend to access a single file?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"NEW Azure ML vs On-Prem SQL",
        "Question_creation_time":1638277834353,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/646058\/new-azure-ml-vs-on-prem-sql.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI understand there was a process how to connect to on-prem sql db from Azure ML studio, but with the transition to the new UI, I don't see the option to connect to the gateway. I have it successfully installed and registered in MS Azure, but from Studio it simply does not offer it as a dataset type when using the Import Data module.\nI can't find any documentation regarding the new UI nor any useful guides for this.\n\nWould anybody know whether this function is still available in the new studio and if so how can an on-prem gateway be connected?\n\nThank you,\nVS",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":22.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Subscription Cost",
        "Question_creation_time":1644315390737,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/726898\/azure-subscription-cost.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"my azure subscription cost is decreasing everyday. Knowing that i have deleted everything from my workspace and in my azureml workspace don't have any cluster, I don't know why it is still decreasing.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to use private docker registry with latest Azure ML release",
        "Question_creation_time":1604958506767,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/157021\/unable-to-use-private-docker-registry-with-latest.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Since the latest Azure ML release, we have been unable to submit any job using a private docker registry. Same jobs were working before the new release.\nWe configure the job as follows (all of this is automated and the code has not changed):\n\nbase_image_name = &#39;REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_1601582281&#39;\n\n # Set the container registry information\n myenv = Environment(name=&#34;lb&#34;)\n myenv.docker.enabled = True\n myenv.docker.base_image = base_image_name\n myenv.docker.base_image_registry.address = &#39;REDACTED.azurecr.io\/lb\/&#39;\n myenv.docker.base_image_registry.username, myenv.docker.base_image_registry.password = get_docker_secrets()\n myenv.python.user_managed_dependencies = True\n myenv.python.interpreter_path = &#34;\/opt\/miniconda\/bin\/python&#34;\n\n\n\nInstead of successful job submission, we are instead getting:\n{\n&#34;error&#34;: {\n&#34;message&#34;: &#34;Activity Failed:\\n{\\n \\&#34;error\\&#34;: {\\n \\&#34;code\\&#34;: \\&#34;UserError\\&#34;,\\n \\&#34;message\\&#34;: \\&#34;Unable to get image details : Specified base docker image REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_16\\&#34;,\\n \\&#34;details\\&#34;: []\\n },\\n \\&#34;correlation\\&#34;: {\\n \\&#34;operation\\&#34;: null,\\n \\&#34;request\\&#34;: \\&#34;c41448d429f9c80b\\&#34;\\n },\\n \\&#34;environment\\&#34;: \\&#34;eastus\\&#34;,\\n \\&#34;location\\&#34;: \\&#34;eastus\\&#34;,\\n \\&#34;time\\&#34;: \\&#34;2020-11-09T21:40:39.699533Z\\&#34;,\\n \\&#34;componentName\\&#34;: \\&#34;execution-worker\\&#34;\\n}&#34;\n}\n}\nThe image has not changed (we tried a few different ones from prior successful jobs) and the use of the SDK has not changed.\nHas anybody else encountered a similar problem since the Nov 5 upgrade (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes)?\nThis is a major block as we cannot proceed with any project that depend on Azure ML at this time.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure machine learning designer - edit columns stuck on loading",
        "Question_creation_time":1617486098190,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/343332\/azure-machine-learning-designer-edit-columns-stuck.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Azure machine learning designer :\n\nI have a dataset on the designer connected to a Normalize data module but it keeps loading when i try to Edit columns on Normalize data module with no result or errors.\nThe same thing happens with Select columns in dataset module.\n\nI have tried to recreate and restart and even deleted the whole resource group but no luck.\nI tried on both mac and windows with different browsers but still getting stuck on the same place.\n\nany idea on how to solve this issue?\n\nThanks!\n\nScreenshot:\nhttps:\/\/i.imgur.com\/P0oWrGR.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":12.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a cross-sectional, unified security check list for Azure?",
        "Question_creation_time":1638435378937,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/648921\/is-there-a-cross-sectional-unified-security-check.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have been used Azure for the first time, and I am overwelmed by the huge quantity of information about Azure.\n\nI think that the information about security on Azure is not unified.\n\nFor example, in Identity Management and access control security best practices page, sometimes there are multiple best practices per one section header.\nHowever, in Security recommendations for Blob storage page,security recommendations are documented in the form of table, one issue per one row.\n\nI wish there was a cross-sectional, unified security check list for Azure as follows.\n\nWe could select Azure services we use.\n\n\nWhen we select the services, the security check list are displayed or could be downloaded as text file.\n\n\nThe security check list are documented so that we can easily understand what we should do. (where on the Azure portal UI, which item, or how to do set the item which is related to security, etc)\n\nI have used Azure services as follows.\n\nAzure Data Factory\n\n\nAzure Data Lake Storage Gen2\n\n\nAzure Functions (App Service)\n\n\nAzure Database for MySQL\n\n\nAzure Machine Learning\n\n\nAzure Monitor (for Application Insights)\n\nEven if I take one service (for example, Azure Data Lake Storage Gen2), I think that I have to check at least two pages (here and here ).\nHowever, I'm not sure if it's covered. Do you have any good ideas?\n\nRegards.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":21.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Kernel not connected",
        "Question_creation_time":1645626985700,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747823\/kernel-not-connected.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have an azureml studio with a notebook and suddenly since today, I cant run notebooks cells anymore. It says kernel not connected.\nI cant either open the terminal it never loads.\n\nI restarted the compute instance several time, but that didnt fix the problem",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"update real interference pipeline",
        "Question_creation_time":1615298736310,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/305899\/update-real-interference-pipeline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I deployed my Training pipeline and my Real-time inference pipeline.\nWith the REST-Api of my training pipeline I'm able to retrain my ML model. Is it possible to use that retrained model automated in my real inference pipeline?\nWhen i trigger the pipeline in ML studio I have to update my real inference pipeline manually. Since I want to trigger my retraining external that is not possible.\nThanks in advance.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"need guidance to use train models in ML studio",
        "Question_creation_time":1664405211343,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1027872\/need-guidance-to-use-train-models-in-ml-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"hello new to ML studio. We have some trained model already but I want to use the studio for my next step. How should I import my model and retrain them?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trigger Azure ML Pipeline from Azure Data Factory",
        "Question_creation_time":1645081864137,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/739240\/trigger-azure-ml-pipeline-from-azure-data-factory.html",
        "Question_upvote_count":5.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have created and published a Azure ML pipeline. I want to trigger the ML pipeline from Azure Data Factory.\n\nIn ADF, i have chosen Machine learning execute pipeline and created the linked service to azure machine learning and able to choose the published pipeline endpoint. However while running, i am getting the below error. I couldn't find much information how to resolve the error.\n\n\"Convert Failed. The value type 'System.String', in key 'azureCloudType' is not expected type 'Microsoft.DataTransfer.Common.Models.AzureCloudType\"",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":10.0,
        "Question_follower_count":30.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Send new data to Deployed model",
        "Question_creation_time":1619546083177,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/374180\/send-new-data-to-deployed-model.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nWe are sending data from a smartwatch -> IoT Central -> Event Hubs -> Data Explorer -> Blob Storage.\n\nWe are then using the blob storage as a datastore in Machine Learning, which we make a dataset of.\n\nWe deployed a model we trained locally to Azure Machine Learning.\n\nWe now want to send new data from the watch to the model to make predictions on.\n\nWe are wondering how we can do this?\n\nDo we just update the dataset the same way we are currently sending the data? and if so, how can we then auto send it to the model?\n\nOr is there another way to send this new data? Can we still send through blob storage? Or should we send the data directly from the watch to the webservice made by the model?\n\nThanks so much!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Executing pipeline in AML from ADF suddenly stopped working",
        "Question_creation_time":1639398869747,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/661588\/how-to-run-a-pipeline-in-aml-from-adf.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a pipeline defined in Azure Machine Learning. It was launched every day with Azure Data Factory with Machine Learning Execute Pipeline activity. This solution worked without any issues for a few weeks, but since 12\/09\/2021 all pipeline runs have failed with error: User starting the run is not an owner or assigned user to the Compute Instance.\nI did not change anything in ADF or AML.\n\nShould I assign compute to ADF? How to do this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"azure guidance video out of date in YouTube",
        "Question_creation_time":1659301109740,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/949113\/azure-guidance-video-out-of-date-in-youtube.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"hello, I was doing some search in the internet about azure machine learning, I found videos published in YouTube but out of date. Please update it",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Attaching local computer to ML Studio and use it with Azure AutoML and Azure Designer",
        "Question_creation_time":1624632132377,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/452250\/attaching-local-computer-to-ml-studio-and-use-it-w.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hey there,\n\nI was wondering, whether it is possible to connect your local computer as a compute target to the workspace and then access it as a compute target for AutoML and the Designer in the ML Studio (instead of a compute cluster)?\nI have read through the documentation and I feel like if this is possible, it is not very well-documented.\n\nThanks in advance!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoML : TensorFlowDNN and TensorFlowLinearRegressor are blacklisted by default",
        "Question_creation_time":1653464529937,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/863297\/automl-tensorflowdnn-and-tensorflowlinearregressor.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI am running an AutoML experiment for a regression task, and looking at the YAML file which is generated it seems that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'.\n\nI tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything.\n\n automl_settings = {\n     \"primary_metric\": 'normalized_mean_absolute_error',\n     \"featurization\": 'auto',\n     \"verbosity\": logging.INFO,\n     \"n_cross_validations\": 5,\n     \"auto_blacklist\": False,\n     \"blacklist_models\": None,\n     \"blacklist_algos\": None\n }\n run = experiment.submit(automl_config, show_output=True)\n\n\n\nThe generated YAML file (excerpt):\n\n \"whitelist_models\":null,\n \"blacklist_algos\":[\"TensorFlowDNN\",\"TensorFlowLinearRegressor\"],\n \"supported_models\":[\"ElasticNet\",\"GradientBoosting\",\"LightGBM\",\"TensorFlowLinearRegressor\",\"TensorFlowDNN\",\"LassoLars\",\"DecisionTree\",\"RandomForest\",\"FastLinearRegressor\",\"OnlineGradientDescentRegressor\",\"ExtremeRandomTrees\",\"TabnetRegressor\",\"XGBoostRegressor\",\"KNN\",\"SGD\"],\n \"private_models\":[],\n \"auto_blacklist\":false\n\n\n\nMaybe the problem comes from the fact that Deep learning is set to 'Disabled' in the configuration settings, as shown on the following picture:\n\nAre deep learning models not supported anymore by AutoML?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"UserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.",
        "Question_creation_time":1620234016640,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/384152\/userprocesskilledbysystemsignal-job-failed-since-t.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I am getting this error when scoring a model.\n\nSeems like it is an out-of-memory issue or a segfault issue (no idea what that means).\n\nI'm using Designer while my compute is Standard Dv2 Family vCPUs. Have made no changes to my storage account key.\n\nAny advice on how to debug this one? Many thanks in advance\n\nAzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\nReason: Process Killed with either 6:aborted or 9:killed or 11:segment fault. exit code here is from wrapping bash hence 128 + n\nCause: killed\nTaskIndex:\nNodeIp: 10.0.0.5\nNodeId: tvmps_ee452edcf7395836bdf60c0e0cd5f3a6308fafbb41c860c50a47be1367393df6_d\nReason: Job failed with non-zero exit Code",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"is move workspace easy?",
        "Question_creation_time":1664405263187,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1027803\/is-move-workspace-easy.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"we are not sure about our location but we do want to start now. If we need to move our workspace, is that possible? easy to accomplishing? Thank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to delete Azure ML real-time endpoints which is in transition state",
        "Question_creation_time":1628858328673,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/513012\/how-to-delete-azure-ml-real-time-endpoints-which-i.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI have made deployment of the model from the AutoML experiment, due to the issue in the resources associated. Deployment has failed.\n\nBut the real-time endpoint has been in the transition state for few hours, I can't delete it and the model registered along with it due to this. How can I force delete in this case. Please provide a solution.\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Request help with Azure machine learning workspace",
        "Question_creation_time":1655371059130,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/891716\/request-help-with-azure-machine-learning-workspace.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have enrolled myself in Azure Machine Learning course and the first step there is to create an azure ML workspace with subscription, resource group, region, storage account etc. I am a new joiner and I am doing this for my learning. Not sure which option to select. Is there any guidance or doc to follow? I have checked with my team and they are suggesting to use my personal account to get a demo account and free azure subscription to do the course and not my microsoft credentials. Require assistance in this regard.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problems connecting to workspace using Azure Machine Learning SDK for Python",
        "Question_creation_time":1606083056563,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171465\/problems-connecting-to-workspace-using-azure-machi.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to connect to my Azure ML workspace using SDK for python, using Virtual Studio Code to do so. After pip installing the needed SDK packages:\npip install azureml-sdk\npip install azureml-sdk[notebooks,automl,explain]\n\nI downloaded the .json configuration file for my workspace, made sure it was in the correct location for the file path and tried the following code (with my subscription id, resource group and workspace name in place of the fillers in this chunk of code):\n\n {\n     \"subscription_id\": \"1234567-abcde-890-fgh...\",\n     \"resource_group\": \"aml-resources\",\n     \"workspace_name\": \"aml-workspace\"\n }\n\n\n\nUpon executing this in my ipy kernel in Virutal Studio Code I got a UserErrorException (see image below, I have blocked out subscription id's and other sensitive information):\n\n\n\n\n\n\nI then tried this alternative way to connect to my workspace using the following code (again with my info filled in instead of the fillers in the code):\nfrom azureml.core import Workspace\n\n from azureml.core import Workspace\n    \n ws = Workspace.get(name='aml-workspace',\n                    subscription_id='1234567-abcde-890-fgh...',\n                    resource_group='aml-resources')\n    \n ws = Workspace.from_config()\n\n\n\nThis produced the same error upon execution. I have tried using different subscriptions with different workspace names and resource groups and it gives me the same error every time. It appears to be telling me I do not have access to the subscription that I am logged in to? I am unsure how to fix this. I am trying to do this as part of the lessons in the Microsoft Azure Data Scientist certification if anyone is familiar with that or has run into the same problem while trying to complete the modules for that certification provided through Microsoft.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML pipeline from an Azure DevOps pipeline",
        "Question_creation_time":1667236694490,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1069739\/azure-ml-pipeline-from-an-azure-devops-pipeline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Tried invoking an Azure ML pipeline from an Azure DevOps pipeline ? I keep running into errors, so I want to make sure my high level process is correct.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem: AML Designer - Batch Inference Pipeline",
        "Question_creation_time":1639744934170,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/667479\/problem-aml-designer-batch-inference-pipeline.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi Team,\n\nWhen I Submit the Batch Inference Pipeline. It is working.\n\n\n\nAfter submitting, I can see the file:\n\n\nThen when I Publish, the file is not in the Datastore. The file is not generated again. I didn't get an error.\n\nKind regards,\nAnaid",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure OpenAI advantages",
        "Question_creation_time":1648125540293,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/785866\/azure-openai-advantages.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We have a customer that is interested in the Azure OpenAI Service and had a few question:\n\nWhat are the advantages for using Azure OpenAI vs. OpenAI API.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to creata a compute instance",
        "Question_creation_time":1616247083753,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323742\/unable-to-creata-a-compute-instance.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to follow the steps given here - https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data\n\nI've tried regions east us2 and east us for creating the instance but it fails after taking more than half an hour. I tried virtual machine sizes - Standard_DS11_v2 & Standard_DS3_v2.\n\nAny help would be appreciated.\n\nEdit - I don't have any other instances running in my subscription, so it should not be a quota issue. The error message says \"An internal server error occurred.\".",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"list of folder names as input for ParallelRunStep-class",
        "Question_creation_time":1647256395817,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/771015\/list-of-folder-names-as-input-for-parallelrunstep.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"In this example, all data files for the parallel run step are stored in one folder.\n\nI also want to create a parallel run step. The task for each of the several folders, in which the multiple data files are stored, is exactly identical.\n\nThe folders:\n\n\n\n\n\nThe content of each folder:\n\n\n\n\n\nHow should I define the ParallelRunStep-class so that the identical task for each folder (here 'a', 'b', 'c', 'd' and 'e') is executed in parallel?\nTwo folders should run simultaneously in parallel.\n\nMoreover, I would like to ask how to get only the stored folder names or folder paths from a given directory path of a blob storage container.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify HTTP response status code in AML R Web Service",
        "Question_creation_time":1612354860267,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/257156\/how-to-specify-http-response-status-code-in-aml-r.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there any way to return a custom HTTP status code from R Web Service in Azure ML?\n\nAll the examples of entry scripts in documentation return the response body from the scoring function. In Python Web Service, it is possible to return a HTTP response object with a custom status code. However, R's httr library does not seem to have any function to create response objects directly (only via HTTP method objects such as POST, which call a given URL).\n\nI would like to implement a custom exception handling scheme in R Web Service. Is there any way to return a custom HTTP code from the entry script?\n\nEDIT: Found this idea on the feedback forum, which suggests that the option is not available in Python Web Service either:\nhttps:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/40122838-make-http-status-codes-controllable-from-your-scor",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Converting textanalytics result to JSON Format",
        "Question_creation_time":1650967532993,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/826603\/converting-textanalytics-result-to-python.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\nI am using the example provided in the Machine Learning Studio Docs for extracting Health Entities from a given string.\nThe code is shown below.\n\nMy question is: what is the easiest way to convert the output result into JSON format?\n\n from azure.core.credentials import AzureKeyCredential\n from azure.ai.textanalytics import TextAnalyticsClient\n import json\n    \n credential = AzureKeyCredential(\"**********************************\")\n endpoint=\"https:\/\/eastus.api.cognitive.microsoft.com\/\"\n    \n text_analytics_client = TextAnalyticsClient(endpoint, credential)\n    \n documents = [\"Subject is taking 100mg of ibuprofen twice daily\"]\n    \n poller = text_analytics_client.begin_analyze_healthcare_entities(documents)\n result = poller.result()\n    \n docs = [doc for doc in result if not doc.is_error]\n    \n print(\"Results of Healthcare Entities Analysis:\")\n for idx, doc in enumerate(docs):\n     for entity in doc.entities:\n         print(\"Entity: {}\".format(entity.text))\n         print(\"...Normalized Text: {}\".format(entity.normalized_text))\n         print(\"...Category: {}\".format(entity.category))\n         print(\"...Subcategory: {}\".format(entity.subcategory))\n         print(\"...Offset: {}\".format(entity.offset))\n         print(\"...Confidence score: {}\".format(entity.confidence_score))\n         if entity.data_sources is not None:\n             print(\"...Data Sources:\")\n             for data_source in entity.data_sources:\n                 print(\"......Entity ID: {}\".format(data_source.entity_id))\n                 print(\"......Name: {}\".format(data_source.name))\n         if entity.assertion is not None:\n             print(\"...Assertion:\")\n             print(\"......Conditionality: {}\".format(entity.assertion.conditionality))\n             print(\"......Certainty: {}\".format(entity.assertion.certainty))\n             print(\"......Association: {}\".format(entity.assertion.association))\n         for relation in doc.entity_relations:\n             print(\"Relation of type: {} has the following roles\".format(relation.relation_type))\n         for role in relation.roles:\n             print(\"...Role '{}' with entity '{}'\".format(role.name, role.entity.text))\n     print(\"------------------------------------------\")",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"azure ml designer: how to call a pipeline from another pipeline",
        "Question_creation_time":1619288778083,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370433\/azure-ml-designer-how-to-call-a-pipeline-from-anot.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm using ML Designer and i have created a sub-pipeline that i want to use it in other pipelines. how do i call that sub-pipeline from the designer?\n\nThe purpose of the subpipeline is to transform data, so the output is a dataset.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cost of running a compute, other tasks",
        "Question_creation_time":1634318912257,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/592299\/cost-of-running-a-compute-other-tasks.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi;\n\nFirst off, where can I find the costs for all the different things I can run in Azure ML? Not just a compute, but editing a notebook, connecting to a datastore, splitting a datastore, etc. Basically where is the price list?\n\nSecond, where can I find what I will be charged for things I ran in the last hour? I want to see what I'm spending before a month is up and the charge is then 100x what I expected (and can afford).\n\nthanks - dave",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use custom environment in Azure Machine Learning Designer",
        "Question_creation_time":1647597231527,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/777745\/use-custom-environment-in-azure-machine-learning-d.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI would like to know if there is the possibility to use a custom environment (created from the AML portal) for the execution of a Python Script Step in the Azure Machine Learning Designer (only using the designer, not using azureml sdk to publish the pipeline from the code).\n\nThanks,\nG",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Facing problem in deplying pyhton application from github- unable to load tensorflow saved model in AZURE.",
        "Question_creation_time":1625428420640,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/462250\/facing-problem-in-deplying-pyhton-application-from.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to deploy a classification TensorFlow model on AZURE from GitHub. It is getting deployed correctly which can be seen from the below logs.\n![111656-image.png][1]\n\n\n\n\nBut I'm getting an OSError on log Stream saying saved model doesn't exist as shown below. The error msg is highlighted in red.\n![111560-image.png][2]\n\nBut this is working correctly on local. This model has been checked locally.\nThe repository for this can be checked at https:\/\/github.com\/Vikeshkr-DSP\/cassava-leaf-disease-prediction.\nThanks in advance for your help.\n[1]: \/answers\/storage\/attachments\/111656-image.png\n[2]: \/answers\/storage\/attachments\/111560-image.png",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I use compressed data on TabularDataset?",
        "Question_creation_time":1638234150553,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/645118\/can-i-use-compressed-data-on-tabulardataset.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have a question about the source of TabularDataset on Azure Machine Learnigng.\n\nCan I use compressed data saved Azure Data Lake Storage Gen2 like below on TablarDataset without expansion?\n\ncsv with bzip2(.bz2)\n\n\nparquet with gzip(gz)\n\n\nparquet with snappy",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tensorflow and Azure machine learning",
        "Question_creation_time":1653989511207,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/871068\/tensorflow-and-azure-machine-learning.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is azure working well with Tensorflow framework? I don\u2019t see any document about it. Any help is good.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model file is not found for Registration of model in training Pipeline.",
        "Question_creation_time":1589329342560,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/26470\/model-file-is-not-found-for-registration-of-model.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"&#34;We want the model to automatically register model every time there is a new model. we created the model in the process and write it out to a pipeline data set.To persist it then we upload and read it for registration.\n\nWe are using .\/output to send the file to output. The issue is that it cannot find it in the file path . How can we validate its existence? &#34;\n\n[Note: As we migrate from MSDN, this question has been posted by an\u202fAzure Cloud Engineer\u202fas a frequently asked question] Source: MSDN",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error while accessing the dataset from a datastore",
        "Question_creation_time":1619698599813,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/377203\/error-while-accessing-the-dataset-from-a-datastore.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have tried to read the dataset from datastore. Also tried to create the dataset also.\n\nThe code for reading the dataset is below\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = Datastore.get(ws, 'qdataset')\n\n\n\nIt works fine still now.\n\n from azureml.core.dataset import Dataset\n six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n\n\nAlso i have tried from azureml.core import Dataset\n\nIt shows the following error:\n\n2021-04-29 11:56:47.284077 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.27.0', 'dataprepVersion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 962.01}, Exception=AttributeError; module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\nAttributeError Traceback (most recent call last)\n<ipython-input-34-ac7a8d35da4d> in <module>\n1 from azureml.core.dataset import Dataset\n----> 2 six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in get_by_name(workspace, name, version)\n87 :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]\n88 \"\"\"\n---> 89 dataset = AbstractDataset._get_by_name(workspace, name, version)\n90 AbstractDataset._track_lineage([dataset])\n91 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _get_by_name(workspace, name, version)\n652 if not success:\n653 raise result\n--> 654 dataset = _dto_to_dataset(workspace, result)\n655 warn_deprecated_blocks(dataset)\n656 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto)\n93 registration=registration)\n94 if dto.dataset_type == _DATASET_TYPE_FILE:\n---> 95 return FileDataset._create(\n96 definition=dataflow_json,\n97 properties=dto.latest.properties,\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info)\n555 from azureml.data._partition_format import parse_partition_format\n556\n--> 557 steps = dataset._dataflow._get_steps()\n558 partition_keys = []\n559 for step in steps:\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _dataflow(self)\n215 raise UserErrorException('Dataset definition is missing. Please check how the dataset is created.')\n216 if self._registration and self._registration.workspace:\n--> 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace)\n218 if not isinstance(self._definition, dataprep().Dataflow):\n219 try:\n\nAttributeError: module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\n\nPlease give a solution to solve this",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does Azure AutoML use (or plan to use) FLAML for the hyperparameter tuning?",
        "Question_creation_time":1623298947587,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/429832\/does-azure-automl-use-or-plan-to-use-flaml-for-the.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"FLAML looks like it performs better than Azure AutoML for hyperparameter tuning (based on the benchmarking in the Arxiv paper): https:\/\/arxiv.org\/pdf\/1911.04706v1.pdf\n\nIs it now being used or is there a plan to integrate it for the hyperparameter tuning in Azure Machine Learning Services? If so, when is that expected to become available?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to configute WebServiceOutput?",
        "Question_creation_time":1606819526077,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/181635\/how-to-configute-webserviceoutput.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I trained and deployed a ML model via Auto ML. The result looks like this:\n\"\\\"{\\\\\\\"result\\\\\\\": [\\\\\\\"Test\\\\\\\"]}\\\"\"\n\nOnce I did the same with an endpoint created with the Azure ML Designer my result looks like this:\n\"{\\\"Results\\\": {\\\"WebServiceOutput0\\\": [{\\\"Scored Labels\\\": \\\"Test\\\"}]}}\"\n\n\n\n\nIs there a way to configure the response that it looks similar to the AutoML response?\n\nThanks :)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What does \"local\" mean in compute target?",
        "Question_creation_time":1600495202147,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/99901\/what-does-34local34-mean-in-compute-target.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi guys, I'm new to Azure ML. Following the URL below, I tried to run my python script on local machine. By local, I meant exactly Windows on my local physical machine in my house. But it seems python script 'transform_titanic.py' was executed on Azure.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target\n\nI executed the script below on my local computer, and expected it runs 'transform_titanic.py' on my local computer.\n\nfrom azureml.core import Environment, Experiment, ScriptRunConfig, Workspace\nfrom dotenv import load_dotenv\nload_dotenv()\nws = Workspace(\n    os.environ['SUBSCRIPTION_ID']\n    os.environ['RESOURCE_GROUP']\n    os.environ['WORKSPACE_NAME']\n)\nexp = Experiment(workspace=ws, name='experiment')\nenv = Environment('user-managed-env')\nenv.python.user_managed_dependencies = True\nscript_run_config = ScriptRunConfig(\n    source_directory='src\/transform',\n    script='transform_titanic.py',\n    arguments=['--input_dataset_name1', 'titanic'],\n)\nscript_run_config.run_config.target = 'local'\nscript_run_config.run_config.environment = env\nrun = exp.submit(config=script_run_config)\nprint(run.get_portal_url())\nrun.wait_for_completion()",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning Data Labelling - Zoom broken on prelabelled tasks??",
        "Question_creation_time":1616185580847,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323305\/azure-machine-learning-data-labelling-zoom-broken.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"My dataset now has enough samples to start pre-labeling a set of labels (bounding boxes for image identification).\n\nHowever, rather worryingly this seems fundamentally broken?\nWe appear to have lost the ability to zoom the image (zoom just appears to zoom the bounding boxes, and not the underlying image) which basically makes this entire functionality useless.\n\nAm I missing something or is this feature completely broken?\nI hope the former, as the pre-labeling was a significant factor in choosing this platform.\n\nWe have tried multiple browsers in case this was a browser issue but to no success, they all present the same issue.\n\nIs anyone able to advise??",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Automated ML(interface) choosing primary metrics to handle imbalanced data",
        "Question_creation_time":1593398061863,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/40792\/azure-automated-mlinterface-choosing-primary-metri.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I figured out that there are some primary metrics I can choose when I run an automated ML experiment. Yet the number of primary metrics is fewer than the run metrics in the result page. I want to deal with imbalanced data(10:1 or 20:1) and\n\nlooked up the links below:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\nand\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\n\nIt seems F1 score is recommended to evaluate each model with imbalanced data.\n\nHere are my questions:\n\nIs there any way to set F1 score or multiple measures as a primary metric?\n\n\nIf there is no such way, should I do it manually?\n\n\nOf all the given primary metrics, which primary metric is the most appropriate(to build a Classification model with imbalanced data)?\n\n\n\n\n\nThanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is Azure supporting distributed GPU?",
        "Question_creation_time":1661977194510,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989398\/is-azure-supporting-distributed-gpu.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Is there any plan? Any date we can expect?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning with on premise SQL Server",
        "Question_creation_time":1592874857830,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/38894\/azure-machine-learning-with-on-premise-sql-server-1.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi is there a way for Azure Machine Learning to be able to perform analytics using data from an on premise SQL Server?\n\nOnly found the below article which is for Azure Machine Learning Studio (classic):\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\n\nThanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Submitting a job to Azure ML from Synapse workspace",
        "Question_creation_time":1648588740927,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/792681\/submitting-a-job-to-azure-ml-from-synapse-workspac.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Assume a data scientist who is coding inside a Synapse notebook, aims to submit his AutoML job to Azure ML. Also assume that we already created the Azure ML workspace, and linked it to Synapse, and also gave Synapse workspace the contributor access to Azure ML workspace. Also the data scientist has the Azure reader role at the synapse workspace level. Data scientist run the following code according to this link (https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial)\n\nfrom azureml.core import Workspace\n\nsubscription_id = \"xxxxxx\" #you should be owner or contributor\nresource_group = \"xxxxx\" #you should be owner or contributor\nworkspace_name = \"xxxxx\" #your workspace name\nworkspace_region = \"xxxxx\" #your region\n\nws = Workspace(workspace_name = workspace_name,\nsubscription_id = subscription_id,\nresource_group = resource_group)\n\nHowever, he receives an error that says he does not have the required contributor\/owner roles at the subscription and resource group level. But we (as the synapse administrators) we don't want to give him the contributor\/owner role at the subscription and resource group name\n\nQuestion: How the data scientist can submit his job without letting him to have the required contributor\/owner role. Can he use the managed identity of the Synapse workspace to connect to the Azure ML workspace?\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Accessing different model versions from same endpoint",
        "Question_creation_time":1653539509343,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/864579\/accessing-different-model-versions-from-same-endpo.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am using model versioning and would like to have different model versions accessible via the same endpoint. Any best practices to access the multiple models from the same endpoint.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best Practices for Routing Requests within Inference Clusters",
        "Question_creation_time":1601043259400,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/107990\/best-practices-for-routing-requests-within-inferen.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI have a Kubernetes Service attached as an inference cluster to an azure machine learning workspace. I have deployed multiple models to that the AKS service, each with their own endpoints. I plan to configure this such that I just need to send the request to one main endpoint, which after applying some conditions, will redirect the request to one of the endpoints (e.g. redirect the request to the appropriate model). Are there any best practices to approach this problem?\n\nThere seems to be an Azure ML router using azureml-fe that does something similar, but I cannot find any documentation about it.\n\nThanks,\nLawrence",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access to neural network model",
        "Question_creation_time":1591889003457,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/34890\/access-to-neural-network-model.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We have built numerous diagnostic models which can be reduced to equations and code that will allow us to repeat the work. We have the code physically available to us, so it can be installed in our own software.\n\nNow I would like to use artificial neural networks to build a prediction model. After I build that model, will I be able to take that model and transfer it to our own software environment? My concern is that the prediction model will just be a black box. Thanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best Approach to Clientside Machine Learning for Text Classification",
        "Question_creation_time":1637112667940,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/629917\/best-approach-to-clientside-machine-learning-for-t.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have approximately 100k rows of text data (initially PDF documents that have been OCR). Most are rows of less than 5000 characters. Each of the source documents are addressed to some department. These are typically in the form of the below examples where the target department would 'Urology' (there are several departments).\n\nUrologly Department\n\n\nUrologly Clinic\n\n\nUrology Out Patients\n\n\nUrology\n\n\nDear urology team\n\nI have read a bit on ML Text Analysis and it seems I should be able to make a pretty good model by reviewing several hundred documents for each department (I have built an App to help me do this) and manually Classifying those documents. Some documents may mention urology but are actually addressed to another department. Typically the addressed department text is at the top third (first 3-7 lines) of the text body.\n\nI cannot use any online tools, i.e. I can't upload any of the Document text to servers to process I need a client side library. I have read and completed several tutorials using the ML.net but these are pretty basic (sentiment, entity detection without any initial training), and read an excellent blog at MonkeyLearn: which seems to acknowledge that can do what I imagine I should be able to do.\n\nSo can anybody point me in the right direction, can I use some offline Microsoft client library to complete my task? Is there some other Open Source client library i should look at. Will I have to learn Go, or python to complete the task (currently a C# dev).\n\nNote: I could get fairly good matches simply using SQL Text search and a bit of C# with plenty of hard coded rules, but I thought I'd try ML -- however its a nest of complications at the moment and i am going around in circles.\n\nMany Thanks\nMike.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Change in Machine Learning Designer",
        "Question_creation_time":1660839023247,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/972775\/change-in-machine-learning-designer.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I can\u2019t find some of the basic modules from this week. Any significant change about Designer?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azuer ml How can I use model version 1 if I delete model version 1",
        "Question_creation_time":1642560823080,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/700512\/azuer-ml-how-can-i-use-model-version-1-if-i-delete.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I don't know where to find version 1\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Customvision run trained tensorflow model in Python: Placeholder:0 refers to a non existing tensor in image classification",
        "Question_creation_time":1621939072900,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/408585\/customvision-run-trained-tensorflow-model-in-pytho.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi all,\n\nI have trained an image classifier with the customvision service, which worked as charm. Now I would like to run the model inference locally with a python script on my PC. Therefore I have been following the tutorial on https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/export-model-python\n\nI am having troubles with sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()\n\nCould you please provide some information on the system requirements and the python package versions? An openCV 4.5.1 C++ code snippet on how to consume the downloaded model would be also great if possible.\n\nI am using Python 3.8.5\n\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What could I be doing wrong to get this result from Azure AutoML timeseries forecasting?",
        "Question_creation_time":1606359622230,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/176361\/what-could-i-be-doing-wrong-to-get-this-result-fro.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm experimenting with Azure AutoML for timeseries forecasting. I have a simple two column training dataset with two years of data at hourly intervals. Column 1 is Date\/Time Column 2 is the variable I want to predict. I've done several runs of Azure AutoML and it seems to complete successfully. However, when I do a forecast and graph it something is obviously wrong. It looks like the forecast is being quantised somehow. The graph below is for the 7 days after the training set. Blue is actual and red is the forecast. This is obviously not right.\n\n\n\n\nHere is my configuration for the training (python):\n\n lags = [1,24,168]\n forecast_horizon = 7 * 24 # 7 days of hourly data\n forecasting_parameters = ForecastingParameters(\n     time_column_name=\"DateTime\",\n     forecast_horizon=forecast_horizon,\n     target_lags=lags,\n     country_or_region_for_holidays='NZ',\n     freq='H',\n     use_stl='season',\n     seasonality='auto'\n )\n automl_config = AutoMLConfig(task='forecasting',\n                              debug_log='automl_forecasting_function.log',\n                              primary_metric='normalized_root_mean_squared_error',\n                              experiment_timeout_hours=1,\n                              experiment_exit_score=0.05, \n                              enable_early_stopping=True,\n                              training_data=train_df,\n                              compute_target=compute,\n                              n_cross_validations=10,\n                              verbosity = logging.INFO,\n                              max_concurrent_iterations=19,\n                              max_cores_per_iteration=19,\n                              label_column_name=\"Output\",\n                              forecasting_parameters=forecasting_parameters,\n                              featurization=\"auto\",\n                              enable_dnn=False)\n\n\n\nThe best model from the run is a VotingEnsemble:\n\n ForecastingPipelineWrapper(pipeline=Pipeline(\n   memory=None,\n   steps=[('timeseriestransformer',\n   TimeSeriesTransformer(\n     featurization_config=None,\n     pipeline_type=<TimeSeriesPipelineType.FULL: 1>)),\n   ('prefittedsoftvotingregressor',\n   PreFittedSoftVotingRegressor(estimators=[('7',\n   Pipeline(memory=None,\n   steps=[('minmaxscaler',\n   MinMaxScaler(copy=True,\n   feature_range=(0,\n   1))...\n   DecisionTreeRegressor(ccp_alpha=0.0,\n   criterion='mse',\n   max_depth=None,\n   max_features=0.5,\n   max_leaf_nodes=None,\n   min_impurity_decrease=0.0,\n   min_impurity_split=None,\n   min_samples_leaf=0.00218714609400816,\n   min_samples_split=0.00630957344480193,\n   min_weight_fraction_leaf=0.0,\n   presort='deprecated',\n   random_state=None,\n   splitter='best'))],\n   verbose=False))],\n   weights=[0.5,\n   0.5]))],\n   verbose=False),\n   stddev=None)",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic)",
        "Question_creation_time":1635432791567,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/607943\/difference-in-processing-time-between-azure-machin.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I used to use Azure Machine Learning Studio (classic).\nCreating the same workout in Azure Machine Learning Studio takes about 20 times longer than classic.\nVirtual machine size is Standard_DS3_v2 (4 core\u300114 GB RAM\u300128 GB disk).\nSteps that have been executed once will be processed quickly from the next time onward, but steps that have been changed even slightly will take 20 times longer than classic.\n\nHow can I process at the same speed as classic?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning ExperimentExecutionException while submitting a distributed training run !",
        "Question_creation_time":1619892981027,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/379458\/azure-machine-learning-experimentexecutionexceptio.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, here is the details of my issue.\nI want to execute a distributed training run with the Tensorflow framework and Horovod.\nTo do this, I've configured a environment called \"tf_env\" as follow :\n\n # Create the environment : the dependencies are in the .yml file\n tf_env = Environment.from_conda_specification(name=\"tensorflow_environment\", file_path=\"experiments\/package-list.yml\")\n    \n # Register the environment\n tf_env.register(workspace=ws)\n    \n # Specify a GPU base image\n tf_env.docker.enabled = True\n tf_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n\n\n\nWhere my \"package-list.yml\" contains all the dependencies my \"train_script.py\" requires.\nI've defined my ScriptConfigRun as follow :\n\n arguments = [\n     (... other arguments ...)\n     \"--ds\",  images_ds.as_mount()\n ]\n    \n src = ScriptRunConfig(\n     source_directory=\"experiments\",\n     script='train_script.py',\n     arguments=arguments,\n     compute_target=compute_target,\n     environment=tf_env,\n     distributed_job_config=MpiConfiguration(node_count=2)\n )\n\n\n\nThen, when I want to submit the run :\n\n run = best_model_experiment.submit(config=src)\n\n\n\n... it raises this error I don't understand :\n\n ExperimentExecutionException: ExperimentExecutionException:\n     Message: {\n     \"error_details\": {\n         \"componentName\": \"execution\",\n         \"correlation\": {\n             \"operation\": \"***\",\n             \"request\": \"***\"\n         },\n         \"environment\": \"westeurope\",\n         \"error\": {\n             \"code\": \"UserError\",\n             \"message\": \"Error when parsing request; unable to deserialize request body\"\n         },\n         \"location\": \"westeurope\",\n         \"time\": \"***\"\n     },\n     \"status_code\": 400,\n     \"url\": \"https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment***\"\n }\n     InnerException None\n     ErrorResponse \n {\n     \"error\": {\n         \"message\": \"{\\n    \\\"error_details\\\": {\\n        \\\"componentName\\\": \\\"execution\\\",\\n        \\\"correlation\\\": {\\n            \\\"operation\\\": \\\"***\\\",\\n            \\\"request\\\": \\\"***\\\"\\n        },\\n        \\\"environment\\\": \\\"westeurope\\\",\\n        \\\"error\\\": {\\n            \\\"code\\\": \\\"UserError\\\",\\n            \\\"message\\\": \\\"Error when parsing request; unable to deserialize request body\\\"\\n        },\\n        \\\"location\\\": \\\"westeurope\\\",\\n        \\\"time\\\": \\\"***\\\"\\n    },\\n    \\\"status_code\\\": 400,\\n    \\\"url\\\": \\\"https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment_***\\\"\\n}\"\n     }\n }\n\n\n\n\nCould you please help me decrypt this error ?\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Comment s\u00e9lectionner Standard_DS11_v2",
        "Question_creation_time":1638368598000,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/647767\/comment-selectionner-standard-ds11-v2.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Bonjour\nJe suis le cours en ligne concernant l'impl\u00e9mentation d'algorithmes de machine learning\nA l'\u00e9tape Create compute resources\nhttps:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\n\nOn me demande Search for and select Standard_DS11_v2\n\nHors, l'interface me dit que je n'ai pas les quotas disponibles.\nJ'utilise l'offre d'essai \u00e0 200 USD.\nComment faire pour que cela fonctionne ?\nCordialement\nThibaut",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Help for Azure ML Studio experiment mapping",
        "Question_creation_time":1620716372030,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/390316\/help-for-azure-ml-studio-experiment-mapping.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am learning Azure ML (Studio) and please help me for below scenarios,\nI have a bank customer data having column labelled as customer age, family members (1,2,3 &4), credit card (Yes \/no) Personal Loan (Yes \/no), education (1. Undergrad 2. Graduate 3. Advanced\/professional).\n\nHow to filter age column and find number of customers less than 45 years in % of total number of customers?\n\n\nAlso need % customers who are having credit card as well Personal loan?\n\n\nwhich education category of customers are more prone to subscribe to personal loan?\n\n\nHow to do visual analysis?\n\n\nHow to calculate correlation between 2 columns?\n\nThanks in advance for your guidance. and incase tag to wrong group please guide to appropriate group",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can't deploy a VM on the Azure Machine Learning.",
        "Question_creation_time":1623772014863,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/437136\/can39t-deploy-a-vm-on-the-azure-machine-learning.html",
        "Question_upvote_count":4.0,
        "Question_view_count":null,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I tried to deploy a VM to Azure Machine Learning, but I get the error message \"You do not have enough quota for the following VM sizes. Click here to view and request quota.\" And the VM cannot be deployed.\n\nBut I have enough quota (24 CPUs).\n\nWhat is causing the problem?\n\nI'm using Azure's Free trial plan.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GPT-3 access",
        "Question_creation_time":1605179070400,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/160489\/gpt-3-access.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'd like to use GPT-3 for my application. I understand MS has licensed GPT-3 from OpenAI, and that there is pricing too. So how do I get to use GPT-3?\n\n\n\n\nChris Powell",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ClusterIdentityNotFound when submitting experiment.",
        "Question_creation_time":1633943984543,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/585373\/clusteridentitynotfound-when-submitting-experiment.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"When I'm submitting my experiment fom notebook, experiment is queing for a long time then I get as error:\n\nAzureMLCompute job failed.\nClusterIdentityNotFound: Identity of the specified\nmanaged compute <hidden cluster location> is not found\n\n\n\n\nI've updated all azure ml packages and restarted cluster, deleted, recreating, ... Nothing seems to be working.\n\nWhat Should I do?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Question_creation_time":1600897231890,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.\n\nFeatures in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python are neat, but that of having API docs\/swagger for the webservice seems missing.\n\nHaving some documentation is useful especially if the model takes in input several features of different type.\n\nTo overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.\n\nCan I do something equivalent to this with AKS in AzureML instead? If so, how?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning - Data Labeling - Refresh",
        "Question_creation_time":1636385510357,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/619198\/azure-machine-learning-data-labeling-refresh.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI've started a new Data labeling project in Azure Machine Learning and I configured the incremental refresh.\n\nHow often is the data refreshed? Is it possible to force a refresh manually? Is it possible to execute this command via SDK (Python or PowerShell)?\n\nThanks.\n\nG",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Automated ML(interface) how do models created from an Automated ML experiment handle Imbalanced Data?",
        "Question_creation_time":1593407718737,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/40727\/azure-automated-mlinterface-how-do-models-created.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have run automated ML experiments with imbalanced data (10:1, 20:1, sometimes 30:1) and deployed the best models which all showed fantastic results.\n\nWhen I looked up the link\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\n, it says Azure automated ML can properly handle imbalance of up to 20:1.\nI started to wonder where the ratio 20:1 came from.\n\nAs far as I understand, Azure automated ML doesn't use upsampling, downsampling or resampling, and is more focused on a column of weights to make a class more or less important, and a performance metric dealing better with imbalanced data.\n\nDoes this 20:1 come from some theory? or from tons of experiments already conducted?\n\n\n\n\n\nAzure automated ML shows the result with warning when I use 30:1(or more) imbalanced data, but I still wonder why it is 20:1.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dataset + Preprocessed Text : Parameter \"Stopwords columns\" value should be less than or equal to parameter \"1\" value. . ( Error 0007 )",
        "Question_creation_time":1612874807950,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/265397\/dataset-preprocessed-test-parameter-34stopwords-co.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I encounter the following error :\n\nParameter \"Stopwords columns\" value should be less than or equal to parameter \"1\" value. . ( Error 0007 )\nwhen building a simple pipeline :\n\nwith a .csv Dataset followed by a \"Preprocessed Text\".\n\nNo parameter 'Stopwords columns' is available in the \"Preprocessed Text\" properties !!!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to delete datasets on AzureML?",
        "Question_creation_time":1632745762197,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/567611\/is-there-a-way-to-delete-datasets-on-azureml.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"We have a list of datasets in our AzureML. Is there a way to delete the datasets that we no longer require?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure OpenAI service capabilities",
        "Question_creation_time":1663989341807,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1021561\/azure-openai-service-capabilities.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How do I get access to the Azure OpenAI service to evaluate it's capabilities?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Machine Learning studio Data Labeling Dataset",
        "Question_creation_time":1623134368530,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/426209\/machine-learning-studio-data-labeling-dataset.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi ,\n\nI have a have a dataset from the labelled data using the ML Data Labeling tool , my question is how can use the dataset to train a model ? , I tried Automated ML but I cannot make ant connection with the dataset .\n\nThanks for your help.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deepspeed gpt-2 megatron-LM problems",
        "Question_creation_time":1610037769210,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/222550\/deepspeed-gpt-2-megatron-lm-problems.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am trying to make a GPT-2 model with deepspeed on an azure VM. I found ~2 bugs which I was able to patch, but I have stumbled upon a really tough one. You see, it says I need pytorch. No surprise. I install pytorch. It still says I don't have it. I used both pip and pip3 many times. I install pytorch from github and run setup.py. It says I need python 3. When I get python 3 it says the same. When I try google colab it gives me the following error:\nTraceback (most recent call last): File \"pretrain_gpt2.py\", line 709, in <module>\nmain() File \"pretrain_gpt2.py\", line 654, in main\nargs.eod_token = get_train_val_test_data(args) File \"pretrain_gpt2.py\", line 600, in get_train_val_test_data\nargs) File \"\/content\/DeepSpeedExamples\/Megatron-LM\/configure_data.py\", line 34, in apply\nreturn make_loaders(args) File \"\/content\/DeepSpeedExamples\/Megatron-LM\/configure_data.py\", line 170, in make_loaders\ntrain, tokenizer = data_utils.make_dataset(**data_set_args) File \"\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/init.py\", line 109, in make_dataset\nds = split_ds(ds, split) File \"\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/datasets.py\", line 194, in split_ds\nrtn_ds[i] = SplitDataset(ds, split_inds) File \"\/content\/DeepSpeedExamples\/Megatron-LM\/data_utils\/datasets.py\", line 134, in init\nself.lens = itemgetter(*self.split_inds)(list(self.wrapped_data.lens)) TypeError: itemgetter expected 1 arguments, got 0\n\nHow do I fix both the google colab and the azure VM errors?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I want to register the model learned by AutoML in Azure Machine learning in ONNX format and call it in Azure Synapse Analitics.",
        "Question_creation_time":1664411309103,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1027830\/i-want-to-register-the-model-learned-by-automl-in.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I found that I can register the model using Mlflow, but I don't know how to register it in ONNX format.\nI found out that the model is registered using Mlflow.\nBut I don't know how to convert AutoML models to ONNX format and register them with Mlflow.\n\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.model import Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.automl.runtime.onnx_convert import OnnxConverter\nfrom random import random\nfrom mlflow.tracking import MlflowClient\nimport mlflow\nimport mlflow.onnx\nimport os\nimport azureml.mlflow\n\nauth = ServicePrincipalAuthentication(\ntenant_id=\"\",\nservice_principal_id=\"\",\nservice_principal_password=\"\")\n\nsubscription_id = ''\nresource_group = ''\nworkspace_name = ''\n\nml_client = MLClient(credential=auth,\nsubscription_id=subscription_id,\nresource_group_name=resource_group)\n\nazure_mlflow_uri = ml_client.workspaces.get(workspace_name).mlflow_tracking_uri\nmlflow.set_tracking_uri(azure_mlflow_uri)\n\nws = Workspace(subscription_id, resource_group, workspace_name, auth=auth)\n\ntrain_data = Dataset.get_by_name(ws, name='iris')\n\nlabel = \"class\"\n\nautoml_settings = {\n\"primary_metric\": 'AUC_weighted',\n\"n_cross_validations\": 2\n}\n\nautoml_classifier = AutoMLConfig(\ntask='classification',\nblocked_models=['XGBoostClassifier'],\nenable_onnx_compatible_models=True,\nexperiment_timeout_minutes=30,\ntraining_data=train_data,\nlabel_column_name=label,\n**automl_settings\n)\n\nexperiment_name = 'experimetn_with_mlflow'\nmlflow.set_experiment(experiment_name)\nexperiment = Experiment(ws, experiment_name)\n\nwith mlflow.start_run() as mlflow_run:\nmlflow.log_metric(\"iris_metric\", random())\n\n mlflow_run = experiment.submit(automl_classifier, show_output=True)\n description = 'iris_Description'\n model = mlflow_run.register_model(description=description,\n                                model_name='iris_Model')\n best_run, onnx_mdl = mlflow_run.get_output(return_onnx_model=True)\n onnx_fl_path = \".\/best_model.onnx\"\n OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)\n model = Model.register(workspace=ws,\n                     description=description,\n                     model_name='iris_onnx_model',\n                     model_path=onnx_fl_path)\n client = MlflowClient()\n finished_mlflow_run = MlflowClient().get_run(mlflow_run.run_id)\n metrics = finished_mlflow_run.data.metrics\n tags = finished_mlflow_run.data.tags\n params = finished_mlflow_run.data.params\n model_path  = \"best_model\"\n model_uri = 'runs:\/{}\/{}'.format(mlflow_run.run_id, model_path)\n mlflow.register_model(model_uri, 'iris_onnx_mlflow_model')",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Execute R Script in ML Studio",
        "Question_creation_time":1627017183467,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/486775\/execute-r-script-in-ml-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI am trying to runt the following R Script in an 'Execute R Script' module in Machine Learning Studio.\n\ndata.set <- data.frame(installed.packages())\nmaml.mapOutputPort(\"data.set\")\n\nThis script is taken from the 'Get started with Machine Learning Studio (classic)' in R page (https:\/\/docs.microsoft.com\/en-au\/azure\/machine-learning\/classic\/r-get-started#timeseries)\n\nWhilst it works in ML (classic) I receive the following error when running it in Machine Learning Studio;\n\nError in maml.mapOutputPort(\"data.set\"): could not find function \"maml.mapOutputPort\"\n\nWhat additional config settings are needed to enable R scripts in ML Studio?\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Loosen azureml-dataprep requirements to cloudpickle<=2.0.0",
        "Question_creation_time":1637242355487,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/632441\/loosen-azureml-dataprep-requirements-to-cloudpickl.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nI couldn\u2019t find a specific github repo for azureml-dataprep so I decided to also write you here. Can you forward it to the devs?\n\n\n\n\nazureml-dataprep (which is a depedency for azureml-dataset-runtime) has requirement cloudpickle<2.0.0 and >=1.1.0. However there is to my knowledage no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0. cloudpickle==2.0.0 introduces some very effective tools for serializing helper scripts which is very helful when working with azureml. So azureml-dataprep should allow cloudpickle<=2.0.0\n\nIntro to new cloudpickle:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs\nPR:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417\nGithub issue:\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I create a dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Question_creation_time":1601468116080,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/112778\/how-can-i-create-a-dataset-in-azure-ml-studio-thro.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.\n\nIn my folder, Spark creates files such as \"_SUCCESS\" or \"_committed_8998000\".\n\nAzure ML Studio is not able to read them or ignore them and tells me:\n\nThe provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  \"message\": \" \"\n}\n\n\n\n\nI selected \"Ignore unmatched files path\" and yet, it still does not work.\n\nIf I remove the \"_SUCCESS\" and other Spark files, it works.\n\nDoes anyone have an idea about a workaround?\n\nThank you.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"After selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state.",
        "Question_creation_time":1617523186537,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/343427\/after-selecting-the-34edit-column34-button-of-the.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello everyone!\n\nI am taking the Create a Regression Model with Azure Machine Learning designer course in Microsoft Learn. When I perform the steps in the Explore Data section, after selecting the \"Edit column\" button of the \"Select Columns in Dataset\" module in Designer, it will be stuck in the \"loading\" state. Therefore, I cannot proceed to the next step.\n\n\n\n\n\nThank you very much!\n\nBest regards,\nLing",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":7.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Enpoint deployment failed EAST US region",
        "Question_creation_time":1594945530060,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/48609\/azure-ml-enpoint-deployment-failed-east-us-region.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have an Azure ML Real-time inference endpoint deployed ran for a month till yesterday. Today it is in the state of \"Failed\".\n\nI did create a new compute and did a new deployment in the same region EAST US and it failed again.\n\nWhat's going? Is this just a problem for me or a general issue?\n\nThanks\n-Dali",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":44.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I access an input parameter in Azure Machine Learning endpoints?",
        "Question_creation_time":1602485723003,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/123204\/how-do-i-access-an-input-parameter-in-azure-machin.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I've created an Azure ML Endpoint Pipeline with a single 'Execute Python Script'. From the script, I am looking for a way to access the input 'ParameterAssignments' that I POST to the endpoint to trigger the pipeline. I expected to see them somewhere in Run.get_context(), but I haven't had any luck. I simply need a way to POST arbitrary values that my Python scripts can access. Thank you!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"YOLO v5 in azure",
        "Question_creation_time":1661985450753,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989581\/yolo-v5-in-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, does Azure Machine Learning support YOLOv5? How we can import it?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Automated machine learning model deployment issue",
        "Question_creation_time":1627371604967,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490809\/automated-machine-learning-model-deployment-issue.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"So I'm having an issue with setting up an endpoint for a machine learning model which was trained using Azure AutoML. When I try to test the deployed model, I get an error saying that the service is temporarily unavailable. After looking online, I found that this might happen because of an error in the run() function in the entry script.\n\nWhen I try to test the entry script on a notebook in Azure ML studio, on a fresh compute instance, there are two problems:\nFirst I get the error: AttributeError: 'MSIAuthentication' object has no attribute 'get_token'\nWhich is solved by running: pip install azureml-core\n\nThen I get the error: ModuleNotFoundError: No module named 'azureml.automl.runtime'\nWhich I try to solve using: pip install azureml-automl-runtime\nBut this throws a lot of incompatibility errors during the installation. When I then try to run the entry script I get an error with the message: \"Failed while applying learned transformations.\"\n\nSo I setup a new virtual environment on my local machine in which I only installed azure-automl-runtime. Using that setup the entry script works perfectly fine. So I created a custom environment in Azure ML studio using the conda file of that local virtual environment. Unfortunatly I still get the error \"service temporarily unavailable\" when trying to test the endpoint.\n\nI have a feeling the default Azure ML containers are incompatible with azureml-automl-runtime, since installing this on a ML studio notebook also throws a lot of errors.\n\nI feel like there should be an elegant way to deploy an AutoML model, am I doing something wrong here?\n\n\n\n\n\nUpdate: I found out I didn't change the environment for the endpoint, so that is why I was getting the same error probably. When using the custom environment I got errors from gunicorn, so I also added that package to the environment. Now I get the following error:\n\n       File \"\/var\/azureml-server\/entry.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 4, in <module>\n     from routes_common import main\n   File \"\/var\/azureml-server\/routes_common.py\", line 39, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n\n\n\n\nSo what do I install to fix this? Is there a list somewhere of required packages for an ML model endpoint?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to add r2 and adj r2 metric in linear regression model - AzureML Studio?",
        "Question_creation_time":1618843927707,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/362850\/how-to-add-r2-metric-in-linear-regression-model-az.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have trained a linear regression model in AzureML studio which was created in designer as pipeline.\n\nI could not able to see R square and adj-R square metric in Evaluate Model step.\n\nCould any throw thoughts how can I add these 2 metrics to my trained model\n\n\n\n\n\n\n\n\n\nThanks\nBhaskar",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"(UserError) Error when parsing request; unable to deserialize request body",
        "Question_creation_time":1591616365497,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/33313\/usererror-error-when-parsing-request-unable-to-des.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, getting this error when i run an azureml experiment with custom_docker_image (basegpu image of mcr) - can anybody help me understand this? Have tested this in local compute and it works, not sure why this does not work on a training cluster vm?\n\n\n\n\n    azureml._restclient.exceptions.ServiceException: ServiceException:\n         Code: 400\n         Message: (UserError) Error when parsing request; unable to deserialize request body\n         Details:\n        \n         Headers: {\n             \"Date\": \"Mon, 08 Jun 2020 11:03:52 GMT\",\n             \"Content-Type\": \"application\/json; charset=utf-8\",\n             \"Transfer-Encoding\": \"chunked\",\n             \"Connection\": \"keep-alive\",\n             \"Request-Context\": \"appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b\",\n             \"x-ms-response-type\": \"error\",\n             \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\"\n         }\n         InnerException: {\n         \"additional_properties\": {},\n         \"error\": {\n             \"additional_properties\": {},\n             \"code\": \"UserError\",\n             \"message\": \"Error when parsing request; unable to deserialize request body\",\n             \"details_uri\": null,\n             \"target\": null,\n             \"details\": [],\n             \"inner_error\": null,\n             \"debug_info\": null,\n             \"message_format\": null,\n             \"message_parameters\": null,\n             \"reference_code\": null\n         },\n         \"correlation\": {\n             \"operation\": \"e96d6285280f5849a4a5e3f172d65d36\",\n             \"request\": \"1beee8ecb7180147\"\n         },\n         \"environment\": \"westeurope\",\n         \"location\": \"westeurope\",\n         \"time\": {}\n     }",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure ML Datastore\\Datasets",
        "Question_creation_time":1626270339670,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/475768\/azure-ml-datastoredatasets.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello:\n\nI want to know that if it is possible automate copy file from azure storage to Azure ML folder.\n\nI understand that it is duplication of data, but I want to know if yes, how I can do that.\n\nAny pointer is greatly appreciated.\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What's the next step after creating a pipeline?",
        "Question_creation_time":1661266129010,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/978610\/what39s-the-next-step-after-creating-a-pipeline.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, the UI is very confused, not straightforward as Studio. Can you guide me to the next step to use the pipeline?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Question_creation_time":1643903626503,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/721792\/how-to-get-model-id-of-the-latest-version-register.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello MS team,\n\nI have registered an ML model in the AML workspace using an Azure Machine learning pipeline and triggered the main control script of the pipeline by linking the repo present in Azure DevOps to the AML workspace(using Service principal).\n\nHow do I download the latest version of the model from the AML workspace to the \"Artifacts\" folder in Azure DevOPs?\n\nAny help is appreciated please.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Machine learning studio stuck",
        "Question_creation_time":1651358386807,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/832643\/machine-learning-studio-stuck.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"My project stuck again this week. It has been three hours for waiting.\n\nAny help?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"machine learning Studio",
        "Question_creation_time":1648415916797,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/789066\/machine-learning-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi:\n\nI wonder when will the classic Machine Learning Studio retire?\n\nAlso in order to save my data and file, any preparation or migration should be done to avoid any loss?\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Permission error while finishing auto ml run",
        "Question_creation_time":1621886630607,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/407580\/permission-error-while-finishing-auto-ml-run.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I get the following error inside the child runs in ML studio while doing an Automated ML experiment.\n\n\"Identity does not have permissions for Microsoft.MachineLearningServices\/workspaces\/metadata\/artifacts\/write actions.\"\n\nI am the owner of the resource group so I am not sure what the issue is.\n\n\n\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Fisher Linear Discriminant Analysis Azure",
        "Question_creation_time":1621855005240,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"anomaly detector with azure",
        "Question_creation_time":1664585272280,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1031454\/anomaly-detector-with-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"May I have some samples about anomaly detector with azure machine learning studio?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"azuremlsdk for R error Could not retrieve user token. Please run 'az login'",
        "Question_creation_time":1621290302507,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/398420\/azuremlsdk-for-r-error-could-not-retrieve-user-tok.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to create a workspace in azure machine learning and receiving this error after 2 browser Windows open and I click log in.\n\nlibrary(azuremlsdk)\nnew_ws <- create_workspace(name = 'muffin',\n\n\n+ subscription_id = 'XXXXXXXXXXXX',\n+ resource_group = 'white',\n+ location = 'eastus2',\n+ create_resource_group = T)\nNote, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\nYou have logged in. Now let us find all the subscriptions to which you have access...\nNote, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\nYou have logged in. Now let us find all the subscriptions to which you have access...\nError in py_call_impl(callable, dots$args, dots$keywords) :\nAuthenticationException: AuthenticationException:\nMessage: Could not retrieve user token. Please run 'az login'\nInnerException It is required that you pass in a value for the \"algorithms\" argument when calling decode().\nErrorResponse\n{\n\"error\": {\n\"code\": \"UserError\",\n\"inner_error\": {\n\"code\": \"Authentication\"\n},\n\"message\": \"Could not retrieve user token. Please run 'az login'\"\n}\n}\n\n\nhow do I get passed this error?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Code: AuthorizationFailed",
        "Question_creation_time":1651917005187,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/840311\/code-authorizationfailed.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Unit 4 of 7\nExercise - Back up an Azure virtual machine\nCreate a backup for Azure virtual machines\n\nI am unable to run the following command in cloud shell to set up the environment:\nRGROUP=$(az group create --name vmbackups --location westus2 --output tsv --query name)\n\nFollowing error pop up:\nERROR: (AuthorizationFailed) The client 'live.com#...... does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.......\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.\nCode: AuthorizationFailed\nMessage: The client 'live.com#l...... with object id '.......' does not have authorization to perform action 'Microsoft.Resources\/subscriptions\/resourcegroups\/write' over scope '\/subscriptions\/.....\/resourcegroups\/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.\n\nWhen I do refresh, sign out or sign in do not helps. Anybody has any idea what to do?\nThank you",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Not able to pull docker image from Container Registry",
        "Question_creation_time":1631798842910,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/555024\/not-able-to-pull-docker-image-from-container-regis.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello community,\nI'm facing a problem, my ACR in my resource group was deleted and I couldn't create any instance. I created again and now I can create instances but i'm having problems to run the dataset profile. It's failing to pull the image docker.\n\nThis is the output\n\n AzureMLCompute job failed.\n FailedPullingImage: Unable to pull docker image\n     imageName: 19acd0cdf57549bcace363c924cf045b.azurecr.io\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\n     error: Run docker command to pull public image failed with error: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.\n .\n     Reason: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.\n    \n     Info: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n\n\n\nThe ML Studio has the following permissions on the ACR permissions\n\n\n\n\nThe docker image appears in the repositories of the ACR\n\n\n\n\nAny hint how can i solve this problem?\n\nThanks in advance",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"memory outage while running module",
        "Question_creation_time":1653902834057,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869594\/memory-outage-while-running-module.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am encountering an issue of error 0138, while training the data, at the end it shows memory has been exhausted exception\n\nI do not think my data has exceed the limit of azure ML studio, is there any way to solve this?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Data Factory : How to pass DataPath as a parameter to Azure ML Pipeline activity?",
        "Question_creation_time":1599771191990,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/91785\/azure-data-factory-how-to-pass-datapath-as-a-param.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello All,\n\nHow to pass a Datapath as a parameter in Azure ML Pipeline activity?\n\nMore details here : Have opened an issue here : https:\/\/github.com\/Azure\/Azure-DataFactory\/issues\/216\n\n\n\n\n\nThanks.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":4.0,
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Question_creation_time":1634739305317,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/597612\/azuremlpython-sdkenvironmentdocker-docker-copy-mis.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,\n\nI am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.\n\nFrom the documentation of Environment.from_dockerfile ( https:\/\/docs.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none- ), I can not find a way to give it some files along with the Dockerfile itself.\n\nSo, how to pass context to enable using COPY in the Dockerfile ?\n\nThank you for your time !",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning - Specify disk storage type",
        "Question_creation_time":1636713894743,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/625035\/azure-machine-learning-specify-disk-storage-type.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,\n\nIs there a way to specify the disk storage type for Compute instances?\nBoth the Azure portal and ARM templates do not have an option to define the disk storage type, which defaults to the P10 disks (Premium SSD).\n\nThanks",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pipeline can not be built using a HyperdriveStep inside a Pipeline",
        "Question_creation_time":1621515534017,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/403018\/pipeline-can-not-be-built-using-a-hyperdrivestep-i.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hei, I'm trying to build a pipeline including a HyperdriveStep to tuen the hyperparameters.\nThe pipeline should later on run automatically and be tuned at each pipeline run.\n\nThe pipeline consists of three steps: a preparation step resulting in a PipelineData Object, the HyperdriveStep and a final PythonRegisterStep, where the best model should be registered.\n\nHowever, when creating the pipeline object I'm getting an error I can not relate to.\n\n\n\n\nTraceback (most recent call last):\n\n       File \"\/Users\/xxx\/Desktop\/azure_test\/pipeline-folder\/azure_pipeline_wrapper1.py\", line 168, in <module>\n         pipeline = Pipeline(workspace=ws, steps=pipeline_steps, description=\"Pipeline for hyperparameter tuning\")\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/core\/_experiment_method.py\", line 104, in wrapper\n         return init_func(self, *args, **kwargs)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/pipeline.py\", line 177, in __init__\n         self._graph = self._graph_builder.build(self._name, steps, finalize=False)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1481, in build\n         graph = self.construct(name, steps)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1503, in construct\n         self.process_collection(steps)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1539, in process_collection\n         builder.process_collection(collection)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1830, in process_collection\n         self._base_builder.process_collection(item)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1533, in process_collection\n         return self.process_step(collection)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1577, in process_step\n         node = step.create_node(self._graph, self._default_datastore, self._context)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py\", line 270, in create_node\n         hyperdrive_config, reuse_hashable_config = self._get_hyperdrive_config(context._workspace,\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py\", line 346, in _get_hyperdrive_config\n         hyperdrive_dto = _search._create_experiment_dto(self._hyperdrive_config, workspace,\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/_search.py\", line 38, in _create_experiment_dto\n         platform_config = hyperdrive_config._get_platform_config(workspace, experiment_name, **kwargs)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/runconfig.py\", line 672, in _get_platform_config\n         platform_config.update(self._get_platform_config_data_from_run_config(workspace))\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/runconfig.py\", line 686, in _get_platform_config_data_from_run_config\n         run_config = get_run_config_from_script_run(self.run_config)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/core\/script_run_config.py\", line 84, in get_run_config_from_script_run\n         run_config.arguments = deepcopy(script_run_config.arguments)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 146, in deepcopy\n         y = copier(x, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 205, in _deepcopy_list\n         append(deepcopy(a, memo))\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 172, in deepcopy\n         y = _reconstruct(x, memo, *rv)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 270, in _reconstruct\n         state = deepcopy(state, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 146, in deepcopy\n         y = copier(x, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 230, in _deepcopy_dict\n         y[deepcopy(key, memo)] = deepcopy(value, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 172, in deepcopy\n         y = _reconstruct(x, memo, *rv)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 270, in _reconstruct\n         state = deepcopy(state, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 146, in deepcopy\n         y = copier(x, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 230, in _deepcopy_dict\n         y[deepcopy(key, memo)] = deepcopy(value, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 172, in deepcopy\n         y = _reconstruct(x, memo, *rv)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 264, in _reconstruct\n         y = func(*args)\n        \n       File \"\/Users\/xxxr\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copyreg.py\", line 91, in __newobj__\n         return cls.__new__(cls, *args)\n        \n     TypeError: __new__() missing 2 required positional arguments: 'workspace' and 'name'\n\n\n\n\n\nMy Code:\n\n # Connect to workspace \n ws = Workspace.from_config()\n print(ws.name, \"loaded\")\n    \n # Set compute target\n cluster_name = \"compcluster234\"\n pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n    \n # Create new environment\n sklearn_env = Environment(\"sklearn_env\")\n # Adds dependencies to PythonSection of sklaern_env\n env_packages = CondaDependencies.create(conda_packages=['scikit-learn'])\n sklearn_env.docker.enabled = True\n sklearn_env.python.conda_dependencies = env_packages\n # Register the environment\n sklearn_env.register(workspace=ws)\n    \n # =============================================================================\n # Run Configuration\n # =============================================================================\n    \n # Create Run configuration \n # Pipeline_folder\n pipeline_folder = path + '\/pipeline-folder'\n # Create a new runconfig object for the pipeline\n pipeline_run_config = RunConfiguration()\n # Use the compute you created above. \n pipeline_run_config.target = pipeline_cluster\n # Assign the environment to the run configuration\n # In comparison to the ScriptRunCnfig object, the RunConfig is more generous\n pipeline_run_config.environment = sklearn_env\n print (\"Run configuration created.\")\n    \n # =============================================================================\n # DataPath\n # =============================================================================\n    \n # Get the default datastore\n default_ds = ws.get_default_datastore()\n # Create a DataPath object \n datapath = DataPath(datastore = default_ds,\n                      path_on_datastore = 'cancer-data')\n # Make the datapath a PipelineParameter\n datapath_pipeline_param = PipelineParameter(name='input-data',   \n                                             default_value=datapath)\n datapath_input = (datapath_pipeline_param, \n                    DataPathComputeBinding(mode = 'mount'))\n    \n # =============================================================================\n # PipelineData\n # =============================================================================\n    \n # Create a PipelineData (temporary Data Reference) for the preppared data folder\n prepped_data_folder = PipelineData(name=\"prepped_data_folder\",\n                                    datastore=ws.get_default_datastore())\n    \n # Create PipelineData objects for the Metrics and the saved model\n metrics_output_name = 'metrics_output'\n metrics_data = PipelineData(name='metrics_data',\n                             datastore=default_ds,\n                             pipeline_output_name=metrics_output_name,\n                             training_output=TrainingOutput(\"Metrics\"))\n    \n model_output_name = 'model_output'\n saved_model = PipelineData(name='saved_model',\n                            datastore=default_ds,\n                            pipeline_output_name=model_output_name,\n                            training_output=TrainingOutput(\"Model\",\n                                                           model_file=\"outputs\/model\/cancer_model.pkl\"))\n    \n # =============================================================================\n # Pipeline Steps\n # =============================================================================\n    \n # Step 1, Run the data prep script\n prep_step = PythonScriptStep(name = \"prepare_data\",\n                                 source_directory = pipeline_folder,\n                                 script_name = \"cancer_pipeline_preprocessing.py\",\n                                 arguments = ['--input-data', datapath_input,\n                                              '--prepped-data', prepped_data_folder],\n                                 inputs=[datapath_input],\n                                 outputs=[prepped_data_folder],\n                                 compute_target = pipeline_cluster,\n                                 runconfig = pipeline_run_config,\n                                 allow_reuse = False)\n    \n # Define the search strategy and parameter space for hyperparameter tuning\n ps = GridParameterSampling({ '--max_depth': choice(1,2,3)})\n # Define a early stopping criteria\n early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n # Define a ScriptRunConfig for the Training script\n # The ScriptRunConfig is based on the RunConfig of the Pipeline\n script_run_config = ScriptRunConfig(script=\"cancer_pipeline_tuning.py\",\n                                     source_directory=pipeline_folder,\n                                     # Add non-hyperparameter arguments -in this case, the training dataset\n                                     arguments = ['--training_folder', prepped_data_folder],\n                                     run_config=pipeline_run_config)\n # Define a HyperDriveConfiguration\n # The primary_metric_name must be completely idential to the metric name logged during training (inside the training script)\n hd_config = HyperDriveConfig(run_config=script_run_config, \n                              hyperparameter_sampling=ps,\n                              policy=early_termination_policy,\n                              primary_metric_name='Accuracy', \n                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                              max_total_runs=3,\n                              max_concurrent_runs=2)\n    \n # Step 2b, define a HyperDriveStep\n # HyperDriveStep can be used to run HyperDrive job as a step in pipeline.\n # No arguments need to be set as they are already set inside the ScriptRunConfig\n hyperdrive_step = HyperDriveStep(name=\"tune_hyperparameters\",\n                                  hyperdrive_config=hd_config,\n                                  inputs=[prepped_data_folder],\n                                  outputs=[metrics_data, saved_model])\n    \n hyperdrive_step.run_after(prep_step)    \n    \n # Step 3, Run the model registration step\n register_step = PythonScriptStep(name=\"register_model\",\n                                        script_name='cancer_pipeline_register1.py',\n                                        source_directory = pipeline_folder,\n                                        arguments=[\"--saved_model\", saved_model],\n                                        inputs=[saved_model],\n                                        compute_target = pipeline_cluster,\n                                        runconfig=pipeline_run_config,\n                                        allow_reuse = False)\n    \n register_step.run_after(hyperdrive_step)    \n print(\"Pipeline steps defined\")\n    \n    \n # Construct the pipeline\n pipeline_steps = [prep_step, hyperdrive_step, register_step]\n pipeline = Pipeline(workspace=ws, steps=pipeline_steps, description=\"Pipeline for hyperparameter tuning\")\n print(\"Pipeline is built.\")",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best compute cluster for training large image datasets !",
        "Question_creation_time":1633703536287,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Good morning,\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.\nIs a cluster that I can used to speed up training?\n\nThanks for your help in advance\n\nMany thanks\n\nRoland",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Copy experiment within workspace",
        "Question_creation_time":1653905030690,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869578\/copy-experiment-within-workspace.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"How to duplicate experiments within workspace during debugging",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy AzureML Model locally: cannot import name 'convert_inputs'",
        "Question_creation_time":1591165699300,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31601\/deploy-azureml-model-locally-cannot-import-name-co.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have trained a model using azure AutoML and downloaded the model. Then I created a new conda env using the conda file and tried to execute the scoring_file_v_1_0_0.py which is in the zip. I receive this error:\n\n&amp;gt; WARNING - Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception cannot import name &amp;#39;convert_inputs&amp;#39;.\n\nIs this still some dependency problem or am I doing something unexpected? I did expect the script to open a web server to serve the model.",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Form recognizer to report on missing information in (near) real-time",
        "Question_creation_time":1647481005860,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775440\/form-recognizer-to-report-on-missing-information-i.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi community,\nI'm interested in what Azure Form Recogniser or another tool can do for us in terms of screening the correctness of uploaded applications. Think of applications for funding grants. I haven't built any models yet, just wondering how feasible the below is. A solution doesn't have to involve AI at all, but must be able to 'read' the uploaded documents.\n\n\n\n\nA client uploads a set of standard documents (usually scanned PDF's) using a file upload in our .net application.\nCan we:\n1) Use form recogniser to extract key value pairs, after training a custom model.\n2) Run a loop over these pairs to find missing information e.g. they forgot to add their date of birth, or didn't enter their income.\n3) Report back to the user the missing information so they can correct the document and reupload them?\nPreferably in real time? So they hit submit on the webpage, it extracts, analyses and provides a result in a few seconds?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Machine Learning (AutoML) export data to SharePoint",
        "Question_creation_time":1631064917827,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543361\/azure-machine-learning-automl-export-data-to-share.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am using Azure Machine Learning Studio to design pipelines to analyze data.\nIs there any possibility to export data to sharepoint?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure Synapse ML predict [Errno 20] Not a directory",
        "Question_creation_time":1648335577807,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/788637\/azure-synapse-ml-predict-errno-20-not-a-directory.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I follow the official tutotial from microsoft: https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\n\nBut when I execute:\n\n #Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=\"Sales\", #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n\n\n\nI\u00b4ve got:\n\n\n\n\nNotADirectoryError: [Errno 20] Not a directory: '\/mnt\/var\/hadoop\/tmp\/nm-local-dir\/usercache\/trusted-service-user\/appcache\/application_1648328086462_0002\/spark-3d802a7e-15b7-4eb6-88c5-f0e01f8cdb35\/userFiles-fbe23a43-67d3-4e65-a879-4a497e804b40\/68603955220f5f8646700d809b71be9949011a2476a34965a3d5c0f3d14de79b.pkl\/MLmodel'\nTraceback (most recent call last):\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_context.py\", line 47, in bind_model\nudf = _create_udf(\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_udf.py\", line 104, in _create_udf\nmodel_runtime = runtime_gen._create_runtime()\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 103, in _create_runtime\nif self._check_model_runtime_compatibility(model_runtime):\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 166, in _check_model_runtime_compatibility\nmodel_wrapper = self._load()\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 78, in _load\nreturn SynapsePredictModelCache._get_or_load(\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_cache.py\", line 172, in _get_or_load\nmodel = load_model(runtime, model_uri, functions)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 257, in load_model\nmodel = loader.load(model_uri, functions)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 122, in load\nmodel = self._load(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 215, in _load\nreturn self._load_mlflow(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 59, in _load_mlflow\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/`init`.py\", line 640, in load_model\nmodel_meta = Model.load(os.path.join(local_path, MLMODEL_FILE_NAME))\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 124, in load\nwith open(path) as f:\n\nNotADirectoryError: [Errno 20] Not a directory: '\/mnt\/var\/hadoop\/tmp\/nm-local-dir\/usercache\/trusted-service-user\/appcache\/application_1648328086462_0002\/spark-3d802a7e-15b7-4eb6-88c5-f0e01f8cdb35\/userFiles-fbe23a43-67d3-4e65-a879-4a497e804b40\/68603955220f5f8646700d809b71be9949011a2476a34965a3d5c0f3d14de79b.pkl\/MLmodel'\n\nHow can I fix that error ?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":3.0,
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Data Import error for Azure table storage to Azure ML studio ?",
        "Question_creation_time":1616070530733,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/320696\/data-import-error-for-azure-table-storage-to-azure.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi Team,\n\nI tried connecting to Azure table storage in Azure ML Studio. It shows connection successful after updating all credentials but after hitting run, import is landing to internal system error.\nBelow is the message :\n[Critical] Error: Sorry, it seems that you have encountered an internal system error. Please contact amlforum@microsoft.com with the full URL in the browser and the time you experienced the failure. We can locate this error with your help and investigate further. Thank you.\n\nRequesting you to please assist in this case.\n\nRegards,\nSachin",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":6.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure AutoML Featurisation Error",
        "Question_creation_time":1638154550033,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/643670\/azure-automl-featurisation-error.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"According the following doc, I should be able to to turn on FeaturizationConfig in the settings:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\n\nHowever I'm getting the following error when I try to change the switch to 'FeaturizationConfig' when setting up the AutoML experiment:\n\nConfigException: ConfigException: Message: Invalid argument(s) 'featurizationconfig' specified. Supported value(s): 'off, auto'\n\nThe following is my settings:\n\nimport logging\n\n\nautoml_settings = {\n\"iteration_timeout_minutes\": 15,\n\"experiment_timeout_hours\": 0.3,\n\"enable_early_stopping\": True,\n\"primary_metric\": 'spearman_correlation',\n\"featurization\": 'FeaturizationConfig',\n\"verbosity\": logging.INFO,\n\"n_cross_validations\": 5\n}",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Not enough quota available when deploying a machine learning model on Azure",
        "Question_creation_time":1666148560563,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1053752\/not-enough-quota-available-when-deploying-a-machin.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I was trying to deploy and score a machine learning model by using an online endpoint.\n\nWhen I was trying to run code this on Azure Machine Learning Wordspace,\n\n !az ml online-deployment create --name fraud-ga --endpoint endpoint-name -f ..\/deployment\/deployment.yml --all-traffic\n\n\n\nI got this error:\n\n {\"errors\":{\"VmSize\":[\"Not enough quota available for Standard_F16s_v2 in SubscriptionId 671ef6e1-2ded-466b-8fd1-91363cf12275. Current usage\/limit: 4\/6. Additional needed: 32 Please see troubleshooting guide, available here: https:\/\/aka.ms\/oe-tsg#error-outofquota\"]},\"type\":\"https:\/\/tools.ietf.org\/html\/rfc7231#section-6.5.1\",\"title\":\"One or more validation errors occurred.\",\"status\":400,\"traceId\":\"00-a308e99ddee5fc8714e34fd0808b7e93-2031400dbc3e84d1-01\"}\n\n\n\n\nWhat I understood is that I need more cores.\n\nSo, in this case how many cores I needed and how to solve this error?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"feature in new generation of classic studio",
        "Question_creation_time":1648513803980,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/791041\/feature-in-new-generation-of-classic-studio.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"hi:\n\nI have a question regarding to the new generation as I have already known that the classic old version of machine learning studio is retiring in August 2024.\n\nI wonder if all the features will be continouslty supported in the new generation of new version of studio in the future?",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":0.0,
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Azure On-Demand ML cluster from a search in the data catalog",
        "Question_creation_time":1619209726297,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/369952\/azure-on-demand-ml-cluster-from-a-search-in-the-da.html",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand\/automatically with the dataset they found a search in the Azure Data Catalog. I visualize, once the user finds the data in a search, there will be a link that will take him\/her to a Notebook and the dataset can be used for analysis. Any suggestion would be very much appreciated!",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":1.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"NameError when trying to run an ScriptRunConfig in Azure Machine Learning",
        "Question_creation_time":1640331391010,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in.html",
        "Question_upvote_count":0.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I'm trying to deploy a locally trained RandomForest model into Azure Machine Learning Studio.\n\ntraining code (whentrain.ipynb) :\n\n #import libs and packages\n import numpy as np\n import pandas as pd\n    \n from sklearn.preprocessing import MinMaxScaler\n from sklearn.model_selection import train_test_split\n    \n from sklearn import metrics\n from sklearn.metrics import r2_score\n from math import sqrt\n    \n from sklearn.ensemble import RandomForestRegressor\n    \n from sklearn.preprocessing import LabelEncoder\n from imblearn.over_sampling import SMOTE\n    \n import xgboost as xgb\n from sklearn.metrics import accuracy_score\n from azureml.core import Workspace, Dataset\n    \n # get existing workspace\n workspace = Workspace.from_config(path=\"config.json\")\n    \n # get the datastore to upload prepared data\n datastore = workspace.get_default_datastore()\n    \n # load the dataset which is placed in the data folder\n dataset = Dataset.Tabular.from_delimited_files(datastore.path('UI\/12-23-2021_023530_UTC\/prepped_data101121.csv'))\n dataset = dataset.to_pandas_dataframe()\n    \n # Create the outputs directories to save the model and images\n os.makedirs('outputs\/model', exist_ok=True)\n os.makedirs('outputs\/output', exist_ok=True)\n dataset['Date'] = pd.to_datetime(dataset['Date'])\n dataset = dataset.set_index('Date')\n ###\n scaler = MinMaxScaler()\n    \n #inputs\n X = dataset.iloc[:, 1:]\n #output\n y = dataset.iloc[:, :1]\n    \n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, shuffle=True)\n    \n X_train = scaler.fit_transform(X_train)\n X_test = scaler.fit_transform(X_test)\n    \n ###\n    \n model1 = RandomForestRegressor(n_estimators = 6,\n                                    max_depth = 10,\n                                    min_samples_leaf= 1,\n                                    oob_score = 'True',\n                                    random_state=42)\n model1.fit(X_train, y_train.values.ravel())\n    \n y_pred2 = model1.predict(X_test)\n\n\n\n\nAnd here is the code on the estimator part (estimator.ipynb):\n\n from azureml.core import Experiment\n from azureml.core import Workspace\n from azureml.core.compute import ComputeTarget, AmlCompute\n from azureml.core.compute_target import ComputeTargetException\n from azureml.train.dnn import TensorFlow\n from azureml.widgets import RunDetails\n    \n import os\n    \n workspace = Workspace.from_config(path=\"config.json\")\n exp = Experiment(workspace=workspace, name='azure-exp')\n cluster_name = \"gpucluster\"\n    \n try:\n     compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n     print('Found existing compute target')\n except ComputeTargetException:\n     print('Creating a new compute target...')\n     compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n                                                            max_nodes=1)\n    \n     compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n    \n     compute_target.wait_for_completion(show_output=True)  # , min_node_count=None, timeout_in_minutes=20)\n     # For a more detailed view of current AmlCompute status, use get_status()\n     print(compute_target.get_status().serialize())\n from azureml.core import ScriptRunConfig\n source_directory = os.getcwd()\n    \n from azureml.core import Environment\n    \n myenv = Environment(\"user-managed-env\")\n myenv.python.user_managed_dependencies =True\n from azureml.core import Dataset\n test_data_ds = Dataset.get_by_name(workspace, name='prepped_data101121')\n    \n src = ScriptRunConfig(source_directory=source_directory,\n                       script='whentrain.ipynb',\n                          \n                       arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],\n                       compute_target=compute_target,\n                       environment=myenv)\n run = exp.submit(src)\n RunDetails(run).show()\n run.wait_for_completion(show_output=True)\n\n\n\n\nThe error that happens in run.wait_for_completion states :\n\n [stderr]Traceback (most recent call last):\n [stderr]  File \"whentrain.ipynb\", line 107, in <module>\n [stderr]    \"notebookHasBeenCompleted\": true\n [stderr]NameError: name 'true' is not defined\n [stderr]\n\n\n\nAs you can see in my whentrain.ipynb, it does not even reach line 107, and I could not find where this error come from. So how do I fix it?\n\nI'm running the Notebook on Python 3.\n\n\n\n\n\nUPDATE:\n\nOkay, after a little adjustment that should not affect the whole code (I just removed some extra columns, added model save code in whentrain.ipynb making use of import os) it's now giving me somewhat the same error.\n\n [stderr]Traceback (most recent call last):\n [stderr]  File \"whentrain.ipynb\", line 115, in <module>\n [stderr]    \"source_hidden\": false,\n [stderr]NameError: name 'false' is not defined\n [stderr]",
        "Tool":"Azure Machine Learning",
        "Question_comment_count":2.0,
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use previous version of model as initial weights",
        "Question_creation_time":1602183567290,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/use-previous-version-of-model-as-initial-weights\/525",
        "Question_upvote_count":3.0,
        "Question_view_count":456.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a pipeline the output of which is a model tracked with dvc and lets say it is tagged v0.1.0. I would like to use the weights of  this model as the initial weights in training a new model. I was wondering what the best way to achieve this would be?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC with bitbucket",
        "Question_creation_time":1564573379958,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-bitbucket\/192",
        "Question_upvote_count":2.0,
        "Question_view_count":1945.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I want to setup DVC on top of bitbucket instead of git. How can i do it. Can DVC integrate with bitbucket.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC integration with AZURE ML Pipeline and versioning IOT data",
        "Question_creation_time":1587729946039,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-integration-with-azure-ml-pipeline-and-versioning-iot-data\/364",
        "Question_upvote_count":0.0,
        "Question_view_count":1728.0,
        "Question_answer_count":12,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have two questions.<\/p>\n<ol>\n<li>My data storage is Azure blob which contain parquet files. These parquet files are output of preprocessing IOT data which means data keep coming in storage blobs. How to version data with dvc because with every change dvc will create a new version and then i will end up in many many data versions.<br>\nPlease help me in understanding how people integrate dvc with continuous data.<\/li>\n<li>I am using Azure ML env. for ML pipeline. How i can integrate dvc with Azure ML env.<br>\nThanks in advance.<\/li>\n<\/ol>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SSH remote: unexpected error - Permission denied",
        "Question_creation_time":1660556492786,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/ssh-remote-unexpected-error-permission-denied\/1300",
        "Question_upvote_count":2.0,
        "Question_view_count":105.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m trying to configure an SSH remote storage. When I try to push my data directory I get:<\/p>\n<pre><code class=\"lang-auto\">$  dvc push\nERROR: unexpected error - Permission denied: Permission denied                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n<\/code><\/pre>\n<p>I can confirm that:<\/p>\n<ul>\n<li>I can connect to the server via <code>ssh<\/code> and <code>sftp<\/code>\n<\/li>\n<li>the URL is correct<\/li>\n<li>I have write permissions in the path (already tried by manually uploading the files via <code>sftp<\/code>.<\/li>\n<\/ul>\n<p>I can\u2019t really make sense of the debug message:<\/p>\n<pre><code class=\"lang-auto\">$  dvc push -v\n2022-08-15 11:38:48,112 DEBUG: Lockfile for 'dvc.yaml' not found      \n2022-08-15 11:38:48,148 WARNING: Output 'data\/numpy'(stage: 'convert_puf_responses') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\n2022-08-15 11:38:48,273 DEBUG: Preparing to transfer data from '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/sram_startup\/.dvc\/cache' to '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,273 DEBUG: Preparing to collect status from '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,273 DEBUG: Collecting status from '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,274 DEBUG: Querying 1 oids via object_exists                                                                  \n2022-08-15 11:38:48,425 ERROR: unexpected error - Permission denied: Permission denied                                            \n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 91, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.10\/contextlib.py\", line 619, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/misc.py\", line 274, in __aenter__\n    self._coro_result = await self._coro\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 7707, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 408, in wait_for\n    return await fut\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 440, in _connect\n    await options.waiter\nasyncssh.misc.PermissionDenied: Permission denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 109, in push\n    return self.transfer(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 88, in transfer\n    return transfer(src_odb, dest_odb, objs, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/transfer.py\", line 158, in transfer\n    status = compare_status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 179, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 136, in status\n    exists = hashes.intersection(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 56, in _indexed_dir_hashes\n    dir_exists.update(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\n    for obj in iterable:\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/db.py\", line 255, in list_oids_exists\n    yield from itertools.compress(oids, in_remote)\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 609, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 269, in exists\n    return self.fs.exists(path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/ssh.py\", line 115, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 76, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 76, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 86, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 66, in sync\n    raise return_result\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 26, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 445, in wait_for\n    return fut.result()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 29, in wrapper\n    raise PermissionError(exc.reason) from exc\nPermissionError: Permission denied\n------------------------------------------------------------\n2022-08-15 11:38:48,557 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 91, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.10\/contextlib.py\", line 619, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/misc.py\", line 274, in __aenter__\n    self._coro_result = await self._coro\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 7707, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 408, in wait_for\n    return await fut\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 440, in _connect\n    await options.waiter\nasyncssh.misc.PermissionDenied: Permission denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 109, in push\n    return self.transfer(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 88, in transfer\n    return transfer(src_odb, dest_odb, objs, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/transfer.py\", line 158, in transfer\n    status = compare_status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 179, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 136, in status\n    exists = hashes.intersection(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 56, in _indexed_dir_hashes\n    dir_exists.update(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\n    for obj in iterable:\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/db.py\", line 255, in list_oids_exists\n    yield from itertools.compress(oids, in_remote)\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 609, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 269, in exists\n    return self.fs.exists(path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/ssh.py\", line 115, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 76, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 76, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 86, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 66, in sync\n    raise return_result\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 26, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 445, in wait_for\n    return fut.result()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 29, in wrapper\n    raise PermissionError(exc.reason) from exc\nPermissionError: Permission denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 68, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 288, in reflink\n    return self.fs.reflink(from_info, to_info)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/local.py\", line 157, in reflink\n    return system.reflink(path1, path2)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/system.py\", line 105, in reflink\n    _reflink_linux(source, link_name)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/system.py\", line 91, in _reflink_linux\n    fcntl.ioctl(d.fileno(), FICLONE, s.fileno())\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 127, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 76, in _try_links\n    raise OSError(\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/sram_startup\/.dvc\/cache\/.QXMYC5qmirjwhLDdpceujW.tmp'\n2022-08-15 11:38:48,569 DEBUG: Version info for developers:\nDVC version: 2.12.0 (pip)\n---------------------------------\nPlatform: Python 3.10.5 on Linux-5.10.135-1-MANJARO-x86_64-with-glibc2.36\nSupports:\n\twebhdfs (fsspec = 2022.5.0),\n\thttp (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),\n\thttps (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),\n\tssh (sshfs = 2022.6.0)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/mapper\/luks-9d8db46c-e044-463a-9dda-ad428bb54390\nCaches: local\nRemotes: ssh\nWorkspace directory: ext4 on \/dev\/mapper\/luks-9d8db46c-e044-463a-9dda-ad428bb54390\nRepo: dvc (subdir), git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-08-15 11:38:48,571 DEBUG: Analytics is enabled.\n2022-08-15 11:38:48,606 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8enuwu2w']'\n2022-08-15 11:38:48,607 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8enuwu2w']'\n<\/code><\/pre>\n<p>What am I missing? Any ideas?<br>\nThanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC and AWS EFS",
        "Question_creation_time":1646678462276,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-aws-efs\/1103",
        "Question_upvote_count":0.0,
        "Question_view_count":172.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi. Can DVC work with an Amazon Web Services EFS volume? I looked at the list of supported storage types and did not see EFS, but figured I\u2019d ask here to be sure because my boss is asking me to be sure <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"First run of DVC - getting a \"failed to reproduce\" error",
        "Question_creation_time":1554389891804,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/first-run-of-dvc-getting-a-failed-to-reproduce-error\/171",
        "Question_upvote_count":2.0,
        "Question_view_count":3468.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI just cloned a project from a colleague; in this project there is a <em>.dvc<\/em> file, with some <em>deps<\/em> and some <em>outs<\/em>.<br>\nWhen I run <em>dvc repro -f myfile.dvc<\/em>, I got an error<\/p>\n<blockquote>\n<p>\u2026failed to reproduce \u2018myfile.dvc\u2019: output \u2018xxx\u2019 does not exist<\/p>\n<\/blockquote>\n<p>Well, obviously the <em>output<\/em> does not exist, this is the first time I run the script, so nothing was created yet !<br>\nI got the same error when I run with the \u2018\u2013force\u2019 flag<\/p>\n<p>How do I run the pipeline for the first time ?<br>\nI am missing something ?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Non-overlapping outs paths",
        "Question_creation_time":1590514518218,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/non-overlapping-outs-paths\/396",
        "Question_upvote_count":1.0,
        "Question_view_count":896.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>My working directory contains the following folders:<\/p>\n<ul>\n<li>.dvc<\/li>\n<li>.git<\/li>\n<li>data<\/li>\n<li>models<\/li>\n<li>src<\/li>\n<\/ul>\n<p>And files:<\/p>\n<ul>\n<li>.gitignore<\/li>\n<li>data.dvc<\/li>\n<\/ul>\n<p>The <code>data.dvc<\/code> file currently points to all the contents of the data folder. The src folder contains a <code>prepare_data.py<\/code> file that takes in images from both the data\/train and data\/test folders as inputs and then outputs four files:<\/p>\n<ul>\n<li>data\/train\/imgs_train.npy<\/li>\n<li>data\/train\/train_labels.pkl<\/li>\n<li>data\/test\/imgs_test.npy<\/li>\n<li>data\/test\/test_labels.pkl<\/li>\n<\/ul>\n<p>Now, I want to create a reproducible stage for <code>src\/prepare_data.py<\/code>. To do this, I ran the following command:<\/p>\n<pre><code class=\"lang-auto\">dvc run -f prepare_data.dvc \\\n        -d src\/prepare_data.py -d data\/train -d data\/test \\\n        -o data\/train\/imgs_train.npy -o data\/train\/train_labels.pkl \\\n        -o data\/test\/imgs_test.npy -o data\/test\/test_labels.pkl \\\n        python src\/prepare_data.py\n<\/code><\/pre>\n<p>However, I received the following error message:<\/p>\n<pre><code class=\"lang-auto\">ERROR: failed to run command - Paths for outs:\n'data'('data.dvc')\n'data\\train\\imgs_train.npy'('prepare_data.dvc')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>I can see that there is a problem with referencing data\/train and data\/test when the data folder has already been tracked using the <code>data.dvc<\/code> file. However, I still want to create the reproducible stage, so any suggestions?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AttributeError with dvc.api.read to azure blob storage",
        "Question_creation_time":1614251830608,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/attributeerror-with-dvc-api-read-to-azure-blob-storage\/688",
        "Question_upvote_count":0.0,
        "Question_view_count":349.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I am new to DVC and evaluate it in a proof of concept implementation for our ML projects, which seems to fit perfectly! But I encounter a problem with dvc.api.open while using an Azure Blob Storage.<\/p>\n<p>What I have done:<\/p>\n<ul>\n<li>cloned <a href=\"https:\/\/github.com\/iterative\/dataset-registry\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/dataset-registry: Dataset registry DVC project<\/a>\n<\/li>\n<li>dvc remote add -d myremote azure:\/\/BLOB\/PATH<\/li>\n<li>dvc remote modify --local myremote connection_string \u2018CONNECTION_STRING\u2019<\/li>\n<li>created test file<\/li>\n<li>dvc add &amp; push<\/li>\n<li>removed the test file incl. cache from local repo<\/li>\n<li>dvc.api.read ==&gt; AttributeError<\/li>\n<li>dvc pull<\/li>\n<li>dvc.api.read ==&gt; works<\/li>\n<\/ul>\n<p>I am able to use dvc push and dvc pull, but by using dvc.api.read I get \u201cAttributeError: \u2018NoneType\u2019 object has no attribute \u2018account_key\u2019\u201d (see attached screenshots). If the file is downloaded with dvc pull and it is available in the cache folder everything works.<\/p>\n<p>Can anyone point me to the problem or my misunderstanding? I want to use the streaming functionality, as we have very large files and do not want to store them on the storage of a virtual machine.<\/p>\n<p>Thanks!<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/3a37b2884021232141bb6f25c162527b698ad682.jpeg\" data-download-href=\"\/uploads\/short-url\/8j11vMuBq2Jq8JpCuTrAfmDO8eu.jpeg?dl=1\" title=\"Unbenannt\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_431x500.jpeg\" alt=\"Unbenannt\" data-base62-sha1=\"8j11vMuBq2Jq8JpCuTrAfmDO8eu\" width=\"431\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_431x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_646x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_862x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Unbenannt<\/span><span class=\"informations\">1000\u00d71158 273 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Migration tutorials",
        "Question_creation_time":1608660663563,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/migration-tutorials\/601",
        "Question_upvote_count":1.0,
        "Question_view_count":413.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>is there any tutorial how to migrate from git lfs to dvc?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error \"Unable to find DVC file with output\"",
        "Question_creation_time":1635579910534,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unable-to-find-dvc-file-with-output\/942",
        "Question_upvote_count":1.0,
        "Question_view_count":176.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I successfully added a directory with <code>dvc add data<\/code>. The directory contains a subdirectory <code>A<\/code>. Now I want to rename the directory with <code>dvc move data\/A data\/B<\/code> but this gives me the error: <code>ERROR: failed to move 'A -&gt; 'B' - Unable to find DVC file with output A'<\/code>. What am I missing here?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a review mechanism for pushing dataset through DVC?",
        "Question_creation_time":1628521483560,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-there-a-review-mechanism-for-pushing-dataset-through-dvc\/837",
        "Question_upvote_count":1.0,
        "Question_view_count":187.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Usually when we are writing code, we work on a branch and then raise a merge request to merge it into the main branch. A reviewer, then reviews the code and approves the merge request.<\/p>\n<p>I am wondering if there is any approval\/review mechanism before we do a \u201cdvc push\u201d so that the reviewer has an opprtunity to catch dataset errors before the dataset is uploaded to the remote storage?<\/p>\n<p>For instance, I am following this tutorial: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>\n<p>Here, I would like a reviewer to review and approve the dataset before \u201cdvc push\u201d command is run and the dataset is uploaded to the remote storage.<\/p>\n<p>If not, what procedures do you guys follow to keep a check before the dataset upload?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practices for collaborating with DVC",
        "Question_creation_time":1595014568166,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-for-collaborating-with-dvc\/449",
        "Question_upvote_count":8.0,
        "Question_view_count":1173.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello all!<\/p>\n<p>I am new to dvc and I am trying to figure out how to fit it into our workflows.<\/p>\n<p>The first use case that I am trying to figure out is collaborating on building a dataset, particularly with trainees or collaborators of unknown skill. (Imagine we are all working on labelling new data).<\/p>\n<p>What are best practices for reviewing the trainees work before allowing them to do a dvc push?<\/p>\n<p>Currently we use gitlab UI to require merge reviews for all code changes, and this allows me to visually inspect all changes before allowing a merge. What is the equivalent for dvc?<\/p>\n<p>Thanks for any input!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc non bare remote",
        "Question_creation_time":1632379867905,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-non-bare-remote\/901",
        "Question_upvote_count":2.0,
        "Question_view_count":188.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nlet\u2019s say I want to use dvc with azure blob storage. Is there a way that I can use the data from the blob storage directly in other azure projects? Normally a blob storage just gets mounted into another resource. With dvc this seems a problem, because files on the remote are stored differently than locally. Is there a way around that? With git for example it is possible to push into a non bare repository. Is there a similar option for dvc?<br>\nThanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Parameterlike dependencies",
        "Question_creation_time":1606585276131,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/parameterlike-dependencies\/565",
        "Question_upvote_count":2.0,
        "Question_view_count":692.0,
        "Question_answer_count":12,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI have a question about the following scenario:<br>\nLet data1 and data2 be two data folders, script.py a script and the command to run the script like: <code>python  script.py --datadir .\/df<\/code> where df is equal to data1 or data2.<br>\nWhat is a good way to setup the stage now? I thought about these options:<\/p>\n<ol>\n<li>one stage + make datadir a parameter -&gt; dvc will not update after changes in data1\/data2<\/li>\n<li>one stage + just one data folder -&gt; I have to checkout the right data all the time (for example tests) and maybe another script needs both folders at the same time.<\/li>\n<li>two stages, one for each datafolder -&gt; I would have to add extra argument \u201cstage\u201d to  script.py to load the right parameters.<\/li>\n<\/ol>\n<p>Is there a better way to do this?<\/p>\n<p>Thanks for your great work by the way : )<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Handle cache to keep the latest version of data",
        "Question_creation_time":1535720101051,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/handle-cache-to-keep-the-latest-version-of-data\/87",
        "Question_upvote_count":4.0,
        "Question_view_count":592.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi Ruslan,<\/p>\n<p>I would like to ask the best practice on how to handle the following situation with DVC.<br>\nLet\u2019s suppose we have a big raw dataset stored in the cloud.<\/p>\n<ul>\n<li>So, we initialize a project (git + dvc) to work with this dataset. At first we would like to trace how to get the dataset locally. For this we use <code>dvc run<\/code> with commands as <code>wget url<\/code> to download the raw dataset. We commit changes etc.<\/li>\n<li>Next we develop some scripts to preprocess the raw dataset into a dataset we call v0.1.0. Then we use <code>dvc run<\/code> to run the preprocessing on the raw dataset and store the way to obtain the dataset v0.1.0. Thus, we have two folders like <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>. We commit and push with git and dvc. At this step we would like to keep only <code>dataset_v0.1.0<\/code> in the cache and remove <code>raw_data<\/code> from cache.<\/li>\n<li>We have another developer to work on the project, so he\/she uses git to clone the project and run <code>dvc checkout<\/code> or <code>dvc pull<\/code> to get the latest data version state. After the last command he\/she gets from remote both data: <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>.<\/li>\n<\/ul>\n<p>Is it possible to handle the cache in such manner that we keep only the latest version without using in-place preprocessing for a given commit ?<\/p>\n<p>Thank you<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc metrics diff on metrics stored in dvc-cache?",
        "Question_creation_time":1589548601677,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-metrics-diff-on-metrics-stored-in-dvc-cache\/386",
        "Question_upvote_count":4.0,
        "Question_view_count":712.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello!<\/p>\n<p>Does the <code>dvc metrics diff ref_a ref_b<\/code> command work on metrics which are tracked <em>not<\/em> by git, but instead by dvc?<\/p>\n<p>Our current setup is the following:<\/p>\n<ul>\n<li>We have a dvc-pipeline-stage, which produces a <code>eval.json<\/code>, which is cached by dvc<br>\n<code> dvc run -d dep1 -d dep2 -m eval.json  -f data\/evaluation.dvc python my_script.py<\/code>\n<ul>\n<li>According to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#options\" rel=\"nofollow noopener\">docs<\/a>, the lowercase <code>-m<\/code> puts <code>eval.json<\/code> on the gitignore and into the dvc cache<\/li>\n<\/ul>\n<\/li>\n<li>Our model is trained remotely via a CI-Pipeline, which after a successful run executes a <code>git add . &amp;&amp; git commit &amp;&amp; git push &amp;&amp; dvc push<\/code>\n<\/li>\n<li>On our local machines then we execute a <code>git pull &amp;&amp; dvc pull<\/code>. We now have the trained model and our <code>eval.json<\/code> locally available<\/li>\n<li>When we now execute <code>dvc metrics diff some_other_commit_hash <\/code> the command does not find the <code>eval.json<\/code> for the <code>some_other_commit_hash<\/code>:<pre><code class=\"lang-auto\">$ dvc metrics diff 5f036012bc9d35b7240eab3b7a42792093612ba8 \nWARNING: Metrics file 'data\/evaluation\/eval.json' does not exist at the reference '5f036012bc9d35b7240eab3b7a42792093612ba8'.\n        Path               Metric          Value                Change      \ndvc_pipeline\/data\/evaluation   f1       0.11549247896660152   diff not supported\n\/eval.json                                                                      \ndvc_pipeline\/data\/evaluation   acc      0.6784822940826416    diff not supported\n\/eval.json                                                                      \ndvc_pipeline\/data\/evaluation   loss     1.1063932177428895    diff not supported\n\/eval.json                                                                      \ndata\/evaluation\/eval.json      f1       0.43913782532829426   diff not supported\ndata\/evaluation\/eval.json      acc      0.750374436378479     diff not supported\ndata\/evaluation\/eval.json      loss     0.6777220140143865    diff not supported\n<\/code><\/pre>\n<\/li>\n<li>I\u2019m assuming this command fails, since (I\u2019m guessing) dvc looks for the <code>eval.json<\/code> in the git-cache, but not in the dvc-cache (where it is stored)<\/li>\n<\/ul>\n<p>Is there a flag, to tell <code>dvc metrics diff ...<\/code> that the metrics-file is stored in the dvc cache? Or is this expected to work, and we have an error somewhere else?<\/p>\n<p>Thanks in advance!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Configure: DVC + CircleCI + GDRIVE",
        "Question_creation_time":1639931189346,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/configure-dvc-circleci-gdrive\/1011",
        "Question_upvote_count":0.0,
        "Question_view_count":244.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Trying to configure DVC to pull gdrive stored artifacts from a CircleCI build? I followed instructions from <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote\" rel=\"noopener nofollow ugc\">docs<\/a>, hints in this <a href=\"https:\/\/discuss.dvc.org\/t\/cml-github-actions-google-drive-service-account\/795\">discussion-question<\/a> &amp; this <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\" rel=\"noopener nofollow ugc\">git-hub issue<\/a> but there seems to be something missing.<\/p>\n<p>Here is the excerpt of the circle-ci configuration being used:<\/p>\n<pre><code class=\"lang-auto\">- run:\n     name: Data checkout\n     command: |\n        dvc remote modify storage --local gdrive_use_service_account true;\n        dvc remote modify storage --local gdrive_service_account_json_file_path \/dev\/null;\n        dvc remote modify storage --local gdrive_service_account_user_email &lt;my@service.iam.gserviceaccount.com&gt;;\n        dvc pull --recursive -r storage --verbose;\n     environment:\n        GDRIVE_CREDENTIALS_DATA: $GDRIVE_CREDENTIALS_DATA\n<\/code><\/pre>\n<p>The <code>$GDRIVE_CREDENTIALS_DATA<\/code> environment variable was configured in Circle-CI server to be that of my Google\u2019s Service Account JSON to access my google drive. But the CircleCI runner reports this error as if the environment variable did not get read.<\/p>\n<pre><code class=\"lang-auto\">0% 0\/1 [00:00&lt;?, ?file\/s{'info': ''}]\nGo to the following link in your browser:\n\n    https:\/\/accounts.google.com\/o\/oauth2\/auth?client_id=710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.appdata&amp;access_type=offline&amp;response_type=code&amp;approval_prompt=force\n\nEverything is up to date.              \nERROR: failed to pull data from the cloud - GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'.\nBackup first, remove or fix them, and run DVC again.\nIt should do auth again and refresh the credentials.\n<\/code><\/pre>\n<p>Things I am unsure about.<\/p>\n<ol>\n<li>Is the <code>GDRIVE_CREDENTIALS_DATA<\/code> environment variable the right name DVC commands looks for?<\/li>\n<li>Which JSON content should this environment variable have?\n<ul>\n<li>\n<code>.dvc\/tmp\/gdrive-user-credentials.json<\/code> ?<\/li>\n<li>Google Service account JSON looking like this?<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">{\n  \"type\": \"service_account\",\n  \"project_id\": \"&lt;my_project&gt;\",\n  \"private_key_id\": \"&lt;key_id&gt;\",\n  \"private_key\": \"&lt;key_hash&gt;\",\n  \"client_email\": \"&lt;my@service.iam.gserviceaccount.com&gt;\",\n  \"client_id\": \"&lt;client_id&gt;\",\n  \"auth_uri\": \"https:\/\/accounts.google.com\/o\/oauth2\/auth\",\n  \"token_uri\": \"https:\/\/oauth2.googleapis.com\/token\",\n  \"auth_provider_x509_cert_url\": \"https:\/\/www.googleapis.com\/oauth2\/v1\/certs\",\n}\n<\/code><\/pre>\n<ul>\n<li>Or Google client secret JSON looking like this?<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">{\n  \"installed\": {\n    \"client_id\": \"&lt;google_client_id&gt;\",\n    \"project_id\": \"&lt;google_project_id&gt;\",\n    \"auth_uri\": \"https:\/\/accounts.google.com\/o\/oauth2\/auth\",\n    \"token_uri\": \"https:\/\/oauth2.googleapis.com\/token\",\n    \"auth_provider_x509_cert_url\": \"https:\/\/www.googleapis.com\/oauth2\/v1\/certs\",\n    \"client_secret\": \"&lt;client_secret&gt;\",\n    \"redirect_uris\": [\n      \"urn:ietf:wg:oauth:2.0:oob\",\n      \"http:\/\/localhost\"\n    ]\n  }\n}\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Is the 1st time google browser authentication step still required for CI machines?<\/li>\n<\/ol>\n<p>Many thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Authenticate DVC python api with GitHub app credentials",
        "Question_creation_time":1666090734944,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/authenticate-dvc-python-api-with-github-app-credentials\/1364",
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi Folks,<\/p>\n<p>I am trying to use dvc python api which refers to private GitHub repo.<\/p>\n<pre><code class=\"lang-auto\">dvc.api.get_url(\n    path='data\/data.json',\n    repo='https:\/\/github.com\/owner\/private-repo.git'\n)\n<\/code><\/pre>\n<p>Without any credentials I get <code>dulwich.client.HTTPUnauthorized: No valid credentials provided<\/code> error.<\/p>\n<p>I have registered Github app and want to use credentials of same to do some operations using python api. Can someone help me with how do we provide credentials to dvc python api?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC local storage usecase",
        "Question_creation_time":1611043180303,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-local-storage-usecase\/628",
        "Question_upvote_count":2.0,
        "Question_view_count":740.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I\u2019m trying to understand if DVC is a good solution for our company use-case. We currently have several tens of TB of data, and constantly adding to it every week. I would like to add versioning to this, so that our scientists can run experiments on various subsets and track all changes.<\/p>\n<p>All the data is stored locally in a network drive accessible from the scientists\u2019 computers. My question would be if DVC can be used in this way for this use-case, and how to get around copying the data multiple time (e.g. if 5 scientists access 40TB worth of data, this shouldn\u2019t be copied to their versioned repos).<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trying to understand data storage",
        "Question_creation_time":1667019094738,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trying-to-understand-data-storage\/1378",
        "Question_upvote_count":5.0,
        "Question_view_count":53.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I am a newcomer to DVC. I read through the tutorial for Data and Model Versioning, and I want to understand how data is stored.<\/p>\n<p>Specifically, say I pull from my remote storage 1000 images, and add 1000 images to create a new dataset of 2000 images. If I dvc add the new data folder and dvc push, how is it stored in remote? Is it like 1000 images (original folder), and then add 1000 images, or is it 1000 images, and then a folder with 2000 images? The reason for the confusion is because at <a href=\"https:\/\/dvc.org\/doc\/start\/data-management\/data-versioning\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Get Started: Data Versioning<\/a> (7:54), it shows that there are 2 folders in the remote google drive. I want to know this because I am a ML engineer for a computer vision company and we would have huge datasets, so we don\u2019t want any duplicates across versions.<\/p>\n<p>Furthermore, if I do a dvc checkout to a previous version (1000 images) of the dataset in my local, do 1000 images just disappear from my newer dataset (2000 images). I see this happen in the above video, where dvc checkout with a previous data\/data.xml.dvc causes the data to be 36M instead of 72M. How exactly does dvc do this switch so quickly, and does it work for a folder of images vs just a single xml files?<\/p>\n<p>Sorry if these are stupid questions, I am completely new to this.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Fill-back metrics",
        "Question_creation_time":1594737345910,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/fill-back-metrics\/441",
        "Question_upvote_count":4.0,
        "Question_view_count":313.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Say I am maintaining some data using dvc, and at some point decide I want to have a metric showing some data statistics (i.e. how many positive samples I have). Can I back-fill this metric to previous commits?  (the goal is to track the number of positives I had after each data-commit).<\/p>\n<p>If the commit history is A-&gt;B-&gt;C, I know I can go back to any commit and run a pipeline, but the metric output of this pipeline will need to be saved in a different commit, right?  So I will have to create new git commits: A-&gt;A\u2019  B-&gt;B\u2019, C-&gt;C\u2019 that will store the metric results, or is there a different way?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVClive + MMCV in Container",
        "Question_creation_time":1656944958449,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvclive-mmcv-in-container\/1236",
        "Question_upvote_count":0.0,
        "Question_view_count":145.0,
        "Question_answer_count":11,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi I\u2019m running into problems using the DVClive hook for MMCV running in a separate \u201ctraining\u201d container.<\/p>\n<p>I\u2019m running my <code>dvc exp run<\/code> from within a devcontainer (on a remote host). The actual training is run on the host in another container (that has all the right dependencies installed). Within my <code>config.py<\/code> I included the <code>DvcliveLoggerHook<\/code> as per the documentation. I noticed the <code>interval<\/code> and <code>by_epoch<\/code> arguments where not applying. I.e., I wasn\u2019t seeing any step 1, 2, 3 etc. (until <code>max_epochs<\/code>) in my  <code>dvc exp show<\/code>.<\/p>\n<p>I realized after some hours of reading the documentation and trying out different configs, that it might be because <code>Live.set_step()<\/code> is called from within a <code>DvcliveLoggerHook<\/code> running in one container and the DVC \u201cinstance\u201d in another. So there is no DVC to notify in the container that is trying to notify DVC.<\/p>\n<p>Could this be the problem? Is there a way to fix it?<\/p>\n<p>One way to \u201cfix\u201d, i.e., to work around the problem, I can imagine is just to install everything in the devcontainer. This is doable for the current container I need, but I would rather not.<\/p>\n<p>Thank you!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Specify AWS profile when adding external data from S3",
        "Question_creation_time":1647607974097,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/specify-aws-profile-when-adding-external-data-from-s3\/1126",
        "Question_upvote_count":0.0,
        "Question_view_count":500.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>how can I make DVC use a specific AWS-profile when adding external data?<\/p>\n<p>If my credentials are stored in the default-profile I can add external data as follows (just following the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">S3-Example for Managing External Data<\/a>)<\/p>\n<pre><code class=\"lang-auto\">$ git init\n$ dvc init\n$ dvc remote add s3cache s3:my-bucket\/cache\n$ dvc config cache.s3 s3cache\n$ dvc add --external s3:\/\/my-bucket\/remote-data.txt\n<\/code><\/pre>\n<p>But I want to use another profile (<code>MyProfile<\/code>) that I can add to <code>s3cache<\/code>:<\/p>\n<pre><code class=\"lang-auto\">$ dvc remote modify s3cache profile MyProfile\n<\/code><\/pre>\n<p>If the default profile is removed or changed from the <code>.aws\/credentials<\/code> the following gives a <code>Bad Request<\/code>:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external s3:\/\/my-bucket\/remote-data.txt\n# ERROR: unexpected error - [Errno 22] Bad Request: An error occurred (400) when calling the HeadObject operation: Bad Request\n<\/code><\/pre>\n<p>How can I provide the profile information to the add?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc_api.get_url is not working with external data",
        "Question_creation_time":1655903625685,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-api-get-url-is-not-working-with-external-data\/1218",
        "Question_upvote_count":2.0,
        "Question_view_count":137.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<br>\nI\u2019m fairly new to DVC. I would like to explain my set up first and then seek help to make best use of DVC.<\/p>\n<p><strong>My Setup:<\/strong><\/p>\n<ul>\n<li>\n<p>I want to use DVC to track and version external data. The data is large in size(~500GB+ currently) and Located on a NAS server which can be accessed with SSH. I\u2019m referring to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">manage external data tutoria<\/a>l<\/p>\n<\/li>\n<li>\n<p>I want to create a <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registry\" rel=\"noopener nofollow ugc\">Data Registry<\/a> with all of my external data and share it with multiple users to avoid multiple copies of the data using shared cache as explained in \u2018How to share external Cache\u2019<\/p>\n<\/li>\n<\/ul>\n<p><strong>The steps I Followed<\/strong><\/p>\n<ol>\n<li>On my local machine inside a git tracked directory, Created a DVC workspace with <code>dvc_init<\/code>.<\/li>\n<li>Configured a remote directory as dvc remote, let\u2019s say at <code>ssh:\/\/server\/home\/temp_registry<\/code>\n<\/li>\n<li>Created a external cache in <code>ssh:\/\/server\/home\/temp_registry\/temp_cache<\/code> as stated in manage external dataset<\/li>\n<li>I kept a folder named <code>'Dogs' at dvc remote<\/code>, this folder has 2 subfolders, and 2 text files, each subfolders having some images. From my local machine used <code>dvc add --external ssh:\/\/server\/home\/temp_registry\/Dogs\/.<\/code>  to add the external data under dvc tracking.<\/li>\n<li>timely commited .dvc\/config to the gitlab repo from my local machine.<\/li>\n<\/ol>\n<p><strong>The output I got:<\/strong><\/p>\n<ol>\n<li>A Dogs.dvc file was created on my local machine, which was commited to gitlab<\/li>\n<li>In cache directory under <code>ssh:\/\/server\/home\/temp_registry\/temp_cache<\/code>, I found the cached files.<\/li>\n<\/ol>\n<p><strong>The exploration:<\/strong><br>\nI tried to use python api on my local machine to fetch the url of the dvc remote\/Dogs folder.<br>\nI used<br>\n<code>dvc.api.get_url(path='Dogs.dvc', repo='gitlab_repo\/project_name.git', rev=temp_registry_branch')<\/code><\/p>\n<p>Expected output was the URL pointing to the Dogs dir, instead I received the error saying <code>dvc.exceptions.OutputNotFoundError: Unable to find DVC file with output 'Dogs.dvc'<\/code><\/p>\n<p><strong>Now the questions:<\/strong><\/p>\n<ol>\n<li>Is my approach of creating data registry using external data management techniques correct? and is it advisable to use DVC in such a way?<\/li>\n<li>Did I miss any step that should be carried out while setting up a registry on remote?<\/li>\n<li>\n<code>was the usage of dvc_api.get_url correct?<\/code> what can be done to get the url of Dogs folder located on remote?<\/li>\n<\/ol>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why DVC not working with AWS role on AWS Batch?",
        "Question_creation_time":1646921366784,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/why-dvc-not-working-with-aws-role-on-aws-batch\/1114",
        "Question_upvote_count":0.0,
        "Question_view_count":121.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m using DVC on AWS Batch, and have configured the job definition with a role with correct S3 policies (I know its correct because it worked on github actions). But, when running the job with dvc pull I got this error:<\/p>\n<pre><code class=\"lang-auto\">dvc pull -R \/app\/data\/training\nERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>The same job, if runned with the envs AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY configured, runs ok.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Add dataset metadata in .dvc file?",
        "Question_creation_time":1600251171227,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-dataset-metadata-in-dvc-file\/494",
        "Question_upvote_count":3.0,
        "Question_view_count":1246.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear Support,<\/p>\n<p>I am using DVC to track different versions of a dataset consisting of annotations on a NER dataset. In particular, I would like to keep track of some dataset metadata which may change from one version to another, and I would also like to quickly compare different versions of the dataset through the evolution of these metadata.<\/p>\n<p>The current solution I am adopting is to track the information on the dataset in a README.md but I would like to keep the link with the dataset file more clear than that.<\/p>\n<p>How to do that? I thought about adding this metadata to the <code>dataset.dvc<\/code> file that is automatically generated, but I am not sure that this is a good idea: would a future <code>dvc add<\/code> overwrite\/delete that for instance?<\/p>\n<p>Thank you in advance for your help!<\/p>\n<p>Best,<\/p>\n<p>\u2014Francesco<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc error \"file not owned by user\"",
        "Question_creation_time":1612845083215,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-error-file-not-owned-by-user\/661",
        "Question_upvote_count":0.0,
        "Question_view_count":236.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI am configuring dvc on shared system.<br>\nThe ubuntu system has multiple user ids as logins.<\/p>\n<p>The folder dataset1 is copied by one user  \u201cuser1\u201d.<br>\nNow when I am doing \u201cdvc add dataset1\u201d, its showing as<br>\n\u201cfile not owned by user\u201d.<\/p>\n<p>I have rwx permissions on the dataset1. I verified using getfacl.<br>\nCan anyone please guide how can i do \u201cdvc add\u201d without getting error. I dont want to do as root user as later during git push we would need username for tracking.<\/p>\n<p>Kindly help please.<\/p>\n<p>Thanks &amp; Regards<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I remove files which are no longer tracked by DVC from DVC remote SSH storage",
        "Question_creation_time":1548846841252,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-do-i-remove-files-which-are-no-longer-tracked-by-dvc-from-dvc-remote-ssh-storage\/113",
        "Question_upvote_count":2.0,
        "Question_view_count":2707.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I had added a folder inputs to remote ssh dvc storage.<br>\nAfter some time, i decided to remove this folder<br>\nso i did<\/p>\n<ul>\n<li><code>dvc remove inputs.dvc<\/code><\/li>\n<li>did a git commit and push of the dvc changes<br>\nBut if i use an older commit where the inputs folder was tracked by dvc i am still able to pull inputs folder from remote dvc storage<\/li>\n<\/ul>\n<p>How can i remove the file from the remote ssh dvc storage?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Opening a tracked file using python API: dvc.fs.azure.AzureAuthError: Authentication to Azure Blob Storage requires either account_name or connection_string",
        "Question_creation_time":1624290879559,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/opening-a-tracked-file-using-python-api-dvc-fs-azure-azureautherror-authentication-to-azure-blob-storage-requires-either-account-name-or-connection-string\/801",
        "Question_upvote_count":0.0,
        "Question_view_count":362.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m trying to open a tracked file (blob storage Azure) and getting the following error.<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):                                                                                                                                                              \n  File \"dvc_test.py\", line 10, in &lt;module&gt;\n    with dvc.api.open(\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 113, in __enter__\n    return next(self.gen)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/api.py\", line 76, in _open\n    with _repo.open_by_relpath(\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 113, in __enter__\n    return next(self.gen)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 488, in open_by_relpath\n    with fs.open(\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/repo.py\", line 143, in open\n    return dvc_fs.open(path_info, mode=mode, encoding=encoding, **kwargs)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/dvc.py\", line 79, in open\n    remote_obj = self.repo.cloud.get_remote(remote)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 29, in get_remote\n    return self._init_remote(name)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 49, in _init_remote\n    return get_remote(self.repo, name=name)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/remote\/__init__.py\", line 8, in get_remote\n    fs = cls(**config)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/azure.py\", line 83, in __init__\n    super().__init__(**config)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 19, in __init__\n    self.fs_args.update(self._prepare_credentials(**kwargs))\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/azure.py\", line 119, in _prepare_credentials\n    raise AzureAuthError(\ndvc.fs.azure.AzureAuthError: Authentication to Azure Blob Storage requires either account_name or connection_string.\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;\n<\/code><\/pre>\n<p>Code snippet:<\/p>\n<pre><code class=\"lang-auto\">with dvc.api.open(\n    \"Risk_Identification_Models\/pipelines\/data\/train_data.xlsx\",\n    repo=\"git@ssh.dev.azure.com:v3\/aaa\/bbb\/ccc\",\n    remote=\"risk_model_remote\",\n    rev=\"hyper_param_opt_loop_0.82_f1_on_test\",\n    mode=\"rb\",\n) as fd:\n<\/code><\/pre>\n<p>It is worth noting that I have configured remote and connection strings.<\/p>\n<p><code>dvc remote add -d risk_model_remote azure:\/\/aaa\/data<\/code><\/p>\n<p><code>dvc remote modify --local risk_model_remote connection_string 'xxx'<\/code><\/p>\n<p><code>dvc push\/pull<\/code> are working as expected but the above error throws when I tried to use Python API.<\/p>\n<p>I hope the following will help to debug this issue.<\/p>\n<pre><code class=\"lang-auto\">venv) \u279c  .dvc git:(master) \u2717 dvc doctor\nDVC version: 2.3.0 (pip)\n---------------------------------\nPlatform: Python 3.8.5 on Linux-5.8.0-55-generic-x86_64-with-glibc2.29\nSupports: azure, http, https\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: azure\nWorkspace directory: ext4 on \/dev\/nvme0n1p2\nRepo: dvc, git\n\n<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trouble modifying and saving dvc data file which lives outside the repo",
        "Question_creation_time":1594430779787,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trouble-modifying-and-saving-dvc-data-file-which-lives-outside-the-repo\/435",
        "Question_upvote_count":1.0,
        "Question_view_count":1960.0,
        "Question_answer_count":22,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a dvc setup where the git\/dvc repo, the data-file directory and the dvc-cache directory are all peers.<br>\ne.g.<br>\nmy-test-repo<br>\ntest-files<br>\nmy-test-dvc-cache<br>\nare all directories at the same level<\/p>\n<p>All the files under test-files were added using dvc add test-files<\/p>\n<p>my-test-repo\/test-files.dvc contains this line:<br>\npath: \u2026\/test-files<\/p>\n<p>.\/dvc\/config contains this line<br>\n[cache]<br>\ndir = \u2026\/\u2026\/my-test-dvc-cache<\/p>\n<p>I did a dvc push and all the file went to the specified remote.<br>\nI did a git clone and a dvc pull (on several different machines) and all the files came down in the directory structure specified above.<br>\nCode which runs from the repo and uses the files works.<\/p>\n<p>However, when I modify a data  file, I am having trouble saving it.<br>\nI changed a file in the test-files directory and dvc status shows that test-files is modified.<br>\n$ dvc status<br>\nunit_test_input.dvc:<br>\nchanged outs:<br>\nmodified:           \u2026\\test-files<br>\nchanged checksum<\/p>\n<p>When I do dvc commit it gives this error message:<br>\n$ dvc commit<br>\nERROR: failed to commit - unable to commit changed stage: \u2018test-files.dvc\u2019. Use <code>-f|--force<\/code> to force.<\/p>\n<p>So I entered dvc commit -f and it complains about files outside of the repo.<br>\nI read that  it is ok to have files outside the repo and the original push and pull operations worked fine.<\/p>\n<p>ERROR: unexpected error - Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\test-files<br>\nstderr: \u2018fatal: C:\\test-files: \u2018C:\\test-files\u2019 is outside repository at \u2018C:\\test-files\/my-test-repo\u2019\u2019<\/p>\n<p>It seems like dvc doesn\u2019t care that the data files are outside the repo, but the commit command is try to perform git commands on those files and git doesn\u2019t like them being outside the repo.<\/p>\n<p>Is that what\u2019s going on?<br>\nIs there something I can do about it?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to change metrics computation and recompute",
        "Question_creation_time":1575979302220,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-change-metrics-computation-and-recompute\/278",
        "Question_upvote_count":1.0,
        "Question_view_count":401.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<br>\nI trained several models and computed a simple metrics summary for each of them. Now, those models (and their corresponding metrics.json files) are versioned by dvc and git and there is a git tag associated with each model.<br>\nHaving the models trained and stored, I would like to enrich the metrics summary and go back and recompute it for each model \u2013 and store it in git as with the simple summary. This means the last part of the pipeline, say compute_metrics.py, will be changed and I would like to run dvc repro again. Is there a simple way of recomputing the metrics for all models (that is for all tags)?<\/p>\n<p>Thank you,<br>\nMichal<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc get git@github.com:*.git folder giving ERROR: unexpected error - Response payload is not completed",
        "Question_creation_time":1634238227604,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-git-github-com-git-folder-giving-error-unexpected-error-response-payload-is-not-completed\/919",
        "Question_upvote_count":0.0,
        "Question_view_count":281.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have successfully added a folder with 2gb of files using DVC to an s3 bucket. After that I tried to get the folder using the dvc get command. It downloaded half the folder correctly and after that the download stopped and gave me this message: ERROR: Unexpected error - Response payload not completed<br>\nAny suggestions on how I can resolve this issue?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem with dvc list",
        "Question_creation_time":1646317899344,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-dvc-list\/1089",
        "Question_upvote_count":0.0,
        "Question_view_count":897.0,
        "Question_answer_count":28,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi. New to dvc and trying to get things to work with a little sample project.<\/p>\n<p>I was able to initialize git and dvc and use an S3 bucket as my data store. I committed and pushed my code to Bitbucket (my company\u2019s internal Bitbucket instance) and I see everything there but the data (as expected). I am able to clone the git repo and pull the data from S3 as well.<\/p>\n<p>I am trying to run dvc list to see the data tracked by dvc.<\/p>\n<p>I\u2019ve tried running the following commands unsuccessfully:<br>\ndvc list <a href=\"https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd\/dvc\" rel=\"noopener nofollow ugc\">https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd\/dvc<\/a><br>\ndvc list <a href=\"https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd\" rel=\"noopener nofollow ugc\">https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd<\/a> dvc<\/p>\n<p>ERROR: failed to list \u2018ssh:\/\/cesc@sourcecode.mycompany.com:3268\/asx-jbbd\u2019 - Failed to clone repo \u2018ssh:\/\/cesc@sourcecode.mycompany.com:3268\/asx-jbbd\u2019 to \u2018C:\\Users\\cesc\\AppData\\Local\\Temp\\1\\tmp43z389r6dvc-clone\u2019<\/p>\n<p>What am I doing wrong?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker - Getting file URL",
        "Question_creation_time":1617987668667,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/sagemaker-getting-file-url\/717",
        "Question_upvote_count":0.0,
        "Question_view_count":210.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there,<br>\nI am trying to get the url to my dvc remote data from SageMaker Studio by running the below code. However, I get the below error and have not been able to solve it. Any advice would be greatly appreciated.<\/p>\n<p>!pip install dvc<br>\nfrom dvc.api import get_url<\/p>\n<p>url = get_url(<br>\nrepo=\u201c<a href=\"https:\/\/github.com\/...\/machine_learning\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/...\/machine_learning<\/a>\u201d,<br>\npath=\u201cexperiments\/\u2026\/data.pkl\u201d<br>\n)<br>\nprint(url)<\/p>\n<p>CloneError: Failed to clone repo \u2018<a href=\"https:\/\/github.com\/...\/machine_learning\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/...\/machine_learning<\/a>\u2019 to \u2018\/tmp\/tmpo6kgus_3dvc-clone\u2019<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Raw data from google big query",
        "Question_creation_time":1591031860157,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/raw-data-from-google-big-query\/400",
        "Question_upvote_count":2.0,
        "Question_view_count":910.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>so I have created a dvc to create a local raw data file based on a query of a gbq database.<br>\nnow I want to add this file to dvc for tracking so if the database will not be available in the future I am fully reproducible.<br>\nWhen I do <code>dvc add<\/code> I get the following error:<br>\ndata.csv is specified as an output in more than one stage:<br>\nraw_data.dvc<br>\nThis is not allowed. Consider using a different output name.<\/p>\n<p>What am I doing wrong?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deleting files from a directory data set",
        "Question_creation_time":1552387162914,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deleting-files-from-a-directory-data-set\/134",
        "Question_upvote_count":3.0,
        "Question_view_count":1068.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m currently tracking with dvc a directory containing the set of files of my data base. I added this on a single \u201cdvc add\u201d call, and a single entry appears on the \u201c.dvc\u201d file. I would like to erase some files from the directory without having to remove and re-add the whole directory, but I can\u2019t find information on how to do this. Is there a safe way to do it while keeping the soon-to-be-deleted files on history so I can get them back if I move to an older commit?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Upgrade to .60 produces .dvc file dependency error -- breaks pipeline",
        "Question_creation_time":1569421768226,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/upgrade-to-60-produces-dvc-file-dependency-error-breaks-pipeline\/230",
        "Question_upvote_count":1.0,
        "Question_view_count":513.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I just upgraded to dvc 0.60.0 and when attempting to run the same commands I have run for ages I get the following error<\/p>\n<p>Command run:<br>\n<code>dvc run -f entropy_filter.dvc -d entropy_config.json -d ..\/..\/..\/scripts\/entropy_filter.py -O junk_clusters.txt  python ..\/..\/..\/scripts\/entropy_filter.py entropy_config.json<\/code><\/p>\n<p>output<br>\n<code>ERROR: run_model.dvc cannot be a dependency<\/code><\/p>\n<p>Note that the dependency to entropy_filter.dvc stage (entropy_config.json) is produced by run_model.dvc.<\/p>\n<p>I then attempted to rerun the base command to regenerate run_model.dvc only to get the exact same error, that run_model.dvc cannot be a dependency. I then attempted to remove run_model.dvc and run the command again only to be given the same error. Finally I attempted a <code>dvc gc<\/code> which again gave the same error\u2026<\/p>\n<p>Only way to fix was to downgrade back to 59.2. Any suggestions on how I can change my pipeline to work with 0.60.0?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"CML + Github actions + Google Drive \/ Service Account",
        "Question_creation_time":1623930889746,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-github-actions-google-drive-service-account\/795",
        "Question_upvote_count":1.0,
        "Question_view_count":1203.0,
        "Question_answer_count":16,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>What is the correct way to use Github secrets with Google Drive key (json file)?<br>\nWe are getting the following error:<\/p>\n<p>ERROR: failed to pull data from the cloud - To use service account, set <code>gdrive_service_account_json_file_path<\/code>, and optionally<code>gdrive_service_account_user_email<\/code> in DVC config<\/p>\n<p>Local execution is configured by<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote gdrive_use_service_account true\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote --local \\\n             gdrive_service_account_json_file_path path\/to\/file.json\n<\/code><\/pre>\n<p>Cheers<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Clear local cache completely and rely on remote",
        "Question_creation_time":1608292066842,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/clear-local-cache-completely-and-rely-on-remote\/596",
        "Question_upvote_count":2.0,
        "Question_view_count":2473.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi!<\/p>\n<p>First off thanks for a great tool! While we don\u2019t use the pipelines very much, we do use DVC to store our data in my team, several sets of 100s of GBs each.<\/p>\n<p>I often work with one data set at the time, usually for several weeks. It would be nice to be able to clear the data sets not currently in use completely from my local machine. We use a monorepo for everything, and if I understand gc correctly, the reason my cache doesn\u2019t get cleared is because there are .dvc files of all sets in the branch head.<\/p>\n<p>Since there are some datasets I almost never use, it would be nice to be able to clear them completely from the local cache and then once i need it I\u2019ll take my punishment and wait for it to download from the remote using dvc pull. My question, I guess, is if there is a nice way of doing this that I\u2019m missing? Currently I\u2019ve resorted to manually deleting everything i the cache folder every few months to start fresh and pull what I need. It works but doesn\u2019t feel like the correct way to go about it.<\/p>\n<p>All the best!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc kerberos ticket issue",
        "Question_creation_time":1624344341763,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-kerberos-ticket-issue\/804",
        "Question_upvote_count":0.0,
        "Question_view_count":393.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all, have a trouble with setting up dvc remote hdfs with kerberos ticket.<br>\nDVC version: 2.3.0 (pip)<br>\nAccording to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify<\/a><br>\nI added valid <code>kerb_ticket<\/code> - path to the Kerberos ticket cache for Kerberos-secured HDFS clusters<\/p>\n<p>But have a following error during dvc push:<br>\n2021-06-22 12:42:07,580 DEBUG: Collecting information from remote cache\u2026<br>\n2021-06-22 12:42:07,580 DEBUG: Querying 1 hashes via object_exists<br>\n0% Querying remote cache|                                                        |0\/1 [00:00&lt;?,     ?file\/s]<br>\n\u2026<br>\nLoginException: Unable to obtain password from user<br>\norg.apache.hadoop.security.KerberosAuthException: failure to login: for principal: *** using ticket cache file: FILE:\/tmp\/krb5cc_1030 javax.security.auth.login.LoginException: Unable to obtain password from user<\/p>\n<p>python3.7\/site-packages\/dvc\/fs\/pool.py\", line 54, in get_connection<br>\nreturn self._conn_func(*self._conn_args, **self._conn_kwargs)<br>\nFile \u201cpyarrow\/_hdfs.pyx\u201d, line 83, in pyarrow._hdfs.HadoopFileSystem.<strong>init<\/strong><br>\nFile \u201cpyarrow\/error.pxi\u201d, line 141, in pyarrow.lib.pyarrow_internal_check_status<br>\nFile \u201cpyarrow\/error.pxi\u201d, line 112, in pyarrow.lib.check_status<br>\nOSError: HDFS connection failed<\/p>\n<p>can anybody help to understand this error?<br>\nthanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc push error: unexpected keyword argument",
        "Question_creation_time":1632168889736,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-unexpected-keyword-argument\/896",
        "Question_upvote_count":0.0,
        "Question_view_count":184.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I added my data with <code>dvc add mydata\/path<\/code> then i got <code>mydata.dvc<\/code><br>\nthe <code>config<\/code> file in <code>.dvc<\/code> folder is<\/p>\n<blockquote>\n<p>[core]<br>\nremote = my_remote<br>\n[cache]<br>\ntype = \u201creflink,hardlink\u201d<br>\n[\u2018remote \u201cmy_remote\u201d\u2019]<br>\nurl = s3:\/\/\u2026<\/p>\n<\/blockquote>\n<p>when i ran <code>dvc push<\/code><br>\nit raised an error<\/p>\n<blockquote>\n<p>unexpected error - <strong>init<\/strong>() got an unexpected keyword argument \u2018cache_regions\u2019<\/p>\n<\/blockquote>\n<p>can anyone help me figure out this problem?<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/main.py\u201d, line 55, in main<br>\nret = cmd.do_run()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/command\/base.py\u201d, line 45, in do_run<br>\nreturn self.run()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py\u201d, line 57, in run<br>\nprocessed_files_count = self.repo.push(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/repo\/<strong>init<\/strong>.py\u201d, line 50, in wrapper<br>\nreturn f(repo, *args, **kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\u201d, line 48, in push<br>\npushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\u201d, line 85, in push<br>\nreturn transfer(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py\u201d, line 153, in transfer<br>\nstatus = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\u201d, line 160, in compare_status<br>\ndest_exists, dest_missing = status(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\u201d, line 122, in status<br>\nexists = hashes.intersection(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\u201d, line 48, in _indexed_dir_hashes<br>\ndir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 415, in list_hashes_exists<br>\nret = list(itertools.compress(hashes, in_remote))<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/_base.py\u201d, line 611, in result_iterator<br>\nyield fs.pop().result()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/_base.py\u201d, line 432, in result<br>\nreturn self.__get_result()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/_base.py\u201d, line 388, in __get_result<br>\nraise self._exception<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/thread.py\u201d, line 57, in run<br>\nresult = self.fn(*self.args, **self.kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 406, in exists_with_progress<br>\nret = self.fs.exists(path_info)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\u201d, line 97, in exists<br>\nreturn self.fs.exists(self._with_bucket(path_info))<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/funcy\/objects.py\u201d, line 50, in <strong>get<\/strong><br>\nreturn prop.<strong>get<\/strong>(instance, type)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/funcy\/objects.py\u201d, line 28, in <strong>get<\/strong><br>\nres = instance.<strong>dict<\/strong>[self.fget.<strong>name<\/strong>] = self.fget(instance)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/fs\/s3.py\u201d, line 157, in fs<br>\nreturn _S3FileSystem(**self.fs_args)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/fsspec\/spec.py\u201d, line 75, in <strong>call<\/strong><br>\nobj = super().<strong>call<\/strong>(*args, **kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/s3fs\/core.py\u201d, line 187, in <strong>init<\/strong><br>\nself.s3 = self.connect()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/s3fs\/core.py\u201d, line 280, in connect<br>\nself.session = botocore.session.Session(**self.kwargs)<br>\nTypeError: <strong>init<\/strong>() got an unexpected keyword argument \u2018cache_regions\u2019<\/p>\n<\/blockquote>\n<h2>\n<a name=\"debug-version-info-for-developers-dvc-version-274-pip-1\" class=\"anchor\" href=\"#debug-version-info-for-developers-dvc-version-274-pip-1\"><\/a>DEBUG: Version info for developers:<br>\nDVC version: 2.7.4 (pip)<\/h2>\n<p>Platform: Python 3.8.5 on Linux-5.4.0-1056-aws-x86_64-with-glibc2.10<br>\nSupports:<br>\nhdfs (pyarrow = 4.0.1),<br>\nhttp (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),<br>\nhttps (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),<br>\ns3 (s3fs = 0.4.2, boto3 = 1.17.72)<br>\nCache types: hardlink, symlink<br>\nCache directory: ext4 on \/dev\/nvme0n1p1<br>\nCaches: local<br>\nRemotes: s3<br>\nWorkspace directory: ext4 on \/dev\/nvme0n1p1<br>\nRepo: dvc, git<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What type of protocole for huge dataset",
        "Question_creation_time":1616517263719,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-type-of-protocole-for-huge-dataset\/707",
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I want to use dvc for my dataset versioning. My dataset can go up to 10Go and I want to setup my protocole to retrieve my data.<br>\nI see that protocole to be able to retrieve data:<br>\n(<a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>)<\/p>\n<ul>\n<li>Amazon S3<\/li>\n<li>SSH<\/li>\n<li>HDFS<\/li>\n<li>Local files and directories outside the workspace<\/li>\n<\/ul>\n<p>What protocole should I use to be able to pull that quantity of data?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Add'ed files are duplicated in the cache, no links",
        "Question_creation_time":1566732869315,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/added-files-are-duplicated-in-the-cache-no-links\/209",
        "Question_upvote_count":1.0,
        "Question_view_count":598.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have started using dvc by running <code>dvc init<\/code> and then <code>dvc add<\/code> for two directories and one file.<\/p>\n<p>All worked fine, all filed have been copied to the cache but I noted that the total disk usage has doubled as the original files remained in addition to the cached copies.<\/p>\n<p>The original files do not show any signs of being reflinks.<\/p>\n<p>Is this right?<\/p>\n<p>I am using Ubuntu 16.04.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Rewriting git history and moving binaries to dvc",
        "Question_creation_time":1636163254101,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/rewriting-git-history-and-moving-binaries-to-dvc\/953",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Are there any tools or guides for rewriting a git repo containing binaries in the git history to dvc based storage?<\/p>\n<p>I can delete the existing binary files and use dvc and S3 to store them but would like to be able to rewrite the git history.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Recovering pushed files after losing .dvc reference",
        "Question_creation_time":1594065943801,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/recovering-pushed-files-after-losing-dvc-reference\/428",
        "Question_upvote_count":2.0,
        "Question_view_count":679.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Let\u2019s say I\u2019m using DVC and I:<\/p>\n<ol>\n<li><code>dvc add my_folder\/<\/code><\/li>\n<li><code>dvc push<\/code><\/li>\n<li>Delete <code>my_folder.dvc<\/code>\n<\/li>\n<li>Delete all DVC cache contents<\/li>\n<\/ol>\n<p>Without the reference provided by <code>my_folder.dvc<\/code> and no backup using <code>git<\/code>, is it still possible for me to see if <code>my_folder<\/code> has been uploaded and list it\u2019s contents? Once I have listed this folder and some of it\u2019s details, can I also download it\u2019s contents?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to remove old versions of data files?",
        "Question_creation_time":1574756258176,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-remove-old-versions-of-data-files\/256",
        "Question_upvote_count":4.0,
        "Question_view_count":900.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>My dvc repository got big, and I do not need the old versions of data files.<\/p>\n<p>Is there any way to remove them from the repository, to save the disk space?<\/p>\n<p>In fact, if there are no other options, I would remove my dvc repository and start it again.<\/p>\n<p>However, I do not want to recreate my dvc files (pipelines), keeping the existing ones.<\/p>\n<p>Is this possible?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sync S3 remote bucket auth w\/GitHub repo access settings",
        "Question_creation_time":1587768239117,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/sync-s3-remote-bucket-auth-w-github-repo-access-settings\/365",
        "Question_upvote_count":3.0,
        "Question_view_count":495.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi folks,<\/p>\n<p>My org is a happy GitHub user and we\u2019re looking to adopt DVC. Question: has anyone attempted a way to sync a GitHub repo\u2019s access settings with an S3 bucket? I\u2019d like to avoid having to replicate the same access settings used on every GitHub repo w\/its associated DVC S3 remote AWS permissions.<\/p>\n<p><strong>edit: similar to how <a href=\"https:\/\/help.github.com\/en\/github\/managing-large-files\/collaboration-with-git-large-file-storage\" rel=\"nofollow noopener\">LFS auth appears to \u201cjust work\u201d<\/a> when used w\/Gitub.<\/strong><\/p>\n<p>I\u2019m referring to the \u201cManage Access\u201d area of a GitHub repo:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a.png\" data-download-href=\"\/uploads\/short-url\/foQOK0UABlsex4GkRdJJ5CU3UT0.png?dl=1\" title=\"Manage_access\" rel=\"nofollow noopener\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_690x467.png\" alt=\"Manage_access\" data-base62-sha1=\"foQOK0UABlsex4GkRdJJ5CU3UT0\" width=\"690\" height=\"467\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_690x467.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_1035x700.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_1380x934.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Manage_access<\/span><span class=\"informations\">1504\u00d71018 70.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practices for specifying dependencies",
        "Question_creation_time":1536759423939,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-for-specifying-dependencies\/96",
        "Question_upvote_count":0.0,
        "Question_view_count":731.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>In a pipeline, if <code>folder1<\/code> is the output of <code>stage1<\/code> and used in <code>stage2<\/code>, then dvc will know about it if we run <code>dvc run -d folder1 -o folder2 python stage2.py<\/code><\/p>\n<p>Equivalently, since stage1 creates a dvc file, we could run: <code>dvc run -d stage1.dvc -o folder2 python stage2.py<\/code><\/p>\n<p>What is the recommended way ?<\/p>\n<p>Thanks !<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Merging of files in DVC-tracked directories",
        "Question_creation_time":1608643957319,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/merging-of-files-in-dvc-tracked-directories\/599",
        "Question_upvote_count":3.0,
        "Question_view_count":815.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I just tested DVC with some dummy data and have run into a situation which would be less than ideal for production. Is there an elegant way out?<\/p>\n<h1>Situation<\/h1>\n<ul>\n<li>The workspace has only one DVC-tracked directory named \u201cdata\/\u201d<\/li>\n<li>There are two git branches. In both of them, we added\/removed separate files to \u201cdata\/\u201d<\/li>\n<li>Now, we want to merge both branches<\/li>\n<\/ul>\n<h1>Expected outcome<\/h1>\n<ul>\n<li>Unless there are conflicts, a simple git merge yields the union of the file operations in both branches<\/li>\n<\/ul>\n<h1>Actual outcome<\/h1>\n<ul>\n<li>A git conflict in data.dcv. I can\u2019t really merge, but only pick the data version in one of the branches<\/li>\n<\/ul>\n<p>Given that the command \u201cdvc diff\u201d shows some very useful output, is there a way to merge both data versions semi-automatic? I have read the page <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts<\/a>, but this only mentions the \u201cappend-only\u201d strategy, not even mentioning dvc diff :(.<\/p>\n<p>P.s.: As a side question: Can \u201cdvc diff\u201d detect and highlight renamed files (since they have the same hash value)?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practice for queuing experiments on code changes",
        "Question_creation_time":1617283616396,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-queuing-experiments-on-code-changes\/710",
        "Question_upvote_count":0.0,
        "Question_view_count":365.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear community,<\/p>\n<p>Thank you very much for DVC and the new features from the version 2!<\/p>\n<p><strong>What would be your recommendation on how to queue experiments which depend on code changes?<\/strong><\/p>\n<p>The use case is to queue as individual experiment each change of the content of a Python file declared as a <code>deps<\/code> of a DVC stage.<\/p>\n<p>For example:<\/p>\n<ol>\n<li>one is on a branch,<\/li>\n<li>makes a change on a Python file,<\/li>\n<li>do <code>dvc exp run --queue<\/code> for a DVC stage depending on this Python file,<\/li>\n<li>then do another change on the same Python file,<\/li>\n<li>then do <code>dvc exp run --queue<\/code> for the same DVC stage,<\/li>\n<li>and then do <code>dvc exp run --run-all<\/code>.<\/li>\n<\/ol>\n<p>It seems that committing each change is an anti-pattern and not very usable.<\/p>\n<p>But if the change are not committed, <code>git<\/code> shows the changed file hanging around as modified and the changes corresponding to an experiment seem then not tracked.<\/p>\n<p>The idea would be to replicate what is below but when the changes are not on a <code>params.yaml<\/code> file:<\/p>\n<aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n      <a href=\"https:\/\/dvc.org\/doc\/start\/experiments#queueing-experiments\" target=\"_blank\" rel=\"noopener nofollow ugc\">dvc.org<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/start\/experiments#queueing-experiments\" target=\"_blank\" rel=\"noopener nofollow ugc\">Get Started: Experiments<\/a><\/h3>\n\n<p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Thank you.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error: Unexpected Error - Database is Locked",
        "Question_creation_time":1636415219705,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-database-is-locked\/960",
        "Question_upvote_count":0.0,
        "Question_view_count":235.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all! I\u2019m trying to install dvc in a new environments with no luck. I\u2019m probably messing something up on my end but I can\u2019t find it.<\/p>\n<p>install method: pip install dvc[all]<br>\nOS: Ubuntu 20.04<br>\nPython: Python3.9<br>\nPip: 21.3.1<\/p>\n<p>The install appears successful, however, any dvc command hangs, times out, and then returns the following:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - database is locked\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/command\/base.py\", line 59, in do_run\n    return self.run()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/command\/init.py\", line 44, in run\n    with Repo.init(\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 352, in init\n    return init(\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/init.py\", line 77, in init\n    proj = Repo(root_dir)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 214, in __init__\n    self.state = State(self.root_dir, state_db_dir, self.dvcignore)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/state.py\", line 64, in __init__\n    self.links = Cache(directory=os.path.join(tmp_dir, \"links\"), **config)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 481, in __init__\n    self.reset(key, value, update=False)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 2452, in reset\n    sql('PRAGMA %s = %s' % (pragma, value)).fetchall()\nsqlite3.OperationalError: database is locked\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dvc\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/main.py\", line 88, in main\n    dvc_info = get_dvc_info()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/info.py\", line 38, in get_dvc_info\n    with Repo() as repo:\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 214, in __init__\n    self.state = State(self.root_dir, state_db_dir, self.dvcignore)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/state.py\", line 64, in __init__\n    self.links = Cache(directory=os.path.join(tmp_dir, \"links\"), **config)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 481, in __init__\n    self.reset(key, value, update=False)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 2452, in reset\n    sql('PRAGMA %s = %s' % (pragma, value)).fetchall()\nsqlite3.OperationalError: database is locked\n<\/code><\/pre>\n<p>Could this be a permission issue with accessing one of the dvc db files?<\/p>\n<p>Any help is much appreciated as I am new to dvc.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best Practices: How to track data?",
        "Question_creation_time":1649753952914,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-how-to-track-data\/1157",
        "Question_upvote_count":1.0,
        "Question_view_count":165.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello together,<\/p>\n<p>I wounder if there are some best practices for tracking data with dvc, because there are multiple ways and I want to know the advantages and disadvantages.<\/p>\n<p>For example, having a project setup as the following, where there are just a few files in the directories, but these are really large ( &gt;=1Gb ) or having a lot of smaller files like images or mp3s.<\/p>\n<p>Large files:<\/p>\n<pre><code class=\"lang-auto\">|--- data\/\n|  |--- # raw data files\n|  |--- train.csv\n|  |--- test.csv\n<\/code><\/pre>\n<p>Many files:<\/p>\n<pre><code class=\"lang-auto\">|--- data\/\n|  |--- img_01.jpg\n|  |--- [...]\n|  |--- img_10000000.jpg\n<\/code><\/pre>\n<p>I can add the folder to dvc by adding <code>dvc add data\/<\/code>, what would create on <code>data.dvc<\/code> that tracks all files in one checksum. Or I can add the files with <code>dvc add data\/*<\/code> which would create one <code>.dvc<\/code> file per file in the folder and than I need to add every new file manually.<\/p>\n<p>Which way is recommended, what are the benefits of the different approaches? I can image tracking a lot of files with one <code>.dvc<\/code> per file can be a nightmare (having tonnes of files to track with git). I have also experienced that dvc checksum calculation can take a while if you track a whole folder.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Manually deleted .dvc files -- these files still appear to be tracked by DVC",
        "Question_creation_time":1611306516441,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-deleted-dvc-files-these-files-still-appear-to-be-tracked-by-dvc\/634",
        "Question_upvote_count":1.0,
        "Question_view_count":191.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I have manually deleted .dvc files that were previously tracked (and the tracked files themselves), expecting this to remove them from tracking (I realise now I should have used dvc gc).<\/p>\n<p>My question is how do I now remove these from tracking? I cannot re-add then remove them properly as the original files are not there.<\/p>\n<p>I am syncing data in two machines, on the machine I deleted the files on it says everything is up to date but on my other machine I am getting cache errors for the now deleted items. I am confused as I have pushed the deletes of the .dvc files so I assumed this would prevent them being tracked on both machines. Are they tracked somewhere else? I do not have a dvc.yaml file.<\/p>\n<p>Thanks,<\/p>\n<p>Justin<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Compliance - administering data that is under DVC control",
        "Question_creation_time":1556203634652,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174",
        "Question_upvote_count":8.0,
        "Question_view_count":1431.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi there<\/p>\n<p>We are currently looking to implement DVC on our stack.<\/p>\n<p>One thing that isn\u2019t clear from the available documentation, however, is how our data compliance team would go about administering and managing our data store using DVC. What kind of user access control is available to us? Or is this based on our existing Git\/Bitbucket setup?<\/p>\n<p>We\u2019ve been through the tutorials available to us and we understand how to add data to the repository, branch off, update etc, but what we\u2019re looking for clarity on is how we would centrally manage this for multiple users.<\/p>\n<p>Any advice or guidance would be appreciated.<\/p>\n<p>EDIT: To add clarity to the question, our requirement is for all of our incoming data sources to be handled by a specialist compliance team who will act as gatekeepers to the data and make it available to the data scientists within the firm as needed. The intention is for our DS team to pull data as is needed but not be able to update it without approval from the compliance team. Thanks.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Archive \/ Share a snapshot of a DVC remote",
        "Question_creation_time":1613033068998,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/archive-share-a-snapshot-of-a-dvc-remote\/664",
        "Question_upvote_count":1.0,
        "Question_view_count":303.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear DVC team,<\/p>\n<p>Thank you for the great work and coming with an agnostic approach for DVC!<\/p>\n<p>It seems that a) one could create a tar.gz of the directory used as a DVC remote and b) that someone else could unpack this directory somewhere else and use it as a local remote.<\/p>\n<p><strong>Is there any counter-argument for distributing an archive of a DVC remote?<\/strong><\/p>\n<p>I have seen that it seems fine according to <a href=\"https:\/\/discuss.dvc.org\/t\/copying-a-dvc-repository\/213\" class=\"inline-onebox\">Copying a dvc repository<\/a>. But it was in another context (filesystem specificities).<\/p>\n<p>Why archiving a DVC remote? <strong>The idea is to share publicly a snapshot of a DVC remote.<\/strong><\/p>\n<p>One way could be to <code>dvc push<\/code> to a specific public remote when someone would want to create a snapshot. However, this implies two things one might not want. First, this requires a dedicated public server. Second, this prevents from having a DOI for the snapshot.<\/p>\n<p><strong>Would you have a better suggestion than distributing an archive to share a snapshot of a DVC remote?<\/strong><\/p>\n<p>Thanks,<br>\nPierre-Alexandre<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wevdav problems",
        "Question_creation_time":1646986350383,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/wevdav-problems\/1115",
        "Question_upvote_count":0.0,
        "Question_view_count":125.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I am trying to use dvc with a local nas server and webdav. I have 2 windows machines and a nas server on a local network.<\/p>\n<p>Problem 1:<br>\nWhen pushing, for some of the files I get errors like this:<br>\n<code>ERROR: failed to transfer 'md5: &lt;...&gt;' - the server does not allow creation in the namespaceor cannot accept members: received 403 (Forbidden): Client error '403 Forbidden' for url &lt;url&gt;' For more information check: https:\/\/httpstatuses.com\/403<\/code><\/p>\n<p>Pushing a second time will push files it could not push on the first time. Maybe the server can\u2019t handle so many requests?<\/p>\n<p>Problem 2:<br>\nI can push\/pull with machine 1 but not with machine 2, although both have the same dvc configuration.<br>\nThe error is:<br>\n<code>ERROR: unexpected error - [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer  die Verbindung verweigerte: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte<\/code><\/p>\n<p>What I actually can do:<\/p>\n<ul>\n<li>ftp access with both machines (with winscp)<\/li>\n<li>webdav access with both machine (with winscp)<\/li>\n<\/ul>\n<p>Maybe it has not anything to do with dvc.<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.9.3 (exe)\n---------------------------------\nPlatform: Python 3.8.10 on Windows-10-10.0.18363-SP0\nSupports:\n        azure (adlfs = 2021.10.0, knack = 0.9.0, azure-identity = 1.7.1),\n        gdrive (pydrive2 = 1.10.0),\n        gs (gcsfs = 2021.11.0),\n        hdfs (fsspec = 2021.11.0, pyarrow = 6.0.1),\n        webhdfs (fsspec = 2021.11.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        s3 (s3fs = 2021.11.0, boto3 = 1.17.106),\n        ssh (sshfs = 2021.11.2),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.3),\n        webdavs (webdav4 = 0.9.3)\nCache types: hardlink\nCache directory: NTFS on D:\\\nCaches: local\nRemotes: azure, webdavs, local, local, local\nWorkspace directory: NTFS on D:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>Many thanks.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Importing from a git repo, then pulling",
        "Question_creation_time":1581796222088,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/importing-from-a-git-repo-then-pulling\/320",
        "Question_upvote_count":5.0,
        "Question_view_count":564.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all!<\/p>\n<p>I\u2019ve got a dvc repo which <code>dvc import<\/code>s a directory from another normal git repo. This works fine.<\/p>\n<p>However if I clone the dvc repo (or do a <code>git clean -xdf<\/code> to simulate) then run <code>dvc pull<\/code>, I can\u2019t pull the imports properly. I notice that they\u2019re not in my remote: presumably that\u2019s because dvc is getting them from the original git repo url instead which is fine.<\/p>\n<p>The error I get is:<\/p>\n<pre><code>$ dvc pull imported_steps\/step_A.dvc \nERROR: unexpected error - 'ExternalGitRepo' object has no attribute 'cache'\n<\/code><\/pre>\n<p>The associated .dvc file is<\/p>\n<pre><code>md5: 89ee29a257a80e53398e0927103f9c40\nlocked: true\ndeps:\n- path: src\n  repo:\n    url: \/path\/to\/original\/repo\/step_A\n    rev_lock: 4244a25681d4e9dcbd9795e9406c9d0734dac3f9\nouts:\n- md5: d3a51ab352355f4f39f5adf02bcd698a.dir\n  path: step_A\n  cache: true\n  metric: false\n  persist: false<\/code><\/pre>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Batch run rules",
        "Question_creation_time":1603811944490,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/batch-run-rules\/537",
        "Question_upvote_count":8.0,
        "Question_view_count":287.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello! I read tutorial and have a question about DVC possibilities. Using <code>make<\/code> one can specify a rule in <code>Makefile<\/code> which can applied to all files which names satisfy some template. Is it possible in DVC? I\u2019d like to batch process of a large number of files once and then re-process only new\/updated files<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC run and add: store command and data",
        "Question_creation_time":1534263237672,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-run-and-add-store-command-and-data\/68",
        "Question_upvote_count":2.0,
        "Question_view_count":472.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to know whether we could store run info for reproducibility and add output data to a storage that can be than pulled.<br>\nFor example, I have a raw dataset and a cleanup script<\/p>\n<pre><code class=\"lang-auto\">$ ls \nraw cleanup.py\n<\/code><\/pre>\n<p>I can run the cleanup<\/p>\n<pre><code class=\"lang-auto\">$ dvc run -d raw -o clean python cleanup.py raw clean\n$ cat clean.dvc\ncmd: python cleanup.py\ndeps:\n- md5: xxxx\n  path: raw\nmd5: yyyyy\nouts:\n- cache: true\n  md5: zzzz\n  path: clean\n<\/code><\/pre>\n<p>and I observe how clean folder is produced. However if I add clean folder with dvc in order to share it the information on how to produce clean folder is modifed<\/p>\n<pre><code class=\"lang-auto\">$ dvc add clean\n$ cat clean.dvc\nmd5: xxxx\nouts:\n- cache: true\n   md5: xxxxx\n<\/code><\/pre>\n<p>So, can we have both features : stored command on how dataset can be produced and is stored in the cache ?<\/p>\n<p>Thanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to continually update a model with new data",
        "Question_creation_time":1644516069762,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-continually-update-a-model-with-new-data\/1055",
        "Question_upvote_count":0.0,
        "Question_view_count":141.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a reinforcement learning use case with the following initial steps:<\/p>\n<ol>\n<li>Construct initial episodes<\/li>\n<li>Add episodes to initial experience replay buffer<\/li>\n<li>Train initial model<\/li>\n<\/ol>\n<p>Then the following steps which repeat every day:<\/p>\n<ol start=\"4\">\n<li>Construct most recent time steps<\/li>\n<li>Add new time steps to existing experience replay buffer<\/li>\n<li>Fine-tune existing model<\/li>\n<li>Compare existing model and fine-tuned model and keep the one that\u2019s best<\/li>\n<\/ol>\n<p>What would be the best way to do this using DVC? Is it even possible given that circular dependencies are not allowed?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Updating files within a tracked directory",
        "Question_creation_time":1649255177076,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/updating-files-within-a-tracked-directory\/1145",
        "Question_upvote_count":0.0,
        "Question_view_count":98.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have a large directory tracked by dvc with a single .dvc file. Often, I\u2019ll will want to just pull a single file within that directory, make some modifications, and then push those changes back up to the remote repo. However, if I just do <code>dvc add my_directory\/<\/code>, then it will think I want to <em>delete<\/em> all the other files. Do I have to pull down the entire directory in order to modify a single file?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Removal of partial pull's",
        "Question_creation_time":1659111282606,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/removal-of-partial-pulls\/1277",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Dear DVC community,<\/p>\n<p>I am rather new to DVC. I am interested to use DVC in a use case that is maybe related to what is called \u201cData Registrty\u201d, but it is not entirely the same.<\/p>\n<p>I do have a repository that stores in a structured way folders with outputs from expensive computational runs with many files and high storage volume. I added the individual folders as one object, i.e. have one .dvc file per folder.<\/p>\n<p>Example:<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_d.dvc\n<\/code><\/pre>\n<p>A user of the repository would first clone it and, thanks to your partial pull feature, only pull those directories from the remote that are necessary for the next steps in the data analysis pipeline. Thereby the huge repository, for which a full pull would not fit on standard storage, can still be properly used.<\/p>\n<p>Example (cont.):<br>\n(result after \u201cdvc pull folder_a.dvc folder_c.dvc\u201d)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_a\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>Let\u2019s assume now that the user stopped to use one of those folders (say \u201cfolder_a\u201d) after having pushed its updates. Is there an obvious way to remove that partial pull (here \u201cfolder_a\u201d) from the local working copy, without affecting the remote? (This would be done for keeping the local storage requirements on a moderate level.)<\/p>\n<p>Example (cont.):<br>\n(result after applying the searched for operation)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>I do not ask for removing data from the remote but rather a proper way to remove the folder (not it\u2019s .dvc file) and all remaining data in the local cache. I assume here that a simple removal  (\u201crm -fR folder_a\u201d) of the folder would not be enough\u2026<\/p>\n<p>Then, the simplest solution would of course be to delete the local working directory and just clone a new one, where one would start from scratch with a partial pull. However, that might become a bit unhandy over time.<\/p>\n<p>So is there any simple way to do this by a kind of \u201cunpull\u201d operation that I overlooked?<\/p>\n<p>If you need further details, please let me know.<\/p>\n<p>Thanks a lot in advance!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ERROR: unexpected error - OpenSSH private key encryption requires bcrypt with KDF support",
        "Question_creation_time":1646237971898,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-openssh-private-key-encryption-requires-bcrypt-with-kdf-support\/1085",
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m using DVC for several weeks, but I\u2019m now trying to use a ssh remote for the first time.<br>\nI configured the ssh remote as indicated in the documentation and each time I\u2019m trying to pull or push on the remote storage I get this error:<\/p>\n<p>\u201cERROR: unexpected error - OpenSSH private key encryption requires bcrypt with KDF support\u201d<\/p>\n<p>I have this error even when I\u2019m providing my ssh password in the dvc config file, or when I\u2019m asking the password with the \u201cask_password true\u201d option.<\/p>\n<p>My ssh key is using a passphrase and was generated with ssh-keygen (no special CLI arguments).<br>\nI\u2019m not sure if ssh-keygen is generating by default a key with bcrypt and KDF support (but ideally I would prefer to use my ssh password and not an ssh private key).<\/p>\n<p>Am I doing something wrong here?<br>\nIf someone has some insights about this issue, I would be very glad to know how to solve this issue.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Integrate DVC to an existing github repo with S3",
        "Question_creation_time":1634478544631,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/integrate-dvc-to-an-existing-github-repo-with-s3\/922",
        "Question_upvote_count":1.0,
        "Question_view_count":329.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>We are considering to start using DVC and integrate it to our current git repo, and I have some questions about what is the best practice to integrate DVC once you already have a flow you work with.<\/p>\n<p>Our current data workflow, before using DVC, is as follows:<\/p>\n<ol>\n<li>All is the is stored in a bucket in s3 under a specific folder (we can call it the \u201cresources folder\u201d). This folder is in the folder tree of the git repo but the whole resources folder is specified in the gitignore file of the git repo.<\/li>\n<li>We have a configuration yaml file in which we specify which data in the resources folder on S3 we want to use.<\/li>\n<li>The configuration yaml file is connected to the code we use to train the model.<\/li>\n<li>Once the training process starts the code first checks if data specified in the configuration yaml file is already present locally or not, and if the file is missing then the code gets the data from the resources folder in the S3 bucket to the local computer.<\/li>\n<li>model is being trained.<\/li>\n<li>Output model files (e.g.,weights file) created when the model training is finished are saved locally and then uploaded (using the code, not manually) to the resources folder in S3.<\/li>\n<\/ol>\n<ul>\n<li>Locally means it is either the local computer (if the amount of data is small) or something like an EC2 (if the amount of data is too large to train on the local computer).<\/li>\n<\/ul>\n<p><strong>My questions are:<\/strong><br>\na. We will need to exclude the whole content of the resources folder from the gitignore\u05e5<br>\nb. Then add one file at a time to dvc tracking using <code>dvc add<\/code> ?<br>\nc. Followed by adding the dvc file created from the <code>dvc add<\/code> and the raw file to gitignore, add them to git and commit.<br>\nd. Do <code>dvc push<\/code> to remote storage (that will also be in s3, but in a different bucket).<br>\ne. Once we register all the files we will no longer need the files in the resources folder from which we started right?<\/p>\n<p>Any advices on how to best integrate dvc to the current workflow and other best practices will be greatly appreciated.<\/p>\n<p>Thank you,<br>\nAyala<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to handle general metadata without experiments?",
        "Question_creation_time":1613669179072,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-handle-general-metadata-without-experiments\/680",
        "Question_upvote_count":1.0,
        "Question_view_count":325.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>Thanks for this amazing package <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> !<\/p>\n<p>I am using DVC to store some models but the way I am training my models does not allow me to easily use the pipelines. And it does not necessarily make sense in my case. Nevertheless, I have some metadata I would like to keep as well as some metrics.<\/p>\n<p>So my question is how should I do this? Because reading the doc and looking at some posts form here, I should use the dvc.yaml. But <code>cmd<\/code> is not optional and does not really make sense in my case. So right now I could just have a fake <code>cmd<\/code> but this does not sounds right.<\/p>\n<p>Thanks!<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Best practice for applying pipelines to many datasets?",
        "Question_creation_time":1649258414789,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-applying-pipelines-to-many-datasets\/1147",
        "Question_upvote_count":1.0,
        "Question_view_count":113.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi folks,<\/p>\n<p>I have a data analysis problem which I want to use DVC to help me with. I have a series of many different datasets which each need to have an analysis stage applied to them. Each dataset is a folder with a specific set of (large) files, and the same python analysis code needs to be applied to each one.<\/p>\n<p>Rather than just run one monolithic script that loops over all the datasets, I would like to create a pipeline to help me handle this. I am imagining some sort of pipeline where the script gets applied to each dataset separately (and possibly in parallel).<\/p>\n<p>Is something like this even possible with dvc?<\/p>\n<p>Thanks,<\/p>\n<p>Steve<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"CML runner specify ec2 ami id",
        "Question_creation_time":1623680757727,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-runner-specify-ec2-ami-id\/792",
        "Question_upvote_count":1.0,
        "Question_view_count":217.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I am reading through the CML documentation and is anyone aware of a was to specify the AMI id of the ec2 instance in the cml runner command?<\/p>\n<p>thanks,<br>\nKunal.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC and data lake",
        "Question_creation_time":1568116726435,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-data-lake\/215",
        "Question_upvote_count":0.0,
        "Question_view_count":811.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey guys, quick question.<\/p>\n<p>Following the ML flow best practice, we must store our raw data in one data lake structure, right?<\/p>\n<p>DVC can use Google Storage\/S3 and a lot of other storage support to store our code\/dataset and models in one common place.<\/p>\n<p>Is it correct to say than I have one \u201cData Lake\u201d structure just using DVC?<\/p>\n<p>I\u2019m using DVC to store my dataset and models, and now I would like to know if is necessary to implement one data lake architecture on top of that.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"DVC support for the local storage",
        "Question_creation_time":1534489981766,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-support-for-the-local-storage\/71",
        "Question_upvote_count":0.0,
        "Question_view_count":1833.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>HI<br>\nCan we add local storage for DVC. like (i dont want to store it on s3 or gcp, need  to only point to local storage)<br>\nex :<\/p>\n<ol>\n<li>dvc add file:\\\\ xxx.xx.xx.x\\images\\annex\\dvc-storage<br>\nor<\/li>\n<li>dvc add X:\/annex\/dvc-storage\/data.xml ( local storage)<br>\nAfter trying above option. i am getting error.<br>\nInitialization error: Config file error: Unsupported URL.<br>\nPlease provide an appropriate solution or syntax<\/li>\n<\/ol>\n<p>Note : storage is tyron.   the storage location is mounted to window or on linux.<\/p>\n<p>With ref : <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-fit-in-a-local-area-network-infrastucture-where-git-repos-are-not-in-the-computing-server\/24\/3\" class=\"inline-onebox\">Does DVC fit in a Local Area Network infrastucture where git repos are not in the computing server?<\/a><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple data series plot",
        "Question_creation_time":1663248655019,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-data-series-plot\/1344",
        "Question_upvote_count":2.0,
        "Question_view_count":54.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hey,<br>\nI\u2019m trying to plot multiple lines in the same plot using example showed here:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/show\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/show\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/388;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"388\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/show\" target=\"_blank\" rel=\"noopener nofollow ugc\">plots show<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">plots:\n  - test_vs_train_loss:\n      x: epoch\n      y:\n        training_data.csv: [test_loss, train_loss]\n      title: Compare loss training versus test\n<\/code><\/pre>\n<p>expected str, in stages \u2192 train_model \u2192 plots \u2192 0 \u2192 test_vs_train_loss \u2192 y, line 56, column 13<br>\n55 \u2502   \u2502     y:<br>\n56 \u2502   \u2502   \u2502   training_data.csv: [test_loss, train_loss]<\/p>\n<p>The example doesn\u2019t seem to work. Array object seems not be supported? What am I missing?<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Refactor existing project from single root .dvc to subprojects (dvc init --subdir)",
        "Question_creation_time":1634692636711,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/refactor-existing-project-from-single-root-dvc-to-subprojects-dvc-init-subdir\/928",
        "Question_upvote_count":0.0,
        "Question_view_count":173.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have an existing project that was initialized at the git root with a single .dvc directory.<\/p>\n<p>Are there going to be any gotchas or unexpected behavior if I want to refactor this by running \u201cdvc init --subdir\u201d in a couple of subdirectories to create subprojects with their own cache directories and remotes?  If I understand things correctly, I can:<\/p>\n<ol>\n<li>Update my cache and ensure no one will push changes to remote while the steps below are done<\/li>\n<li>Run <code>dvc init --subdir<\/code> in a subdirectory that I want to turn into a subproject<\/li>\n<li>Set cache dir to the same as the root project, set the remote to the new remote for this subproject<\/li>\n<li>Run <code>dvc push<\/code> to copy current versions of all files in the subproject to the new remote<\/li>\n<li>Change subproject cache dir to its new path<\/li>\n<li>git commit and push<\/li>\n<\/ol>\n<p>Anything I\u2019m missing?<\/p>\n<p>One confusing issue will be that dvc commands will have a different scope depending whether a developer is on a commit upstream or downstream from this change.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using DVC without Cache (file references only)",
        "Question_creation_time":1614671673780,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-without-cache-file-references-only\/694",
        "Question_upvote_count":1.0,
        "Question_view_count":718.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We have a HDFS data directory, which is only growing, so no files are deleted or edited. And we use these files for some machine learning stuff. Analysis A uses files for time period [1-a] and Analysis B uses files of time period [1-b] (see example below).<\/p>\n<p>Since our (HDFS) data directory is only increasing, we wonder if it is possible to use DVC for tracking a \u201cfile list\u201d or \u201cfile reference\u201d only, instead of copying the current directory content into some cache.<\/p>\n<p>For our case, something like a pointer would be enough and no cache directory for tracking deleted\/edited files is needed.<\/p>\n<p>For example:<\/p>\n<pre><code>Data File of Day 1\nData File of Day 2  ---&gt; Run Analysis A (DVC must hold a reference to files of day 1 to 2)\nData File of Day 3\nData File of Day 4\nData File of Day 5 ---&gt; Run Analysis B (DVC must hold a reference to files of day 1 to 5)\nData File of Day 6 ---&gt; Run Analysis C (DVC must hold a reference to files of day 1 to 6)\nData File of Day 7\n...\n\n=&gt; No cache needed, since data is only growing and not deleted or edited. \n=&gt; Every Analysis uses data from time period starting at day 1 to current day.\n<\/code><\/pre>\n<p>Is there a way to realize something with DVC? E.g. when I want to re-run analysis A, DVC knows which files to download from out HDFS data directory, without using a separate cache directory?<\/p>\n<p>Thank you for your help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can my workspace be an s3 location?",
        "Question_creation_time":1646192567568,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-my-workspace-be-an-s3-location\/1083",
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I would like to run <code>dvc pull<\/code> directly into an s3 bucket. Is that possible?<\/p>\n<p>Long story short: I would like end users to version control their csv files using dvc. I then would like to have a recurring process that will do a dvc pull, of the main branch, directly into an s3 bucket. I understand that dvc can use s3 for storage, but I would like to re-create the data in s3, in an identical structure as the dvc git repository.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can the command `dvc add` just add new and changed files?",
        "Question_creation_time":1575200338304,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-the-command-dvc-add-just-add-new-and-changed-files\/262",
        "Question_upvote_count":5.0,
        "Question_view_count":937.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have just started playing with dvc to investigate whether it can help me adding version control to large sets of images used for deep learning.<\/p>\n<p>I made my own <em>small<\/em> toy example to try to understand how dvc works.<\/p>\n<p>I have a data folder with 10 images of approx 10 MB each, in total ~100 MB<\/p>\n<p>When I run \u201cdvc add data\u201d I can see that the dvc cache grows with ~100 MB<\/p>\n<p>I now replace one of the images in my data folder with another one, i.e. remove one 10 MB image and add another 10 MB image. The total size is still 100 MB<\/p>\n<p>When I run \u201cdvc add data\u201d again to track the changes, the dvc cache grows by another 100 MB! I was thinking that dvc would automatically add the difference between the data sets, but that does not seems to be the case? Does it <em>copy<\/em> all the content for each \u201cdvc add\u201d command?<\/p>\n<p>If yes, is there any practically convenient way to make dvc detect and store the <em>difference<\/em> between versions of data sets?<\/p>\n<p>Thanks,<br>\nSven<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Right way to provide optional parameters to script in experiments",
        "Question_creation_time":1622567503315,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/right-way-to-provide-optional-parameters-to-script-in-experiments\/773",
        "Question_upvote_count":2.0,
        "Question_view_count":528.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi! What is a supposed way to deal with optional params for scripts in dvc.yaml?<br>\nLet\u2019s suppose we have a script which could be run like <code>python train.py<\/code> or <code>python train.py --resume path-to-model-weights<\/code>.<\/p>\n<p>I can come up to something like this:<\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nstages:\n  train:\n    deps:\n      - train.py\n    cmd: python train.py ${resume}\n<\/code><\/pre>\n<pre><code class=\"lang-auto\"># params.yaml\nresume: \"\"\n<\/code><\/pre>\n<p>and in case I want to run an experiment and resume training, use <code>dvc exp run -S resume=\"--resume path-to-model-weights\"<\/code><\/p>\n<p>But maybe I\u2019ve missed more elegant solution? Something that will allow <code>dvc exp run -S resume=path-to-model-weights<\/code>.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Python api with remote repo and local cache",
        "Question_creation_time":1611678288864,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/python-api-with-remote-repo-and-local-cache\/637",
        "Question_upvote_count":1.0,
        "Question_view_count":430.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nI am using the Python api to pull a file from remote s3. The call looks like this:<\/p>\n<blockquote>\n<p>with dvc.api.open(<br>\n\u2018&lt;path_to_file&gt;\u2019,<br>\nrepo=\u2018git@github.com:&lt;my_remote_github_repo&gt;\u2019,<br>\nrev= \u2018&lt;commit_reference&gt;\u2019<br>\n) as fd:<br>\ndf = pd.read_csv(fd)<\/p>\n<\/blockquote>\n<p>This works successfully to download the file; however, I\u2019m trying to achieve an increase in speed. I would like the api to download the file from s3 if it doesn\u2019t already exist in my local cache. If it\u2019s in my local cache, then I would like to get the file from there since it would be much faster.<\/p>\n<p>I realize that I could dvc pull the file from the command line and change my api to my local repo; however, I\u2019m trying to keep the code \u2018path\u2019 agnostic so that other users (who share the data repo) can run this script without having to modify paths in the code.<\/p>\n<p>I\u2019ve looked through the forum and the api documentation, but haven\u2019t been able to find a solution. Is this something that is possible, or perhaps a potential feature in the future? It would really speed up my workflow.<\/p>\n<p>Thanks,<br>\nMike<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Check if files are in same versions in local and remote without doing dvc pull",
        "Question_creation_time":1644317539862,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/check-if-files-are-in-same-versions-in-local-and-remote-without-doing-dvc-pull\/1045",
        "Question_upvote_count":0.0,
        "Question_view_count":159.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to kwow if there is a way to know if files are not up to date on my github repo (with the .dvc files) compare to dvc storage without doing a dvc pull and dvc repro. In other words, how can I check if nobody forget to do a dvc push before git push.<\/p>\n<p>I can\u2019t use dvc pull to check that because the check is doing in a github action and files are too large to be pull on it. And I don\u2019t want to depends on some hooks because it implies to be sure that everybody install it.<\/p>\n<p>I have the intuition that it coulb be possible to check the md5 stock in .dvc files are the same that in remote without pulling them but I don\u2019t find the solution.<\/p>\n<p>Thank you in advance for your help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dvc pull --glob",
        "Question_creation_time":1634219444762,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-pull-glob\/917",
        "Question_upvote_count":2.0,
        "Question_view_count":671.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi, I want to share just some of my files on a remote. For that I used <code>dvc push  --glob data\/**\/*.txt data <\/code>, which seemed to work. But how can someone download and checkout only this subset? All other data files are stored just locally and were never pushed. The command <code>dvc pull --glob data\/**\/*.txt<\/code> throws a lot of errors, because dvc tries to download files from the remote that I never uploaded.<br>\nThanks<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error: Newline or carriage return character detected in HTTP status message or header",
        "Question_creation_time":1657195873640,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-newline-or-carriage-return-character-detected-in-http-status-message-or-header\/1240",
        "Question_upvote_count":1.0,
        "Question_view_count":224.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello<\/p>\n<p>I get the following error when pulling from dvc. Any ideas where such problems come from or how to debug my configuration?<\/p>\n<blockquote>\n<p>dvc pull<\/p>\n<\/blockquote>\n<blockquote>\n<p>ERROR: unexpected error - An HTTP Client raised an unhandled exception: Newline or carriage return character detected in HTTP status message or header. This is a potential security issue.<\/p>\n<\/blockquote>\n<p>Here the system info<\/p>\n<blockquote>\n<h2>DVC version: 2.10.0 (pip)<\/h2>\n<p>Platform: Python 3.9.5 on Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.27<br>\nSupports:<br>\nwebhdfs (fsspec = 2022.5.0),<br>\nhttp (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),<br>\nhttps (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),<br>\ns3 (s3fs = 2022.5.0, boto3 = 1.21.21)<br>\nCache types: <a href=\"https:\/\/error.dvc.org\/no-dvc-cache\" rel=\"noopener nofollow ugc\">https:\/\/error.dvc.org\/no-dvc-cache<\/a><br>\nCaches: local<br>\nRemotes: s3<br>\nWorkspace directory: ext4 on \/dev\/sdb<br>\nRepo: dvc, git<\/p>\n<\/blockquote>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple params.yaml",
        "Question_creation_time":1618304562199,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-params-yaml\/720",
        "Question_upvote_count":1.0,
        "Question_view_count":861.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Is it possible to have more than one params.yaml? If so, how should they be organised and named?<\/p>\n<p>The context is I am considering of breaking up my dvc.yaml to multiple pipelines and would like to do the same for the params as well. Is the answer to save a params.yaml at the same place where the pipeline for dvc.yaml will live? something like pipelines\/pip1\/dvc.yaml,params.yaml and pipelines\/pip2\/dvc.yaml,params.yaml<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Single cache or multiple caches in NAS with External Data",
        "Question_creation_time":1648633274669,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/single-cache-or-multiple-caches-in-nas-with-external-data\/1136",
        "Question_upvote_count":2.0,
        "Question_view_count":421.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>We have a large NAS with large datasets both in number of files and file size. Those data sets need to be used by several gitlab projects (data wrangling, data processing, modelling, making plots, etc).<\/p>\n<p>We want to have some kind of version control of the data (mainly, say \u201cthis experiment was run with these data\u201d and check that it hasn\u2019t changed), so I have made some simple tests with DVC, but I\u2019d like the opinion of people with more insight. My main question is whether I should set up a single shared cache for all projects, or independent caches for each project (and whether my proposed solution is sound at all). A secondary question is whether the setup will allow to run experiments on the cloud down the line. Here are the details of the problem and my attempted solutions:<\/p>\n<p>Main constraints:<\/p>\n<ul>\n<li>we cannot migrate the data to a flat directory structure where files get renamed with their hash, so the data has to co-exist with the cache. That is, we need to maintain the original directory structure and filenames in the NAS as they are<\/li>\n<li>the NAS data needs to be accessed and processed by several people from several machines (the NAS is an ext2 filesystem that gets mounted with NFS on those machines), typically by cloning the gitlab project to their own \/home\/john_doe\/Software\/my_cats_projects local directory.<\/li>\n<li>we cannot have duplicates of the data, e.g. the one on the NAS and then on each machine that needs to process it<\/li>\n<\/ul>\n<p>The solution I have come up with, and that seems to work is to combine DVC\u2019s \u201cManaging External Data\u201d (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>) and \u201cLarge Dataset Optimization\u201d (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>).<\/p>\n<p>We can have gitlab projects with a softlink to the external data in the NAS, e.g.<\/p>\n<p>my_cats_project<br>\n\u251c\u2500\u2500 bin<br>\n\u2502       \u2514\u2500\u2500 process_some_data.sh<br>\n\u251c\u2500\u2500 data<br>\n\u2502       \u2514\u2500\u2500 some_small_local_data.csv<br>\n\u251c\u2500\u2500 external_data \u2192 \/nas_server\/laboratory_1\/big_dataset_with_cats\/<br>\n|               \u251c\u2500\u2500  cat_movie_001.mov<br>\n|               \u2514\u2500\u2500  cat_movie_002.mov<br>\n\u251c\u2500\u2500 README.md<br>\n\u2514\u2500\u2500 src<br>\n\u2514\u2500\u2500 python_code.py<\/p>\n<p>We initialise dvc in the gitlab project<\/p>\n<p>$ cd my_data_project<br>\n$ dvc init<\/p>\n<p>We then tell dvc to use an external cache on the NAS. This is the main part of my question, whether it\u2019s better to use a single external cache living on the NAS for all datasets and projects,<\/p>\n<p>$ dvc cache dir \/nas_server\/common_dvc_cache<\/p>\n<p>or have separate caches for each project,<\/p>\n<p>$ dvc cache dir \/nas_server\/projects_dvc_cache\/my_cats_project_dvc_cache<\/p>\n<p>Either way, the cache or caches will live on the NAS, avoiding transferring large data files to \/home directories.<\/p>\n<p>To avoid data duplication, we configure the cache to use hardlinks (as reflinks are not available on ext2)<\/p>\n<p>$ dvc config cache.type hardlink<\/p>\n<p>This way, the original data files (cat_movie_001.mov) and their cache \u201ccopies\u201d (asd9890w908ad9sfasd9fasdf980asfd) will point to the same inode.<\/p>\n<p>In this set up, data cannot be overwritten or updates. For example, if we want new versions of the cat movies above, we\u2019ll have to create new directories for those, but that\u2019s fine in this case.<\/p>\n<p>DVC will make the original files non-writable to protect the cache from being corrupted. But we can still move the original data files on the NAS using \u201cdvc move\u201d, right?<\/p>\n<p>Finally, I\u2019d be grateful for any insights on whether this setup would allow to then run projects on cloud service providers supported by DVC.<\/p>\n<p>Thanks a lot!<\/p>\n<p>Ramon.<\/p>",
        "Tool":"DVC",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Notebook copying to html error",
        "Question_creation_time":1659633919425,
        "Question_link":"https:\/\/my.guild.ai\/t\/notebook-copying-to-html-error\/906",
        "Question_upvote_count":0.0,
        "Question_view_count":187.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I did manage to get Guild to run my notebook, but I did get a couple warnings and 1 error:<\/p>\n<h2>\n<a name=\"h-2-warnings-1\" class=\"anchor\" href=\"#h-2-warnings-1\"><\/a>2 Warnings<\/h2>\n<p>1). \u201cWARNING: Skipping potential source code file .\/tcn_kerasAPI_gpu_data_generator.ipynb because it\u2019s too big. To control which files are copied, define \u2018sourcecode\u2019 for the operation in a Guild file.\u201d<\/p>\n<p>I\u2019m just wondering why I was even seeing that warning when my actual run command was:<\/p>\n<p>guild run tcn_kerasAPI_gpu_guild_ai_data_generator.ipynb<\/p>\n<p>So you see I wasn\u2019t even running the notebook that it says is \u201ctoo big\u201d.  That\u2019s a different notebook file entirely.  Is there some reason Guild would be \u201ccopying over\u201d a notebook file other than the one I\u2019m running?<\/p>\n<p>2). I\u2019m not sure why I got this warning:<\/p>\n<p>[NbConvertApp] WARNING | Config option <code>kernel_spec_manager_class<\/code> not recognized by <code>NbConvertApp<\/code>.<\/p>\n<p>===================<\/p>\n<h2>\n<a name=\"h-1-error-2\" class=\"anchor\" href=\"#h-1-error-2\"><\/a>1 Error<\/h2>\n<p>Tho\u2019 I did manage to get a successful run and saw expected output images (in Tensorboard) for my notebook, at the end of the run, when Guild was trying to convert the notebook to html via \u201cnbconvert\u201d, I did see this error:<\/p>\n<p>\u201cAttributeError: module \u2018jinja2\u2019 has no attribute \u2018Markup\u2019\u201d<\/p>\n<p>I guess it\u2019s not critical, but of course it\u2019s nicer for a run not to end on an error of course <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"> :).<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Run R script returns: guild: error running r: [WinError 193] %1 is not a valid Win32 application",
        "Question_creation_time":1606477388142,
        "Question_link":"https:\/\/my.guild.ai\/t\/run-r-script-returns-guild-error-running-r-winerror-193-1-is-not-a-valid-win32-application\/467",
        "Question_upvote_count":0.0,
        "Question_view_count":539.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I tried to run R script. I have guild.yml file inside my root directory (R project). Ia m not sure if guild can work for plain R scripts inside R project?<\/p>\n<p>Here is my guild.yml:<\/p>\n<pre><code>r:\n  description: Backtesting with BackCUSUM estimation of volatility structural breaks\n  # exec: Rscript .guild\/sourcecode\/train.r ${flag_args}\n  exec: C:\/Users\/Mislav\/Documents\/GitHub\/alphar\/R\/volatilityR.R ${flag_args}\n  flags:\n    contract: 'SPY5'\n    upsample: FALSE\n    std_window: 30\n    backcusum_rolling_window: 100\n    backcusum_type: 'bq'\n  output-scalars:\n    cumulatice_return: 'cumulative_return: (\\value)'\n    sharpe_ratio: 'sharpe_ratio: (\\value)'\n<\/code><\/pre>\n<p>When I execute the script with <code>guild run r<\/code> I get the error:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\alphar&gt; guild run r\nYou are about to run r\n  backcusum_rolling_window: 100\n  backcusum_type: bq\n  contract: SPY5\n  std_window: 30\n  upsample: no\nContinue? (Y\/n) y\nguild: error running r: [WinError 193] %1 is not a valid Win32 application\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild view not working in jupyterhub",
        "Question_creation_time":1664820465043,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-view-not-working-in-jupyterhub\/922",
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I am using GuildAI in a Jupyter Lab server provisioned in Jupyterhub. To view the UI, I run the command <code>guild view --host=localhost --port=5000<\/code> but am getting the error \u201cFailed to load resource: the server responded with a status of 404 (Not Found)\u201d. On the other hand, <code>guild tensorboard --host=localhost --port=5000<\/code> works fine for me. I\u2019m not sure what\u2019s the issue or how to debug it.<\/p>\n<p>Packages:<br>\nguildai==0.8.1<br>\njupyterhub==1.5.0<br>\njupyterlab==3.4.7<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Local run vs remote run dependencies",
        "Question_creation_time":1645549217715,
        "Question_link":"https:\/\/my.guild.ai\/t\/local-run-vs-remote-run-dependencies\/812",
        "Question_upvote_count":1.0,
        "Question_view_count":161.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have run some operations on remotes successfully in the past. However, there was always a some discrepancy between the imports for local and remote runs that I needed to fix by trial and error.<\/p>\n<p>In my current setup, I switched from flags as global variables in the training script to flags in config.yml files. And I\u2019m unable to make it work on remotes.<\/p>\n<p><strong>Project structure<\/strong><br>\nProject:<\/p>\n<ul>\n<li>[some folders]<\/li>\n<li>datasets \u2192 module, contains data loaders + their config.yml files<\/li>\n<li>zoo \u2192 guild Home for local runs<\/li>\n<li>models \u2192 model definitions<\/li>\n<li>\n<ul>\n<li>guild.yml<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>abstract_model.py<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>conv_lstm \u2192 model I want to run<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>model.py \u2192 model definition<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>train.py \u2192 training script<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>config.yml \u2192 flags<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><strong>Guild file<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># Standard convolutional LSTM\n- model: conv_lstm\n  description: Convolutional LSTM\n  operations:\n    train_local:\n      description: Train Convolutional LSTM\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        epochs: 100\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n    train_remote:\n      description: Train Convolutional LSTM on remote\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        optimizer: Adam\n        loss: mse\n        learning_rate: 0.001\n        epochs: 100\n        dev: True\n        gpus: [7]\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n            train_path: ~\/data\/ucsd\/UCSDped1\/Train\/\n            test_path: ~\/data\/ucsd\/UCSDped1\/Test\/\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n<\/code><\/pre>\n<p><strong>Training script<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">sys.path.append('..\/')\nsys.path.append('..\/..\/datasets')\n# Tensorflow logging level\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport tensorflow as tf\nimport yaml\nfrom model import ConvLSTM\nfrom datasets.data_loader import DataLoader\n\n\n# Load the model configuration\nclass Config(object):\n    def __init__(self, filename):\n        self.__dict__.update(yaml.safe_load(open(filename)))\n\n\nconfig = Config(\"config.yml\")\n\n(...)\n<\/code><\/pre>\n<p><strong>Current situation &amp; error<\/strong><br>\nI\u2019m able to run \u2018conv_lstm:train_local\u2019 without any issues, and everything works as expected. However, almost the same configuration, with a few flags changed, fails to run on remote.<\/p>\n<p>Issue 1: I cannot see any evidence of the config.yml file being copied to the remote<br>\nIssue 2: the remote run fails to find the main training script, even though it works locally.<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild -H \/home\/bleporowski\/Projects\/mad\/zoo run conv_lstm:train_remote --remote [remote_name] --gpus 7\nYou are about to run conv_lstm:train_remote as a batch (1 trial) on [remote_name]\n  dataset_args: [{batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}]\n  dev: yes\n  epochs: 100\n  gpus: [7]\n  learning_rate: 0.001\n  loss: mse\n  optimizer: Adam\nContinue? (Y\/n) y\nBuilding package\npackage src: \/home\/bleporowski\/Projects\/mad\/models\npackage dist: \/tmp\/guild-remote-stage-eq7ahi7e\nrunning clean\nremoving 'build\/lib' (and everything under it)\nremoving 'build\/bdist.linux-x86_64' (and everything under it)\n'build\/scripts-3.8' does not exist -- can't clean it\nremoving 'build'\nrunning bdist_wheel\nrunning build\nrunning build_py\npackage init file '\/home\/bleporowski\/Projects\/mad\/models\/__init__.py' not found (or not a regular file)\ncreating build\ncreating build\/lib\ncreating build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/abstract_model.py -&gt; build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/guild.yml -&gt; build\/lib\/conv_lstm\ninstalling to build\/bdist.linux-x86_64\/wheel\nrunning install\nrunning install_lib\ncreating build\/bdist.linux-x86_64\ncreating build\/bdist.linux-x86_64\/wheel\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/guild.yml -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/abstract_model.py -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\nrunning install_egg_info\nrunning egg_info\nwriting conv_lstm.egg-info\/PKG-INFO\nwriting dependency_links to conv_lstm.egg-info\/dependency_links.txt\nwriting entry points to conv_lstm.egg-info\/entry_points.txt\nwriting namespace_packages to conv_lstm.egg-info\/namespace_packages.txt\nwriting top-level names to conv_lstm.egg-info\/top_level.txt\nreading manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nwriting manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nCopying conv_lstm.egg-info to build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0-py3.8.egg-info\nrunning install_scripts\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0.dist-info\/WHEEL\ncreating '\/tmp\/guild-remote-stage-eq7ahi7e\/conv_lstm-0.0.0-py2.py3-none-any.whl' and adding 'build\/bdist.linux-x86_64\/wheel' to it\nadding 'conv_lstm\/abstract_model.py'\nadding 'conv_lstm\/guild.yml'\nadding 'conv_lstm-0.0.0.dist-info\/METADATA'\nadding 'conv_lstm-0.0.0.dist-info\/PACKAGE'\nadding 'conv_lstm-0.0.0.dist-info\/WHEEL'\nadding 'conv_lstm-0.0.0.dist-info\/entry_points.txt'\nadding 'conv_lstm-0.0.0.dist-info\/namespace_packages.txt'\nadding 'conv_lstm-0.0.0.dist-info\/top_level.txt'\nadding 'conv_lstm-0.0.0.dist-info\/RECORD'\nremoving build\/bdist.linux-x86_64\/wheel\nInitializing remote run\nCopying package\nsending incremental file list\nconv_lstm-0.0.0-py2.py3-none-any.whl\n\nsent 3,558 bytes  received 35 bytes  1,437.20 bytes\/sec\ntotal size is 3,424  speedup is 0.95\nInstalling package and its dependencies\nProcessing .\/conv_lstm-0.0.0-py2.py3-none-any.whl\nInstalling collected packages: conv-lstm\nSuccessfully installed conv-lstm-0.0.0\nStarting conv_lstm:train_remote on charybdis as 8a26ca399039412fb31c7791d293b507\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nINFO: [guild] Running trial 05afef0858f74b4198af160c6d904e2e: conv-lstm\/conv_lstm:train_remote (dataset_args={batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}, dev=yes, epochs=100, gpus=7, learning_rate=0.001, loss=mse, optimizer=Adam)\nINFO: [guild] Resolving config:conv_lstm\/config.yml dependency\nERROR: [guild] Trial 05afef0858f74b4198af160c6d904e2e exited with an error: (1) run failed because a dependency was not met: could not resolve 'config:conv_lstm\/config.yml' in config:conv_lstm\/config.yml resource: cannot find source file 'conv_lstm\/config.yml'\nRun 8a26ca399039412fb31c7791d293b507 stopped with a status of 'completed'\n\n<\/code><\/pre>\n<p>Do remote runs require everything to become a module, with \u2018<strong>init<\/strong>.py\u2019? Or should the guild file be in a different location?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild --remote option not recognized",
        "Question_creation_time":1616750140636,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-remote-option-not-recognized\/578",
        "Question_upvote_count":0.0,
        "Question_view_count":260.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m running Guild AI 0.7.3 and following the instructions for remote running <a href=\"https:\/\/my.guild.ai\/t\/remotes\/171\" class=\"inline-onebox\">Remotes<\/a> I run the command to check everything is set up correctly<\/p>\n<pre><code>guild --remote dev check\n<\/code><\/pre>\n<p>guild: unrecognized option \u2018\u2013remote\u2019<br>\nTry \u2018guild --help\u2019 for more information.<\/p>\n<p>I have configured my remote and can access it using ssh, my ~\/.guild\/config.yml looks like this.<\/p>\n<pre><code>remotes:\n    dev:\n      type: ssh\n      description: DSVM on Azure for development\n      host: dl\n<\/code><\/pre>\n<p>Is the tutorial outdated? Should I be able to run things remotely?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Dependency mismatch between guild.ai and tensorflow",
        "Question_creation_time":1610084094576,
        "Question_link":"https:\/\/my.guild.ai\/t\/dependency-mismatch-between-guild-ai-and-tensorflow\/512",
        "Question_upvote_count":1.0,
        "Question_view_count":546.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I currently have tensorflow installed, When I try to install guild.ai There is a dependency version conflict and pipenv is not able to resolve it. The version conflicting dependency is tensorboard.<\/p>\n<pre><code>sarat@sarat-pc:~\/Codes\/guild_start $ pipenv install guildai\nInstalling guildai...\nAdding guildai to Pipfile's [packages]...\n\u2714 Installation Succeeded \nPipfile.lock (66d06e) out of date, updating to (085a73)...\nLocking [dev-packages] dependencies...\nLocking [packages] dependencies...\nBuilding requirements...\nResolving dependencies...\n\u2718 Locking Failed! \n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/resolver.py\", line 741, in _main\n[ResolutionFailure]:       resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages, dev)\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/resolver.py\", line 702, in resolve_packages\n[ResolutionFailure]:       results, resolver = resolve(\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/resolver.py\", line 684, in resolve\n[ResolutionFailure]:       return resolve_deps(\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/utils.py\", line 1395, in resolve_deps\n[ResolutionFailure]:       results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/utils.py\", line 1108, in actually_resolve_deps\n[ResolutionFailure]:       resolver.resolve()\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/utils.py\", line 833, in resolve\n[ResolutionFailure]:       raise ResolutionFailure(message=str(e))\n[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.\n  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.\n Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.\n  Hint: try $ pipenv lock --pre if it is a pre-release dependency.\nERROR: Could not find a version that matches tensorboard&lt;2.3.0,&gt;=2.0.0,~=2.4 (from tensorflow==2.4.0-&gt;-r \/tmp\/pipenvjhzpx270requirements\/pipenv-2yv9nm20-constraints.txt (line 2))\nTried: 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.12.0, 1.12.1, 1.12.2, 1.13.0, 1.13.1, 1.14.0, 1.15.0, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.4.0\nSkipped pre-versions: 1.6.0rc0\nThere are incompatible versions in the resolved dependencies:\n  tensorboard&lt;2.3.0,&gt;=2.0.0 (from guildai==0.7.1.post1-&gt;-r \/tmp\/pipenvjhzpx270requirements\/pipenv-2yv9nm20-constraints.txt (line 3))\n  tensorboard~=2.4 (from tensorflow==2.4.0-&gt;-r \/tmp\/pipenvjhzpx270requirements\/pipenv-2yv9nm20-constraints.txt (line 2))\n<\/code><\/pre>\n<p>So pipenv is not able to find compatible tensorflow and guild.ai versions.<\/p>\n<p>What version of tensorflow does guild.ai support?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"EDITOR with VS Code not working on Windows",
        "Question_creation_time":1613457478077,
        "Question_link":"https:\/\/my.guild.ai\/t\/editor-with-vs-code-not-working-on-windows\/545",
        "Question_upvote_count":0.0,
        "Question_view_count":256.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"146\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"> guildai:<\/div>\n<blockquote>\n<p>Guild uses the editor defined in <code>VISUAL<\/code> or <code>EDITOR<\/code> environment variables.<\/p>\n<\/blockquote>\n<\/aside>\n<p>I had set the EDITOR variable to code in Windows 10. This is opening up a file in VS Code. But the file is immediately deleted and guild is auto-confirming the default values, So I am not able to change the values in the text editor.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issue with defining source code in guild.yml",
        "Question_creation_time":1647306584584,
        "Question_link":"https:\/\/my.guild.ai\/t\/issue-with-defining-source-code-in-guild-yml\/833",
        "Question_upvote_count":2.0,
        "Question_view_count":89.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">sort_sv_by_score:\n  description: Runs validation of sorting singular vectors by score\n  main: tests\/sort_sv_by_score\n  sourcecode:\n    -exclude: '*.json'\n<\/code><\/pre>\n<p>This is my guild.yml file. I have a bunch of .json files in my directory (my results logs from previous experiments). When I run this operation though, despite specifying exclude, it attempts to copy over all the .json files.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Scalars not getting saved",
        "Question_creation_time":1611248492978,
        "Question_link":"https:\/\/my.guild.ai\/t\/scalars-not-getting-saved\/527",
        "Question_upvote_count":1.0,
        "Question_view_count":328.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019m new to guild.ai and trying to use the guild.yml files to save all relevant information including scalars. The scalars are however not being saved.<\/p>\n<p>This is my code:<\/p>\n<pre><code>boston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nnamedf = ['X_train', 'X_test']\nnamenp = ['y_train', 'y_test']\n\nreg = Ridge(alpha=0.9)\nfitted = reg.fit(X, y)\n\nprint(\"score: %f\" % fitted.score(X, y))\n<\/code><\/pre>\n<p>and this is my guild.yml file:<\/p>\n<pre><code>ridge-regression:\n  description: fit ridge regression using boston data.\n  notebook: Checklist.ipynb\n  flags:\n    alpha:\n      description: alpha value in ridge regression\n      nb-replace: 'alpha=(\\d+)'\n  output-scalers:\n    score: 'score: (\\value)'\n<\/code><\/pre>\n<p>When I print out all information on the command line there are no scalars to be seen. Does anyone know why?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Status flag is \"terminated\" when experiment is still \"running\"",
        "Question_creation_time":1606300246699,
        "Question_link":"https:\/\/my.guild.ai\/t\/status-flag-is-terminated-when-experiment-is-still-running\/461",
        "Question_upvote_count":3.0,
        "Question_view_count":328.0,
        "Question_answer_count":10,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I currently run experiments. But their status flag says \u201cterminated\u201d instead of \u201crunning\u201d. This is quite annoying since now I can\u2019t delete all terminated runs without also deleting active runs.<\/p>\n<p>Excerpt of output when running <code>guild runs info<\/code>:<\/p>\n<pre><code>status: terminated\nstarted: 2020-11-25 05:59:38\nstopped:\n<\/code><\/pre>\n<p>Is it bug or did I cause this somehow?<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild runs on remote not found",
        "Question_creation_time":1626996133100,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-on-remote-not-found\/738",
        "Question_upvote_count":2.0,
        "Question_view_count":230.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve setup a remote in the <code>guild.yml<\/code> file. No problem running <code>guild pull --remote server<\/code> and the like. But how does one run a remote op from the local computer? There must be something simple I\u2019m missing. Running:<\/p>\n<pre><code>guild run train --remote server\n<\/code><\/pre>\n<p>Always yields <code>guild: cannot find operation train<\/code>. This kind of makes sense, how would guild know the location of the ops on the remote? My <code>guild.yml<\/code> looks something like:<\/p>\n<pre><code>  server:\n    type: ssh\n    description: Remote servers\n    user: username\n    host: server.domain\n    conda-env: \/home\/path\/to\/anaconda3\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error while running gp optimizer for Hyperparameter optimization: Cannot find objective 'loss'",
        "Question_creation_time":1618193158528,
        "Question_link":"https:\/\/my.guild.ai\/t\/error-while-running-gp-optimizer-for-hyperparameter-optimization-cannot-find-objective-loss\/679",
        "Question_upvote_count":2.0,
        "Question_view_count":445.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use Guild to do hyperparameter optimization using the gp optimizer. The first three runs started using random initializations as expected. However, even after the 3rd run, it continues to perform random initializations. On examining the output, I noticed that the following information was posted:<\/p>\n<p>INFO: [guild] Random start for optimization (cannot find objective \u2018loss\u2019)<\/p>\n<p>Is this expected? Or am I missing something?<\/p>\n<p>I looked at the scalars using guild runs info and found that all the scalars that I am logging using Tensorboard are displayed correctly.<\/p>\n<p>I would appreciate any help in this matter.<\/p>\n<p>Thanks,<br>\nVishal<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild run can't find module\/relative import",
        "Question_creation_time":1626396791066,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-run-cant-find-module-relative-import\/735",
        "Question_upvote_count":0.0,
        "Question_view_count":836.0,
        "Question_answer_count":18,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I have my project structured as follows:<\/p>\n<pre><code>classification\/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 coordinate_conv.py\n\u251c\u2500\u2500 cosine_annealing.py\n\u251c\u2500\u2500 data_generator.py\n\u251c\u2500\u2500 deformable_conv.py\n\u251c\u2500\u2500 drop_block.py\n\u251c\u2500\u2500 nnet_blocks.py\n\u251c\u2500\u2500 infer.py\n\u251c\u2500\u2500 model.py\n\u251c\u2500\u2500 train.py\n\u2514\u2500\u2500 utils.py\n<\/code><\/pre>\n<p>Where every python script is a module, and arguments are passed with <code>argparse<\/code>. So I would run <code>train.py<\/code> as<\/p>\n<pre><code>python -m classification.train \\\n    --model-name model \\\n    --train-data path\/to\/train\/data \\\n    --cycles 3 \\\n    --no-require-clean\n<\/code><\/pre>\n<ol>\n<li>\n<p>If I run <code>guild run<\/code> outside the <code>classification<\/code> folder, having configured <code>guild.yml<\/code> according to the documentation (tried both: <code>main: classification.train<\/code> and <code>main: classification\/train<\/code>), <code>guild<\/code> tells me it cannot find the <code>classification.train<\/code> module.<\/p>\n<\/li>\n<li>\n<p>if I run <code>guild run train.py<\/code> from inside the <code>classification<\/code> folder, <code>guild<\/code> tells me it cannot do relative imports with no known parent package (expected I guess).<\/p>\n<\/li>\n<\/ol>\n<p>So how would I run the training script with the above project structure, argparse, and relative imports?<\/p>\n<p>Thanks!<br>\n-fernando<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tensorboard FileNot found error on Windows-10, guild-0.7.3.dev1",
        "Question_creation_time":1616632467283,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-filenot-found-error-on-windows-10-guild-0-7-3-dev1\/569",
        "Question_upvote_count":2.0,
        "Question_view_count":370.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<pre><code>C:\\Users\\sarat.chinni\\Codes_sequencing\\biobench\\sandbox\\Sarat\\incorp_basecalling (feature\/incorp_basecalling -&gt; origin)\n(biobench-thMxqAli) \u03bb guild tensorboard\nPreparing runs for TensorBoard\nERROR: error removing C:\\Users\\SARAT~1.CHI\\AppData\\Local\\Temp\\guild-tensorboard-88wle55x: [WinError 3] The system cannot find the path specified: \"C:\\\\Users\\\\SARAT~1.CHI\\\\AppData\\\\Local\\\\Temp\\\\guild-tensorboard-88wle55x\\\\d83224a9 dnn_classifier_train 2021-03-24 17_18_48 batch_size=32 dense_activation=relu dense_units='128 128' drop_out=0.0 fileName=black_and_white_1sec.pickle l2_decay=0.001 num_epochs=100 seed=42 verify_saved=no\"\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\.virtualenvs\\biobench-thMxqAli\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 94, in _run_tensorboard\n    monitor.run_once(exit_on_error=True)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\run_util.py\", line 89, in run_once\n    self._refresh_logdir(runs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\run_util.py\", line 97, in _refresh_logdir\n    self.refresh_run_cb(run, path)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 275, in f\n    return _refresh_run(run, run_logdir, state)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 281, in _refresh_run\n    _refresh_tfevent_links(run, run_logdir, state)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 292, in _refresh_tfevent_links\n    _init_tfevent_link(tfevent_path, link, run, state)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 304, in _init_tfevent_link\n    util.ensure_dir(link_dir)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\util.py\", line 74, in ensure_dir\n    os.makedirs(d)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\os.py\", line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\os.py\", line 223, in makedirs\n    mkdir(name, mode)\nFileNotFoundError: [WinError 3] The system cannot find the path specified: \"C:\\\\Users\\\\sarat.chinni\\\\AppData\\\\Local\\\\Temp\\\\guild-tensorboard-88wle55x\\\\d83224a9 dnn_classifier_train 2021-03-24 17_18_48 batch_size=32 dense_activation=relu dense_units='128 128' drop_out=0.0 fileName=black_and_white_1sec.pickle l2_decay=0.001 num_epochs=100 seed=42 verify_saved=no\"\n\n<\/code><\/pre>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"NameError: name 'argparse' is not defined",
        "Question_creation_time":1595612979502,
        "Question_link":"https:\/\/my.guild.ai\/t\/nameerror-name-argparse-is-not-defined\/256",
        "Question_upvote_count":0.0,
        "Question_view_count":2973.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi,<br>\nStarting from a fresh environment with python 3.7.3 and doing a <code>pip install guildai<\/code>, when trying to run<br>\n<code>guild tensorboard<\/code><br>\nI get this error:<br>\nFile \u201c\/Users\/louis-emmanuelmartinet\/.pyenv\/versions\/3.7.3\/envs\/ds-gathering\/lib\/python3.7\/site-packages\/tensorboard_plugin_wit\/wit_plugin_loader.py\u201d, line 73, in define_flags<br>\nexcept argparse.ArgumentError:<br>\nNameError: name \u2018argparse\u2019 is not defined<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Guild compare \/ view \/ tensorboard hangs",
        "Question_creation_time":1603449092502,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-compare-view-tensorboard-hangs\/427",
        "Question_upvote_count":0.0,
        "Question_view_count":347.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I have about 15 runs which where I perform about 60000 steps and log a loss for each step. When I try to view these using <code>guild runs<\/code> it works fine, but trying to extract the best run using compare or viewing the results using view \/ tensorboard it loads for a long time until I can actually view the information.<\/p>\n<p>What could be the reason for this? Am I logging too much per run?<\/p>\n<p>EDIT: I realised that <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a> has said elsewhere that the view is due for an overhaul, perhaps that will fix this problem.<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Staged pipeline steps not given labels",
        "Question_creation_time":1641249403629,
        "Question_link":"https:\/\/my.guild.ai\/t\/staged-pipeline-steps-not-given-labels\/791",
        "Question_upvote_count":0.0,
        "Question_view_count":154.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>I\u2019ve noticed that guild applies that labels I give to pipelines to each pipeline step when the pipeline stage is ran directly.  However, if I instead stage the pipeline operation and then run the staged pipeline operation this doesn\u2019t happen.  Instead only the pipeline operation receives label, not the steps of the pipeline.   For example, lets say my <code>guild.yml<\/code> is given as follows.  The contents of <code>train.py<\/code> and <code>test.py<\/code> aren\u2019t really important.<\/p>\n<pre><code>- operations:\n    mypipeline:\n        steps:\n            - train\n            - test\n\n    train:\n        sourcecode:\n            dest: .\n            select: train.py\n        exec: \"python train.py\"\n\n    test:\n        sourcecode:\n            dest: .\n            select: test.py\n        exec: \"python test.py\"\n<\/code><\/pre>\n<p>If I run <code>guild run mypipeline -y --label debug<\/code> I get<\/p>\n<pre><code>[1:b21131b7]   test                                      2022-01-03 17:27:55  completed  debug\n[2:8dd06463]   train                                     2022-01-03 17:27:55  completed  debug\n[3:092f43f2]   mypipeline                                2022-01-03 17:27:54  completed  debug\n<\/code><\/pre>\n<p>But if I run <code>guild run mypipeline -y --label debug --stage &amp;&amp; guild run queue -y<\/code> I get the following.  Note that only the <code>mypipeline<\/code> operation receives the label <code>debug<\/code> while the step operations receive no label.<\/p>\n<pre><code>[1:5b2c8b79]   test                                      2022-01-03 17:28:48  completed\n[2:85ae374c]   train                                     2022-01-03 17:28:47  completed\n[3:a0dc7ef1]   mypipeline                                2022-01-03 17:28:46  completed   debug\n<\/code><\/pre>\n<p>Note that similar commands like <code>guild run mypipeline -y --label debug --stage &amp;&amp; guild run --start $(guild select 1)<\/code> have the same effect.<\/p>\n<p>Is this intended behavior? If so how can I make it so that the steps of the staged pipeline operation receive the same label as the pipeline operation that started them?  Thanks<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Exporting guild runs from remote sever",
        "Question_creation_time":1606228056115,
        "Question_link":"https:\/\/my.guild.ai\/t\/exporting-guild-runs-from-remote-sever\/457",
        "Question_upvote_count":2.0,
        "Question_view_count":421.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":0.0,
        "Question_body":"<p>Hi Garret and co,<br>\nI have used guild on my last project and liked it! This time, I am running my scripts on different remote servers. I would like to import the runs to my desktop to view them all together. I am not sure how to do this. I would prefer a solution that doesnt involve the remote configuration, because I dont know much about it and my system requires often to enter passwords.<br>\nThanks for the help!<\/p>",
        "Tool":"Guild AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Could it trigger the train model with Restful API?",
        "Question_creation_time":1543301201000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/D38-4wDIVsg",
        "Question_upvote_count":null,
        "Question_view_count":24.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi, there\n\u00a0 \u00a0 I sent the message\u00a0 to \"2.0\/preview\/mlflow\/runs\/create\" \uff0cthen I can see the detail on the UI including\u00a0the run command.\u00a0 But when the start time is up\uff0c still not run.\u00a0\n\n\n\u00a0I hope that it could trigger a JOB when call the some Restful API. How could it work ?\u00a0\n\n\n\u00a0The following is the response message:\n\n\nhttp:\/\/host:port\/api\/2.0\/preview\/mlflow\/runs\/create\n\n\n\n\n{\n\"experiment_id\": \"0\",\n\"user_id\": \"me\",\n\"run_name\": \"test_run\",\n\"source_type\": \"PROJECT\",\n\"source_name\": \"git@test\/test.git\",\n\"entry_point_name\": \"main\",\n\"start_time\":1543314730000,\n\"source_version\": \"\",\n\"tags\":[]\n}\n\n\nrequest :\n\n\n{\n\u00a0 \u00a0 \"run\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \"info\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"start_time\": \"1543314730000\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"user_id\": \"me\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"run_uuid\": \"3e178d7173874811a27b83ebbb3881e7\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"entry_point_name\": \"main\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"experiment_id\": \"0\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"source_type\": \"PROJECT\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"source_name\": \"git@test\/test.git\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"lifecycle_stage\": \"active\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"status\": \"RUNNING\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"artifact_uri\": \"$WORK_PATH\/mlruns\/0\/3e178d7173874811a27b83ebbb3881e7\/artifacts\"\n\u00a0 \u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 }\n}",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Organizaions using MLFlow",
        "Question_creation_time":1616546187000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/h4GyDKFUWcg",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi\n\nWe at\u00a0 Directorate General of Customs and Excise (https:\/\/www.beacukai.go.id) are using MLFlow extensively in an in-house Machine Learning platform. Please add our organization to the list on your website.\n\nRegards",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Anyone trying backend-store-uri ?",
        "Question_creation_time":1552644949000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/T6lt0HQxGr4",
        "Question_upvote_count":null,
        "Question_view_count":43.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":null,
        "Question_body":"I am trying to use backend-store-uri for a mysql db.\nI pulled from master today and built wheel\n\n\n\u00a0 \u00a0 \u00a0 \u00a0python3.6 setup.py bdist_wheel\n\n\nI am using the wheel in this Dockerfile below.\n\n\nGetting 403 error when I go to localhost:5000\n\n\nI checked my db connections using a local Sqlachemy script and it works fine.\n\n\nAnyone trying backend-store-uri ?\n------------------------\n\n\nFROM centos:6\n\n\nENV LC_ALL=en_US.utf-8\nENV LANG=en_US.utf-8\n\n\nRUN yum update -y\nRUN yum install yum-utils -y\nRUN yum install -y https:\/\/centos6.iuscommunity.org\/ius-release.rpm\nRUN yum install -y python36u python36u-libs python36u-devel python36u-pip\n\n\nRUN yum install -y which gcc\n\n\nRUN ln -fs \/usr\/bin\/pip3.6 \/bin\/pip\nRUN ln -fs \/usr\/bin\/python3.6 \/usr\/bin\/python\n\n\nRUN python --version\nRUN pip --version\n\n\n\n\nENV TERM linux\nENV BUCKET #####\n\n\n#ENVs for mysql Aurora\nENV USERNAME #######\nENV PASSWORD #######\nENV HOST #######\nENV DATABASE #######\n\n\nCOPY mlflow-0.8.3.dev0-py3-none-any.whl .\/\nRUN pip install mlflow-0.8.3.dev0-py3-none-any.whl\n\n\nRUN mkdir -p \/mlflow\/\n\n\nEXPOSE 5000\n\n\nCMD mlflow server \\\n\u00a0 \u00a0 --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE} \\\n\u00a0 \u00a0 --default-artifact-root s3:\/\/${BUCKET}\/mlflow-artifacts \\\n\u00a0 \u00a0 --host 0.0.0.0 --gunicorn-opts \"--access-logfile -\"",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow feature roadmap",
        "Question_creation_time":1531377865000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/FG3X_OaEgtw",
        "Question_upvote_count":null,
        "Question_view_count":314.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi there,\u00a0\n\n\nIs there a roadmap of features that are planned for MLflow and that you maybe would like help with?\u00a0\n\n\n-- Bruno",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trying to use the mysql db with backend-store-uri",
        "Question_creation_time":1557761819000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-N-LhiQuJ9w",
        "Question_upvote_count":null,
        "Question_view_count":34.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"When I use a mysql url for the backend-store-uri flag, I cannot get the mlflow server to run.\u00a0If I use a path and the filestore, it works fine.\n\n\u00a0\n\nI'm getting the standard error homepage: \"Oops! Something went wrong\"\n\n\n\n\nAny suggestions?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Support for Oracle dialect in SqlAlchemyStore",
        "Question_creation_time":1560524168000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/EC5HClf5fAE",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello:\n\n\nIs anyone working on allowing Oracle databases as an option for backend storage?\u00a0 I made the necessary modifications and have it working in my local development environment, but it would need a little more work to ensure that the other dialects don't break with the changes.\u00a0 Is it worth proceeding, or is it something that someone else is already working on?\n\n\n\nThanks",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiuser environment and Slurm",
        "Question_creation_time":1539159503000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/33RwUm-BPh0",
        "Question_upvote_count":null,
        "Question_view_count":330.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\u00a0\n\n\nI have two questions about mlflow.\u00a0\n\n\nIs it possible to use mlflow for multiuser environment ? How they can access gui running their own experiments ?\u00a0\nIs there a way to integrate slurm job manager in mlflow ?\u00a0\n\n\nPlease let me know. Thank you.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can't run MLflow web-based user interface",
        "Question_creation_time":1530092849000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/BoUzYBMGml4",
        "Question_upvote_count":null,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI have problem running MLflow web-based user interface. I gave the details in the following StackOverflow question:\n\n\n\u00a0\u00a0 https:\/\/stackoverflow.com\/questions\/51064366\/cant-run-mlflow-web-based-user-interface\n\n\nAny ideas?\n\n\n--\nEmre",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"store data preprocessing information",
        "Question_creation_time":1600770098000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/gZC21Br284g",
        "Question_upvote_count":null,
        "Question_view_count":35.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\nI've tried mlflow to store metrics, params, artifacts and models.\nI'd like to know if it can store also data-preprocessing information (feature extraction, selection, transformation, data cleansing, etc.).\nThanks\n--giacomo",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there any way to display the entire source path of the file",
        "Question_creation_time":1564461471000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CKfSORLcfRs",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\nIn the MLFLOW UI, the source only displays the name of the file whereas when I set the source tag I set the entire path in that.\nis there any way to display the entire path, specially if the file is saved in a GIT repository.\n\n\nThanks",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Spark UDF for Keras model - 'int' object is not iterable",
        "Question_creation_time":1542241994000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-Ukvbx443hs",
        "Question_upvote_count":null,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi mlflow team,\n\n\nI've been trying to export a Keras model as a Spark UDF, but met problem while trying to use it to predict on dataframe.\n\n\nI'm using example\/hyperparam to train the Keras model, using example dataset wine-quality.csv\n\n\nAnd here is what I use to export the mode as Spark UDF and then predict:\nmodel_path_local_keras = '~\/mlruns\/0\/625ddb14a0924a3bb2b12c201a775828\/artifacts\/model\/'\n\nwine_udf = mlflow.pyfunc.spark_udf(spark, keras_model_path)\n\n\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option('delimiter', ',').load('wine-quality.csv')\n\n\ncolumns = [ \"fixed acidity\", \"volatile acidity\", \"citric acid\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"residual sugar\", \"chlorides\", \"free sulfur dioxide\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"total sulfur dioxide\", \"density\", \"pH\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"sulphates\", \"alcohol\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\n\ndf.withColumn('prediction', wine_udf(*columns)).show(100, False)\n\nBut cannot get the expected result.\n\n\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"\/usr\/local\/Cellar\/apache-spark\/2.3.2\/libexec\/python\/lib\/pyspark.zip\/pyspark\/worker.py\", line 253, in main\n    process()\n  File \"\/usr\/local\/Cellar\/apache-spark\/2.3.2\/libexec\/python\/lib\/pyspark.zip\/pyspark\/worker.py\", line 248, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"\/usr\/local\/Cellar\/apache-spark\/2.3.2\/libexec\/python\/lib\/pyspark.zip\/pyspark\/serializers.py\", line 267, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"\/usr\/local\/Cellar\/apache-spark\/2.3.2\/libexec\/python\/lib\/pyspark.zip\/pyspark\/worker.py\", line 92, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"\/usr\/local\/Cellar\/apache-spark\/2.3.2\/libexec\/python\/lib\/pyspark.zip\/pyspark\/worker.py\", line 83, in verify_result_length\n    result = f(*a)\n  File \"\/usr\/local\/Cellar\/apache-spark\/2.3.2\/libexec\/python\/lib\/pyspark.zip\/pyspark\/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"\/Users\/RL250043\/anaconda3\/envs\/py36\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/__init__.py\", line 227, in predict\n    return pandas.Series(result)\n  File \"\/Users\/RL250043\/anaconda3\/envs\/py36\/lib\/python3.6\/site-packages\/pandas\/core\/series.py\", line 275, in __init__\n    raise_cast_failure=True)\n  File \"\/Users\/RL250043\/anaconda3\/envs\/py36\/lib\/python3.6\/site-packages\/pandas\/core\/series.py\", line 4167, in _sanitize_array\n    subarr = com._asarray_tuplesafe(data, dtype=dtype)\n  File \"\/Users\/RL250043\/anaconda3\/envs\/py36\/lib\/python3.6\/site-packages\/pandas\/core\/common.py\", line 320, in _asarray_tuplesafe\n    values = [tuple(x) for x in values]\n  File \"\/Users\/RL250043\/anaconda3\/envs\/py36\/lib\/python3.6\/site-packages\/pandas\/core\/common.py\", line 320, in <listcomp>\n    values = [tuple(x) for x in values]\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:171)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:121)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:131)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:93)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\nThe Spark UDF exported from a sklearn model works perfectly under same dataset.\n\n\nDoes current mlflow support exporting Keras\/Tensorflow model as a Spark UDF?\n\n\nThank you for you help.\n\n\nBest Regards,\nRising Liu",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow.pyfunc.log_model doesn't work correctly when artifact storage is HDFS and artifact size is size higher than 2gb",
        "Question_creation_time":1611675747000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/xKI3gYkayYU",
        "Question_upvote_count":null,
        "Question_view_count":121.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello MLflow team\n\n\nThere is an issue on pyarrow side which leads to the following misbehaving of MLflow when artifacts are kept on HDFS:\n\n\n1. When a size of an artifact is less than 6144mb, then\u00a0mlflow.pyfunc.log_model uploads corrupted artifact to HDFS with size not greater than 2gb.\n\n2. When a size of an artifact is higher or equals to 6144mb, then there will be an exception.\n\n\nStacktrace:\n\"\"\"\nsite-packages\/mlflow\/store\/artifact\/hdfs_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\n\u00a0 \u00a0 \u00a066\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0destination = posixpath.join(hdfs_subdir_path, each_file)\n\u00a0 \u00a0 \u00a067\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0with hdfs.open(destination, 'wb') as output_stream:\n---> 68\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0output_stream.write(open(source, \"rb\").read())\n\u00a0 \u00a0 \u00a069\u00a0\n\u00a0 \u00a0 \u00a070\u00a0 \u00a0 \u00a0def list_artifacts(self, path=None):\n\n\nsite-packages\/pyarrow\/io.pxi in pyarrow.lib.NativeFile.write()\nsite-packages\/pyarrow\/error.pxi in pyarrow.lib.check_status()\n\n\nOSError: HDFS Write failed, errno: 22 (Invalid argument)\n\"\"\"\n\n\n\nPython script to reproduce the issue just on pyarrow lib:\n\"\"\"\n\nimport os\nimport pyarrow as pa\n\nos.environ[\"JAVA_HOME\"]=\"<java_home>\"\nos.environ['ARROW_LIBHDFS_DIR'] = \"<path>\/libhdfs.so\"\nconnected = pa.hdfs.connect(host=\"<host>\",port=8020)\ndestination = \"hdfs:\/\/<host>:8020\/user\/tmp\/6144m.txt\"\nsource = \"\/tmp\/6144m.txt\"\nwith connected.open(destination, \"wb\") as output_stream:\n\u00a0 \u00a0 output_stream.write(open(source, \"rb\").read())\nconnected.close()\n\n\"\"\"\n\n\n\nThe issue was reported to pyarrow team and the answer is:\n\"\"\"\nIt appears that writes over 2GB are implemented incorrectly.\nhttps:\/\/github.com\/apache\/arrow\/blob\/master\/cpp\/src\/arrow\/io\/hdfs.cc#L277\n\nthe tSize type in libhdfs is an int32_t. So that static cast is truncating data\nhttps:\/\/issues.apache.org\/jira\/browse\/ARROW-11391\n\nI would recommend breaking the work into smaller pieces as a workaround\n\n\"\"\"\n\n\nCheers,\nSergey",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Could you add Criteo in the MLflow homepage list of contributors ?",
        "Question_creation_time":1570788691000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ZZaVlGTlc_0",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello\n\n\nHere in Criteo we like a lot MLflow and contribute to make it even better.\nWould you agree to add our logo to the list of contributors ?\n\n\nHere is the image and the link\u00a0https:\/\/www.criteo.com\/\n\n\nRegards",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What's the purpose of mlflow.models.FlavorBackend.serve API ?",
        "Question_creation_time":1563949854000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/O_w-8r1UVmg",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\nI was looking into the\u00a0mlflow.models.FlavorBackend.serve API. What is the purpose of this API? Has anyone used it?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issue 3962",
        "Question_creation_time":1611154753000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/yY-qH8rC_0c",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI posted Issue 3962 12 days ago. When should I expect a response from the team?\n\n\nThank you,\nJuan",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow ui error",
        "Question_creation_time":1571425023000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ELSSZ5ibVe4",
        "Question_upvote_count":null,
        "Question_view_count":328.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"I am on Mac Mojave, python 3.7.3\n\n\nI am trying Mlflow for first time.\n\n\nI successfully ran\u00a0python examples\/sklearn_elasticnet_wine\/train.py from tutorial page.\n\n\n\n\nI getting this when I run, I can see ui at\u00a0http:\/\/127.0.0.1:5000\/\u00a0but no data shows\n\n\n\n$mlflow ui\n\n\n\n\n[2019-10-18 15:49:00 -0700] [26101] [INFO] Starting gunicorn 19.9.0\n\n[2019-10-18 15:49:00 -0700] [26101] [INFO] Listening at: http:\/\/127.0.0.1:5000 (26101)\n\n[2019-10-18 15:49:00 -0700] [26101] [INFO] Using worker: sync\n\n[2019-10-18 15:49:00 -0700] [26104] [INFO] Booting worker with pid: 26104\n\n[2019-10-18 15:49:32 -0700] [26101] [INFO] Handling signal: winch\n\n[2019-10-18 15:49:32 -0700] [26101] [INFO] Handling signal: winch\n\n......\n\n[2019-10-18 15:49:33 -0700] [26101] [INFO] Handling signal: winch\n\n[2019-10-18 15:49:33 -0700] [26101] [INFO] Handling signal: winch\n\n[2019-10-18 15:49:33 -0700] [26101] [INFO] Handling signal: winch\n\n[2019-10-18 15:49:33 -0700] [26101] [INFO] Handling signal: winch\n\n[2019-10-18 15:49:33 -0700] [26101] [INFO] Handling signal: winch\n\n[2019-10-18 15:50:37 -0700] [26101] [CRITICAL] WORKER TIMEOUT (pid:26104)\n\n[2019-10-18 15:50:37 -0700] [26104] [INFO] Worker exiting (pid: 26104)\n\n[2019-10-18 15:50:37 -0700] [26219] [INFO] Booting worker with pid: 26219\n\n\n\n\nAny clue what could be wrong?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"File store backup strategy",
        "Question_creation_time":1553245389000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/kTZMgpLKWew",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"I have deployed mlflow on kubernetes for the production environment. I am planning to backup 'file store' so that experiment metadata can be retrieved in case of storage failure. What would be the preferred interval at which I should take backup. Also if I simply restore data from the backup location, will mlflow resume its operation?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trying to use the 'log_metric() function.",
        "Question_creation_time":1541184539000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/qZONNmbAASk",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":null,
        "Question_body":"I used\u00a0 'with mlflow.start_run()\u00a0 to log and show the params in the web ui and it works,\u00a0 when i try to use log_metric() i cant get it to show in the ui.\u00a0\n\n\nI used a variable called 'history' to store the values output from model.compile, then a function to extract the float value, and then pass that variable to log_metric()\u00a0\n\n\nmy 'history' variable is not getting anything stored in it though.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"run projects using my HDP3 cluster",
        "Question_creation_time":1539538319000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/a6tRozzVEgc",
        "Question_upvote_count":null,
        "Question_view_count":41.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":null,
        "Question_body":"In section \"Scalability and Big Data\" in https:\/\/mlflow.org\/docs\/latest\/concepts.html#scalability-and-big-data:\n\"An individual MLflow run can execute on a distributed cluster, for example, using Apache Spark.\"\n\n\n\nThen https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.projects.html#mlflow-projects:\n\"cluster_spec \u2013 When mode is \u201cdatabricks\u201d, path to a JSON file containing a Databricks cluster specification to use when launching a run.\"\n\n\nI have a HDP3.0 cluster with spark 2.3.1 in our data center. How can I use my cluster for MLFlow? Does MLflow support running a project in my own cluster instead of a cluster in cloud providers (e.g. databrick, aws, azure)? Thanks for any hints.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to import a tensorflow model to spark",
        "Question_creation_time":1548755952000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Bx6GadlNgbE",
        "Question_upvote_count":null,
        "Question_view_count":24.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I have a requirement where I am doing a training and exporting a keras\/tensorflow model using mlflow. Is it possile to import that model to spark mlflow and do scoring there? I could not find such an option in documentation. When I tried the same I am getting the expection MlflowException: Model does not have the \"spark\" flavor\nml flow and then import that ml flow model into spark and do scoring there",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Performance issue of UI with O(100) runs?",
        "Question_creation_time":1560261092000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Nh23E3ncxzw",
        "Question_upvote_count":null,
        "Question_view_count":67.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nwe have been using MLFlow (only the tracking API) for some time now and\nhave recently upgraded to MLFlow 1.0.0 with a MySQL tracking store. Over\ntime, we have accumulated a (still moderate) number of runs in our\nlarger experiments, and the first signs of a performance issue start to\nshow up:\n\nWe have experiments with typically ~100 runs in them. This is enough so\nthat it takes the web UI several seconds (> 5s) to display the list of\nruns initially (before I can even add search queries). I have not done\nmore detailed profiling so far, but it seems strange to me to see such a\nresponse time in a still rather small setup.\n\nThus my question: Are there people around with large production setups\n(I would expect that larger databases with thousands of runs in a single\nexperiment could be quite common)? How well can MLFlow handle this? Or\nare you keeping single experiments small with only few runs (which does\nnot seem viable to me, since I cannot compare runs between different\nexperiments in the UI)?\n\nAny experience would be highly appreciated! Could well be that we just\nhave a badly configured SQL server, but I do not really know what to expect.\n\nThanks a lot!\n\nDa",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deployed an mlflow sklearn model to Azure ACI using Databrics how to get the prediction probabilities",
        "Question_creation_time":1570153632000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/0yQ7tWc1Pgc",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"I have deployed the model to Azure Container Instance using model.pkl file along with conda.yaml and MLproject file created by mlflow. I can get the output class for input data by hitting the endpoint\u00a0http:\/\/1fa00837-1734-487c-998e-82b402c2451a.eastasia.azurecontainer.io\/score  \nhowever I am interested in getting the prediction probabilities that we get in scikit learn model.predict_proba(). Can anyone suggest how I can do that.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ML flow artifact save to Azure Blob Storage",
        "Question_creation_time":1569205838000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9xW4OfPGdEI",
        "Question_upvote_count":null,
        "Question_view_count":8.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I am running mlflow sklearn_elasticnet_wine example. It saves the model locally to my computer. I want to modify the example so that I can save the model to Azure Blob and later deploy the model to Azure ACI. Can anyone guide me on how can I do it.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 0.8.1 Released!",
        "Question_creation_time":1545833546000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-T0gTl9Zy9M",
        "Question_upvote_count":null,
        "Question_view_count":33.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow 0.8.1 has been released!\n\n\nMLflow 0.8.1 introduces several significant improvements:\n\n\nImproved UI responsiveness and load time, especially when displaying experiments containing hundreds to thousands of runs.\n\n\nImproved visualizations, including interactive scatter plots for MLflow run comparisons.\n\n\nExpanded support for scoring Python models as Spark UDFs. For more information, see the updated documentation for this feature.\n\n\nBy default, saved models will now include a Conda environment specifying all of the dependencies necessary for loading them in a new environment.\n\n\nMLflow projects can now be run from ZIP files.\n\n\n\nFor a comprehensive list of features, see the release change log, and check out the latest documentation on mlflow.org.\n\n\n\n\n\nNote: The MLflow R package for 0.8.1 is not yet available on CRAN because CRAN's submission system will be offline until January 2nd. Version 0.8.0 of the R package is compatible with the remaining components of MLflow 0.8.1.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Online Learning - fit",
        "Question_creation_time":1545296754000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/bpllkE4Rdio",
        "Question_upvote_count":null,
        "Question_view_count":40.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":null,
        "Question_body":"Hey,\n\n\nThanks for this project, MLflow really help us to leverage our data-science team.\n\n\nI'm trying to understand what is the best option to have an online learning algo, which does \"model.fit\" every 30 minutes with new data and re-deployed to production.\nOur case is recommendations systems that works with REST-API and needs to be refreshed often.\n\n\nWhat is the best way to do it in mlflow?\n\n\nThanks,",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlruns and artifact cleanup",
        "Question_creation_time":1580314344000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/xVnEg6hfdi0",
        "Question_upvote_count":null,
        "Question_view_count":22.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"hi all. thanks for mlflow and the great info.\n\n\nim trying to clear-up some disk space on my tracking server. when i delete experiments in the ui, i notice the mlruns folder remains the same size. is there a standard way to clear out the artifacts for deleted experiments?\n\n\nthanks!\n\n\n-matt",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Scheduling an MLflow meetup in the next few weeks",
        "Question_creation_time":1529425986000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/dlTQmJNTnqI",
        "Question_upvote_count":null,
        "Question_view_count":83.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\nWe're planning to schedule an in-person MLflow meetup at Databricks in\nthe next few weeks to give people an overview of the project and get\ninput on the roadmap. If you'd like to attend, can you tell us your\ndate preferences at https:\/\/goo.gl\/forms\/8ZEnF9DXEVBWJlKU2? We'll also\nrecord the content presented for people not in the Bay Area.\n\nThis initial meetup will cover:\n\n* Introductory concepts and MLflow tutorial\n* Deploying MLflow\n* Project roadmap\n* How to contribute to MLflow\n\nAll the best,\n\nMatei and the MLflow team",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging graph plot in pyspark form into mlflow",
        "Question_creation_time":1573769729000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lrZwJNTVH6M",
        "Question_upvote_count":null,
        "Question_view_count":7.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I am trying to log K elbow result in pyspark form into mlflow, any thought on how this could be done?",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Model Monitoring with mlflow",
        "Question_creation_time":1568955792000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Gam2UGs84sE",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\u00a0\nI have deployed a network with mlflow and can get the predictions be hitting the endpoint. Is there any way to monitor the predictions being made by my model, for example i want to be able to monitor all the prediction calls received by my model and the predictions produced by it and I want to view it in the mlflow UI.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"download dependencies from a specified conda repo server",
        "Question_creation_time":1539528659000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/S289Bx72GZY",
        "Question_upvote_count":null,
        "Question_view_count":12.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"I am learning MLFlow and wondering how to make MLFlow work in an environment that does not have internet access. For example, our production environment will not have internet access. For MLFlow to work, I need to make MLFlow download conda packages from a local conda repo server inside this environment. Is this doable? I did not find the answer in MLFlow doc. Thanks for any hints.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLFlow Get Run information in R API and NGINX issue",
        "Question_creation_time":1541534149000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/6nWaATd4uTw",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\nGreat work on mlflow, we have been using it to run many experiments.\n\n\nI am trying to get a list of runs for a specific experiment. I am hosting mlflow in a container running on an EC2 instance in AWS.\u00a0\n\n\n\nI am using the R API.\n\n\n# set tracking URI\nmlflow_set_tracking_uri(ML_FLOW_TRACKING_URI)\n\n\n# init MLflow client object\nclient <- mlflow:::mlflow_client(ML_FLOW_TRACKING_URI)\n\n\n# extract experimment information\nd_exp <- mlflow:::mlflow_client_get_experiment(client, EXPERIMENT_ID)\n\n\n\nNow when I use this on experiments with few runs (<30) I get back the data in a named list with the data being stores in d_exp$runs\n\n\nHowever, for experiments with many runs I receive a cURL error as follow:\n\n\nError in curl::curl_fetch_memory(url, handle = handle) :\u00a0\n\u00a0 Timeout was reached: Operation timed out after 1003 milliseconds with 0 bytes received\n\n\nHas anyone had a similar\u00a0problem and happened to fix it? Or is this an issue with the API?\n\n\nNGINX Issue\n\n\nAlso another issue I have is sending requests to mlflow when I secure it with SSL certs using nGINX (with authentication) as a reverse proxy.\n\n\nIs there an example implementation on how to get this setup and working? I saw that there is a HostCreds class in the source code, but how do I initialise this?\n\n\nCurrent fix has been to whitelist the IP address, however this is obviously not a long-term solution.\n\n\nCheers,\n--\n\nVivek Katial\nData Scientist\n\nLevel 1, 155 Karangahape Road, Auckland Central,\u00a01010\nvivek....@quantiful.co.nz | \u00a00210435892\nwww.quantiful.co.nz",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Experimental status of Model Registry API",
        "Question_creation_time":1589868379000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Ke9c9AASIHM",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nModel Registry API has been around since v1.4.0 and is currently marked as experimental.\nIt's useful for managing the processes around model life-cycle, but committing to an experimental feature feels risky.\n\n\nCould someone please share if there are plans on significantly altering\/removing it?\nWhen would it be considered production ready?\n\n\nBest regards,\nAlexey",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Microtargetting Model Deployment Using ML Flow and AML",
        "Question_creation_time":1576648818000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/u8irjnEImeI",
        "Question_upvote_count":null,
        "Question_view_count":18.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Team,\n\n\nIn one of the scenarios we have to run Random Forest regressor for predicting scenario but the issue is we want to do prediction at the most granular level. Example scenario:\n\n\nProduct\tVolume\nA\t100ml\nA\t150 ml\nA\t200 ml\nA\t250 ml\nB\t100ml\nB\t150 ml\nB\t200 ml\nB\t250 ml\nC\t100ml\nC\t150 ml\nC\t200 ml\nC\t250 ml\nD\t100ml\nD\t150 ml\nD\t200 ml\nD\t250 ml\n\n\nHere in the above case we could either run a random forest regressor on products A,B,C,D a generic one. But in our case Product and Volume forms a key to be predicted so that we go to a granular level.\n\n\nIn our case Product A\/100ml is different from Product A\/150ml is different from Product A\/200ml is different from Product A\/250ml.\n\n\nSo thats the reason we need to run our model in loop which keeps on growing and we are generating multiple models inside loop. How is it possible to deploy such kind of models using MLFLOW and Azure Mahine Learning.or is there a better way to achieve such granularity.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Scaling and monitoring multiple experiments",
        "Question_creation_time":1543533458000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/akoCzxB_oz0",
        "Question_upvote_count":null,
        "Question_view_count":23.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi,\n\n\nI am new to MLflow and I would like to ask a few questions. I am a beginner so please be patient :)\n\n\n1) Does MLflow allow scaling of ML experiments\/jobs on a multi-gpu server?\n2) Does MLflow allow monitoring of such ML experiments\/jobs (e.g. failures, \"livestream\" of ML experiment\/job stage, etc.)?\n\n\nPerhaps my questions are very dummy. Thank you for your understanding.\u00a0\n\n\nThanks,\nAlex",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to debug mlflow artificat connection?",
        "Question_creation_time":1638472803000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/b0UFc00NzK0",
        "Question_upvote_count":null,
        "Question_view_count":464.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi, I am running into a problem where the mlflow server does not load the data from the artifact storage (which is a Minio for me) for a given run. Is there a way to debug mlflow side of communication with Minio server by enabling mlflow debug flags?\n\n\nFrom the tables I know the artifact location and I am able to retrieve objects from the Minio server for that same artifact bucket and path.\u00a0\n\n\nmlflow_db=# select * from experiments ;\n\u00a0experiment_id |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| artifact_location | lifecycle_stage\u00a0\n---------------+-------------------------+-------------------+-----------------\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01 | wine-pyfile-model\u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/1\u00a0 \u00a0 \u00a0| deleted\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02 | wine-pyfile-model-minio | s3:\/\/mlflow\/2\u00a0 \u00a0 \u00a0| deleted\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a03 | mlflow-wine\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/3\u00a0 \u00a0 \u00a0| active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a04 | mlflow-wine-minio\u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/4\u00a0 \u00a0 \u00a0| active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a05 | rest-wine-1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/5\u00a0 \u00a0 \u00a0| active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a07 | rest-wine-2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/7\u00a0 \u00a0 \u00a0| active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 | rest-wine-2-updated\u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/0\u00a0 \u00a0 \u00a0| active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a08 | Default\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/8\u00a0 \u00a0 \u00a0| active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a09 | rest-wine-3\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/9\u00a0 \u00a0 \u00a0| deleted\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 13 | rest-wine-4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/13\u00a0 \u00a0 | active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 14 | rest-wine-5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/14\u00a0 \u00a0 | active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 15 | rest-wine-51\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | s3:\/\/mlflow\/15\u00a0 \u00a0 | active\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 16 | rest-wine-6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| s3:\/\/mlflow\/16\u00a0 \u00a0 | deleted\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 17 | rest-wine-61\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | s3:\/\/mlflow\/17\u00a0 \u00a0 | active\n\n\nAny debugging ideas?\n\n\nThanks\nShounak",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mlflow inference tensorflow inception saved model giving error",
        "Question_creation_time":1566743416000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/GrCd-t0gx8U",
        "Question_upvote_count":null,
        "Question_view_count":21.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi All ,\n\n\nI am trying to serve a already saved tensorflow\u00a0 inception model using the below code :\nmodel_path = \"\/Users\/i311157\/Downloads\/data\/inception\"\nmlflow.tensorflow.save_model(tf_saved_model_dir=model_path, tf_meta_graph_tags=[tag_constants.SERVING],\n                             tf_signature_def_key=\"predict_images\",\n                             path=\"savedmodel\",\n                            conda_env =\"\/Users\/i311157\/PycharmProjects\/mlflow-tf\/conda.yaml\"\n\n\n\n\n\nI start serving the model from cli using below command :\n\n\n\nmlflow models serve --model-uri <MODEL-PATH-URL> -p 54321\n\n\n\nOnce the https server is up I try to do inference using the below :\n\n\n\ncurl -X POST \\\n\u00a0 http:\/\/127.0.0.1:54321\/invocations \\\n\u00a0 -H 'Accept: *\/*' \\\n\u00a0 -H 'Accept-Encoding: gzip, deflate' \\\n\u00a0 -H 'Cache-Control: no-cache' \\\n\u00a0 -H 'Connection: keep-alive' \\\n\u00a0 -H 'Content-Length: 20552' \\\n\u00a0 -H 'Content-Type: application\/json' \\\n\u00a0 -H 'Host: 127.0.0.1:54321' \\\n\u00a0 -H 'Postman-Token: 442368b4-808a-4c87-a514-0058853226be,4faba288-a91f-4f23-9714-94ffaf72cea1' \\\n\u00a0 -H 'User-Agent: PostmanRuntime\/7.15.2' \\\n\u00a0 -H 'cache-control: no-cache' \\\n\u00a0 -H 'format: pandas-split' \\\n\u00a0 -d '{\"columns\":[\"images\"],\"index\":[0],\"data\":[\"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\"}}\n'\n\nBut i am getting the below error :\n\n\n\n{\n\n      ret = Operation(c_op, self)\\n  File \\\"\/Users\/i311157\/PycharmProjects\/mlf-python-grpc-model-server\/venv\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py\\\", line 1768, in __init__\\n    self._traceback = tf_stack.extract_stack()\\n\\nInvalidArgumentError (see above for traceback): Expected image (JPEG, PNG, or GIF), got unknown format starting with 'image\/jpeg;base6'\\n\\t [[{{node map\/while\/DecodeJpeg}} = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\\\"\\\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\\\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\\\"](map\/while\/TensorArrayReadV3)]]\\n\\n\"\n\n\n\n\n\nIt gives me unknown format error but the test image is jpeg file still it gives the above error .\n\nBut when i try directly from the code like the below i get the prediction result:\n\n\ntf_graph = tf.Graph()\ntf_sess = tf.Session(graph=tf_graph)\nsignature_def =tensorflow.load_model(\"\/Users\/i311157\/PycharmProjects\/mlflow-tf\/mlruns\/0\/8167e43b47474b88b81a9069d1d45110\/artifacts\/model\",tf_sess=tf_sess)\nmodel = _tensorflow(tf_sess=tf_sess, tf_graph=tf_graph, signature_def=signature_def)\nimage_data = tf.gfile.GFile(\"\/Users\/i311157\/PycharmProjects\/mlflow-tf\/data\/testmlflow.jpeg\", 'rb').read()                    columns=[\"images\"])\ndf = pd.DataFrame(\n    data=[image_data],\n    columns=[\"images\"])\ndf.to_csv(\"test1.csv\")\njson_data=df.to_json(orient=\"split\")\npredict_df = model.predict(df)\n\n   scores                                 classes\n0  10.083101         b'lycaenid, lycaenid butterfly'\n1   7.066910           b'ringlet, ringlet butterfly'\n2   4.377772                    b'cabbage butterfly'\n3   3.555962  b'sulphur butterfly, sulfur butterfly'\n4   2.427257                              b'admiral'\n\n\nSo I am not sure what is wrong when i call http endpoint .\n\nKindly please help.\nMany Thanks.\nPLease let me know for any other information.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 1.21.0 released!",
        "Question_creation_time":1635184341000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CSKBY9hL2bo",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"We are happy to announce the availability of\u00a0MLflow\u00a01.21.0!\n\n\n\nMLflow 1.21.0 includes several major features and improvements:\n\nFeatures:\n\n[UI] Add a diff-only toggle to the runs table for filtering out columns with constant values (#4862,\u00a0@marijncv)\n[UI] Add a duration column to the runs table (#4840,\u00a0@marijncv)\n[UI] Display the default column sorting order in the runs table (#4847,\u00a0@marijncv)\n[UI] Add\u00a0start_time\u00a0and\u00a0duration\u00a0information to exported runs CSV (#4851,\u00a0@marijncv)\n[UI] Add lifecycle stage information to the run page (#4848,\u00a0@marijncv)\n[UI] Collapse run page sections by default for space efficiency, limit artifact previews to 50MB (#4917,\u00a0@dbczumar)\n[Tracking] Introduce autologging capabilities for PaddlePaddle model training (#4751,\u00a0@jinminhao)\n[Tracking] Add an optional tags field to the CreateExperiment API (#4788,\u00a0@dbczumar;\u00a0#4795,\u00a0@apurva-koti)\n[Tracking] Add support for deleting artifacts from SFTP stores via the\u00a0mlflow gc\u00a0CLI (#4670,\u00a0@afaul)\n[Tracking] Support AzureDefaultCredential for authenticating with Azure artifact storage backends (#4002,\u00a0@marijncv)\n[Models] Upgrade the fastai model flavor to support fastai V2 (>=2.4.1) (#4715,\u00a0@jinzhang21)\n[Models] Introduce an\u00a0mlflow.prophet\u00a0model flavor for Prophet time series models (#4773,\u00a0@BenWilson2)\n[Models] Introduce a CLI for publishing MLflow Models to the SageMaker Model Registry (#4669,\u00a0@jinnig)\n[Models] Print a warning when inferred model dependencies are not available on PyPI (#4891,\u00a0@dbczumar)\n[Models, Projects] Add\u00a0MLFLOW_CONDA_CREATE_ENV_CMD\u00a0for customizing Conda environment creation (#4746,\u00a0@giacomov)\n\nBug fixes and documentation updates:\n\n[UI] Fix an issue where column selections made in the runs table were persisted across experiments (#4926,\u00a0@sunishsheth2009)\n[UI] Fix an issue where the text\u00a0null\u00a0was displayed in the runs table column ordering dropdown (#4924,\u00a0@harupy)\n[UI] Fix a bug causing the metric plot view to display NaN values upon click (#4858,\u00a0@arpitjasa-db)\n[Tracking] Fix a model load failure for paths containing spaces or special characters on UNIX systems (#4890,\u00a0@BenWilson2)\n[Tracking] Correct a migration issue that impacted usage of MLflow Tracking with SQL Server (#4880,\u00a0@marijncv)\n[Tracking] Spark datasource autologging tags now respect the maximum allowable size for MLflow Tracking (#4809,\u00a0@dbczumar)\n[Model Registry] Add previously-missing certificate sources for Model Registry REST API requests (#4731,\u00a0@ericgosno91)\n[Model Registry] Throw an exception when users supply invalid Model Registry URIs for Databricks (#4877,\u00a0@yunpark93)\n[Scoring] Fix a schema enforcement error that incorrectly cast date-like strings to datetime objects (#4902,\u00a0@wentinghu)\n[Docs] Expand the documentation for the MLflow Skinny Client (#4113,\u00a0@eedeleon)\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does mlflow have Slack channel?",
        "Question_creation_time":1561615635000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/aG4Mm0nuIE4",
        "Question_upvote_count":null,
        "Question_view_count":39.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"I think communication in Slack will be much efficient.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Syncing between remote and local tracking servers",
        "Question_creation_time":1582080664000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/LBRtfMy0WAc",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nwe are considering to use mlflow for our next project. Most of our experiments will run inside AWS so we plan to setup a tracking server there, however it is still possible that some researchers would like to run some experiments locally on their machines. The problem is that due to security policies we can't expose the tracking server outside of our AWS environment.\n\n\nMy idea is that they would use local mlflow server and then use a script that would copy the state to S3 where we would somehow sync the runs to the tracking server in AWS.\n\n\nIs there a convenient way how to achieve that? Have you seen this use case before or do we approach the problem from a wrong perspective?\n\n\nThanks,\nJan",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow Users Slack",
        "Question_creation_time":1595961957000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CQ7-suqwKo0",
        "Question_upvote_count":null,
        "Question_view_count":115.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\nI'm trying to join the MLflow Slack channel (https:\/\/mlflow-users.slack.com\/) but it seems like I need a databricks email to create an account? The invite link on the MLflow website seems to be inactive. Just wondering if I could get an updated invite link to join the Slack!\n\n\n\nThanks!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying a model in scale",
        "Question_creation_time":1533808953000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-JLdZ6r7IdY",
        "Question_upvote_count":null,
        "Question_view_count":152.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello. I am developing a system which uses predictions with stored ML model, like *.pkl from sklearn.\n\n\nI'd like to use mlflow as a prediction service server but still doubt about its scalability. AFAIU, mlflow is providing Rest API only with flask. Although it will work well in dev or test environments, but I can't sure that it can handle millions of requests per second, which is so common nowadays.\n\n\nIn short, I have following questions to the main developers:\nDo you have any plan to support other types of RPC call in the future? I found that you are using protobuf to generate requests\/responses. IMHO, it would be not so difficult to extend it to support gRPC on top of those definitions.\nIs there any plan to build a dedicated model service entity for scalability?\nThanks in advance. All kinds of opinions are welcome.\n\n\n\nBest,\nDongjin",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Website section \"organizations using MLflow\" - we are using MLflow in our AI training courses",
        "Question_creation_time":1584959243000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/owFO6D5kPZk",
        "Question_upvote_count":null,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi all,\n\n\u00a0\n\njust wanted to let you know, that we are convinced of using MLflow in our AI \/ Deep Learning training courses. We originally started with TensorBoard in our courses, but realized MLflow is the tool to go for the course particpants using Keras \/ Tensorflow. MLFlow is easier to use and has much more functionality compared to TensorBoard. Keep up your great work :)\n\n\u00a0\n\nFeel free to add us (https:\/\/enable-ai.de) to your \"Organizations using MLflow\" section on your website. We are a startup from Germany which enables people to use Artificial Intelligence by teaching how to code with Python & Keras-Tensorflow.\n\n\u00a0\n\nBest,\u00a0\n\nJan",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Atlas Integration",
        "Question_creation_time":1529644466000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/YRUxFOeiP4U",
        "Question_upvote_count":null,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi guys, me again ;)\n\n\nAre you guys planning on integrating Atlas for keeping hold of which version of a model is deployed for which business case?\n\n\nCheers,\nD.",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLlofw artifacts to differnt environments",
        "Question_creation_time":1648020576000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-AQJqwq1l1c",
        "Question_upvote_count":null,
        "Question_view_count":14.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi team,,\n\n\u00a0 I have three subscription how does mlflow communicate between dev ,stagging and prod envs. please let me know what the process to move mlflow artifacts to different environments with three separate azure data bricks subscriptions\n\n\u00a0\n\n\u00a0\n\nRegards,\n\nNaveen",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot join the mlflow_user slack channel",
        "Question_creation_time":1570779317000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/MP1DcwqnXAI",
        "Question_upvote_count":null,
        "Question_view_count":39.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello,\n\n\nI'm trying to join the mlflow-users slack channel but I'm facing 2 issues :\n\n\n1\/ The link on the MLflow.org main page bring me to a slack page saying that the invitation link is no longer active :\u00a0public Slack server\n2\/ If I try to connect to https:\/\/mlflow-users.slack.com\nI need a @databricks.com email to be able to create an account.\n\n\nRegards",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"using scikit-learn in mlflow pyfunc",
        "Question_creation_time":1601627709000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/cnoGg6blwow",
        "Question_upvote_count":null,
        "Question_view_count":51.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi All, Is it possible to use scikit-learn inside pyfunc? I have tried it myself in a project i am working on, however, I received ModuleNotFoundError: No module named 'sklearn'. Many thanks, Cheers!",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Filament AI Logo for inclusion on your website!",
        "Question_creation_time":1609933995000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/3yzDfHEs0F0",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi there,\n\n\nWe'd be very grateful if we could get our logo on your site.\u00a0\n\n\nWe've been using MLFlow a lot in 2020 and love what you folks are up to. I recently wrote a blog about our use of MLFlow with NLP models we've been developing https:\/\/medium.com\/filament-ai\/serving-nlp-models-with-mlflow-b19d310ec4c6\n\n\nI've attached our logo - I hope that's OK.\n\n\nBest wishes,\n\n\n\n\nJames Ravenscroft | \u00a0Chief Technology Officer\n\n\n\nFilament\u00a0-\u00a0Experts in Applied AI\n\n1 King William Street,\nLondon\n\nEC4N 7BJ\nfilament.ai\n\n\n\nLONDON | SOUTHAMPTON | TORONTO\n\n\n\nFilament Consultancy Group Limited - Registered in England and Wales | Company # 10180537\n\nRegistered in Ontario, Corporation # 1995332",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"About Website Section \"Organizations using MLflow\" - Insider",
        "Question_creation_time":1601382719000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ZJkqm7nF62Y",
        "Question_upvote_count":null,
        "Question_view_count":19.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hello all,\n\n\nAt Insider we have developed our internal Machine Learning Platform from scratch and we have integrated MLflow to our pipeline for ml framework independent serving purposes and running\u00a0it on production for a couple of months. It would be a pleasure for us to support MLflow and appear in \"Organizations using MLflow\" section on the website. Our company logo is attached to this email.\n\n\nInsider is a SaaS company providing Growth Management Platform (GMP) that delivers a broad feature set across the sales funnel from Acquisition to Activation, Retention, and Revenue, leveraging real-time predictive segmentation powered by Artificial Intelligence and machine learning capabilities.\u00a0Built on a unified data layer, GMP is easy to implement and simple to use, with no need for complicated integrations. We are working with 500+ companies including global brands such as Singapore Airlines, CNN, Samsung, Huawei, and Marks & Spencer. Insider ranked as #1 Leader on G2 Grid for Mobile Marketing and positioned in Gartner\u2019s Magic Quadrant for Multichannel Marketing Hubs 2020. You can read more on our website:\u00a0https:\/\/useinsider.com\n\n\nBest,\n\n\nDeniz",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"rest api request create-run",
        "Question_creation_time":1615321248000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/XaAAvIMX2Vc",
        "Question_upvote_count":null,
        "Question_view_count":11.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"Hi, i try to learn to make rest call but could not find out using the documentation.\nhttps:\/\/mlflow.org\/docs\/latest\/rest-api.html#create-run\nI try using post on the address and did not work out.\n\nhttp:\/\/192.168.1.3:5000\/api\/2.0\/preview\/mlflow\/runs\/create\n\n\n{\"experiment_id\":\"2\",\n\"user_id\":\"meunome\",\n\"start_time\": 1615324496000,\n\"tags\":{\n\"metrics\":{\"key\":\"f1\",\"value\":1},\n\"params\":{\"key\":\"param1\",\"value\":\"1\"}\n}\n}\n\n\nO got one\u00a0 error because the tags are not correct set. So, how to set the tags correct?\n\nThis is the error:\n\n<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n<title>500 Internal Server Error<\/title>\n<h1>Internal Server Error<\/h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or\nthere is an error in the application.<\/p>",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"MLflow 0.9.0 Released!",
        "Question_creation_time":1553788560000,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/nmStGjJT720",
        "Question_upvote_count":null,
        "Question_view_count":21.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":null,
        "Question_body":"MLflow 0.9.0 has been released!\n\n\nMLflow 0.9.0 introduces several major features:\nSupport for running MLflow Projects in Docker containers.\nDatabase stores for the MLflow Tracking Server.\nSimplified custom Python model packaging.\nPlugin systems allowing third party libraries to extend MLflow functionality.\nSupport for HTTP authentication to the Tracking Server in the R client.\nAnd a few breaking changes:\n[Scoring] The pyfunc scoring server now expects requests with the application\/json content type to contain json-serialized pandas dataframes in the split format, rather than the records format. Also, when reading the pandas dataframes from JSON, the scoring server no longer automatically infers data types as it can result in unintentional conversion of data types.\n[API] Removed GetMetric & GetParam from the REST API as they are subsumed by GetRun.\nFor a comprehensive list of features, see the release change log , and check out the latest documentation on mlflow.org .",
        "Tool":"MLFlow",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to start a tensorboard for all runs that are part of a hyperparameter tuning operation",
        "Question_creation_time":1651747709000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1503",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"From slack\n\nCurrently, I am selecting all runs from the table and then creating a multi-run downstream operation. with the id of those runs.\nIdeally, there should be a simple way to start such multi-run tensorboard without manually selecting the runs, especially when the number of run of the hyperparameter operation is very large, at least larger than the maximum number of runs allowed by the pagination: 50.",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Issue building docker image using example build config",
        "Question_creation_time":1618113122000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1303",
        "Question_upvote_count":1.0,
        "Question_view_count":null,
        "Question_answer_count":14,
        "Question_has_accepted_answer":null,
        "Question_body":"Describe the bug\n\nAttempting to deploy the docker image build yaml file available under https:\/\/github.com\/polyaxon\/polyaxon-examples\/blob\/master\/in_cluster\/build_image\/build-ml.yaml using the polyaxon cli returns the following error:\n\n>polyaxon run -f build_docker_image.yaml -l\nPolyaxonfile is not valid.\nError message: The Polyaxonfile's version specified is not supported by your current CLI.Your CLI support Polyaxonfile versions between: 1.1 <= v <= 1.1.You can run `polyaxon upgrade` and check documentation for the specification..\n\nTo reproduce\n\nThe contents of build_docker_image.yaml\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: python:3.8.8-buster\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nExpected behavior\n\nA build operation should begin that results in an image being saved to the docker registry connected under destination: connection:\n\nEnvironment\n\nPolyaxon 1.8.0\nPolyaxon CLI 1.8.0",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":1271.0
    },
    {
        "Question_title":"Image push with Kaniko stuck on ContainersNotReady containers with unready status: [polyaxon-main polyaxon-sidecar]",
        "Question_creation_time":1619182392000,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1296",
        "Question_upvote_count":2.0,
        "Question_view_count":null,
        "Question_answer_count":1,
        "Question_has_accepted_answer":null,
        "Question_body":"This question was resolved and discussed on Polyaxon Slack. Posting for visibility if someone stumbles upon the same issue.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd credentials setup from Kaniko github: Pushing to Amazon ECR\nAnd Kaniko integration in polyaxon-config.yml:\nconnections:\n  - name: docker-registry\n    kind: registry\n    description: \"aws docker repository\"\n    schema:\n      url: https:\/\/ID.dkr.ecr.SOME-REGION.amazonaws.com\n    secret:\n      name: docker-conf\n      mountPath: \/kaniko\/.docker\nAnd polyaxonfile.yml from polyaxon example:\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: docker-registry\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"tensorflow\/tensorflow:2.0.1-py3\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\",\"polytune\"]'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\nThen warning was raised:\n\nand job would be stuck like this until manually stopped.\n\nTYPE     STATUS    REASON              MESSAGE                                                           LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n-------  --------  ------------------  ----------------------------------------------------------------  ------------------  ----------------------\nwarning  True      ContainersNotReady  containers with unready status: [polyaxon-main polyaxon-sidecar]  a few seconds ago   a few seconds ago",
        "Tool":"Polyaxon",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0
    },
    {
        "Question_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Question_creation_time":1645930620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/td-p\/397685\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":441.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello,I am trying to run a Custom Training Job in the Vertex AI Training service.The job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).When I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"gcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.Granted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?Please note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image. Thanks,\nurig",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Cloud Translate API",
        "Question_creation_time":1668394320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API\/td-p\/489124\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi,i am use free trails of Google Cloud Translate send request on this URL  https:\/\/translation.googleapis.com\/language\/translate\/v2 with API key with body rowget error response kindly see below\"code\": 403,\n\"message\": \"The request is missing a valid API key.\",\n\"errors\":\n\n\"message\": \"The request is missing a valid API key.\",\n\"domain\": \"global\",\n\"reason\": \"forbidden\"\n\"status\": \"PERMISSION_DENIED\"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to determin which GCP VM do I need for ML",
        "Question_creation_time":1658842140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/td-p\/447075\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":73.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi to allIm trying to run a procedure looking to reduce the number of features for a model.The first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM n1-highmem-8 that has: GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)and still not getting the process done.The question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to create model",
        "Question_creation_time":1659578880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-create-model\/td-p\/450348\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":75.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am working on demand forecasting where my timestamp duration is 15 minutes and i have attached sample output to below documents.The issue i am facing is despite setting DATA_FREQUENCY = [AUTO_FREQUENCY].ii am getting the error \"Invalid time series: the finest data frequency supported is PER_MINUTE. All input time intervals must be at least one minute\" and the query for create model is given below  ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"WARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it",
        "Question_creation_time":1659595500000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-BlockManager-Block-rdd-6-0-already-exists-on-this-machine\/td-p\/450462\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi there.\nI am working with Vertex AI Jupyterlab Notebook.\nThere were a few such warningson this as the model was getting trained.\nMay I know if we are safe to ignore this?\nWhat do they mean actually?\nThanks in advance.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Version 2 model in natural language API",
        "Question_creation_time":1667301900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Version-2-model-in-natural-language-API\/td-p\/484641\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, could anyone share the python code on how to get  natural language API to use version 2 classify text  categories?I can get it working well with the default (version 1) categories but can't figure out where to adapt the standard code (as here: https:\/\/cloud.google.com\/natural-language\/docs\/samples\/language-classify-text-tutorial-classify?hl=e...)  to  use model version 2.Many thanks ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google translate API gave a mixed language translation result?",
        "Question_creation_time":1657498980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-translate-API-gave-a-mixed-language-translation-result\/td-p\/440841\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":105.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am using Google Translate API for translating a Japanese sentence to Portuguese.On July 6 to 8, the translation result was a sentence of mixed English and Portuguese words, but on July 9 the result seems to be a correct Portuguese sentence.Was there any event on July 6 to 8 such that Google Translate API gave a mixed language translation result?Thank you for your time. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Google Translate API and Serbian Latin script",
        "Question_creation_time":1665455160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-and-Serbian-Latin-script\/td-p\/476723\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":128.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi there,In Serbia we are using 2 scripts side by side - Cyrillic and Latin script.I am heaving an issue with translation to Serbian Latin.\nBy default Google translate offer translation to Serbian Cyrillic , but bellow that default translation there is a translation to Serbian Latin.\nTake a look at this example:\nhttps:\/\/translate.google.com\/?sl=en&tl=sr&text=Hello%20world!&op=translateI have found this post from early 2019.\nhttps:\/\/support.google.com\/translate\/thread\/1836538?hl=enLike in that post my question is the same:\nI need it to support Serbian Latin, for some projects I don`t use the Cyrillic script. Also there is a problem with translating pages or similar plugins, e.g.: Google Language Translator for WordPress and some others CMS system like Kopage you can translate only to Serbian Cyrillic script.As I found this post on stackoverflow:\nhttps:\/\/stackoverflow.com\/questions\/73699065\/google-cloud-translate-serbian-latin-not-workingIt seems, according to the poster of that article, that there was a workaround.\nInstead of \"sr\" ISO-639 code you can put \"sr_Latn\" - and you will get translation into Serbian Latin script.\nBut that workaround stop working several weeks ago - according to the poster.Is there a workaround to translate into Serbian Latin characters rather into Serbian Cyrillic characters?Regards,\nBranislav",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoML Vision - Error type - No valid preprocessed examples",
        "Question_creation_time":1641909300000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Vision-Error-type-No-valid-preprocessed-examples\/td-p\/183067\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I have set up an Image classification (Single-label).The model trained for 18 min 25 sec before I recieved the following error:Due to one or more errors, this training job was canceled on Jan 11, 2022 at 07:34AM Batch prediction job GAF-prediction-test encountered the following errors: No valid preprocessed examples.There is no documentation that I could find that explains this error type. Anyone with any ideas what this means?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Should I custom split my image data?",
        "Question_creation_time":1625459160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Should-I-custom-split-my-image-data\/td-p\/163031\/jump-to\/first-unread-message",
        "Question_upvote_count":1.0,
        "Question_view_count":461.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML? And what difference does it make?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using RegEx Entity with other entitiy in Traning Phrase gives empty value for the parameter",
        "Question_creation_time":1666145760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-RegEx-Entity-with-other-entitiy-in-Traning-Phrase-gives\/td-p\/479671\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":101.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"n Dialogflow ES I am using a Regular Expression for the date format (DD\/MM\/YYYY) validation. If I have the training phrase as 22\/05\/2021 it works perfectly.But I need to have the training phrase as 22\/05\/2021 sample@sample.com. When I use like that, for the date it gives empty value.Not even for this mydate validation, this is not working if I use any RegEx entity along with other entity in the same training phrase.myDigit RegEx is ^[0-9]$So can't it be used Regex entity along with other entity in the same training phrase in Dialogflow?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Issues with importing aiplatform",
        "Question_creation_time":1668388200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/td-p\/489087\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":93.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.When running \"from google.cloud import aiplatform\", I get the following error message:The versions of the concerned libraries are shown below.I have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?Thank you!",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"enhanced speech feature",
        "Question_creation_time":1629336840000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/enhanced-speech-feature\/td-p\/167747\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":414.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi I have a queryIn Dailogflow if we enable enhanced speech feature, specifically, credit card info (i.e. number), if that is spoken by user, is that stored by Google. Please help",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"cloud vision API",
        "Question_creation_time":1653293640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cloud-vision-API\/td-p\/425445\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":123.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hello everyone,\nMy question is really hypothetical.  I am right now using Cloud vision api with feature web detection. Desc: Detect topical entities such as news, events, or celebrities within the image, and find similar images on the web using the power of Google Image Search.Does profile login effects the results that web detection will return. For example if I type in google search python, I will get python programming language, while my mum will snake. Does the same logic works for cloud vision API web detection ? \n\nWhy this question even raised, because same image executed from 2 different profiles ( I have 2 accounts on cloud Vision API) the model return different probabilities. ",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"exporting a google autoML translate model",
        "Question_creation_time":1664280720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/td-p\/471646\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
        "Question_creation_time":1638115980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI\/td-p\/176629\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":74.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deep Reinforcement Learning",
        "Question_creation_time":1649832960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Reinforcement-Learning\/td-p\/413277\/jump-to\/first-unread-message",
        "Question_upvote_count":0.0,
        "Question_view_count":73.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":1.0,
        "Question_body":"Hi is it possible to implement Deep Reinforcement Learning for structured data frames? If son can someone help me with an example?",
        "Tool":"Vertex AI",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb.log inconsistent behavior with step parameter",
        "Question_creation_time":1658196834539,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-log-inconsistent-behavior-with-step-parameter\/2771",
        "Question_upvote_count":1.0,
        "Question_view_count":94.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Why does these code snippets produce different results?<\/p>\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    \nfor i in range(100):\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/ioofli05?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">for i in range(100):\n    wandb.log({\"train\/loss\": i}, step=i)\n    wandb.log({\"val\/loss\": i**2}, step=i)\n<\/code><\/pre>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/626b9c1d1ceb6cb5d3bb90cf6ab8d2894a6b8b14.png\" class=\"thumbnail onebox-avatar\" width=\"128\" height=\"128\">\n\n<h3><a href=\"https:\/\/wandb.ai\/dminn\/WandbHelp\/runs\/146hdnar?workspace=user-dminn\" target=\"_blank\" rel=\"noopener\">dminn<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Getting `AttributeError : 'NoneType' object has no attribute '_log' `when trying to run test set",
        "Question_creation_time":1662745940278,
        "Question_link":"https:\/\/community.wandb.ai\/t\/getting-attributeerror-nonetype-object-has-no-attribute-log-when-trying-to-run-test-set\/3090",
        "Question_upvote_count":0.0,
        "Question_view_count":215.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p><strong>Framework: Pytorch<\/strong><br>\n<strong>wandb version : 0.13.3<\/strong><br>\n<strong>workspace: Google colab<\/strong><\/p>\n<pre><code class=\"lang-python\">config = dict(\n    dropout = 0.4,\n    train_batch = 3,\n    val_batch = 1,\n    test_batch = 1,\n    learning_rate = 0.001,\n    epochs = 5,\n    architecture = \"CNN\",\n    model_name = \"efficientnet-b0\",\n    infra = \"Colab\",\n    dataset=\"dysphagia_dataset2\"\n    )\n\n<\/code><\/pre>\n<p>My test function<\/p>\n<pre><code class=\"lang-auto\">def test_model():\n    running_correct = 0.0\n    running_total = 0.0\n    true_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data in dataloaders[TEST]:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.append(labels.item())\n            outputs = model_ft(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            pred_labels.append(preds.item())\n            running_total += labels.size(0)\n            running_correct += (preds == labels).sum().item()\n        acc = running_correct\/running_total\n    return (true_labels, pred_labels, running_correct, running_total, acc)\n\n\ntrue_labels, pred_labels, running_correct, running_total, acc = test_model()\n\n<\/code><\/pre>\n<p><strong>Error<\/strong><\/p>\n<pre><code class=\"lang-bash\">AttributeError                            Traceback (most recent call last)\n\n&lt;ipython-input-26-b7dbeaddcbbb&gt; in &lt;module&gt;\n----&gt; 1 true_labels, pred_labels, running_correct, running_total, acc = test_model()\n      2 \n\n4 frames\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    254             bins = torch.Tensor(bins_np)\n    255 \n--&gt; 256         wandb.run._log(\n    257             {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))},\n    258             commit=False,\n\nAttributeError: 'NoneType' object has no attribute '_log'\n<\/code><\/pre>\n<p>This is how i initialize training:<\/p>\n<pre><code class=\"lang-python\">model_ft = train_model(model_ft, \n                       criterion, \n                       optimizer_ft,\n                       config\n                       )\n<\/code><\/pre>\n<p>my wandb init:<\/p>\n<pre><code class=\"lang-python\">wandb.init(config=config,\n           name='efficientnet0+albumentions',\n           group='pytorch-efficientnet-baseline', \n           project='dysphagia_image_classification',\n           job_type='train')\nconfig = wandb.config\n\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Code Comparer can only show the difference of main training file",
        "Question_creation_time":1661772961165,
        "Question_link":"https:\/\/community.wandb.ai\/t\/code-comparer-can-only-show-the-difference-of-main-training-file\/3020",
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>If I call <code>wandb.run.log_code(\".\")<\/code>, all python source code files in the current directory are saved in W&amp;B cloud. That\u2019s what I want.<\/p>\n<p>However, only changes in main training file where I call <code>wandb.init()<\/code> can be shown in <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/code#code-comparer\">Code Comparer<\/a>. The change in other file (like <code>helper_funcs.py<\/code>) will not appear in code panel. Do you have any suggestions about it?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb Local on Singularity?",
        "Question_creation_time":1643215979794,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-local-on-singularity\/1820",
        "Question_upvote_count":2.0,
        "Question_view_count":100.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Has anyone found a good way to host wandb local on singularity rather than Docker? My understanding is that many institutions, mine included, do not allow use of docker because of security flaws within it.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Adding tags cause internal server error",
        "Question_creation_time":1667338403355,
        "Question_link":"https:\/\/community.wandb.ai\/t\/adding-tags-cause-internal-server-error\/3363",
        "Question_upvote_count":1.0,
        "Question_view_count":32.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Adding tags to any project in my account will cause an internal server error. Is there an outage or it\u2019s something specific to my account?<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/cchi\/tags_test_project\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/cchi\/tags_test_project\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/7\/7e3d01bfbb9f37b8c9af3076e9688c9bef1b6347.png\" class=\"thumbnail onebox-avatar\" width=\"120\" height=\"120\">\n\n<h3><a href=\"https:\/\/wandb.ai\/cchi\/tags_test_project\" target=\"_blank\" rel=\"noopener\">cchi<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png\" data-download-href=\"\/uploads\/short-url\/o8iWO0KA8b01xdqNjqP8vCU1eUh.png?dl=1\" title=\"Screen Shot 2022-11-01 at 5.32.04 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051_2_690x258.png\" alt=\"Screen Shot 2022-11-01 at 5.32.04 PM\" data-base62-sha1=\"o8iWO0KA8b01xdqNjqP8vCU1eUh\" width=\"690\" height=\"258\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051_2_690x258.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png 2x\" data-dominant-color=\"FBF3F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-11-01 at 5.32.04 PM<\/span><span class=\"informations\">789\u00d7296 13.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Connecting to existing sweep from Python",
        "Question_creation_time":1641567235002,
        "Question_link":"https:\/\/community.wandb.ai\/t\/connecting-to-existing-sweep-from-python\/1721",
        "Question_upvote_count":1.0,
        "Question_view_count":177.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I have a few questions regarding the hyperparameter sweeps from Python.<br>\nI am wanting to essentially start a few tmux sessions on my server, and connect them all to the same sweep agent, but no keyword in the sweep_config (that i have found) allow me to connect to a specific sweep ID, and rather just a sweep name that doesnt connect to the same sweep, but just makes multiple sweeps of the same name.  If this possible or strongly advised against due to computational usage or similar?<\/p>\n<p>Furthermore, sweeps take up a great deal of storage requirements due to saving all the models, is it possible to store the model file from the best model only, while keeping the statistics from all the models for plots and interpretation? This would allow me to keep the great information gathered from sweeps, while not taking up 100+ GB from a single sweep.<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging a table from a list of python dicts",
        "Question_creation_time":1657037898222,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-a-table-from-a-list-of-python-dicts\/2701",
        "Question_upvote_count":1.0,
        "Question_view_count":69.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Is it possible to log tables to WANDB from a list \/sequence of dicts where the keys are the column names and the values wandb.Images (for example)?<\/p>\n<p>Once the my tables are logged to WANDB, if they share a column, can I join them in the WANDB web GUI?<\/p>\n<p>Thanks  beforehand!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Exporting GPU utilization, power usage data",
        "Question_creation_time":1649321943188,
        "Question_link":"https:\/\/community.wandb.ai\/t\/exporting-gpu-utilization-power-usage-data\/2194",
        "Question_upvote_count":0.0,
        "Question_view_count":280.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I\u2019ve been wanting to export the data on GPU usage for my algorithm, but when I export the CSV file there is a single line which does not contain all the data.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42.png\" data-download-href=\"\/uploads\/short-url\/pk2t25q25HlYQzj2LOJ1W7xb1e2.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42.png\" alt=\"image\" data-base62-sha1=\"pk2t25q25HlYQzj2LOJ1W7xb1e2\" width=\"690\" height=\"35\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1893\u00d797 7.21 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>For some reason all other data exports work, but any data which has to do with the GPU does not. I have 4 GPUs. The plot shows the right data:<\/p>\n<p>I could also not find the complete data using the API. Is this a bug?<\/p>\n<p>Best,<\/p>\n<p>Mario<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Save_period Not Working",
        "Question_creation_time":1636684136179,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-period-not-working\/1264",
        "Question_upvote_count":1.0,
        "Question_view_count":197.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I\u2019m trying to train a model, but I keep receiving an error that tells me \u201ctrain.py: error: unrecognized arguments: --save_period 1.\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png\" data-download-href=\"\/uploads\/short-url\/8oz1v5B3P0W95o6FWMDht4yfVLD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png\" alt=\"image\" data-base62-sha1=\"8oz1v5B3P0W95o6FWMDht4yfVLD\" width=\"690\" height=\"337\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_1035x505.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1326\u00d7648 66 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhat issue do I have here?  Thanks in advance.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No config file and system plots for offline runs",
        "Question_creation_time":1651135719417,
        "Question_link":"https:\/\/community.wandb.ai\/t\/no-config-file-and-system-plots-for-offline-runs\/2337",
        "Question_upvote_count":0.0,
        "Question_view_count":318.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>When uploading offline runs, there is no config shown in the dashboard.<br>\nAdditionally, there are also no system plots.<\/p>\n<p>Is there any way to fix this issue?<\/p>\n<p>Thank you very much for your help!<br>\nCedric<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Question_creation_time":1660719705149,
        "Question_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Question_upvote_count":2.0,
        "Question_view_count":68.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Logging Metrics for each sample per epoch",
        "Question_creation_time":1656534480479,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-metrics-for-each-sample-per-epoch\/2678",
        "Question_upvote_count":0.0,
        "Question_view_count":50.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hey everyone,<br>\nLet\u2019s say that I have a dataset with 50000 samples and I am training my model for 10 epochs. Now, in each epoch, I am recording the <em>per sample loss<\/em> (i.e. loss of each sample - Not the average loss of all samples). This means that there are 50000 loss values per epoch. I want to log these values for <em>each epoch<\/em>, so that I can later perform some analysis on how the loss values for the samples change as training progresses (And, if possible, observe the loss values of a particular sample across epochs). For reference, <a href=\"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/62000dee5a05a6a71de3a6127a68778a-Paper.pdf\" rel=\"noopener nofollow ugc\">this<\/a> paper tracks such  statistics. Here are two ways I can think of doing this -<\/p>\n<ul>\n<li>A simple way to do this is to update the values in a 50000x10 array, then log the array as a table at the end of training (I would obviously need to track which indices belong to which samples). However, I need to wait for the training to end in this scenario.<\/li>\n<li>I can also log each sample\u2019s statistic with wandb.log (Maybe put them under \u201csample_statistics\/\u201d to pull them more easily). This ensures that the metrics are logged as and when they are observed, however, I am not sure if this is the most optimal solution.<\/li>\n<\/ul>\n<p>Is there any other way in which I can do this so that I can analyse the resulting data effectively? Open to all suggestions!<br>\nThank you!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb.plot.confusion_matrix() just show a Table!",
        "Question_creation_time":1641781829922,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744",
        "Question_upvote_count":0.0,
        "Question_view_count":381.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I used this code to create a confusion matrix:<\/p>\n<pre><code class=\"lang-auto\"># confusion matrix\n        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<p>However, Wanda\u2019s website only shows a table instead of the confusion matrix. This is a screenshot from the issue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046.png\" data-download-href=\"\/uploads\/short-url\/8UGiFwpsOZ6Pivp7qmFXgL1OuvI.png?dl=1\" title=\"Screenshot from 2022-01-09 20-58-35\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png\" alt=\"Screenshot from 2022-01-09 20-58-35\" data-base62-sha1=\"8UGiFwpsOZ6Pivp7qmFXgL1OuvI\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-01-09 20-58-35<\/span><span class=\"informations\">1741\u00d7629 29.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"String bug in the parallel coordinates chart",
        "Question_creation_time":1641230890493,
        "Question_link":"https:\/\/community.wandb.ai\/t\/string-bug-in-the-parallel-coordinates-chart\/1674",
        "Question_upvote_count":1.0,
        "Question_view_count":148.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hey Guys, i just started using wandb and so far everything is working pretty well, however I noticed that there seems to be a problem with strings as parameters in parallel coordinates chart<\/p>\n<p>Attached is a screenshot to illustrate the problem<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7.jpeg\" data-download-href=\"\/uploads\/short-url\/yfjVQxyNGXrfluwDks1707BnsO3.jpeg?dl=1\" title=\"wandb_string_problem\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_690x374.jpeg\" alt=\"wandb_string_problem\" data-base62-sha1=\"yfjVQxyNGXrfluwDks1707BnsO3\" width=\"690\" height=\"374\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_690x374.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_1035x561.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_1380x748.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f0054aff74ba40b8cb33e86b18b172e51cd527c7_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wandb_string_problem<\/span><span class=\"informations\">1502\u00d7816 332 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>So now I wonder if I made a mistake or if I have to wait for a fix from you.<\/p>\n<p>Kind regards<br>\nChris<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Filtering runs by job_type using API is not working",
        "Question_creation_time":1667921959400,
        "Question_link":"https:\/\/community.wandb.ai\/t\/filtering-runs-by-job-type-using-api-is-not-working\/3390",
        "Question_upvote_count":1.0,
        "Question_view_count":34.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi everyone!<\/p>\n<p>I am trying to retrieve  filtered runs from a project using the WandB API and a filter dictionary.<\/p>\n<p>I try to do the following:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nfilter_dict = {\"job_type\":  \"my_job_type\"}\nruns = api.runs(\"my_entity\/my_project\", filters=filter_dict)\nfor run in runs:\n    print(run)\n<\/code><\/pre>\n<p>When I do this, I get the following error message:<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 980, in __next__\n    if not self._load_page():\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 965, in _load_page\n    self.last_response = self.client.execute(\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 168, in wrapped_fn\n    return retrier(*args, **kargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\retry.py\", line 108, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\apis\\public.py\", line 207, in execute\n    return self._client.execute(*args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\wandb\\vendor\\gql-0.2.0\\wandb_gql\\transport\\requests.py\", line 39, in execute\n    request.raise_for_status()\n  File \"C:\\Users\\KoljaBauer\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\n<\/code><\/pre>\n<p>However, other filters do work. For example I can do the above described procedure with<\/p>\n<pre><code class=\"lang-auto\">filter_dict = {\"group\":  \"my_group\"}\n<\/code><\/pre>\n<p>and it yields the correctly filtered jobs.<\/p>\n<p>What I am currently doing as a workaround is this:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nruns = api.runs(\"my_entity\/my_project\")\nfor run in runs:\n    if run.job_type == \"my_job_type\":\n        print(run)\n<\/code><\/pre>\n<p>However, I would prefer to directly filter the runs with the API call. Any idea what I am doing wrong?<\/p>\n<p>Thanks in advance!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi-level nesting in yaml for sweeps",
        "Question_creation_time":1663098229093,
        "Question_link":"https:\/\/community.wandb.ai\/t\/multi-level-nesting-in-yaml-for-sweeps\/3108",
        "Question_upvote_count":0.0,
        "Question_view_count":406.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I am trying to start a sweep using this yaml file.<\/p>\n<p>sweep.yaml<\/p>\n<pre><code class=\"lang-auto\">method: bayes\nmetric:\n  goal: maximize\n  name: val_f1_score\nparameters:\n  notes:\n    value: \"\"\n  seed:\n    value: 42\n  lr:\n    values: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n  epochs:\n    value: 30\n  augmentation:\n    value: True\n  class_weights:\n    value: True\n  optimizer:\n    value: adam\n  loss:\n    value: categorical_crossentropy\n  metrics:\n    value: [\"accuracy\"]\n  batch_size:\n    value: 64\n  num_classes:\n    value: 7\n  paths:\n    - \n      data:\n        value: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  -\n    use:\n      value: True\n    project:\n      value: Whats-this-rock\n\ndataset:\n  -\n    id:\n      value: [1, 2, 3, 4]\n    dir:\n      value: data\/3_consume\/\n    image:\n      size:\n        value: 124\n      channels:\n        value: 3\n    classes:\n      value: 10\n    sampling:\n      value: None\n\nmodel:\n  -\n    backbone:\n      value: efficientnetv2m\n    use_pretrained_weights:\n      value: True\n    trainable:\n      value: True\n    preprocess:\n      value: True\n    dropout_rate:\n      value: 0.3\n\ncallback:\n  -\n    monitor:\n      value: \"val_f1_score\"\n    earlystopping:\n      patience:\n        value: 10\n    reduce_lr:\n      factor:\n        values: [.9, .7, .5]\n      min_lr: 0.00001\n      patience:\n        values: [1, 2, 3, 4]\n    save_model:\n      status:\n        value: True\n      best_only:\n        value: True\n\nprogram: src\/models\/train.py\n\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">Error: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>Here\u2019s the full traceback of the error:-<\/p>\n<pre><code class=\"lang-auto\">During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 97, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/cli\/cli.py\", line 942, in sweep\n    launch_scheduler=_launch_scheduler_spec,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/internal.py\", line 102, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 62, in wrapper\n    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/apis\/normalize.py\", line 26, in wrapper\n    return func(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2178, in upsert_sweep\n    raise e\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2175, in upsert_sweep\n    check_retry_fn=no_retry_4xx,\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/lib\/retry.py\", line 129, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/internal\/internal_api.py\", line 2153, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.CommError: Invalid sweep config: invalid hyperparameter configuration: paths\n<\/code><\/pre>\n<p>I am using hydra and trying to replicate a config.yaml for wandb sweeps<\/p>\n<p>config.yaml<\/p>\n<pre><code class=\"lang-auto\">notes: \"\"\nseed: 42\nlr: 0.001\nepochs: 30\naugmentation: True\nclass_weights: True\noptimizer: adam\nloss: categorical_crossentropy\nmetrics: [\"accuracy\"]\nbatch_size: 64\nnum_classes: 7\n\npaths:\n  data: ${hydra:runtime.cwd}\/data\/4_tfds_dataset\/\n\nwandb:\n  use: True\n  project: Whats-this-rock\n\ndataset:\n  id: [1, 2, 3, 4]\n  dir: data\/3_consume\/\n  image:\n    size: 124\n    channels: 3\n  classes: 10\n  sampling: None\n\nmodel:\n  backbone: efficientnetv2m\n  use_pretrained_weights: True\n  trainable: True\n  preprocess: True\n  dropout_rate: 0.3\n\ncallback:\n  monitor: \"val_f1_score\"\n  earlystopping:\n    patience: 10\n  reduce_lr:\n    factor: 0.4\n    min_lr: 0.00001\n    patience: 2\n  save_model:\n    status: True\n    best_only: True\n\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Additional System Metrics From e.g., `dstat`",
        "Question_creation_time":1651079776562,
        "Question_link":"https:\/\/community.wandb.ai\/t\/additional-system-metrics-from-e-g-dstat\/2333",
        "Question_upvote_count":1.0,
        "Question_view_count":107.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi W&amp;B Community,<\/p>\n<p>Is there a possibility to get additional live system metrics like the network read\/write rates, disk read\/write rates, virtual memory major\/minor page faults, filesystem inodes, and system context switches?<\/p>\n<p>Basically, most of the metrics that dstat provides with the following flags:<\/p>\n<ul>\n<li>\u2013disk<\/li>\n<li>\u2013mem (memory)<\/li>\n<li>\u2013net (network)<\/li>\n<li>\u2013sys (system)<\/li>\n<li>\u2013fs (filesystem)<\/li>\n<li>\u2013vm (virtual memory)<\/li>\n<\/ul>\n<p>I\u2019m deep into pipeline profiling and found that having these helps a lot when looking for performance tuning opportunities. Also, allowing to add to the system metric log might be helpful generally to have everything related to actual ML in one log, and everything related to system metrics in another.<\/p>\n<p>I saw that the <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/system-metrics\">current documentation<\/a> suggests that you use this script - github(.)com\/nicolargo\/nvidia-ml-py3\/blob\/master\/pynvml.py - to get the GPU metrics, however, I did not find the system metrics there.<\/p>\n<p>The first workaround for me would be to run  <code>dstat<\/code> in parallel to the process, save the profiling log,<br>\ndownload your system metrics and join over the <code>_timestamp<\/code>. This, however, would negate your wonderful automatic visualization.<\/p>\n<p>The other solution would be to use some system monitoring library and add manually via <code>wandb.log({'my_metric': x})<\/code> to the \u201cML\u201d-log. This would show the metric in your visualization but not at the correct place and would not be easily compared to the other system metrics. I do not know how well this would work in practice as there would need to be additions to this log ideally every (few) seconds. This would be an asynchronous running thread that is not inside of the training loop. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/logging-faqs#what-if-i-want-to-log-some-metrics-on-batches-and-some-metrics-only-on-epochs\">The solution proposed here<\/a>  seems like it could work if I use \u201ctimestamps\u201d as the X-axis? This still does not seem like a clean solution.<\/p>\n<p>What are your thoughts on this proposed feature? I\u2019m very much a novice regarding your service so I might not know the in\u2019s and out\u2019s, maybe I have overlooked some trivial solution.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Remove multiple runs at the same time",
        "Question_creation_time":1652776776526,
        "Question_link":"https:\/\/community.wandb.ai\/t\/remove-multiple-runs-at-the-same-time\/2435",
        "Question_upvote_count":1.0,
        "Question_view_count":99.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<\/p>\n<p>I think it would be beneficial to select and delete several experiments at the same time.<br>\nNow I have to delete one by one and it is very time consuming.<\/p>\n<p>Thank you!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to change job_type?",
        "Question_creation_time":1650192014093,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-there-a-way-to-change-job-type\/2253",
        "Question_upvote_count":1.0,
        "Question_view_count":90.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hiya!<\/p>\n<p>Last night I ran an experiment and didn\u2019t bother to check the inputs to <code>wandb.init<\/code>. It turned out later that I mixed some things up and there was a mistake in <code>job_type<\/code> kwarg.  Now I am wondering is there a way to change this parameter (I really need it for grouping) through API or UI?<\/p>\n<p>Thanx<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cannot connect to Amazon S3 from self hosted wandb",
        "Question_creation_time":1668028369307,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-connect-to-amazon-s3-from-self-hosted-wandb\/3399",
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I\u2019m trying to setup a self hosted wandb on k8s using helm charts. Unfortunately, I am not able to connect to my Amazon S3.<\/p>\n<p>I tried two ways:<\/p>\n<ol>\n<li>\n<p>Based on the example here <a href=\"https:\/\/docs.wandb.ai\/guides\/self-hosted\/setup\/on-premise-baremetal\" class=\"inline-onebox\">On Prem \/ Baremetal - Documentation<\/a>, I used the format:<br>\ns3:\/\/myaccess:myseceret@s3.amazonaws.com\/ofer-bucket-1<br>\nHowever when the wandb pod starts, it says that the URL is not valid, as \u201c:mysecret\u201d is not a valid port.<br>\nFor some reason it considers the secret to indicate URL port and not secret<\/p>\n<\/li>\n<li>\n<p>I also tried changing my bucket to public,  but wandb pod failed to initialize again, this time with error 403 access denied.<\/p>\n<\/li>\n<\/ol>\n<p>Anyone has an example for the correct format of the BUCKET value or can explain how it should be structured? I prefer to have it with access\/secret key. But I\u2019m ok with public as well.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Barchart Grouping by Time\/Step\/Count",
        "Question_creation_time":1663605136917,
        "Question_link":"https:\/\/community.wandb.ai\/t\/barchart-grouping-by-time-step-count\/3157",
        "Question_upvote_count":0.0,
        "Question_view_count":399.0,
        "Question_answer_count":6,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Dear W&amp;B Community,<\/p>\n<p>I have system metrics logged like the \u201c<em>time per step<\/em>\u201d or \u201c<em>time per backward pass<\/em>\u201d for a model.<br>\nWhen doing this on different hardware, I would like to compare the effect this has on these metrics.<br>\nIn the following examples, I profile the basic Torch CIFAR10 model on a 1,2,4,8,16 and 32 CPU VM.<\/p>\n<p>When looking at a <code>Linechart<\/code>, the full history of these metrics is visible, however, it is very hard to compare them due to the overlapping and oscillation:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351.png\" data-download-href=\"\/uploads\/short-url\/zZROm2jlGDN4WUrxx2lQXAA8jYZ.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_22_16 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_22_16 PM\" data-base62-sha1=\"zZROm2jlGDN4WUrxx2lQXAA8jYZ\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fc4a70f56ac98f28ece07b404e7c5e734d81d351_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_22_16 PM<\/span><span class=\"informations\">3539\u00d71859 509 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>When using a <code>Barchart<\/code>, only the last value is visualized:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/48a4597177e867b3eb511112ad23b561f18f1137.png\" data-download-href=\"\/uploads\/short-url\/amCuG3pzRgnimYoyoJeru5muDMH.png?dl=1\" title=\"W&amp;amp;B Chart 9_19_2022, 6_20_31 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png\" alt=\"W&amp;B Chart 9_19_2022, 6_20_31 PM\" data-base62-sha1=\"amCuG3pzRgnimYoyoJeru5muDMH\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_1380x724.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/48a4597177e867b3eb511112ad23b561f18f1137_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">W&amp;amp;B Chart 9_19_2022, 6_20_31 PM<\/span><span class=\"informations\">3539\u00d71859 251 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The functionality that would be nice is to group values based on their count or occurrence, as grouping by runs already works perfectly. Here\u2019s the same data but run through <code>seaborn.barplot<\/code>:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479.png\" data-download-href=\"\/uploads\/short-url\/7VTQur5SLq8cPHTQtTqGrDuwPRn.png?dl=1\" title=\"download\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png\" alt=\"download\" data-base62-sha1=\"7VTQur5SLq8cPHTQtTqGrDuwPRn\" width=\"690\" height=\"427\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_690x427.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1035x640.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_1380x854.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/379ac43b0eee017cfc2884cd7a3635262eb5d479_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">download<\/span><span class=\"informations\">3777\u00d72341 159 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Would this be possible to implement? Or does anybody know a way to get that functionality?<\/p>\n<p>My current workaround is to download the data manually and run it through seaborn. Unfortunately, I did not understand the errors I\u2019ve gotten with the <code>Custom Chart<\/code> functionality when trying to port Vega examples to use wandb as a data basis.<\/p>\n<p>I\u2019d be very glad if anybody can point me to a tutorial on how to migrate existing Vega examples to be used with wandb (and the common problems, like differences between v3\/v4\/v5, as these seemed to be an issue for me).<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Track power\/energy consumption?",
        "Question_creation_time":1658216876213,
        "Question_link":"https:\/\/community.wandb.ai\/t\/track-power-energy-consumption\/2774",
        "Question_upvote_count":1.0,
        "Question_view_count":33.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hey all,<\/p>\n<p>I wondered if there was a way to track system power consumption caused by model development? I\u2019ve checked the W&amp;B docs and can\u2019t see anything.<br>\nIdeally I\u2019d love to be able to keep track of runs to see how much power is used by different runs but also the whole project.<\/p>\n<p>Elsewhere I\u2019ve seen packages such as <a href=\"https:\/\/pypi.org\/project\/energyusage\/\" rel=\"noopener nofollow ugc\">energyusage<\/a> but ideally would like to use something more integrated and could be aggregated across runs for whole projects.<br>\nIf something already exists I\u2019d love to hear about it, otherwise either if W&amp;B fancied adding this functionality that would be great or if it came to it if anyone would like to help me with this project.<\/p>\n<p>Thanks,<\/p>\n<p>Jeff.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"CI Credentials Not Tied to User",
        "Question_creation_time":1667918249037,
        "Question_link":"https:\/\/community.wandb.ai\/t\/ci-credentials-not-tied-to-user\/3388",
        "Question_upvote_count":2.0,
        "Question_view_count":38.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi there.<\/p>\n<p>We currently use wandb artifacts for model versioning during experiments. We\u2019d also like to integrate this into our production pipeline so that we can automatically pull specific model versions during builds.<\/p>\n<p>I am wondering if it\u2019s possible to get credentials that are not tied to a specific wandb user so that they don\u2019t expire if the team member that implements this happens to leave our company.<\/p>\n<p>Thanks!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I manually cancel a run in a sweep?",
        "Question_creation_time":1654794512062,
        "Question_link":"https:\/\/community.wandb.ai\/t\/can-i-manually-cancel-a-run-in-a-sweep\/2583",
        "Question_upvote_count":1.0,
        "Question_view_count":70.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Can I manually stop a run in a sweep so that the sweep agent will just continue with the next run?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to set the environment variable WANDB_IGNORE_GLOBS correctly?",
        "Question_creation_time":1668582283551,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-set-the-environment-variable-wandb-ignore-globs-correctly\/3423",
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>The usage in the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables#optional-environment-variables\">Docs<\/a> is:<\/p>\n<blockquote>\n<p>Set this to a comma separated list of file globs to ignore. These files will not be synced to the cloud<\/p>\n<\/blockquote>\n<p>So, is the below code correct?<\/p>\n<pre><code class=\"lang-python\">os.environ['WANDB_IGNORE_GLOBS'] = '[*.pth, *.npy]'\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Understanding define_metric parameters",
        "Question_creation_time":1659447036761,
        "Question_link":"https:\/\/community.wandb.ai\/t\/understanding-define-metric-parameters\/2836",
        "Question_upvote_count":2.0,
        "Question_view_count":40.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I would like to understand the difference between those two function calls:<\/p>\n<p>I am referring to the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-the-summary\">documentation of define_metric<\/a>:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">wandb.define_metric(\"acc\", summary=\"max\")\nwandb.define_metric(\"acc\", summary=\"best\", objective=\"maximize\")\n<\/code><\/pre>\n<p>Is it the \u201cbest\u201d accuracy ever measured (during training) versus the accuracy of the \u201cbest\u201d (validation) model? I understand that wandb does not care what metric I log, but what is the intended use?<\/p>\n<p>Thank you for clarification.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to export data from local run files?",
        "Question_creation_time":1660879483281,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-export-data-from-local-run-files\/2959",
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>The doc <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\">Import &amp; Export Data<\/a> gives the way how to export data from cloud. Can I use api to export data from local run files? I tried use path to local run directory instread of <code>&lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;<\/code>, but it doesn\u2019t work.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sweep error - AttributeError: 'SettingsStatic' object has no attribute 'git_root'",
        "Question_creation_time":1666905593478,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-error-attributeerror-settingsstatic-object-has-no-attribute-git-root\/3335",
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>When trying to make a new sweep I get the following error<br>\n<code>AttributeError: 'SettingsStatic' object has no attribute 'git_root'<\/code><\/p>\n<p>Seems to be repeated no matter what I try.<br>\nThe full log from <code>debug-internal.log<\/code><\/p>\n<pre><code class=\"lang-bash\">2022-10-27 21:15:00,236 INFO    StreamThr :3165542 [internal.py:wandb_internal():88] W&amp;B internal server running at pid: 3165542, started at: 2022-10-27 21:15:00.235637\n2022-10-27 21:15:00,237 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: status\n2022-10-27 21:15:00,374 DEBUG   SenderThread:3165542 [sender.py:send_request():317] send_request: status\n2022-10-27 21:15:00,376 DEBUG   SenderThread:3165542 [sender.py:send():303] send: header\n2022-10-27 21:15:00,376 INFO    WriterThread:3165542 [datastore.py:open_for_write():75] open: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/run-xj06c5a6.wandb\n2022-10-27 21:15:00,376 DEBUG   SenderThread:3165542 [sender.py:send():303] send: run\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [dir_watcher.py:__init__():216] watching files in: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [sender.py:_start_run_threads():928] run started: xj06c5a6 with start time 1666905300.0\n2022-10-27 21:15:00,760 DEBUG   SenderThread:3165542 [sender.py:send():303] send: summary\n2022-10-27 21:15:00,760 INFO    SenderThread:3165542 [sender.py:_save_file():1171] saving file wandb-summary.json with policy end\n2022-10-27 21:15:00,761 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: check_version\n2022-10-27 21:15:00,761 DEBUG   SenderThread:3165542 [sender.py:send_request():317] send_request: check_version\n2022-10-27 21:15:01,040 DEBUG   HandlerThread:3165542 [handler.py:handle_request():138] handle_request: run_start\n2022-10-27 21:15:01,044 DEBUG   HandlerThread:3165542 [meta.py:__init__():34] meta init\n2022-10-27 21:15:01,381 INFO    WriterThread:3165542 [datastore.py:close():279] close: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/run-xj06c5a6.wandb\n2022-10-27 21:15:01,761 INFO    Thread-14 :3165542 [dir_watcher.py:_on_file_created():275] file\/dir created: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json\n2022-10-27 21:15:02,031 INFO    SenderThread:3165542 [sender.py:finish():1331] shutting down sender\n2022-10-27 21:15:02,031 INFO    SenderThread:3165542 [dir_watcher.py:finish():362] shutting down directory watcher\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():392] scan: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():406] scan save: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json wandb-summary.json\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [dir_watcher.py:finish():406] scan save: \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/config.yaml config.yaml\n2022-10-27 21:15:02,762 INFO    SenderThread:3165542 [file_pusher.py:finish():168] shutting down file pusher\n2022-10-27 21:15:02,764 INFO    SenderThread:3165542 [file_pusher.py:join():173] waiting for file pusher\n2022-10-27 21:15:04,333 INFO    Thread-19 :3165542 [upload_job.py:push():143] Uploaded file \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/config.yaml\n2022-10-27 21:15:04,349 INFO    Thread-18 :3165542 [upload_job.py:push():143] Uploaded file \/home\/cin\/Projects\/MotionLatentSpace\/wandb\/run-20221027_211500-xj06c5a6\/files\/wandb-summary.json\n2022-10-27 21:15:04,954 ERROR   StreamThr :3165542 [internal.py:wandb_internal():163] Thread HandlerThread:\nTraceback (most recent call last):\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 263, in _process\n    self._hm.handle(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 130, in handle\n    handler(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 140, in handle_request\n    handler(record)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/handler.py\", line 672, in handle_request_run_start\n    run_meta = meta.Meta(settings=self._settings, interface=self._interface)\n  File \"\/home\/cin\/.local\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/meta.py\", line 40, in __init__\n    root=self._settings.git_root,\nAttributeError: 'SettingsStatic' object has no attribute 'git_root'\n<\/code><\/pre>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Update offline run before syncing",
        "Question_creation_time":1658741360433,
        "Question_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Question_upvote_count":0.0,
        "Question_view_count":64.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb integration using class and sweep running twice under the same name",
        "Question_creation_time":1652719487380,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-integration-using-class-and-sweep-running-twice-under-the-same-name\/2433",
        "Question_upvote_count":0.0,
        "Question_view_count":139.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m implementing W&amp;B into an existing project in which Agent, Model creation and Environment are constructed in classes. The code structure in the Python file (<code>AIAgent.py<\/code>) looks like this:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nconfig = {\n    'layer_sizes': [17, 16, 12, 4],\n    'batch_minsize': 32,\n    'max_memory': 100_000,\n    'episodes': 2,\n    'epsilon': 1.0,\n    'epsilon_decay': 0.998,\n    'epsilon_min': 0.01,\n    'gamma': 0.9,\n    'learning_rate': 0.001,\n    'weight_decay': 0,\n    'optimizer': 'sgd',\n    'activation': 'relu',\n    'loss_function': 'mse'\n}\n\nclass AIAgent:\n    def __init__(self):\n        self.config = config\n        self.pipeline(self.config)\n\n\n    def pipeline(self, config):\n        wandb.init()\n        config = wandb.config\n\n        model, criterion, optimizer = self.make(config)\n        self.train(model, criterion, optimizer, config) \n\n\n    def make(self, config):\n        model = LinearQNet(config).to(device)\n\n        if config['loss_function'] == 'mse':\n            criterion = nn.MSELoss()\n\n        if config['optimizer'] == 'adam':\n            optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=config['weight_decay'], amsgrad=False)\n \n        wandb.watch(model, criterion, log='all', log_freq=1)\n        summary(model)\n\n        return model, criterion, optimizer\n\n\n    def train(self, model, criterion, optimizer, config):\n        for episode in range(1, config['episodes'] + 1):\n            while True:\n                # Where the training is performed\n\n                if done:\n                    if (episode % 1) == 0:\n                        wandb.log({'episode': episode, 'epsilon': epsilon, 'score': score, 'loss': loss_mean, 'reward': reward_mean, 'score_mean': score_mean, 'images': [wandb.Image(img) for img in env_images]}, step=episode})\n                    break\n\n            if episode &lt; config['episodes']:\n                game.game_reset()\n            else:\n                wandb.finish()\n                break\n\n\nclass LinearQNet(nn.Module):\n    def __init__(self, config):\n        super(LinearQNet, self).__init__()\n        self.config = config\n        # Where the NN is configured\n\n\nif __name__ == '__main__':\n    AIAgent.__init__(AIAgent())\n<\/code><\/pre>\n<p>I\u2019m currently initializing the sweep configuration via a .yaml file calling  <code>wandb sweep sweep.yaml<\/code>. The sweep.yaml file looks like this:<\/p>\n<pre><code class=\"lang-auto\">program: AIAgent.py\nproject: evaluation-sweep-1\nmethod: random\nmetric:\n  name: score_mean\n  goal: maximize\ncommand:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args}\nparameters:\n  layer_sizes:\n    distribution: constant\n    value: [17, 16, 512, 4]\n  batch_minsize:\n    distribution: int_uniform\n    max: 1024\n    min: 32\n  max_memory:\n    distribution: constant\n    value: 100_000\n  episodes:\n    distribution: constant\n    value: 50\n  epsilon:\n    distribution: constant\n    value: 1.0\n  epsilon_decay:\n    distribution: constant\n    value: 0.995\n  epsilon_min:\n    distribution: constant\n    value: 0.01\n  gamma:\n    distribution: uniform\n    max: 0.99\n    min: 0.8\n  learning_rate:\n    distribution: uniform\n    max: 0.1\n    min: 0.0001  \n  weight_decay:\n    distribution: constant\n    value: 0\n  optimizer:\n    distribution: categorical\n    values: ['sgd', 'adam', 'adamw']\n  activation:\n    distribution: categorical\n    values: ['relu', 'sigmoid', 'tanh', 'leakyrelu']\n  loss_function:\n    distribution: constant\n    value: 'mse'\nearly_terminate:\n  type: hyperband\n  min_iter: 5\n<\/code><\/pre>\n<p>Besides general feedback on the implementation I\u2019m a bit dumbfounded with a current bug. The sweeps run fine and show up in the W&amp;B interface but every sweep is performed twice under the same name of which only the loffing of the first is displayed and the second runs \u2018silently\u2019 in the environment without update of wandb.log. Does anybody have an idea what the reason for this might be?<\/p>\n<p>Thanks,<br>\nTobias<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multiple tables",
        "Question_creation_time":1659744551959,
        "Question_link":"https:\/\/community.wandb.ai\/t\/multiple-tables\/2856",
        "Question_upvote_count":0.0,
        "Question_view_count":78.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I know how to create a table with a data frame programmatically. However, I have two data frames, and they have different number of rows, so I cannot combine them into a single data frame. How do I upload two different tables to a Weights&amp;Biases project? Somehow, I suspect that the following is not the correct approach:<\/p>\n<pre><code class=\"lang-python\">    train_df = pd.DataFrame({\n        'tx':train_x,\n        'ty':train_y,\n    })\n    valid_df = pd.DataFrame({\n        'vx':valid_x,\n        'vy':valid_y\n    })\n\n    # How to add multiple tables\n\n    wandb.log({\"table\": train_df}, commit=False)\n    wandb.log({\"table\": valid_df}, commit=False)\n<\/code><\/pre>\n<p>Any help is greatly appreciated.<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access to study group",
        "Question_creation_time":1643620301251,
        "Question_link":"https:\/\/community.wandb.ai\/t\/access-to-study-group\/1850",
        "Question_upvote_count":2.0,
        "Question_view_count":127.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Can you please help with content for hf-fastai2<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Group by multiple variables in charts",
        "Question_creation_time":1668697430270,
        "Question_link":"https:\/\/community.wandb.ai\/t\/group-by-multiple-variables-in-charts\/3435",
        "Question_upvote_count":0.0,
        "Question_view_count":14.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I have some variables in my sweeps that i want to be able to group by at the same time in my charts.<\/p>\n<p>In this specific case it\u2019s 3 hyperparameters of the architecture: \u201clevels\u201d, \u201cconvolutions per level\u201d and \u201cstarting features\u201d.<\/p>\n<p>I can have multiple charts, grouping by one at a time, and see how each individual variable affects the runs, but it would be much more beneficial to see the effects of all three together.<\/p>\n<p>The \u201ccustom chart\u201d seemed the way to go, but i couldn\u2019t make it work so far. Any help would be really appreciated!<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error with wandb on win10",
        "Question_creation_time":1641055368878,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-with-wandb-on-win10\/1656",
        "Question_upvote_count":0.0,
        "Question_view_count":187.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I got this on win10,it\u2019s stucked<br>\nthe enviornment is<\/p>\n<ul>\n<li>python3.7.10<\/li>\n<li>wandb 0.12.9<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" data-download-href=\"\/uploads\/short-url\/pvNysR6Ps6qxY0fl0Y5gjzfovBK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430.png\" alt=\"image\" data-base62-sha1=\"pvNysR6Ps6qxY0fl0Y5gjzfovBK\" width=\"547\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b2ce90f5e63f7db9be89aa163ee55f0ab468a430_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">686\u00d7626 26.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb.watch with pytorch not logging anything",
        "Question_creation_time":1647450501888,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-not-logging-anything\/2096",
        "Question_upvote_count":2.0,
        "Question_view_count":791.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use <code>wandb.watch<\/code> for a pytorch model, unfortunately without success. I checked the documentation and these two threads:<\/p>\n<ul>\n<li>Wandb.watch not logging parameters<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>But none of the suggested solutions solves my problem. I run in my environment the code from the colab notebook linked in <a href=\"https:\/\/community.wandb.ai\/t\/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients\/518\/3\">this post<\/a> (with <code>N, log_freq = 50, 2<\/code>) and still nothing is logged.<\/p>\n<p>Interestingly, if I set the <code>log_graph=True<\/code> there is a JSON file logged as a file, under <code>root \/ media \/ graph<\/code> in the files section. But I was expecting to get a result similar to <a href=\"https:\/\/wandb.ai\/ayush-thakur\/debug-neural-nets\/runs\/jh061uaf\/model\">this<\/a>.<\/p>\n<p>I am using wandb version 0.12.10.<\/p>\n<p>Kind regards,<br>\nMaciej<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Wandb automatically logeed into the wrong user -- why?",
        "Question_creation_time":1660314798111,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-automatically-logeed-into-the-wrong-user-why\/2916",
        "Question_upvote_count":1.0,
        "Question_view_count":358.0,
        "Question_answer_count":14,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I followed the usual instructions:<\/p>\n<pre><code class=\"lang-auto\">pip install wandb\nwandb login\n<\/code><\/pre>\n<p>but then it never asked me for the user and thus when I pasted my key into the terminal when asked it was there in the <code>.netrc<\/code> file but it was all wrong:<\/p>\n<pre><code class=\"lang-auto\">(iit_term_synthesis) brandomiranda~ \u276f\n(iit_term_synthesis) brandomiranda~ \u276f wandb login\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\n(iit_term_synthesis) brandomiranda~ \u276f wandb login --relogin\nwandb: Logging into wandb.ai. (Learn how to deploy a W&amp;B server locally: https:\/\/wandb.me\/wandb-server)\nwandb: You can find your API key in your browser here: https:\/\/wandb.ai\/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\nwandb: Appending key for api.wandb.ai to your netrc file: \/Users\/brandomiranda\/.netrc\n(iit_term_synthesis) brandomiranda~ \u276f cat \/Users\/brandomiranda\/.netrc\nmachine api.wandb.ai\n  login user\n  password djkfhkjsdhfkjshdkfj...SECRET...sdhjfjhsdjkfhsdjf\n<\/code><\/pre>\n<p>how to fix this?<\/p>\n<aside class=\"onebox stackexchange\" data-onebox-src=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">stackoverflow.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img alt=\"Charlie Parker\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e9c0a0caedbda92f5ad9bc087e52e143936f9f5.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n  <\/a>\n\n<h4>\n  <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">Wandb automatically logeed into the wrong user -- why?<\/a>\n<\/h4>\n\n<div class=\"tags\">\n  <strong>wand<\/strong>\n<\/div>\n\n<div class=\"date\">\n  asked by\n  \n  <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    Charlie Parker\n  <\/a>\n  on <a href=\"https:\/\/stackoverflow.com\/questions\/73335735\/wandb-automatically-logeed-into-the-wrong-user-why\" target=\"_blank\" rel=\"noopener nofollow ugc\">02:32PM - 12 Aug 22 UTC<\/a>\n<\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Description field",
        "Question_creation_time":1660092658434,
        "Question_link":"https:\/\/community.wandb.ai\/t\/description-field\/2881",
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>I am loving <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> and all it can do for me. I have a question whose answer I cannot find anywhere.<br>\nAmong the various fields in the wandb.config file are a few that wandb generates automatically. One of them is <code>Description<\/code>. I tried setting it from a Python program via my configuration file, but to no avail. So I am wondering how to set the Description field programmatically. This will allow me to \u201cdescribe\u201d several hundred simulations for easy retrieval. Thanks,<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to early stop bad runs in sweeps to save time",
        "Question_creation_time":1654584250764,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-early-stop-bad-runs-in-sweeps-to-save-time\/2563",
        "Question_upvote_count":1.0,
        "Question_view_count":128.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hello,<br>\nthat\u2019s my first topic in the community, so I hope I am posting that in the correct category <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I started exploring sweeps last week for a university project, and it is incredible! As we also got a new PyTorch version with support for the new apple silicon, I wanted to try that on my M1 Pro. As this is not as powerful as, for example, using GoogleColab for a fraction of the time, I wanted to ask if it is somehow possible to stop bad runs after a few epochs.<\/p>\n<p>As you can see in the report linked below, the run hopeful-sweep-2 does not look promising. It would be nice to cancel that run and start a new one instead.<\/p>\n<p>Thanks,<br>\nMarkus<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_750x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_10x10.png\">\n\n<h3><a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">Weights &amp; Biases<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I get the version of an artifact?",
        "Question_creation_time":1646690111212,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-get-the-version-of-an-artifact\/2035",
        "Question_upvote_count":2.0,
        "Question_view_count":184.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>Hey,<br>\nI\u2019m trying to get the version of an artifact directly after logging my model (encoder) as an artifact to WandB.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code class=\"lang-auto\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact)\nversion = artifact.version\n<\/code><\/pre>\n<p>Logging works so far, but\u2026<br>\nwhen trying to access the version of the artifact I get an error.<br>\n<strong>Error:<\/strong><\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/main.py\", line 48, in &lt;module&gt;\n    train_setups.start_training_sessions(project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/train_setups.py\", line 18, in start_training_sessions\n    model_pipeline(config, project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 84, in model_pipeline\n    save_model(model_ae=model, model_encoder=model_encoder, model_decoder=model_decoder)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 124, in save_model\n    version = artifact_enc.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_artifacts.py\", line 191, in version\n    return self._logged_artifact.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2899, in version\n    return self._assert_instance().version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2871, in _assert_instance\n    raise ValueError(\nValueError: Must call wait() before accessing logged artifact properties\n<\/code><\/pre>\n<p>What should I do now?<\/p>\n<p>For context:<br>\nI want to print out the version number with some other parameters so that I can easier start my evaluation process for certain runs.<\/p>\n<p>Best regards<br>\nDezzardHD<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I upload code to WandB every time I run it",
        "Question_creation_time":1644892988773,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-upload-code-to-wandb-every-time-i-run-it\/1922",
        "Question_upvote_count":0.0,
        "Question_view_count":140.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":1.0,
        "Question_body":"<p>To back up the code, I want to upload my training code to Wandb every time I run it. Is this possible?<\/p>",
        "Tool":"Weights & Biases",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    }
]