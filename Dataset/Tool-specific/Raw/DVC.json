[
    {
        "Question_title":"DVC compared with GitLFS for storage and versioning only",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-compared-with-gitlfs-for-storage-and-versioning-only\/241",
        "Question_created_time":1570236390787,
        "Question_answer_count":12,
        "Question_score_count":10,
        "Question_view_count":4789,
        "Question_body":"<p>Hi all - I\u2019m considering using DVC on an existing project that has other functionality for reproducability and has access to essentially unlimited Git+LFS on GitHub (affiliated with a university). If I don\u2019t want to use DVC\u2019s reproducability tools right now, is there a use case for DVC for this project? That is, does DVC provide me a benefit over GitLFS?<\/p>\n<p>I\u2019ve been looking through the docs and it sounds like DVC versions using the whole file (similar to GitLFS), but keeps the cache locally (which I like) and gives me control of the remote storage of the repository (which I also like - I\u2019ve seen weird errors before in GitLFS where a file disappeared - not sure what happened)<\/p>\n<p>Would those be the main benefits?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"About the Questions category",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/about-the-questions-category\/306",
        "Question_created_time":1577903864561,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":540,
        "Question_body":"<p>Need help? Have a question about how to use DVC? Ask it here!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":".dvc\/plots missing",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-plots-missing\/1676",
        "Question_created_time":1684981705563,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":30,
        "Question_body":"<p>Hello, I am trying for the first time to use the dvc\u2019s plotting feature. I am able to execute <code>dvc plots show<\/code> - however, my resulting html is blank. I am specifically trying to show training loss. Following along through the dvc course,  the templates should be located within <code>.dvc\/plots<\/code>. I have no such directory. Perhaps I am overlooking something simple?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/3bcd3a4538867c65280a43932d1f854cf751b590.png\" data-download-href=\"\/uploads\/short-url\/8x1RS3CaP6lqvepoqEwtpdUvK4U.png?dl=1\" title=\"Screenshot from 2023-05-24 22-26-48\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3bcd3a4538867c65280a43932d1f854cf751b590_2_614x500.png\" alt=\"Screenshot from 2023-05-24 22-26-48\" data-base62-sha1=\"8x1RS3CaP6lqvepoqEwtpdUvK4U\" width=\"614\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3bcd3a4538867c65280a43932d1f854cf751b590_2_614x500.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3bcd3a4538867c65280a43932d1f854cf751b590_2_921x750.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/3bcd3a4538867c65280a43932d1f854cf751b590.png 2x\" data-dominant-color=\"333435\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2023-05-24 22-26-48<\/span><span class=\"informations\">929\u00d7756 68.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc.yaml for feature analysis",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-yaml-for-feature-analysis\/1679",
        "Question_created_time":1685016991433,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":25,
        "Question_body":"<p>Hi everyone.<\/p>\n<p>I\u2019m a beginner in DVC. I would like to have an ability to analyse my generating features.<\/p>\n<p>I have some stages in my dvc.yaml file: unzip, prepare, featurize, train, evaluate.<\/p>\n<p>Is it possible to add a stage between featurize and train steps for example clusterization stage parallel to main line and make something like a new branch from featurize step?  Could I run this stpe when it needs and not always. This step should genereate some plots for analysis.<\/p>\n<p>So the graph of stages should be like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/85530dcf9f7ab87c2545ec7b42f792b25665fbcd.png\" alt=\"image\" data-base62-sha1=\"j1rrDN51UzwZ1TcuErIIAUHufjT\" width=\"300\" height=\"67\"><\/p>\n<p>What is the best prctice of using dvc in thises step?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Total data usage and use of external shared cache",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/total-data-usage-and-use-of-external-shared-cache\/1678",
        "Question_created_time":1685013931066,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":28,
        "Question_body":"<p>Hi together,<br>\nI\u2019m fairly new to DVC and I\u2019m trying to understand certain concepts in order to set up a data versioning system.<\/p>\n<p>1.) The documentation gives an explanation about the cache and how to optimize it for large datasets. (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/data-management\/large-dataset-optimization#large-dataset-optimization\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Large Dataset Optimization<\/a>).<br>\nHowever, in terms of pure data size it\u2019s still unclear to me how the total data size increases when using DVC.<br>\nLet\u2019s say my total data usage is 100GB before using DVC. Now after setting up DVC  with a cache that uses copy link (default) my total data usage would be 300GB (100GB local + 100GB cache + 100GB remote storage).<br>\n(See: <a href=\"https:\/\/littlebigcode.fr\/wp-content\/uploads\/2022\/05\/image-20220227-182610.png\" rel=\"noopener nofollow ugc\">https:\/\/littlebigcode.fr\/wp-content\/uploads\/2022\/05\/image-20220227-182610.png<\/a>)<br>\nSo in my understanding the best I can do in terms of total storage size is to set a reflink\/hardlink\/symlink so that my local copy of the data now shrinks to a fraction of the total data size. Still, the cache AND the data at the remote location have a total of 200GB. Is it correct that even the lighter links would still result in &gt;=2x of the total storage compared to not using DVC?<\/p>\n<p>2.) My second concern is about the shared cache.<br>\nLet\u2019s say I have a mounted drive with data that is accessed by multiple people working on the same project. Let\u2019s say access to data is cheap and infrequent but the data is large and should not be located at the local workspace permanently. Now I want to set up DVC for data versioning. Is a shared external cache suitable for this scenario, e.g. on the same mounted drive as the data? Also if the the data would be on an S3 instead of the mounted drive, would it be useful to set up the external shared cache also on an S3?<br>\nAre there any additional advantages of the shared cache besides data de-duplication if data access is already fast?<br>\nMaybe someone can share insights about when (not) to use a shared external cache.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple data series plot",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-data-series-plot\/1344",
        "Question_created_time":1663248655019,
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":169,
        "Question_body":"<p>Hey,<br>\nI\u2019m trying to plot multiple lines in the same plot using example showed here:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/show\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/show\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/388;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"388\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/show\" target=\"_blank\" rel=\"noopener nofollow ugc\">plots show<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">plots:\n  - test_vs_train_loss:\n      x: epoch\n      y:\n        training_data.csv: [test_loss, train_loss]\n      title: Compare loss training versus test\n<\/code><\/pre>\n<p>expected str, in stages \u2192 train_model \u2192 plots \u2192 0 \u2192 test_vs_train_loss \u2192 y, line 56, column 13<br>\n55 \u2502   \u2502     y:<br>\n56 \u2502   \u2502   \u2502   training_data.csv: [test_loss, train_loss]<\/p>\n<p>The example doesn\u2019t seem to work. Array object seems not be supported? What am I missing?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Directly modify files on remote, bypassing DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/directly-modify-files-on-remote-bypassing-dvc\/1674",
        "Question_created_time":1684865550556,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":49,
        "Question_body":"<p>Hello,<\/p>\n<p>I have a Git\/DVC repository that contains multiple commits and versions of datasets. Each dataset version is stored as a single binary file containing numerous records. My goal is to remove a specific subset of records from each version in the repository\u2019s history. Essentially, I need to modify all dataset files throughout the repository\u2019s history (rather than deleting the files from history as discussed in other threads using methods like <code>dvc gc ...<\/code>).<\/p>\n<p>One possible straightforward solution is to crawl the DVC remote (S3), identify each data file, load it, remove the desired records, and save it back to the remote. However, I want to ensure that this approach won\u2019t cause any issues in the future, so I would appreciate any input or advice.<\/p>\n<p>Here are some thoughts and tests regarding this potential solution:<\/p>\n<p>The main concern is that the modified files will be inconsistent with the checksums stored in the \u201c*.dvc\u201d file. As far as I know, there are four points where checksums play a role:<\/p>\n<ul>\n<li>\n<p><code>dvc pull<\/code>: Initially, I expected this to be a major issue when pulling the modified files from the remote. However, it seems that by default, checksum validation is not performed during the pull process, as per my understanding of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2676\" rel=\"noopener nofollow ugc\">this<\/a> issue. Also, in tests where we modified remote files and then pulled them, no errors were encountered.<\/p>\n<\/li>\n<li>\n<p><code>dvc repro<\/code>: After pulling the modified files, I would expect that running <code>dvc repro<\/code> would re-execute all stages that have the modified file as input since its content is inconsistent with the previous MD5 checksum. We plan to test this next. Surprisingly, when running <code>dvc status<\/code> after pulling the modified files, it reported no changes. If this is similar to <code>dvc repro<\/code> then maybe the comparison (if any) is between the MD5 of the file in the cache and the MD5 of the file in the workspace, rather than comparing the MD5 of the workspace file with the stored MD5 hash computed in the past. In summary, it appears that this approach might work.<\/p>\n<\/li>\n<li>\n<p><code>dvc push<\/code>: This step should not pose any problems.<\/p>\n<\/li>\n<li>\n<p><code>dvc add<\/code>: Similarly, this step should not cause any issues.<\/p>\n<\/li>\n<\/ul>\n<p>It\u2019s worth noting that initially, we considered using dvc\/git for this process, involving checking out each commit, loading the files, modifying them, and using \u201cgit-filter-branch.\u201d However, we decided against this approach due to its complexity and the risk of making mistakes.<\/p>\n<p>Thank you in advance!<\/p>\n<p>Jonas<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Check DVC status from PythonAPI",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/check-dvc-status-from-pythonapi\/1673",
        "Question_created_time":1684831989685,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":34,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m wondering if there is an option to check the DVC status of the repo using any of the Python APIs. I\u2019m trying to write a utility function that shall abort training run if any of the tracked data is modified or not tracked.<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Python virtual environment",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/python-virtual-environment\/1670",
        "Question_created_time":1684689172465,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"<p>hey,<\/p>\n<p>is DVC meant to work with virtual environment (python -m venv .venv)?<br>\nlooks like some command do not deal with virtual environment quite well.<br>\nfor example<br>\ndvc list \/ dvc get, for some reason look for the --global \/ --system config and do not use the \u201clocal\u201d config of the DVC inside the virtual environment.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"WebDAV error: Server disconnected without sending a response",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/webdav-error-server-disconnected-without-sending-a-response\/1668",
        "Question_created_time":1684321627166,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"<p>Good morning,<\/p>\n<p>I experience an issue when I\u2019m pulling a large quantity of data from my remote storage with WebDAV.<\/p>\n<p>The downloading works properly for a while and then, suddenly, it stops with:<br>\n<code>ERROR: failed to transfer '801e4353008d32e0fbf25cd8b4591fd2' - Server disconnected without sending a response.: Server disconnected without sending a response.                                            <\/code><\/p>\n<p>If I retry it, it restarts where it stopped so I manage to do the full download eventually but it\u2019s not very handy.<\/p>\n<p>I\u2019ve tried to reduce the number of jobs and increase the timeout but in vain.<br>\nI\u2019ve tried to use the API to get the stack trace but it\u2019s not helpful as it\u2019s just raise a FileNotFoundError from the cache, and I think the actual issue is with the downloading before.<\/p>\n<p>I suppose it\u2019s not directly linked to DVC but more to the WebDAV underlying library, but by any chance where it comes from, and if some change in the configuration would help with that?<\/p>\n<p>Thanks a lot in advance for your help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Module 'platformdirs' has no attribute 'site_cache_dir'",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/module-platformdirs-has-no-attribute-site-cache-dir\/1636",
        "Question_created_time":1682906115069,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":157,
        "Question_body":"<p>Hello, I am trying to get my dvc.yaml file set up but am having trouble making the initial stage. For whatever reason I am getting this error. This error persists if I create a new project folder and virtual environment.<\/p>\n<p><code>$ dvc run<\/code> \u2026<\/p>\n<p><code>ERROR: unexpected error - module 'platformdirs' has no attribute 'site_cache_dir'<\/code><\/p>\n<p>I installed dvc with pip in a virtual environment on a newly created Linux machine running Python 3.10. I\u2019ve listed the packages and their versions below. If anyone knows a resolution to the error, or a workaround, let me know.<\/p>\n<p>aiohttp==3.8.4<br>\naiohttp-retry==2.8.3<br>\naiosignal==1.3.1<br>\namqp==5.1.1<br>\nantlr4-python3-runtime==4.9.3<br>\nappdirs==1.4.4<br>\nasync-timeout==4.0.2<br>\nasyncssh==2.13.1<br>\natpublic==3.1.1<br>\nattrs==22.1.0<br>\nbilliard==3.6.4.0<br>\nbuild==0.10.0<br>\nCacheControl==0.12.11<br>\ncelery==5.2.7<br>\ncertifi==2022.9.24<br>\ncffi==1.15.1<br>\ncharset-normalizer==2.1.1<br>\ncleo==2.0.1<br>\nclick==8.1.3<br>\nclick-didyoumean==0.3.0<br>\nclick-plugins==1.1.1<br>\nclick-repl==0.2.0<br>\ncolorama==0.4.6<br>\nconfigobj==5.0.8<br>\ncrashtest==0.4.1<br>\ncryptography==37.0.4<br>\ndictdiffer==0.9.0<br>\ndiskcache==5.6.1<br>\ndistlib==0.3.6<br>\ndistro==1.8.0<br>\ndpath==2.1.5<br>\ndulwich==0.21.3<br>\ndvc==2.55.0<br>\ndvc-data==0.47.2<br>\ndvc-http==2.30.2<br>\ndvc-objects==0.21.2<br>\ndvc-render==0.3.1<br>\ndvc-studio-client==0.8.0<br>\ndvc-task==0.2.1<br>\nfilelock==3.8.0<br>\nflatten-dict==0.4.2<br>\nflufl.lock==7.1.1<br>\nfrozenlist==1.3.3<br>\nfsspec==2023.4.0<br>\nfuncy==2.0<br>\ngitdb==4.0.10<br>\nGitPython==3.1.31<br>\ngrandalf==0.8<br>\nhtml5lib==1.1<br>\nhydra-core==1.3.2<br>\nidna==3.4<br>\ninstaller==0.7.0<br>\niterative-telemetry==0.0.8<br>\njaraco.classes==3.2.3<br>\njeepney==0.8.0<br>\njoblib==1.2.0<br>\njsonschema==4.16.0<br>\nkeyring==23.9.3<br>\nkombu==5.2.4<br>\nlockfile==0.12.2<br>\nmarkdown-it-py==2.2.0<br>\nmdurl==0.1.2<br>\nmore-itertools==8.14.0<br>\nmsgpack==1.0.4<br>\nmultidict==6.0.4<br>\nnanotime==0.5.2<br>\nnetworkx==3.1<br>\nnumpy==1.24.3<br>\nomegaconf==2.3.0<br>\norjson==3.8.11<br>\npackaging==22.0<br>\npandas==2.0.1<br>\npathspec==0.11.1<br>\npexpect==4.8.0<br>\npipenv==2023.3.20<br>\npkginfo==1.9.6<br>\nplatformdirs==2.5.2<br>\npoetry==1.4.2<br>\npoetry-core==1.5.2<br>\npoetry-plugin-export==1.3.0<br>\nprompt-toolkit==3.0.38<br>\npsutil==5.9.5<br>\nptyprocess==0.7.0<br>\npycparser==2.21<br>\npydot==1.4.2<br>\npygit2==1.12.0<br>\nPygments==2.15.1<br>\npygtrie==2.5.0<br>\npyparsing==3.0.9<br>\npyproject_hooks==1.0.0<br>\npyrsistent==0.18.1<br>\npython-dateutil==2.8.2<br>\npytz==2023.3<br>\nPyYAML==6.0<br>\nrapidfuzz==2.13.7<br>\nrequests==2.28.1<br>\nrequests-toolbelt==0.9.1<br>\nrich==13.3.5<br>\nruamel.yaml==0.17.21<br>\nruamel.yaml.clib==0.2.7<br>\nscikit-learn==1.2.2<br>\nscipy==1.10.1<br>\nscmrepo==1.0.2<br>\nSecretStorage==3.3.3<br>\nshellingham==1.5.0<br>\nshortuuid==1.0.11<br>\nshtab==1.6.1<br>\nsix==1.16.0<br>\nsmmap==5.0.0<br>\nsqltrie==0.3.1<br>\ntabulate==0.9.0<br>\nthreadpoolctl==3.1.0<br>\ntomli==2.0.1<br>\ntomlkit==0.11.5<br>\ntqdm==4.65.0<br>\ntrove-classifiers==2022.12.22<br>\ntyping_extensions==4.5.0<br>\ntzdata==2023.3<br>\nurllib3==1.26.12<br>\nvine==5.0.0<br>\nvirtualenv==20.21.0<br>\nvirtualenv-clone==0.5.7<br>\nvoluptuous==0.13.1<br>\nwcwidth==0.2.6<br>\nwebencodings==0.5.1<br>\nyarl==1.9.2<br>\nzc.lockfile==3.0.post1<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc exp queue. git\\index.lock. Error",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-queue-git-index-lock-error\/1663",
        "Question_created_time":1683882360009,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"<p>Hi, eveyone.<\/p>\n<p>I am using the solution for tuning hyperparams in my project as described in this video <a href=\"https:\/\/www.youtube.com\/watch?v=W48Tvx2p-xE\" rel=\"noopener nofollow ugc\">Tuning Hyperparameters Using Grid Search and Random Search with DVC - YouTube<\/a>.<\/p>\n<p>Sometimes I get a message with error during adding experiments in queue.<\/p>\n<blockquote>\n<p>ERROR: unexpected error - [WinError 5] \u041e\u0442\u043a\u0430\u0437\u0430\u043d\u043e \u0432 \u0434\u043e\u0441\u0442\u0443\u043f\u0435: \u2018D:\\GD Anton\\dev\\data\\04.dvc\\\u2026\\.git\\index.lock\u2019 \u2192 \u2018D:\\GD Anton\\dev\\data\\04.dvc\\\u2026\\.git\\index\u2019<\/p>\n<\/blockquote>\n<p>Finally a queue with experiment created and I can start them. Can this error influence on something imoprtant?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc exp run and pythonpath",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-run-and-pythonpath\/1654",
        "Question_created_time":1683300333268,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"<p>I tried to run experiments by comand:<\/p>\n<p>dvc exp run --run-all<\/p>\n<p>How I understand execute files are located in my case in <em>.dvc\\tmp\\exps\\tmp105umt89\\src\\stages<\/em>.<br>\nBut there are <em>src.utils.files<\/em> module in my directory and this files tree copy to <em>.dvc\\tmp\\exps\\tmp105umt89<\/em> folder. I am using VS code and I use options *Terminal &gt; Integrated &gt; Env to add the workspace folder to pythonpath. It works good with <em>dvc repro<\/em> but it\u2019s not working with <em>dvc run exp<\/em>.<\/p>\n<p>The main reason is that I can\u2019t use relative import in python modulus which run as scripts. These experiments folders don\u2019t have constant pathes.<\/p>\n<p>So, how can I run the exps in this case? Does the dvc have an option to add path to pythonpath localy?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Adding experiments. Long time",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adding-experiments-long-time\/1662",
        "Question_created_time":1683870266696,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":27,
        "Question_body":"<p>Hi, eveyone.<\/p>\n<p>I am using the solution for tuning hyperparams in my project as described in this video <a href=\"https:\/\/www.youtube.com\/watch?v=W48Tvx2p-xE\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Tuning Hyperparameters Using Grid Search and Random Search with DVC - YouTube<\/a>.<\/p>\n<p>But when start an adding experiments usually it takes a long time. It takes the same time as a grid searh in my small project. Is it a common situation or I have something like a bug?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - No module named 'rich",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-no-module-named-rich\/1660",
        "Question_created_time":1683746657896,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":37,
        "Question_body":"<p>Installed dvc and python-dvc-http on arch using yay.<br>\nWhen I try <code>dvc diff data<\/code>, I get \u201cERROR: unexpected error - No module named \u2018rich\u2019\u201d<br>\nI don\u2019t see any *-dvc-rich packages.<br>\nAny suggestions?<br>\nThanks!<\/p>\n<p><s>Edit: and this one:<\/s><\/p><s>\n<\/s><p><s>ERROR: unexpected error - ssh is supported, but requires \u2018dvc-ssh\u2019 to be installed: No module named \u2018dvc_ssh\u2019<\/s><br>\nFound dvc-ssh <a href=\"https:\/\/github.com\/iterative\/dvc-ssh\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"MLEM + Modal + nanoGPT blog code",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/mlem-modal-nanogpt-blog-code\/1657",
        "Question_created_time":1683571052061,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":39,
        "Question_body":"<h2>\n<a name=\"i-am-reading-the-blog-httpsiterativeaiblogmlem-nanogpt-modal-flyio-i-could-not-go-past-python-trainpy-configtrain_mlemaipy-devicecuda-dtypefloat32-max_iters3000-init_fromscratch-running-on-google-colab-with-gpu-set-error-message-is-as-attached-1\" class=\"anchor\" href=\"#i-am-reading-the-blog-httpsiterativeaiblogmlem-nanogpt-modal-flyio-i-could-not-go-past-python-trainpy-configtrain_mlemaipy-devicecuda-dtypefloat32-max_iters3000-init_fromscratch-running-on-google-colab-with-gpu-set-error-message-is-as-attached-1\"><\/a>I am reading the blog - <a href=\"https:\/\/iterative.ai\/blog\/mlem-nanogpt-modal-flyio\/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">MLEM + Modal + nanoGPT<\/a><br>\nI could not go past \u201c!python train.py config\/train_mlemai.py --device=cuda --dtype=float32 --max_iters=3000 --init_from=scratch\u201d running on Google Colab with GPU set.<br>\nError message is as attached.<\/h2>\n<p>Overriding config with config\/train_mlemai.py:<\/p>\n<h1>\n<a name=\"train-a-miniature-character-level-shakespeare-model-2\" class=\"anchor\" href=\"#train-a-miniature-character-level-shakespeare-model-2\"><\/a>train a miniature character-level shakespeare model<\/h1>\n<h1>\n<a name=\"good-for-debugging-and-playing-on-macbooks-and-such-3\" class=\"anchor\" href=\"#good-for-debugging-and-playing-on-macbooks-and-such-3\"><\/a>good for debugging and playing on macbooks and such<\/h1>\n<p>out_dir = \u2018out-mlemai-char\u2019<br>\neval_interval = 250 # keep frequent because we\u2019ll overfit<br>\neval_iters = 200<br>\nlog_interval = 10 # don\u2019t print too too often<\/p>\n<h1>\n<a name=\"we-expect-to-overfit-on-this-small-dataset-so-only-save-when-val-improves-4\" class=\"anchor\" href=\"#we-expect-to-overfit-on-this-small-dataset-so-only-save-when-val-improves-4\"><\/a>we expect to overfit on this small dataset, so only save when val improves<\/h1>\n<p>always_save_checkpoint = False<\/p>\n<p>wandb_log = False # override via command line if you like<\/p>\n<p>dataset = \u2018mlem-docs\u2019<br>\nbatch_size = 64<br>\nblock_size = 256 # context of up to 128 previous characters<\/p>\n<h1>\n<a name=\"baby-gpt-model-5\" class=\"anchor\" href=\"#baby-gpt-model-5\"><\/a>baby GPT model <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\">\n<\/h1>\n<p>n_layer = 6<br>\nn_head = 6<br>\nn_embd = 384<br>\ndropout = 0.2<\/p>\n<p>learning_rate = 1e-3 # with baby networks can afford to go a bit higher<br>\nmax_iters = 5000<br>\nlr_decay_iters = 5000 # make equal to max_iters usually<br>\nmin_lr = 1e-4 # learning_rate \/ 10 usually<br>\nbeta2 = 0.99 # make a bit bigger because number of tokens per iter is small<\/p>\n<p>warmup_iters = 100 # not super necessary potentially<\/p>\n<h1>\n<a name=\"on-macbook-also-add-6\" class=\"anchor\" href=\"#on-macbook-also-add-6\"><\/a>on macbook also add<\/h1>\n<p>device = \u2018mps\u2019  # run on cpu only<br>\ncompile = False # do not torch compile the model<\/p>\n<h2>\n<a name=\"overriding-device-cuda-overriding-dtype-float32-overriding-max_iters-3000-overriding-init_from-scratch-vocab_size-153-from-datamlem-docsmetapkl-initializing-a-new-model-from-scratch-number-of-parameters-1080m-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-3000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-3100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6300-assertion-srcindex-srcselectdimsize-failed-traceback-most-recent-call-last-file-contentdrivemydriveiteractiveainanogpttrainpy-line-235-in-module-losses-estimate_loss-file-usrlocallibpython310dist-packagestorchutils_contextlibpy-line-115-in-decorate_context-return-funcargs-kwargs-file-contentdrivemydriveiteractiveainanogpttrainpy-line-196-in-estimate_loss-logits-loss-modelx-y-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-contentdrivemydriveiteractiveainanogptmodelpy-line-139-in-forward-x-blockx-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-contentdrivemydriveiteractiveainanogptmodelpy-line-89-in-forward-x-x-selfattnselfln_1x-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-contentdrivemydriveiteractiveainanogptmodelpy-line-47-in-forward-q-k-v-selfc_attnxsplitselfn_embd-dim2-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-usrlocallibpython310dist-packagestorchnnmoduleslinearpy-line-114-in-forward-return-flinearinput-selfweight-selfbias-runtimeerror-cuda-error-cublas_status_not_initialized-when-calling-cublascreatehandle-7\" class=\"anchor\" href=\"#overriding-device-cuda-overriding-dtype-float32-overriding-max_iters-3000-overriding-init_from-scratch-vocab_size-153-from-datamlem-docsmetapkl-initializing-a-new-model-from-scratch-number-of-parameters-1080m-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-6900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-7900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-8900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-9500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-1900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-2900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-3000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-7500-thread-3100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-9900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-10900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-11900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-3500-thread-12700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-3900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-4900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5300-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5400-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5500-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5600-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5700-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5800-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-5900-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6000-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6100-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6200-assertion-srcindex-srcselectdimsize-failed-atensrcatennativecudaindexingcu1146-indexselectlargeindex-block-400-thread-6300-assertion-srcindex-srcselectdimsize-failed-traceback-most-recent-call-last-file-contentdrivemydriveiteractiveainanogpttrainpy-line-235-in-module-losses-estimate_loss-file-usrlocallibpython310dist-packagestorchutils_contextlibpy-line-115-in-decorate_context-return-funcargs-kwargs-file-contentdrivemydriveiteractiveainanogpttrainpy-line-196-in-estimate_loss-logits-loss-modelx-y-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-contentdrivemydriveiteractiveainanogptmodelpy-line-139-in-forward-x-blockx-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-contentdrivemydriveiteractiveainanogptmodelpy-line-89-in-forward-x-x-selfattnselfln_1x-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-contentdrivemydriveiteractiveainanogptmodelpy-line-47-in-forward-q-k-v-selfc_attnxsplitselfn_embd-dim2-file-usrlocallibpython310dist-packagestorchnnmodulesmodulepy-line-1501-in-_call_impl-return-forward_callargs-kwargs-file-usrlocallibpython310dist-packagestorchnnmoduleslinearpy-line-114-in-forward-return-flinearinput-selfweight-selfbias-runtimeerror-cuda-error-cublas_status_not_initialized-when-calling-cublascreatehandle-7\"><\/a>Overriding: device = cuda<br>\nOverriding: dtype = float32<br>\nOverriding: max_iters = 3000<br>\nOverriding: init_from = scratch<br>\nvocab_size = 153 (from data\/mlem-docs\/meta.pkl)<br>\nInitializing a new model from scratch<br>\nnumber of parameters: 10.80M<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [64,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [65,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [66,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [67,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [68,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [69,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [70,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [71,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [72,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [73,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [74,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [75,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [76,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [77,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [78,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [79,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [80,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [81,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [82,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [83,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [84,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [85,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [86,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [87,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [88,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [89,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [90,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [91,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [92,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [93,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [94,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [95,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [0,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [1,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [2,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [3,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [4,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [5,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [6,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [7,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [8,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [9,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [10,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [11,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [12,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [13,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [14,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [15,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [16,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [17,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [18,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [19,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [20,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [21,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [22,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [23,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [24,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [25,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [26,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [27,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [28,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [29,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [30,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [75,0,0], thread: [31,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [96,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [97,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [98,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [99,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [100,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [101,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [102,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [103,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [104,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [105,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [106,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [107,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [108,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [109,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [110,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [111,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [112,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [113,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [114,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [115,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [116,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [117,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [118,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [119,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [120,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [121,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [122,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [123,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [124,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [125,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [126,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [35,0,0], thread: [127,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [32,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [33,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [34,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [35,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [36,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [37,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [38,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [39,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [40,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [41,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [42,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [43,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [44,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [45,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [46,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [47,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [48,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [49,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [50,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [51,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [52,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [53,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [54,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [55,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [56,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [57,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [58,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [59,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [60,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [61,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [62,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\n\u2026\/aten\/src\/ATen\/native\/cuda\/Indexing.cu:1146: indexSelectLargeIndex: block: [4,0,0], thread: [63,0,0] Assertion <code>srcIndex &lt; srcSelectDimSize<\/code> failed.<br>\nTraceback (most recent call last):<br>\nFile \u201c\/content\/drive\/MyDrive\/Iteractive.ai\/nanoGPT\/train.py\u201d, line 235, in <br>\nlosses = estimate_loss()<br>\nFile \u201c\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/_contextlib.py\u201d, line 115, in decorate_context<br>\nreturn func(*args, **kwargs)<br>\nFile \u201c\/content\/drive\/MyDrive\/Iteractive.ai\/nanoGPT\/train.py\u201d, line 196, in estimate_loss<br>\nlogits, loss = model(X, Y)<br>\nFile \u201c\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 1501, in _call_impl<br>\nreturn forward_call(*args, **kwargs)<br>\nFile \u201c\/content\/drive\/MyDrive\/Iteractive.ai\/nanoGPT\/model.py\u201d, line 139, in forward<br>\nx = block(x)<br>\nFile \u201c\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 1501, in _call_impl<br>\nreturn forward_call(*args, **kwargs)<br>\nFile \u201c\/content\/drive\/MyDrive\/Iteractive.ai\/nanoGPT\/model.py\u201d, line 89, in forward<br>\nx = x + self.attn(self.ln_1(x))<br>\nFile \u201c\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 1501, in _call_impl<br>\nreturn forward_call(*args, **kwargs)<br>\nFile \u201c\/content\/drive\/MyDrive\/Iteractive.ai\/nanoGPT\/model.py\u201d, line 47, in forward<br>\nq, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)<br>\nFile \u201c\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 1501, in _call_impl<br>\nreturn forward_call(*args, **kwargs)<br>\nFile \u201c\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/nn\/modules\/linear.py\u201d, line 114, in forward<br>\nreturn F.linear(input, self.weight, self.bias)<br>\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling <code>cublasCreate(handle)<\/code>\n<\/h2>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Change the data chunk size in the cache directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/change-the-data-chunk-size-in-the-cache-directory\/1643",
        "Question_created_time":1683049450858,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":43,
        "Question_body":"<p>Hi!<\/p>\n<p>I\u2019ve started to use DVC recently and experienced slow upload speed toward my remote storage.<br>\nI suspect this is related to the large amount of data chunks in the cache directory that are to be uploaded.<br>\nAs far as I\u2019ve seen, the size of those chunks is 1MB.<br>\nIs there a way to change that value so I have fewer, but bigger, blocks of data?<\/p>\n<p>Many thanks in advance for your help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Symlink semantics in tracked directories",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/symlink-semantics-in-tracked-directories\/1645",
        "Question_created_time":1683060015210,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":50,
        "Question_body":"<p>What are the semantics of symlinks inside a directory tree i track all at once with <code>dvc add &lt;some directory&gt;<\/code>?<\/p>\n<p>I have some COCO image datasets of this form:<\/p>\n<pre><code class=\"lang-auto\">rootdir\/\n\u251c\u2500\u2500 clientdata\n\u2502   \u251c\u2500\u2500 images\n|   |-- dataset-val.json\n\u2502   \u2514\u2500\u2500 dataset-train.json\n\u251c\u2500\u2500 clientdata-merged\n\u2502   \u251c\u2500\u2500 images\n|   |-- dataset-merged-val.json\n\u2502   \u2514\u2500\u2500 dataset-merged-train.json\n\u2514\u2500\u2500 clientdata-supplemental\n|     |-- dataset-supplemental-val.json\n\u2502     \u2514\u2500\u2500 dataset-supplemental-train.json\n|     \u2514\u2500\u2500 images\n<\/code><\/pre>\n<p>And what i would like to have is this situation:<\/p>\n<ul>\n<li>all the images in <code>clientdata\/images<\/code> are dvc-tracked image files<\/li>\n<li>all the images in <code>clientdata-supplemental\/images<\/code> are dvc-tracked image files<\/li>\n<li>all the images in <code>clientdata-merged\/images<\/code> are dvc-tracked symlinks to the other two image subdirectories<\/li>\n<\/ul>\n<p>and this way no data is duplicated.<\/p>\n<p>I <em>think<\/em> that what is happening now is that running <code>dvc add rootdir<\/code> doesn\u2019t respect the symlinks, and when i <code>dvc pull<\/code> on another host, i get three subdirectories of images.   I\u2019m confident that that is what happens for windows; i am pretty sure it\u2019s true for linux as well.<\/p>\n<p>Am i thinking about things the right way here? Are there other strategies here that i should be considering?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"dvc get\" gets empty files for self-hosted data registry",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-gets-empty-files-for-self-hosted-data-registry\/1607",
        "Question_created_time":1681744639354,
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"<p>Hi! I am new to DVC. Seems like a very helpful tool. However, I have been trying to configure a self-hosted data registry and I have faced an issue when downloading the data from the remote storage. I have followed the <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registry\/tutorial\" rel=\"noopener nofollow ugc\">Data Registry tutorial<\/a> but has not worked for me<\/p>\n<p>So I have a server that I can access through ssh. My idea is to use this server as a Data Registry, so me and my colleagues can use it to centralize our data.<\/p>\n<p>So first I created the data registry on the server:<\/p>\n<pre><code class=\"lang-auto\">mkdir \/shared\/test_dvc\ncd \/shared\/test_dvc\ngit init\ndvc init\n<\/code><\/pre>\n<p>And copied some data with the following structure:<\/p>\n<pre><code class=\"lang-auto\">.\n\u2514\u2500\u2500 data\n   \u251c\u2500\u2500 001.wav\n   \u251c\u2500\u2500 002.wav\n   \u2514\u2500\u2500 ... \n<\/code><\/pre>\n<p>Then:<\/p>\n<pre><code class=\"lang-auto\">dvc add data\ngit add .gitignore data.dvc\ngit commit -m \"first commit\"\n<\/code><\/pre>\n<p>Then I create the storage in my server and push the data<\/p>\n<pre><code class=\"lang-auto\">dvc remote add storage \/shared\/storage\ndvc push -r storage\n<\/code><\/pre>\n<p>Then on my laptop. In a new folder I run the following:<\/p>\n<pre><code class=\"lang-auto\">dvc list -R ssh:\/\/&lt;ssh_server_name&gt;:\/shared\/test_dvc\n<\/code><\/pre>\n<p>This shows only these files:<\/p>\n<ul>\n<li>.dvcignore<\/li>\n<li>.gitignore<\/li>\n<li>data.dvc<\/li>\n<\/ul>\n<p>And when running:<\/p>\n<pre><code class=\"lang-auto\">dvc get ssh:\/\/&lt;ssh_server_name&gt;:\/shared\/test_dvc data\n<\/code><\/pre>\n<p>It creates an empty directory data. Why am I not getting the data under the data directory stored in my remote server?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Evaluate. Replace in previous commits",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/evaluate-replace-in-previous-commits\/1623",
        "Question_created_time":1682426635102,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":46,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>I am a beginner in DVC and GIT.<\/p>\n<p>I have a long chain of commits. And my evaluate.py file has changed significantly.<\/p>\n<p>And I would like to use it in previous commits to compare new metrics. Is it possible to apply my new evaluate.py to previous commits.<\/p>\n<p>But when I wrote about commits I thought about dvc.yaml where some stage\u2019s parameters are also described for example plots.<\/p>\n<p>So how could I do it? What is the best practice?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Single plots with multiple x-axes",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/single-plots-with-multiple-x-axes\/1630",
        "Question_created_time":1682546203394,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":39,
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to combine some multi-channel data that are generated in my pipeline into a single plot, however, the data vectors that I want to plot can have a different amount of samples. E.g., I\u2019ve got x1 and y1 and z1 vectors of length N1 and x2, y2 and z2 vectors of length N2. The data is stored in two json files, like:<\/p>\n<p>data1.json:<\/p>\n<pre><code class=\"lang-auto\">[\n    {\"x\": .., \"y\": .., \"z\": ..},\n    ..\n    {\"x\": .., \"y\": .., \"z\": ..}\n ]\n<\/code><\/pre>\n<p>and data2.json:<\/p>\n<pre><code class=\"lang-auto\">[\n    {\"x\": .., \"y\": .., \"z\": ..},\n    ..\n    {\"x\": .., \"y\": .., \"z\": ..}\n] \n<\/code><\/pre>\n<p>What I\u2019d like to do is to have <code>plots<\/code> section in my yaml file that looks like<\/p>\n<pre><code class=\"lang-auto\">plots:\n  - data:\n      y:\n        data1.json: [y, z]\n        data2.json: [y, z]\n      x:\n        data1.json: x\n        data2.json: x\n<\/code><\/pre>\n<p>and have all data combined in 1 plot (i.e. 4 curves). However, if I try this I get an empty plot. If I only use a single channel for the y-axis (e.g. z), then it works. Is this a bug or is it intended?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - [Errno 2] No such file or directory:",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-errno-2-no-such-file-or-directory\/1381",
        "Question_created_time":1667696501930,
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":699,
        "Question_body":"<p>Hi all,<\/p>\n<p>I ran into an <code>ERROR: unexpected error - [Errno 2] No such file or directory<\/code> from running <code>dvc run<\/code> on my  stage in my <code>dvc.yaml<\/code>. I am running into a similar error when I try to do <code>dvc repro<\/code>. I am not sure how to debug the error.<\/p>\n<details>\n<summary>\nFull stacktrace from `dvc run --force -vvv ...`<\/summary>\n<pre><code class=\"lang-auto\">$ dvc run --force -vvv -n split_state_and_fed -d data\/source -d split_state_and_fed.py -o data\/fed\/LabeledRegulations_Fed.csv -o data\/state\/LabeledRegulations_State.csv python split_state_and_fed.py split_state_and_fed --csv_path=data\/source --output_state=data\/state\/LabeledRegulations_State.csv --output_fed=data\/fed\/LabeledRegulations_Fed.csv\n2022-11-05 21:00:28,578 TRACE: Namespace(cprofile=False, yappi=False, yappi_separate_threads=False, viztracer=False, viztracer_depth=None, cprofile_dump=None, pdb=False, instrument=False, instrument_open=False, show_stack=False, quiet=0, verbose=3, version=None, cd='.', cmd='run', name='split_state_and_fed', file=None, single_stage=False, force=True, deps=['data\/source', 'split_state_and_fed.py'], params=[], outs=['data\/fed\/LabeledRegulations_Fed.csv', 'data\/state\/LabeledRegulations_State.csv'], outs_no_cache=[], checkpoints=[], external=False, outs_persist=[], outs_persist_no_cache=[], metrics=[], metrics_no_cache=[], plots=[], plots_no_cache=[], live=None, live_no_cache=None, live_no_summary=False, live_no_html=False, wdir=None, always_changed=False, desc=None, command=['python', 'split_state_and_fed.py', 'split_state_and_fed', '--csv_path=data\/source', '--output_state=data\/state\/LabeledRegulations_State.csv', '--output_fed=data\/fed\/LabeledRegulations_Fed.csv'], no_exec=False, no_commit=False, no_run_cache=False, func=&lt;class 'dvc.commands.run.CmdRun'&gt;, parser=DvcParser(prog='dvc', usage=None, description='Data Version Control', formatter_class=&lt;class 'argparse.RawTextHelpFormatter'&gt;, conflict_handler='error', add_help=False))\n2022-11-05 21:00:29,131 TRACE: params.yaml does not exist, it won't be used in parametrization\n2022-11-05 21:00:29,132 TRACE: Context during resolution of stage split_state_and_fed:\n{}\n2022-11-05 21:00:29,133 DEBUG: Lockfile for 'dvc.yaml' not found\n2022-11-05 21:00:29,139 TRACE:    41.75 ms in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\n2022-11-05 21:00:29,147 TRACE:     7.54 ms in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\data\n2022-11-05 21:00:29,150 TRACE:   958.90 mks in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\src\n2022-11-05 21:00:29,152 TRACE:    12.30 mks in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\src\\tracker_healthcare_qa\n2022-11-05 21:00:29,154 TRACE:    17.80 mks in collecting stages from C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\tests\n2022-11-05 21:00:29,336 TRACE: params.yaml does not exist, it won't be used in parametrization\n2022-11-05 21:00:29,336 TRACE: Context during resolution of stage split_state_and_fed:\n{}\n2022-11-05 21:00:29,338 DEBUG: Lockfile for 'dvc.yaml' not found\n2022-11-05 21:00:29,348 DEBUG: Removing output 'data\\fed\\LabeledRegulations_Fed.csv' of stage: 'split_state_and_fed'.\n2022-11-05 21:00:29,349 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\data\\fed\\LabeledRegulations_Fed.csv'\n2022-11-05 21:00:29,350 DEBUG: Removing output 'data\\state\\LabeledRegulations_State.csv' of stage: 'split_state_and_fed'.\n2022-11-05 21:00:29,352 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\data\\state\\LabeledRegulations_State.csv'\n2022-11-05 21:00:29,371 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,378 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,379 DEBUG: {'data\\\\source': 'modified'}\n2022-11-05 21:00:29,387 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,398 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:29,424 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\nRunning stage 'split_state_and_fed':\n&gt; python split_state_and_fed.py split_state_and_fed --csv_path=data\/source --output_state=data\/state\/LabeledRegulations_State.csv --output_fed=data\/fed\/LabeledRegulations_Fed.csv\n2022-11-05 21:00:33,363 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:33,400 DEBUG: Computed stage: 'split_state_and_fed' md5: 'bfb841b6a7ea23cf95f75c8b2a8e7ea3'\n2022-11-05 21:00:33,406 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:33,412 DEBUG: built tree 'object 24fdb62da3d871204e3c29da681a9939.dir'\n2022-11-05 21:00:33,440 ERROR: unexpected error - [Errno 2] No such file or directory: 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\e0d88e8d4dd7e19c58922c66af34a089d91cd70f419cbb28773bd22b72e5f967.hAv7gnwFPvcZY9cUZGJiHN'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 825, in move\n    os.rename(src, real_dst)\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\tmpiy81j0vt' -&gt; 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\e0d88e8d4dd7e19c58922c66af34a089d91cd70f419cbb28773bd22b72e5f967.hAv7gnwFPvcZY9cUZGJiHN'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\cli\\__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\cli\\command.py\", line 22, in do_run\n    return self.run()\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\commands\\run.py\", line 47, in run\n    self.repo.run(**kwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\repo\\__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\repo\\scm_context.py\", line 156, in run\n    return method(repo, *args, **kw)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\repo\\run.py\", line 33, in run\n    stage.run(no_commit=no_commit, run_cache=run_cache)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\funcy\\decorators.py\", line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\decorators.py\", line 43, in rwlocked\n    return call()\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\funcy\\decorators.py\", line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\__init__.py\", line 574, in run\n    self.save(allow_missing=allow_missing)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\__init__.py\", line 486, in save\n    self.repo.stage_cache.save(self)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc\\stage\\cache.py\", line 181, in save\n    self.repo.odb.local.move(tmp, path)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_data\\hashfile\\db\\local.py\", line 34, in move\n    super().move(from_info, to_info)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\db.py\", line 91, in move\n    self.fs.move(from_info, to_info)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\fs\\base.py\", line 371, in mv\n    self.fs.mv(from_info, to_info)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\fs\\local.py\", line 107, in mv\n    move(path1, path2)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\site-packages\\dvc_objects\\fs\\utils.py\", line 64, in move\n    shutil.move(src, tmp)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 845, in move\n    copy_function(src, real_dst)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 444, in copy2\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n  File \"C:\\Users\\bd122850\\Anaconda3\\envs\\tracker_healthcare_qa\\lib\\shutil.py\", line 266, in copyfile\n    with open(dst, 'wb') as fdst:\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\bd122850\\\\OneDrive - The Bureau of National Affairs, Inc\\\\Documents\\\\projects\\\\reports\\\\tracker_healthcare_qa\\\\.dvc\\\\cache\\\\runs\\\\5c\\\\5cfa45a43abf76c684266397388c048a50ede05c4e0ed9d7ccb8107c1ee70601\\\\e0d88e8d4dd7e19c58922c66af34a089d91cd70f419cbb28773bd22b72e5f967.hAv7gnwFPvcZY9cUZGJiHN'\n------------------------------------------------------------\n2022-11-05 21:00:33,798 DEBUG: link type reflink is not available ([Errno 129] no more link types left to try out)\n2022-11-05 21:00:33,798 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\.69MpunVkddMa4TZdFpmKaC.tmp'\n2022-11-05 21:00:33,800 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\.69MpunVkddMa4TZdFpmKaC.tmp'\n2022-11-05 21:00:33,803 DEBUG: link type symlink is not available ([WinError 1314] A required privilege is not held by the client: 'C:\/Users\/bd122850\/OneDrive - The Bureau of National Affairs, Inc\/Documents\/projects\/reports\/tracker_healthcare_qa\/.dvc\/cache\/.ZBKsiaxpxhhyMox828VWLX.tmp' -&gt; 'C:\/Users\/bd122850\/OneDrive - The Bureau of National Affairs, Inc\/Documents\/projects\/reports\/.69MpunVkddMa4TZdFpmKaC.tmp')\n2022-11-05 21:00:33,803 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\.69MpunVkddMa4TZdFpmKaC.tmp'\n2022-11-05 21:00:33,804 DEBUG: Removing 'C:\\Users\\bd122850\\OneDrive - The Bureau of National Affairs, Inc\\Documents\\projects\\reports\\tracker_healthcare_qa\\.dvc\\cache\\.ZBKsiaxpxhhyMox828VWLX.tmp'\n2022-11-05 21:00:33,854 DEBUG: Version info for developers:\nDVC version: 2.33.2 (pip)\n---------------------------------\nPlatform: Python 3.9.13 on Windows-10-10.0.19043-SP0\nSubprojects:\n        dvc_data = 0.25.2\n        dvc_objects = 0.12.2\n        dvc_render = 0.0.12\n        dvc_task = 0.1.4\n        dvclive = 1.0\n        scmrepo = 0.1.3\nSupports:\n        http (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.3, aiohttp-retry = 2.8.3)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: local\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-11-05 21:00:33,868 DEBUG: Analytics is enabled.\n2022-11-05 21:00:34,127 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\bd122850\\\\AppData\\\\Local\\\\Temp\\\\tmpsxs91nti']'\n2022-11-05 21:00:34,157 DEBUG: Spawned '['daemon', '-q', 'analytics', 'C:\\\\Users\\\\bd122850\\\\AppData\\\\Local\\\\Temp\\\\tmpsxs91nti']'\n<\/code><\/pre>\n<\/details>\n<p>DVC doctor output:<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.33.2 (pip)\n---------------------------------\nPlatform: Python 3.9.13 on Windows-10-10.0.19043-SP0\nSubprojects:\n        dvc_data = 0.25.2\n        dvc_objects = 0.12.2\n        dvc_render = 0.0.12\n        dvc_task = 0.1.4\n        dvclive = 1.0\n        scmrepo = 0.1.3\nSupports:\n        http (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.3, aiohttp-retry = 2.8.3)\nCache types: hardlink\nCache directory: NTFS on C:\\\nCaches: local\nRemotes: local\nWorkspace directory: NTFS on C:\\\nRepo: dvc, git\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Running DVC on AWS Batch",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/running-dvc-on-aws-batch\/1481",
        "Question_created_time":1675135870597,
        "Question_answer_count":11,
        "Question_score_count":0,
        "Question_view_count":249,
        "Question_body":"<p>I\u2019m trying to get a DVC job to run on AWS Batch, but I\u2019m having trouble getting dvc to access the s3 bucket.<\/p>\n<p>This is the script I\u2019m running:<\/p>\n<pre><code class=\"lang-auto\">#!\/bin\/bash\n\nAWS_CREDENTIALS=$(curl http:\/\/169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI)\n\nexport AWS_DEFAULT_REGION=us-east-1\n\nexport AWS_ACCESS_KEY_ID=$(echo \"$AWS_CREDENTIALS\" | jq .AccessKeyId -r)\n\nexport AWS_SECRET_ACCESS_KEY=$(echo \"$AWS_CREDENTIALS\" | jq .SecretAccessKey -r)\n\nexport AWS_SESSION_TOKEN=$(echo \"$AWS_CREDENTIALS\" | jq .Token -r)\n\necho \"AWS_ACCESS_KEY_ID=&lt;$AWS_ACCESS_KEY_ID&gt;\"\n\necho \"AWS_SECRET_ACCESS_KEY=&lt;$AWS_SECRET_ACCESS_KEY&gt;\"\n\necho \"AWS_SECRET_ACCESS_KEY=&lt;$(cat &lt;(echo \"$AWS_SECRET_ACCESS_KEY\" | head -c 6) &lt;(echo -n \"...\") &lt;(echo \"$AWS_SECRET_ACCESS_KEY\" | tail -c 6))&gt;\"\n\necho \"AWS_SESSION_TOKEN=&lt;$(cat &lt;(echo \"$AWS_SESSION_TOKEN\" | head -c 6) &lt;(echo -n \"...\") &lt;(echo \"$AWS_SESSION_TOKEN\" | tail -c 6))&gt;\"\n\nalias python=python3\n\naws s3 ls s3:\/\/duolingo-dvc\/\n\naws s3 ls s3:\/\/duolingo-dvc\/det-grade\/\n\naws s3 cp s3:\/\/duolingo-dvc\/det-grade\/00\/0e4343c163bd70df0a6f9d81e1b4d2 mycopy.txt\n\ndvc remote modify s3 --local region us-east-1\n\ndvc remote modify s3 --local access_key_id $AWS_ACCESS_KEY_ID\n\ndvc remote modify s3 --local secret_access_key $AWS_SECRET_ACCESS_KEY\n\ndvc remote modify s3 --local session_token $AWS_SESSION_TOKEN\n\necho \"Starting DVC pull\"\n\ndvc pull -v scripts\/writing\/data\/$1\/responses.train-model.csv\ndvc pull -v scripts\/writing\/data\/$1\/responses.test.csv\ndvc pull -v scripts\/writing\/data\/$1\/corrected-responses.pkl\n\necho \"Stopping DVC pull\"\n\ndvc repro correct-responses@$1 --force --single-item\n<\/code><\/pre>\n<p>All the AWS CLI commands, accessing the same bucket, work.  However, when it tries to run the DVC pull commands, I get access denied errors, even though it should be using the same credentials.<\/p>\n<pre><code class=\"lang-auto\">botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n<\/code><\/pre>\n<p>Just before that, it says its \u201cPreparing to collect status from \u2018duolingo-dvc\/det-grade\u2019\u201d.<\/p>\n<p>In case its relevant.  The role\u2019s permissions to the s3 bucket are defined as follows:<\/p>\n<pre><code class=\"lang-auto\">data \"aws_iam_policy_document\" \"standard-batch-job-role\" {\n  # S3 read access to related buckets\n  statement {\n    actions = [\n      \"s3:Get*\",\n      \"s3:List*\",\n    ]\n    resources = [\n      data.aws_s3_bucket.duolingo-dvc.arn,\n      \"${data.aws_s3_bucket.duolingo-dvc.arn}\/*\",\n    ]\n    effect = \"Allow\"\n  }\n}\n<\/code><\/pre>\n<p>AWS doesn\u2019t make it easy to copy the full stack trace, but here is a screenshot:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/7bd06124d7d53dd522e687473927233c3b909550.png\" data-download-href=\"\/uploads\/short-url\/hFjb9dvov2M6NvqQdUR86ZgtAZ2.png?dl=1\" title=\"Screen Shot 2023-01-30 at 10.36.42 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_690x230.png\" alt=\"Screen Shot 2023-01-30 at 10.36.42 PM\" data-base62-sha1=\"hFjb9dvov2M6NvqQdUR86ZgtAZ2\" width=\"690\" height=\"230\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_690x230.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_1035x345.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7bd06124d7d53dd522e687473927233c3b909550_2_1380x460.png 2x\" data-dominant-color=\"EDEDED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2023-01-30 at 10.36.42 PM<\/span><span class=\"informations\">2772\u00d7926 393 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practices with data regsitry",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-with-data-regsitry\/1615",
        "Question_created_time":1682010746158,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":45,
        "Question_body":"<p>Hi !<\/p>\n<p>I\u2019m fairly new to the concept of data registry and I\u2019m starting to set it up for my company. I would have liked to have your opinion on good practices on the following points:<\/p>\n<ol>\n<li>\n<p>Should I put code in my data registry repository, e.g. utility functions that cleaned up the data when received (removed duplicates, etc.) or should I store them in a separate repo and dedicated my data registry repository just for data versioning ?<\/p>\n<\/li>\n<li>\n<p>Is it relevant to store multiple datasets in a repo? For example if we want to tag our dataset it can quickly become a mess. Isn\u2019t it more relevant to do a repo per dataset ?<\/p>\n<\/li>\n<\/ol>\n<p>Thx in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using DVC in github codespaces",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-in-github-codespaces\/1461",
        "Question_created_time":1673510308004,
        "Question_answer_count":9,
        "Question_score_count":6,
        "Question_view_count":234,
        "Question_body":"<p>I\u2019m trying to use dvc in a github codespace and gdrive remote storage. However once I try to \u201cdvc push\u201d<br>\nI get this error<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" data-download-href=\"\/uploads\/short-url\/1ES8Pl3nOEFzmQGXs0qEaUQLYRC.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" alt=\"image\" data-base62-sha1=\"1ES8Pl3nOEFzmQGXs0qEaUQLYRC\" width=\"690\" height=\"169\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1015\u00d7249 15.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Experiment duplicates",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/experiment-duplicates\/1611",
        "Question_created_time":1681848287784,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"<p>Hi! I am trying to integrate experiments usage, but faced with some issues.<\/p>\n<p>I have the following pipeline:<\/p>\n<pre><code class=\"lang-auto\">\nstages:\n  train:\n    cmd: python train.py data\/dataset\/ --config config.yaml --out data\/model.pt\n    deps:\n    - data\/dataset\/\n    - train.py\n    outs:\n    - data\/model.pt\n<\/code><\/pre>\n<p>I run experiment and push:<\/p>\n<pre><code class=\"lang-auto\">dvc exp run -n v1.0 -S config.yaml:model.dim=64\ndvc exp push origin v1.0\n<\/code><\/pre>\n<p>The I try on other server run the same exp:<\/p>\n<pre><code class=\"lang-auto\">dvc exp pull origin v1.0\ndvc exp run -n v1.1 -S config.yaml:model.dim=64 -v \u2014dry\n&gt;&gt;&gt;\ntrain stage modified\ngit status\n&gt;&gt;&gt;\nchanges in config.yaml\n<\/code><\/pre>\n<p>i.e. dvc thinks that there are changes in config and runs pipeline, but experiment was pulled and in a cache<\/p>\n<p>But if I apply this exp:<\/p>\n<pre><code class=\"lang-auto\">dvc exp apply v1.0\ndvc chekout\ndvc exp run -n v1.1 -S config.yaml:model.dim=64 -v \u2014dry\n&gt;&gt;&gt;\nno changes, get from cache\n<\/code><\/pre>\n<p>i.e. dvc sees that experiment is duplicate only if apply it<\/p>\n<p>I expected that dvc allows not to run exps if it was run already. So, my questions are:<\/p>\n<ul>\n<li>what am I doing wrong?<\/li>\n<li>if smth is ok, why dvc can\u2019t detect that new experiment was already run? Is this feature going to be implemented?<\/li>\n<\/ul>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc import from minIO",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-import-from-minio\/1606",
        "Question_created_time":1681743588821,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":52,
        "Question_body":"<p>Hello everyone!<br>\nI try to track some data which already are on my s3 bucket (minIO). I\u2019ve configured a dvc remote with endpoint and credentials but when I run import-url command I\u2019ve the error below:<br>\nBad Request: An error occurred (400) when calling the HeadObject operation<br>\nAnyone have idea to fix or the process to config dvc with minIO?<br>\nThanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC doesn't seem to notice that I delete or restore a folder",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-doesnt-seem-to-notice-that-i-delete-or-restore-a-folder\/1590",
        "Question_created_time":1681138545689,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":54,
        "Question_body":"<p>This is probably a bug in my wetware, but I don\u2019t understand what\u2019s happening here.<\/p>\n<pre><code class=\"lang-bash\">dev on git by-19-channels [$] via conda company \n&gt; ll data\/production_models \ntotal 256K\ndrwxr-xr-x 1 john john  152 Apr 10 10:37 .\ndrwxr-xr-x 1 john john  294 Apr 10 10:36 ..\ndrwxrwxr-x 1 john john   18 Apr 10 09:59 epi_46621521\ndrwxr-xr-x 1 john john   30 Apr  6 10:04 cond_202304041327\ndrwxr-xr-x 1 john john  120 Apr 10 10:04 epi_mc_202304061050\n-rw-rw-r-- 1 john john 249K Apr 10 10:04 pcaer.pkl\n-rw-rw-r-- 1 john john 1.2K Apr 10 10:04 transformer.pkl\n<\/code><\/pre>\n<p>I move epi_46621521 to the Trash using the file browser.<\/p>\n<pre><code class=\"lang-bash\">dev on git by-19-channels [$] via conda company \n&gt; ll data\/production_models\ntotal 256K\ndrwxr-xr-x 1 john john  128 Apr 10 10:38 .\ndrwxr-xr-x 1 john john  294 Apr 10 10:36 ..\ndrwxr-xr-x 1 john john   30 Apr  6 10:04 cond_202304041327\ndrwxr-xr-x 1 john john  120 Apr 10 10:04 epi_mc_202304061050\n-rw-rw-r-- 1 john john 249K Apr 10 10:04 pcaer.pkl\n-rw-rw-r-- 1 john john 1.2K Apr 10 10:04 transformer.pkl\n\ndev on git by-19-channels [$] via conda company \n&gt; dvc status\nData and pipelines are up to date.                                                                                                                                                                                                                          \n<\/code><\/pre>\n<p>DVC didn\u2019t seem to notice.<br>\nI restore the folder from the trash.<\/p>\n<pre><code class=\"lang-bash\">dev on git by-19-channels [$] via conda company \n&gt; ll data\/production_models\ntotal 256K\ndrwxr-xr-x 1 john john  152 Apr 10 10:38 .\ndrwxr-xr-x 1 john john  294 Apr 10 10:36 ..\ndrwxrwxr-x 1 john john   18 Apr 10 09:59 epi_46621521\ndrwxr-xr-x 1 john john   30 Apr  6 10:04 cond_202304041327\ndrwxr-xr-x 1 john john  120 Apr 10 10:04 epi_mc_202304061050\n-rw-rw-r-- 1 john john 249K Apr 10 10:04 pcaer.pkl\n-rw-rw-r-- 1 john john 1.2K Apr 10 10:04 transformer.pkl\n\ndev on git by-19-channels [$] via conda company \n&gt; dvc status               \nData and pipelines are up to date.                                                                                                                                                                                                                          \n<\/code><\/pre>\n<p>Again, DVC didn\u2019t notice a change.<br>\nI try just <code>rm<\/code>ing the folder.<\/p>\n<pre><code class=\"lang-bash\">dev on git by-19-channels [$] via conda company \n&gt; rm -rf data\/production_models\/epi_46621521 \n\ndev on git by-19-channels [$] via conda company \n&gt; dvc status\nData and pipelines are up to date.                                                                                                                                                                                                                          \n<\/code><\/pre>\n<p>And DVC didn\u2019t notice that either.<\/p>\n<p>Just as a sanity check:<\/p>\n<pre><code class=\"lang-bash\">dev on git by-19-channels [$] via conda company \n&gt; touch data\/production_models\/test.txt\n\ndev on git by-19-channels [$] via conda company \n&gt; dvc status\ndata.dvc:                                                                                                                                                                                                                                                   \n\tchanged outs:\n\t\tmodified:           data\n\ndev on git by-19-channels [$] via conda company \n&gt; rm data\/production_models\/test.txt \n\ndev on git by-19-channels [$] via conda company \n&gt; mkdir data\/production_models\/test\n\ndev on git by-19-channels [$] via conda company \n&gt; dvc status\nData and pipelines are up to date.                                                                                                                                                                                                                          \n\ndev on git by-19-channels [$] via conda company \n&gt; \n<\/code><\/pre>\n<p>I must be missing something, right?<\/p>\n<pre><code class=\"lang-plaintext\">DVC 2.51.0\nLinux pop-os 6.2.6-76060206-generic #202303130630~1679424972~22.04~4a8cde1 SMP PREEMPT_DYNAMIC Tue M x86_64 x86_64 x86_64 GNU\/Linux\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking metrics.json",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-metrics-json\/1597",
        "Question_created_time":1681362072833,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":46,
        "Question_body":"<p>Should <code>metrics.json<\/code> be in git?<\/p>\n<p>I\u2019ve noticed that if I switch to a branch, run <code>dvc repro<\/code> then switch back to main, my metrics.json doesn\u2019t change - it doesn\u2019t appear to be tracked by either git or dvc so it\u2019s out of sync with the experiment.<\/p>\n<p>All the model artifacts are tracked by dvc, and params.yaml is tracked by git but metrics.yaml isn\u2019t tracked by either - should I add it to git, or is my stage config wrong<\/p>\n<p>model:<br>\ncmd: python src\/scripts\/train.py<br>\ndeps:<br>\n- data\/datasets\/data.csv<br>\n- model\/tokenizer.json<br>\nouts:<br>\n- model\/model.pt<br>\n- model\/text_transform.pt<br>\n- model\/index_to_name.json<br>\nparams:<br>\n- params.yaml:<br>\nmetrics:<br>\n- metrics.json<\/p>\n<p>It\u2019s not in .gitignore or .dvcignore either so I\u2019m assuming I\u2019ve messed up my config but not sure how to rectify<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc exp run thinks model hasn't change",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-run-thinks-model-hasnt-change\/1595",
        "Question_created_time":1681357760614,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":34,
        "Question_body":"<p>DVC doesn\u2019t seem to be properly tracking my <code>params.yaml<\/code> - whenever I run<\/p>\n<p><code>dvc exp run --set-param model.encoder.kernel_sizes=[7,6,5,4,3,2]<\/code><\/p>\n<p>It updates <code>dvc.lock<\/code> with the new parameters, tells me none of the stages has changed and says it successfully ran the experiment without actually doing anything.<\/p>\n<p>I\u2019ve configured the stage as follows - do I need to explicitly make params.yaml a dependency?<\/p>\n<p>model:<br>\ncmd: python src\/scripts\/train\/model.py<br>\ndeps:<br>\n- data\/datasets\/data.csv<br>\n- model\/tokenizer.json<br>\n- model\/embeddings.pt<br>\nouts:<br>\n- model\/model.pt<br>\n- model\/text_transform.pt<br>\n- model\/index_to_name.json<br>\nparams:<br>\n- params.yaml:<br>\nmetrics:<br>\n- metrics.json<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/forbidden-an-error-occurred-403-when-calling-the-headobject-operation-forbidden\/828",
        "Question_created_time":1627166936199,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":13435,
        "Question_body":"<p>this might sounds similar to what others have asked by I couldn\u2019t really find a solution that fix my problem.<\/p>\n<p>my <em>~\/.aws\/credentials<\/em> looks like<\/p>\n<pre><code class=\"lang-auto\">[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<p><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test<\/code><br>\nand use<\/p>\n<p>I have made my<code> .dvc\/config.local<\/code> to look like<\/p>\n<p>[\u2018remote \u201cmyremote\u201d\u2019]<br>\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test<br>\naccess_key_id = XYZ<br>\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS<br>\nprofile=\u2018testing\u2019<br>\ncredentialpath = \/Users\/nyt21\/.aws\/credentials<\/p>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<p><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/code><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to remove cache for specific targets\/imports?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-remove-cache-for-specific-targets-imports\/1586",
        "Question_created_time":1680780335505,
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":39,
        "Question_body":"<p>Hi, I am working with several quite large datasets, dvc pipelines and a few of big outputs they produce.<\/p>\n<p>Let\u2019s say that I have the following pipeline (pseudo-graph):<\/p>\n<pre><code class=\"lang-auto\">        DVC imports                DVC pipeline             Pipeline outs\n\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502                \u2502        \u2502                \u2502         \u2502               \u2502\n     \u2502                \u2502        \u2502   Stage  AB    \u2502         \u2502   Output      \u2502\n     \u2502    Import      \u2502        \u2502                \u2502         \u2502               \u2502\n     \u2502      A         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502                \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502     AB        \u2502\n     \u2502                \u2502        \u2502                \u2502         \u2502               \u2502\n     \u2502                \u2502        \u2502                \u2502         \u2502               \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u25b2\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n     \u2502                \u2502                \u2502\n     \u2502                \u2502                \u2502\n     \u2502    Import      \u2502                \u2502\n     \u2502      B         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502                \u2502\n     \u2502                \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502                \u2502        \u2502                \u2502         \u2502                \u2502\n     \u2502                \u2502        \u2502                \u2502         \u2502    Output      \u2502\n     \u2502    Import      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502    Stage C     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502                \u2502\n     \u2502      C         \u2502        \u2502                \u2502         \u2502      C         \u2502\n     \u2502                \u2502        \u2502                \u2502         \u2502                \u2502\n     \u2502                \u2502        \u2502                \u2502         \u2502                \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n<\/code><\/pre>\n<p>I download cache for the imports (A, B, C) (<code>dvc pull A.dvc B.dvc C.dvc<\/code>) and start working on the parts of DVC pipeline that reproduce the C outputs. After finishing my work, I run <code>dvc repro<\/code>, commit everything and push DVC cache to remote. Now I want to focus on the part of the pipeline concerned with AB. However I do not need neither the import C, nor the outputs of the C stage. Unfortunately, they are taking my disk space by residing in the cache. Can I somehow remove C and its output from (local) cache but keep A, and B? Thanks for any help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc exp list shows nothing, but VS Code extension still shows experiments",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-list-shows-nothing-but-vs-code-extension-still-shows-experiments\/1583",
        "Question_created_time":1680466360596,
        "Question_answer_count":8,
        "Question_score_count":4,
        "Question_view_count":68,
        "Question_body":"<p>Hi,<br>\nI want to remove all the experiments. I removed some experiments using <code>dvc exp remove<\/code> and there is nothing showing when executing <code>dvc exp list<\/code>, however there are still experiments which were committed showing in the VS Code extension, how can I remove them as well?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error importing dvc.api",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-importing-dvc-api\/1582",
        "Question_created_time":1680251103867,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"<p>Trying follow the guide to read a file from DVC remote (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">open()<\/a>) but having issues import the DVC python API:<\/p>\n<p><code>import dvc.api<\/code><\/p>\n<p>Getting the error:<\/p>\n<pre><code class=\"lang-auto\">File c:\\...\\env\\lib\\site-packages\\dvc_objects\\fs\\__init__.py:5\n...\n--&gt; 157     ResourceEror = resource.error\n    159 _DEFAULT_BATCH_SIZE = 128\n    160 _NOFILES_DEFAULT_BATCH_SIZE = 1280\n\nAttributeError: module 'resource' has no attribute 'error'\n<\/code><\/pre>\n<p>Environment: Windows 11. Python 3.9.6<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"This example in the docs seems a bit odd, and a basic question",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/this-example-in-the-docs-seems-a-bit-odd-and-a-basic-question\/1576",
        "Question_created_time":1679787594381,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":40,
        "Question_body":"<p>I\u2019m a bit of a noob, and I\u2019m trying to understand how <code>dvc install<\/code> changes the steps introduced in Dr. O\u2019Brien\u2019s introductory videos.<\/p>\n<h3>\n<a name=\"question-1-1\" class=\"anchor\" href=\"#question-1-1\"><\/a>Question 1<\/h3>\n<p>The final example on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"noopener nofollow ugc\">this page<\/a> of the docs shows a source file stored in a dvc repository, thus when changed, dictates running repro to create the data, then everything is somehow up to date.<\/p>\n<p>I would think that:<\/p>\n<ul>\n<li>source would be saved to git<\/li>\n<li>running modified source with repro would result in a changed dataset that would need to be added, committed, and pushed.<\/li>\n<\/ul>\n<h3>\n<a name=\"question-2-2\" class=\"anchor\" href=\"#question-2-2\"><\/a>Question 2<\/h3>\n<p>When I\u2019m working on my project and I run git commit, dvc status is run and (let\u2019s assume) shows me that some of my dvc-managed files have changed. Am I correct in these commands (from memory):<\/p>\n<pre><code class=\"lang-bash\"># add alll modified files in data folder\ndvc add data\ndvc push\n# make sure files changed by dvc makes sense\ngit status \ngit add -a -m \u201cdvc modified\u201d\ngit push\n<\/code><\/pre>\n<p>This seems pretty convoluted, so if there\u2019s a better way, would love to know. If not, not looking a gift horse in the mouth <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<h3>\n<a name=\"question-3-3\" class=\"anchor\" href=\"#question-3-3\"><\/a>Question 3<\/h3>\n<p>Okay, a bonus - what about merging, say, a branch into main? Say, the signatures in the *.dvc files don\u2019t match. Is it just a matter of always selecting the branch data over main in the conflicted .dvc files (unless something has gone really wrong)?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it Possible to train data in s3 bucket without downloading to local machine with DVC?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-train-data-in-s3-bucket-without-downloading-to-local-machine-with-dvc\/1429",
        "Question_created_time":1671676533145,
        "Question_answer_count":11,
        "Question_score_count":1,
        "Question_view_count":194,
        "Question_body":"<p>Is it Possible to train data in s3 bucket without downloading to local machine with DVC?<br>\nIf we have TBS of data, is it possible to train without taking to local machine?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc.yaml file deletes",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-yaml-file-deletes\/1573",
        "Question_created_time":1679476703857,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":35,
        "Question_body":"<p>Hello all,<br>\nI am new here. I have been working on a project for sometime and it used data from devices for energy systems and then defines the best model for machine learning on that data. My question is whenever I use <code>dvc repro<\/code> and run the pipeline after the complete run which is after training stage, the dvc.yaml file deletes itself from the folder of the device which I manually created. Any help as to how I can prevent this file deleting by itself? Thank you.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I use DVC with SSH remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-do-i-use-dvc-with-ssh-remote\/279",
        "Question_created_time":1576046193023,
        "Question_answer_count":24,
        "Question_score_count":12,
        "Question_view_count":18128,
        "Question_body":"<p><strong>Q:<\/strong> I\u2019m just getting started with DVC, but I\u2019d like to use it for multiple developers to access the data and share models and code.<\/p>\n<p>I do own the server, but I\u2019m not sure how to use DVC with SSH remote?<\/p>\n<p><strong>A:<\/strong> There\u2019s now an official guide that incorporates a lot of this info thanks to <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a>: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/data-management\/remote-storage\/ssh\">SSH &amp; SFTP<\/a>. . But also feel free to read the full thread.<\/p>\n<hr>\n<p>More context to this question and original versions are <a href=\"https:\/\/discordapp.com\/channels\/485586884165107732\/563406153334128681\/598866528984891403\">here<\/a>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unexpected error when push to SSH remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/unexpected-error-when-push-to-ssh-remote\/1105",
        "Question_created_time":1646732453833,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":754,
        "Question_body":"<p>System: Ubuntu 20.04<\/p>\n<pre><code class=\"lang-auto\">$ dvc remote add --default dataRepo ssh:\/\/leo@l0.162.2.83\/home\/leo\/Videos\n$ dvc push\nERROR: unexpected error - [Errno -2] Name or service not known\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n$ cat .dvc\/config\n[core]\n    remote = datarepo\n['remote \"datarepo\"']\n    url = ssh:\/\/leo@l0.162.2.83\/home\/leo\/Videos\n\n$ dvc push -v\n2022-03-08 17:56:36,153 DEBUG: Preparing to transfer data from '\/home\/leo\/docs\/dvcEx\/.dvc\/cache' to '\/home\/leo\/Videos\/'\n2022-03-08 17:56:36,154 DEBUG: Preparing to collect status from '\/home\/leo\/Videos\/'\n2022-03-08 17:56:36,154 DEBUG: Collecting status from '\/home\/leo\/Videos\/'\n2022-03-08 17:56:36,154 DEBUG: Querying 1 hashes via object_exists\n2022-03-08 17:56:36,262 ERROR: unexpected error - [Errno -2] Name or service not known\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/__init__.py\", line 78, in main\n    ret = cmd.do_run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/transfer.py\", line 154, in transfer\n    status = compare_status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 163, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 124, in status\n    exists = hashes.intersection(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 49, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists, jobs))                                                                                                                                                                      File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 381, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 372, in exists_with_progress\n    ret = self.fs.exists(fs_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 81, in exists\n    return self.fs.exists(path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/ssh.py\", line 124, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/spec.py\", line 68, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 77, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 85, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 65, in sync\n  raise return_result                                                                                                                                                                                                                           File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 25, in _runner                                                                                                                                                    result[0] = await coro                                                                                                                                                                                                                        File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 494, in wait_for                                                                                                                                                                                 return fut.result()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 92, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 568, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/misc.py\", line 223, in __aenter__\n    self._result = await self._coro\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 6892, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 455, in wait_for\n    return await fut\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 298, in _connect\n    _, conn = await loop.create_connection(conn_factory, host, port,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 986, in create_connection\n    infos = await self._ensure_resolved(\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 1365, in _ensure_resolved\n    return await loop.getaddrinfo(host, port, family=family, type=type,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 825, in getaddrinfo\n    return await self.run_in_executor(\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/usr\/lib\/python3.8\/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n------------------------------------------------------------\n2022-03-08 17:56:36,979 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] 'reflink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/__init__.py\", line 78, in main\n    ret = cmd.do_run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/transfer.py\", line 154, in transfer\n    status = compare_status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 163, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 124, in status\n    exists = hashes.intersection(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/data\/status.py\", line 49, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists, jobs))\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 381, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\nFile \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result                                                                                                                                                     [56\/1809]    raise self._exception\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/objects\/db.py\", line 372, in exists_with_progress\n    ret = self.fs.exists(fs_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 81, in exists\n    return self.fs.exists(path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/ssh.py\", line 124, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/spec.py\", line 68, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 77, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 85, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 65, in sync\n    raise return_result\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 494, in wait_for\n    return fut.result()\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/sshfs\/spec.py\", line 92, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 568, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/misc.py\", line 223, in __aenter__\n    self._result = await self._coro\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 6892, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.8\/asyncio\/tasks.py\", line 455, in wait_for\n    return await fut\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/asyncssh\/connection.py\", line 298, in _connect\n    _, conn = await loop.create_connection(conn_factory, host, port,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 986, in create_connection\n    infos = await self._ensure_resolved(\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 1365, in _ensure_resolved\n    return await loop.getaddrinfo(host, port, family=family, type=type,\n  File \"\/usr\/lib\/python3.8\/asyncio\/base_events.py\", line 825, in getaddrinfo\n    return await self.run_in_executor(\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/usr\/lib\/python3.8\/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/local.py\", line 145, in reflink\n    System.reflink(from_info, to_info)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/system.py\", line 112, in reflink\n    System._reflink_linux(source, link_name)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/system.py\", line 96, in _reflink_linux\n    fcntl.ioctl(d.fileno(), FICLONE, s.fileno())\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 69, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 32, in _link\n    raise OSError(\nOSError: [Errno 95] 'reflink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 124, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/home\/leo\/apps\/dvc\/.env\/lib\/python3.8\/site-packages\/dvc\/fs\/utils.py\", line 77, in _try_links\n    raise OSError(\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/.3YzRahnxL5SVcRyPvvgfDR.tmp'\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/.3YzRahnxL5SVcRyPvvgfDR.tmp'\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/.3YzRahnxL5SVcRyPvvgfDR.tmp'\n2022-03-08 17:56:36,982 DEBUG: Removing '\/home\/leo\/docs\/dvcEx\/.dvc\/cache\/.33LPManfkHLF5iPjktauyB.tmp'\n2022-03-08 17:56:36,986 DEBUG: Version info for developers:\nDVC version: 2.9.5 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.4.0-84-generic-x86_64-with-glibc2.29\nSupports:\n        azure (adlfs = 2022.2.0, knack = 0.9.0, azure-identity = 1.8.0),\n        gdrive (pydrive2 = 1.10.0),\n        gs (gcsfs = 2022.2.0),\n        hdfs (fsspec = 2022.2.0, pyarrow = 7.0.0),\n        webhdfs (fsspec = 2022.2.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        s3 (s3fs = 2022.2.0, boto3 = 1.20.24),\n        ssh (sshfs = 2021.11.2),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.4),\n        webdavs (webdav4 = 0.9.4)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/sda5\nCaches: local\nRemotes: ssh\nWorkspace directory: ext4 on \/dev\/sda5\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-03-08 17:56:36,988 DEBUG: Analytics is enabled.\n2022-03-08 17:56:37,078 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpb10lmwwg']'\n2022-03-08 17:56:37,079 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpb10lmwwg']'\n<\/code><\/pre>\n<p>I can <code>ssh<\/code> and <code>sftp<\/code> to <a href=\"mailto:leo@10.162.2.83\">leo@10.162.2.83<\/a> with SSH keys.<\/p>\n<p>How to fix this error?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc remote drive",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-remote-drive\/1569",
        "Question_created_time":1679392193507,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":39,
        "Question_body":"<p>Hi DVC,<\/p>\n<p>I am following along (Mac system; MacBook Air) one of the DVC tutorials and I am using the latest DVC version.<br>\nI am using gdrive for remote storage. I have created a folder data\/remote, and I followed all the instructions from the tutorial.<br>\nwhen I do a dvc push, I get the following error:<\/p>\n<pre><code class=\"lang-auto\"> $ dvc push\n\nERROR: unexpected error - : &lt;HttpError 404 when requesting https:\/\/www.googleapis.com\/drive\/v2\/files\/1uLgZuvVlB0fnU-Fytp9kWdRyBjLmmktu?fields=driveId&amp;supportsAllDrives=true&amp;alt=json returned \"File not found: 1uLgZuvVlB0fnU-Fytp9kWdRyBjLmmktu\". Details: \"[{'message': 'File not found: 1uLgZuvVlB0fnU-Fytp9kWdRyBjLmmktu', 'domain': 'global', 'reason': 'notFound', 'location': 'file', 'locationType': 'other'}]\"&gt;\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n<\/code><\/pre>\n<p>Thanks<\/p>\n<p>Ravi<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to limit the size of the local cache directory?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-limit-the-size-of-the-local-cache-directory\/1565",
        "Question_created_time":1678888393262,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":59,
        "Question_body":"<p>Just getting set up and figuring out the config that will work best for us - is there a way to set warnings or limits at the system level on the local cache directory size and\/or the maximum file size of a single <code>pull<\/code>?<\/p>\n<p>It looks as though <code>add --to-remote<\/code> and <code>pull &lt;target&gt;<\/code> will cover most of our use cases, but I\u2019m still fully expecting to run a straightforward <code>dvc pull<\/code> at some point without thinking and accidentally fill all the space on the machine I\u2019m using.<\/p>\n<p>Are there any config settings to put some guardrails around that?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data directory not tracked by git",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/data-directory-not-tracked-by-git\/1562",
        "Question_created_time":1678786523329,
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"<p>Hi,<br>\nI\u2019m new to dvc and have just created my first dvc controlled project and it has been a great experience until now.<br>\nFollowing the documentation, I\u2019ve added the directory that contained all my data with<\/p>\n<pre><code class=\"lang-auto\">dvc add my_data_directory\n<\/code><\/pre>\n<p>DVC added my_data_directory accordingly to the top level .gitignore file.<\/p>\n<p>The problem is now that I am not able to check out the remote storage of my data on a different machine, since<br>\nthe root data-directory (my_data_directory) doesn\u2019t exist in the git-repository.  and accordingly missing when I clone the repo.<\/p>\n<p>Additionally, our data is organized in sub-folders which i would like to track with git.<\/p>\n<p>How could I achieve this? Does the .gitignore file need some adaption?<\/p>\n<p>The structure of my data directory is the following:<\/p>\n<p>my_data_directory<br>\n|<br>\n|<br>\n|\u2013&gt; subject_1_dir \u2014&gt; data<br>\n|\u2013&gt; subject_2_dir \u2014&gt; data<br>\n|\u2013&gt; subject_ 3_dir \u2014&gt; data<\/p>\n<p>How could I tell git to track directory names, and dvc to track the containing files?<\/p>\n<p>Thanks for your help<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"Error: unexpected error - Can't create any SFTP connections!\"",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-cant-create-any-sftp-connections\/1559",
        "Question_created_time":1678653943289,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":106,
        "Question_body":"<p>Not sure where to go with this.<br>\nDVC can\u2019t create an sftp connection, but running the sftp command works.<br>\nThanks for any suggestions.<\/p>\n<pre><code class=\"lang-plaintext\">x dvc status\ndata.dvc:                                                                                                                                                                                                       \n\tchanged outs:\n\t\tmodified:           data\nlogs.dvc:\n\tchanged outs:\n\t\tmodified:           logs\n(base) \nEpilepsy on git powertransformec-pca [$!&gt;] took 6s \n&gt; dvc add logs data\n100% Adding...|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2\/2 [00:22, 11.38s\/file]\n(base) \nEpilepsy on git powertransformec-pca [$!+&gt;] took 26s \n&gt; dvc push\nERROR: unexpected error - Can't create any SFTP connections!                                                                                                                                                   \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n(base) \nEpilepsy on git powertransformec-pca [$!+&gt;] took 8s \nx git st\nOn branch powertransformec-pca\nYour branch is ahead of 'origin\/powertransformec-pca' by 3 commits.\n  (use \"git push\" to publish your local commits)\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n\tmodified:   data.dvc\n\tmodified:   logs.dvc\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n\tmodified:   src\/train.ipynb\n\n(base) \nEpilepsy on git powertransformec-pca [$!+&gt;] \n&gt; git commit -a -m \"dvc modified\"\nData and pipelines are up to date.                                                                                                                                                                              \n[powertransformec-pca 34a814f] dvc modified\n 3 files changed, 15 insertions(+), 41 deletions(-)\n(base) \nEpilepsy on git powertransformec-pca [$&gt;] took 8s \n&gt; git push\nERROR: unexpected error - Can't create any SFTP connections!                                                                                                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\nerror: failed to push some refs to 'github.com:JohnAtl\/epilepsy.git'\n(base) \nEpilepsy on git powertransformec-pca [$&gt;] took 6s \nx dvc status\nData and pipelines are up to date.                                                                                                                                                                              \n(base) \nEpilepsy on git powertransformec-pca [$&gt;] took 7s \n&gt; dvc push\nERROR: unexpected error - Can't create any SFTP connections!                                                                                                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n(base) \nEpilepsy on git powertransformec-pca [$&gt;] took 5s \nx sftp johns-imac-pro.local\nConnected to johns-imac-pro.local.\nsftp&gt; exit\n(base) \nEpilepsy on git powertransformec-pca [$!&gt;] took 3s\nx dvc remote list  \nimac\tssh:\/\/johns-imac-pro.local:\/Volumes\/Data\/dvc\/epilepsy\n(base) \nEpilepsy on git powertransformec-pca [$!&gt;] took 3s \n&gt; \n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking external data on google cloud storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-external-data-on-google-cloud-storage\/1557",
        "Question_created_time":1678628157326,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":36,
        "Question_body":"<p>Hi,<\/p>\n<p>I have data and model already existing on my google storage bucket which i want to be able to track along with the other project files in the git repo. i\u2019ve come across the \u201cadd --external\u201d option but when i try to add a whole directory, it outputs \u201cERROR: unexpected error\u201d without any further elaboration. when I try to add a single file instead of a whole directory, it outputs \u201cERROR: checkout failed for the following targets: \u2026 is your cache up to data?\u201d and as a result that single file is lost on my google bucket (is this normal?) and cache sub directory remains empty!<br>\nan image of both errors and my config file is attached. the blue bar is my local path while the red is my remote path.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/cc825fca7e15b64750ea279aad41d3d23a4686a4.png\" data-download-href=\"\/uploads\/short-url\/tbaJdHFWvHkzCaTkmXszduhEZZq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/cc825fca7e15b64750ea279aad41d3d23a4686a4.png\" alt=\"image\" data-base62-sha1=\"tbaJdHFWvHkzCaTkmXszduhEZZq\" width=\"690\" height=\"345\" data-dominant-color=\"8E8587\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1065\u00d7534 14.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What is the suggested way to use `desc`, `type`, `label`, and `meta` in imports?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-is-the-suggested-way-to-use-desc-type-label-and-meta-in-imports\/1553",
        "Question_created_time":1678444929262,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":46,
        "Question_body":"<p>DVC imports have options for a few extra variables like shown here in the docs: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url#options\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">import-url<\/a><\/p>\n<p>However, it is not explained what purpose each of them serves. E.g. it is unclear what is the difference between <code>label<\/code> and <code>desc<\/code>, or whether the <code>meta<\/code> key-value pairs are used by any other feature of DVC. Also I am not sure what <code>type<\/code> is for, is it the same <em>type<\/em> as storage type (e.g. s3), or rather is it a note about files type (xml or pngs)? Are these options totally arbitrary in interpretation and up to the user, or is there any correct way to distinguish them?<\/p>\n<p>If these values are used by other DVC features it wold be great to link them in the docs. If they are self-sufficient, but there is a suggested way of using them it would be nice to leave a note about the intended usage in the docs. If it is totally up to the user to make sense of how to utilize them best it could also be noted.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pull Data only when necessary and delete it afterwards",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pull-data-only-when-necessary-and-delete-it-afterwards\/1551",
        "Question_created_time":1678382721544,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":48,
        "Question_body":"<p>Hi community,<\/p>\n<p>we are currently testing DVC for our ML-Project. We have to deal with a lot of large files which we only need occasionally. In order to not occupy the whole disk space, I am wondering, if there is the possibility to pull the actual data only when needed and delete it again when the work is done.<\/p>\n<p>When I delete the actual file from the disk, the DVC status obviously changes. I\u2019d like to omit this behavior as I still want the file to be tracked, I just don\u2019t want an actual copy on my disk anymore.<\/p>\n<p>Thanks so much for your help.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc and S3 permissions management",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-s3-permissions-management\/887",
        "Question_created_time":1631267338282,
        "Question_answer_count":3,
        "Question_score_count":4,
        "Question_view_count":598,
        "Question_body":"<p>Hello dvc people!<\/p>\n<p>At my team we are experimenting with dvc as an interface to work with trained models and datasets. So far it has been working nicely, but we found a mayor roadblock when it comes to permissions management.<\/p>\n<p>In our ideal scenario, when a teammate gets access to one of our repositories in GitHub that person will only have access to the data from that particular repository. This permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in AWS.<\/p>\n<p>However, this seems challenging to achieve through S3 + DVC. Our considered solution right now is to have a single S3 bucket with all our DVC repositories. If we grant access to all teammates to the whole bucket it would mean that they will have access to all the company\u2019s data, which is less than ideal. So we were considering having a \u201cJunior\u201d role which we will have to give access manually to whatever specific S3 folders they need and a \u201cSenior\u201d role with access to everything.<\/p>\n<p>This solution is suboptimal as we will need to handle different permissions for GitHub and for S3, which adds overhead.<\/p>\n<p>Is there any other pattern we are missing here? Is there any easy way for a teammate to be given access to ONLY the dvc remote from the GitHub repository automatically?<\/p>\n<p>Thanks for the help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/relaxed.png?v=10\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - database is locked while trying to run dvc on Azure",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-database-is-locked-while-trying-to-run-dvc-on-azure\/1549",
        "Question_created_time":1678287265338,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":48,
        "Question_body":"<p>I am unable to run dvc on Azure ML instance, even though it works on local computer. I get <code>ERROR: unexpected error - database is locked<\/code> . Not sure why, I am not accessing any database.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ways to connect stages (DAG)",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/ways-to-connect-stages-dag\/1547",
        "Question_created_time":1677871165150,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":50,
        "Question_body":"<p>Hi,<br>\nAs far as I know the stages are connected through the dependency\/output pair relation. Is there another way I can connect them?<br>\nLet\u2019s say I have a script for data validation and it is not producing any output but I still want it to be run first before any other stages. I also want the pipeline to stop if the script throws any exception.<\/p>\n<p>Or is this a bad design?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Access remote data instead of downloading it",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/access-remote-data-instead-of-downloading-it\/1537",
        "Question_created_time":1677053979636,
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":89,
        "Question_body":"<p>Hello,<\/p>\n<p>I would like to version a Dataset of 20 TB that continue to increase in a bucket S3.<\/p>\n<p>Some colleagues and myself will run some experiments using this data on our PCs, there is a way to access the remote (using boto3 or other option) data instead of downloading it with dvc pull ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc push error on Self signed certificate",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-on-self-signed-certificate\/1507",
        "Question_created_time":1675861031553,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":273,
        "Question_body":"<p>Hi all,<br>\nUsing dvc push on remote defined by minio (S3 compatible) I got this well known error <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>dvc -v push -r minio --all-commits --all-tags --all-branches<br>\nERROR: unexpected error - SSL validation failed for <a href=\"https:\/\/minio.dev.csai.crigen.myengie.com\/dvc\/04\/c083370107dcaa417e33081249ad97.dir\" rel=\"noopener nofollow ugc\">https:\/\/minio.dev.csai.crigen.myengie.com\/dvc\/04\/c083370107dcaa417e33081249ad97.dir<\/a> Cannot connect to host <a href=\"http:\/\/minio.dev.csai.crigen.myengie.com:443\" rel=\"noopener nofollow ugc\">minio.dev.csai.crigen.myengie.com:443<\/a> ssl:True [SSLCertVerificationError: (1, \u2018[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)\u2019)]<\/p>\n<h2>\n<a name=\"dvc-doctor-output-is-dvc-version-2381-conda-1\" class=\"anchor\" href=\"#dvc-doctor-output-is-dvc-version-2381-conda-1\"><\/a>dvc doctor output is<br>\nDVC version: 2.38.1 (conda)<\/h2>\n<p>Platform: Python 3.10.8 on Windows-10-10.0.19044-SP0<br>\nSubprojects:<br>\ndvc_data = 0.28.4<br>\ndvc_objects = 0.14.0<br>\ndvc_render = 0.0.15<br>\ndvc_task = 0.1.8<br>\ndvclive = 1.2.2<br>\nscmrepo = 0.1.4<br>\nSupports:<br>\nhttp (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\nhttps (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\ns3 (s3fs = 2023.1.0, boto3 = 1.24.59)<br>\nCache types: hardlink<br>\nCache directory: NTFS on C:<br>\nCaches: local<br>\nRemotes: local, s3<br>\nWorkspace directory: NTFS on C:<br>\nRepo: dvc, git<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Format error: extra keys not allowed @ data[u'outs'][0][u'size']",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/format-error-extra-keys-not-allowed-data-uouts-0-usize\/966",
        "Question_created_time":1636706233863,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":1833,
        "Question_body":"<p>I\u2019m trying to integrate dvc with my project for easy pull\/push of AI\/ML models, while the dvc works fine on my Windows, When I placed the code on an ubuntu machine and after setting up the cloud credentials and adding the remote storage. I tried <code>dvc status<\/code> command it started giving the following error. I\u2019m not able to interact with dvc at all.<\/p>\n<p><strong>format error: extra keys not allowed @ data[u\u2019outs\u2019][0][u\u2019size\u2019]<\/strong><\/p>\n<p>Does anybody knows the solution for the same?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc experiments missing names",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-experiments-missing-names\/1540",
        "Question_created_time":1677172456444,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":160,
        "Question_body":"<h1>\n<a name=\"initial-issue-dead-experiments-missing-names-1\" class=\"anchor\" href=\"#initial-issue-dead-experiments-missing-names-1\"><\/a>initial issue (dead experiments, missing names)<\/h1>\n<pre><code class=\"lang-auto\">  \u251c\u2500\u2500 2f2a178            Feb 18, 2023   Running   -          0.70166   0.83538    0.42834                46791    0.15652              185381            \n  \u251c\u2500\u2500 9dc97e9            Feb 18, 2023   Running   -          0.70166   0.83538    0.42834                46791    0.15652              185381            \n  \u251c\u2500\u2500 65a6585            Feb 18, 2023   Running   dvc-task   0.70166   0.83538    0.42834                46791    0.15652              185381            \n  \u251c\u2500\u2500 e5c1060            Feb 18, 2023   Running   -          0.70166   0.83538    0.42834                46791    0.15652              185381            \n  \u251c\u2500\u2500 9aa34d4 [exp_50]   Feb 18, 2023   Queued    -                -         -          -                    -          -                   -            \n  \u2514\u2500\u2500 f4c9181 [exp_51]   Feb 18, 2023   Queued    -   \n<\/code><\/pre>\n<p>Above is what <code>dvc exp show<\/code> looks like. When I call <code>dvc queue kill 2f2a178<\/code> for example, I get:<\/p>\n<pre><code class=\"lang-auto\">ERROR: '2f2a178' is not a valid queued experiment name\n<\/code><\/pre>\n<p>I ran the experiments from vscode and set the number of jobs to 4. At this point it looks like DVC only thinks 1 is running? However none of them are actually running. <code>dvc queue status<\/code> is too slow to even run on the computer, it gets cancelled by the OS or something. See <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-exp-show-dvc-queue-status-execution-time\/1531\/6\" class=\"inline-onebox\">Dvc exp show, dvc queue status execution time - #6 by gregstarr<\/a>. Where did the experiment names go? When I check for running processes, there is only a single process related to dvc:<\/p>\n<pre><code class=\"lang-auto\">starrgw1 256549  0.0  0.0 568940 21844 ?        SNl  11:37   0:00 [...]\/.vscode-server\/bin\/[...]\/node [...]\/.vscode-server\/extensions\/iterative.dvc-0.6.10\/dist\/node_modules\/dvc-vscode-lsp\/dist\/server.js --node-ipc --clientProcessId=256403\n<\/code><\/pre>\n<p>Usually when multiple experiments are running there are a bunch of dvc-related processes.<\/p>\n<p>Here is my dvc doctor:<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.41.1 (pip)\n---------------------------------\nPlatform: Python 3.9.11 on Linux-3.10.0-693.el7.x86_64-x86_64-with-glibc2.17\nSubprojects:\n        dvc_data = 0.29.0\n        dvc_objects = 0.14.1\n        dvc_render = 0.0.17\n        dvc_task = 0.1.9\n        dvclive = 1.3.2\n        scmrepo = 0.1.5\nSupports:\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.8.3)\nCache types: symlink\nCache directory: lustre on 192.168.199.212@o2ib:192.168.199.213@o2ib:\/scratch\nCaches: local\nRemotes: local\nWorkspace directory: nfs on master:\/home\nRepo: dvc, git\n<\/code><\/pre>\n<h1>\n<a name=\"other-issue-dvc-slowness-2\" class=\"anchor\" href=\"#other-issue-dvc-slowness-2\"><\/a>other issue (dvc slowness)<\/h1>\n<p>I tried to downgrade to dvc 2.9.2 to avoid the slowness and got this error when running <code>dvc exp show<\/code><\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py\", line 54, in main\n    cmd = args.func(args)\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py\", line 35, in __init__\n    from dvc.repo import Repo\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 13, in &lt;module&gt;\n    from dvc.ignore import DvcIgnoreFilter\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/ignore.py\", line 10, in &lt;module&gt;\n    from dvc.fs.base import FileSystem\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/__init__.py\", line 5, in &lt;module&gt;\n    from .azure import AzureFileSystem\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/azure.py\", line 5, in &lt;module&gt;\n    from fsspec.asyn import fsspec_loop\nImportError: cannot import name 'fsspec_loop' from 'fsspec.asyn' (\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/fsspec\/asyn.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/starrgw1\/.local\/bin\/dvc\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py\", line 84, in main\n    from dvc.info import get_dvc_info\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/info.py\", line 10, in &lt;module&gt;\n    from dvc.fs import FS_MAP, get_fs_cls, get_fs_config\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/__init__.py\", line 5, in &lt;module&gt;\n    from .azure import AzureFileSystem\n  File \"\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/azure.py\", line 5, in &lt;module&gt;\n    from fsspec.asyn import fsspec_loop\nImportError: cannot import name 'fsspec_loop' from 'fsspec.asyn' (\/home\/starrgw1\/.local\/lib\/python3.8\/site-packages\/fsspec\/asyn.py)\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: failed to pull data from the cloud - CI-CD pipeline",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-failed-to-pull-data-from-the-cloud-ci-cd-pipeline\/1535",
        "Question_created_time":1677004566284,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":95,
        "Question_body":"<p>Hi,<br>\nI am getting this error \u201cERROR: failed to pull data from the cloud - Checkout failed for following targets:\u201d  with some files. The details are below<\/p>\n<ul>\n<li>Creating CI-CD pipeline for ML Model<br>\ncommands in git workflow yaml  file are below<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">  - name: Dvc pull updated files\n      run: | \n         dvc pull -v\n    - name: Run pipeline\n      run: |\n         dvc repro -f \n<\/code><\/pre>\n<pre><code class=\"lang-auto\">2023-02-21 17:58:37,096 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/processed\/churn_train.csv\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/processed\/churn_test.csv\n\/home\/runner\/work\/MLOTest\/MLOTest\/models\/model.joblib\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/external\/train.csv\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/raw\/train.csv\n<\/code><\/pre>\n<p>dvc version: 2.45.1<\/p>\n<p>Please give your inputs to resolve the issue<\/p>\n<p>Thanks,<br>\nVeerendra<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc push - ERROR: failed to transfer",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-failed-to-transfer\/1533",
        "Question_created_time":1676995646296,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":112,
        "Question_body":"<p>hi, i just  created a new repo with dvc and now I added 1 data file, but I can\u2019t seem to manage to do <code>dvc push<\/code>, what could be the reason?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d.png\" data-download-href=\"\/uploads\/short-url\/aZ5U8AjYSacHl6fq6MwI4Pk4quN.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_690x113.png\" alt=\"image\" data-base62-sha1=\"aZ5U8AjYSacHl6fq6MwI4Pk4quN\" width=\"690\" height=\"113\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_690x113.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_1035x169.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d.png 2x\" data-dominant-color=\"3F4349\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1333\u00d7220 26.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc push error, remotedir with minio",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-remotedir-with-minio\/1532",
        "Question_created_time":1676985547136,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"<p>S3 bucket on minio<br>\nis ok with mc<\/p>\n<p>dvc push -vv -r minio --all-commits --all-tags --all-branches<\/p>\n<h2>\n<a name=\"h-2023-02-21-134539387-debug-collecting-status-from-dvc-0-querying-remote-cache-02-0000-filess2023-02-21-134539387-debug-querying-2-oids-via-object_exists-2023-02-21-134539903-error-unexpected-error-forbidden-an-error-occurred-403-when-calling-the-headobject-operation-forbidden-1\" class=\"anchor\" href=\"#h-2023-02-21-134539387-debug-collecting-status-from-dvc-0-querying-remote-cache-02-0000-filess2023-02-21-134539387-debug-querying-2-oids-via-object_exists-2023-02-21-134539903-error-unexpected-error-forbidden-an-error-occurred-403-when-calling-the-headobject-operation-forbidden-1\"><\/a>2023-02-21 13:45:39,387 DEBUG: Collecting status from \u2018dvc\u2019<br>\n0% Querying remote cache|                                                                 |0\/2 [00:00&lt;?,    ?files\/s]2023-02-21 13:45:39,387 DEBUG: Querying 2 oids via object_exists<br>\n2023-02-21 13:45:39,903 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/h2>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\s3fs\\core.py\u201d, line 112, in _error_wrapper<br>\nreturn await func(*args, **kwargs)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\aiobotocore\\client.py\u201d, line 358, in _make_api_call<br>\nraise error_class(parsed_response, operation_name)<br>\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n<p>The above exception was the direct cause of the following exception:<\/p>\n<h2>\n<a name=\"traceback-most-recent-call-last-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvccli__init__py-line-185-in-main-ret-cmddo_run-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcclicommandpy-line-22-in-do_run-return-selfrun-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvccommandsdata_syncpy-line-59-in-run-processed_files_count-selfrepopush-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcrepo__init__py-line-48-in-wrapper-return-frepo-args-kwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcrepopushpy-line-91-in-push-result-selfcloudpush-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcdata_cloudpy-line-143-in-push-return-selftransfer-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcdata_cloudpy-line-124-in-transfer-return-transfersrc_odb-dest_odb-objs-kwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfiletransferpy-line-190-in-transfer-status-compare_status-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfilestatuspy-line-179-in-compare_status-dest_exists-dest_missing-status-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfilestatuspy-line-136-in-status-exists-hashesintersection-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfilestatuspy-line-56-in-_indexed_dir_hashes-dir_existsupdate-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagestqdmstdpy-line-1195-in-__iter__-for-obj-in-iterable-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_objectsdbpy-line-308-in-list_oids_exists-yield-from-itertoolscompressoids-in_remote-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-621-in-result_iterator-yield-_result_or_cancelfspop-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-319-in-_result_or_cancel-return-futresulttimeout-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-458-in-result-return-self__get_result-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-403-in-__get_result-raise-self_exception-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfuturesthreadpy-line-58-in-run-result-selffnselfargs-selfkwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_objectsfsbasepy-line-308-in-exists-return-selffsexistspath-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesfsspecasynpy-line-113-in-wrapper-return-syncselfloop-func-args-kwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesfsspecasynpy-line-98-in-sync-raise-return_result-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesfsspecasynpy-line-53-in-_runner-result0-await-coro-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-974-in-_exists-await-self_infopath-bucket-key-version_idversion_id-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-1238-in-_info-out-await-self_call_s3-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-339-in-_call_s3-return-await-_error_wrapper-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-139-in-_error_wrapper-raise-err-permissionerror-forbidden-2\" class=\"anchor\" href=\"#traceback-most-recent-call-last-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvccli__init__py-line-185-in-main-ret-cmddo_run-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcclicommandpy-line-22-in-do_run-return-selfrun-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvccommandsdata_syncpy-line-59-in-run-processed_files_count-selfrepopush-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcrepo__init__py-line-48-in-wrapper-return-frepo-args-kwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcrepopushpy-line-91-in-push-result-selfcloudpush-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcdata_cloudpy-line-143-in-push-return-selftransfer-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvcdata_cloudpy-line-124-in-transfer-return-transfersrc_odb-dest_odb-objs-kwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfiletransferpy-line-190-in-transfer-status-compare_status-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfilestatuspy-line-179-in-compare_status-dest_exists-dest_missing-status-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfilestatuspy-line-136-in-status-exists-hashesintersection-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_datahashfilestatuspy-line-56-in-_indexed_dir_hashes-dir_existsupdate-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagestqdmstdpy-line-1195-in-__iter__-for-obj-in-iterable-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_objectsdbpy-line-308-in-list_oids_exists-yield-from-itertoolscompressoids-in_remote-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-621-in-result_iterator-yield-_result_or_cancelfspop-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-319-in-_result_or_cancel-return-futresulttimeout-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-458-in-result-return-self__get_result-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfutures_basepy-line-403-in-__get_result-raise-self_exception-file-cusersjb6407anaconda3envsdvc-venvlibconcurrentfuturesthreadpy-line-58-in-run-result-selffnselfargs-selfkwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesdvc_objectsfsbasepy-line-308-in-exists-return-selffsexistspath-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesfsspecasynpy-line-113-in-wrapper-return-syncselfloop-func-args-kwargs-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesfsspecasynpy-line-98-in-sync-raise-return_result-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagesfsspecasynpy-line-53-in-_runner-result0-await-coro-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-974-in-_exists-await-self_infopath-bucket-key-version_idversion_id-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-1238-in-_info-out-await-self_call_s3-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-339-in-_call_s3-return-await-_error_wrapper-file-cusersjb6407anaconda3envsdvc-venvlibsite-packagess3fscorepy-line-139-in-_error_wrapper-raise-err-permissionerror-forbidden-2\"><\/a>Traceback (most recent call last):<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\cli_<em>init<\/em>_.py\u201d, line 185, in main<br>\nret = cmd.do_run()<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\cli\\command.py\u201d, line 22, in do_run<br>\nreturn self.run()<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\commands\\data_sync.py\u201d, line 59, in run<br>\nprocessed_files_count = self.repo.push(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\repo_<em>init<\/em>_.py\u201d, line 48, in wrapper<br>\nreturn f(repo, *args, **kwargs)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\repo\\push.py\u201d, line 91, in push<br>\nresult = self.cloud.push(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\data_cloud.py\u201d, line 143, in push<br>\nreturn self.transfer(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc\\data_cloud.py\u201d, line 124, in transfer<br>\nreturn transfer(src_odb, dest_odb, objs, **kwargs)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc_data\\hashfile\\transfer.py\u201d, line 190, in transfer<br>\nstatus = compare_status(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc_data\\hashfile\\status.py\u201d, line 179, in compare_status<br>\ndest_exists, dest_missing = status(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc_data\\hashfile\\status.py\u201d, line 136, in status<br>\nexists = hashes.intersection(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc_data\\hashfile\\status.py\u201d, line 56, in _indexed_dir_hashes<br>\ndir_exists.update(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\tqdm\\std.py\u201d, line 1195, in <strong>iter<\/strong><br>\nfor obj in iterable:<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc_objects\\db.py\u201d, line 308, in list_oids_exists<br>\nyield from itertools.compress(oids, in_remote)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\concurrent\\futures_base.py\u201d, line 621, in result_iterator<br>\nyield _result_or_cancel(fs.pop())<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\concurrent\\futures_base.py\u201d, line 319, in _result_or_cancel<br>\nreturn fut.result(timeout)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\concurrent\\futures_base.py\u201d, line 458, in result<br>\nreturn self.__get_result()<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\concurrent\\futures_base.py\u201d, line 403, in __get_result<br>\nraise self._exception<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\concurrent\\futures\\thread.py\u201d, line 58, in run<br>\nresult = self.fn(*self.args, **self.kwargs)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\dvc_objects\\fs\\base.py\u201d, line 308, in exists<br>\nreturn self.fs.exists(path)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\fsspec\\asyn.py\u201d, line 113, in wrapper<br>\nreturn sync(self.loop, func, *args, **kwargs)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\fsspec\\asyn.py\u201d, line 98, in sync<br>\nraise return_result<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\fsspec\\asyn.py\u201d, line 53, in _runner<br>\nresult[0] = await coro<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\s3fs\\core.py\u201d, line 974, in _exists<br>\nawait self._info(path, bucket, key, version_id=version_id)<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\s3fs\\core.py\u201d, line 1238, in _info<br>\nout = await self._call_s3(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\s3fs\\core.py\u201d, line 339, in _call_s3<br>\nreturn await _error_wrapper(<br>\nFile \u201cC:\\Users\\JB6407\\Anaconda3\\envs\\dvc-venv\\lib\\site-packages\\s3fs\\core.py\u201d, line 139, in _error_wrapper<br>\nraise err<br>\nPermissionError: Forbidden<\/h2>\n<h2>\n<a name=\"h-2023-02-21-134540066-debug-link-type-reflink-is-not-available-errno-129-no-more-link-types-left-to-try-out-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-link-type-symlink-is-not-available-winerror-1314-le-client-ne-dispose-pas-dun-privilge-ncessaire-cusersjb6407documentsmooc-dvcexample-versioningdvccaches6ufwuno7wpqkqvoaaof37tmp-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcexample-versioningdvccaches6ufwuno7wpqkqvoaaof37tmp-2023-02-21-134541407-debug-version-info-for-developers-dvc-version-2381-conda-3\" class=\"anchor\" href=\"#h-2023-02-21-134540066-debug-link-type-reflink-is-not-available-errno-129-no-more-link-types-left-to-try-out-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-link-type-symlink-is-not-available-winerror-1314-le-client-ne-dispose-pas-dun-privilge-ncessaire-cusersjb6407documentsmooc-dvcexample-versioningdvccaches6ufwuno7wpqkqvoaaof37tmp-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcyma3oz9ux849yih3y3ja4jtmp-2023-02-21-134540066-debug-removing-cusersjb6407documentsmooc-dvcexample-versioningdvccaches6ufwuno7wpqkqvoaaof37tmp-2023-02-21-134541407-debug-version-info-for-developers-dvc-version-2381-conda-3\"><\/a>2023-02-21 13:45:40,066 DEBUG: link type reflink is not available ([Errno 129] no more link types left to try out)<br>\n2023-02-21 13:45:40,066 DEBUG: Removing \u2018C:\\Users\\JB6407\\Documents\\Mooc-DVC.YMA3oz9UX849yiH3y3Ja4j.tmp\u2019<br>\n2023-02-21 13:45:40,066 DEBUG: Removing \u2018C:\\Users\\JB6407\\Documents\\Mooc-DVC.YMA3oz9UX849yiH3y3Ja4j.tmp\u2019<br>\n2023-02-21 13:45:40,066 DEBUG: link type symlink is not available ([WinError 1314] Le client ne dispose pas d\u2019un privil\u00e8ge n\u00e9cessaire: \u2018C:\/Users\/JB6407\/Documents\/Mooc-DVC\/example-versioning\/.dvc\/cache\/.S6ufwuNo7wpqkQVoaaoF37.tmp\u2019 \u2192 \u2018C:\/Users\/JB6407\/Documents\/Mooc-DVC\/.YMA3oz9UX849yiH3y3Ja4j.tmp\u2019)<br>\n2023-02-21 13:45:40,066 DEBUG: Removing \u2018C:\\Users\\JB6407\\Documents\\Mooc-DVC.YMA3oz9UX849yiH3y3Ja4j.tmp\u2019<br>\n2023-02-21 13:45:40,066 DEBUG: Removing \u2018C:\\Users\\JB6407\\Documents\\Mooc-DVC\\example-versioning.dvc\\cache.S6ufwuNo7wpqkQVoaaoF37.tmp\u2019<br>\n2023-02-21 13:45:41,407 DEBUG: Version info for developers:<br>\nDVC version: 2.38.1 (conda)<\/h2>\n<p>Platform: Python 3.10.8 on Windows-10-10.0.19044-SP0<br>\nSubprojects:<br>\ndvc_data = 0.28.4<br>\ndvc_objects = 0.14.0<br>\ndvc_render = 0.0.15<br>\ndvc_task = 0.1.8<br>\ndvclive = 1.2.2<br>\nscmrepo = 0.1.4<br>\nSupports:<br>\nhttp (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\nhttps (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\ns3 (s3fs = 2023.1.0, boto3 = 1.26.70)<br>\nCache types: hardlink<br>\nCache directory: NTFS on C:<br>\nCaches: local<br>\nRemotes: local, s3, s3<br>\nWorkspace directory: NTFS on C:<br>\nRepo: dvc, git<\/p>\n<p>Having any troubles? Hit us up at <a href=\"https:\/\/dvc.org\/support\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Support<\/a>, we are always happy to help!<br>\n2023-02-21 13:45:41,422 DEBUG: Analytics is enabled.<br>\n2023-02-21 13:45:41,656 DEBUG: Trying to spawn \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018C:\\Users\\JB6407\\AppData\\Local\\Temp\\tmp9h1ybzep\u2019]\u2019<br>\n2023-02-21 13:45:41,656 DEBUG: Spawned \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018C:\\Users\\JB6407\\AppData\\Local\\Temp\\tmp9h1ybzep\u2019]\u2019<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc exp show, dvc queue status execution time",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-show-dvc-queue-status-execution-time\/1531",
        "Question_created_time":1676716920982,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":142,
        "Question_body":"<p>Hello,<\/p>\n<p>How long should dvc queue status and dvc exp show take to run? Dvc queue status is going on ten minutes now for 20 experiments or so. Earlier it took 5 minutes for dvc exp show. These seem like they should be quick commands like git status which I spam constantly.<\/p>\n<p>Any tips for how to speed this up?<\/p>\n<p>Thanks,<br>\nGreg<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC Status in vscode",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-status-in-vscode\/1511",
        "Question_created_time":1676010980563,
        "Question_answer_count":8,
        "Question_score_count":0,
        "Question_view_count":113,
        "Question_body":"<p>I\u2019m using vscode with DVC. I\u2019m confused by what\u2019s being displayed in the SOURCE CONTROL sidebar<\/p>\n<p>There\u2019s the standard git section which I understand, and below that, there\u2019s a section for DVC<\/p>\n<p>It\u2019s showing all my model artifacts in a section saying Committed with D (deleted) beside them after I\u2019ve run and experiment and done a dvc push. The same files are also showing as deleted when I use the explorer sidebar (except they\u2019re there as symlinks to the cache)<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/6b6250df837965815ecd3785e216f17801e166a9.png\" alt=\"image\" data-base62-sha1=\"fjXJgUhnFGtJVWSdWqlGFuW8nuF\" width=\"369\" height=\"156\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Permission denied error when overwritting files to DVC tracked directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/permission-denied-error-when-overwritting-files-to-dvc-tracked-directory\/1518",
        "Question_created_time":1676391679082,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":114,
        "Question_body":"<p>Hi there,<br>\nI\u2019m still figuring DVC out so sorry if this is an obvious question. I feel like I\u2019m missing something.<\/p>\n<p>I have a project where some directories are tracked with DVC.<br>\nWhen someone reruns a script that overwrites existing files inside a DVC tracked directory we get \u201cPermission denied error\u201d. If we try to manually modify the same file with the vim editor we get that the file is flagged as readonly but we can force writting the file and it gets modified. But when we check file permissions on the file they all seem to be set properly. If we delete all files and rerun the script regenerating the files everything works fine again.<\/p>\n<p>Is this how DVC is supposed to work? Shouldn\u2019t we be able to overwrite files and then add the changes to the DVC tracked directory and then push them?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC on HPC with CML and large(r) number of experiments",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-on-hpc-with-cml-and-large-r-number-of-experiments\/1319",
        "Question_created_time":1662118461346,
        "Question_answer_count":9,
        "Question_score_count":0,
        "Question_view_count":273,
        "Question_body":"<p>Hi, we are currently testing and configuring a new HPC setup and I will be working on setting up a best practice guide and template projects for running ML\/Data science project using DVC, CML and possibly other tools.<\/p>\n<p>Based on the examples I read, I am wondering if the CML workflow also supports large numbers of parameter grid searches?<\/p>\n<p>Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to permanently stop tracking a file\/folder",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-permanently-stop-tracking-a-file-folder\/1509",
        "Question_created_time":1675977687625,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":117,
        "Question_body":"<p>I\u2019m having issues with dvc after I added a folder to dvc then attempted to remove it. Specifically, I was originally tracking notebooks using get and I decided to see if the experience was better using dvc - in the end I decided to revert back to using git.<\/p>\n<p>I started with <code>dvc add \/src\/notebooks<\/code> and I added the entire folder to dvc. This appears to copy all the files into the cache (renaming each as a hash) and creating a symlink.<\/p>\n<p>I couldn\u2019t find any way of deleting (dvc remove appears to remove stages not files\/folders) so to revert the add I removed all the references I could find to \/src\/notebooks and added back to git<\/p>\n<p>Now every time I checkout files from git, they get replaced with the original symlink to the old file that\u2019s still in the dvc cache<\/p>\n<p>In the end I had to track down each and every symlink and manually remove it from the cache.<\/p>\n<p>What\u2019s the proper way to remove a file\/folder from dvc - having it automatically relink files that happen to be the same names as files that used to be tracked causes all sorts of issues? Even adding src\/notebooks to .dvcignore didn\u2019t work<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to version files independently?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-version-files-independently\/1498",
        "Question_created_time":1675461764980,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"<p>Hello DVC community!<\/p>\n<p>I have a tricky use case and I\u2019m not sure this is feasible with DVC.<br>\nMy project features several datasets that I would like to version independently. I would like to keep track of:<\/p>\n<ul>\n<li>dataset 1 with its own set of tags<\/li>\n<li>datset 2 with its own set of tags<\/li>\n<li>dataset 3 with its own set of tags<br>\nso that I can execute a script with any combination of the versions of the three datasets (ex: dataset 1 v1, dataset 2 v2, dataset 3 v3 or dataset 1 v3, dataset 2 v2, dataset 3 v3, etc.).<\/li>\n<\/ul>\n<p>Is this use case feasible with DVC?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Failed to reproduce 'train': output 'model.pt' does not exist",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/failed-to-reproduce-train-output-model-pt-does-not-exist\/1515",
        "Question_created_time":1676314134533,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":66,
        "Question_body":"<p>Im using Hydra and Data Version Control for the first time, and I\u2019ve got some problems. I\u2019ve written dvc.yaml file but I get <strong>failed to reproduce \u2018train\u2019: output \u2018model.pt\u2019 does not exist<\/strong> as the result of <strong>dvc repro<\/strong><\/p>\n<p>My <strong>dvc.yaml<\/strong> looks like this.<\/p>\n<pre><code class=\"lang-auto\">stages:\n  prepare_data:\n    cmd: python prepare_data.py\n    deps:\n    - prepare_data.py\n    outs:\n    - cifar10\n  train:\n    cmd: python train.py\n    deps:\n    - cifar10\n    - modeling\n    - train.py\n    outs:\n    - model.pt\n  generate_sample:\n    cmd: python generate_sample.py\n    deps:\n    - modeling\n    - generate_sample.py\n    - model.pt\n    outs:\n    - samples\n<\/code><\/pre>\n<p>train.py fits model and then saves it to model.pt:<\/p>\n<pre><code class=\"lang-auto\">@hydra.main(config_path='conf', config_name='config')    \ndef main(cfg: DictConfig):\n...\ntorch.save(ddpm.state_dict(), hydra.utils.get_original_cwd() + \"\/model.pt\")\n<\/code><\/pre>\n<p>What am I missing? <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to fix database disk image is malform",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-fix-database-disk-image-is-malform\/691",
        "Question_created_time":1614622947594,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":343,
        "Question_body":"<p>Hi,<br>\nHow to fix in this errot:<br>\nERROR: unexpected error - database disk image is malformed<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Problem with dvc list",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-dvc-list\/1089",
        "Question_created_time":1646317899344,
        "Question_answer_count":30,
        "Question_score_count":0,
        "Question_view_count":1509,
        "Question_body":"<p>Hi. New to dvc and trying to get things to work with a little sample project.<\/p>\n<p>I was able to initialize git and dvc and use an S3 bucket as my data store. I committed and pushed my code to Bitbucket (my company\u2019s internal Bitbucket instance) and I see everything there but the data (as expected). I am able to clone the git repo and pull the data from S3 as well.<\/p>\n<p>I am trying to run dvc list to see the data tracked by dvc.<\/p>\n<p>I\u2019ve tried running the following commands unsuccessfully:<br>\ndvc list <a href=\"https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd\/dvc\" rel=\"noopener nofollow ugc\">https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd\/dvc<\/a><br>\ndvc list <a href=\"https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd\" rel=\"noopener nofollow ugc\">https:\/\/sourcecode.mycompany.com\/scm\/asx-jbbd<\/a> dvc<\/p>\n<p>ERROR: failed to list \u2018ssh:\/\/cesc@sourcecode.mycompany.com:3268\/asx-jbbd\u2019 - Failed to clone repo \u2018ssh:\/\/cesc@sourcecode.mycompany.com:3268\/asx-jbbd\u2019 to \u2018C:\\Users\\cesc\\AppData\\Local\\Temp\\1\\tmp43z389r6dvc-clone\u2019<\/p>\n<p>What am I doing wrong?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Large dataset, dvc pull\/add\/push jobs options",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/large-dataset-dvc-pull-add-push-jobs-options\/1496",
        "Question_created_time":1675391803569,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":138,
        "Question_body":"<p>Hello , DVC users<\/p>\n<p>I am a newbie who wants to apply dvc to large dataset management.<br>\nTo understand dvc, I did the following experiment.<\/p>\n<pre><code class=\"lang-auto\">## normal transfer case \n\n# In DVC_Main directory \ngit init\ndvc init\ndvc remote add -d &lt;storage_name&gt;  &lt;local_data_storage_url&gt;\n\ndvc add mesh_dataset   # mesh_dataset (40GB)\ndvc push \n\n# In DVC_Sub directory \ngit init\ndvc init\n\n#copy &amp; pasted files from ( dvc_main directory)  -&gt; .dvc\/config , .mesh_dataset.dvc files \ndvc pull \n\n<\/code><\/pre>\n<p>The above process is a code that tests whether the data worked in the main directory can be loaded from the sub directory.<br>\nBut it took too long (about 80 minutes)<\/p>\n<p>So I looked for options and there was a -j option for dvc add\/pull\/push.<\/p>\n<pre><code class=\"lang-auto\">## parallel transfer case \n\n# In DVC_Main directory \ngit init\ndvc init\ndvc remote add -d &lt;storage_name&gt;  &lt;local_data_storage_url&gt;\ndvc remote modify jobs 64\n\ndvc add -j 64 --to-remote mesh_dataset   # mesh_dataset (40GB)\n\n# In DVC_Sub directory \ngit init\ndvc init\n\n#copy &amp; pasted files from ( dvc_main directory)  -&gt; .dvc\/config , .mesh_dataset.dvc files \ndvc pull -j 64\n\n<\/code><\/pre>\n<p>The parallel transmission method should be faster than the normal transmission method, but I don\u2019t understand why there is no speed difference.<\/p>\n<p>In fact, the parallel transmission method is slightly faster, but it was a difference that appeared because the \u2018dvc push\u2019 process was omitted in the dvc_main directory.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/29914ea01d5fc60bbbfb026c5970afc795dc9522.png\" alt=\"image\" data-base62-sha1=\"5VITvQ8yavHQ6oc7bTYs1pe4u3g\" width=\"662\" height=\"114\"><\/p>\n<p>According to the explanation above, I have 1 cpu (8 cores, 14 logical cores), so the default value = 34.<\/p>\n<p>Can you explain why there is no difference in transfer speed between jobs option 32 and 64?<\/p>\n<p>Or does dvc not support parallel transmission of large data?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"CML runner + docker on Github",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-runner-docker-on-github\/1108",
        "Question_created_time":1646812178953,
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":284,
        "Question_body":"<p>Hi. I\u2019m using cml runner to deploy a container on EC2 and I need to use docker commands inside (build an image based on dvc pipeline results). I have seen there is a \u201cdocker-volumes\u201d option which can enable using docker but it\u2019s only available on GitLab, is there any solution or workaround for GitHub?<br>\nThanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Redundant Data across version and within versions",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/redundant-data-across-version-and-within-versions\/60",
        "Question_created_time":1533613591080,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":1001,
        "Question_body":"<p>I have data projects where very large files change by only a few characters each new version. What disk usage expectations should users have?<\/p>\n<p>Also, there are often cases where we have many files in a directory that look very similar. Are there optimizations by which chunks of data within each file are hashed and de-duplicated?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Create and run multiple experiments from python",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/create-and-run-multiple-experiments-from-python\/1490",
        "Question_created_time":1675278304799,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":79,
        "Question_body":"<p>Hi,<br>\nI see that the python API is read-only.<br>\nhow can I have python iterate over parameters and run an experiment for each,<br>\ni.e. doing gridsearch or something similar?<\/p>\n<p>os.system(\u201cdvc exp run -n exp_name\u201d) doesn\u2019t seem very \u2018integrated\u2019 and it has generated issues with git index lock and dvc locks.<br>\nthanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Programatic access to experiments",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/programatic-access-to-experiments\/1464",
        "Question_created_time":1674002301727,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":98,
        "Question_body":"<p>Hello,<\/p>\n<p>What is the best way to programatically access experiments? I want to write a script which goes through all of the experiments on a particular ref e.g. HEAD, and collects some data \/ information from the files there. The files could be git or dvc tracked. I have lots of output from my pipeline, so it is more than I would want to put in a metrics json. It seems like DVCFileSystem (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/dvcfilesystem\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">DVCFileSystem<\/a>) might do this? Would I just need to get a list of all the experiment revs, then loop over them and create a DVCFileSystem for each one?<\/p>\n<p>Thanks,<br>\nGreg<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"JAVA API for DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/java-api-for-dvc\/1486",
        "Question_created_time":1675200951194,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"<p>Hi,<\/p>\n<p>Is there a Java API similar to Python to access DVC?<\/p>\n<p>Regards<br>\nManju<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc with git sparse-checkout",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-git-sparse-checkout\/1480",
        "Question_created_time":1675094654696,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":113,
        "Question_body":"<p>Hello<br>\nI have a large number of datasets, each in its own subdirectory. So far, I was always pulling all datasets with <code>git clone ... &amp; dvc pull<\/code>, which works perfectly.<\/p>\n<p>Now, I would like to load only a subset of my datasets. I tried to archive this by doing a sparse checkout of the git repository. However, if I run <code>git sparse-checkout init &amp; git sparse-checkout add .dvc dataset_name<\/code> followed by <code>dvc pull<\/code>, I get<\/p>\n<blockquote>\n<p>ERROR: unexpected error - (b\u2019worktreeConfig\u2019, b\u2019true\u2019)<\/p>\n<\/blockquote>\n<p>Is there a way to fix this, or maybe a better way to solve the problem in the first place?<br>\nI am aware that I could simply run <code>dvc pull --recursive dataset_name<\/code> - but given the large number of directories I am dealing with, I\u2019d prefer to clone only those that I actually need.<\/p>\n<p>Any hint is highly appreciated.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Remote s3 cache storage with minio",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/remote-s3-cache-storage-with-minio\/1472",
        "Question_created_time":1674558901821,
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":483,
        "Question_body":"<p>Hello,<br>\nFirst of all thank you for your contribution.<br>\nI would like to ask if there is a way to keep all my cache of a dataset into a remote minio bucket and not appearing into my local storage.<\/p>\n<p>I have added my dataset into a remote minio bucket<\/p>\n<pre><code class=\"lang-auto\">dvc remote add myminio -d s3:\/\/abucket\/DVC\ndvc remote modify myminio endpointurl 'http:\/\/.....\/'\ndvc remote modify myminio access_key_id 'user'\ndvc remote modify myminio secret_access_key 'pass'\n\ndvc add data\ngit add .gitignore data.dvc .dvc\/config\ngit commit -m '-Added remote storage,-Added data'\ndvc push\nrm -r .dvc\/cache\n<\/code><\/pre>\n<p>But when I try to do a dvc pull, the cache directory always return.<\/p>\n<p>Is there a way to define that my cache folder is my remote?<br>\nAlso can I use dvc pull on another PC without data.dvc?<\/p>\n<p>Thank you in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Plots multiple data sources in yaml doesn't work",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/plots-multiple-data-sources-in-yaml-doesnt-work\/1470",
        "Question_created_time":1674555585760,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":56,
        "Question_body":"<p>Hello I\u2019m following a tutorial in the docs and while trying to make plots work I\u2019ve encountered this problem while writing my dvc yaml file.<br>\nIm pretty sure the syntax is correct and the dvc version is \u201cDVC version: 2.42.0 (pip)\u201d<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/877ac1badc4f4c7e57686f072182404fdbdd1037.png\" alt=\"image\" data-base62-sha1=\"jkvsKgVyoHOCmtKTpCR7TAUV1Eb\" width=\"588\" height=\"475\"><\/p>\n<p>I also tried this writing:<br>\nplots:<\/p>\n<pre><code class=\"lang-auto\">-ROC:\n        # Configure template and axes.\n        x: fpr\n        y:\n          eval\/live\/plots\/sklearn\/roc\/train.json: tpr\n          eval\/live\/plots\/sklearn\/roc\/test.json: tpr\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to mount a remote DVC data registry in a local filesystem?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-mount-a-remote-dvc-data-registry-in-a-local-filesystem\/1466",
        "Question_created_time":1674142569107,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":79,
        "Question_body":"<p>Hi! This is my first post here. I\u2019ve been looking through the docs and topics to find out a best solution.<br>\nI think I almost got everything clear in terms of setup. We are currently using a DVC data registry (i.e. a DVC+GIT in charge of tracking changes in a remote storage). We can import this data registry into other DVC projects that just read data from the remote storage.<\/p>\n<p>To reduce data storage (i.e. not creating another cache for each project importing the data registry that includes tracking changes in the data registry \u2014 these changes are already taken care of by the data registry itself) we wanted to use the data registry just as a filesystem where we specify the version (so that still the dependency of the code and the version of the registry is kept \u2014 though with a somewhat manual supervision instead of automatically).<\/p>\n<p>We find the DVCFileSystem API great for this as this exactly what we need. However it only provides support for python. Furthermore, some python libraries only work with local paths. Is it possible to mount such a DVC filesystem at the OS level. Something with the same functionality as the python API but working in the same way than <code>sshfs<\/code> for example (with the addition of the revision that we want to mount)?<\/p>\n<p>Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Basic DVC workflow",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/basic-dvc-workflow\/1467",
        "Question_created_time":1674171670865,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":80,
        "Question_body":"<p>I\u2019m a new DVC user and I have a question about the basic model management workflow.<\/p>\n<p>I\u2019ve configured a repo for DVC, I ran a test experiment by creating a feature branch. I ran a sweep and selected the <code>best<\/code> model, i.e.<\/p>\n<pre><code class=\"lang-auto\">dvc exp run -S 'train.batch_size=16,32,64,128' --queue\ndvc queue start\ndvc exp apply ex1\ngit add .\ngit commit -m 'My Experiment'\n<\/code><\/pre>\n<p>I have a question about how to merge this back to main correctly, the process I\u2019ve been following is:<\/p>\n<ul>\n<li>switch to <code>main<\/code>\n<\/li>\n<li>merge the feature branch to <code>main<\/code>\n<\/li>\n<li><code>dvc pull<\/code><\/li>\n<\/ul>\n<p>The last step seems to be important - if I don\u2019t do the the vscode source control sidebar shows uncommitted DVC tracked model files. Is this the correct workflow (I\u2019m about to learn GTO but I wanted to ensure that I can correctly manage the state of my <code>main<\/code> branch first).<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Gdrive-user-credentials.json is missing",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/gdrive-user-credentials-json-is-missing\/1453",
        "Question_created_time":1672953158705,
        "Question_answer_count":4,
        "Question_score_count":4,
        "Question_view_count":183,
        "Question_body":"<p>Hello<br>\nI\u2019m learning CI with DVC and GitHub actions and I have a problem: the file drive-user-credentials.json cant be found in my .dvc directory even after dvc push\/pull as on tutorials. I read all material on this particular matter available on DVC page but could not find an answer. Can you help me? Without this file, GitHub actions do not have the authorization to run my pipeline.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Adding files with names containing :",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adding-files-with-names-containing\/1451",
        "Question_created_time":1672920239313,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"<p>Hi,<\/p>\n<p>Maybe this is a well known thing, but it\u2019s tricky to search for. Most of the files that I want to use with DVC have the : character in their names, as the name includes an ISO timestamp. When I try adding them with dvc add I get an error of the form <code>Unsupported URL type &lt;StartOfFilename&gt;:\/\/ <\/code>, is there a way of escaping the : so that DVC recognises it a file rather than a URL?<\/p>\n<p>Thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Downloading data from azure storage container using dvc",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/downloading-data-from-azure-storage-container-using-dvc\/1426",
        "Question_created_time":1671625017118,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":125,
        "Question_body":"<p>I have an azure blob container with data which I have not uploaded myself. The data is not locally on my computer.<br>\nIs it possible to use dvc to download the data to my computer when I haven\u2019t uploaded the data with dvc? Is it possible with dvc import-url?<br>\nI have tried using dvc pull, but can only get it to work if I already have the data locally on the computer and have used dvc add  and dvc push .<br>\nAnd if I do it that way, then the folders on azure are not human-readable. Is it possible to upload them in a human-readable format?<br>\nIf it is not possible is there then another way to download data automatically from azure?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cannot apply the first exp after new commit",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cannot-apply-the-first-exp-after-new-commit\/1444",
        "Question_created_time":1672649058557,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":116,
        "Question_body":"<p>Hello,<\/p>\n<p>Iv been trying to use dvc to manage my experiments and like it very much so for but I encountered rather unexpected behavior.<\/p>\n<p>Anytime I commit the code, the very first experiment which I performs afterwards cannot be applied to workspace. The command \u201cdvc exp run\u201d will proceed without errors and environment is modified accordingly. But when I change the environment and then try to \u201cdvc exp apply\u201d it back, no changes to the environment are made (again, no errors or warnings). Applying any other experiments works as expected.<\/p>\n<p>The minimal example is the following:<\/p>\n<p><strong>dvc.yaml:<\/strong><br>\nstages:<br>\nmain:<br>\ncmd: Rscript src\/main.R<br>\nparams:<br>\n- par.yaml:<br>\nmetrics:<br>\n- out.yaml:<br>\ncache: false<\/p>\n<p><strong>par.yaml:<\/strong><br>\na: 1<br>\nb: 1<br>\nc: 1<br>\nd: 1<\/p>\n<p><strong>main.R<\/strong><br>\nlibrary(yaml)<br>\npar \u2190 yaml.load_file(\u201cpar.yaml\u201d)<br>\nout \u2190 list(<br>\nperf = par$a * 1 + par$b * 0.1 + par$c * 0.01 + par$d * 0.001<br>\n)<br>\nprint(out)<br>\nwrite_yaml(out, \u201cout.yaml\u201d)<\/p>\n<p>However, I believe that the issue may not be with the dvc.yaml but rather in my workflow as the same happens also when creating the environment with \u201cdvc exp init\u201d and when cloning the official example <a href=\"https:\/\/github.com\/iterative\/example-dvc-experiments\" rel=\"noopener nofollow ugc\">repo<\/a>. Em I supposed to call some other command after new commit to prime the environment for new experiments? I had not found anything in the reference manual.<\/p>\n<p>Right now I\u2019m sidestepping the issue by running the first experiment twice and disregarding the first one but it is cumbersome for computationally costly experiments.<\/p>\n<p>Thank you very much for your help.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practices for data stored on cloud?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-for-data-stored-on-cloud\/1448",
        "Question_created_time":1672841410736,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":105,
        "Question_body":"<p>Hi!<br>\nI wanted to get some advice on what are the best practices for using DVC for the following use case:<\/p>\n<ul>\n<li>Data will be stored on S3 buckets and updated regularly. The previous data is not changed, only updated with new entries.<\/li>\n<li>Every time the data is updated, we will run some analysis and train a model on this data. This should be done on the cloud as well.<\/li>\n<li>Data size will be in the order of hundred GBs. Will this cause problems regarding caching and data duplication? Should reflinks be used in this case?<\/li>\n<li>We are intending to use DVC to track the data versions used for each experiment (analysis and models) and have the option to rollback to previous versions.<\/li>\n<li>Nothing will be executed locally.<\/li>\n<\/ul>\n<p>What would be the recommended DVC flow for this pipeline?<br>\nPlease let me know if you\u2019d like more clarifications.<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Possible to push to Azure fileshare?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/possible-to-push-to-azure-fileshare\/1442",
        "Question_created_time":1672348219458,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":96,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m attempting to authenticate and push to Azure using dvc.<\/p>\n<p>I can successfully push to an Azure blob, but when I attempt to connect to an Azure Filshare endpoint my <code>dvc push<\/code> call returns that the container can\u2019t be found.<\/p>\n<p>Is there anyway to configure dvc to push to a Azure fileshare instead of a blob container?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Use different SSH credentials",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/use-different-ssh-credentials\/1438",
        "Question_created_time":1672092262659,
        "Question_answer_count":4,
        "Question_score_count":4,
        "Question_view_count":160,
        "Question_body":"<p>I\u2019ve a server that handles multiple id_rsa keys for different repositories. Is there any way to set something like GIT_SSH_COMMAND before calling dvc command so I can authenticate with the correct private key? Currently this doesn\u2019t seem to be supported by DVC.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How can I use in jenkins",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-can-i-use-in-jenkins\/1436",
        "Question_created_time":1672043494603,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"<p>Hello.<br>\nI tried to using in jenkins for build.<br>\nI want to first dvc pull and docker build in jenkins pipeline.<br>\nbut error<\/p>\n<pre><code class=\"lang-auto\">ModuleNotFoundError: No module named 'encodings'\n<\/code><\/pre>\n<p>I have docker volumes<\/p>\n<pre><code class=\"lang-auto\"> volumes:\n   - \/usr\/bin\/dvc:\/usr\/bin\/dvc\n   - \/usr\/lib\/dvc\/libpython3.10.so.1.0:\/usr\/bin\/libpython3.10.so.1.0\n   - \/usr\/lib\/python3.8\/dist-packages:\/usr\/lib\/python3.8\/dist-packages\n<\/code><\/pre>\n<p>How do you usually use dvc in CI?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to deal with multiple metric files that may define homonymous metrics?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-deal-with-multiple-metric-files-that-may-define-homonymous-metrics\/1428",
        "Question_created_time":1671639328676,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":98,
        "Question_body":"<p>I have a pipeline with multiple independent evaluation stages. Each one generates its own <code>.yaml<\/code> file with the computed metrics inside.<\/p>\n<p>I noticed that if any of these files defines a metric that shares the same name as another metric in another metrics file, dvc will name the metric <code>&lt;filename&gt;:&lt;metric_name&gt;<\/code> to avoid ambiguities. Although better than being ambiguous, this is not optimal for me, as some of the metrics will be named simply <code>&lt;metric_name&gt;<\/code> (according to its name in the corresponding .yaml file, if no other metric file defines a metric with the same name) or instead <code>&lt;filename&gt;:&lt;metric_name&gt;<\/code> (if there were conflicts). This makes it hard to regex for a specific metric when running e.g. <code>dvc exp show<\/code>, as you have to know in advance if there were conflicts or not to know the name of the metric.<\/p>\n<p>Is there a way to tell dvc to name all my metrics <code>&lt;filename&gt;:&lt;metric_name&gt;<\/code>? If not, what\u2019s the best practice in this scenario, never having metrics with the same name being defined in multiple files (e.g. I manually append the filename to the metric name inside the yaml files)?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Got error while pushing dvc file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/got-error-while-pushing-dvc-file\/1420",
        "Question_created_time":1671342258326,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":195,
        "Question_body":"<p>I got an error when I triled to use <strong>dvc push<\/strong>. The error message is like that<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - gdrive is supported, but requires 'dvc-gdrive' to be installed: No module named 'dvc_gdrive'\n<\/code><\/pre>\n<p>Version<\/p>\n<p>OS - Ubuntu 22.04<br>\nPython - 3.10.6<br>\ndvc - 2.38.1<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does dvc have a model sharing feature?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-have-a-model-sharing-feature\/1418",
        "Question_created_time":1671113441042,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":71,
        "Question_body":"<p>Data versioning of models is possible using DVC, but is there a way to centralize all models so that users can share and resuse models?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error: Unexpected Error - Database is Locked",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-database-is-locked\/960",
        "Question_created_time":1636415219705,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":311,
        "Question_body":"<p>Hi all! I\u2019m trying to install dvc in a new environments with no luck. I\u2019m probably messing something up on my end but I can\u2019t find it.<\/p>\n<p>install method: pip install dvc[all]<br>\nOS: Ubuntu 20.04<br>\nPython: Python3.9<br>\nPip: 21.3.1<\/p>\n<p>The install appears successful, however, any dvc command hangs, times out, and then returns the following:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - database is locked\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/command\/base.py\", line 59, in do_run\n    return self.run()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/command\/init.py\", line 44, in run\n    with Repo.init(\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 352, in init\n    return init(\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/init.py\", line 77, in init\n    proj = Repo(root_dir)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 214, in __init__\n    self.state = State(self.root_dir, state_db_dir, self.dvcignore)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/state.py\", line 64, in __init__\n    self.links = Cache(directory=os.path.join(tmp_dir, \"links\"), **config)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 481, in __init__\n    self.reset(key, value, update=False)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 2452, in reset\n    sql('PRAGMA %s = %s' % (pragma, value)).fetchall()\nsqlite3.OperationalError: database is locked\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dvc\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/main.py\", line 88, in main\n    dvc_info = get_dvc_info()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/info.py\", line 38, in get_dvc_info\n    with Repo() as repo:\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 214, in __init__\n    self.state = State(self.root_dir, state_db_dir, self.dvcignore)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/state.py\", line 64, in __init__\n    self.links = Cache(directory=os.path.join(tmp_dir, \"links\"), **config)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 481, in __init__\n    self.reset(key, value, update=False)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 2452, in reset\n    sql('PRAGMA %s = %s' % (pragma, value)).fetchall()\nsqlite3.OperationalError: database is locked\n<\/code><\/pre>\n<p>Could this be a permission issue with accessing one of the dvc db files?<\/p>\n<p>Any help is much appreciated as I am new to dvc.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"I have added my google drive as a remote storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/i-have-added-my-google-drive-as-a-remote-storage\/1061",
        "Question_created_time":1644813615375,
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":1774,
        "Question_body":"<p>\u2026but when I do a dvc push, i get the below error. Any idea how to resolve this. i am following the getting started sessions to get myself familiar with the product<\/p>\n<p>ERROR: unexpected error - : &lt;HttpError 404 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json<\/a> returned \u201cFile not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u201d. Details: \u201c[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018notFound\u2019, \u2018message\u2019: \u2018File not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018file\u2019}]\u201d&gt;<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC list shows libssl-so.1.1.1k not found",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-list-shows-libssl-so-1-1-1k-not-found\/1387",
        "Question_created_time":1668460404483,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":230,
        "Question_body":"<p>Hi there!<br>\nI have been trying to use the DVC list and DVC get to get files from a GitHub URL, but, it shows \u201cERROR: unexpected error - libssl-9ad06800.so.1.1.1k: cannot open shared object file: No such file or directory\u201d<\/p>\n<p>I have tried \u201clocate libssl.so.1.1.1k\u201d, it returns file present.<\/p>\n<h2>\n<a name=\"dvc-version-2332-pip-1\" class=\"anchor\" href=\"#dvc-version-2332-pip-1\"><\/a>DVC version: 2.33.2 (pip)<\/h2>\n<p>Platform: Python 3.9.13 on Linux-4.18.0-348.2.1.el8_5.x86_64-x86_64-with-glibc2.28<br>\nSubprojects:<br>\ndvc_data = 0.25.2<br>\ndvc_objects = 0.12.2<br>\ndvc_render = 0.0.12<br>\ndvc_task = 0.1.4<br>\ndvclive = 1.0<br>\nscmrepo = 0.1.3<br>\nSupports:<br>\nazure (adlfs = 2022.10.0, knack = 0.10.0, azure-identity = 1.12.0),<br>\ngdrive (pydrive2 = 1.14.0),<br>\ngs (gcsfs = 2022.10.0),<br>\nhdfs (fsspec = 2022.10.0, pyarrow = 10.0.0),<br>\nhttp (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\nhttps (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),<br>\noss (ossfs = 2021.8.0),<br>\ns3 (s3fs = 2022.10.0, boto3 = 1.24.59),<br>\nssh (sshfs = 2022.6.0),<br>\nwebdav (webdav4 = 0.9.8),<br>\nwebdavs (webdav4 = 0.9.8),<br>\nwebhdfs (fsspec = 2022.10.0)<br>\nCache types: hardlink, symlink<br>\nCache directory: xfs on \/dev\/mapper\/centos-home<br>\nCaches: local<br>\nRemotes: local<br>\nWorkspace directory: xfs on \/dev\/mapper\/centos-home<br>\nRepo: dvc, git<\/p>\n<p>Also, \u201clink type reflink is not available ([Errno 95] no more link types left to try out)\u201d<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"1T dataset with distributed deep learning",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/1t-dataset-with-distributed-deep-learning\/1374",
        "Question_created_time":1666885670842,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":196,
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m currently evaluating DVC for a very large scale Deep Learning project.<\/p>\n<p>We have an ever growing dataset of +1M video files totaling to more than a Tb.<br>\nFrom this we do:<\/p>\n<ol>\n<li>preprocessing of each video file<\/li>\n<li>sub sample a subset of the original dataset, which will become the new training\/val\/test dataset. This dataset is currently at +400k video files totaling to 800G<\/li>\n<li>Train a distributed deep learning video classification model on aws with pytorch distributed<\/li>\n<\/ol>\n<p>I have already used DVC extensively but only in a more local-first approach where I could run the whole pipeline on my machine and would just run the same pipeline on more powerful servers.<\/p>\n<p>In my new use-case, I would like to scale the whole process on aws where all the data is on S3 buckets and steps of the pipeline is parallelized on light aws lambdas as much as possible.<br>\nI would also like to be able to launch the whole pipeline from my local machine (which would run all the processing on aws) but also on a CI job.<\/p>\n<p>Has anyone ever had such a challenging setup ?<br>\nI feel like it\u2019s the distributed parallel execution features that would be the most challenging to put in place with DVC.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Trying to understand data storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trying-to-understand-data-storage\/1378",
        "Question_created_time":1667019094738,
        "Question_answer_count":7,
        "Question_score_count":5,
        "Question_view_count":174,
        "Question_body":"<p>Hi, I am a newcomer to DVC. I read through the tutorial for Data and Model Versioning, and I want to understand how data is stored.<\/p>\n<p>Specifically, say I pull from my remote storage 1000 images, and add 1000 images to create a new dataset of 2000 images. If I dvc add the new data folder and dvc push, how is it stored in remote? Is it like 1000 images (original folder), and then add 1000 images, or is it 1000 images, and then a folder with 2000 images? The reason for the confusion is because at <a href=\"https:\/\/dvc.org\/doc\/start\/data-management\/data-versioning\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Get Started: Data Versioning<\/a> (7:54), it shows that there are 2 folders in the remote google drive. I want to know this because I am a ML engineer for a computer vision company and we would have huge datasets, so we don\u2019t want any duplicates across versions.<\/p>\n<p>Furthermore, if I do a dvc checkout to a previous version (1000 images) of the dataset in my local, do 1000 images just disappear from my newer dataset (2000 images). I see this happen in the above video, where dvc checkout with a previous data\/data.xml.dvc causes the data to be 36M instead of 72M. How exactly does dvc do this switch so quickly, and does it work for a folder of images vs just a single xml files?<\/p>\n<p>Sorry if these are stupid questions, I am completely new to this.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Manually prompt Gdrive authentication step",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-prompt-gdrive-authentication-step\/858",
        "Question_created_time":1630114625044,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":383,
        "Question_body":"<p>Hi, I\u2019m using DVC with my team and we\u2019re using google drive as our storage solution. By following the tutorial, the google account authentication step appears when I attempt to do my first dvc push, which is ok. Now my teammates new to link their accounts too in order to access the gdrive folder (shared between all of us) but we don\u2019t know how to prompt the authenctication step wihout having them pushing something, all they will do in this repo is pull data.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Problem with DVC and Azure + tenant_id, client_id and client_secret",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-dvc-and-azure-tenant-id-client-id-and-client-secret\/1376",
        "Question_created_time":1666890271387,
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":195,
        "Question_body":"<p>I am on the DVC version: 2.31.0 (pip)<br>\nMy command are :<br>\npip install \u2018dvc[azure]\u2019<br>\ndvc init --no-scm<br>\ndvc remote add -d myremote azure:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<br>\ndvc remote modify --local myremote tenant_id \u201c12345678-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_id \u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_secret \u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<\/p>\n<p>dvc add an_image.jpg<br>\ndvc push<\/p>\n<p>All the tenant_id, client_id and client_secret are the same than used with azcopy :<br>\nI use azcopy with this variables without probleme :<br>\nexport AZCOPY_SPA_APPLICATION_ID=\u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\nexport AZCOPY_SPA_CLIENT_SECRET=\u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<br>\nexport AZCOPY_TENANT_ID=\u201c12345678-1234-1234-1234-123456789012\u201d<br>\nazcopy login --service-principal --application-id $AZCOPY_SPA_APPLICATION_ID --tenant-id $AZCOPY_TENANT_ID<br>\nazcopy copy \u2018.\/01_RAW_From_Data_Lake\/*\u2019 \u2018<a href=\"https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/\" class=\"inline-onebox-loading\" rel=\"noopener nofollow ugc\">https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<\/a>\u2019 --recursive<\/p>\n<p>Why the dvc push return this error message :<br>\nERROR: configuration error - Authentication to Azure Blob Storage requires either account_name or connection_string.<br>\nERROR: Authentication to Azure Blob Storage requires either account_name or connection_string.<br>\nLearn more about configuration settings at <a href=\"https:\/\/man.dvc.org\/remote\/modify\" rel=\"noopener nofollow ugc\">https:\/\/man.dvc.org\/remote\/modify<\/a>.<br>\n\/<br>\nThe tenant_id, client_id and client_secret are enough for login with azcopy. where is the mistake with DVC?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Authenticate DVC python api with GitHub app credentials",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/authenticate-dvc-python-api-with-github-app-credentials\/1364",
        "Question_created_time":1666090734944,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":213,
        "Question_body":"<p>Hi Folks,<\/p>\n<p>I am trying to use dvc python api which refers to private GitHub repo.<\/p>\n<pre><code class=\"lang-auto\">dvc.api.get_url(\n    path='data\/data.json',\n    repo='https:\/\/github.com\/owner\/private-repo.git'\n)\n<\/code><\/pre>\n<p>Without any credentials I get <code>dulwich.client.HTTPUnauthorized: No valid credentials provided<\/code> error.<\/p>\n<p>I have registered Github app and want to use credentials of same to do some operations using python api. Can someone help me with how do we provide credentials to dvc python api?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deletion of storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deletion-of-storage\/1370",
        "Question_created_time":1666479337651,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":88,
        "Question_body":"<p>Hello.<br>\nI have created a toy project with google drive storage. I created 3 versions of the dataset and then deleted the drive folder to create a new storage folder. Then create 3 more dataset versions. When I want to download the first version it throws me an error:<\/p>\n<blockquote>\n<p>Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 176fc4479b1573160fd05222c20976d2.dir<\/p>\n<\/blockquote>\n<p>Is there a way to recover or rebuild the versions before the storage change?<\/p>\n<p>Thanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc pull --glob",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-pull-glob\/917",
        "Question_created_time":1634219444762,
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":948,
        "Question_body":"<p>Hi, I want to share just some of my files on a remote. For that I used <code>dvc push  --glob data\/**\/*.txt data <\/code>, which seemed to work. But how can someone download and checkout only this subset? All other data files are stored just locally and were never pushed. The command <code>dvc pull --glob data\/**\/*.txt<\/code> throws a lot of errors, because dvc tries to download files from the remote that I never uploaded.<br>\nThanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC and Hydra integration",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-hydra-integration\/868",
        "Question_created_time":1630598266361,
        "Question_answer_count":5,
        "Question_score_count":3,
        "Question_view_count":1241,
        "Question_body":"<p>Hello!<\/p>\n<p>I want to integrate DVC pipelines and parameters\/metrics tracking with Hydra loggings and configs. What best practices could I check?<\/p>\n<p>Some questions:<\/p>\n<ul>\n<li>how to configure hydra outputs dir to normal save logs and artifacts from the different pipeline stages?<\/li>\n<li>how to connect dvc params.yaml with hydra conf?<\/li>\n<\/ul>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to avoid data duplication between cache and workspace",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-avoid-data-duplication-between-cache-and-workspace\/1340",
        "Question_created_time":1663141540288,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":281,
        "Question_body":"<p>I am using DVC on macOS. According to the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noopener nofollow ugc\">documentation<\/a>, DVC won\u2019t save the same file twice (one in the workspace and one in the cache).<\/p>\n<p><code>In order to have the files present in both directories without duplication, DVC can automatically create **file links** to the cached data in the workspace. In fact, by default it will attempt to use reflinks* if supported by the file system.<\/code><\/p>\n<p>However, for me, it keeps two copies of the same file, thereby increasing the disk space significantly. For example, if I have a file of around 5GB, the disk space for that project goes to around 10GB. Here is the output of the <code>dvc version<\/code> command. According to the docs, the reflinks are supported by macOS and can be seen from dvc version output then why isn\u2019t it working in my case? Any ideas on what I am missing here?<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.18.0 (brew)\n---------------------------------\nPlatform: Python 3.10.6 on macOS-12.5.1-x86_64-i386-64bit\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.14.0),\n        gs (gcsfs = 2022.7.1),\n        webhdfs (fsspec = 2022.7.1),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        s3 (s3fs = 2022.7.1, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk1s5s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk1s5s1\nRepo: dvc (subdir), git\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Problem with top-level plot definitions",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-top-level-plot-definitions\/1342",
        "Question_created_time":1663152137956,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":278,
        "Question_body":"<p>Hey!<\/p>\n<p>I am currently having problems with a top-level plot definition.<br>\nMy dvc.yaml file contains a stage producing<br>\n<code>output_data\/plot_metrics\/metrics_plot_dict.json<\/code> as output (also defined as outs in dvc.yaml)<\/p>\n<p>I am trying to define a plot like this in dvc.yaml<\/p>\n<pre><code class=\"lang-auto\">plots:\n  acceptance_rate_histogram:\n    template: plot_templates\/histogram_template.json\n    x: \n      output_data\/plot_metrics\/metrics_plot_dict.json: acceptance_rate\n    x_label: Acceptance_Rate\n    title: Histogram\n<\/code><\/pre>\n<p>But with <code>dvc plots show<\/code> I receive :<\/p>\n<pre><code class=\"lang-auto\">2022-09-14 12:37:13,109 WARNING: 'acceptance_rate_histogram' was not found in current workspace.\n2022-09-14 12:37:13,113 DEBUG: 'acceptance_rate_histogram' - file type error\nOnly JSON, YAML, CSV and TSV formats are supported.\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/tim\/miniconda3\/envs\/pymc_stable\/lib\/python3.10\/site-packages\/dvc\/utils\/__init__.py\", line 410, in wrapper\n    vals = func(*args, **kwargs)\n  File \"\/home\/tim\/miniconda3\/envs\/pymc_stable\/lib\/python3.10\/site-packages\/dvc\/repo\/plots\/__init__.py\", line 548, in parse\n    raise PlotMetricTypeError(path)\ndvc.repo.plots.PlotMetricTypeError: 'acceptance_rate_histogram' - file type error\nOnly JSON, YAML, CSV and TSV formats are supported.\n------------------------------------------------------------\nDVC failed to load some plots for following revisions: 'workspace'.\nfile:\/\/\/home\/tim\/Workspaces\/pystoms\/pipelines\/pipe_A\/dvc_plots\/index.html\n2022-09-14 12:37:13,120 DEBUG: Analytics is enabled.\n2022-09-14 12:37:13,190 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpaooxh7mo']'\n2022-09-14 12:37:13,191 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpaooxh7mo']'\n<\/code><\/pre>\n<p>It seems as if dvc tries to find a file named <code>acceptance_rate_histogram<\/code>, but this string is supposed to be an id. If I exchange the id for the file\u2019s path, it works.<\/p>\n<p>My dvc version: 2.25.0<br>\nMy python version: 3.10.6<br>\nMy OS: Ubuntu 22.04<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Probelm when running dvc exp run",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/probelm-when-running-dvc-exp-run\/1329",
        "Question_created_time":1662547455901,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"<p>Hello, upon running a dvc exp run i get the following error message, after all the stages are finished:<br>\nERROR: configuration error - config file error: expected \u2018url\u2019 for dictionary value \u2026<br>\nIn addition to that a very old version of the project is being restored eventhough commits to both git and dvc have been made. What could be the reason for that?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Facing issues in gitlab-runner for ci-cml and face issue to use AWS-s3 bucket for dvc in gitlab",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/facing-issues-in-gitlab-runner-for-ci-cml-and-face-issue-to-use-aws-s3-bucket-for-dvc-in-gitlab\/1332",
        "Question_created_time":1662585071904,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":138,
        "Question_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple dvc.yaml files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-dvc-yaml-files\/1321",
        "Question_created_time":1662306863720,
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":142,
        "Question_body":"<p>Hello, is it possible to track 2 or more projects in one git\/dvc repo? For example having 2 separate folders for 2 different neural net models. Like that:<br>\nDVC_repo\/PyTorch_model and DVC_repo\/TensorFlow_model where DVC_repo is the root directory. Is it possible to have 2 dvc.files in each folder and run them one at a time but without needing to <strong>dvc destroy<\/strong> when switching between them<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What DVC does when git merge is executed?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-dvc-does-when-git-merge-is-executed\/1316",
        "Question_created_time":1662026528437,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":139,
        "Question_body":"<p>I have two git branches. DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dynamic parameter import",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dynamic-parameter-import\/1312",
        "Question_created_time":1661757180755,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":185,
        "Question_body":"<p>Hi, I have the following scenario: I\u2019m working with a Computer Vision Deep Learning framework for which I basically can write a config file only and train. This config file is in python. I would like to input this config file as parameter dependency (under params in dvc.yaml) such that I have one place where I can change the parameters. If I don\u2019t I need to maintain two places because some things I have to set in the <code>config.py<\/code> file\u2026<\/p>\n<p>I know I can specify the <code>config.py <\/code>as a params file. However, I have different architectures with potentially different parameters I would like to change. I made a stage using templating that switches the <code>config.py<\/code> based on the architecture name. Is it possible to create a <code>deps.yaml<\/code> (or any file) that contains a list of the keys present in <code>config.py<\/code> that should be considered parameters and ignore all the other valid potential parameters in <code>config.py<\/code>?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Keeping a record of old and rejected experiments",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/keeping-a-record-of-old-and-rejected-experiments\/1318",
        "Question_created_time":1662031931395,
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":145,
        "Question_body":"<p>Hi, what is the workflow for saving old experiments that haven\u2019t \u201cfailed\u201d in the sense there was some error and no output was created, but were not the best performing (by whatever metric) and therefore not persisted?<\/p>\n<p>The documentation speaks of persisting experiments (by turning them into a branch) and removing the old ones by <code>dvc exp gc\/remove<\/code>, but I would like to keep a record of my past attempts of trying to improve the performance. The parameters and performance metrics at a minimum and preferably without bloating the cache and remote storage (I don\u2019t actually need the data as I have rejected them).<\/p>\n<p>Thank  you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Large Data Registry on NAS with multiple DVC and non-DVC users",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/large-data-registry-on-nas-with-multiple-dvc-and-non-dvc-users\/1294",
        "Question_created_time":1660121162723,
        "Question_answer_count":8,
        "Question_score_count":8,
        "Question_view_count":290,
        "Question_body":"<p>[I posted this question in the <span class=\"hashtag\">#dvc<\/span> Discord channel, and <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> suggested that other users can benefit from the discussion, so I\u2019m copying it here]<\/p>\n<p>Some time ago I asked in the forum about using DVC to manage our data on a NAS drive (<a href=\"https:\/\/discuss.dvc.org\/t\/single-cache-or-multiple-caches-in-nas-with-external-data\/1136\" class=\"inline-onebox\">Single cache or multiple caches in NAS with External Data<\/a>). I got useful advice from <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> , thanks, who suggested setting up a Data Registry. I did, but we are having trouble with users getting permission errors to add new datasets.<\/p>\n<p>The problem is that we have some constraints that go a bit against how DVC seems to be designed for. Namely, we have a folder <code>datasets<\/code> on the NAS with large datasets, both in size and number of files. Multiple users need to be able to save directories\/files to this folder, and then add\/commit\/push them with DVC. We cannot have data duplication either, and the directories\/files in folder <code>datasets<\/code> need to remain there, for non-DVC users to read them.<\/p>\n<p>So, our current solution is that the NAS is mounted on a server at <code>\/nas\/<\/code>, and looks like this:<\/p>\n<pre><code class=\"lang-auto\">nas\n  \u251c\u2500\u2500  dvc_cache\n  \u2514\u2500\u2500  datasets\n           \u251c\u2500\u2500 .dvc\n           \u251c\u2500\u2500 .git\n           \u251c\u2500\u2500 dataset_1\n           \u2514\u2500\u2500 dataset_2\n<\/code><\/pre>\n<p>with a configuration file (hardlinks\/symlinks to avoid data duplication)<\/p>\n<blockquote>\n<p>[cache]<br>\ndir = \/nas\/dvc_cache\/<br>\nshared = group<br>\ntype = \u201chardlink,symlink\u201d<br>\n[core]<br>\nautostage = true<\/p>\n<\/blockquote>\n<p>But an immediate problem is that if user A does <code>dvc add dataset_1<\/code>, and user B does <code>dvc add dataset_2<\/code>, then user A can <code>git commit -a<\/code> and <code>git push<\/code> and commit\/push both datasets.<\/p>\n<p>Another issue we are having is that it looks like if <code>user_A<\/code> adds a dataset, the directory created in the cache, e.g. <code>\/nas\/dvc_cache\/e0<\/code>, instead of having owner <code>user_A<\/code> and group <code>everybody<\/code>, it has both owner and group <code>user_A<\/code>, which blocks for anybody else any dvc operations that involve <code>\/nas\/dvc_cache\/e0<\/code>.<\/p>\n<p>My feeling is that each user should have their clone of the Data Registry on <code>\/nas<\/code>, but I\u2019m not sure this works with the Data Registry idea. Suggestions would be very welcome, thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SSH remote: unexpected error - Permission denied",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/ssh-remote-unexpected-error-permission-denied\/1300",
        "Question_created_time":1660556492786,
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":1010,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m trying to configure an SSH remote storage. When I try to push my data directory I get:<\/p>\n<pre><code class=\"lang-auto\">$  dvc push\nERROR: unexpected error - Permission denied: Permission denied                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n<\/code><\/pre>\n<p>I can confirm that:<\/p>\n<ul>\n<li>I can connect to the server via <code>ssh<\/code> and <code>sftp<\/code>\n<\/li>\n<li>the URL is correct<\/li>\n<li>I have write permissions in the path (already tried by manually uploading the files via <code>sftp<\/code>.<\/li>\n<\/ul>\n<p>I can\u2019t really make sense of the debug message:<\/p>\n<pre><code class=\"lang-auto\">$  dvc push -v\n2022-08-15 11:38:48,112 DEBUG: Lockfile for 'dvc.yaml' not found      \n2022-08-15 11:38:48,148 WARNING: Output 'data\/numpy'(stage: 'convert_puf_responses') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\n2022-08-15 11:38:48,273 DEBUG: Preparing to transfer data from '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/sram_startup\/.dvc\/cache' to '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,273 DEBUG: Preparing to collect status from '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,273 DEBUG: Collecting status from '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,274 DEBUG: Querying 1 oids via object_exists                                                                  \n2022-08-15 11:38:48,425 ERROR: unexpected error - Permission denied: Permission denied                                            \n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 91, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.10\/contextlib.py\", line 619, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/misc.py\", line 274, in __aenter__\n    self._coro_result = await self._coro\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 7707, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 408, in wait_for\n    return await fut\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 440, in _connect\n    await options.waiter\nasyncssh.misc.PermissionDenied: Permission denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 109, in push\n    return self.transfer(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 88, in transfer\n    return transfer(src_odb, dest_odb, objs, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/transfer.py\", line 158, in transfer\n    status = compare_status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 179, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 136, in status\n    exists = hashes.intersection(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 56, in _indexed_dir_hashes\n    dir_exists.update(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\n    for obj in iterable:\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/db.py\", line 255, in list_oids_exists\n    yield from itertools.compress(oids, in_remote)\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 609, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 269, in exists\n    return self.fs.exists(path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/ssh.py\", line 115, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 76, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 76, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 86, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 66, in sync\n    raise return_result\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 26, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 445, in wait_for\n    return fut.result()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 29, in wrapper\n    raise PermissionError(exc.reason) from exc\nPermissionError: Permission denied\n------------------------------------------------------------\n2022-08-15 11:38:48,557 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 91, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.10\/contextlib.py\", line 619, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/misc.py\", line 274, in __aenter__\n    self._coro_result = await self._coro\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 7707, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 408, in wait_for\n    return await fut\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 440, in _connect\n    await options.waiter\nasyncssh.misc.PermissionDenied: Permission denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 109, in push\n    return self.transfer(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 88, in transfer\n    return transfer(src_odb, dest_odb, objs, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/transfer.py\", line 158, in transfer\n    status = compare_status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 179, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 136, in status\n    exists = hashes.intersection(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 56, in _indexed_dir_hashes\n    dir_exists.update(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\n    for obj in iterable:\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/db.py\", line 255, in list_oids_exists\n    yield from itertools.compress(oids, in_remote)\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 609, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 269, in exists\n    return self.fs.exists(path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/ssh.py\", line 115, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 76, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 76, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 86, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 66, in sync\n    raise return_result\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 26, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 445, in wait_for\n    return fut.result()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 29, in wrapper\n    raise PermissionError(exc.reason) from exc\nPermissionError: Permission denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 68, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 288, in reflink\n    return self.fs.reflink(from_info, to_info)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/local.py\", line 157, in reflink\n    return system.reflink(path1, path2)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/system.py\", line 105, in reflink\n    _reflink_linux(source, link_name)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/system.py\", line 91, in _reflink_linux\n    fcntl.ioctl(d.fileno(), FICLONE, s.fileno())\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 127, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 76, in _try_links\n    raise OSError(\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/sram_startup\/.dvc\/cache\/.QXMYC5qmirjwhLDdpceujW.tmp'\n2022-08-15 11:38:48,569 DEBUG: Version info for developers:\nDVC version: 2.12.0 (pip)\n---------------------------------\nPlatform: Python 3.10.5 on Linux-5.10.135-1-MANJARO-x86_64-with-glibc2.36\nSupports:\n\twebhdfs (fsspec = 2022.5.0),\n\thttp (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),\n\thttps (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),\n\tssh (sshfs = 2022.6.0)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/mapper\/luks-9d8db46c-e044-463a-9dda-ad428bb54390\nCaches: local\nRemotes: ssh\nWorkspace directory: ext4 on \/dev\/mapper\/luks-9d8db46c-e044-463a-9dda-ad428bb54390\nRepo: dvc (subdir), git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-08-15 11:38:48,571 DEBUG: Analytics is enabled.\n2022-08-15 11:38:48,606 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8enuwu2w']'\n2022-08-15 11:38:48,607 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8enuwu2w']'\n<\/code><\/pre>\n<p>What am I missing? Any ideas?<br>\nThanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc exp show: experiment not showing \/ wrong position",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-show-experiment-not-showing-wrong-position\/1019",
        "Question_created_time":1641551077155,
        "Question_answer_count":14,
        "Question_score_count":0,
        "Question_view_count":681,
        "Question_body":"<p>Hi there,<br>\nI am trying out dvc for a few days now and have a question regarding dvc exp show. My setup is:<\/p>\n<ul>\n<li>Have a git &lt;branch 1&gt; with a head commit &lt;commit 1&gt;<\/li>\n<li>Run an experiment on a remote machine (checkout &lt;commit 1&gt;, dvc pull, dvc exp run)<\/li>\n<li>Create a new branch (dvc exp branch) which creates &lt;branch 2&gt; and &lt;commit 2&gt; in it<\/li>\n<li>I push the experiment (dvc exp push)<\/li>\n<li>On my local machine, I fetch the experiment (dvc exp pull)<br>\n(* In the mean time, I have more commits on &lt;branch 1&gt;)<\/li>\n<\/ul>\n<p>Then, dvc exp show behaves unexpectedly:<\/p>\n<ul>\n<li>If I run dvc exp show -a, it shows &lt;branch 1&gt; and &lt;branch 2&gt;, but no experiment<\/li>\n<li>If I run dvc exp show -A, it shows the experiment under &lt;commit 1&gt;<\/li>\n<\/ul>\n<p>Expected behavior:<\/p>\n<ul>\n<li>If I run dvc exp show -a, it shows the experiment under &lt;branch 2&gt;<\/li>\n<\/ul>\n<p>Hope I got this across. Am I doing something wrong here?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - 'NoneType' object has no attribute 'flush'",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-nonetype-object-has-no-attribute-flush\/1031",
        "Question_created_time":1643641582804,
        "Question_answer_count":10,
        "Question_score_count":0,
        "Question_view_count":2122,
        "Question_body":"<p>Hi,<\/p>\n<p>Trying to use DVC on windows, got the error below when trying to track a data file with DVC.<br>\nAny idea of the cause and how to resolve ?<br>\nThanks !<\/p>\n<h2>\n<a name=\"dvc-add-testficsv-vv-2022-01-31-160239340-trace-namespacecd-cmdadd-cprofilefalse-cprofile_dumpnone-descnone-externalfalse-filenone-funcclass-dvccommandaddcmdadd-globfalse-instrumentfalse-instrument_openfalse-jobsnone-no_commitfalse-outnone-pdbfalse-quiet0-recursivefalse-remotenone-targetstestficsv-to_remotefalse-verbose2-versionnone-yappifalse-2022-01-31-160239743-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160239762-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160239763-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160239777-error-unexpected-error-nonetype-object-has-no-attribute-flush-1\" class=\"anchor\" href=\"#dvc-add-testficsv-vv-2022-01-31-160239340-trace-namespacecd-cmdadd-cprofilefalse-cprofile_dumpnone-descnone-externalfalse-filenone-funcclass-dvccommandaddcmdadd-globfalse-instrumentfalse-instrument_openfalse-jobsnone-no_commitfalse-outnone-pdbfalse-quiet0-recursivefalse-remotenone-targetstestficsv-to_remotefalse-verbose2-versionnone-yappifalse-2022-01-31-160239743-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160239762-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160239763-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160239777-error-unexpected-error-nonetype-object-has-no-attribute-flush-1\"><\/a>$ dvc add testfi.csv -vv<br>\n2022-01-31 16:02:39,340 TRACE: Namespace(cd=\u2019.\u2019, cmd=\u2018add\u2019, cprofile=False, cprofile_dump=None, desc=None, external=False, file=None, func=&lt;class \u2018dvc.command.add.CmdAdd\u2019&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, no_commit=False, out=None, pdb=False, quiet=0, recursive=False, remote=None, targets=[\u2018testfi.csv\u2019], to_remote=False, verbose=2, version=None, yappi=False)<br>\n2022-01-31 16:02:39,743 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\config.local\u2019 to gitignore file.<br>\n2022-01-31 16:02:39,762 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\tmp\u2019 to gitignore file.<br>\n2022-01-31 16:02:39,763 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\cache\u2019 to gitignore file.<br>\n2022-01-31 16:02:39,777 ERROR: unexpected error - \u2018NoneType\u2019 object has no attribute \u2018flush\u2019<\/h2>\n<h2>\n<a name=\"traceback-most-recent-call-last-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcmainpy-line-55-in-main-ret-cmddo_run-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandbasepy-line-45-in-do_run-return-selfrun-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandaddpy-line-21-in-run-selfrepoadd-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcutilscollectionspy-line-163-in-inner-result-funcbaargs-bakwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-48-in-wrapper-with-lock_reporepo-file-cusersddumetappdatalocalprogramspythonpython38libcontextlibpy-line-113-in-__enter__-return-nextselfgen-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-36-in-lock_repo-with-repolock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-131-in-__enter__-selflock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-120-in-lock-lock_retry-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-45-in-wrapper-return-decocall-dargs-dkwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncyflowpy-line-127-in-retry-return-call-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-66-in-__call__-return-self_funcself_args-self_kwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-103-in-_do_lock-with-tqdm-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcprogresspy-line-85-in-__init__-super__init__-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-1107-in-__init__-selfsp-selfstatus_printerselffp-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-339-in-status_printer-sysstderrflush-attributeerror-nonetype-object-has-no-attribute-flush-2\" class=\"anchor\" href=\"#traceback-most-recent-call-last-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcmainpy-line-55-in-main-ret-cmddo_run-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandbasepy-line-45-in-do_run-return-selfrun-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvccommandaddpy-line-21-in-run-selfrepoadd-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcutilscollectionspy-line-163-in-inner-result-funcbaargs-bakwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-48-in-wrapper-with-lock_reporepo-file-cusersddumetappdatalocalprogramspythonpython38libcontextlibpy-line-113-in-__enter__-return-nextselfgen-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcrepo__init__py-line-36-in-lock_repo-with-repolock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-131-in-__enter__-selflock-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-120-in-lock-lock_retry-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-45-in-wrapper-return-decocall-dargs-dkwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncyflowpy-line-127-in-retry-return-call-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesfuncydecoratorspy-line-66-in-__call__-return-self_funcself_args-self_kwargs-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvclockpy-line-103-in-_do_lock-with-tqdm-file-cusersddumetpythonenvsvenv-senseenlibsite-packagesdvcprogresspy-line-85-in-__init__-super__init__-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-1107-in-__init__-selfsp-selfstatus_printerselffp-file-cusersddumetpythonenvsvenv-senseenlibsite-packagestqdmstdpy-line-339-in-status_printer-sysstderrflush-attributeerror-nonetype-object-has-no-attribute-flush-2\"><\/a>Traceback (most recent call last):<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\main.py\u201d, line 55, in main<br>\nret = cmd.do_run()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\command\\base.py\u201d, line 45, in do_run<br>\nreturn self.run()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\command\\add.py\u201d, line 21, in run<br>\nself.repo.add(<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\utils\\collections.py\u201d, line 163, in inner<br>\nresult = func(*ba.args, **ba.kwargs)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\repo_<em>init<\/em>_.py\u201d, line 48, in wrapper<br>\nwith lock_repo(repo):<br>\nFile \u201cC:\\Users\\ddumet\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py\u201d, line 113, in <strong>enter<\/strong><br>\nreturn next(self.gen)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\repo_<em>init<\/em>_.py\u201d, line 36, in lock_repo<br>\nwith repo.lock:<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\lock.py\u201d, line 131, in <strong>enter<\/strong><br>\nself.lock()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\lock.py\u201d, line 120, in lock<br>\nlock_retry()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\funcy\\decorators.py\u201d, line 45, in wrapper<br>\nreturn deco(call, *dargs, **dkwargs)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\funcy\\flow.py\u201d, line 127, in retry<br>\nreturn call()<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\funcy\\decorators.py\u201d, line 66, in <strong>call<\/strong><br>\nreturn self._func(*self._args, **self._kwargs)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\lock.py\u201d, line 103, in _do_lock<br>\nwith Tqdm(<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\dvc\\progress.py\u201d, line 85, in <strong>init<\/strong><br>\nsuper().<strong>init<\/strong>(<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\tqdm\\std.py\u201d, line 1107, in <strong>init<\/strong><br>\nself.sp = self.status_printer(self.fp)<br>\nFile \u201cc:\\users\\ddumet\\pythonenvs\\venv-senseen\\lib\\site-packages\\tqdm\\std.py\u201d, line 339, in status_printer<br>\nsys.stderr.flush()<br>\nAttributeError: \u2018NoneType\u2019 object has no attribute \u2018flush\u2019<\/h2>\n<h2>\n<a name=\"h-2022-01-31-160240219-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160240224-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160240225-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160240236-debug-version-info-for-developers-dvc-version-293-pip-3\" class=\"anchor\" href=\"#h-2022-01-31-160240219-debug-adding-csenseen-datasets-reposdvcconfiglocal-to-gitignore-file-2022-01-31-160240224-debug-adding-csenseen-datasets-reposdvctmp-to-gitignore-file-2022-01-31-160240225-debug-adding-csenseen-datasets-reposdvccache-to-gitignore-file-2022-01-31-160240236-debug-version-info-for-developers-dvc-version-293-pip-3\"><\/a>2022-01-31 16:02:40,219 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\config.local\u2019 to gitignore file.<br>\n2022-01-31 16:02:40,224 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\tmp\u2019 to gitignore file.<br>\n2022-01-31 16:02:40,225 DEBUG: Adding \u2018C:\\senseen-datasets-repos.dvc\\cache\u2019 to gitignore file.<br>\n2022-01-31 16:02:40,236 DEBUG: Version info for developers:<br>\nDVC version: 2.9.3 (pip)<\/h2>\n<p>Platform: Python 3.8.10 on Windows-10-10.0.22000-SP0<br>\nSupports:<br>\nwebhdfs (fsspec = 2022.1.0),<br>\nhttp (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),<br>\nhttps (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)<br>\nCache types: <a href=\"https:\/\/error.dvc.org\/no-dvc-cache\" rel=\"noopener nofollow ugc\">https:\/\/error.dvc.org\/no-dvc-cache<\/a><br>\nCaches: local<br>\nRemotes: None<br>\nWorkspace directory: NTFS on C:<br>\nRepo: dvc, git<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Problems with generating metrics",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problems-with-generating-metrics\/1289",
        "Question_created_time":1659981583328,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":97,
        "Question_body":"<p>I have been trying to create a simple ML pipeline. On the evaluate stage I am creating the evaluation.json file that I state in the dvc.yaml \u2192 metrics: evaluation.json. However, when running dvc metrics show the terminal doesn\u2019t output anything. Instead running dvc metrics show --json, yields an empty json. And whe running dvc metrics show -a -v, I get this:<br>\n2022-08-08 20:50:23,307 DEBUG: Analytics is enabled.<br>\n2022-08-08 20:50:23,344 DEBUG: Trying to spawn \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018\/tmp\/tmp6wqqs37o\u2019]\u2019<br>\n2022-08-08 20:50:23,346 DEBUG: Spawned \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018\/tmp\/tmp6wqqs37o\u2019]\u2019<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"dvc status\" confusion in concurrent use case",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-status-confusion-in-concurrent-use-case\/1284",
        "Question_created_time":1659618058906,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":164,
        "Question_body":"<p>Dear DVC community,<\/p>\n<p>I am currently about to develop a data repository based on GIT+DVC. I am new to DVC, hence might still overlook things.<\/p>\n<p>Let us assume to have the following repository after having cloned the GIT repo and pulled everything via DVC.<\/p>\n<pre><code class=\"lang-auto\">repo1\n|- data_file1\n|- data_file1.dvc\n|- data_file2\n|- data_file2.dvc\n<\/code><\/pre>\n<p>Now, another user of the repository has meanwhile cloned &amp; dvc pulled the repository in an identical way to a directory repo2 (on a different machine, i.e. no cache issues, here). But that user has modified data_file2, followed by all that is necessary to push these changes to the GIT repo and DVC remote storage (in autostage mode, this is: dvc add data_file2, git commit, git push, dvc push). At the end, that user will have<\/p>\n<pre><code class=\"lang-auto\">repo2\n|- data_file1\n|- data_file1.dvc\n|- data_file2         &lt;- different to repo1\n|- data_file2.dvc     &lt;- different to repo1\n<\/code><\/pre>\n<p>which will also be the latest version on the server.<\/p>\n<p>Now, as a common habit, the first user with his repo1 directory will do a \u201cgit pull\u201d and will have<\/p>\n<pre><code class=\"lang-auto\">repo1\n|- data_file1\n|- data_file1.dvc\n|- data_file2      &lt;- as originally present in repo1\n|- data_file2.dvc  &lt;- as uploaded by the second user, i.e identical to repo2\n<\/code><\/pre>\n<p>If this first user issues<\/p>\n<pre><code class=\"lang-auto\">dvc status data_file2.dvc\n<\/code><\/pre>\n<p>it will get with my dvc version 2.11.0 this output<\/p>\n<pre><code class=\"lang-auto\">data_file2.dvc:                                                 \n\tchanged outs:\n\t\tnot in cache:       data_file2\n<\/code><\/pre>\n<p>which is understandable (but still somewhat misleading), as indeed data_file2, as uploaded from repo2, is not in the local cache. If the first user then issues<\/p>\n<pre><code class=\"lang-auto\">dvc status --remote myremote\n<\/code><\/pre>\n<p>where myremote is the remote used by both users, the result is<\/p>\n<pre><code class=\"lang-auto\">deleted:            data_file2\n<\/code><\/pre>\n<p>This is however now confusing, as on the remote, the file was changed, but not deleted.<\/p>\n<p>Am I missing something here?<\/p>\n<p>Somewhat related in the same setting: What is the suggested way to get the latest version then of data_file2? Would it just be a general \u201cdvc pull\u201d?<\/p>\n<p>But what if one has a huge repo and only did partial pulls on individual .dvc files. Would one then have to manually go through all these files? It feels like a \u201cdvc update\u201d that only updates already pulled contents would be good, here. But my take is that \u201cdvc update\u201d only does this for files that have been imported from another repo\u2026<\/p>\n<p>Thank you so much for your patience and your help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Get older version of data files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/get-older-version-of-data-files\/18",
        "Question_created_time":1522591456249,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":2828,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. How do i revert back to an older version of data files using dvc cli?<\/p>\n<p>Thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Removal of partial pull's",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/removal-of-partial-pulls\/1277",
        "Question_created_time":1659111282606,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":181,
        "Question_body":"<p>Dear DVC community,<\/p>\n<p>I am rather new to DVC. I am interested to use DVC in a use case that is maybe related to what is called \u201cData Registrty\u201d, but it is not entirely the same.<\/p>\n<p>I do have a repository that stores in a structured way folders with outputs from expensive computational runs with many files and high storage volume. I added the individual folders as one object, i.e. have one .dvc file per folder.<\/p>\n<p>Example:<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_d.dvc\n<\/code><\/pre>\n<p>A user of the repository would first clone it and, thanks to your partial pull feature, only pull those directories from the remote that are necessary for the next steps in the data analysis pipeline. Thereby the huge repository, for which a full pull would not fit on standard storage, can still be properly used.<\/p>\n<p>Example (cont.):<br>\n(result after \u201cdvc pull folder_a.dvc folder_c.dvc\u201d)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_a\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>Let\u2019s assume now that the user stopped to use one of those folders (say \u201cfolder_a\u201d) after having pushed its updates. Is there an obvious way to remove that partial pull (here \u201cfolder_a\u201d) from the local working copy, without affecting the remote? (This would be done for keeping the local storage requirements on a moderate level.)<\/p>\n<p>Example (cont.):<br>\n(result after applying the searched for operation)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>I do not ask for removing data from the remote but rather a proper way to remove the folder (not it\u2019s .dvc file) and all remaining data in the local cache. I assume here that a simple removal  (\u201crm -fR folder_a\u201d) of the folder would not be enough\u2026<\/p>\n<p>Then, the simplest solution would of course be to delete the local working directory and just clone a new one, where one would start from scratch with a partial pull. However, that might become a bit unhandy over time.<\/p>\n<p>So is there any simple way to do this by a kind of \u201cunpull\u201d operation that I overlooked?<\/p>\n<p>If you need further details, please let me know.<\/p>\n<p>Thanks a lot in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Delete remote artifacts",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/delete-remote-artifacts\/1272",
        "Question_created_time":1659001950703,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":139,
        "Question_body":"<p>Hi,<\/p>\n<p>is there a DVC command to delete all remote artifacts referenced by a <code>.dvc<\/code> file without deleting the  <code>.dvc<\/code> file? My understanding is that the (complex) <code>dvc gc<\/code> command offers means to clean unused artifacts but I struggle to find a way to simply remove pushed artifacts from a remote.<br>\nThis question relates to <a href=\"https:\/\/discuss.dvc.org\/t\/zero-byte-files-created-in-artifactory-remote-http-based\/1271\">my other post<\/a> where I tried to delete corrupted zero-byte remote artifacts uploaded by a <code>dvc push<\/code> command.<\/p>\n<p>Thank you for your help!<\/p>\n<p>Best,<br>\nJoe<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Peer reviews with DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/peer-reviews-with-dvc\/1274",
        "Question_created_time":1659085404628,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":138,
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m in a team of Computer Vision Engineers, and I\u2019m working on improving ours datasets\u2019 management.<br>\nUntil now, we use git LFS to track our dataset. We reach its limits, as we get huge repository, long pull time. We also want  a more secure way to store ours datasets, that could be accessible only by authorize persons. It\u2019s important to precise that our datasets are living, often modified with content added or removed, and we currently do it with PR system.<\/p>\n<p>To improve our process, I found out that DVC may answer some of our issues, and also provides more features afterward (experiments, pipeline\u2026)<br>\nDocuments are sent to the remote, that will be accessible only by authorized people, so it solves the security part.<br>\nDVC is also better for huge files management, I won\u2019t say more about it, I think you already know this.<\/p>\n<p>Now I\u2019ve explained my case, I have some questions:<\/p>\n<p>Relating to PR, I\u2019m aware that it will no longer be possible to use our Git web interface to check PR before approving them, as we won\u2019t see file content change (We also have annotated JSON\/TXT files that will be tracked by DVC, it\u2019s not huge file, but they may contain sensitive data, so we prefer to add them in DVC remote for security purpose).<br>\nWhat are best practices about pull requests review with DVC ? DVC is made for git and for versioning, so I suppose it exists a simple way to check PR ? Or do we have to check locally, by pulling everything ?<\/p>\n<p>Another point, we want to be able to easily know what files have changed, so I\u2019m planning on using dvc add with --recursive flag, to ensure each file is tracked individually. I saw on documentation that it\u2019s better to avoid this flag when a folder contains numerous files. What are the limits ? How it will impact performances ?<\/p>\n<p>Thanks !<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"A separate data-registry for each dataset or combine them into one?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/a-separate-data-registry-for-each-dataset-or-combine-them-into-one\/1262",
        "Question_created_time":1658511793287,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":141,
        "Question_body":"<p>I wonder if it is better to use the same data-registry for two or more separate projects? Or would it better to use a new one for each dataset? For example, what would happen if I have two folders <code>\/data<\/code> in two different projects, then check them add them to a single data repository and then try to run <code>dvc checkout data<\/code> ?? Or <code>dvc pull<\/code>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What to do if the file is not found in the repo?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-to-do-if-the-file-is-not-found-in-the-repo\/1263",
        "Question_created_time":1658515683027,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":145,
        "Question_body":"<p>FileNotFoundError: [Errno 2] No such file or directory: \u2018\/home\/runner\/work\/lrg-omics\/lrg-omics\/.dvc\/cache\/0d\/ce0a9a9337eac485acfc7859b1e28e.dir\u2019<\/p>\n<p>This is what I get in a GitHub action.<\/p>\n<p>I tried <code>dvc checkout --relink<\/code> locally, but that did not fix it.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVClive + MMCV in Container",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvclive-mmcv-in-container\/1236",
        "Question_created_time":1656944958449,
        "Question_answer_count":11,
        "Question_score_count":0,
        "Question_view_count":371,
        "Question_body":"<p>Hi I\u2019m running into problems using the DVClive hook for MMCV running in a separate \u201ctraining\u201d container.<\/p>\n<p>I\u2019m running my <code>dvc exp run<\/code> from within a devcontainer (on a remote host). The actual training is run on the host in another container (that has all the right dependencies installed). Within my <code>config.py<\/code> I included the <code>DvcliveLoggerHook<\/code> as per the documentation. I noticed the <code>interval<\/code> and <code>by_epoch<\/code> arguments where not applying. I.e., I wasn\u2019t seeing any step 1, 2, 3 etc. (until <code>max_epochs<\/code>) in my  <code>dvc exp show<\/code>.<\/p>\n<p>I realized after some hours of reading the documentation and trying out different configs, that it might be because <code>Live.set_step()<\/code> is called from within a <code>DvcliveLoggerHook<\/code> running in one container and the DVC \u201cinstance\u201d in another. So there is no DVC to notify in the container that is trying to notify DVC.<\/p>\n<p>Could this be the problem? Is there a way to fix it?<\/p>\n<p>One way to \u201cfix\u201d, i.e., to work around the problem, I can imagine is just to install everything in the devcontainer. This is doable for the current container I need, but I would rather not.<\/p>\n<p>Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: configuration error - GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-configuration-error-gdrive-remote-auth-failed-with-credentials-in-gdrive-credentials-data\/1254",
        "Question_created_time":1658153900112,
        "Question_answer_count":13,
        "Question_score_count":0,
        "Question_view_count":613,
        "Question_body":"<p>I am not sure what is going wrong here. I copy pasted the credentials files from my Gdrive service account into the secret <code>GDRIVE_CREDENTIALS_DATA<\/code>. I can see that the variable is populated when I run GH action. I tried the credentials on two other computers, but not with the environment variable.<\/p>\n<blockquote>\n<p>ERROR: configuration error - GDrive remote auth failed with credentials in \u2018GDRIVE_CREDENTIALS_DATA\u2019.<\/p>\n<\/blockquote>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: actions\/setup-python@v2\n      - uses: iterative\/setup-dvc@v1\n      - name: sanity-check\n        run: |\n          dvc pull\n          pip install -r requirements.txt\n          python src\/test.py\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n          GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}   \n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Help with dvc add and conda? ERROR: unexpected error - escape character followed by unknown character",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/help-with-dvc-add-and-conda-error-unexpected-error-escape-character-followed-by-unknown-character\/1259",
        "Question_created_time":1658281615328,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"<p>dvc had been working fine for awhile. I created a new branch and at the same time created a new conda enviornment to work in. I was having issues with module not found error with scikit-learn in VSC. After trying to add my data to dvc tracking I was met with this error. Could creating new conda environment cause this issue?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/54eed7e1fd3d2e67164c6642bb380e44d5dadd70.png\" data-download-href=\"\/uploads\/short-url\/c7lPgE2biebyj3m8dhpf5gNW3qo.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/54eed7e1fd3d2e67164c6642bb380e44d5dadd70.png\" alt=\"image\" data-base62-sha1=\"c7lPgE2biebyj3m8dhpf5gNW3qo\" width=\"690\" height=\"46\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/54eed7e1fd3d2e67164c6642bb380e44d5dadd70_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1219\u00d783 5.43 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pull data from Gdrive in GH actions?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pull-data-from-gdrive-in-gh-actions\/1246",
        "Question_created_time":1657891978346,
        "Question_answer_count":10,
        "Question_score_count":0,
        "Question_view_count":386,
        "Question_body":"<p>Hi, I was wondering how I have to setup the github actions yaml file, so that it pulls the data and the model from google drive? Maybe I need to make the Gdrive folder public, or add my Gdrive credentials? Right now, the sanity-check fails using the workflow file from the video.<\/p>\n<p>test.yaml:<\/p>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker:\/\/dvcorg\/cml-py3:latest\n    steps:\n      - uses: actions\/checkout@v2\n      - name: sanity-check\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          python test.py\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best Practices: DVC with cloud training and local evaluation",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-dvc-with-cloud-training-and-local-evaluation\/1258",
        "Question_created_time":1658219537077,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"<p>I have been search for a solution to integrate DVC pipelines with a remote training and a local evaluation during development process of a ML model. First, I like to describe our use case as clear as possible. I will improve the description if you have any questions.<\/p>\n<hr>\n<p>We have one git repository per project, where our code is stored. This includes code for training AI models, some Jupyter notebooks for data analysis, some notebooks for evaluation and unit tests.<\/p>\n<p>Our current process looks like the following:<\/p>\n<ol>\n<li>(Experimental) Data analysis<\/li>\n<li>Development on an AI model<\/li>\n<li>Training of the model<\/li>\n<li>Evaluation of the model results, normally within a notebook<\/li>\n<\/ol>\n<p>Steps 2 - 4 might be repeated various times. Since our models are normally not trainable on a local machine, we are using Google Cloud Vertex AI Custom Training, there you can basically run your training code inside a container. But you need to fulfill some specif requirements:<\/p>\n<ol>\n<li>Training code must be build as python package and uploaded to a bucket<\/li>\n<li>Data should also be available in bucket<\/li>\n<li>Results are stored in a bucket.<\/li>\n<\/ol>\n<p>We are starting the training from our a local machine (this might change in the future, e.g. starting via GitHub Actions). In order to automate some steps above, we wrote a simple command line tool. This builds a python package and most imported queries the bucket location of our files track with DVC. We have the advantage that our DVC Remote Storage is always mounted to the container, and therefor we just need to replace the local path to the data files with remote paths in the bucket. Before each training, we fully commit our code (GIT) and data (DVC). We are able to assign the training runs in the cloud directly to a GIT commit. In some cases, the training container also directly predicts the results of our validation and test sets, since we need to make use of the GPUs, results are again stored in a bucket, which is assigned to the training.<\/p>\n<p>Our evaluation of the model, is commonly experimental. Sometimes it is hard to determinate a fixed score before training a model, and we develop multiple metrics and plots for the evaluation. At the moment, we manually download the training results from the bucket and add with DVC in our repo. But often this done several commits later (this is bad, but happens). If we store the metrics and plots in the DVC format, this commit would not match the commit of the training, also the question is how to automated query the predictions from the bucket.<\/p>\n<hr>\n<p>For now, I don\u2019t have any Ideas how to connect the training step with the evaluation step. In an ideal world, I would create one DVC pipeline which does all the steps. But I even do not have an idea how to connect DVC to the cloud training, and even less how to connect to evaluation with the training.<\/p>\n<p><strong>Do you have any ideas or suggestions how to connect a cloud training with DVC and track the metrics assigned to the training results?<\/strong><\/p>\n<p><em>I do not expect a perfect solution and like to utilize this thread for discussion and exchange of ideas. Thanks for your help.<\/em><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"CML + Github actions + Google Drive \/ Service Account",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-github-actions-google-drive-service-account\/795",
        "Question_created_time":1623930889746,
        "Question_answer_count":16,
        "Question_score_count":1,
        "Question_view_count":1608,
        "Question_body":"<p>Hi,<\/p>\n<p>What is the correct way to use Github secrets with Google Drive key (json file)?<br>\nWe are getting the following error:<\/p>\n<p>ERROR: failed to pull data from the cloud - To use service account, set <code>gdrive_service_account_json_file_path<\/code>, and optionally<code>gdrive_service_account_user_email<\/code> in DVC config<\/p>\n<p>Local execution is configured by<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote gdrive_use_service_account true\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote --local \\\n             gdrive_service_account_json_file_path path\/to\/file.json\n<\/code><\/pre>\n<p>Cheers<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Connect Gdrive on a remote computer",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/connect-gdrive-on-a-remote-computer\/1249",
        "Question_created_time":1657911967756,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":493,
        "Question_body":"<p>I am working on a remote workstation and would like to pull data. I can copy and past that link<\/p>\n<pre><code class=\"lang-auto\">dvc pull\n  0% Querying remote cache|                                                                                                                                                                                       |0\/1 [00:00&lt;?,    ?files\/s]Your browser has been opened to visit:\n\n    https:\/\/accounts.google.com\/o\/oauth2\/...\n<\/code><\/pre>\n<p>into my local browser and login allow access, but then the token is not transfered to the remote machine, which gets stuck.<\/p>\n<p>To clarify, I am logged in to a workstation via ssh and want to pull the data to that workstation, while the data repository is on GDrive. I want to execute all commands on that workstation remotely and I don\u2019t want to use that workstation as my data-repo.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"No write permissions when using GDrive service account on another computer?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/no-write-permissions-when-using-gdrive-service-account-on-another-computer\/1252",
        "Question_created_time":1658144854976,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"<p>Using the Google drive service account with the same json file. On one computer I can execute <code>dvc push<\/code> successfully. On the other computer I get the following error:<\/p>\n<p>ERROR: failed to transfer \u2018md5: 848a6aa790bd34053eb457062307bd45\u2019 - &lt;HttpError 403 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files?supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files?supportsAllDrives=true&amp;alt=json<\/a> returned \u201cInsufficient permissions for the specified parent.\u201d. Details: \u201c[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018forbidden\u2019, \u2018message\u2019: \u2018Insufficient permissions for the specified parent.\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018parent.permissions\u2019}]\u201d&gt;<\/p>\n<p>When I use the same json file, why do I have different permissions on different computers?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Import\/list + webdavs remote not working",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/import-list-webdavs-remote-not-working\/1243",
        "Question_created_time":1657631945188,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":302,
        "Question_body":"<p>I have set up two repositories. The first one, a data registry, currently with a single dataset. The remote git repository is hosted on a company git on <a href=\"http:\/\/github.com\" rel=\"noopener nofollow ugc\">github.com<\/a>. I have access to a DVC remote using the <code>webdavs<\/code> protocol.<\/p>\n<p>I am able to connect to the DVC storage through some web interface and managed to set up the data registry, i.e., I can push and pull data from it. I achieved this with the default <code>dvc remote add data-remote webdavs:\/\/cloud.com\/data-registry<\/code> and storing the username and password combination in a config.local file.<\/p>\n<p>I\u2019ve created a second project in which I would like to use the data stored in the registry. I.e., a project I would like to run some experiments. This project\u2019s remote is hosted by the same cloud provider only I created a different folder for it. I\u2019ve added the credentials for the experiment and the data registry remote in the config.local (these are thus the same credentials). The experiment project also knows about both remotes.<\/p>\n<p>To test I\u2019ve added a stage with output and tracked some empty files. I was able to push these to the remote. Thus, the connection is working. However, when I try to list or import the data registry, DVC reports (with some privacy edits):<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - received 401 (Unauthorized)\nClient error '401 Unauthorized' for url 'https:\/\/cloud.com\/data-registry\/21\/8ff9c6f73767c8126b66f7213cd0d2.dir'\n<\/code><\/pre>\n<p>What am I missing here? Thanks for the help!<\/p>\n<p><strong>EDIT<\/strong>:<\/p>\n<p>When using <code>dvc import<\/code> to import the dataset I am actually also getting the following error, which might explain why I am getting the 401.<\/p>\n<pre><code class=\"lang-auto\">WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:                                                                                        \nname: None, md5: 218ff9c6f73767c8126b66f7213cd0d2.dir\n<\/code><\/pre>\n<p>I updated the data registry <code>git push\/pull<\/code> and <code>dvc push\/pull<\/code> + <code>dvc status --cloud<\/code> and they say they are all up-to-date. I also checked the storage through the web interface and the file does actually exist, well <code>21\/8ff9c6f73767c8126b66f7213cd0d2.dir<\/code> exists.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc list - [WinError 5] Access denied & tracked data file has no associated .dvc file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-list-winerror-5-access-denied-tracked-data-file-has-no-associated-dvc-file\/1250",
        "Question_created_time":1658082967034,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":197,
        "Question_body":"<p>I have two issues I am encountering while working project for school. I am trying to  pull a dataset I created and added to my school gdrive via DVC. Repository, <a href=\"https:\/\/github.com\/takorejsza\/yachts_dataset.git\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - takorejsza\/yachts_dataset: Scrapes sailboat listings.<\/a><\/p>\n<p>First, when trying to run dvc list  I receive an access denied error. It\u2019s odd to me since I am the creator and was able to remotely add it to my school google drive.<\/p>\n<p>Secondly, I don\u2019t have .dvc associated with my data set of interest. When trying to run dvc add  is says it is already staged. I am assuming if I solve the first problem I still wont be able to see the file from dvc list.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0be91019c1612e52997195e7913f7fa3509a7ab2.png\" data-download-href=\"\/uploads\/short-url\/1HmAoTcsMcjiFFv3grXahjT5eym.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/0be91019c1612e52997195e7913f7fa3509a7ab2_2_690x231.png\" alt=\"image\" data-base62-sha1=\"1HmAoTcsMcjiFFv3grXahjT5eym\" width=\"690\" height=\"231\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/0be91019c1612e52997195e7913f7fa3509a7ab2_2_690x231.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/0be91019c1612e52997195e7913f7fa3509a7ab2_2_1035x346.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/0be91019c1612e52997195e7913f7fa3509a7ab2_2_1380x462.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/0be91019c1612e52997195e7913f7fa3509a7ab2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1731\u00d7581 80.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC support for the local storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-support-for-the-local-storage\/71",
        "Question_created_time":1534489981766,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":2330,
        "Question_body":"<p>HI<br>\nCan we add local storage for DVC. like (i dont want to store it on s3 or gcp, need  to only point to local storage)<br>\nex :<\/p>\n<ol>\n<li>dvc add file:\\\\ xxx.xx.xx.x\\images\\annex\\dvc-storage<br>\nor<\/li>\n<li>dvc add X:\/annex\/dvc-storage\/data.xml ( local storage)<br>\nAfter trying above option. i am getting error.<br>\nInitialization error: Config file error: Unsupported URL.<br>\nPlease provide an appropriate solution or syntax<\/li>\n<\/ol>\n<p>Note : storage is tyron.   the storage location is mounted to window or on linux.<\/p>\n<p>With ref : <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-fit-in-a-local-area-network-infrastucture-where-git-repos-are-not-in-the-computing-server\/24\/3\" class=\"inline-onebox\">Does DVC fit in a Local Area Network infrastucture where git repos are not in the computing server?<\/a><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error: Newline or carriage return character detected in HTTP status message or header",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-newline-or-carriage-return-character-detected-in-http-status-message-or-header\/1240",
        "Question_created_time":1657195873640,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":711,
        "Question_body":"<p>Hello<\/p>\n<p>I get the following error when pulling from dvc. Any ideas where such problems come from or how to debug my configuration?<\/p>\n<blockquote>\n<p>dvc pull<\/p>\n<\/blockquote>\n<blockquote>\n<p>ERROR: unexpected error - An HTTP Client raised an unhandled exception: Newline or carriage return character detected in HTTP status message or header. This is a potential security issue.<\/p>\n<\/blockquote>\n<p>Here the system info<\/p>\n<blockquote>\n<h2>DVC version: 2.10.0 (pip)<\/h2>\n<p>Platform: Python 3.9.5 on Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.27<br>\nSupports:<br>\nwebhdfs (fsspec = 2022.5.0),<br>\nhttp (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),<br>\nhttps (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),<br>\ns3 (s3fs = 2022.5.0, boto3 = 1.21.21)<br>\nCache types: <a href=\"https:\/\/error.dvc.org\/no-dvc-cache\" rel=\"noopener nofollow ugc\">https:\/\/error.dvc.org\/no-dvc-cache<\/a><br>\nCaches: local<br>\nRemotes: s3<br>\nWorkspace directory: ext4 on \/dev\/sdb<br>\nRepo: dvc, git<\/p>\n<\/blockquote>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to clean up remote storage?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-clean-up-remote-storage\/1234",
        "Question_created_time":1656865000345,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":174,
        "Question_body":"<p>Hello!<\/p>\n<p>I tried to run the following command<\/p>\n<pre><code class=\"lang-auto\">dvc gc -w -c -v -f\n<\/code><\/pre>\n<p>and expected to clear unnecessary data from the remote storage.<br>\nBut unfortunately, the command froze and nothing happens. And nothing happens on the remote storage<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data (registry) and remote GPU cluster with local DVC repositories",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/data-registry-and-remote-gpu-cluster-with-local-dvc-repositories\/1224",
        "Question_created_time":1656406515356,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":226,
        "Question_body":"<p>I\u2019ve been looking through the documentation and questions for the past days and cannot wrap my head around how to use DVC with the following setup. Or maybe I just need some confirmation, that how I want to do it is reasonable.<\/p>\n<p>As I\u2019m not 100% sure what would all be relevant to provide here at the moment I will try to give a brief description which I can improve on your request(s).<\/p>\n<ol>\n<li>I have a (remote) network drive containing the data (Windows network drive, NTFS) for my projects (mounted locally). I think it is good practice, just in case we need it later for a different project, to make this into a data registry. This is where a first issue arises. First, what I did: 1. I initialize a git + dvc repo on my laptop locally, 2. I setup a remote cache on the network drive.<\/li>\n<\/ol>\n<p><strong>Question<\/strong>: Can I add data that is not in the workspace to the data registry? The mounting point is the same for everybody within the company.<\/p>\n<p>Now, assuming I manage to setup the data registry properly, i.e., it works. I have another issue.<\/p>\n<ol start=\"2\">\n<li>I have another local git + dvc project which will contain the code to train some ML on the data. However, the training is not done on my laptop, but on a remote \u201cGPU cluster\u201d.  I don\u2019t need the  the data on my laptop, but directly to the GPU cluster as it has its own faster storage space.<\/li>\n<\/ol>\n<p><strong>Question:<\/strong> How do I achieve copying the data directly from the data-registries storage (cache or remote?) without losing the data versioning connection?<\/p>\n<p>I am considering to import (dvc import) the data and mount the GPU cluster storage to a folder within the local dvc project. Where I would store the dvc file locally and the data would be downloaded\/uploaded to the GPU cluster.<\/p>\n<p><strong>Question:<\/strong> Is this a good approach? Or is there a better way?<\/p>\n<p>Hope somebody can help. Please ask for specification if you need it.<\/p>\n<p>Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Manage data from one dvc folder with colleagues",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manage-data-from-one-dvc-folder-with-colleagues\/1229",
        "Question_created_time":1656560875701,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":153,
        "Question_body":"<p>Thank you for creating this convenient data versioning tool.<\/p>\n<p>We\u2019re a team of several developers and we\u2019re dealing with a lot of image data. My question is how to manage conflicts when multiple people manage versions in one dvc folder.<\/p>\n<p>For example, if there is a total of 1000 data, developer 1 is learning the model using only 1 to 300 data, and developer 2 updates the data folder to use 200 to 500 data, developer 1 will lose its data.<\/p>\n<p>We\u2019ve also considered downloading and using data in our respective workspaces, but this is impossible because the amount of data is too large. Therefore, it seems that multiple people will have to manage the version in one data folder.<\/p>\n<p>Does the DVC support solutions for these problems?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc_api.get_url is not working with external data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-api-get-url-is-not-working-with-external-data\/1218",
        "Question_created_time":1655903625685,
        "Question_answer_count":10,
        "Question_score_count":2,
        "Question_view_count":322,
        "Question_body":"<p>Hello,<br>\nI\u2019m fairly new to DVC. I would like to explain my set up first and then seek help to make best use of DVC.<\/p>\n<p><strong>My Setup:<\/strong><\/p>\n<ul>\n<li>\n<p>I want to use DVC to track and version external data. The data is large in size(~500GB+ currently) and Located on a NAS server which can be accessed with SSH. I\u2019m referring to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">manage external data tutoria<\/a>l<\/p>\n<\/li>\n<li>\n<p>I want to create a <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registry\" rel=\"noopener nofollow ugc\">Data Registry<\/a> with all of my external data and share it with multiple users to avoid multiple copies of the data using shared cache as explained in \u2018How to share external Cache\u2019<\/p>\n<\/li>\n<\/ul>\n<p><strong>The steps I Followed<\/strong><\/p>\n<ol>\n<li>On my local machine inside a git tracked directory, Created a DVC workspace with <code>dvc_init<\/code>.<\/li>\n<li>Configured a remote directory as dvc remote, let\u2019s say at <code>ssh:\/\/server\/home\/temp_registry<\/code>\n<\/li>\n<li>Created a external cache in <code>ssh:\/\/server\/home\/temp_registry\/temp_cache<\/code> as stated in manage external dataset<\/li>\n<li>I kept a folder named <code>'Dogs' at dvc remote<\/code>, this folder has 2 subfolders, and 2 text files, each subfolders having some images. From my local machine used <code>dvc add --external ssh:\/\/server\/home\/temp_registry\/Dogs\/.<\/code>  to add the external data under dvc tracking.<\/li>\n<li>timely commited .dvc\/config to the gitlab repo from my local machine.<\/li>\n<\/ol>\n<p><strong>The output I got:<\/strong><\/p>\n<ol>\n<li>A Dogs.dvc file was created on my local machine, which was commited to gitlab<\/li>\n<li>In cache directory under <code>ssh:\/\/server\/home\/temp_registry\/temp_cache<\/code>, I found the cached files.<\/li>\n<\/ol>\n<p><strong>The exploration:<\/strong><br>\nI tried to use python api on my local machine to fetch the url of the dvc remote\/Dogs folder.<br>\nI used<br>\n<code>dvc.api.get_url(path='Dogs.dvc', repo='gitlab_repo\/project_name.git', rev=temp_registry_branch')<\/code><\/p>\n<p>Expected output was the URL pointing to the Dogs dir, instead I received the error saying <code>dvc.exceptions.OutputNotFoundError: Unable to find DVC file with output 'Dogs.dvc'<\/code><\/p>\n<p><strong>Now the questions:<\/strong><\/p>\n<ol>\n<li>Is my approach of creating data registry using external data management techniques correct? and is it advisable to use DVC in such a way?<\/li>\n<li>Did I miss any step that should be carried out while setting up a registry on remote?<\/li>\n<li>\n<code>was the usage of dvc_api.get_url correct?<\/code> what can be done to get the url of Dogs folder located on remote?<\/li>\n<\/ol>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Change SSH remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/change-ssh-remote\/1208",
        "Question_created_time":1654850566708,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":200,
        "Question_body":"<p>Hi there, I had a configuration with SSH remote working fine.<br>\nThen, I had to change to other IP so I changed the IP in the configuration file. Then new server is a copy from the previous one.<br>\nNow, I get the following error when I try to push:<\/p>\n<p>ERROR: failed to transfer \u2018md5: 52ad4e880291989c4ac4419cc4d85376\u2019 - Permission denied<\/p>\n<p>Can be something related to the cache?<br>\nfailed to upload \u2018.dvc\/cache\/ca\/217e1b1f3a0c37a2b0c490558d7ddf.dir\u2019 to 'ssh:\/\/user@IP\/path\/data\/ca\/217e1b1f3a0c37a2b0c490558d7ddf<\/p>\n<p>Does anyone know why? Am I missing any step?<br>\nThanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Access to files uploaded with dvc without dvc",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/access-to-files-uploaded-with-dvc-without-dvc\/1207",
        "Question_created_time":1654848969858,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":194,
        "Question_body":"<p>Hi,<br>\nI was playing with DVC and realized it is really powerfull but not sure it can fit my use case.<br>\nSome context, I did some tests with dvc and managed to upload my files to S3, I can push and pull data perfectly.<br>\nI need to be able to access the files uploaded to S3 with other tool in which I cannot integrate dvc, this tool should be able to access the files on S3.  I was expecting to be able to access the files on S3 on any way, but I cannot find where they are located, to me looks like the files themselves are not uploaded but a binary instead (correct me if I am wrong). So, my question is: \u00bfis there any way in which I can access the real files uploaded to s3 without doing a dvc pull?<br>\nThanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC push to SSH remote ERROR: No such file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-to-ssh-remote-error-no-such-file\/1203",
        "Question_created_time":1654677490086,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":384,
        "Question_body":"<p>I do the Getting started guide.<br>\nWhen it is time to add a remote, instead of proposed S3 storage, I use an SSH remote that I add using the following command:<\/p>\n<pre><code>dvc remote add -d storage ssh:\/\/ws\/hddb\/data\/dvc-tutorial\n<\/code><\/pre>\n<p>where ws is a hostname of a workstation with the SSH access enabled, and <code>hddb\/data\/dvc-tutorial<\/code> is the path to the directory in which I would like to keep the data. I actually execute this command being on the workstation through SSH (I can recursively ssh to this workstation when I am already ssh\u2019ed to it).<\/p>\n<p>Then I have to execute the following command to use passphrase-protected SSH keys:<\/p>\n<pre><code>dvc remote modify --local storage password my-lovely-passphrase\n<\/code><\/pre>\n<p>When I do <code>dvc push<\/code> to transfer <code>data\/data.xml<\/code> from the Getting started guide, it fails when the following error:<\/p>\n<pre><code class=\"lang-auto\">\u279c dvc push                                             \nERROR: failed to transfer 'md5: 22a1a2931c8370d3aeedd7183606fd7f' - [Errno 2] No such file or directory: No such file\nERROR: failed to push data to the cloud - 1 files failed to upload\n<\/code><\/pre>\n<p>What should I do to make an SSH remote work? Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC gc issues with shared cache and remote for several repos",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-gc-issues-with-shared-cache-and-remote-for-several-repos\/1187",
        "Question_created_time":1652261294184,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":385,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using DVC for two projects sharing the same remote and cache.<\/p>\n<p>I would like to clean the remote and the cache in order to keep only the data used in the two project workspaces.<br>\nBut when using the command \u201cgc\u201d with these options: \u201cdvc gc -c -w -p PATH_TO_FIRST_PROJECT PATH_TO_SECOND_PROJECT\u201d I\u2019m getting this error:<\/p>\n<p>\"<br>\nERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly. Check the page <a href=\"https:\/\/dvc.org\/doc\/user-guide\/troubleshooting#lock-issue\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/troubleshooting#lock-issue<\/a> for other possible reasons and to learn how to resolve this.<br>\n\"<\/p>\n<p>Looking at the indicated webpage I tried to remove the \u201c.dvc\/tmp\/lock\u201d files in the two projects but the error is still occuring when trying dvc gc.<br>\nThis error also happens without the cloud \u201c-c\u201d option, just trying to clean the shared cache.<br>\nHowever I\u2019m able to clean the cache with a \u201cdvc gc -w\u201d command in any of the two projects (without the \u201c-p\u201d options, meaning that I\u2019m removing the files tracked in the workspace of the other project).<\/p>\n<p>If someone already had this kind of issue and has a solution to handle this situation, I would be glad to have your feedback on this.<br>\nI don\u2019t know if this issue is due to a bad configuration on my side or a bug in DVC\u2026<\/p>\n<p>PS: The output of the \u201cdvc version\u201d command on my system is<\/p>\n<h2>\n<a name=\"dvc-version-2101-conda-1\" class=\"anchor\" href=\"#dvc-version-2101-conda-1\"><\/a>DVC version: 2.10.1 (conda)<\/h2>\n<p>Platform: Python 3.9.12 on Linux-5.13.0-40-generic-x86_64-with-glibc2.31<br>\nSupports:<br>\nwebhdfs (fsspec = 2022.3.0),<br>\nhttp (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),<br>\nhttps (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),<br>\nssh (sshfs = 2022.3.1)<br>\nCache types: hardlink, symlink<br>\nCache directory: ext4 on \/dev\/mapper\/vgubuntu-root<br>\nCaches: local<br>\nRemotes: local<br>\nWorkspace directory: ext4 on \/dev\/mapper\/vgubuntu-root<br>\nRepo: dvc, git<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Single cache or multiple caches in NAS with External Data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/single-cache-or-multiple-caches-in-nas-with-external-data\/1136",
        "Question_created_time":1648633274669,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":722,
        "Question_body":"<p>We have a large NAS with large datasets both in number of files and file size. Those data sets need to be used by several gitlab projects (data wrangling, data processing, modelling, making plots, etc).<\/p>\n<p>We want to have some kind of version control of the data (mainly, say \u201cthis experiment was run with these data\u201d and check that it hasn\u2019t changed), so I have made some simple tests with DVC, but I\u2019d like the opinion of people with more insight. My main question is whether I should set up a single shared cache for all projects, or independent caches for each project (and whether my proposed solution is sound at all). A secondary question is whether the setup will allow to run experiments on the cloud down the line. Here are the details of the problem and my attempted solutions:<\/p>\n<p>Main constraints:<\/p>\n<ul>\n<li>we cannot migrate the data to a flat directory structure where files get renamed with their hash, so the data has to co-exist with the cache. That is, we need to maintain the original directory structure and filenames in the NAS as they are<\/li>\n<li>the NAS data needs to be accessed and processed by several people from several machines (the NAS is an ext2 filesystem that gets mounted with NFS on those machines), typically by cloning the gitlab project to their own \/home\/john_doe\/Software\/my_cats_projects local directory.<\/li>\n<li>we cannot have duplicates of the data, e.g. the one on the NAS and then on each machine that needs to process it<\/li>\n<\/ul>\n<p>The solution I have come up with, and that seems to work is to combine DVC\u2019s \u201cManaging External Data\u201d (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>) and \u201cLarge Dataset Optimization\u201d (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>).<\/p>\n<p>We can have gitlab projects with a softlink to the external data in the NAS, e.g.<\/p>\n<p>my_cats_project<br>\n\u251c\u2500\u2500 bin<br>\n\u2502       \u2514\u2500\u2500 process_some_data.sh<br>\n\u251c\u2500\u2500 data<br>\n\u2502       \u2514\u2500\u2500 some_small_local_data.csv<br>\n\u251c\u2500\u2500 external_data \u2192 \/nas_server\/laboratory_1\/big_dataset_with_cats\/<br>\n|               \u251c\u2500\u2500  cat_movie_001.mov<br>\n|               \u2514\u2500\u2500  cat_movie_002.mov<br>\n\u251c\u2500\u2500 README.md<br>\n\u2514\u2500\u2500 src<br>\n\u2514\u2500\u2500 python_code.py<\/p>\n<p>We initialise dvc in the gitlab project<\/p>\n<p>$ cd my_data_project<br>\n$ dvc init<\/p>\n<p>We then tell dvc to use an external cache on the NAS. This is the main part of my question, whether it\u2019s better to use a single external cache living on the NAS for all datasets and projects,<\/p>\n<p>$ dvc cache dir \/nas_server\/common_dvc_cache<\/p>\n<p>or have separate caches for each project,<\/p>\n<p>$ dvc cache dir \/nas_server\/projects_dvc_cache\/my_cats_project_dvc_cache<\/p>\n<p>Either way, the cache or caches will live on the NAS, avoiding transferring large data files to \/home directories.<\/p>\n<p>To avoid data duplication, we configure the cache to use hardlinks (as reflinks are not available on ext2)<\/p>\n<p>$ dvc config cache.type hardlink<\/p>\n<p>This way, the original data files (cat_movie_001.mov) and their cache \u201ccopies\u201d (asd9890w908ad9sfasd9fasdf980asfd) will point to the same inode.<\/p>\n<p>In this set up, data cannot be overwritten or updates. For example, if we want new versions of the cat movies above, we\u2019ll have to create new directories for those, but that\u2019s fine in this case.<\/p>\n<p>DVC will make the original files non-writable to protect the cache from being corrupted. But we can still move the original data files on the NAS using \u201cdvc move\u201d, right?<\/p>\n<p>Finally, I\u2019d be grateful for any insights on whether this setup would allow to then run projects on cloud service providers supported by DVC.<\/p>\n<p>Thanks a lot!<\/p>\n<p>Ramon.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking Data Provenance with DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-provenance-with-dvc\/1186",
        "Question_created_time":1652228110629,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":209,
        "Question_body":"<p>I recently discovered DVC and am looking to replace my current shell script-based approach for downloading source datasets and building derived datasets with DVC. In my current process, I have a clear record of data provenance as my scripted pipelines begin with downloads of the source datasets from the web.<\/p>\n<p>The question I have is the following: does DVC provide functionality that allows me to capture data provenance somehow? Can I record the URL from which the data was originally sourced and bind that to metadata associated with the data file(s)? Or will I need to maintain scripts that allow me to easily reacquire the data from the web? In the documentation, it seems like the story begins with the source data already in hand.<\/p>\n<p>I would love to have functionality that allows me to easily reacquire the source data from the web if needed and verify that indeed the dataset is equivalent to the original form that was used in the original pipeline development (via hash comparison).<\/p>\n<p>As of yet, I\u2019m not quite seeing how I would accomplish this with existing DVC commands. Any pointers would be greatly appreciated!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Create a stage where the output is a directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/create-a-stage-where-the-output-is-a-directory\/1182",
        "Question_created_time":1651250034111,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":181,
        "Question_body":"<p>I have a process that has a dependency on <code>input.txt<\/code>, and its output is a directory with files of the form <code>outputs\/*.txt<\/code>.<\/p>\n<p>What is the correct way to specify such a stage? I see from <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\/tutorial#second-model-version\" rel=\"noopener nofollow ugc\">this tutorial<\/a> that you can <code>dvc add<\/code> an entire directory. But is it possible to set this directory as the output from a <code>dvc run<\/code> stage?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Train using SageMaker but track output using DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/train-using-sagemaker-but-track-output-using-dvc\/782",
        "Question_created_time":1623143837370,
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":1127,
        "Question_body":"<p>I wonder what is the best way to use Sagemaker with DVC in particular for running a train step which is part of a DVC pipeline. The problem I am running into is that SageMaker will create a training job and output somewhere different than the machine that invokes the training job.<\/p>\n<p>Prior to using SageMaker our flow was to ssh into the relevant EC2 instance and run dvc repro. Now we have a command that invokes the same train job from our local machine which essentially kicks out train in an EC2 instance of our choice and stores the output in a s3 bucket.<\/p>\n<p>Is there a way to run the training job, sync the output locally and inform DVC that this is the output of running a particular step in the pipeline? If that is complicated, is there a better alternative to working with SageMaker and DVC?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best Practices: How to track data?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-how-to-track-data\/1157",
        "Question_created_time":1649753952914,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":344,
        "Question_body":"<p>Hello together,<\/p>\n<p>I wounder if there are some best practices for tracking data with dvc, because there are multiple ways and I want to know the advantages and disadvantages.<\/p>\n<p>For example, having a project setup as the following, where there are just a few files in the directories, but these are really large ( &gt;=1Gb ) or having a lot of smaller files like images or mp3s.<\/p>\n<p>Large files:<\/p>\n<pre><code class=\"lang-auto\">|--- data\/\n|  |--- # raw data files\n|  |--- train.csv\n|  |--- test.csv\n<\/code><\/pre>\n<p>Many files:<\/p>\n<pre><code class=\"lang-auto\">|--- data\/\n|  |--- img_01.jpg\n|  |--- [...]\n|  |--- img_10000000.jpg\n<\/code><\/pre>\n<p>I can add the folder to dvc by adding <code>dvc add data\/<\/code>, what would create on <code>data.dvc<\/code> that tracks all files in one checksum. Or I can add the files with <code>dvc add data\/*<\/code> which would create one <code>.dvc<\/code> file per file in the folder and than I need to add every new file manually.<\/p>\n<p>Which way is recommended, what are the benefits of the different approaches? I can image tracking a lot of files with one <code>.dvc<\/code> per file can be a nightmare (having tonnes of files to track with git). I have also experienced that dvc checksum calculation can take a while if you track a whole folder.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Failed transfer to remote S3 storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/failed-transfer-to-remote-s3-storage\/1166",
        "Question_created_time":1650448395548,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":333,
        "Question_body":"<p>Hey, I am an intern and trying to learn DVC for S3 remote storage. I got this error . Can you help me?<br>\nThank you,<\/p>\n<p>dvc push<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 70257f9e4b0b3ec72deeed5e1b880c3b.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 8a31b1338b609cf1133d1df639de606c.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 0ebacd64e04c0e4a97d8fa48a4d707e6.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: b6c29e96813679dd8c451c9b5ca86e2c.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: e42412b82dcab425ce9c7e2d0abdfb78.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 84453a3661e17405f48087e6cf409c51.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 22e3f61e52c0ba45334d973244efc155.dir<br>\nWARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: tutorials\/versioning\/new-labels.zip, md5: 2eaa473159443e75e6fb7b29e56c0787<br>\nname: tutorials\/nlp\/pipeline.zip, md5: 1d2070ee188fc5e4d94ad920e6cc82aa<br>\nname: images\/owl_sticker.png, md5: fa8b9e82c893eb401f30c353bd550ada<br>\nname: tutorials\/versioning\/data.zip, md5: fa9c0eb4173d86695b4e800219651360<br>\nname: mnist\/images.tar.gz, md5: acb39268fb9dd785aac887f69b89282e<br>\nname: tutorials\/nlp\/Posts.xml.zip, md5: ce68b98d82545628782c66192c96f2d2<br>\nname: fashion-mnist\/images.tar.gz, md5: 3c8f026df7c3973bcec8e9f945026eb2<br>\nname: images\/dvc-logo-outlines.png, md5: 5846bf188572bbc78a10124f36e92631<br>\nname: images\/owl_sticker.svg, md5: c76a031091a240c014baea8eff8619c4<br>\nERROR: failed to transfer \u2018md5: a304afb96060aad90176268345e10355\u2019 - [Errno 13] Permission denied: \u2018\/path\u2019<br>\nERROR: failed to push data to the cloud - 1 files failed to upload<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to make DVC wait before checking if output file exists?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-make-dvc-wait-before-checking-if-output-file-exists\/1170",
        "Question_created_time":1650539400631,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":159,
        "Question_body":"<p>I\u2019m running a command remotely through ssh, and it takes the remote cluster a while to finish the script. The problem is that DVC fails on this step because it immediately checks if the output file was created. Apart from just removing this file as an output, is there a way to have DVC wait and\/or ignore whether the file was created?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Version control of the raw data with the colleagues simultaneously",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/version-control-of-the-raw-data-with-the-colleagues-simultaneously\/1080",
        "Question_created_time":1646030816402,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":314,
        "Question_body":"<p>Hello, I recently came across DVC while looking for a tool for data versioning and I\u2019ve found DVC very useful, so I\u2019m testing several features for data versioning with my colleagues.<\/p>\n<p>But we have difficulties with managing raw dataset when we deal with it simultaneously and want to ask you some help\/guide.<\/p>\n<p>First, we describe our system as follows:(Assume there are two colleagues - denoted as Col1, Col2, respectively)<\/p>\n<ul>\n<li>\n<p>Col1 &amp; Col2 have a workspace respectively (denoted as WS1 and WS2) and each workspace has \u2018mounted NAS\u2019 which stores image data and should be shared.<br>\nPath for WS1 : \/home\/work\/dvc_col1    %% Here Col1 execute git init, dvc init, etc.<br>\nPath for image data to share among colleagues in NAS: \/home\/work\/mnt\/storage\/img_data<\/p>\n<p>Path for WS2: \/home\/work\/dvc_col2     %% Here Col2 execute git init, dvc init, etc.<br>\nPath for image data to share among colleagues in NAS: \/home\/work\/mnt\/storage\/img_data<\/p>\n<\/li>\n<\/ul>\n<p>Col1 initially starts data versioning following DVC docs as follows:<br>\nSince the data to be versioned is located outside of the workspace, Col1 execute the followings in WS1:<\/p>\n<pre><code>dvc cache dir \/home\/work\/mnt\/save_cache\ndvc config cache.shared group\ngit add .dvc\ngit commit -m \"set cache dir\"\ndvc add \/home\/work\/mnt\/storage\/img_data --external\ngit add img_data.dvc\ngit commit -m \"1st version data, 300 images\"\ngit tag -a \"v1.0\" -m \"1st version\"\n<\/code><\/pre>\n<p>(\u2014&gt;&gt; This 1st version includes 300 images.)<\/p>\n<ul>\n<li>\n<p>And additional 200 images are added to \/home\/work\/mnt\/storage\/img_data. (so in total 500 images.)<br>\nTo mange the version of the dataset, Col1 execute the followings:<\/p>\n<p>dvc add \/home\/work\/mnt\/storage\/img_data --external<br>\ngit add img_data.dvc<br>\ngit commit -m \u201c2nd version data, 200 images added, in total 500 images\u201d<br>\ngit tag -a \u201cv2.0\u201d -m \u201c2nd version\u201d<\/p>\n<\/li>\n<li>\n<p>Then Col1 can version the image data with \u2018git checkout v1.0 or v2.0\u2019 and \u2018dvc checkout\u2019.<\/p>\n<\/li>\n<li>\n<p>So, Col1 uses \u2018git push\u2019 to their GitLab project and Col2 downloaded it using git pull.<\/p>\n<\/li>\n<li>\n<p>After this, Col2 can also version the same image data with \u2018git checkout\u2019 and \u2018dvc checkout\u2019 and it works fine.<\/p>\n<\/li>\n<\/ul>\n<p>But problem arises here:<\/p>\n<ul>\n<li>\n<p>When Col1 execute \u2018git checkout v2.0\u2019 &amp; \u2018dvc checkout\u2019 in WS1, the number of images in \u2018\/home\/work\/mnt\/storage\/img_data\u2019 becomes 500 and Col2 can also recognize it.<\/p>\n<\/li>\n<li>\n<p>Since Col2 wants to use the 1st version of dataset, Col2 execute \u2018git checkout v1.0\u2019 &amp; \u2018dvc checkout\u2019 in WS2, and the number of images in \u2018\/home\/work\/mnt\/storage\/img_data\u2019 becomes 300. This makes problem, because now Col1 can\u2019t access the added 200 images anymore. \u2018\/home\/work\/mnt\/storage\/img_data\u2019 shows only 300 images, so Col1 and Col2 can\u2019t access to the dataset each wants to use SIMULTANEOUSLY.<\/p>\n<\/li>\n<\/ul>\n<p>Are we using DVC wrong? We want to access each of the versioned dataset simultaneous with DVC. We\u2019re very appreciated with your comment and help. Thank you.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Wildcards in stage deps?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/wildcards-in-stage-deps\/1151",
        "Question_created_time":1649271460010,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":297,
        "Question_body":"<p>Is it possible to specify dependancies for a stage using wildcard characters (aka glob)? I would like to be able to write something like<\/p>\n<pre><code class=\"lang-auto\">stages:\n  compare:\n    cmd: python src\/compare_data.py\n    deps: \n    - src\/compare_data.py\n    - data\/parameter_*.csv\n<\/code><\/pre>\n<p>In this case, the <code>parameter_001.csv<\/code> etc files came from a previous stage that includes a <code>foreach<\/code> loop that created the <code>csv<\/code> files.<\/p>\n<p>When I tried the above, I get the error <code>dependency 'data\/parameter_*.csv' does not exist<\/code>, even though the file (with <code>*<\/code> replaced by <code>001<\/code>) does actually exist.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Updating files within a tracked directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/updating-files-within-a-tracked-directory\/1145",
        "Question_created_time":1649255177076,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":166,
        "Question_body":"<p>I have a large directory tracked by dvc with a single .dvc file. Often, I\u2019ll will want to just pull a single file within that directory, make some modifications, and then push those changes back up to the remote repo. However, if I just do <code>dvc add my_directory\/<\/code>, then it will think I want to <em>delete<\/em> all the other files. Do I have to pull down the entire directory in order to modify a single file?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practice for applying pipelines to many datasets?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-applying-pipelines-to-many-datasets\/1147",
        "Question_created_time":1649258414789,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":192,
        "Question_body":"<p>Hi folks,<\/p>\n<p>I have a data analysis problem which I want to use DVC to help me with. I have a series of many different datasets which each need to have an analysis stage applied to them. Each dataset is a folder with a specific set of (large) files, and the same python analysis code needs to be applied to each one.<\/p>\n<p>Rather than just run one monolithic script that loops over all the datasets, I would like to create a pipeline to help me handle this. I am imagining some sort of pipeline where the script gets applied to each dataset separately (and possibly in parallel).<\/p>\n<p>Is something like this even possible with dvc?<\/p>\n<p>Thanks,<\/p>\n<p>Steve<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Outputs the results of dvc diff",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/outputs-the-results-of-dvc-diff\/1129",
        "Question_created_time":1648090265972,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":174,
        "Question_body":"<p>Hello,<br>\nI\u2019m trying to find the way I get the results of dvc diff as a file (such as .json).<\/p>\n<p>\u201cdvc diff\u201d command only shows the difference between two commits(or tags) in my ubuntu display and I don\u2019t know how to save the displayed results which afterwards I can process like in jupyter notebook.<\/p>\n<p>Could you tell me how I can save the dvc diff results using ubuntu command?<\/p>\n<p>Thanks you so much.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc checkout takes long time",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-checkout-takes-long-time\/1127",
        "Question_created_time":1647850736880,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":541,
        "Question_body":"<p>Hello,<br>\nI\u2019m testing dvc in the workspace(or dvc project) of a gpu server and NFS mounted NAS to that server.<\/p>\n<p>The dataset is about a million images (~40GB) and is located in NAS.<\/p>\n<p>I run the following command in my workspace:<\/p>\n<pre><code> dvc add \"dataset path in NAS\" -o .   (already set dvc cache dir in a path in NAS.)\n<\/code><\/pre>\n<p>This takes about 2 hours. (I wonder it\u2019s normal).<\/p>\n<p>The problem is when I run \u2018dvc checkout\u2019 in the other workspace to see it works fine, it takes about 1 hour. (I also set dvc config cache.type symlink.)<\/p>\n<p>Even though I make symlink files from the original dataset using os.symlink in python, it doesn\u2019t take this long. (I guess it\u2019s about a few minutes).<\/p>\n<p>Could I shorten this \u2018dvc checkout\u2019 time as short as possible?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/977d7b25475db607c9da54f925de20434f5998cb.png\" data-download-href=\"\/uploads\/short-url\/lC8Wh3txuQbHAjckfoN7Ub1Q1lx.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/977d7b25475db607c9da54f925de20434f5998cb.png\" alt=\"image\" data-base62-sha1=\"lC8Wh3txuQbHAjckfoN7Ub1Q1lx\" width=\"690\" height=\"314\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/977d7b25475db607c9da54f925de20434f5998cb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">758\u00d7345 23.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Specify AWS profile when adding external data from S3",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/specify-aws-profile-when-adding-external-data-from-s3\/1126",
        "Question_created_time":1647607974097,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":1159,
        "Question_body":"<p>Hi all,<\/p>\n<p>how can I make DVC use a specific AWS-profile when adding external data?<\/p>\n<p>If my credentials are stored in the default-profile I can add external data as follows (just following the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">S3-Example for Managing External Data<\/a>)<\/p>\n<pre><code class=\"lang-auto\">$ git init\n$ dvc init\n$ dvc remote add s3cache s3:my-bucket\/cache\n$ dvc config cache.s3 s3cache\n$ dvc add --external s3:\/\/my-bucket\/remote-data.txt\n<\/code><\/pre>\n<p>But I want to use another profile (<code>MyProfile<\/code>) that I can add to <code>s3cache<\/code>:<\/p>\n<pre><code class=\"lang-auto\">$ dvc remote modify s3cache profile MyProfile\n<\/code><\/pre>\n<p>If the default profile is removed or changed from the <code>.aws\/credentials<\/code> the following gives a <code>Bad Request<\/code>:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external s3:\/\/my-bucket\/remote-data.txt\n# ERROR: unexpected error - [Errno 22] Bad Request: An error occurred (400) when calling the HeadObject operation: Bad Request\n<\/code><\/pre>\n<p>How can I provide the profile information to the add?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Timing to create a dvc repo for a 60GB dataset?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/timing-to-create-a-dvc-repo-for-a-60gb-dataset\/945",
        "Question_created_time":1635739140181,
        "Question_answer_count":15,
        "Question_score_count":1,
        "Question_view_count":568,
        "Question_body":"<p>Hello<\/p>\n<p>I am a new user, working in Linux, and I am creating a DVC remote repo on NFS. I have a dataset split across four sub-directories, each one with 10000 images, total size of all four sub-directories is 62GB.<\/p>\n<p>If I do a straight copy (not dvc) of the dataset to the repo location, the copy takes 50 min.<\/p>\n<p>If I do \u2018dvc add\u2019 to the remote repo, the command is still running after 3.5 days. I assume that this is not expected? What would be expected time for dvc add to a remote repo, versus a straight copy to the same location?<\/p>\n<p>Thanks, Paul<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc experiments multiple branches workflow",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-experiments-multiple-branches-workflow\/1124",
        "Question_created_time":1647558581044,
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":262,
        "Question_body":"<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"><\/a>Background<\/h3>\n<p>I am using dvc with tensorflow. I have two versions of my model which are different enough to merit their own git branches, i.e. they have different architecture definitions and different data generators.<\/p>\n<p>To run experiments, I <code>git checkout<\/code> each branch, make my changes and queue up an experiment. I have two branches so I end up queuing up 2 experiments. Then I call <code>dvc exp run --run-all -j 2<\/code> to run them both in parallel.<\/p>\n<p>This is basically what <code>dvc exp show -a<\/code> looks like (don\u2019t pay attention to the metrics and params):<\/p>\n<pre><code class=\"lang-auto\">\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Experiment                 \u2503 Created      \u2503 auc.model_params \u2503 auc.model_t_params \u2503 auc.model_nt_params \u2503 train_model.walltime \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 workspace                  \u2502 -            \u2502         10787481 \u2502           10782921 \u2502                4560 \u2502 3:00:00:00           \u2502\n\u2502 branch_1                   \u2502 12:48 PM     \u2502         10787481 \u2502           10782921 \u2502                4560 \u2502 1:00:00:00           \u2502\n\u2502 \u2514\u2500\u2500 508af88 [exp_1]        \u2502 06:49 PM     \u2502                - \u2502                  - \u2502                   - \u2502 3:00:00:00           \u2502\n\u2502 \u251c\u2500\u2500 6d926b6 [exp_2]        \u2502 06:47 PM     \u2502                - \u2502                  - \u2502                   - \u2502 3:00:00:00           \u2502\n\u2502 branch_2                   \u2502 06:22 PM     \u2502           697998 \u2502             696878 \u2502                1120 \u2502 1:00:00:00           \u2502\n\u2502 master                     \u2502 Nov 17, 2021 \u2502                - \u2502                  - \u2502                   - \u2502 -                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n<\/code><\/pre>\n<h3>\n<a name=\"question-1-2\" class=\"anchor\" href=\"#question-1-2\"><\/a>Question 1<\/h3>\n<p>How do I interpret the lines next to the experiment names? It looks like <code>exp_1<\/code> experiment correctly connects to branch_1, but why does it look like the <code>exp_2<\/code> experiment connects to both branch_1 and branch_2? Did I set up these experiments incorrectly or is this a display bug or what?<\/p>\n<h3>\n<a name=\"question-2-3\" class=\"anchor\" href=\"#question-2-3\"><\/a>Question 2<\/h3>\n<p>My other more general question is: is this the intended workflow for managing experiments on multiple branches? Any comments on this question from a dvc expert would be very much appreciated.<\/p>\n<h2>\n<a name=\"thanks-4\" class=\"anchor\" href=\"#thanks-4\"><\/a>Thanks!<\/h2>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc repro --force-downstream not working",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-repro-force-downstream-not-working\/1122",
        "Question_created_time":1647474556097,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":184,
        "Question_body":"<p>Hi,<\/p>\n<p>I setup some dvc data pipeline and use dvc repro command generated some .pkl files. I later on decided to regenerate the files (due to code change, where the dependency wasn\u2019t captured in the dvc.yaml file), so I deleted the file manually in the directory. This is probably the first mistake I made.<\/p>\n<p>So now when I run dvc repro, dvc refuse to re-run and report nothing has been change.<\/p>\n<p>I thought dvc repro --force-downstream [target] should work, but it\u2019s still reporting \u201ceverything is up to date\u201d and not reproducing the stage.<\/p>\n<p>How do I force dvc to re-run the stage to regenerate the .pkl file?<\/p>\n<p>Thanks,<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC with external data is very slow",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-external-data-is-very-slow\/1121",
        "Question_created_time":1647375655860,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":215,
        "Question_body":"<p>Hi there,<\/p>\n<p>I think my question has two parts: firstly, is the use of external data (as described <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">here<\/a>) the best way in my case and, if so, why is it so slow to add files?<\/p>\n<p>The context: my ML code + git repo live on my local machine. I develop this codebase using pycharm,  and use pycharm\u2019s remote ssh deployment to run my code on a remote machine where the data lives. The dataset is a large imaging dataset, &gt;200GB with 1000s of files, and will not fit on my local machine. When I run my code, pycharm copies it to the remote machine to run it, but it does not copy the git files so there is no git repo on my remote machine.<\/p>\n<p>As my git repo and data live on different machines, it seemed like adding the external dataset to DVC using ssh would be a good way to go about things. However the dvc add command is taking a long time to run. In fact I tried to run it on a 6GB subset of my data and it errored out:<\/p>\n<p><code>ERROR: too many open files, please visit &lt;https:\/\/error.dvc.org\/many-files&gt; to see how to handle this problem<\/code><\/p>\n<p>So my questions are:<\/p>\n<ol>\n<li>Is this a sensible way to be using DVC?<\/li>\n<li>If so, how can I speed things up? It seems like everything is getting downloaded to my local machine, even though I have set up a remote cache over ssh. Is there any way I can get it to compute the hashes on the remote machine?<\/li>\n<\/ol>\n<p>Thanks for your help,<br>\nMark<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cache duplication for external dataset",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cache-duplication-for-external-dataset\/1100",
        "Question_created_time":1646637633567,
        "Question_answer_count":9,
        "Question_score_count":0,
        "Question_view_count":323,
        "Question_body":"<p>Hello, I have questions about the DVC cache duplication when I have an external dataset.<\/p>\n<p>There are two cases I\u2019ve tried to figure out according to the location of cache, dataset:<\/p>\n<p>(Assume there are two datasets to version control, i.e., dataset 1 &amp; dataset 2. The size of dataset 1 = 10M, the size of dataset 2 = 10M, and there is 5M duplication between dataset 1 &amp; dataset 2.)<\/p>\n<p><strong>[Case 1]<\/strong> Cache and both datasets are in the same storage as the DVC workspace.<br>\nIn this case, after I dvc add dataset1 and then dvc add dataset1 + dataset2, the size of the cache dir is 15M(10M + 5M). It seems ok.<\/p>\n<p><strong>[Case 2]<\/strong> Datasets are in the external storage with respect to the DVC workspace. (Cache is in or out of the storage where the DVC workspace is.)<br>\nIn this case, after I dvc add datset1 and then dvc add dataset1 + dataset2, the size of the cache dir is 25M.(10M + 15M) So, the feature DVC provides for eliminating the cache duplication doesn\u2019t work fine for external dataset.<\/p>\n<p>I checked the size of the folders using du -hs in my Ubuntu environment.<br>\nCan I use that DVC feature to eliminate cache duplication when I have an external dataset?<\/p>\n<p>Thank you.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple dvc runs in parallel",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-dvc-runs-in-parallel\/1102",
        "Question_created_time":1646665895959,
        "Question_answer_count":10,
        "Question_score_count":0,
        "Question_view_count":738,
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying to run multiple dvc experiments simultaneously on a gpu cluster. The way the cluster works is that you send a bash script to the cluster and it runs the job when its your turn. In my bash script, I configure CUDA_VISIBLE_DEVICES, activate my conda environment and run the experiment using <code>dvc exp run --temp<\/code>. Before I submit my job, I make the changes I want for my experiment, e.g. set the variables in params.yaml. I am trying to send a job, wait for it to start running, then make changes and send another. The second job fails with this error:<\/p>\n<p><code>ERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly. Check the page &lt;https:\/\/dvc.org\/doc\/user-guide\/troubleshooting#lock-issue&gt; for other possible reasons and to learn how to resolve this.<\/code><\/p>\n<p>I was hoping that using <code>--temp<\/code> would allow me run these jobs at the same time because it isolates the project in a separate directory. One thing I should note is that my cache dir is in a separate directory outside my project directory because I am not allowed to use that much space in my user directory.<\/p>\n<p>My question is related to this one: <a href=\"https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929\/2\">https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929\/2<\/a> in which the suggested course was to use <code>--queue<\/code> and <code>-j<\/code> to run the experiments in parallel. How can I use the built in parallel experiment execution but run the experiments in separate job submissions to the cluster? Or what is the intended workflow for this sort of situation?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Track remote data on Azure",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-remote-data-on-azure\/1110",
        "Question_created_time":1646821961294,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":487,
        "Question_body":"<p>Hello DVC community!<\/p>\n<p>I\u2019m currently trying out DVC on Ubuntu 20.04 LTS, with Python 3.8.10, on WSL2 with Windows 10.<br>\nI am running into some seemingly inconsistent issues trying to track ZIP files in a remote Azure BLOB Storage container.<\/p>\n<p>At least in the initial set-up, I am trying to track those files without downloading them to the local machine. Apologies if this is really a basic question but I a bit stumped at the moment.<\/p>\n<p>I used \u201cdvc remote add\u201d and \u201cdvc remote modify\u201d, the generated config \/ config.local files in the .dvc folder look good. I added a valid connection string for the Azure storage account with those commands, then I also added that connection string to my environment with:<\/p>\n<blockquote>\n<p>export AZURE_STORAGE_CONNECTION_STRING=\u2018myconnectionstring\u2019<\/p>\n<\/blockquote>\n<p>I am trying the following command:<\/p>\n<blockquote>\n<p>dvc import-url azure:\/\/[mycontainer]\/[myfile] --to-remote<\/p>\n<\/blockquote>\n<p>The remote is another BLOB container in the same Storage Account where the files that I want to track are located.<\/p>\n<p>Yesterday I managed to use the \u201c<em>import-url \u2026 --to-remote<\/em>\u201d command even though the .dvc files that were created locally landed in the wrong folder (my mistake with paths etc). Today I am simply getting the error:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - : \u2018azure\u2019<\/p>\n<\/blockquote>\n<p>If I try to run:<\/p>\n<blockquote>\n<p>dvc import-url azure:\/\/[mycontainer]\/[myfile] .\/path\/to\/local\/folder\/ --to-remote<\/p>\n<\/blockquote>\n<p>The error I get is:<\/p>\n<blockquote>\n<p>ERROR: failed to import azure:\/\/[mycontainer]\/[myzipfilename]. You could also try downloading it manually, and adding it with <code>dvc add<\/code>. - bad DVC file name \u2018path\/to\/local\/folder\/[myzipfilename].dvc\u2019 is git-ignored.<\/p>\n<\/blockquote>\n<p>Same error as the above appears if I move into the folder where I want the .dvc files to land and run the dvc import-url command from there, regardless of whether I do this with specifying the output file name or not.<\/p>\n<p>EDIT: I ran it with -v and this is what I get:<\/p>\n<blockquote>\n<p>2022-03-09 11:39:09,229 DEBUG: Lockfile for \u2018dvc.yaml\u2019 not found<br>\n2022-03-09 11:39:09,231 ERROR: unexpected error - : \u2018azure\u2019<\/p>\n<\/blockquote>\n<p>Somehow there is no lock file, seems I am missing something super basic <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>SECOND EDIT:<br>\nIf I run the import-url command from the root project folder without specifying a path that is different from this project root, it works, with the \u201cproblem\u201d that the local .dvc file is in the root and not in a subdirectory where I actually want it to be. I still get the \u201cLockfile for \u2018dvc.yaml\u2019 not found\u201d debug msg but then \u201cComputing md5 for a large file \u2026\u201d indicating that the command \u201cworks\u201d.<\/p>\n<p>I am getting more confused by the minute lol.<br>\nAny ideas how I can fix this?<\/p>\n<p>Thanks a bunch in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Wevdav problems",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/wevdav-problems\/1115",
        "Question_created_time":1646986350383,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":194,
        "Question_body":"<p>Hi, I am trying to use dvc with a local nas server and webdav. I have 2 windows machines and a nas server on a local network.<\/p>\n<p>Problem 1:<br>\nWhen pushing, for some of the files I get errors like this:<br>\n<code>ERROR: failed to transfer 'md5: &lt;...&gt;' - the server does not allow creation in the namespaceor cannot accept members: received 403 (Forbidden): Client error '403 Forbidden' for url &lt;url&gt;' For more information check: https:\/\/httpstatuses.com\/403<\/code><\/p>\n<p>Pushing a second time will push files it could not push on the first time. Maybe the server can\u2019t handle so many requests?<\/p>\n<p>Problem 2:<br>\nI can push\/pull with machine 1 but not with machine 2, although both have the same dvc configuration.<br>\nThe error is:<br>\n<code>ERROR: unexpected error - [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer  die Verbindung verweigerte: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte<\/code><\/p>\n<p>What I actually can do:<\/p>\n<ul>\n<li>ftp access with both machines (with winscp)<\/li>\n<li>webdav access with both machine (with winscp)<\/li>\n<\/ul>\n<p>Maybe it has not anything to do with dvc.<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.9.3 (exe)\n---------------------------------\nPlatform: Python 3.8.10 on Windows-10-10.0.18363-SP0\nSupports:\n        azure (adlfs = 2021.10.0, knack = 0.9.0, azure-identity = 1.7.1),\n        gdrive (pydrive2 = 1.10.0),\n        gs (gcsfs = 2021.11.0),\n        hdfs (fsspec = 2021.11.0, pyarrow = 6.0.1),\n        webhdfs (fsspec = 2021.11.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        s3 (s3fs = 2021.11.0, boto3 = 1.17.106),\n        ssh (sshfs = 2021.11.2),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.3),\n        webdavs (webdav4 = 0.9.3)\nCache types: hardlink\nCache directory: NTFS on D:\\\nCaches: local\nRemotes: azure, webdavs, local, local, local\nWorkspace directory: NTFS on D:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>Many thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Why DVC not working with AWS role on AWS Batch?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/why-dvc-not-working-with-aws-role-on-aws-batch\/1114",
        "Question_created_time":1646921366784,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":189,
        "Question_body":"<p>I\u2019m using DVC on AWS Batch, and have configured the job definition with a role with correct S3 policies (I know its correct because it worked on github actions). But, when running the job with dvc pull I got this error:<\/p>\n<pre><code class=\"lang-auto\">dvc pull -R \/app\/data\/training\nERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>The same job, if runned with the envs AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY configured, runs ok.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC and AWS EFS",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-aws-efs\/1103",
        "Question_created_time":1646678462276,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":284,
        "Question_body":"<p>Hi. Can DVC work with an Amazon Web Services EFS volume? I looked at the list of supported storage types and did not see EFS, but figured I\u2019d ask here to be sure because my boss is asking me to be sure <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - OpenSSH private key encryption requires bcrypt with KDF support",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-openssh-private-key-encryption-requires-bcrypt-with-kdf-support\/1085",
        "Question_created_time":1646237971898,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":320,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m using DVC for several weeks, but I\u2019m now trying to use a ssh remote for the first time.<br>\nI configured the ssh remote as indicated in the documentation and each time I\u2019m trying to pull or push on the remote storage I get this error:<\/p>\n<p>\u201cERROR: unexpected error - OpenSSH private key encryption requires bcrypt with KDF support\u201d<\/p>\n<p>I have this error even when I\u2019m providing my ssh password in the dvc config file, or when I\u2019m asking the password with the \u201cask_password true\u201d option.<\/p>\n<p>My ssh key is using a passphrase and was generated with ssh-keygen (no special CLI arguments).<br>\nI\u2019m not sure if ssh-keygen is generating by default a key with bcrypt and KDF support (but ideally I would prefer to use my ssh password and not an ssh private key).<\/p>\n<p>Am I doing something wrong here?<br>\nIf someone has some insights about this issue, I would be very glad to know how to solve this issue.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Rewriting git history and moving binaries to dvc",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/rewriting-git-history-and-moving-binaries-to-dvc\/953",
        "Question_created_time":1636163254101,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":245,
        "Question_body":"<p>Are there any tools or guides for rewriting a git repo containing binaries in the git history to dvc based storage?<\/p>\n<p>I can delete the existing binary files and use dvc and S3 to store them but would like to be able to rewrite the git history.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can my workspace be an s3 location?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-my-workspace-be-an-s3-location\/1083",
        "Question_created_time":1646192567568,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":250,
        "Question_body":"<p>I would like to run <code>dvc pull<\/code> directly into an s3 bucket. Is that possible?<\/p>\n<p>Long story short: I would like end users to version control their csv files using dvc. I then would like to have a recurring process that will do a dvc pull, of the main branch, directly into an s3 bucket. I understand that dvc can use s3 for storage, but I would like to re-create the data in s3, in an identical structure as the dvc git repository.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Correct flow to update file in tracked directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/correct-flow-to-update-file-in-tracked-directory\/1077",
        "Question_created_time":1645544777705,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":217,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019ve been reading the documentation (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/update-tracked-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/update-tracked-data<\/a>) on how to update tracked data. The examples here seem to talk about modifying files which are tracked individually.<\/p>\n<p>I\u2019d like to know what is the correct way to modify a file inside a tracked directory. So say I have the following tracked directory with two files inside it:<\/p>\n<pre><code class=\"lang-auto\">my_dir\n  - foo.csv\n  - bar.csv\n<\/code><\/pre>\n<p>and i want to modify foo.csv and then commit the updates, is this the correct flow?<\/p>\n<pre><code class=\"lang-auto\">dvc unprotect foo.csv\n&lt;modify the file&gt;\ndvc add my_dir\ndvc push\n<\/code><\/pre>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Explicitly define pipeline execution order",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/explicitly-define-pipeline-execution-order\/1073",
        "Question_created_time":1645444562953,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":154,
        "Question_body":"<p>Is there a way to specify a dependency between two pipeline stages except for a <code>outs<\/code> and <code>deps<\/code> combination? My use case is to run two stages consecutive, although they don\u2019t read \/ write files, that could be tracked by DVC, but depend on each other in a different way, e.g. by one stage writing a value to an external database and the other one reading it again.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc add: Location of .gitignore File",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-location-of-gitignore-file\/1075",
        "Question_created_time":1645453082147,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":439,
        "Question_body":"<p>During <code>dvc add<\/code> the .gitingnore file is always created at the targets location (with <code>--file<\/code> the location of the .dvc file can be set but this does not seem to affect the location of the .gitignore file which is created or added to).<\/p>\n<p>This is not always desired, for example one could want a single .gitignore file in a projects root directory.<\/p>\n<ol>\n<li>\n<p>Did i miss something and there is a way to set the .gitignore file location?<\/p>\n<\/li>\n<li>\n<p>Wouldn\u2019t it be more convenient that the .gitignore location is always the same as the .dvc location (when using the <code>--file<\/code> argument)?<\/p>\n<\/li>\n<\/ol>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Right way to provide optional parameters to script in experiments",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/right-way-to-provide-optional-parameters-to-script-in-experiments\/773",
        "Question_created_time":1622567503315,
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":922,
        "Question_body":"<p>Hi! What is a supposed way to deal with optional params for scripts in dvc.yaml?<br>\nLet\u2019s suppose we have a script which could be run like <code>python train.py<\/code> or <code>python train.py --resume path-to-model-weights<\/code>.<\/p>\n<p>I can come up to something like this:<\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nstages:\n  train:\n    deps:\n      - train.py\n    cmd: python train.py ${resume}\n<\/code><\/pre>\n<pre><code class=\"lang-auto\"># params.yaml\nresume: \"\"\n<\/code><\/pre>\n<p>and in case I want to run an experiment and resume training, use <code>dvc exp run -S resume=\"--resume path-to-model-weights\"<\/code><\/p>\n<p>But maybe I\u2019ve missed more elegant solution? Something that will allow <code>dvc exp run -S resume=path-to-model-weights<\/code>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to only pull\/get a subfolder from a existing repo",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-only-pull-get-a-subfolder-from-a-existing-repo\/1071",
        "Question_created_time":1645141965445,
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":837,
        "Question_body":"<p>Hi,<\/p>\n<p>I am very new to dvc and I am in the process of setting up all existing datasets that we have using dvc, so we can all share the data efficiently in a meaningful way.<\/p>\n<p>The dvc has been setup successfully using network drives as data storage. I have created a repo, which contains several smaller datasets. Each dataset is placed in a subfolder within a directory, which has been pushed to the save location on the network drive using dvc push as one repo.<\/p>\n<p>My question is, is it possible to only pull one dataset (one subfolder) within this repo? Later we will be applying different transformation\/data augmentation techniques on these datasets to create new datasets. If we can\u2019t pull subdirectories within the same repo, does this mean we need to create new repos instead?<\/p>\n<p>Thanks,<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC for analytics pipeline with runtime parameters and variable dependencies",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-for-analytics-pipeline-with-runtime-parameters-and-variable-dependencies\/1065",
        "Question_created_time":1645031335208,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":476,
        "Question_body":"<p>Hi all! I\u2019m quite new to DVC and still in the process of fully understanding its capabilities and benefits. Similar to the discussion <a href=\"https:\/\/discuss.dvc.org\/t\/need-to-build-non-ml-data-pipeline-is-dvc-good-fit\/849\" class=\"inline-onebox\">Need to build non-ML data pipeline, is DVC good fit?<\/a>, I am still struggling with finding out whether DVC is a good fit for our data processing workflow or not.<\/p>\n<p>Obviously, DVC suits perfectly for straightforward ML projects, where inputs are data and a set of hyper-parameters and the output is a trained model that can then be deployed somewhere. However, things seem to be a bit more complex for different types of use cases.<\/p>\n<h2>\n<a name=\"use-case-example-pipeline-1\" class=\"anchor\" href=\"#use-case-example-pipeline-1\"><\/a>Use Case + Example Pipeline<\/h2>\n<p>Consider this example.<\/p>\n<p>We\u2019re building a data processing pipeline to analyze geo- and time-referenced images and also want to deploy it to a production server, where it then gets triggered on-demand or on a regular basis.<\/p>\n<p>On a high level, input to that process are images plus some additional meta data. Output is the processing result, e.g. in form of a small snippet of JSON data.<\/p>\n<p>The pipeline would approximately consist of these steps:<\/p>\n<ol>\n<li>\n<strong>Download data:<\/strong> Given two parameters (<code>location<\/code> (an <a href=\"https:\/\/en.wikipedia.org\/wiki\/Military_Grid_Reference_System\" rel=\"noopener nofollow ugc\">MGRS<\/a> code) and <code>dates<\/code> (as comma-separated list)), downloads all images for that location and day from a web service and saves them them to a network share in a file structure like this:\n<ul>\n<li>\n<code>image_data<\/code>\n<ul>\n<li>\n<code>4QFJ<\/code> (the location code)\n<ul>\n<li>\n<code>2022<\/code>\n<ul>\n<li>\n<code>02<\/code>\n<ul>\n<li>\n<code>16<\/code>\n<ul>\n<li><code>image.jpg<\/code><\/li>\n<li>\n<code>...<\/code> (some auxiliary data, skipped for brevity)<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<strong>Filter data:<\/strong> Given a parameter <code>filtering_method<\/code>, it filters such of these images, that satisfy certain criteria. The result could, for instance, be a JSON list of image paths. The method parameter would probably also go inside <code>params.yaml<\/code>, I presume.<\/li>\n<li>\n<strong>Process data:<\/strong> Takes the set of valid images from the previous step, loads them from the network drive (see first step), analyzes and processes them. Depending on the <code>filtering_method<\/code> this list is longer or shorter. In reality, this step is actually multiple separate steps, each of which has in- and outputs itself, but for simplicity that part is skipped. Output is a JSON file that is then saved to the network drive again, e.g. at <code>results\/4QFJ_2022-02-16.json<\/code>.<\/li>\n<\/ol>\n<h2>\n<a name=\"issues-2\" class=\"anchor\" href=\"#issues-2\"><\/a>Issues<\/h2>\n<p>As you can see, this is quite different from a classical ML <em>preprocess - train - evaluate<\/em> flow. Specifically, what makes me doubt if DVC is easily applicable here is:<\/p>\n<ol>\n<li>The pipeline depends on <strong>\u201cruntime\u201d parameters<\/strong> (<code>location<\/code>, <code>date<\/code>) as input, that likely change for each run and are passed by the user (or an automated script, rather). How to model these? Update <code>params.yaml<\/code> each time?\n<ul>\n<li>Additional question for general understanding: day <code>params.yaml<\/code> contained only one key, say <code>foo<\/code>. If I run the pipeline with <code>foo: 4<\/code>, then with <code>foo: 5<\/code> and once again with <code>foo: 4<\/code>, would the last run be skipped, because it had earlier run with the same set of dependencies already?<\/li>\n<\/ul>\n<\/li>\n<li>The files to be used as <strong>dependencies are not a static set<\/strong>, but rather vary, depending on input parameters. It\u2019s not always <code>data\/train.csv<\/code> or so, but instead the set of dependencies (the filenames) to stage 2 vary with varying input parameters to step 1. How to help with this? Maybe using template variables in <code>dvc.yaml<\/code>?<\/li>\n<li>Preferably, we\u2019d also like to send a notification for each pipeline run by calling an online REST API. In other words, some of the pipeline steps have <strong>side effects<\/strong>, that can\u2019t really be tracked with files. This is probably also a point where DVC falls short for us, because, if I ran the pipeline twice with same parameters and data, only one notification would be sent. However, this point is not too crucial.<\/li>\n<\/ol>\n<p>What do you think, is DVC a good fit for us and if yes, how do I work around these \u201cissues\u201d?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to continually update a model with new data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-continually-update-a-model-with-new-data\/1055",
        "Question_created_time":1644516069762,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":205,
        "Question_body":"<p>I have a reinforcement learning use case with the following initial steps:<\/p>\n<ol>\n<li>Construct initial episodes<\/li>\n<li>Add episodes to initial experience replay buffer<\/li>\n<li>Train initial model<\/li>\n<\/ol>\n<p>Then the following steps which repeat every day:<\/p>\n<ol start=\"4\">\n<li>Construct most recent time steps<\/li>\n<li>Add new time steps to existing experience replay buffer<\/li>\n<li>Fine-tune existing model<\/li>\n<li>Compare existing model and fine-tuned model and keep the one that\u2019s best<\/li>\n<\/ol>\n<p>What would be the best way to do this using DVC? Is it even possible given that circular dependencies are not allowed?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc pull error from Google Cloud",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-pull-error-from-google-cloud\/1056",
        "Question_created_time":1644549708643,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":193,
        "Question_body":"<p>I\u2019m not able to pull from a virtual machine running Ubuntu in GCP but it does work in Amazon EC2. Could I be missing an outbound network rule or something? Logs below, thanks!<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull -v\n2022-02-11 03:16:56,723 DEBUG: Adding '\/home\/koz\/test\/.dvc\/config.local' to gitignore file.\n2022-02-11 03:16:56,725 DEBUG: Adding '\/home\/koz\/test\/.dvc\/tmp' to gitignore file.\n2022-02-11 03:16:56,726 DEBUG: Adding '\/home\/koz\/test\/.dvc\/cache' to gitignore file.\n2022-02-11 03:16:56,756 DEBUG: failed to pull cache for 'data'        \n2022-02-11 03:16:56,775 WARNING: No file hash info found for '\/home\/koz\/test\/data'. It won't be created.                                                        \n1 file failed                                                                                                                                                   \n2022-02-11 03:16:56,776 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/home\/koz\/test\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/koz\/venv\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py\", line 30, in run\n    stats = self.repo.pull(\n  File \"\/home\/koz\/venv\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/koz\/venv\/lib\/python3.8\/site-packages\/dvc\/repo\/pull.py\", line 40, in pull\n    stats = self.checkout(\n  File \"\/home\/koz\/venv\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/koz\/venv\/lib\/python3.8\/site-packages\/dvc\/repo\/checkout.py\", line 110, in checkout\n    raise CheckoutError(stats[\"failed\"], stats)\ndvc.exceptions.CheckoutError: Checkout failed for following targets:\n\/home\/koz\/test\/data\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n------------------------------------------------------------\n2022-02-11 03:16:56,782 DEBUG: Analytics is enabled.\n2022-02-11 03:16:56,812 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpnugg1s2n']'\n2022-02-11 03:16:56,813 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpnugg1s2n']'\n(venv) koz@ubuntu-01:~\/test$ dvc --version\n2.9.3\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - 'retries'",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-retries\/1051",
        "Question_created_time":1644477401580,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":380,
        "Question_body":"<p>Hi, i run dvc from inside a docker container ,i call dvc  commands from python script , inside the container ,everything is configured ,repo ,remote storage and remote credentials, when i run the script without the container it works perfectly ,but when using the container it raise this error<br>\nwhen it comes to \u201cdvc push\u201d in the script \u201cERROR: unexpected error - \u2018retries\u2019\u201d ,the error obviously not informative ,so please need a help for that<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best Practice - Test pipeline with smaller dataset?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-test-pipeline-with-smaller-dataset\/1047",
        "Question_created_time":1644331496554,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":217,
        "Question_body":"<p>I currently have a DVC pipeline that takes about 12 hours to run. I would like a way to test the entire pipeline on a small subset of my data, so that I can quickly verify each stage after a code change. Ideally, this would be an option on <code>dvc repro<\/code>. This leads me to think that I can configure my pipeline to have separate \u201ctest\u201d versions of each stage, using the same cmds as their full counterparts, but using different parameters to reduce the time to reproduce.<\/p>\n<p>Is this the correct approach? Is there a cleaner way to do this? How have other people solved this problem?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Check if files are in same versions in local and remote without doing dvc pull",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/check-if-files-are-in-same-versions-in-local-and-remote-without-doing-dvc-pull\/1045",
        "Question_created_time":1644317539862,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":266,
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to kwow if there is a way to know if files are not up to date on my github repo (with the .dvc files) compare to dvc storage without doing a dvc pull and dvc repro. In other words, how can I check if nobody forget to do a dvc push before git push.<\/p>\n<p>I can\u2019t use dvc pull to check that because the check is doing in a github action and files are too large to be pull on it. And I don\u2019t want to depends on some hooks because it implies to be sure that everybody install it.<\/p>\n<p>I have the intuition that it coulb be possible to check the md5 stock in .dvc files are the same that in remote without pulling them but I don\u2019t find the solution.<\/p>\n<p>Thank you in advance for your help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Not keeping history of certain files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/not-keeping-history-of-certain-files\/1033",
        "Question_created_time":1643803048280,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":166,
        "Question_body":"<p>Hi, I\u2019m wondering if there is a way to have a file be managed by dvc, but flag it somehow so that a history of that file isnt kept. I\u2019m thinking specifically about a data registry scenario, normally if i change a file and push the new version to the registry, the old versions are also stored. In my hypothetical scenario i\u2019d like to somehow configure that registry so that certain files can still be pushed\/pulled from the registry but versions aren\u2019t kept, so if i make a change and push, the old version just gets overwritten. Is that possible? Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Need help to connect to COS through DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/need-help-to-connect-to-cos-through-dvc\/1027",
        "Question_created_time":1642433993771,
        "Question_answer_count":10,
        "Question_score_count":0,
        "Question_view_count":436,
        "Question_body":"<p>I\u2019m unable to connect to COS using DVC and him getting these errors.<br>\n\u201cERROR: unexpected error - The config profile (***********<span class=\"mention\">@in.ibm.com<\/span>) could not be found\u201d and my config file has Here is my .dvc\/config<\/p>\n<p>[core]<br>\nremote = s3remote<br>\n[\u2018remote \u201cstorage\u201d\u2019]<br>\nurl = gdrive:\/\/***************************************<br>\n[\u2018remote \u201cs3remote\u201d\u2019]<br>\nurl = s3:\/\/mystore\/path<br>\nendpointurl = https:\/\/***************.cloud<br>\nregion = ***geo<br>\nprofile = ***********************<span class=\"mention\">@in.ibm.com<\/span><br>\nread_timeout = 300<br>\nconnect_timeout = 300<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using DVC outside git",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-outside-git\/1014",
        "Question_created_time":1640005495644,
        "Question_answer_count":10,
        "Question_score_count":0,
        "Question_view_count":484,
        "Question_body":"<p>Hello here,<br>\nI am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering  the data folder is not a git\/dvc working directory?,  such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: unexpected error - unable to open database file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-unable-to-open-database-file\/1015",
        "Question_created_time":1640012114896,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":831,
        "Question_body":"<p>Hi,<br>\nI am using dvc exp to run different experiment from my pipeline. However when I try to apply the experiment I have the following error: <code>ERROR: unexpected error - unable to open database file <\/code>.<\/p>\n<p>I will describe all the steps that I made:<\/p>\n<ol>\n<li><code>dvc init --subdir<\/code><\/li>\n<li><code>dvc repro<\/code><\/li>\n<li>\n<code>dvc exp run<\/code> \u2026<\/li>\n<li><code>dvc exp push origin experiment_name<\/code><\/li>\n<li>Then I made some commits (with other work)<\/li>\n<li>Then I tried to do <code>dvc exp apply experiment_name<\/code>\n<\/li>\n<li>I get the error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<li>I did not know that I needed to be in the same project version as when the experiment was run. So I made reset for the commit where I run the experiences<\/li>\n<li><code>dvc exp apply experiment_name<\/code><\/li>\n<li>I got the same error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<li><code>dvc exp pull origin experiment_name<\/code><\/li>\n<li>I got the same error: <code>ERROR: unexpected error - unable to open database file<\/code>\n<\/li>\n<\/ol>\n<p>Can anyone help me to understant how can I resolve this issue?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Switching between virtual Python environments within `cmd`",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/switching-between-virtual-python-environments-within-cmd\/1004",
        "Question_created_time":1639444412795,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":298,
        "Question_body":"<p>Hello,<\/p>\n<p>We are using Poetry to manage Python environments. It is generally possible to invoke a python script B to run in a given Poetry environment from within python program A. I have so far done this for instance via issuing shell commands from within a Python program. Like This:<\/p>\n<p>Given this structure with two differnt envs:<\/p>\n<p>folder_a<\/p>\n<ul>\n<li>prog_a.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>folder_b<\/p>\n<ul>\n<li>prog_b.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\"># within prog_a.py\nimport subprocess\ncmd = \"cd folder_b; poetry run python prog_b.py\"\nsubprocess.run(cmd, shell=True, check=True)                  \n<\/code><\/pre>\n<p>This worked without problem.<\/p>\n<p>Now, when I try the same from DVC it unfortunately does not work. Say I now have the following setup:<\/p>\n<p>folder_a<\/p>\n<ul>\n<li>dvc.yaml<\/li>\n<li>pyproject.toml   # has DVC installed<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>folder_b<\/p>\n<ul>\n<li>prog_b.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>If I specify a <code>cmd<\/code> in a <code>dvc.yaml<\/code> like this: <code>cmd : \"cd abs\/path\/to\/folder_b; poetry run python prog_b.py\"<\/code>, then upon <code>dvc repro ...<\/code> I expected prog_b to be run in the Poetry env from folder_b. Instead it is run in the env from folder_a (where DVC is installed).<\/p>\n<p>There seems to be no way to escape this. I have even tried instead calling a wrapper through <code>cmd<\/code> that does the exact same subprocess call as in the first example (meaning it gets the full rest of the command as string, including the cd\u2026). Still, no luck, the env stays the same.<\/p>\n<p>Does anyone have any clue why this is the case? How does DVC execute <code>cmd<\/code>? In some special way that prevents running a process in a new env? I have tried a lot of variants of DVC \/ Poetry configs, to no avail.<\/p>\n<p>Alternatively I appreciate suggestions how else to deal with the need to execute different stages in different environments.<\/p>\n<p>Thanks lots and lots. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Best<\/p>\n<p>Jonas<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Configure: DVC + CircleCI + GDRIVE",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/configure-dvc-circleci-gdrive\/1011",
        "Question_created_time":1639931189346,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":388,
        "Question_body":"<p>Trying to configure DVC to pull gdrive stored artifacts from a CircleCI build? I followed instructions from <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote\" rel=\"noopener nofollow ugc\">docs<\/a>, hints in this <a href=\"https:\/\/discuss.dvc.org\/t\/cml-github-actions-google-drive-service-account\/795\">discussion-question<\/a> &amp; this <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\" rel=\"noopener nofollow ugc\">git-hub issue<\/a> but there seems to be something missing.<\/p>\n<p>Here is the excerpt of the circle-ci configuration being used:<\/p>\n<pre><code class=\"lang-auto\">- run:\n     name: Data checkout\n     command: |\n        dvc remote modify storage --local gdrive_use_service_account true;\n        dvc remote modify storage --local gdrive_service_account_json_file_path \/dev\/null;\n        dvc remote modify storage --local gdrive_service_account_user_email &lt;my@service.iam.gserviceaccount.com&gt;;\n        dvc pull --recursive -r storage --verbose;\n     environment:\n        GDRIVE_CREDENTIALS_DATA: $GDRIVE_CREDENTIALS_DATA\n<\/code><\/pre>\n<p>The <code>$GDRIVE_CREDENTIALS_DATA<\/code> environment variable was configured in Circle-CI server to be that of my Google\u2019s Service Account JSON to access my google drive. But the CircleCI runner reports this error as if the environment variable did not get read.<\/p>\n<pre><code class=\"lang-auto\">0% 0\/1 [00:00&lt;?, ?file\/s{'info': ''}]\nGo to the following link in your browser:\n\n    https:\/\/accounts.google.com\/o\/oauth2\/auth?client_id=710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.appdata&amp;access_type=offline&amp;response_type=code&amp;approval_prompt=force\n\nEverything is up to date.              \nERROR: failed to pull data from the cloud - GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'.\nBackup first, remove or fix them, and run DVC again.\nIt should do auth again and refresh the credentials.\n<\/code><\/pre>\n<p>Things I am unsure about.<\/p>\n<ol>\n<li>Is the <code>GDRIVE_CREDENTIALS_DATA<\/code> environment variable the right name DVC commands looks for?<\/li>\n<li>Which JSON content should this environment variable have?\n<ul>\n<li>\n<code>.dvc\/tmp\/gdrive-user-credentials.json<\/code> ?<\/li>\n<li>Google Service account JSON looking like this?<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">{\n  \"type\": \"service_account\",\n  \"project_id\": \"&lt;my_project&gt;\",\n  \"private_key_id\": \"&lt;key_id&gt;\",\n  \"private_key\": \"&lt;key_hash&gt;\",\n  \"client_email\": \"&lt;my@service.iam.gserviceaccount.com&gt;\",\n  \"client_id\": \"&lt;client_id&gt;\",\n  \"auth_uri\": \"https:\/\/accounts.google.com\/o\/oauth2\/auth\",\n  \"token_uri\": \"https:\/\/oauth2.googleapis.com\/token\",\n  \"auth_provider_x509_cert_url\": \"https:\/\/www.googleapis.com\/oauth2\/v1\/certs\",\n}\n<\/code><\/pre>\n<ul>\n<li>Or Google client secret JSON looking like this?<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">{\n  \"installed\": {\n    \"client_id\": \"&lt;google_client_id&gt;\",\n    \"project_id\": \"&lt;google_project_id&gt;\",\n    \"auth_uri\": \"https:\/\/accounts.google.com\/o\/oauth2\/auth\",\n    \"token_uri\": \"https:\/\/oauth2.googleapis.com\/token\",\n    \"auth_provider_x509_cert_url\": \"https:\/\/www.googleapis.com\/oauth2\/v1\/certs\",\n    \"client_secret\": \"&lt;client_secret&gt;\",\n    \"redirect_uris\": [\n      \"urn:ietf:wg:oauth:2.0:oob\",\n      \"http:\/\/localhost\"\n    ]\n  }\n}\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Is the 1st time google browser authentication step still required for CI machines?<\/li>\n<\/ol>\n<p>Many thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Same remote for multiple repos",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/same-remote-for-multiple-repos\/75",
        "Question_created_time":1534853808824,
        "Question_answer_count":6,
        "Question_score_count":3,
        "Question_view_count":1493,
        "Question_body":"<p>Hi<\/p>\n<p>I was wondering how the remote mechanisms work.<br>\nAssuming I have two different DVC repos, that may use the same base dataset.<br>\nCan I use the same remote for both of them, so the dataset is not stored twice?<\/p>\n<p>This is strictly speaking not possible because of hashing collisions, is it?<\/p>\n<p>Regards<br>\nMatthias<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Adjusting absolute paths in stage command w\/o triggering re-run",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adjusting-absolute-paths-in-stage-command-w-o-triggering-re-run\/999",
        "Question_created_time":1639271248863,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":361,
        "Question_body":"<p>Hello,<\/p>\n<p>first of all, thanks for this awesome project! We are very likely to use it as the backbone of our data versioning infrastructure.<\/p>\n<p>One question came up in the process. I am building a template for pipelines that combine various existing software components or scripts. These scripts are called by pipeline stages (via <code>cmd<\/code>) and take various command line arguments. These specify the paths to input and output files ingested\/created by the scripts. So far so normal. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=10\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>\n<p>The issue is that our scripts need absolute paths. They have no way of resolving relative paths and more importantly the relation of the script path to the data path may not always be the same when the repo is used.<\/p>\n<p>For instance say some input data is stored in <code>my_dvc_repo\/data\/input.csv<\/code>. Then the command for the stage may look like <code>python path\/to\/script\/my_script.py --input my_dvc_repo\/data\/input.csv<\/code>. Another user might clone to <code>repos\/my_dvc_repo\/data\/input.csv<\/code>. Then the command for the stage must include this different path <code>python path\/to\/script\/my_script.py --input repos\/my_dvc_repo\/data\/input.csv<\/code>.<\/p>\n<p>We can deal with that of course. A shell script could modify <code>dvc.yaml<\/code> after each clone and adjust the <code>cmd<\/code> string to include the correct absolute paths.<\/p>\n<p><strong>The blocker seems to be that the stage\u2019s <code>cmd<\/code> is tracked in <code>dvc.lock<\/code>.<\/strong> Modifying it appears to count as a change of the stage, meaning it will have to be rerun even though all we changed was the absolute location of the input\/output files, not their content and not their relation to the rest of the DVC files.<\/p>\n<p>I have come up with various ways how this as well can be solved, but they appear quite hacky*.<\/p>\n<p>So I would just like to sanity check whether I am missing some obvious way already available in DVC. For instance I know about <code>dvc root<\/code> but don\u2019t see how it can be employed in the <code>cmd<\/code> multiple times without making the resulting command string overly complex (thinking about combining <code>realpath<\/code> with <code>dvc root<\/code> each time the absolute repo path is needed).<\/p>\n<p>Appreciating any help! Thanks a lot!<\/p>\n<p>Best regards<\/p>\n<p>Jonas<\/p>\n<p>(*) My current plan is to write a wrapper script which by convention needs to be called first in each <code>cmd<\/code>. It would essentially get the rest of the command as string arguments. Paths could use a placeholder for the absolute path (e.g. <code>ROOT<\/code>) and the wrapper can then replace and call the actual script. E.g. <code>wrapper python my_script --input ROOT\/data\/input.csv<\/code>. This way the <code>cmd<\/code> itself would remain unchanged regardless the absolute location of the repo. Another thought was to use <code>vars<\/code> in <code>dvc.yaml<\/code> and adjust these to the absolute paths programmatically upon cloning, but I do not think these can be used <em>within<\/em> the <code>cmd<\/code> string.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error on trying to run dvc pipeline",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-on-trying-to-run-dvc-pipeline\/989",
        "Question_created_time":1638161185366,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":423,
        "Question_body":"<p>I\u2019m trying to run the training stage of a machine learning pipeline on dvc with the following command(s):<\/p>\n<pre><code class=\"lang-auto\">dvc run -n train \\ \n-d data\/census_clean.csv -d starter\/train_model.py \\ \n-o model\/model.joblib -o model\/encoder.joblib -o model\/lb.joblib \\\npython starter\/train_model.py\n\n<\/code><\/pre>\n<p>But this throws up the following error everytime and the run fails to start.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/06e6383b7e3e4f59a41f78bd245925f207064081.png\" alt=\"Screenshot 2021-11-29 at 10.13.50 AM\" data-base62-sha1=\"Z26HMFZc92veBXN62ytKqjdtTP\" width=\"474\" height=\"142\"><\/p>\n<p>I\u2019m not sure where the mistake on my part is - the same error repeats if I try to run without the output commands, and not specifying the params since hyerparameter tuning is not required at this stage. Please let me know what I\u2019m missing.<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can't \"dvc pull\"",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cant-dvc-pull\/991",
        "Question_created_time":1638181467849,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":557,
        "Question_body":"<p>Hello, I\u2019ve tried to use dvc with an existing repository. When I do a \u201cdvc pull\u201d, I get the following error:<\/p>\n<p>Authentication successful.<\/p>\n<p>ERROR: unexpected error - : &lt;HttpError 404 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files\/1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B?fields=driveId&amp;supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files\/1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B?fields=driveId&amp;supportsAllDrives=true&amp;alt=json<\/a> returned \u201cFile not found: 1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B\u201d. Details: \"[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018notFound\u2019, \u2018message\u2019: \u2018File not found: 1Hn9uNSx3An6bDVe5Di_ki-AbAAbB9G0B\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018file\u2019}]\u201d&gt;<\/p>\n<p>Running \u201cdvc doctor\u201d returns the following message:<\/p>\n<h2>\n<a name=\"dvc-version-282-osxpkg-1\" class=\"anchor\" href=\"#dvc-version-282-osxpkg-1\"><\/a>DVC version: 2.8.2 (osxpkg)<\/h2>\n<p>Platform: Python 3.8.12 on macOS-10.16-x86_64-i386-64bit<br>\nSupports:<br>\nazure (adlfs = 2021.9.1, knack = 0.8.2, azure-identity = 1.7.0),<br>\ngdrive (pydrive2 = 1.10.0),<br>\ngs (gcsfs = 2021.10.1),<br>\nhdfs (fsspec = 2021.10.1, pyarrow = 5.0.0),<br>\nwebhdfs (fsspec = 2021.10.1),<br>\nhttp (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.6),<br>\nhttps (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.6),<br>\ns3 (s3fs = 2021.10.1, boto3 = 1.17.106),<br>\nssh (sshfs = 2021.9.0),<br>\noss (ossfs = 2021.8.0),<br>\nwebdav (webdav4 = 0.9.3),<br>\nwebdavs (webdav4 = 0.9.3)<\/p>\n<p>Could anyone help me dealing with this? Much appreciated!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Concept and data drift",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/concept-and-data-drift\/986",
        "Question_created_time":1637792304507,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":289,
        "Question_body":"<p>Hello<br>\nI am new-ish to DVC and still getting orientated. Please could I ask - are there any specific tools or processes within DVC Studio for monitoring concept and data drift? Or any plans in that area?<br>\nThanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC and FDA compliance",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-fda-compliance\/983",
        "Question_created_time":1637738029985,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":281,
        "Question_body":"<p>Hi,<\/p>\n<p>Does DVC falls under FDA regulatory system?<\/p>\n<p>We\u2019re in the phase of developing an SaMD which requires approvals by FDA, for all tools that are used in developing this device.<\/p>\n<p>Knowing us of any referrals\/suggestions is greatly appreciated.<\/p>\n<p>Thanks in Advance!!!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc fails due to outs stage entry",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-fails-due-to-outs-stage-entry\/975",
        "Question_created_time":1637252513725,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":439,
        "Question_body":"<p>Hi,<br>\nI cannot run dvc due to the following error:<br>\nERROR: failed to reproduce \u2018dvc.yaml\u2019: output \u2018\u2026\/data-registry\/forecast\/models\u2019 does not exist<\/p>\n<p>I already tried different paths, relative, absolute and using direclty the paths instaed of using it by params var. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/confused.png?v=10\" title=\":confused:\" class=\"emoji\" alt=\":confused:\"><br>\nThe weird thing is that this already worked. And suddently stops working.<br>\nI already use dvc destroy to clean everthing and I starded again and the problems continues <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/confused.png?v=10\" title=\":confused:\" class=\"emoji\" alt=\":confused:\"><\/p>\n<p>Can someone help me?<br>\nthanks<\/p>\n<p>My dvc.yaml is:<\/p>\n<pre><code class=\"lang-auto\">vars:\n  - db_config_file: \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/config\/config.yaml\n  - params_file: \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/params.yaml\n\nstages:\n  train:\n    wdir: ..\n    cmd: python -m forecasting_module.pipeline train ${db_config_file} ${params_file}\n    params:\n      - ${params_file}:\n        - common.paths.models\n        - common.columns.id\n        - common.columns.date\n        - common.columns.value\n        - prophet.columns.prophet_date\n        - prophet.columns.prophet_value\n        - prophet.train_parameters.seasonality_mode\n    outs:\n      - ${common.paths.models}\n<\/code><\/pre>\n<p>The debug outputs is:<\/p>\n<pre><code class=\"lang-auto\">2021-11-18 16:14:12,120 DEBUG: Dependency 'params.yaml' of stage: 'train' changed because it is '{'common.paths.models': 'new', 'common.columns.id': 'new', 'common.columns.date': 'new', 'common.columns.value': 'new', 'prophet.columns.prophet_date': 'new', 'prophet.columns.prophet_value': 'new', 'prophet.train_parameters.seasonality_mode': 'new'}'.\n2021-11-18 16:14:12,121 DEBUG: stage: 'train' changed.\n2021-11-18 16:14:12,122 DEBUG: Removing output '..\/data-registry\/forecast\/models' of stage: 'train'.\n2021-11-18 16:14:12,122 DEBUG: Removing '..\/data-registry\/forecast\/models'\n2021-11-18 16:14:12,129 DEBUG: defaultdict(&lt;class 'dict'&gt;, {'params.yaml': {'common.paths.models': 'new', 'common.columns.id': 'new', 'common.columns.date': 'new', 'common.columns.value': 'new', 'prophet.columns.prophet_date': 'new', 'prophet.columns.prophet_value': 'new', 'prophet.train_parameters.seasonality_mode': 'new'}})\nRunning stage 'train':\n&gt; python -m forecasting_module.pipeline train \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/config\/config.yaml \/home\/dsmendes\/Documents\/implementation\/forecasting_module\/params.yaml\n2021-11-18 16:14:12,937 ERROR: failed to reproduce 'dvc.yaml': output '..\/data-registry\/forecast\/models' does not exist\n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error with dvc import",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-with-dvc-import\/755",
        "Question_created_time":1621628598446,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":1659,
        "Question_body":"<p>Hi,<\/p>\n<p>My team and I are evaluating dvc for use within our projects. All of our data and code are stored on a NAS which is mounted to several different machines. We are currently testing with two repositories located at:<br>\n<code>\/nas\/projects\/project1\/code\/user1\/repo1<\/code><br>\n<code>\/nas\/projects\/project1\/code\/user2\/repo2<\/code><\/p>\n<p>And the shared cache is located at:<br>\n<code>\/nas\/data\/project1\/dvc<\/code><\/p>\n<p>Repo1 currently has two files being tracked by dvc:<br>\n<code>data\/test\/file1.pq.gz<\/code><br>\n<code>data\/test\/file2.pq.gz<\/code><\/p>\n<p>We are trying to import these files from repo1 to repo2. From user2\/repo2\/data\/test, we are running:<br>\n<code>dvc import \/nas\/projects\/project1\/code\/user1\/repo1 data\/test\/file1.pq.gz<\/code><\/p>\n<p>And we are receiving the following error:<br>\n<code>ERROR: unexpected error - [Errno 2] No such file or directory: PosixPathInfo: '..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/tmp\/tmptxvv7xk0dvc-clone\/data\/test\/file1.pq.gz'<\/code><\/p>\n<p>We have tried this from multiple machines with both absolute and relative paths to the project. We have tried with DVC installed both in venv and with conda. We receive the same error, but sometimes the number of \u201c\u2026\/\u201d in the path changes between absolute and relative paths. The kicker is that I ran into this issue the other day and, off a hunch, decided to try it on another machine. It worked, but then going back to the original machine, the command ran for file2 without issue. Then, today, we started receiving the error again while testing.<\/p>\n<p>I don\u2019t know if this is a bug, a configuration issue, or some other error on our part, so any thoughts or insight into potential issues are greatly appreciated.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to retrieve output files from dvc pipeline",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-retrieve-output-files-from-dvc-pipeline\/948",
        "Question_created_time":1635890105406,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":284,
        "Question_body":"<p>I have created a pipeline and run it successfully, creating output files as per the pipeline I specified. I might have done something wrong, but when I try and <code>dvc get<\/code> this output file I get an error<br>\n<code>Unable to find DVC file with output<\/code><\/p>\n<p>My aim is to be able to get the model output file created via dvc pipeline into another project. Should I approach it differently?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error \"Unable to find DVC file with output\"",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unable-to-find-dvc-file-with-output\/942",
        "Question_created_time":1635579910534,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":235,
        "Question_body":"<p>I successfully added a directory with <code>dvc add data<\/code>. The directory contains a subdirectory <code>A<\/code>. Now I want to rename the directory with <code>dvc move data\/A data\/B<\/code> but this gives me the error: <code>ERROR: failed to move 'A -&gt; 'B' - Unable to find DVC file with output A'<\/code>. What am I missing here?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Output with timestamp in it's name",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/output-with-timestamp-in-its-name\/940",
        "Question_created_time":1635505180672,
        "Question_answer_count":5,
        "Question_score_count":3,
        "Question_view_count":240,
        "Question_body":"<p>Hello everyone,<\/p>\n<p>When running <code>dvc run<\/code> Is there any way to precise an output with an unknown name?<\/p>\n<p>Here is my situation: I have a step named <code>train_model<\/code> which will create a folder named in this format <code>models\/{timestamp}\/model.py<\/code> and another stage <code>optimize_model<\/code> which will use this output to create <code>models\/*\/model_freezed_jit.pt<\/code> where <code>*<\/code> is the timestamp.<\/p>\n<p>Because I can\u2019t precise the specific folder name, dvc can\u2019t find the output : <code>failed to reproduce 'dvc.yaml': output 'models\/*\/model.pt' does not exist<\/code>.<\/p>\n<p>Is there any way to do that?<\/p>\n<pre><code class=\"lang-auto\">stages:\n  train_model:\n    cmd: python src\/train_model.py\n    deps:\n    - data\/formated\/\n    - src\/train_model.py\n    outs:\n    - models\/*\/model.pt:\n        cache: false\n        persist: true\n  optimize_model:\n    cmd: python src\/model_optimizer.py\n    deps:\n    - models\/*\/model.pt\n    outs:\n    - models\/*\/model_freezed_jit.pt:\n        cache: false\n        persist: true\n    - models\/*\/half_model_freezed_jit.pt:\n        cache: false\n        persist: true\n    - models\/*\/quant_model_freezed_jit.pt:\n        cache: false\n        persist: true\n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does DVC duplicate dataset in storage blobs when the same dataset is used in different tags?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-duplicate-dataset-in-storage-blobs-when-the-same-dataset-is-used-in-different-tags\/938",
        "Question_created_time":1635503758715,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":225,
        "Question_body":"<p>Hi,<\/p>\n<p>Suppose I have a dataset of 5 GB (images and jsons). I version this dataset using DVC and give a tag name dataset_1. I push the dataset to S3.<\/p>\n<p>Now I add a new dataset of 7GB (images and jsons) to the old dataset, take a tag out called dataset_2 and push to the same S3 bucket. So dataset_2 tag contains the old 5GB and the new 7GB data.<\/p>\n<p>How much storage space is being taken by S3. Will it be 12GB (5 +7) or 15 GB (5 + (5 + 7))?<br>\nIn other words, is the older dataset being duplicated in S3?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Lock error with parallelized dvc repro",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929",
        "Question_created_time":1634713211772,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":318,
        "Question_body":"<p>I have a pipeline with many parallel stages. I use a combination of a (generated) <code>params.yaml<\/code> file and <code>foreach<\/code> approach. See below for simplified examples of <code>dvc.yaml<\/code>, <code>params.yaml<\/code>, and <code>dvc.lock<\/code>.<\/p>\n<p>I wrote a simple scheduler in Python to run the stages with <code>dvc repro<\/code> in parallel using multiprocessing. Essentially, it\u2019s running running <code>dvc repro setup@sim1<\/code>, <code>dvc repro setup@sim2<\/code>, etc. in parallel.<\/p>\n<p>From what I <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#parallel-stage-execution\" rel=\"noopener nofollow ugc\">understand<\/a> this should work. However, I\u2019m getting lock errors: \u201cERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly.\u201d<\/p>\n<p>dvc.yaml:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  setup:\n    foreach: ${sims}\n    do:\n      cmd: python my_script.py --simlabel ${item.label}\n      deps:\n        - my_script.py\n      outs:\n        - ${item.work_dir}\/outputs.txt\n<\/code><\/pre>\n<p>params.yaml<\/p>\n<pre><code class=\"lang-auto\">sims:\n\tsim1:\n\t\tlabel: label1\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label1_workdir\n\tsim2:\n\t\tlabel: label2\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label2_workdir\n<\/code><\/pre>\n<p>dvc.lock:<\/p>\n<pre><code class=\"lang-auto\">schema: '2.0'\nstages:\n  setup@sim1\n    cmd: python python my_script.py --simlabel label1\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label1_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n  setup@sim2\n    cmd: python python my_script.py --simlabel label2\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label2_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Refactor existing project from single root .dvc to subprojects (dvc init --subdir)",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/refactor-existing-project-from-single-root-dvc-to-subprojects-dvc-init-subdir\/928",
        "Question_created_time":1634692636711,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":230,
        "Question_body":"<p>I have an existing project that was initialized at the git root with a single .dvc directory.<\/p>\n<p>Are there going to be any gotchas or unexpected behavior if I want to refactor this by running \u201cdvc init --subdir\u201d in a couple of subdirectories to create subprojects with their own cache directories and remotes?  If I understand things correctly, I can:<\/p>\n<ol>\n<li>Update my cache and ensure no one will push changes to remote while the steps below are done<\/li>\n<li>Run <code>dvc init --subdir<\/code> in a subdirectory that I want to turn into a subproject<\/li>\n<li>Set cache dir to the same as the root project, set the remote to the new remote for this subproject<\/li>\n<li>Run <code>dvc push<\/code> to copy current versions of all files in the subproject to the new remote<\/li>\n<li>Change subproject cache dir to its new path<\/li>\n<li>git commit and push<\/li>\n<\/ol>\n<p>Anything I\u2019m missing?<\/p>\n<p>One confusing issue will be that dvc commands will have a different scope depending whether a developer is on a commit upstream or downstream from this change.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to set up remote storage with SSH and jump host",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-set-up-remote-storage-with-ssh-and-jump-host\/927",
        "Question_created_time":1634565937633,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":405,
        "Question_body":"<p>I am trying to set up an ssh storage on a server that can only be accessed via a jump host. While with ssh  I can use the option -j and with sftp I can use the option -o \u2018ProxyJump user@IPadress -p port_number\u2019, I do not know how to do that for dvc remote storage.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Integrate DVC to an existing github repo with S3",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/integrate-dvc-to-an-existing-github-repo-with-s3\/922",
        "Question_created_time":1634478544631,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":489,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>We are considering to start using DVC and integrate it to our current git repo, and I have some questions about what is the best practice to integrate DVC once you already have a flow you work with.<\/p>\n<p>Our current data workflow, before using DVC, is as follows:<\/p>\n<ol>\n<li>All is the is stored in a bucket in s3 under a specific folder (we can call it the \u201cresources folder\u201d). This folder is in the folder tree of the git repo but the whole resources folder is specified in the gitignore file of the git repo.<\/li>\n<li>We have a configuration yaml file in which we specify which data in the resources folder on S3 we want to use.<\/li>\n<li>The configuration yaml file is connected to the code we use to train the model.<\/li>\n<li>Once the training process starts the code first checks if data specified in the configuration yaml file is already present locally or not, and if the file is missing then the code gets the data from the resources folder in the S3 bucket to the local computer.<\/li>\n<li>model is being trained.<\/li>\n<li>Output model files (e.g.,weights file) created when the model training is finished are saved locally and then uploaded (using the code, not manually) to the resources folder in S3.<\/li>\n<\/ol>\n<ul>\n<li>Locally means it is either the local computer (if the amount of data is small) or something like an EC2 (if the amount of data is too large to train on the local computer).<\/li>\n<\/ul>\n<p><strong>My questions are:<\/strong><br>\na. We will need to exclude the whole content of the resources folder from the gitignore\u05e5<br>\nb. Then add one file at a time to dvc tracking using <code>dvc add<\/code> ?<br>\nc. Followed by adding the dvc file created from the <code>dvc add<\/code> and the raw file to gitignore, add them to git and commit.<br>\nd. Do <code>dvc push<\/code> to remote storage (that will also be in s3, but in a different bucket).<br>\ne. Once we register all the files we will no longer need the files in the resources folder from which we started right?<\/p>\n<p>Any advices on how to best integrate dvc to the current workflow and other best practices will be greatly appreciated.<\/p>\n<p>Thank you,<br>\nAyala<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc garbage collector permissions for remote SSH",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-garbage-collector-permissions-for-remote-ssh\/700",
        "Question_created_time":1615400634250,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":395,
        "Question_body":"<p>Is there a way to prevent people to run dvc gc with the \u201c\u2013cloud\u201d option ?<\/p>\n<p>To give a bit more context, we would want to put dvc in place in our company with a SSH remote storage but we realized there was no user management in DVC for now. So every people would be able to push to the remote, but also to garbage collect. For data safety  purposes, this is not something we want. To the best of our knowledge, there is also no possibility to restrict directly on the SSH storage side beause there is no way to authorize write access but no deletion access.<\/p>\n<p>What we would want is to have only an \u201cadmin\u201d user that could run this garbage collection on the remote.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc get git@github.com:*.git folder giving ERROR: unexpected error - Response payload is not completed",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-git-github-com-git-folder-giving-error-unexpected-error-response-payload-is-not-completed\/919",
        "Question_created_time":1634238227604,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":481,
        "Question_body":"<p>I have successfully added a folder with 2gb of files using DVC to an s3 bucket. After that I tried to get the folder using the dvc get command. It downloaded half the folder correctly and after that the download stopped and gave me this message: ERROR: Unexpected error - Response payload not completed<br>\nAny suggestions on how I can resolve this issue?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Read only access using Azure Blog Storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/read-only-access-using-azure-blog-storage\/910",
        "Question_created_time":1633526016502,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":498,
        "Question_body":"<p>I have a public repo on GitHub using dvc. I want maintainers to add files using dvc. I will give them a SAS token so they can use the dvc client with their own token. as the repo and contents are public I would like to create a generic read-only token so anyone can download files with DVD pull. I thought I could do that by setting up the right permissions for the SAS token:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9.png\" data-download-href=\"\/uploads\/short-url\/f75fyugJBe0friORTHNGfOB2LTH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_567x499.png\" alt=\"image\" data-base62-sha1=\"f75fyugJBe0friORTHNGfOB2LTH\" width=\"567\" height=\"499\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_567x499.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_850x748.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/69eda0df2b25cb562b1de9f19f8a6812e0c1bef9_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1055\u00d7930 77.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>but it seems you can also write with that token. In fact, I was able to add a file with DVD push with the above configuration.<\/p>\n<p>Related: <a href=\"https:\/\/techcommunity.microsoft.com\/t5\/azure\/read-only-access-to-azure-storage-account-blob-containers-via\/m-p\/359229\" rel=\"noopener nofollow ugc\">https:\/\/techcommunity.microsoft.com\/t5\/azure\/read-only-access-to-azure-storage-account-blob-containers-via\/m-p\/359229<\/a><\/p>\n<p>Does anyone know how to do it properly? IS there any documentation about it?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Push same files in different branches",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/push-same-files-in-different-branches\/914",
        "Question_created_time":1633590580855,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":270,
        "Question_body":"<p>I\u2019m migrating from using LFS to DVC.<br>\nNow I\u2019m totally get rid of LFS legacy and want to upload big files to DVC.<br>\nHelp me figure out whether the same files from different branches will be stored in the repository in a single copy or not? Is there a way to add same files to all branches at once<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pipeline template",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipeline-template\/907",
        "Question_created_time":1632646722796,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":289,
        "Question_body":"<p>hey!<\/p>\n<p>I want to create a template of a pipeline  -<br>\ni.e. i want to run the same pipeline with different parameters each time.<br>\ni read the templating section documentation and looks like the only solution is<br>\n<code>foreach<\/code> in each stage,<\/p>\n<p>is it possible to run multiple stages with one foreach block?<br>\nwhat if I want to run one specific set of params?<\/p>\n<p>any suggestions?<\/p>\n<p>thanks!!!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pipline with many parallel stages",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipline-with-many-parallel-stages\/904",
        "Question_created_time":1632401878212,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":383,
        "Question_body":"<p>I would like to use DVC for project that\u2019s not ML but a sensitivity analysis for a bunch of a parameters of a simulation model.<\/p>\n<p>The DAG of my pipline looks basically like the diagram below. There\u2019s a single \u201ccommand\u201d file (box at the top) in which all simulations are defined (a csv file with one row for each simulation). Each column in the 3x4 blocks represents the set of steps for one simulation (preprocessing, running, postprocessing). Finally, combined results are created from groups of simulations (the 3x4 blocks).<\/p>\n<p>I want to make sure that, if a change is made to the command file for one simulation, only the stuff for that simulation is redone. It would be OK if the first step (the arrows from the command file to the first row) is always run for all simulations (this is a minor step). But for the following steps should be done only for the relevant simulations.<br>\nI could create a stage for every simulation and every step but this is hassle (in reality there are hundreds of simulations). Also I want to make it easy to add more simulations, by adding more lines to the command csv file.<\/p>\n<p>Is there functionality in DVC to handle this kind of situation? Is that a way to restructure my project to make things easier?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838.png\" data-download-href=\"\/uploads\/short-url\/hNYqo5HDfYLp8U1HKRw7uFANtxK.png?dl=1\" title=\"dvc_dag_example\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_690x426.png\" alt=\"dvc_dag_example\" data-base62-sha1=\"hNYqo5HDfYLp8U1HKRw7uFANtxK\" width=\"690\" height=\"426\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_690x426.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_1035x639.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">dvc_dag_example<\/span><span class=\"informations\">1083\u00d7670 39.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc non bare remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-non-bare-remote\/901",
        "Question_created_time":1632379867905,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":233,
        "Question_body":"<p>Hi,<br>\nlet\u2019s say I want to use dvc with azure blob storage. Is there a way that I can use the data from the blob storage directly in other azure projects? Normally a blob storage just gets mounted into another resource. With dvc this seems a problem, because files on the remote are stored differently than locally. Is there a way around that? With git for example it is possible to push into a non bare repository. Is there a similar option for dvc?<br>\nThanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Shared cache on NFS is slow",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-on-nfs-is-slow\/892",
        "Question_created_time":1631759389078,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":648,
        "Question_body":"<p>I have multiple users who each can launch their own AWS instance for data analysis, then shut it down when they are done. Rather than having to rebuild the dvc cache on each of these instances when they startup, we want to have a persistent shared cache.<\/p>\n<p>So I\u2019ve created a shared DVC cache on EFS (AWS\u2019s version of NFS) with symlinks in the workspace, but the problem is that \u2018dvc fetch\u2019 and \u2018dvc checkout\u2019 are now quite slow for certain branches that have 1000s of files tracked by DVC. First, for a \u2018dvc fetch\u2019, even if no downloads need to be done from the remote, just \u201cQuerying cache\u2026\u201d can take up to 8 minutes. Then \u2018dvc checkout\u2019 can take 6 more minutes. Presumably this has to do with NFS communication overhead to list files in the cache (I see the progress bar saying things like \u201c110 files\/sec\u201d).<\/p>\n<p>Is there any way to speed up a shared cache stored on NFS, especially when you have 1000s of tracked files?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc push error: unexpected keyword argument",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-unexpected-keyword-argument\/896",
        "Question_created_time":1632168889736,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":260,
        "Question_body":"<p>I added my data with <code>dvc add mydata\/path<\/code> then i got <code>mydata.dvc<\/code><br>\nthe <code>config<\/code> file in <code>.dvc<\/code> folder is<\/p>\n<blockquote>\n<p>[core]<br>\nremote = my_remote<br>\n[cache]<br>\ntype = \u201creflink,hardlink\u201d<br>\n[\u2018remote \u201cmy_remote\u201d\u2019]<br>\nurl = s3:\/\/\u2026<\/p>\n<\/blockquote>\n<p>when i ran <code>dvc push<\/code><br>\nit raised an error<\/p>\n<blockquote>\n<p>unexpected error - <strong>init<\/strong>() got an unexpected keyword argument \u2018cache_regions\u2019<\/p>\n<\/blockquote>\n<p>can anyone help me figure out this problem?<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/main.py\u201d, line 55, in main<br>\nret = cmd.do_run()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/command\/base.py\u201d, line 45, in do_run<br>\nreturn self.run()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py\u201d, line 57, in run<br>\nprocessed_files_count = self.repo.push(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/repo\/<strong>init<\/strong>.py\u201d, line 50, in wrapper<br>\nreturn f(repo, *args, **kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\u201d, line 48, in push<br>\npushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\u201d, line 85, in push<br>\nreturn transfer(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py\u201d, line 153, in transfer<br>\nstatus = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\u201d, line 160, in compare_status<br>\ndest_exists, dest_missing = status(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\u201d, line 122, in status<br>\nexists = hashes.intersection(<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\u201d, line 48, in _indexed_dir_hashes<br>\ndir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 415, in list_hashes_exists<br>\nret = list(itertools.compress(hashes, in_remote))<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/_base.py\u201d, line 611, in result_iterator<br>\nyield fs.pop().result()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/_base.py\u201d, line 432, in result<br>\nreturn self.__get_result()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/_base.py\u201d, line 388, in __get_result<br>\nraise self._exception<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/concurrent\/futures\/thread.py\u201d, line 57, in run<br>\nresult = self.fn(*self.args, **self.kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 406, in exists_with_progress<br>\nret = self.fs.exists(path_info)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\u201d, line 97, in exists<br>\nreturn self.fs.exists(self._with_bucket(path_info))<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/funcy\/objects.py\u201d, line 50, in <strong>get<\/strong><br>\nreturn prop.<strong>get<\/strong>(instance, type)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/funcy\/objects.py\u201d, line 28, in <strong>get<\/strong><br>\nres = instance.<strong>dict<\/strong>[self.fget.<strong>name<\/strong>] = self.fget(instance)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/dvc\/fs\/s3.py\u201d, line 157, in fs<br>\nreturn _S3FileSystem(**self.fs_args)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/fsspec\/spec.py\u201d, line 75, in <strong>call<\/strong><br>\nobj = super().<strong>call<\/strong>(*args, **kwargs)<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/s3fs\/core.py\u201d, line 187, in <strong>init<\/strong><br>\nself.s3 = self.connect()<br>\nFile \u201c\/home\/ubuntu\/anaconda3\/lib\/python3.8\/site-packages\/s3fs\/core.py\u201d, line 280, in connect<br>\nself.session = botocore.session.Session(**self.kwargs)<br>\nTypeError: <strong>init<\/strong>() got an unexpected keyword argument \u2018cache_regions\u2019<\/p>\n<\/blockquote>\n<h2>\n<a name=\"debug-version-info-for-developers-dvc-version-274-pip-1\" class=\"anchor\" href=\"#debug-version-info-for-developers-dvc-version-274-pip-1\"><\/a>DEBUG: Version info for developers:<br>\nDVC version: 2.7.4 (pip)<\/h2>\n<p>Platform: Python 3.8.5 on Linux-5.4.0-1056-aws-x86_64-with-glibc2.10<br>\nSupports:<br>\nhdfs (pyarrow = 4.0.1),<br>\nhttp (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),<br>\nhttps (aiohttp = 3.7.4.post0, aiohttp-retry = 2.4.5),<br>\ns3 (s3fs = 0.4.2, boto3 = 1.17.72)<br>\nCache types: hardlink, symlink<br>\nCache directory: ext4 on \/dev\/nvme0n1p1<br>\nCaches: local<br>\nRemotes: s3<br>\nWorkspace directory: ext4 on \/dev\/nvme0n1p1<br>\nRepo: dvc, git<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to update dvc version using snap",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-update-dvc-version-using-snap\/899",
        "Question_created_time":1632232389286,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":189,
        "Question_body":"<p>Hi community,<\/p>\n<p>I have seen the new 2.7.4 version has been released. However, I have installed 2.7.2 and it seems is not updating properly. If i run:<\/p>\n<p><code>snap  info dvc<\/code><\/p>\n<p>I get the following output:<\/p>\n<p>name:      dvc<br>\nsummary:   Data Version Control<br>\npublisher: Casper (casper-dcl)<br>\nstore-url: <a href=\"https:\/\/snapcraft.io\/dvc\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Install dvc on Linux | Snap Store<\/a><br>\ncontact:   <a href=\"mailto:support@dvc.org\">support@dvc.org<\/a><br>\nlicense:   Apache-2.0<br>\ndescription: |<br>\nGit for Data &amp; Models <a href=\"https:\/\/dvc.org\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org<\/a><br>\ncommands:<\/p>\n<ul>\n<li>dvc<br>\nsnap-id:      ceYKZQ2pf75cN9OVM33Bk36vVEwz3HaP<br>\ntracking:     latest\/edge<br>\nrefresh-date: 12 days ago, at 13:29 CEST<br>\nchannels:<br>\nlatest\/stable:    2.7.2   2021-09-09 (1317)  68MB classic<br>\nlatest\/candidate: \u2191<br>\nlatest\/beta:      \u2191<br>\nlatest\/edge:      \u2191<br>\nv2\/stable:        2.7.2   2021-09-09 (1317)  68MB classic<br>\nv2\/candidate:     \u2191<br>\nv2\/beta:          \u2191<br>\nv2\/edge:          \u2191<br>\nv1\/stable:        1.11.13 2021-01-26 (1282) 102MB classic<br>\nv1\/candidate:     \u2191<br>\nv1\/beta:          \u2191<br>\nv1\/edge:          \u2191<br>\nv0\/stable:        0.94.1  2020-06-25  (572) 101MB classic<br>\nv0\/candidate:     \u2191<br>\nv0\/beta:          \u2191<br>\nv0\/edge:          \u2191<br>\ninstalled:          2.7.2              (1317)  68MB classic<\/li>\n<\/ul>\n<p>Why is not the 2.7.4 available?<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC assigns user permissions to folders in the ssh repository",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-assigns-user-permissions-to-folders-in-the-ssh-repository\/898",
        "Question_created_time":1632174009351,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":776,
        "Question_body":"<p>Hi everybody. I need some help with a dvc repo. I have a very simple repository with a couple of dirs shared. The repo has a remote in an ssh server owned and managed by our team. As a repo folder in the ssh-server I created a folder (dvc.cache) owned by a unix group \u2018dvc-group\u2019.  The problem is that when some user pushes files to the repo (with dvc push) and this requires the creation of a new repo folder (e.g., dvc.cache\/fa), this folder is created with writing permissions only for the user, not for the dvc-group. This causes that when other users try to push modifications occuring in that folder he\/she gets an [Errno 13] Permission denied, as expected.<br>\nMy problem is that I couldn\u2019t find a way to fix this other than manually logging into the server an changing the permissions of these folders.<br>\nAny ideas how to fix this for good with DVC?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"List all remote paths of tracked files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/list-all-remote-paths-of-tracked-files\/863",
        "Question_created_time":1630558728484,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":242,
        "Question_body":"<p>For a given commit, I\u2019d like to get a list of the local paths &amp; corresponding remote paths of all files tracked by DVC, possibly limited to a certain directory. Is there not an easy way to do this?<\/p>\n<p>Currently looking at a workaround involving \u201cdvc list -R --dvc-only --rev  \u2026\u201d to get all tracked files, reading the md5 out of the corresponding .dvc files, and then using those to build the remote paths. Doesn\u2019t seem very elegant.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to refactor a directory of dvc managed files?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-refactor-a-directory-of-dvc-managed-files\/875",
        "Question_created_time":1630713202727,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":214,
        "Question_body":"<p>I have a case where I created a directory of files that are individually managed by DVC (so there are a number of files, each with a corresponding .dvc file).  I\u2019d like to refactor this so that I just tell DVC to managed the entire directory and all of its contents to get rid of all those .dvc files cluttering things up.<\/p>\n<p>I also have this situation in the recursive case (I\u2019d like to tell DVC to just manage a directory parent and all ancestor contents).<\/p>\n<p>What is the correct way to do this?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"'dvc push' multiple small files to aws s3 causes timeout error",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-multiple-small-files-to-aws-s3-causes-timeout-error\/865",
        "Question_created_time":1630591814110,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":824,
        "Question_body":"<p>Hi,<br>\nI have problem with pushing large amount of small files to s3 via \u2018dvc push\u2019 command (~2000 files few hundred kb each) from a local machine (Ubuntu).<\/p>\n<p>Stable wired internet connection (~50Mbps measured speed), latest DVC (2.6.4).<\/p>\n<p>While \u2018querying cache\u2026\u2019 phase progress slows down at 85% and eventually gives an \u2018unexpected error\u2019 (FSTimeoutError).<\/p>\n<p>Tried to increase max number of opened files (system-wide, by changing \/etc\/sysctl.conf) - no effect.<\/p>\n<p>Thought if it\u2019s worth to change timeout value (if any) but after checking dvc-remote page couldn\u2019t find any info about it.<\/p>\n<p>Could someone advice how to solve this issue?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using DVC without Cache (file references only)",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-without-cache-file-references-only\/694",
        "Question_created_time":1614671673780,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":1093,
        "Question_body":"<p>We have a HDFS data directory, which is only growing, so no files are deleted or edited. And we use these files for some machine learning stuff. Analysis A uses files for time period [1-a] and Analysis B uses files of time period [1-b] (see example below).<\/p>\n<p>Since our (HDFS) data directory is only increasing, we wonder if it is possible to use DVC for tracking a \u201cfile list\u201d or \u201cfile reference\u201d only, instead of copying the current directory content into some cache.<\/p>\n<p>For our case, something like a pointer would be enough and no cache directory for tracking deleted\/edited files is needed.<\/p>\n<p>For example:<\/p>\n<pre><code>Data File of Day 1\nData File of Day 2  ---&gt; Run Analysis A (DVC must hold a reference to files of day 1 to 2)\nData File of Day 3\nData File of Day 4\nData File of Day 5 ---&gt; Run Analysis B (DVC must hold a reference to files of day 1 to 5)\nData File of Day 6 ---&gt; Run Analysis C (DVC must hold a reference to files of day 1 to 6)\nData File of Day 7\n...\n\n=&gt; No cache needed, since data is only growing and not deleted or edited. \n=&gt; Every Analysis uses data from time period starting at day 1 to current day.\n<\/code><\/pre>\n<p>Is there a way to realize something with DVC? E.g. when I want to re-run analysis A, DVC knows which files to download from out HDFS data directory, without using a separate cache directory?<\/p>\n<p>Thank you for your help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Clone of repo with symlinked files creates copies not links",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/clone-of-repo-with-symlinked-files-creates-copies-not-links\/855",
        "Question_created_time":1629913961956,
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":219,
        "Question_body":"<p>I created a simple test repository with structure similar to this (there are actually a couple more levels of subfolders, but that does not change the issue):<br>\ndata\/<br>\nrepo\/<br>\n1.png             (actual image files)<br>\n2.png<br>\nworking\/<br>\n1.png             symlink to actual 1.png created using \u201cln -s \u2026\/repo\/1.png 1.png\u201d<br>\nDid a dvc add of everything in the data folder  \u201cdvc add data\u201d, then git add, etc.<br>\nPushed the dvc repository to a Google cloud remote, and pushed git to a remote.<br>\nFinally, create a fresh working folder, then:<br>\ngit clone   <br>\ndvc pull<br>\nThis DOES create the right folder structure but the working folder contains a full copy of 1.png, not a symlink to 1.png.<br>\nIs this expected?  Is there a way to have DVC cache and re-create a symlink rather than just follow it and end up with two copies of the file?<br>\nThanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Need to build non-ML data pipeline, is DVC good fit?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/need-to-build-non-ml-data-pipeline-is-dvc-good-fit\/849",
        "Question_created_time":1629579502912,
        "Question_answer_count":7,
        "Question_score_count":3,
        "Question_view_count":522,
        "Question_body":"<p>I need to build a data pipeline that basically goes like this:<\/p>\n<ol>\n<li>download new data hourly occasionally creating a new file in a directory<\/li>\n<li>extract data into usable form, create new file in output directory<\/li>\n<li>munch on all data for any day that any changed hourly data<\/li>\n<li>position changed output files into directory for on-line query service<\/li>\n<\/ol>\n<p>As I see it, having an entire directory as an output for step 1, 2 and 3 and entire directories as dependencies for steps 2, 3, and 4 could work, but processing steps would need to examine input directories to see what has changed since they last ran. I would prefer to push that logic into the workflow manager.<\/p>\n<p>An alternative would be to add dependencies or outputs each time a new file appears but that seems like just as much work as managing the workflow manually and leads to a kind of ugly DAG. At least I could intelligently version workflow steps this way, unlike option 3.<\/p>\n<p>A third option would be be to create an entire dependency chain for each new file downloaded by step (1). This leads to a really furry processing DAG that will be hard to understand and nearly impossible to consistently change processing steps.<\/p>\n<p>Alternative workflow engines like airflow and luigi and pachyderm might be alternatives if I am trying to force-fit DVC into a round hole, but I really like DVC\u2019s general philosophy.<\/p>\n<p>What is the best way to handle this kind of problem? Is DVC appropriate?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: failed to pull data from the cloud",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-failed-to-pull-data-from-the-cloud\/852",
        "Question_created_time":1629637181584,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":3920,
        "Question_body":"<p>DVC pull data from the s3 bucket is always failed in GitHub Action. My project link:<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/github.com\/MLHafizur\/heroku_fastapi_repo\">\n  <header class=\"source\">\n      <img src=\"https:\/\/github.githubassets.com\/favicons\/favicon.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/github.com\/MLHafizur\/heroku_fastapi_repo\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/345;\"><img src=\"https:\/\/opengraph.githubassets.com\/c16abf07deea2709a674279a5802a61fde6492555090e09fc9c585d9f5cf17e6\/MLHafizur\/heroku_fastapi_repo\" class=\"thumbnail\" width=\"690\" height=\"345\"><\/div>\n\n<h3><a href=\"https:\/\/github.com\/MLHafizur\/heroku_fastapi_repo\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - MLHafizur\/heroku_fastapi_repo<\/a><\/h3>\n\n  <p>Contribute to MLHafizur\/heroku_fastapi_repo development by creating an account on GitHub.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/f79ac308f2009c6bf7e87581cbe6140902ff9a24.jpeg\" data-download-href=\"\/uploads\/short-url\/zkpvG4S9ufLs3jIDWfuh3FeYCmo.jpeg?dl=1\" title=\"Screen Shot 2021-08-22 at 8.55.10 AM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/f79ac308f2009c6bf7e87581cbe6140902ff9a24_2_690x431.jpeg\" alt=\"Screen Shot 2021-08-22 at 8.55.10 AM\" data-base62-sha1=\"zkpvG4S9ufLs3jIDWfuh3FeYCmo\" width=\"690\" height=\"431\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/f79ac308f2009c6bf7e87581cbe6140902ff9a24_2_690x431.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/f79ac308f2009c6bf7e87581cbe6140902ff9a24_2_1035x646.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/f79ac308f2009c6bf7e87581cbe6140902ff9a24_2_1380x862.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/f79ac308f2009c6bf7e87581cbe6140902ff9a24_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2021-08-22 at 8.55.10 AM<\/span><span class=\"informations\">1920\u00d71200 101 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Tried dvc pull and dvc push , giving same output everyting is up to date.<\/p>\n<p>Hopping for your help.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC + Github Actions + GCP Storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-github-actions-gcp-storage\/840",
        "Question_created_time":1628712122880,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":834,
        "Question_body":"<p>I\u2019m trying triggering a pipeline to run DVC and download the data from GCP Storage but the log of GitHub Actions returns the following error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\n<\/code><\/pre>\n<p>I think this happens due to giving the right permissions to the Service Account but the one that I\u2019m using has the <em><strong>Storage Object Viewer<\/strong><\/em>, which gives the permission I need.<\/p>\n<p>Here is part of my pipeline file<\/p>\n<pre><code class=\"lang-auto\">- name: Setup Cloud SDK\n  uses: google-github-actions\/setup-gcloud@v0.2.0\n  with:\n    project_id: ${{ secrets.GCP_PROJECT }}\n    service_account_key: ${{ secrets.GCP_KEY }}\n    export_default_credentials: true\n\n- name: CML Run\n  shell: bash\n  env:\n    repo_token: ${{ secrets.GITHUB_TOKEN }}\n    GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_KEY }}\n  run: |\n    # run-cache and reproduce pipeline\n    dvc remote add -d -f myremote gs:\/\/myproject\/\n    dvc pull mypath\/data.csv.zip.dvc\n    dvc repro -m\n    \n    # Report metrics\n    echo \"## Metrics\" &gt;&gt; report.md\n    git fetch --prune\n    dvc metrics diff main --show-md &gt;&gt; report.md\n    \n    # Publish confusion matrix diff\n    echo -e \"## Plots\\n### Confusion Matrix\" &gt;&gt; report.md\n    cml-publish $PWD\/mypath\/reports\/confusion-matrix.png --md &gt;&gt; report.md\n    cml-send-comment report.md\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is there a review mechanism for pushing dataset through DVC?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-there-a-review-mechanism-for-pushing-dataset-through-dvc\/837",
        "Question_created_time":1628521483560,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":250,
        "Question_body":"<p>Hi,<\/p>\n<p>Usually when we are writing code, we work on a branch and then raise a merge request to merge it into the main branch. A reviewer, then reviews the code and approves the merge request.<\/p>\n<p>I am wondering if there is any approval\/review mechanism before we do a \u201cdvc push\u201d so that the reviewer has an opprtunity to catch dataset errors before the dataset is uploaded to the remote storage?<\/p>\n<p>For instance, I am following this tutorial: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>\n<p>Here, I would like a reviewer to review and approve the dataset before \u201cdvc push\u201d command is run and the dataset is uploaded to the remote storage.<\/p>\n<p>If not, what procedures do you guys follow to keep a check before the dataset upload?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR: failed to reproduce \u2018dvc.yaml\u2019: [Errno 13] Permission denied",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-failed-to-reproduce-dvc-yaml-errno-13-permission-denied\/835",
        "Question_created_time":1627673145405,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":1429,
        "Question_body":"<p>Dear community,<\/p>\n<p>please help me what I can do with the following\u2026<br>\nI am running <code>dvc repro<\/code> and get the following error:<\/p>\n<p><strong>ERROR: failed to reproduce \u2018dvc.yaml\u2019: [Errno 13] Permission denied: '\/mnt\/c\/Users\/michael\/projects\/deeplearing\/.dvc\/cache\/57\/01bf0661ad5b16b5165d9c6042a0cd597\u2019<\/strong><\/p>\n<p>notes:<\/p>\n<ul>\n<li>I was running the <code>dvc repro<\/code> in a console window of VSCode within a WSL environment<\/li>\n<li>I am using a conda environment with dvc installed<\/li>\n<li>I already tried deleting the .cache directory<\/li>\n<li>it looks like this is not a DVC problem, but an OS problem\u2026 still hoping for suggestions because the first stage is running successfully (almost identical to the second stage except that the amount of processed files is smaller)<\/li>\n<\/ul>\n<p>Maybe somebody has a clue what I can try?<\/p>\n<p>My <code>dvc.yaml<\/code> looks like this:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  create_first_dataset:\n    cmd: python create_first_dataset.py\n    deps:\n    - original_dataset\n    - create_first_dataset.py\n    outs:\n    - first_dataset\n  create_second_dataset:\n    cmd: python create_second_dataset.py\n    deps:\n    - original_dataset\n    - create_second_dataset.py\n    outs:\n    - second_dataset\n<\/code><\/pre>\n<p>dvc doctor output:<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.17\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        ssh (paramiko = 2.7.2)\nCache types: hardlink, symlink\nCache directory: 9p on C:\\\nCaches: local\nRemotes: ssh\nWorkspace directory: 9p on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>Thanks all!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Automated testing when merging to main",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/automated-testing-when-merging-to-main\/826",
        "Question_created_time":1626960959115,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":265,
        "Question_body":"<p>If we assume our currently deployed model is on the <code>main<\/code> branch. When we train a better model on a new branch and we decide to merge into main, we want to run some CI\/CD tests that will confirm our branch is ok to merge. In particular we want to test that 1) all of the dvc tracked files have been pushed to the remote and 2) if you run <code>dvc status<\/code> it return something like \u2018All data and pipeline up to date.\u2019.  We have set up some python tests to confirm this but it doesn\u2019t seem the most robust, I wondered if there was a more formal approach using dvc\/cml functionality for doing this?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cml \/ GitHub Actions \/ aws",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-github-actions-aws\/816",
        "Question_created_time":1625520013117,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":681,
        "Question_body":"<p>Guys,<\/p>\n<p>I\u2019m trying to run cml-runner in aws via GitHub actions.<br>\nThe script below opens an instance that we can see in the aws console.<br>\nThe result can be summarized as:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8.png\" data-download-href=\"\/uploads\/short-url\/dDWeV0VmqQQaNdVK2EcQMCnboO4.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_690x144.png\" alt=\"image\" data-base62-sha1=\"dDWeV0VmqQQaNdVK2EcQMCnboO4\" width=\"690\" height=\"144\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_690x144.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_1035x216.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_1380x288.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5fa009dc647acb5c0be050d942fbe26c5dbb67d8_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2798\u00d7584 33.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Since we don\u2019t have any information about what\u2019s going on, guessing isn\u2019t so easy. Again, a suggestion for resolving it is appreciated.<\/p>\n<p>Note : This is related to <a href=\"https:\/\/discuss.dvc.org\/t\/cml-dvc-github-actions-hyper-parameter-tuning\/812\">Question 812<\/a><\/p>\n<pre><code class=\"lang-auto\">\nname: Test\non: [push]\njobs:\n  deploy:\n    runs-on: [ubuntu-latest]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: iterative\/setup-cml@v1\n      - run: |\n          cml-runner \\\n          --cloud=aws \\\n          --cloud-region=eu-west-3 \\\n          --cloud-type=t2.micro \\\n          --labels=cml-runner\n        env:\n          REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n  train:\n    needs: deploy\n    runs-on: [self-hosted, cml-runner]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: iterative\/setup-dvc@v1\n      - uses: iterative\/setup-cml@v1\n      - run: |\n\n          echo \"Hello!\"\n        \n          \n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"CML+ DVC \/ GitHub Actions \/ hyper parameter tuning",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-dvc-github-actions-hyper-parameter-tuning\/812",
        "Question_created_time":1625219583155,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":431,
        "Question_body":"<p>Hi folks<\/p>\n<p>Looking for advice.<\/p>\n<p>We\u2019re trying to use the actions triggered manually by GitHub to train models and, of course, do hyperparameter tuning. We want to use CML to run in the cloud + DVC to track everything.<br>\nIn GitHub actions, you can define inputs and then GitHub provides the UI to specify them.<br>\nWe want to keep the structure of the DVC file as much as possible with hyperparameters specified in a yaml file. Is there an easy way to create tokens to map GitHub action entries, update param.yaml accordingly, commit the change, and use cml-runner to run a pipeline?<br>\nA simple solution is to send hyperparameters as arguments to the python script, but we will miss some interesting DVC tracking functionality.<\/p>\n<p>Cheers<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Opening a tracked file using python API: dvc.fs.azure.AzureAuthError: Authentication to Azure Blob Storage requires either account_name or connection_string",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/opening-a-tracked-file-using-python-api-dvc-fs-azure-azureautherror-authentication-to-azure-blob-storage-requires-either-account-name-or-connection-string\/801",
        "Question_created_time":1624290879559,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":515,
        "Question_body":"<p>I\u2019m trying to open a tracked file (blob storage Azure) and getting the following error.<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):                                                                                                                                                              \n  File \"dvc_test.py\", line 10, in &lt;module&gt;\n    with dvc.api.open(\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 113, in __enter__\n    return next(self.gen)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/api.py\", line 76, in _open\n    with _repo.open_by_relpath(\n  File \"\/usr\/lib\/python3.8\/contextlib.py\", line 113, in __enter__\n    return next(self.gen)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 488, in open_by_relpath\n    with fs.open(\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/repo.py\", line 143, in open\n    return dvc_fs.open(path_info, mode=mode, encoding=encoding, **kwargs)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/dvc.py\", line 79, in open\n    remote_obj = self.repo.cloud.get_remote(remote)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 29, in get_remote\n    return self._init_remote(name)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 49, in _init_remote\n    return get_remote(self.repo, name=name)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/remote\/__init__.py\", line 8, in get_remote\n    fs = cls(**config)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/azure.py\", line 83, in __init__\n    super().__init__(**config)\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 19, in __init__\n    self.fs_args.update(self._prepare_credentials(**kwargs))\n  File \"\/home\/airudi\/office\/python_codes\/test\/venv\/lib\/python3.8\/site-packages\/dvc\/fs\/azure.py\", line 119, in _prepare_credentials\n    raise AzureAuthError(\ndvc.fs.azure.AzureAuthError: Authentication to Azure Blob Storage requires either account_name or connection_string.\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;\n<\/code><\/pre>\n<p>Code snippet:<\/p>\n<pre><code class=\"lang-auto\">with dvc.api.open(\n    \"Risk_Identification_Models\/pipelines\/data\/train_data.xlsx\",\n    repo=\"git@ssh.dev.azure.com:v3\/aaa\/bbb\/ccc\",\n    remote=\"risk_model_remote\",\n    rev=\"hyper_param_opt_loop_0.82_f1_on_test\",\n    mode=\"rb\",\n) as fd:\n<\/code><\/pre>\n<p>It is worth noting that I have configured remote and connection strings.<\/p>\n<p><code>dvc remote add -d risk_model_remote azure:\/\/aaa\/data<\/code><\/p>\n<p><code>dvc remote modify --local risk_model_remote connection_string 'xxx'<\/code><\/p>\n<p><code>dvc push\/pull<\/code> are working as expected but the above error throws when I tried to use Python API.<\/p>\n<p>I hope the following will help to debug this issue.<\/p>\n<pre><code class=\"lang-auto\">venv) \u279c  .dvc git:(master) \u2717 dvc doctor\nDVC version: 2.3.0 (pip)\n---------------------------------\nPlatform: Python 3.8.5 on Linux-5.8.0-55-generic-x86_64-with-glibc2.29\nSupports: azure, http, https\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: azure\nWorkspace directory: ext4 on \/dev\/nvme0n1p2\nRepo: dvc, git\n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc kerberos ticket issue",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-kerberos-ticket-issue\/804",
        "Question_created_time":1624344341763,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":529,
        "Question_body":"<p>Hi all, have a trouble with setting up dvc remote hdfs with kerberos ticket.<br>\nDVC version: 2.3.0 (pip)<br>\nAccording to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify<\/a><br>\nI added valid <code>kerb_ticket<\/code> - path to the Kerberos ticket cache for Kerberos-secured HDFS clusters<\/p>\n<p>But have a following error during dvc push:<br>\n2021-06-22 12:42:07,580 DEBUG: Collecting information from remote cache\u2026<br>\n2021-06-22 12:42:07,580 DEBUG: Querying 1 hashes via object_exists<br>\n0% Querying remote cache|                                                        |0\/1 [00:00&lt;?,     ?file\/s]<br>\n\u2026<br>\nLoginException: Unable to obtain password from user<br>\norg.apache.hadoop.security.KerberosAuthException: failure to login: for principal: *** using ticket cache file: FILE:\/tmp\/krb5cc_1030 javax.security.auth.login.LoginException: Unable to obtain password from user<\/p>\n<p>python3.7\/site-packages\/dvc\/fs\/pool.py\", line 54, in get_connection<br>\nreturn self._conn_func(*self._conn_args, **self._conn_kwargs)<br>\nFile \u201cpyarrow\/_hdfs.pyx\u201d, line 83, in pyarrow._hdfs.HadoopFileSystem.<strong>init<\/strong><br>\nFile \u201cpyarrow\/error.pxi\u201d, line 141, in pyarrow.lib.pyarrow_internal_check_status<br>\nFile \u201cpyarrow\/error.pxi\u201d, line 112, in pyarrow.lib.check_status<br>\nOSError: HDFS connection failed<\/p>\n<p>can anybody help to understand this error?<br>\nthanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc get error: Unable to find DVC file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-error-unable-to-find-dvc-file\/797",
        "Question_created_time":1623955285707,
        "Question_answer_count":12,
        "Question_score_count":2,
        "Question_view_count":2031,
        "Question_body":"<p>In a cloned local git repo, there is already some dvc files and I want to add new data.<br>\nIn the config file under .dvc folder, i have<\/p>\n<pre><code class=\"lang-auto\">[core]\n    remote = my-remote\n[cache]\n    type = \"reflink,hardlink\"\n['remote \"my-remote\"']\n    url = s3:\/\/...\n<\/code><\/pre>\n<p>And I did<\/p>\n<pre><code class=\"lang-auto\">dvc add --external \/path\/to\/mydata\/\ngit add mydata.dvc\ngit commit -m \"added my data\"\ndvc remote default my-remote\ndvc push\n<\/code><\/pre>\n<p>Everything is good so far since it shows <code>10000 files pushed <\/code><\/p>\n<p>But when I tried to download the data by <code>dvc get  &lt;git url&gt;   &lt;folder_name\/dvc_filename&gt;<\/code><br>\nIt returned<\/p>\n<pre><code class=\"lang-auto\">unexpected error - : Unable to find DVC file with output \n'..\/..\/..\/..\/..\/private\/var\/folders\/5v\/6xws5skx46z5rg2y33_nwd1mqcvql4\/T\/tmp26wt3huydvc-clone\/folder_name\/dvc_filename'\n<\/code><\/pre>\n<p>Can anyone helped with this?<\/p>\n<p>Thank you<\/p>\n<h2>\n<a name=\"debug-version-info-for-developers-dvc-version-230-pip-1\" class=\"anchor\" href=\"#debug-version-info-for-developers-dvc-version-230-pip-1\"><\/a>DEBUG: Version info for developers:<br>\nDVC version: 2.3.0 (pip)<\/h2>\n<p>Platform: Python 3.8.5 on macOS-10.16-x86_64-i386-64bit<br>\nSupports: http, https, s3, ssh<br>\nCache types: reflink, hardlink, symlink<br>\nCache directory: apfs on \/dev\/disk1s5s1<br>\nCaches: local<br>\nRemotes: s3<br>\nWorkspace directory: apfs on \/dev\/disk1s5s1<br>\nRepo: dvc, git<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"CML runner specify ec2 ami id",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cml-runner-specify-ec2-ami-id\/792",
        "Question_created_time":1623680757727,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":286,
        "Question_body":"<p>Hello,<\/p>\n<p>I am reading through the CML documentation and is anyone aware of a was to specify the AMI id of the ec2 instance in the cml runner command?<\/p>\n<p>thanks,<br>\nKunal.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC Studio - We could not detect a DVC data remote in this repository",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-studio-we-could-not-detect-a-dvc-data-remote-in-this-repository\/788",
        "Question_created_time":1623234665789,
        "Question_answer_count":12,
        "Question_score_count":2,
        "Question_view_count":422,
        "Question_body":"<p>Hi folks,<\/p>\n<p>I\u2019m just trying out Studio.<br>\nBitbucket is used for Git and data is in Google Drive. Created a view and was able to connect to Bitbucket.<br>\nHowever, it appears that no remote control is identified. Note that I am able to dvc push and dvc pull from different computers without any problems. Any idea why?<\/p>\n<p>Cheers<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Statistical significant stage best practice",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/statistical-significant-stage-best-practice\/784",
        "Question_created_time":1623146955270,
        "Question_answer_count":9,
        "Question_score_count":0,
        "Question_view_count":414,
        "Question_body":"<p>Hello,<\/p>\n<p>In machine learning, it is important to repeat multiple time the same training (often with a different seed) and then compute the average and standard deviation for a metric to evaluate the stability of the model and check the statistical significance of the results.<\/p>\n<p>Let\u2019s say we have a stage <code>trainmodel<\/code> which has a parameter <code>seed<\/code> and we want to run the training 10 times with 10 different seeds. Once all trainings have been completed, we have a script that takes as input the results (metric files) from the 10 runs and produces the average and standard deviation for each metric.<\/p>\n<p>The thing is that I don\u2019t want to store the 10 trained models (since it is very heavy) but only keep the best trained model.<br>\nHowever, I want to keep the metrics of all 10 runs in order to be able to add a stage that compute the average and std of all metrics.<br>\nI also want to launch all 10 trainings in parallel.<\/p>\n<p>What would be the best way to do this with dvc ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Direct copy between Shared Cache and External Dependencies\/Outputs",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/direct-copy-between-shared-cache-and-external-dependencies-outputs\/487",
        "Question_created_time":1599692610587,
        "Question_answer_count":10,
        "Question_score_count":13,
        "Question_view_count":1298,
        "Question_body":"<p>I\u2019m trying to get my head around external dependencies, outputs and cache. It\u2019s not quite clear if I can achieve what I need with DVC, even though it seems to be very close. Here is what I\u2019m trying to get.<\/p>\n<p>I need to implement <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" rel=\"nofollow noopener\">Shared Development Server<\/a> scenario while optimizing download\/upload operations between the cache and external datasets. Here is why and how:<\/p>\n<ol>\n<li>I have a fast cloud storage mounted to a Kubernetes cluster via NFS or SMB, and I want to use it as Shared Local Cache. Think about mounted AWS S3 bucket or Azure File Share.<\/li>\n<li>I have a large dataset that is stored in <strong>the same or another<\/strong> bucket\/account, or potentially elsewhere, but always with an ability to copy files to\/from my cache storage many times faster than if I was copying them via the machine where the DVC command runs.<\/li>\n<li>When I execute <strong>dvc run<\/strong>, the command can initiate <strong>direct copy<\/strong> between the cache storage bucket\/share and the dataset bucket\/account.<br>\nBasically, what I want is the commands from these examples to run in the context of the shared cache folder, then dvc to create a hard\/sym\/ref-link to file in the shared cache, so the actual dataset bytes never make it to cluster until the pipeline code actually reads files. In the example with AWS S3, it would be similar to running<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">aws s3 cp s3:\/\/my_dataset_bucket\/data.txt s3:\/\/my_cachhe_bucket\/cache_path\/data.txt\n<\/code><\/pre>\n<p>Let\u2019s assume the command can actually recognize that the download destination is in the mounted cloud storage, not a usual directory. Same for the opposite direction.<br>\nThe way it looks to me now,  this command<\/p>\n<pre><code class=\"lang-auto\">aws s3 cp s3:\/\/mybucket\/data.txt data.txt\n<\/code><\/pre>\n<p>downloads data.txt somewhere locally (where?), then moves that to the cache, and make a link in the workspace.<br>\nThank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can dvc support remote large DBs?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-dvc-support-remote-large-dbs\/770",
        "Question_created_time":1622532408317,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":738,
        "Question_body":"<p>Hey all,<br>\nI tried DVC with small data set and I really liked it. The main thing DVC helped me with is controlling the data pipeline and versioning the data accordingly. In order to compare experiments we already use MLflow.<\/p>\n<p>My team have another project with much bigger data set which is stored on postgres (on AWS). Can we use DVC in order to version our tables? For example:<br>\nRaw_Table \u2192 One_hot_conversion_table \u2192 Normalized_one_hot_conversion_table<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Freeze stage with parameter substitution",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/freeze-stage-with-parameter-substitution\/761",
        "Question_created_time":1622098167167,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":463,
        "Question_body":"<p>Hi, I am not sure if this is expected or not, but freezing a stage with parameter substitution gives:<br>\n<code>ERROR: failed to freeze 'Test' - cannot dump a parametrized stage: 'Test'<\/code><\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nvars:\n  - folder: .\n\nstages:\n  Test:\n    cmd: dir ${folder}\n    always_changed: true\n<\/code><\/pre>\n<p>Command to run: <code>dvc freeze Test<\/code>. Is this normal behaviour? If yes, why?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to manage when remote is being updated",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-manage-when-remote-is-being-updated\/760",
        "Question_created_time":1621929845821,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":257,
        "Question_body":"<p>Hello community,<br>\nI am new to data version control. All the examples I see talk about how to update the remote if you update the dataset in the data folder in your local working environment. My question is the following:<\/p>\n<p>suppose I have a remote with 100 images and I pull. That command will copy those files to local, how do I correctly version those 100 images that I have in a remote folder?<br>\nThen, let\u2019s suppose someone adds 100 more images to the remote, getting 200. How do I indicate again that this is the second version of the dataset?<\/p>\n<p>If the data were in local, it would be more or less clear. However, I\u2019m not sure how to do it when the cycle starts on the remote.<\/p>\n<p>Best regards and thank you very much!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to track file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-track-file\/739",
        "Question_created_time":1619613397408,
        "Question_answer_count":13,
        "Question_score_count":1,
        "Question_view_count":483,
        "Question_body":"<p>hello,<\/p>\n<p>\u0131 couldn\u2019t  track my folder. What am I doing wrong ?<\/p>\n<p>My folder hiearchy<\/p>\n<p>Folder<br>\n-01<\/p>\n<p>\u0131 m using \u201cdvc add Folder\u201d comment.<br>\nthen \u0131 added to git.<br>\ngit tag -a \u201cv1\u201d -m \u201cmessage\u201d<\/p>\n<p>then \u0131 added other file to Folder.<br>\nNew data hiearchy is<\/p>\n<p>Folder<br>\n-01<br>\n-02<br>\n-03<\/p>\n<p>\u0131 repeated same things.<\/p>\n<p>dvc add Folder<br>\ngit add Folder.dvc<br>\ngit tag -a \u201cv1.1\u201d -m \u201cmessage\u201d<\/p>\n<p>then \u0131 check the tag v1 \u0131 want to see just 01 file in folder but \u0131 saw 01 02 and 03 file. Why? What am I doing wrong ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Import-url on local network",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/import-url-on-local-network\/749",
        "Question_created_time":1621351703852,
        "Question_answer_count":8,
        "Question_score_count":4,
        "Question_view_count":257,
        "Question_body":"<p>Hi,<br>\nI have a question regarding the import-url function of dvc. I have windows machines on a local network and want to import  (logfiles) files from one machine to another. I have the windows path, password and ip address and I am already able to <code>robocopy<\/code> the files I want. Is it now possible to import those files?<br>\nThe copy command that works is:<br>\n<code>robocopy \\\\{ipaddress}\\c$\\data\\file.txt C:\\Path\\file.txt<\/code><\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple users in one repo",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-users-in-one-repo\/746",
        "Question_created_time":1620974102574,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":1618,
        "Question_body":"<p>Hi,<\/p>\n<p>We have a scenario that someone might shed some light on.<\/p>\n<p>We are sucessfully implementing Shared Development Server and Data Registries with shared cache and user groups in our deep learning development. It all works perfectly, shared cache, remotes, etc except one particular issue very specific for our use case.<\/p>\n<p>Our current approach for data registries is that for any particular dataset we have a master copy of raw original data in that data registry repo on the server. Any data injections are done only from within that repo. So we always have a copy of raw datasets (not hashed) along with caches and a copy on the remote, which are hashed.<\/p>\n<p>The issue with this scenario is that we need several users be able to add, commit and push from within a particular repo. And even when they are in the same group we have to manually set permissions to 2775 and 0664 for everythiong in .dvc folder (cache is not there as per shared cache scenario) so that dvc works. Git commands work without any permission issues. And from time to time permissions break on the dvc remote too and we have to reset them manually to 2775 and 0444.<\/p>\n<p>Is it something that we might need to setup dvc some other way? Or is it just not supported but can be supported and possibly needs contribution?<\/p>\n<p>Thanks!<br>\nSimon<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Experiments with checkpoint",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/experiments-with-checkpoint\/744",
        "Question_created_time":1620824231813,
        "Question_answer_count":8,
        "Question_score_count":5,
        "Question_view_count":483,
        "Question_body":"<p>Hello,<\/p>\n<p>So currently, I have a pipeline with a stage \u201ctraindetector\u201d which produces a number of checkpoint files (one for each epoch).<\/p>\n<p>After running<\/p>\n<blockquote>\n<p>dvc exp run<\/p>\n<\/blockquote>\n<p>once, my experiment is done.<br>\nHowever, if I do:<\/p>\n<blockquote>\n<p>dvc status traindetector<br>\ntraindetector:<br>\nalways changed<\/p>\n<\/blockquote>\n<p>The \u201ctraindetector\u201d stage is marked as \u201calways changed\u201d, and if I redo:<\/p>\n<blockquote>\n<p>dvc exp run<\/p>\n<\/blockquote>\n<p>with no modification to the project, the \u201ctraindetector\u201d stage will be redone.<\/p>\n<p>Is this normal ?<br>\nCould somebody help me better understand experiments with checkpoints ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does dvc work for live streaming data versioning and batch data versioning ? If yes, can someone explain briefly",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-work-for-live-streaming-data-versioning-and-batch-data-versioning-if-yes-can-someone-explain-briefly\/738",
        "Question_created_time":1619414992269,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":908,
        "Question_body":"<p>I couldn\u2019t find any dvc documentation for batch data, and live streaming data versioning.<br>\nIs it possible in dvc to track streaming data and also fetch data in batch or time travel data?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AllAccessDisabled Error for mino set as remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/allaccessdisabled-error-for-mino-set-as-remote\/735",
        "Question_created_time":1618956655103,
        "Question_answer_count":2,
        "Question_score_count":5,
        "Question_view_count":1675,
        "Question_body":"<p>Hello DVC community,<\/p>\n<p>I have configure minio as the remote via dvc remote add and also modified the endpointUrl to the correct value.<\/p>\n<p>However I get the following error when I run dvc push:<\/p>\n<blockquote>\n<p>botocore.exceptions.ClientError: An error occurred (AllAccessDisabled) when calling the ListObjects operation: All access to this bucket has been disabled.<\/p>\n<\/blockquote>\n<p>Is this due to wrong access key and secret for minio? How does the plumbing between dvc and minio work in terms of credentials?<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"dvc.api.get_url()\" is not working for --external outputs",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-api-get-url-is-not-working-for-external-outputs\/730",
        "Question_created_time":1618813629880,
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":764,
        "Question_body":"<p>I have added dvc remote external output to track (external cache and external data storage),<\/p>\n<ul>\n<li>\n<p>Following is the .dvc generated,<br>\n<strong>path in github:<\/strong> remoteTrack\/wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>etag: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: remote:\/\/s3remote\/wine-quality.csv<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>But same works if not mentioned as external data, following is .csv for non external<br>\n<strong>path in github:<\/strong> wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>md5: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: wine-quality.csv<br>\n.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" data-download-href=\"\/uploads\/short-url\/nRHnFdVZK97Uul8v5E9CBTwtFPv.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" alt=\"image\" data-base62-sha1=\"nRHnFdVZK97Uul8v5E9CBTwtFPv\" width=\"685\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">900\u00d7656 37.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nEven after mentioning path as \u201cremote:\/\/s3remote\/wine-quality.csv\u201d, it is not working.<br>\n<strong>Error<\/strong>: PathMissingError: The path \u2018remoteTrack\/wine-quality.csv\u2019 does not exist in the target repository  neither as a DVC output nor as a Git-tracked file.<\/p>\n<p>What should be the path for remote external data?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc external output add after changing files data in remote is failing",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-external-output-add-after-changing-files-data-in-remote-is-failing\/731",
        "Question_created_time":1618818748017,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":498,
        "Question_body":"<p>I know that, after changing data being tracked by dvc, we can use \u201cdvc add\u201d command and then \u201cdvc push\u201d to github. I can use different .dvc files to get back data using \u201cdvc pull\u201d.<\/p>\n<p>The same when am trying to do with dvc external output, it is not working,<br>\nI\u2019m using remote storage and remote cache. And adding data of remote using \u201cadd --external\u201d command.<\/p>\n<ul>\n<li>dvc add --external remote:\/\/s3remote\/wine-quality.csv # tracks data, creates a cache folder in remote<\/li>\n<li>git push<\/li>\n<\/ul>\n<p>Now, am changing data in remote place, and I want to track the news changes,<\/p>\n<ul>\n<li>dvc add --external remote:\/\/s3remote\/wine-quality.csv<br>\n[I\u2019m using custom hosted Minion s3 bucket, to make changes am deleting the data file and uploading new one with same name with changes in data]<br>\nThis is failing with following error,<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/73a7302944e800f404f5a12f8b6123e45fbaa3bd.png\" data-download-href=\"\/uploads\/short-url\/gv76HdBPPYpiEEDxtGnScWBoiO9.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/73a7302944e800f404f5a12f8b6123e45fbaa3bd.png\" alt=\"image\" data-base62-sha1=\"gv76HdBPPYpiEEDxtGnScWBoiO9\" width=\"690\" height=\"23\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/73a7302944e800f404f5a12f8b6123e45fbaa3bd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1104\u00d738 2.62 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div>\n<\/li>\n<\/ul>\n<p>FYI,<\/p>\n<ul>\n<li>\n<strong>config file<\/strong><br>\n[cache]<br>\ns3 = s3cache<br>\n[\u2018remote \u201cs3remote\u201d\u2019]<br>\nurl = S3:\/\/datasource-bucket\/<br>\nendpointurl = <a href=\"http:\/\/localhostminio:10009\/\" rel=\"noopener nofollow ugc\">http:\/\/localhostminio:10009\/<\/a><br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<br>\n[\u2018remote \u201cs3cache\u201d\u2019]<br>\nurl = s3:\/\/datasource-bucket\/cache\/<br>\nendpointurl = <a href=\"http:\/\/localhostminio:10009\/\" rel=\"noopener nofollow ugc\">http:\/\/localhostminio:10009\/<\/a><br>\naccess_key_id = user<br>\nsecret_access_key = password<br>\nuse_ssl = false<\/li>\n<\/ul>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practice for handling large data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-handling-large-data\/721",
        "Question_created_time":1618304859684,
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":1304,
        "Question_body":"<p>I am working with a 25GB raw dataset which when processed to be prepared for training produces another 10+GB file. Using <code>dvc push<\/code> and <code>dvc pull<\/code> with s3 takes a significant amount of time to move data through the network. Is there a way to keep the data only in s3? or use a flag that chooses when to pull or push but that knows that a stage has run and does not need reruning?<\/p>\n<p>What would be the recommended way of working with the data both locally and using a remote?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"dvc add -external S3:\/\/mybucket\/data.csv\" is failing with access error even after giving correct remote cache configurations",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-external-s3-mybucket-data-csv-is-failing-with-access-error-even-after-giving-correct-remote-cache-configurations\/726",
        "Question_created_time":1618473589784,
        "Question_answer_count":9,
        "Question_score_count":8,
        "Question_view_count":1589,
        "Question_body":"<p>Hi All,<\/p>\n<p>Am connecting to remote S3 for data and also setting remote dvc cache in same S3.<br>\nFollowing is configure file,<\/p>\n<pre><code class=\"lang-ini\">[core]\n    remote = s3remote\n[cache]\n    s3 = s3cache\n['remote \"s3remote\"']\n    url = S3:\/\/dvc-example\n    endpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n['remote \"s3cache\"']\n    url = s3:\/\/dvc-example\/cache\n\tendpointurl = http:\/\/localhost:9000\/\n    access_key_id = user\n    secret_access_key = password\n    use_ssl = false\n<\/code><\/pre>\n<p>Am able to push and pull from remote repository to local.<br>\nBut when I try to add external data by configuring cache, am getting error.<br>\nBoth s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610.png\" data-download-href=\"\/uploads\/short-url\/iSi5wqipNKK3zGD6gzqzzcaTV28.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610.png\" alt=\"image\" data-base62-sha1=\"iSi5wqipNKK3zGD6gzqzzcaTV28\" width=\"690\" height=\"32\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/844a3dfabb3e006bfe4f8e15222af4a0236fc610_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1553\u00d773 5.72 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any help would be much appreciated.<\/p>\n<blockquote>\n<p>Also posted in <a href=\"https:\/\/stackoverflow.com\/questions\/67104752\" rel=\"noopener nofollow ugc\">https:\/\/stackoverflow.com\/questions\/67104752<\/a><\/p>\n<\/blockquote>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Track all experiments with a separate directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-all-experiments-with-a-separate-directory\/722",
        "Question_created_time":1618393378968,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":258,
        "Question_body":"<p>I am reading about experiment management <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/experiment-management<\/a> and I am interested in exploring the directories organisation pattern where you can keep all your experiments that you have run recorded at the same time and not through git history.<\/p>\n<p>What is not clear to me is how to implement that pattern though? Anyone has a better idea of how this works?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple params.yaml",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-params-yaml\/720",
        "Question_created_time":1618304562199,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":1251,
        "Question_body":"<p>Is it possible to have more than one params.yaml? If so, how should they be organised and named?<\/p>\n<p>The context is I am considering of breaking up my dvc.yaml to multiple pipelines and would like to do the same for the params as well. Is the answer to save a params.yaml at the same place where the pipeline for dvc.yaml will live? something like pipelines\/pip1\/dvc.yaml,params.yaml and pipelines\/pip2\/dvc.yaml,params.yaml<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker - Getting file URL",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/sagemaker-getting-file-url\/717",
        "Question_created_time":1617987668667,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":260,
        "Question_body":"<p>Hi there,<br>\nI am trying to get the url to my dvc remote data from SageMaker Studio by running the below code. However, I get the below error and have not been able to solve it. Any advice would be greatly appreciated.<\/p>\n<p>!pip install dvc<br>\nfrom dvc.api import get_url<\/p>\n<p>url = get_url(<br>\nrepo=\u201c<a href=\"https:\/\/github.com\/...\/machine_learning\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/...\/machine_learning<\/a>\u201d,<br>\npath=\u201cexperiments\/\u2026\/data.pkl\u201d<br>\n)<br>\nprint(url)<\/p>\n<p>CloneError: Failed to clone repo \u2018<a href=\"https:\/\/github.com\/...\/machine_learning\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/...\/machine_learning<\/a>\u2019 to \u2018\/tmp\/tmpo6kgus_3dvc-clone\u2019<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Commit\/add only changed files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/commit-add-only-changed-files\/714",
        "Question_created_time":1617883124373,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":442,
        "Question_body":"<p>Hi, we try to add all locally checked out and modified files with one command to the cache to further push it to the remote.<\/p>\n<p>We are using a S3 remote from which we only want to check out certain files to reduce the size of our repository on local hard drives. When modifying some of the checked-out files we want to add them to dvc without specifying all their names as targets for dvc add\/commit.<\/p>\n<p>E.g.: We track the files foo.txt bar.txt baz.txt but only have the file bar.txt checkout. The other two will remain only .dvc files and not get resolved. When we then modify the content of bar.txt and try to run dvc commit it tries to also add the deletion of foo.txt and baz.txt, because they were not checked out in the first place. We know we could specify bat.txt as a target for dvc commit. However, this approach is not practical when modifying a large number of files.<\/p>\n<p>Is there a way to only add the really modified files to dvc? We where thinking about a list of file-names like dvc diff returns but only for modified files which we could use as input for dvc commit.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practice for queuing experiments on code changes",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-queuing-experiments-on-code-changes\/710",
        "Question_created_time":1617283616396,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":493,
        "Question_body":"<p>Dear community,<\/p>\n<p>Thank you very much for DVC and the new features from the version 2!<\/p>\n<p><strong>What would be your recommendation on how to queue experiments which depend on code changes?<\/strong><\/p>\n<p>The use case is to queue as individual experiment each change of the content of a Python file declared as a <code>deps<\/code> of a DVC stage.<\/p>\n<p>For example:<\/p>\n<ol>\n<li>one is on a branch,<\/li>\n<li>makes a change on a Python file,<\/li>\n<li>do <code>dvc exp run --queue<\/code> for a DVC stage depending on this Python file,<\/li>\n<li>then do another change on the same Python file,<\/li>\n<li>then do <code>dvc exp run --queue<\/code> for the same DVC stage,<\/li>\n<li>and then do <code>dvc exp run --run-all<\/code>.<\/li>\n<\/ol>\n<p>It seems that committing each change is an anti-pattern and not very usable.<\/p>\n<p>But if the change are not committed, <code>git<\/code> shows the changed file hanging around as modified and the changes corresponding to an experiment seem then not tracked.<\/p>\n<p>The idea would be to replicate what is below but when the changes are not on a <code>params.yaml<\/code> file:<\/p>\n<aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n      <a href=\"https:\/\/dvc.org\/doc\/start\/experiments#queueing-experiments\" target=\"_blank\" rel=\"noopener nofollow ugc\">dvc.org<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/start\/experiments#queueing-experiments\" target=\"_blank\" rel=\"noopener nofollow ugc\">Get Started: Experiments<\/a><\/h3>\n\n<p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Thank you.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dataset-level access control for data registry",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dataset-level-access-control-for-data-registry\/712",
        "Question_created_time":1617550888575,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":313,
        "Question_body":"<p>Hello!<br>\nI would like to organize data registry with multiple datasets and multiple users (data scientists from different teams). Since some datasets contain sensitive information, it is important to have access control, both read and write. It would be great to have as flexible setup as possible. In the simplest case there could be just public\/private datasets, but RBAC\/ABAC or the possibility to provide dataset access to specific user directly (create many-to-many access control record) would be much better.<\/p>\n<p>DVC does not provide such features, but there are some workarounds, e.g <a href=\"https:\/\/discuss.dvc.org\/t\/restrict-access-to-dvc-repo\/553\">here<\/a> and <a href=\"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174\">there<\/a>. As far as I understood one could create several remotes and to simulate different groups\/roles and provide access to different users respectively. For me it seems to be pretty easy to break involuntary such workaround: e.g. having two remote \u2018public\u2019 and \u2018private\u2019 someone with appropriate access can accidentally push \u2018private\u2019 dataset to \u2018public\u2019 remote, making it available for anybody with \u2018public\u2019 remote access.<br>\nThere could also be two separate repos (or dvc registries) with public and private, it would be less probable to violate policy accidentally, but creation of e.g. 4 or 5 independent access groups (\u2018private_for_project_A\u2019, \u2018private_for_project_B\u2019) would be messy.<\/p>\n<p>I would be very thankful if anybody could share ideas how to organize more reliable\/convenient access control.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What type of protocole for huge dataset",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-type-of-protocole-for-huge-dataset\/707",
        "Question_created_time":1616517263719,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":323,
        "Question_body":"<p>Hello,<\/p>\n<p>I want to use dvc for my dataset versioning. My dataset can go up to 10Go and I want to setup my protocole to retrieve my data.<br>\nI see that protocole to be able to retrieve data:<br>\n(<a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>)<\/p>\n<ul>\n<li>Amazon S3<\/li>\n<li>SSH<\/li>\n<li>HDFS<\/li>\n<li>Local files and directories outside the workspace<\/li>\n<\/ul>\n<p>What protocole should I use to be able to pull that quantity of data?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to add data from \"scratch\" filesystem directly to \"scratch\" cache dir",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-add-data-from-scratch-filesystem-directly-to-scratch-cache-dir\/702",
        "Question_created_time":1615839224546,
        "Question_answer_count":10,
        "Question_score_count":4,
        "Question_view_count":532,
        "Question_body":"<p>My scenario is that I\u2019m on a cluster with really limited home directory quota (40GB) but with TB of \u201cscratch\u201d space on a separate filesystem. I\u2019d like to keep my Git repo on the home filesystem (say at <code>~\/myrepo\/<\/code>), but the DVC cache on the scratch file system (say at <code>\/scratch\/dvc<\/code>).<\/p>\n<p>Its easy enough to configure the cache on the scratch filesystem with e.g. <code>dvc cache dir \/scratch\/dvc<\/code> and then use the <code>symlink<\/code> cache type so files just symlink to here.<\/p>\n<p>But now, suppose I\u2019ve generated a large file at <code>\/scratch\/newfile.dat<\/code>. How can I add it to the repo at a chosen path without it ever touching the home filesystem?<\/p>\n<p>I would have thought:<\/p>\n<pre><code class=\"lang-bash\">dvc add -o ~\/myrepo\/newfile.dat \/scratch\/newfile.dat\n<\/code><\/pre>\n<p>would do it but that still seems to go through the home filesystem.<\/p>\n<p>It seems if I add <code>--to-remote<\/code> it works as expected, but it <em>also<\/em> (obviously) pushes to remote, which I don\u2019t want.<\/p>\n<p>Thanks for any suggestions.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to pull data from GCS without pipelines",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-pull-data-from-gcs-without-pipelines\/697",
        "Question_created_time":1615284819759,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":415,
        "Question_body":"<p>Hi,<\/p>\n<p>I am tracking some folders using DVC with google cloud storage. I don\u2019t have any yaml file because I don\u2019t have any experiment.<\/p>\n<p>How can I pull the data from just some folders?<\/p>\n<p>I tried doing <code>dvc pull path_subfolder<\/code> but this checks for a stage (I can see this with -v). After adding a dummy stage, there with an <code>echo<\/code> as cmd and the path as deps, the command goes through but no data is present (only in cache).<\/p>\n<p>Thanks for your help.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AttributeError with dvc.api.read to azure blob storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/attributeerror-with-dvc-api-read-to-azure-blob-storage\/688",
        "Question_created_time":1614251830608,
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":477,
        "Question_body":"<p>Hello,<\/p>\n<p>I am new to DVC and evaluate it in a proof of concept implementation for our ML projects, which seems to fit perfectly! But I encounter a problem with dvc.api.open while using an Azure Blob Storage.<\/p>\n<p>What I have done:<\/p>\n<ul>\n<li>cloned <a href=\"https:\/\/github.com\/iterative\/dataset-registry\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/dataset-registry: Dataset registry DVC project<\/a>\n<\/li>\n<li>dvc remote add -d myremote azure:\/\/BLOB\/PATH<\/li>\n<li>dvc remote modify --local myremote connection_string \u2018CONNECTION_STRING\u2019<\/li>\n<li>created test file<\/li>\n<li>dvc add &amp; push<\/li>\n<li>removed the test file incl. cache from local repo<\/li>\n<li>dvc.api.read ==&gt; AttributeError<\/li>\n<li>dvc pull<\/li>\n<li>dvc.api.read ==&gt; works<\/li>\n<\/ul>\n<p>I am able to use dvc push and dvc pull, but by using dvc.api.read I get \u201cAttributeError: \u2018NoneType\u2019 object has no attribute \u2018account_key\u2019\u201d (see attached screenshots). If the file is downloaded with dvc pull and it is available in the cache folder everything works.<\/p>\n<p>Can anyone point me to the problem or my misunderstanding? I want to use the streaming functionality, as we have very large files and do not want to store them on the storage of a virtual machine.<\/p>\n<p>Thanks!<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/3a37b2884021232141bb6f25c162527b698ad682.jpeg\" data-download-href=\"\/uploads\/short-url\/8j11vMuBq2Jq8JpCuTrAfmDO8eu.jpeg?dl=1\" title=\"Unbenannt\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_431x500.jpeg\" alt=\"Unbenannt\" data-base62-sha1=\"8j11vMuBq2Jq8JpCuTrAfmDO8eu\" width=\"431\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_431x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_646x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_862x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/3a37b2884021232141bb6f25c162527b698ad682_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Unbenannt<\/span><span class=\"informations\">1000\u00d71158 273 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Loading DVC Data in AWS Sagemaker",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/loading-dvc-data-in-aws-sagemaker\/684",
        "Question_created_time":1614020374734,
        "Question_answer_count":2,
        "Question_score_count":5,
        "Question_view_count":1696,
        "Question_body":"<p>Greetings!<\/p>\n<p>I\u2019ve been working on a project on my local computer and versioning my data via DVC to an S3 bucket.<\/p>\n<p>Now my project is too big and I need to take advantage of the nearly unlimited seeming RAM available on AWS Sagemaker.<\/p>\n<p>Also, I\u2019ve linked my Gitlab to my notebook instances in Sagemaker so I can see the folder containing the dvc files which I\u2019ve pushed to my Gitlab.<\/p>\n<p>However, everything I see on Sagemaker tutorials says I need to point to the actual file in S3 (see here:  <a href=\"https:\/\/stackoverflow.com\/a\/56060184\/4691538\" rel=\"noopener nofollow ugc\">https:\/\/stackoverflow.com\/a\/56060184\/4691538<\/a>).<\/p>\n<p>To that end, can someone tell me how to load a file into Sagemaker which is under data versioning control using DVC and stored in S3? Cheers!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to handle general metadata without experiments?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-handle-general-metadata-without-experiments\/680",
        "Question_created_time":1613669179072,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":429,
        "Question_body":"<p>Hi,<\/p>\n<p>Thanks for this amazing package <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> !<\/p>\n<p>I am using DVC to store some models but the way I am training my models does not allow me to easily use the pipelines. And it does not necessarily make sense in my case. Nevertheless, I have some metadata I would like to keep as well as some metrics.<\/p>\n<p>So my question is how should I do this? Because reading the doc and looking at some posts form here, I should use the dvc.yaml. But <code>cmd<\/code> is not optional and does not really make sense in my case. So right now I could just have a fake <code>cmd<\/code> but this does not sounds right.<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC and MLFlow - reproduce experiments using git commit ids",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-mlflow-reproduce-experiments-using-git-commit-ids\/561",
        "Question_created_time":1606334637517,
        "Question_answer_count":14,
        "Question_score_count":10,
        "Question_view_count":3458,
        "Question_body":"<p>Hello! I\u2019ve started using DVC and I love it, thank you for your work!<\/p>\n<p>I have a question regarding DVC and MLFlow combination. I hope someone can help me with that.<br>\nI am using DVC to build\/run pipelines and version data and models. And I am using MLFlow to have a nice overview of all the experiments and to visualize\/store metrics and plots of the experiments.<br>\nDuring the training and evaluation stages, I am logging stuff to MLFlow (including current git commit id for reproducibility).<\/p>\n<p>Let\u2019s say I want to experiment with a different learning rate.<br>\nMy actions are:<\/p>\n<ul>\n<li>I am updating the learning rate in params.yaml<\/li>\n<li><code>git commit -m 'starting a new run with updated learning rate'<\/code><\/li>\n<li>\n<code>dvc repro<\/code> (new MLFlow run with metrics and current git commit id is created)<\/li>\n<li>I check the results and I like them. I want to save the model.<\/li>\n<li><code>git add 'dvc.lock'<\/code><\/li>\n<li><code>git commit -m 'awesome learning rate'<\/code><\/li>\n<li><code>dvc push<\/code><\/li>\n<li><code>git push<\/code><\/li>\n<\/ul>\n<p>So I have 2 commits here. Commit A before the run and commit B after.<\/p>\n<p>Let\u2019s say I checked my MLFlow dashboard and I want to get the trained model of the last run.<br>\nIt is linked to the commit A.<br>\nIf I <code>git checkout<\/code> commit A, I won\u2019t get the trained model until I <code>dvc repro<\/code> again.<br>\nAnd I can\u2019t log commit B to the MLFlow run during training\/evaluation because I haven\u2019t created commit B yet.<\/p>\n<p>Can\u2019t really see the whole picture\u2026 How to do it properly?<br>\nAny ideas would be very helpful!<\/p>\n<p>Thanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Hi everyone! First question - How to point multiple projects to single dataset?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/hi-everyone-first-question-how-to-point-multiple-projects-to-single-dataset\/671",
        "Question_created_time":1613529152023,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":684,
        "Question_body":"<p>I stumbled upon dvc over the weekend after discussing with my colleagues our need for just such a thing. I got it working in some test repos with data on our private s3 buckets. Very cool! I really appreciate the documentation, it\u2019s very well put-together.<\/p>\n<p>One thing I\u2019m not sure of is:<\/p>\n<p>If I have a dataset on a remote server and I have several different repos\/projects that use it, what is the appropriate way to point them all to that dataset and\/or a single dvc remote representation thereof? (Do I need in each repo\/project to download it, then dvc add it, and assign the dataset to the same remote?)<\/p>\n<p>thanks!<br>\nRory<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Read only mode add dataset",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/read-only-mode-add-dataset\/674",
        "Question_created_time":1613568661758,
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":810,
        "Question_body":"<p>My dataset folder only read mode. I dont want to active write mode. So, how can \u0131 add and track my dataset?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to automate dvc pull request for a single file?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-automate-dvc-pull-request-for-a-single-file\/666",
        "Question_created_time":1613106444368,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":2491,
        "Question_body":"<p>We are using dvc for heavy AI-ML model files in our gitlab repository.<br>\nLets say, with the help of DVC, we can easily push a model file \u2018X\u2019 to cloud but while pulling    same file on some other server, we have to use command \u201cdvc pull X\u201d.<\/p>\n<p>Currently, we run this command \u201cdvc pull X\u201d manually everytime we update our file X. Since, we dont want to pull all the updated files on cloud therefore it is necessary for us to specify \u2018X\u2019 in our dvc pull requests.<\/p>\n<p>My question is how can we automate this dvc pull request in our CI yaml file for a single file X, if this X is a variable for our file name ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Archive \/ Share a snapshot of a DVC remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/archive-share-a-snapshot-of-a-dvc-remote\/664",
        "Question_created_time":1613033068998,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":378,
        "Question_body":"<p>Dear DVC team,<\/p>\n<p>Thank you for the great work and coming with an agnostic approach for DVC!<\/p>\n<p>It seems that a) one could create a tar.gz of the directory used as a DVC remote and b) that someone else could unpack this directory somewhere else and use it as a local remote.<\/p>\n<p><strong>Is there any counter-argument for distributing an archive of a DVC remote?<\/strong><\/p>\n<p>I have seen that it seems fine according to <a href=\"https:\/\/discuss.dvc.org\/t\/copying-a-dvc-repository\/213\" class=\"inline-onebox\">Copying a dvc repository<\/a>. But it was in another context (filesystem specificities).<\/p>\n<p>Why archiving a DVC remote? <strong>The idea is to share publicly a snapshot of a DVC remote.<\/strong><\/p>\n<p>One way could be to <code>dvc push<\/code> to a specific public remote when someone would want to create a snapshot. However, this implies two things one might not want. First, this requires a dedicated public server. Second, this prevents from having a DOI for the snapshot.<\/p>\n<p><strong>Would you have a better suggestion than distributing an archive to share a snapshot of a DVC remote?<\/strong><\/p>\n<p>Thanks,<br>\nPierre-Alexandre<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Versioning predictions",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/versioning-predictions\/656",
        "Question_created_time":1612779326788,
        "Question_answer_count":7,
        "Question_score_count":3,
        "Question_view_count":538,
        "Question_body":"<p>Hi, I am considering to use dvc not only for versioning the training, but also the predictions. Since the project I am working at handles clinical data, it is required to track how a prediction has been produced.<\/p>\n<p>The aim is, given incoming batches of inputs, to version each run of the pipeline, generating a sort of provenance store.<br>\nThe training pipeline and the one for predictions are separated.<\/p>\n<p>I am considering the following workflow:<\/p>\n<ul>\n<li>\n<p>using multistage feature, the dvc.yaml is defined like the following one (for simplicity only a stage is shown)<\/p>\n<p>stages:<br>\nstage1:<br>\nforeach: <span class=\"math\">{files} # From params.yaml\n       do:\n         cmd: predict-stage1 inputs\/<\/span>{item} outputs-stage1\/<span class=\"math\">{item}\n         deps:\n         - inputs\/<\/span>{item}<br>\nouts:<br>\n- outputs-stage1\/${item}<\/p>\n<\/li>\n<li>\n<p>Every time a new batch of files is ready, params.yaml is populated with their names and then dvc repro is launched. I am not sure if to keep dvc.lock or delete it before running the pipeline, so that it is referred only to the current inputs.<\/p>\n<\/li>\n<\/ul>\n<p>Do you see some evident drawbacks in using dvc this way? Do you know anyone using dvc for a use case similiar to this one?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc error \"file not owned by user\"",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-error-file-not-owned-by-user\/661",
        "Question_created_time":1612845083215,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":292,
        "Question_body":"<p>Hi,<br>\nI am configuring dvc on shared system.<br>\nThe ubuntu system has multiple user ids as logins.<\/p>\n<p>The folder dataset1 is copied by one user  \u201cuser1\u201d.<br>\nNow when I am doing \u201cdvc add dataset1\u201d, its showing as<br>\n\u201cfile not owned by user\u201d.<\/p>\n<p>I have rwx permissions on the dataset1. I verified using getfacl.<br>\nCan anyone please guide how can i do \u201cdvc add\u201d without getting error. I dont want to do as root user as later during git push we would need username for tracking.<\/p>\n<p>Kindly help please.<\/p>\n<p>Thanks &amp; Regards<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to deal with variants of the same model (not versions)",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-deal-with-variants-of-the-same-model-not-versions\/659",
        "Question_created_time":1612819150725,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":340,
        "Question_body":"<p>Hey there, I\u2019m a totally new user to dvc, so please keep that in mind when answering my question <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/grinning_face_with_smiling_eyes.png?v=9\" title=\":grinning_face_with_smiling_eyes:\" class=\"emoji\" alt=\":grinning_face_with_smiling_eyes:\"> . I have multiple variants of the same type of model (for example, let\u2019s say a network that has learned to do NLP for english sentences, and a separate network that has learned to do it for french sentences). In my example, as I plan to do language comprehension for more languages, more model variants will need to be added (let\u2019s say I plan to have 1 model variant for each of the hundreds\/thousands of languages\/dialects I want to handle in the future). Let\u2019s say I want to have a model versioning system like dvc to help with this (I am purely interested in the model versioning side of things for the moment, not data versioning). How would I go about this? As far as I can tell, dvc by default expects that 1 repo (i.e. a GCS directory) would be used to store the different versions of all the models. In my toy example, I would be interested in having a separate GCS directory for each language (i.e. all the model versions for english would get stored on 1 directory, all the model versions for french would get stored in another directory). Is there an easy way to do this?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Listing files in the remote storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/listing-files-in-the-remote-storage\/654",
        "Question_created_time":1612427783386,
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":787,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>We have pushed artifacts to remote storage (S3 bucket in our case). They got renamed and appear in S3 the way which makes it impossible to tell artifacts apart.<\/p>\n<p>We would like to delete some artifacts that are not used anymore. But the bucket is used by multiple projects, so we cannot figure out what is what.<br>\nHow to list the artifacts on the remote server?<\/p>\n<p><code>dvs remote list<\/code> only lists storages, not the contents of the storage.<\/p>\n<hr>\n<p>This is also connected to a question of the best practice on organizing remote storages: is it better to have separate S3 buckets per project? Or one bucket, but separate folder for each artifact?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to automate DVC push",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-automate-dvc-push\/649",
        "Question_created_time":1612185766792,
        "Question_answer_count":3,
        "Question_score_count":4,
        "Question_view_count":639,
        "Question_body":"<p>Hello<\/p>\n<p>I have started using DVC for version control in one of my projects and it\u2019s awesome.<\/p>\n<p>Currently, the model training job is executed from an amazon ec2 machine and the output is downloaded to my local machine which is pushed to the remote storage using the DVC push command.<\/p>\n<p>My doubt is, is it possible to programmatically push the model into the remote DVC storage (Amazon S3)? I couldn\u2019t find any helping articles online. Could you please suggest the right way of doing the same?<\/p>\n<p>Thanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Add stage without running its command",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-stage-without-running-its-command\/644",
        "Question_created_time":1611854828167,
        "Question_answer_count":4,
        "Question_score_count":4,
        "Question_view_count":471,
        "Question_body":"<p>Hi, I have a script that takes hours to complete. I already run it outside dvc and collected its output. Is there a way to add a stage to the dag with that script avoiding to run it again, pointing the right input\/output\/dependencies? In other words, since computation takes too  much time, i would like to skip it and let dvc do the hashes and complete the run without actually executing the script. Is it possible?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to locate credentials even though I can call awscli directly",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/unable-to-locate-credentials-even-though-i-can-call-awscli-directly\/650",
        "Question_created_time":1612197332106,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":2841,
        "Question_body":"<p>Hi<\/p>\n<p>I\u2019m getting \u2018Unable to locate credentials\u2019 when I run <code>dvc push<\/code> even though I can run AWS cli commands e.g. ```aws s3 ls s3:\/\/some-bucket<\/p>\n<p>I\u2019m wondering if it\u2019s related to permissions on my .aws files. I\u2019m running on  If I run Mac OS 10.15<\/p>\n<pre><code class=\"lang-auto\">ls -la \/path\/to\/.aws\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code class=\"lang-auto\">drwxr-xr-x   5 username  staff   160  5 Aug 09:18 .\ndrwxr-xr-x+ 90 username  staff  2880  1 Feb 16:22 ..\ndrwxr-xr-x   3 username  staff    96  5 Aug 09:18 cli\n-rw-------@  1 username  staff   542 15 Oct 16:24 config\n-rw-------@  1 username  staff   240 21 Jun  2020 credentials\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Python api with remote repo and local cache",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/python-api-with-remote-repo-and-local-cache\/637",
        "Question_created_time":1611678288864,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":630,
        "Question_body":"<p>Hi,<br>\nI am using the Python api to pull a file from remote s3. The call looks like this:<\/p>\n<blockquote>\n<p>with dvc.api.open(<br>\n\u2018&lt;path_to_file&gt;\u2019,<br>\nrepo=\u2018git@github.com:&lt;my_remote_github_repo&gt;\u2019,<br>\nrev= \u2018&lt;commit_reference&gt;\u2019<br>\n) as fd:<br>\ndf = pd.read_csv(fd)<\/p>\n<\/blockquote>\n<p>This works successfully to download the file; however, I\u2019m trying to achieve an increase in speed. I would like the api to download the file from s3 if it doesn\u2019t already exist in my local cache. If it\u2019s in my local cache, then I would like to get the file from there since it would be much faster.<\/p>\n<p>I realize that I could dvc pull the file from the command line and change my api to my local repo; however, I\u2019m trying to keep the code \u2018path\u2019 agnostic so that other users (who share the data repo) can run this script without having to modify paths in the code.<\/p>\n<p>I\u2019ve looked through the forum and the api documentation, but haven\u2019t been able to find a solution. Is this something that is possible, or perhaps a potential feature in the future? It would really speed up my workflow.<\/p>\n<p>Thanks,<br>\nMike<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to get the date of a dvc remote pushed",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-get-the-date-of-a-dvc-remote-pushed\/640",
        "Question_created_time":1611757273971,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":250,
        "Question_body":"<p>Hi<br>\nwhen I do a dvc pull the files pulled get the current date.<\/p>\n<p>I wonder how to get the date when the files were pushed.<\/p>\n<p>With git I can get this information with <em>git log my_file<\/em>.<br>\nHow to do this kind of information with dvc?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Manually deleted .dvc files -- these files still appear to be tracked by DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-deleted-dvc-files-these-files-still-appear-to-be-tracked-by-dvc\/634",
        "Question_created_time":1611306516441,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":239,
        "Question_body":"<p>Hello,<\/p>\n<p>I have manually deleted .dvc files that were previously tracked (and the tracked files themselves), expecting this to remove them from tracking (I realise now I should have used dvc gc).<\/p>\n<p>My question is how do I now remove these from tracking? I cannot re-add then remove them properly as the original files are not there.<\/p>\n<p>I am syncing data in two machines, on the machine I deleted the files on it says everything is up to date but on my other machine I am getting cache errors for the now deleted items. I am confused as I have pushed the deletes of the .dvc files so I assumed this would prevent them being tracked on both machines. Are they tracked somewhere else? I do not have a dvc.yaml file.<\/p>\n<p>Thanks,<\/p>\n<p>Justin<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC local storage usecase",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-local-storage-usecase\/628",
        "Question_created_time":1611043180303,
        "Question_answer_count":6,
        "Question_score_count":2,
        "Question_view_count":951,
        "Question_body":"<p>Hi, I\u2019m trying to understand if DVC is a good solution for our company use-case. We currently have several tens of TB of data, and constantly adding to it every week. I would like to add versioning to this, so that our scientists can run experiments on various subsets and track all changes.<\/p>\n<p>All the data is stored locally in a network drive accessible from the scientists\u2019 computers. My question would be if DVC can be used in this way for this use-case, and how to get around copying the data multiple time (e.g. if 5 scientists access 40TB worth of data, this shouldn\u2019t be copied to their versioned repos).<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Update same output dir in different stages",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/update-same-output-dir-in-different-stages\/620",
        "Question_created_time":1610622285769,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":1474,
        "Question_body":"<p>Hi, I have a pipeline with many stages that write the output in different subdirectories of the same dir.<br>\nSo basically each stage has as input a dir like this:<\/p>\n<ul>\n<li>input-dir\n<ul>\n<li>file1<\/li>\n<li>file2<\/li>\n<li>fileN<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>The nth stage writes its output:<\/p>\n<ul>\n<li>output-dir (same for all the stages)\n<ul>\n<li>results-file1\n<ul>\n<li>output-stage1<\/li>\n<li>output-stage2<\/li>\n<li>output-stageN<\/li>\n<\/ul>\n<\/li>\n<li>results-file2<br>\n.<br>\n.<br>\n.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Actually, results-file* is  a zarr group (<a href=\"https:\/\/zarr.readthedocs.io\/en\/stable\/api\/hierarchy.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Groups (zarr.hierarchy) \u2014 zarr 2.6.1 documentation<\/a>), whereas each stage appends new arrays (that correspond to output-stage* dirs).<br>\nIs there a way to achieve these with dvc? the command <code>dvc run<\/code> complains the output is already specified in another stage.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Managing pipelines operating per dataset element",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/managing-pipelines-operating-per-dataset-element\/613",
        "Question_created_time":1609963458190,
        "Question_answer_count":6,
        "Question_score_count":3,
        "Question_view_count":964,
        "Question_body":"<p>Hello! I would like to understand the appropriate way to manage pipeline stages that are not \u201csliced\u201d stage-wise, but rather dataset element-wise.<\/p>\n<p>For example, suppose a medical imaging dataset comprises different subjects, each of whom has a collection of files associated to them in a subject-specific directory. Such organization is convenient for manual inspection. Initially, only an image exists for each patient, which is located in S3 and obtained using <code>dvc import-url<\/code>. The to-be-written DVC pipeline will produce additional files (preprocessing, etc.) in each subject\u2019s directory.<\/p>\n<p>My current understanding from DVC\u2019s (excellent) documentation is there are potentially several ways to accomplish this:<br>\n[a] Reshape the dataset such that subjects are distributed across multiple stage directories, which are given as <code>dep<\/code> or <code>out<\/code> to the pipeline.<br>\n[b] List every file as <code>dep<\/code> or <code>out<\/code>.<br>\n[c] Decouple stages from directory structure by having each stage depend on a \u201csummary\u201d or \u201csuccess\u201d file from previous stages instead of a subject\u2019s files or a stage directory.<br>\n[d] Introduce a \u201csummary\u201d stage which creates the desired directory structure view, e.g. via symlinks.<\/p>\n<p>Option [a] discards the desired directory structure, which is extremely undesirable since manual inspection of subjects\u2019 intermediate outputs is common and it is convenient to have all outputs collocated. Option [b] allows to maintain desired directory structure but is unwieldy and perhaps <a href=\"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609\">not performant<\/a>(?). Option [c] allows to maintain desired directory structure but requires error prone user implementation (e.g. the summary file must reflect the unique combination of files it summarizes, which somewhat reinvents what DVC already does for tracking a directory). Option [d] allows to maintain desired directory structure as a view of another structure DVC likes more, but produces outputs that should not be tracked.<\/p>\n<p>Are there perhaps other options possible? Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Advice for versioning many many small files?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609",
        "Question_created_time":1609777134249,
        "Question_answer_count":8,
        "Question_score_count":3,
        "Question_view_count":2524,
        "Question_body":"<p>Hello! I\u2019m new to DVC <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nI was working on versioning a large dataset (totaling ~140gb) which consists of many many very small files. They are mp3 audio files (imagine for training a speech model), each for example 100-200kb (they\u2019re only 5-15 seconds each on average). This means I\u2019ve got a lot of files of course<\/p>\n<p>I ran <code>dvc push<\/code> and let it run for ~16 hours before realizing this probably isn\u2019t a great idea, since it seemed like it was nowhere near completion, and cancelled the run. I found a couple links online telling me dvc isn\u2019t ideal for many small files, but I also found a comment from a dvc maintainer on this forum saying recent changes (that post was ~apr 2020) should improve performance for dvc push on many small files \u2013 Would anyone be able to advise me? Did I possibly do something wrong, or should I be rolling these files into a tar, zip, etc and versioning that? Wouldn\u2019t it make more sense to version individual data files? (I\u2019m new to working with data too <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/upside_down_face.png?v=9\" title=\":upside_down_face:\" class=\"emoji\" alt=\":upside_down_face:\">)<\/p>\n<p>Help would be really appreciated! Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can't go to former version of dataset with `dvc checkout`",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cant-go-to-former-version-of-dataset-with-dvc-checkout\/615",
        "Question_created_time":1610450954470,
        "Question_answer_count":14,
        "Question_score_count":3,
        "Question_view_count":2550,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out DVC for the first time for my deep learning development pipeline. As mentioned in the <a href=\"https:\/\/youtu.be\/kLKBcPonMYw\" rel=\"noopener nofollow ugc\">Version Data with DVC<\/a> tutorial, I created a folder named \u201cdataset\u201d in the main directory and then I added some data into it. And then  I used <code>dvc add dataset<\/code> to track the dataset. After that, I included \u201cdataset\u201d folder in the .gitignore to untrack the dataset with git.<\/p>\n<p>Then I setup Azure storage blob for my remote data repo. After adding some more data, I wanted to go back to the original version of the data so I checkout to that git commit and then used <code>dvc checkout<\/code> command. Then this error comes up.<\/p>\n<pre><code class=\"lang-auto\">+--------------------------------------------------+\n|                                                  |\n|        Update available 1.10.2 -&gt; 1.11.10        |\n|     Run `apt-get install --only-upgrade dvc`     |\n|                                                  |\n+--------------------------------------------------+\n\nWARNING: Cache 'HashInfo(name='md5', value='db4287e726604a63a231cf4462cb27df.dir', dir_info=None, size=497535926, nfiles=8818)' not found. File 'dataset' won't be created.\nERROR: Checkout failed for following targets:\ndataset\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n<\/code><\/pre>\n<p>I tried searching for the solution in the given link but I could find it. Thanks in advance.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to force dvc get",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-force-dvc-get\/417",
        "Question_created_time":1593011149596,
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":997,
        "Question_body":"<p>From my CI pipeline, I would like to download some files tracked using DVC.<\/p>\n<pre><code class=\"lang-auto\">data\n\u2514\u2500\u2500 test_data\n    \u251c\u2500\u2500 tests_subfolder_1\n    \u2502   \u251c\u2500\u2500 file1.test\n    \u2502   \u2514\u2500\u2500 file2.test\n    \u251c\u2500\u2500 tests_subfolder_1.dvc\n    \u251c\u2500\u2500 tests_subfolder_2\n<\/code><\/pre>\n<p>\u2026 etc.<\/p>\n<p>When I run <code>dvc get . data\/test_data --out data\/<\/code>, I get the error:<\/p>\n<pre><code class=\"lang-bash\">ERROR: failed to get 'data\/test_data\/' from '.' - unable to remove 'data\/test_data\/test_subfolder.dvc' without a confirmation. Use `-f` to force.\n<\/code><\/pre>\n<p>This is surprising to me, because I would expect <code>test_subfolder.dvc<\/code> and <code>test_subfolder\/<\/code> to be able to coexist.<\/p>\n<p>However, I don\u2019t know where to put the <code>-f<\/code> argument? I tried:<\/p>\n<ul>\n<li><code>dvc -f get . data\/test_data --out data\/<\/code><\/li>\n<li><code>dvc get . data\/tests_data --out data\/ --force<\/code><\/li>\n<\/ul>\n<p>I also tried to work around the problem with:<br>\n<code>dvc get . data\/test_data\/* --out data\/<\/code><\/p>\n<p>However, my final solution was:<br>\n<code>ls data\/test_data\/*.dvc | sed -e 's\/\\.dvc$\/\/' | xargs -I {} dvc get . {} --out data\/test_data\/<\/code><\/p>\n<p>This work-around feels over-complicated. What was I supposed to do?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC with bitbucket",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-bitbucket\/192",
        "Question_created_time":1564573379958,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":2236,
        "Question_body":"<p>I want to setup DVC on top of bitbucket instead of git. How can i do it. Can DVC integrate with bitbucket.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Merging of files in DVC-tracked directories",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/merging-of-files-in-dvc-tracked-directories\/599",
        "Question_created_time":1608643957319,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":1058,
        "Question_body":"<p>I just tested DVC with some dummy data and have run into a situation which would be less than ideal for production. Is there an elegant way out?<\/p>\n<h1>Situation<\/h1>\n<ul>\n<li>The workspace has only one DVC-tracked directory named \u201cdata\/\u201d<\/li>\n<li>There are two git branches. In both of them, we added\/removed separate files to \u201cdata\/\u201d<\/li>\n<li>Now, we want to merge both branches<\/li>\n<\/ul>\n<h1>Expected outcome<\/h1>\n<ul>\n<li>Unless there are conflicts, a simple git merge yields the union of the file operations in both branches<\/li>\n<\/ul>\n<h1>Actual outcome<\/h1>\n<ul>\n<li>A git conflict in data.dcv. I can\u2019t really merge, but only pick the data version in one of the branches<\/li>\n<\/ul>\n<p>Given that the command \u201cdvc diff\u201d shows some very useful output, is there a way to merge both data versions semi-automatic? I have read the page <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/how-to\/merge-conflicts<\/a>, but this only mentions the \u201cappend-only\u201d strategy, not even mentioning dvc diff :(.<\/p>\n<p>P.s.: As a side question: Can \u201cdvc diff\u201d detect and highlight renamed files (since they have the same hash value)?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Migration tutorials",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/migration-tutorials\/601",
        "Question_created_time":1608660663563,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":516,
        "Question_body":"<p>is there any tutorial how to migrate from git lfs to dvc?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Clear local cache completely and rely on remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/clear-local-cache-completely-and-rely-on-remote\/596",
        "Question_created_time":1608292066842,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":3356,
        "Question_body":"<p>Hi!<\/p>\n<p>First off thanks for a great tool! While we don\u2019t use the pipelines very much, we do use DVC to store our data in my team, several sets of 100s of GBs each.<\/p>\n<p>I often work with one data set at the time, usually for several weeks. It would be nice to be able to clear the data sets not currently in use completely from my local machine. We use a monorepo for everything, and if I understand gc correctly, the reason my cache doesn\u2019t get cleared is because there are .dvc files of all sets in the branch head.<\/p>\n<p>Since there are some datasets I almost never use, it would be nice to be able to clear them completely from the local cache and then once i need it I\u2019ll take my punishment and wait for it to download from the remote using dvc pull. My question, I guess, is if there is a nice way of doing this that I\u2019m missing? Currently I\u2019ve resorted to manually deleting everything i the cache folder every few months to start fresh and pull what I need. It works but doesn\u2019t feel like the correct way to go about it.<\/p>\n<p>All the best!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Link types and run options",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/link-types-and-run-options\/590",
        "Question_created_time":1608203560415,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":732,
        "Question_body":"<p>Hello,<br>\nI have questions about the different link types. I\u2019m mainly working on windows at the moment.<\/p>\n<ol>\n<li>What is the default link type for windows?<\/li>\n<li>While I am training a new model (takes some time), I want to work on the old model. To do this I added the flag <code>--outs-persist<\/code> for this. Is this a good a idea in general? If I change the link type now to symlink, hardlink will this work as expected?<\/li>\n<li>I also track the training with mlflow. To add the <code>mlruns<\/code> folder as output to the training stage, I again added  the <code>--outs-persist<\/code> flag. The main reason for this is, that I want to share the mlflow results with others. This works quite well at the moment. What happens if I change the link type now? Will this break something?<\/li>\n<\/ol>\n<p>Thank you<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc and gcp\/colab tricks",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-gcp-colab-tricks\/584",
        "Question_created_time":1607977224406,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":1333,
        "Question_body":"<p>I am thinking to switch to dvc, but have issues to resolve:<br>\nI have dataset, which is huge. Suppose most of the time I was doing smth like this:<br>\ndirectly downloaded files from gcp or in colab I just mounted drive and took data.<br>\nWhat should I do if I use dvc? In colab if I mount storage, datasets is encripted to repo format.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Parameterlike dependencies",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/parameterlike-dependencies\/565",
        "Question_created_time":1606585276131,
        "Question_answer_count":12,
        "Question_score_count":2,
        "Question_view_count":893,
        "Question_body":"<p>Hi,<br>\nI have a question about the following scenario:<br>\nLet data1 and data2 be two data folders, script.py a script and the command to run the script like: <code>python  script.py --datadir .\/df<\/code> where df is equal to data1 or data2.<br>\nWhat is a good way to setup the stage now? I thought about these options:<\/p>\n<ol>\n<li>one stage + make datadir a parameter -&gt; dvc will not update after changes in data1\/data2<\/li>\n<li>one stage + just one data folder -&gt; I have to checkout the right data all the time (for example tests) and maybe another script needs both folders at the same time.<\/li>\n<li>two stages, one for each datafolder -&gt; I would have to add extra argument \u201cstage\u201d to  script.py to load the right parameters.<\/li>\n<\/ol>\n<p>Is there a better way to do this?<\/p>\n<p>Thanks for your great work by the way : )<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Comparing different metric\/param\/plot files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/comparing-different-metric-param-plot-files\/582",
        "Question_created_time":1607582857470,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":293,
        "Question_body":"<p>Is it possible to perform a diff on 2 distinct metric\/param\/plot files or only the same file can be compared across branch\/commit? The scenario is if instead of having my experiments on different branches I have them on different folders.<br>\nThanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Shared development details",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-development-details\/580",
        "Question_created_time":1607527227363,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":573,
        "Question_body":"<p>Hi,<\/p>\n<p>we are trying to use DVC very close to the described <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" rel=\"noopener nofollow ugc\">shared-development-server<\/a> use-case. This is great for starting, but the devil is in the details.<br>\nThe setup is that we use a shared storage in an HPC environment, where we try to reduce data duplication.<\/p>\n<p>We use a shared cache and because we later might want to connect this to a remote storage, we create a local \u201cremote\u201d storage for the moment.<\/p>\n<p>In the setup we have 2 different types of data. Smaller data files, that are ok if they get downloaded to the individual development folders and a folder containing millions of large files that we don\u2019t want to duplicate.<\/p>\n<p>We have 2 issues now.<\/p>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<p>Secondly, we are not sure how to fare with the large folder with the millions of large files. We probably should create a link to the folder from each individual directory. The data in this folder is generated elsewhere. We want to make sure if we ever have to re-run the data generation, we are informed if this lead to different files. Hence, we want to hash the folder or it\u2019s content, but we are not sure if it\u2019s viable to do this for this amount of files individually. What would be best solution here?<\/p>\n<p>Thanks a lot and best regards<br>\nMarius<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc.yaml: `deps` & `outs` section accessible from code?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-yaml-deps-outs-section-accessible-from-code\/577",
        "Question_created_time":1607463333300,
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":628,
        "Question_body":"<p>Hey everyone,<\/p>\n<p>How do you avoid having to define the path to dependencies\/outputs both in the <em>dvc.yaml<\/em> as well as your code?<\/p>\n<p>In the <em>dvc.yaml<\/em> I have to specify paths to dependencies and outputs. In my code, I then will read\/write from these paths, but have to define these paths (again) somewhere.<br>\nIs there already a concept in place for me to read the paths directly from the yaml (to avoid defining these paths twice?)<\/p>\n<p>Small example:<\/p>\n<p>The <em>dvc.yaml<\/em><\/p>\n<pre><code class=\"lang-auto\">stages:\n  preprocess:\n    cmd:  python src\/preprocess.py\n    deps:\n    - data\/raw\n    - src\/preprocessing\n    outs:\n    - data\/preprocessed\n<\/code><\/pre>\n<p>The <code>src\/preprocess.py<\/code><\/p>\n<pre><code class=\"lang-python\">raw_path = Path(\"data\/raw\")\npreprocessed_path = Path(\"data\/preprocessed\")\n\npreprocess(raw_path, preprocessed_path)\n...\n<\/code><\/pre>\n<p>Obviously, in this case, I could use command line arguments to never spell out the paths inside the script and have my <em>dvc.yaml<\/em>  looking like this:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  preprocess:\n    cmd:  python src\/preprocess.py data\/raw data\/preprocessed\n    deps:\n    - data\/raw\n    - src\/preprocessing\n    outs:\n    - data\/preprocessed\n<\/code><\/pre>\n<p>But this still \u201cduplicates\u201d the path (only inside the yaml), and isn\u2019t always easily feasible (e.g., Jupyter Notebooks).<\/p>\n<p>I\u2019m search for something along the lines of \u201cnamed dependencies and outputs\u201d:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  preprocess:\n    cmd:  python src\/preprocess.py data\/raw data\/preprocessed\n    deps:\n      my_raw_path: data\/raw\n      other:\n      - src\/preprocessing\n    outs:\n      my_preprocessed_path: data\/preprocessed\n<\/code><\/pre>\n<p>Similar to the <em>params.yaml<\/em>  I could then read the <em>dvc.yaml<\/em> in my code and look for the path in the dict:<\/p>\n<pre><code class=\"lang-python\">raw_path = Path(dvc_yaml[\"stages\"][\"preprocess\"][\"deps\"][\"my_raw_path\"])\n...\n<\/code><\/pre>\n<p>I\u2019m not convinced whether this solution is the smartest, and was wondering how everyone else is handling this?<\/p>\n<p>Cheers,<br>\nFabian<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple overlapping datasets",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-overlapping-datasets\/576",
        "Question_created_time":1607351479222,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":278,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m considering using dvc for a project and am wondering if it fits the following use case. Say I have N images which form a dataset, but for any given experiment I may only want to train on some subset of the images. What I would like is to be able to determine which images in the subset are missing locally and only pull those images from the file store. Ideally these subsets could also have names and I\u2019d be able to just do dvc pull datasubset3. Is something like this possible?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR file not owned by user",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-file-not-owned-by-user\/564",
        "Question_created_time":1606494207206,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":674,
        "Question_body":"<p>On <code>dvc add --external \/mnt\/my_storage<\/code> I am getting <code>ERROR: file '..\/..\/..\/..\/mnt\/my_storage\/data.csv' not owned by user!<\/code><\/p>\n<p>I have machine_1 with a project I am working under git control and dvc initialized.<br>\nMachine_2 is a remote file server on the same network mounted to \/mnt\/my_storage\/ via samba. I\u2019d like to keep the training data on \/mnt\/my_storage, but track it in DVC.<\/p>\n<p>Expected behaviour:<br>\ndvc hashes all files in \/mnt\/my_storage and creats a my_storage.dvc in my project folder.<\/p>\n<p>Actual behaviour:<br>\nFiles in one subdirectory are hashed and in another I am getting above mentioned error.<\/p>\n<p>The files on machine_2 are owned by a different user, but I have write access to \/mnt\/my_storage on file system level.<\/p>\n<p>What is this error telling me?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Check size prior to download",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/check-size-prior-to-download\/559",
        "Question_created_time":1605811292214,
        "Question_answer_count":1,
        "Question_score_count":6,
        "Question_view_count":572,
        "Question_body":"<p>Hi,<\/p>\n<p>DVC newbie here. Thanks for developing a much-needed tool.<\/p>\n<p>I was wondering if there is a way to obtain the size information of the dvc files\/folders before issuing a dvc pull. I have searched through the manual but failed to find any information.<\/p>\n<p>Say I have <code>datasets.dvc<\/code> file uploaded by a teammate and I would like to learn it\u2019s size prior to downloading it.<\/p>\n<p>DVC version: 1.7.9<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deleting a DVC tracked file from history",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deleting-a-dvc-tracked-file-from-history\/556",
        "Question_created_time":1605517757309,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":907,
        "Question_body":"<p>I have a dataset tracked by DVC which contains personal information about a group of people. Keeping privacy and security in mind, if I get a request from someone for having their data deleted from the dataset, how do I go about doing that from the entire history of the dataset? Just deleting the file and adding another DVC commit obviously won\u2019t do because the data is still easily recoverable in the history.<\/p>\n<p>I can see that <code>dvc gc<\/code> command does something along this line but it\u2019s not clear to me from the documentation whether you can specify the file that you want deleted. Looks like it only deletes unused files in the history by default.<\/p>\n<p>Is there any command that the deletion of targeted files from the entire DVC history?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"`dvc list -R` invalidates repo path",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-list-r-invalidates-repo-path\/551",
        "Question_created_time":1605037463923,
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":405,
        "Question_body":"<p>Given the folder structure in my <code>data\/<\/code> folder:<\/p>\n<pre><code class=\"lang-auto\">nested_start\n\u251c\u2500\u2500 folder_1\n\u2502   \u251c\u2500\u2500 folder_a\n\u2502   \u2502   \u251c\u2500\u2500 file_0.txt\n\u2502   \u2502   \u2514\u2500\u2500 file_1.txt\n\u2502   \u2514\u2500\u2500 folder_b\n\u2502       \u251c\u2500\u2500 file_0.txt\n\u2502       \u2514\u2500\u2500 file_1.txt\n\u251c\u2500\u2500 folder_2\n\u2502   \u251c\u2500\u2500 folder_a\n\u2502   \u2502   \u251c\u2500\u2500 file_0.txt\n\u2502   \u2502   \u2514\u2500\u2500 file_1.txt\n\u2502   \u2514\u2500\u2500 folder_b\n\u2502       \u251c\u2500\u2500 file_0.txt\n\u2502       \u2514\u2500\u2500 file_1.txt\n\u2514\u2500\u2500 folder_3\n    \u251c\u2500\u2500 folder_a\n    \u2502   \u251c\u2500\u2500 file_0.txt\n    \u2502   \u2514\u2500\u2500 file_1.txt\n    \u2514\u2500\u2500 folder_b\n        \u251c\u2500\u2500 file_0.txt\n        \u2514\u2500\u2500 file_1.txt\n<\/code><\/pre>\n<p>Generated by the following Python script:<\/p>\n<pre><code class=\"lang-python\">import itertools\n\nfrom pathlib import Path\n\nstart = Path(\"data\", \"nested_start\")\nstart.mkdir(exist_ok=True, parents=True)\n\nfor f1, f2 in itertools.product((\"1\", \"2\", \"3\"), (\"a\", \"b\")):\n    final_folder = Path(start, f\"folder_{f1}\", f\"folder_{f2}\")\n    final_folder.mkdir(exist_ok=True, parents=True)\n\n    for file_idx in range(2):\n        with (final_folder \/ f\"file_{file_idx}.txt\").open(\"w\") as fi:\n            fi.write(f\"{f1} {f2} {file_idx}\\n\")\n<\/code><\/pre>\n<p>After adding and pushing, I can list this folder with:<\/p>\n<pre><code class=\"lang-auto\">dvc list . data\/nested_start\n<\/code><\/pre>\n<p>However, I cannot list it recursively.<\/p>\n<pre><code class=\"lang-auto\">$ dvc list -R . data\/nested_start\nERROR: failed to list '.' - The path 'data\/nested_start' does not exist in the target repository '.' neither as a DVC output nor as a Git-tracked file. \n<\/code><\/pre>\n<p>Why does <code>-R<\/code> make my repository invalid?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Working with a small subset of remote data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/working-with-a-small-subset-of-remote-data\/541",
        "Question_created_time":1604100899517,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":1078,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m trying to use DVC for managing data produced from simulations.  Each simulation produces a large number of small files, and a few very large files.  I have worked out how to set up a git repository with DVC and add the data (one .dvc file per simulation), and also how to push it to remote storage.  There are too many files to have one .dvc file per file. I think this is known as the \u201cdata registry\u201d use case.<\/p>\n<p>I then want to start a new project and work on, say, one of the simulations.<br>\nI think what I want to do is to \u201cdvc import\u201d the simulation data directory into the new project.  This works, and I can import just a single simulation.  However, I would like to be able to get just a small subset of the files within a simulation, without downloading the whole simulation.<\/p>\n<p>I think what I want to be able to do is to \u201cdvc import\u201d but tell it <em>not<\/em> to actually fetch the data, and then be able to tell it which files under the simulation I want to fetch.<\/p>\n<p>Is such a thing possible?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple machines setup for one repo",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-machines-setup-for-one-repo\/539",
        "Question_created_time":1603954361775,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":644,
        "Question_body":"<p>Hi,<\/p>\n<p>First of all - thanks for developing such a great tool! We are currently incorporating it in our deep learning workflow for different computer vision projects.<\/p>\n<p>I think we got a grasp of how it all should be set up and are able to run our pipelines properly on one machine. One big question that we still can\u2019t figure out is the best approach to experiment and version control with dvc simultaneuosly on several machines.<\/p>\n<p>Create separate branches per each machine and iterate there? But what about shared data (the very first dependency in our pipeline) then? Do we just <code>dvc add data<\/code> and commit those changes in one branch (e.g. with tag). And then <code>git checkout &lt;tag&gt; data.dvc<\/code> and <code>dvc checkout data.dvc<\/code> on other machines?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Batch run rules",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/batch-run-rules\/537",
        "Question_created_time":1603811944490,
        "Question_answer_count":5,
        "Question_score_count":8,
        "Question_view_count":361,
        "Question_body":"<p>Hello! I read tutorial and have a question about DVC possibilities. Using <code>make<\/code> one can specify a rule in <code>Makefile<\/code> which can applied to all files which names satisfy some template. Is it possible in DVC? I\u2019d like to batch process of a large number of files once and then re-process only new\/updated files<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC with git worktree",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-git-worktree\/532",
        "Question_created_time":1602863664967,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":416,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019ve recently started using git worktree to practice parallel development. It comes very handy when I want to work on two branches in the same repo at the same time.<\/p>\n<p>However, I\u2019m came across issue with adding data to DVC on a tree other than the original dir.<\/p>\n<p>I wonder if anyone has experience on this?<\/p>\n<p>Many thanks,<br>\nMaggie<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error with DVC run in windows docker container",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-with-dvc-run-in-windows-docker-container\/530",
        "Question_created_time":1602854305057,
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":512,
        "Question_body":"<p>Hi everyone,<\/p>\n<p>I am currently getting an error when running:<\/p>\n<p><code>dvc run -n prepare -d .\/data\/raw -d .\/scripts\/prepare.py -o .\/data\/seed-detection\/interim -o .\/data\/seed-detection\/processed --no-commit python3 scripts\/prep are.py --config config\/config.yaml<\/code><\/p>\n<p>The error I get is:<\/p>\n<p><code>ERROR: unexpected error - disk I\/O error<\/code><\/p>\n<p>I\u2019m running this inside a container on windows where my cwd is a mounted volume.<\/p>\n<p>The strange thing is, when I only run the command without using DVC it runs fine, so the problem lies with DVC. Running with <code>--no-exec<\/code> option also fails.<\/p>\n<p>Currently only workaround I can think of is manually making <code>dvc.yaml<\/code>, then execute the command by itself and then commit DVC. However, I would not want to do this every time I want to run the stage.<\/p>\n<p>I\u2019m using Python 3.6.9 on Linux-4.19.76-linuxkit-x86_64-with-Ubuntu-18.04-bionic<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Use previous version of model as initial weights",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/use-previous-version-of-model-as-initial-weights\/525",
        "Question_created_time":1602183567290,
        "Question_answer_count":8,
        "Question_score_count":3,
        "Question_view_count":526,
        "Question_body":"<p>I have a pipeline the output of which is a model tracked with dvc and lets say it is tagged v0.1.0. I would like to use the weights of  this model as the initial weights in training a new model. I was wondering what the best way to achieve this would be?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Define pipelines in multiple files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/define-pipelines-in-multiple-files\/524",
        "Question_created_time":1602014302887,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":717,
        "Question_body":"<p>I have a repo with a bunch of related pipelines (sharing stages, in some cases).  The <code>dvc.yaml<\/code> file is getting very long.  Is there any way to split the pipelines into multiple <code>yaml<\/code> files?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Comparison to DataLad and git-annex",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/comparison-to-datalad-and-git-annex\/92",
        "Question_created_time":1536601415671,
        "Question_answer_count":4,
        "Question_score_count":7,
        "Question_view_count":2028,
        "Question_body":"<p>What are the main differences between DVC and DataLad\/git-annex ( <a href=\"https:\/\/www.datalad.org\/\" rel=\"nofollow noopener\">https:\/\/www.datalad.org\/<\/a> , <a href=\"https:\/\/git-annex.branchable.com\/\" rel=\"nofollow noopener\">https:\/\/git-annex.branchable.com\/<\/a> )?  What would be reasons to use one or the other?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using DVC for non-machine learning models",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-for-non-machine-learning-models\/519",
        "Question_created_time":1601604768780,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":507,
        "Question_body":"<p>Hi all,<\/p>\n<p>I work with simulation models requiring large input datasets and producing similarly large outputs. I\u2019m trying to see if DVC would be an appropriate tool for version control of these datasets. My use case is somewhat different that that of a machine learning developer, I think.<\/p>\n<p>An example would be a global climate model that can be run for different future scenarios of greenhouse gas emissions, or for different regions in the world. Each scenario\/region is associated with a specific (set of) input file(s). The names of file or directories containing the data for each scenario may be different.<\/p>\n<p>These input datasets are not (necessarily) tied to specific versions of the code. In principle one could make a Git branch for each scenario, but that would be hassle, and would potentially result in a great many branches. So what I\u2019m looking for is the ability to checkout a specific dataset, without checking out all the datasets for all scenarios, and without having to switch to different Git branch or commit.<\/p>\n<p>Obviously, adding new datasets representing new scenarios would require a Git commit\u2013that\u2019s OK.<\/p>\n<p>I\u2019ve been trying DVC and looking at the documentation but so far I haven\u2019t figured out if this is possible and if so, how do do this.<\/p>\n<p>Thanks in advance,<\/p>\n<p>Maarten<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Workflow on slurm-like clusters",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/workflow-on-slurm-like-clusters\/484",
        "Question_created_time":1599401341769,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":1364,
        "Question_body":"<p>Hi,<br>\nI have access to a  <a href=\"https:\/\/slurm.schedmd.com\/overview.html\" rel=\"nofollow noopener\">slurm<\/a> (gpu) cluster and my current workflow is quite a mess. Is there any advice or experience with that? So some problems I see:<\/p>\n<ul>\n<li>\n<p>you have to submit jobs:<\/p>\n<ul>\n<li>jobs are scheduled, delayed (sometimes hours)<\/li>\n<li>jobs can run in parallel<\/li>\n<li>you have to \u201cwait\u201d till the last job started, before you can queue new jobs<\/li>\n<li>manual \u201ccommitting\u201d of results<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>docker unfriendly<\/p>\n<\/li>\n<li>\n<p>testing the whole pipeline from data processing to \u201cresult management\u201d  is hard<\/p>\n<\/li>\n<li>\n<p>data security: everybody has access to everything:<br>\nThere are tools like <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noopener\">rclone<\/a> and <a href=\"https:\/\/github.com\/facebookresearch\/CrypTen\" rel=\"nofollow noopener\">crypten<\/a>. Can dvc help with this problem or interact with those tools?<br>\nCan dvc \/ cml  help there to automate things?<br>\nThanks in advance. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/grinning.png?v=9\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\"><\/p>\n<\/li>\n<\/ul>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Add dataset metadata in .dvc file?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-dataset-metadata-in-dvc-file\/494",
        "Question_created_time":1600251171227,
        "Question_answer_count":7,
        "Question_score_count":3,
        "Question_view_count":1596,
        "Question_body":"<p>Dear Support,<\/p>\n<p>I am using DVC to track different versions of a dataset consisting of annotations on a NER dataset. In particular, I would like to keep track of some dataset metadata which may change from one version to another, and I would also like to quickly compare different versions of the dataset through the evolution of these metadata.<\/p>\n<p>The current solution I am adopting is to track the information on the dataset in a README.md but I would like to keep the link with the dataset file more clear than that.<\/p>\n<p>How to do that? I thought about adding this metadata to the <code>dataset.dvc<\/code> file that is automatically generated, but I am not sure that this is a good idea: would a future <code>dvc add<\/code> overwrite\/delete that for instance?<\/p>\n<p>Thank you in advance for your help!<\/p>\n<p>Best,<\/p>\n<p>\u2014Francesco<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Declare python version and packages versions as dependencies",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/declare-python-version-and-packages-versions-as-dependencies\/492",
        "Question_created_time":1599834464519,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":306,
        "Question_body":"<p>Hello!<\/p>\n<p>I recently started using DVC and I found the tutorials very useful and clear. However I still have a question about a use case of <code>dvc run<\/code>.<\/p>\n<p>I know that one can declare parameters and files to be dependencies of a pipeline. However, in most cases also<\/p>\n<ul>\n<li>the python version<\/li>\n<li>the python packages versions<br>\nare fundamental dependencies.<\/li>\n<\/ul>\n<p>To give an example, running the same pipeline (= same files, same parameters) with python 2.7 or python 3.x can give <a href=\"https:\/\/sebastianraschka.com\/Articles\/2014_python_2_3_key_diff.html#integer-division\" rel=\"nofollow noopener\">different results<\/a>. In the same way, if we have loaded <code>tensorflow 1.0<\/code> or <code>2.0<\/code>, the results may vary.<\/p>\n<p>So, I would like the pipeline to be re-run even if the python version or packages versions have changed and anything else has remained the same.<\/p>\n<p>How can I keep track of this kind of special dependencies for my pipeline?<\/p>\n<p>Thank you in advance!<\/p>\n<p>Best,<\/p>\n<p>\u2014Francesco<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How use DVC with Azure Blob Storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-use-dvc-with-azure-blob-storage\/158",
        "Question_created_time":1552940286034,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":5022,
        "Question_body":"<p>DVC supports AWS S3, Google Cloud Storage, Microsoft Azure Blob Storage. However, the instruction is only for AWS S3. How to use DVC  with Microsoft Azure Blob Storage?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc add external with gdrive",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-add-external-with-gdrive\/480",
        "Question_created_time":1599067760059,
        "Question_answer_count":3,
        "Question_score_count":3,
        "Question_view_count":366,
        "Question_body":"<p>I\u2019m just starting out with dvc. I have a bunch of file and models that are saved on my drive. I\u2019d like to add them to dvc without having to download them and then re-adding them. I thought maybe dvc add --external gdrive:\/\/\/ but it says it can\u2019t find it. Is this a problem with my gdrive link setup (I am using a service account) or with --external not supporting gdrive?<br>\nThanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using DVC to keep track of multiple model variants",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-to-keep-track-of-multiple-model-variants\/471",
        "Question_created_time":1597760331365,
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":1757,
        "Question_body":"<p>I have to keep track of models that use the same feature engineering, the same features, the same algorithm, but a different subset of data e.g.the raw data is the same but then it\u2019s filtered to a certain group and then make a model.<\/p>\n<p>My repo structure looks like this<\/p>\n<pre><code>root\n     |- models\n                   |- model_group1\n                                            |- model.xgb\n                                            | params.json\n                   |- model_group2\n                                            |- ...\n                   |- ...\n    |- metrics\n                  |- model_group1\n                                           |- model_eval.json\n                  |- model_group2\n                                           |- ...\n                  |- ...\n<\/code><\/pre>\n<p>AFAIK DVC assumes the pipeline produces one model, which makes total sense.<br>\nMaking one model in which the group is a variant is not possible in my case.<\/p>\n<p>Any suggestions? do I need multiple DVC pipelines in the same repo?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc push to public S3 bucket",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-to-public-s3-bucket\/457",
        "Question_created_time":1595914363359,
        "Question_answer_count":7,
        "Question_score_count":4,
        "Question_view_count":3155,
        "Question_body":"<p>Hello there. I\u2019m excited to try DVC\u2019s features but I\u2019m stuck with a basic problem in S3. Ideally, I\u2019d like to push to a private S3 bucket, which I could do with <code>aws-vault<\/code>. Before doing that, I wanted to check how that would work in a public bucket. But when I try to push to a public S3, I get:<\/p>\n<pre><code>$ dvc remote list\nstorage\ts3:\/\/ml-ci\nhttps3\thttps:\/\/ml-ci.s3.amazonaws.com\/\n\n$ dvc push -r storage\nERROR: unexpected error - An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>So I thought I\u2019d try a suggestion from another thread on this forum and add a HTTPS remote endpoint instead (though I know it was suggested as a read-only solution):<\/p>\n<pre><code>$ dvc push -r https3\nERROR: failed to upload '.dvc\/cache\/a3\/04afb96060aad9017xxxx' to 'https:\/\/ml-ci.s3.amazonaws.com\/a3\/04afb96060aad9017xxxx' - could not perform a POST request\nERROR: failed to push data to the cloud - 1 files failed to upload\n<\/code><\/pre>\n<p>What am I doing incorrectly here with remote setup? Is private + <code>aws-vault<\/code> the only option? I\u2019d really appreciate some help, thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Add a remote directory without adding to the cache",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-a-remote-directory-without-adding-to-the-cache\/452",
        "Question_created_time":1595379583111,
        "Question_answer_count":16,
        "Question_score_count":2,
        "Question_view_count":2313,
        "Question_body":"<p>We have a corpus licensed from a third party that is stored in our S3 bucket.  The corpus is fixed and should never change, so versioning is not much of a concern.  I\u2019d like to integrate it into my repository so that when someone checks it out from git, they can run a simple command like <code>dvc checkout<\/code> and it will download the corpus.  However, since it is already stored in S3 and should never change, I would prefer that it doesn\u2019t get copied into our remote cache.  But, I\u2019m not sure this is really possible with <code>dvc<\/code>.<\/p>\n<p>One solution might be to use something like <code>dvc run -n get-corpus -O [local-corpus-path] dvc get-url [remote-corpus-path] [local-corpus-path]<\/code>.  Then (I think) someone can just run <code>dvc repro get-corpus<\/code> or any downstream tasks, and dvc will get the corpus.  But, will this work?  And is this the best\/recommended way?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pipeline to process only new files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipeline-to-process-only-new-files\/446",
        "Question_created_time":1594916713834,
        "Question_answer_count":2,
        "Question_score_count":3,
        "Question_view_count":535,
        "Question_body":"<p>Say I have a million files in the directory <code>.\/data\/pre<\/code>.<\/p>\n<p>I have a python script <code>process_dir.py<\/code> which goes over each file in <code>.\/data\/pre<\/code> and processes it and creates a file in the same name in a directory <code>.\/data\/post<\/code> (if such file already exists, it skips processing it).<\/p>\n<p>I defined a pipeline:<br>\n<code>dvc run -n process -d process_dir.py -d data\/pre -o data\/post python process_dir.py<\/code><\/p>\n<p>Now let\u2019s say I removed one file from <code>data\/pre<\/code>.<\/p>\n<p>When I run <code>dvc repro<\/code> it will still process all the 999,999 files again, because it will remove the entire content of the .\/data\/post directory before running the <code>process<\/code> stage. Is there any way to manage the pipeline so that <code>process.py<\/code> will not process the same file twice?<\/p>\n<p>The only way I could think of is creating <code>process_file.py<\/code> which handles a single file, and executing 1 million commands like this:<br>\n<code>dvc run -n process_1 -d process_file.py -d data\/pre\/1.txt -o data\/post\/1.txt python process_file 1.txt<\/code><br>\n<code>dvc run -n process_2 -d process_file.py -d data\/pre\/2.txt -o data\/post\/2.txt python process_file 2.txt<\/code><br>\n\u2026<br>\n<code>dvc run -n process_1000000 -d process_file.py -d data\/pre\/1000000.txt -o data\/post\/1000000.txt python process_file 1000000.txt<\/code><\/p>\n<p>Is there a more elegant way?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"`dvc pull --run-cache [target]`",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-pull-run-cache-target\/444",
        "Question_created_time":1594851186816,
        "Question_answer_count":16,
        "Question_score_count":4,
        "Question_view_count":1484,
        "Question_body":"<p>Hi!<\/p>\n<h1>Context<\/h1>\n<p>Here is my use case:<\/p>\n<ol>\n<li>I train a model automatically through whatever way (CML, Airflow)<\/li>\n<li>Now I have a <code>weights.db<\/code> file, which is versioned by DVC alongside other intermediary data files<\/li>\n<li>I do a <code>dvc push --run-cache<\/code> as I do not want to version with <code>dvc.lock<\/code> since I am not committing anything<br>\n\u2013&gt; Now there are several files in my S3.<br>\n\u2013&gt; Time to run my model in prod<\/li>\n<li>In a Docker container, run <code>dvc pull --run-cache generate_weights<\/code> (the stage that created <code>weights.db<\/code>)<\/li>\n<li>DVC starts to pull <strong>all files<\/strong> from remote, not only previous <code>weights.db<\/code> versions, but also caches from other stages.<\/li>\n<\/ol>\n<p>This makes my approach unfeasible as pulling all files from remote takes a long time.<\/p>\n<h1>Question(s)<\/h1>\n<ol>\n<li>Should <code>dvc pull --run-cache [target]<\/code> pull <em>all files<\/em>?<\/li>\n<li>Should I be doing it the way I am doing? I am avoiding CML\/Git-related solutions.<\/li>\n<\/ol>\n<p>Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practices for collaborating with DVC",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-for-collaborating-with-dvc\/449",
        "Question_created_time":1595014568166,
        "Question_answer_count":6,
        "Question_score_count":8,
        "Question_view_count":1406,
        "Question_body":"<p>Hello all!<\/p>\n<p>I am new to dvc and I am trying to figure out how to fit it into our workflows.<\/p>\n<p>The first use case that I am trying to figure out is collaborating on building a dataset, particularly with trainees or collaborators of unknown skill. (Imagine we are all working on labelling new data).<\/p>\n<p>What are best practices for reviewing the trainees work before allowing them to do a dvc push?<\/p>\n<p>Currently we use gitlab UI to require merge reviews for all code changes, and this allows me to visually inspect all changes before allowing a merge. What is the equivalent for dvc?<\/p>\n<p>Thanks for any input!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Fill-back metrics",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/fill-back-metrics\/441",
        "Question_created_time":1594737345910,
        "Question_answer_count":3,
        "Question_score_count":4,
        "Question_view_count":369,
        "Question_body":"<p>Say I am maintaining some data using dvc, and at some point decide I want to have a metric showing some data statistics (i.e. how many positive samples I have). Can I back-fill this metric to previous commits?  (the goal is to track the number of positives I had after each data-commit).<\/p>\n<p>If the commit history is A-&gt;B-&gt;C, I know I can go back to any commit and run a pipeline, but the metric output of this pipeline will need to be saved in a different commit, right?  So I will have to create new git commits: A-&gt;A\u2019  B-&gt;B\u2019, C-&gt;C\u2019 that will store the metric results, or is there a different way?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Trouble modifying and saving dvc data file which lives outside the repo",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trouble-modifying-and-saving-dvc-data-file-which-lives-outside-the-repo\/435",
        "Question_created_time":1594430779787,
        "Question_answer_count":22,
        "Question_score_count":1,
        "Question_view_count":2347,
        "Question_body":"<p>I have a dvc setup where the git\/dvc repo, the data-file directory and the dvc-cache directory are all peers.<br>\ne.g.<br>\nmy-test-repo<br>\ntest-files<br>\nmy-test-dvc-cache<br>\nare all directories at the same level<\/p>\n<p>All the files under test-files were added using dvc add test-files<\/p>\n<p>my-test-repo\/test-files.dvc contains this line:<br>\npath: \u2026\/test-files<\/p>\n<p>.\/dvc\/config contains this line<br>\n[cache]<br>\ndir = \u2026\/\u2026\/my-test-dvc-cache<\/p>\n<p>I did a dvc push and all the file went to the specified remote.<br>\nI did a git clone and a dvc pull (on several different machines) and all the files came down in the directory structure specified above.<br>\nCode which runs from the repo and uses the files works.<\/p>\n<p>However, when I modify a data  file, I am having trouble saving it.<br>\nI changed a file in the test-files directory and dvc status shows that test-files is modified.<br>\n$ dvc status<br>\nunit_test_input.dvc:<br>\nchanged outs:<br>\nmodified:           \u2026\\test-files<br>\nchanged checksum<\/p>\n<p>When I do dvc commit it gives this error message:<br>\n$ dvc commit<br>\nERROR: failed to commit - unable to commit changed stage: \u2018test-files.dvc\u2019. Use <code>-f|--force<\/code> to force.<\/p>\n<p>So I entered dvc commit -f and it complains about files outside of the repo.<br>\nI read that  it is ok to have files outside the repo and the original push and pull operations worked fine.<\/p>\n<p>ERROR: unexpected error - Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\test-files<br>\nstderr: \u2018fatal: C:\\test-files: \u2018C:\\test-files\u2019 is outside repository at \u2018C:\\test-files\/my-test-repo\u2019\u2019<\/p>\n<p>It seems like dvc doesn\u2019t care that the data files are outside the repo, but the commit command is try to perform git commands on those files and git doesn\u2019t like them being outside the repo.<\/p>\n<p>Is that what\u2019s going on?<br>\nIs there something I can do about it?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Recovering pushed files after losing .dvc reference",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/recovering-pushed-files-after-losing-dvc-reference\/428",
        "Question_created_time":1594065943801,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":938,
        "Question_body":"<p>Let\u2019s say I\u2019m using DVC and I:<\/p>\n<ol>\n<li><code>dvc add my_folder\/<\/code><\/li>\n<li><code>dvc push<\/code><\/li>\n<li>Delete <code>my_folder.dvc<\/code>\n<\/li>\n<li>Delete all DVC cache contents<\/li>\n<\/ol>\n<p>Without the reference provided by <code>my_folder.dvc<\/code> and no backup using <code>git<\/code>, is it still possible for me to see if <code>my_folder<\/code> has been uploaded and list it\u2019s contents? Once I have listed this folder and some of it\u2019s details, can I also download it\u2019s contents?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best Practice for CI with Run Cache?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-ci-with-run-cache\/427",
        "Question_created_time":1594029291581,
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":748,
        "Question_body":"<p>I\u2019m having trouble understanding how to use the new <a href=\"https:\/\/dvc.org\/blog\/dvc-1-0-release#run-cache\" rel=\"nofollow noopener\">run cache<\/a> in a continuous integration context.<\/p>\n<p>From what I gathered it\u2019s supposed to work as follows:<\/p>\n<ul>\n<li>\n<code>git commit<\/code> &amp; <code>git push<\/code> some changes to my code<\/li>\n<li>The CI-Pipeline is started, it executes a <code>dvc repro<\/code> and trains the new model<\/li>\n<li>After model training, the CI-Pipeline executes a <code>dvc push --run-cache<\/code>\n<ul>\n<li>The newly trained model, and all other outputs tracked by dvc are pushed to the dvc remote<\/li>\n<\/ul>\n<\/li>\n<li>On my local machine, I execute <code>dvc pull --run-cache<\/code> (without any changes to my local <code>dvc.lock<\/code> file)\n<ul>\n<li>I\u2019m expecting the newly trained model and all other dvc outputs to appear in my working directory?<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>But instead of the files from the run cache appearing in my working dir, I still get the outputs corresponding to my (old, local) <code>dvc.lock<\/code> file.<\/p>\n<p>Am I missing something here, or am I using the commands wrong?<br>\nIn case my expectations are correct I can try to provide a minimal example or more debug information for my current setup.<\/p>\n<p>When I manually copy the (new) <code>dvc.lock<\/code> from the runner and paste it into my local machine, I\u2019m able to <code>dvc pull<\/code> from the dvc remote and get the expected outputs.<br>\nBut from what I understood, it shouldn\u2019t be necessary anmyore to <code>git add dvc.lock &amp;&amp; git commit<\/code> from inside the runner for every experiment?<\/p>\n<p>Thanks in advance,<br>\nRabefabi<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Remove\/Redefine Stage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/remove-redefine-stage\/411",
        "Question_created_time":1592745757708,
        "Question_answer_count":12,
        "Question_score_count":2,
        "Question_view_count":1699,
        "Question_body":"<p>I made a mistake by defining a stage while forgetting to define its output with the <code>-o option<\/code>.  I tried removing the existing stage with <code>dvc remove pipelines.yaml:my-stage-name -p<\/code>, but this does not remove the stage from <code>pipelines.yaml<\/code>.  I eventually manually deleted the stage from <code>pipelines.yaml<\/code>, and created a new stage with the same name using <code>dvc run ...<\/code>.  My questions are:<\/p>\n<ol>\n<li>\n<p>What\u2019s the recommended way to redefine a stage?<\/p>\n<\/li>\n<li>\n<p>Is manually removing the stage from <code>pipelines.yaml<\/code> sufficient to completely delete the stage, or will there still be metadata about the stage elsewhere?<\/p>\n<\/li>\n<\/ol>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Raw data from google big query",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/raw-data-from-google-big-query\/400",
        "Question_created_time":1591031860157,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":1142,
        "Question_body":"<p>so I have created a dvc to create a local raw data file based on a query of a gbq database.<br>\nnow I want to add this file to dvc for tracking so if the database will not be available in the future I am fully reproducible.<br>\nWhen I do <code>dvc add<\/code> I get the following error:<br>\ndata.csv is specified as an output in more than one stage:<br>\nraw_data.dvc<br>\nThis is not allowed. Consider using a different output name.<\/p>\n<p>What am I doing wrong?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Non-overlapping outs paths",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/non-overlapping-outs-paths\/396",
        "Question_created_time":1590514518218,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":1167,
        "Question_body":"<p>My working directory contains the following folders:<\/p>\n<ul>\n<li>.dvc<\/li>\n<li>.git<\/li>\n<li>data<\/li>\n<li>models<\/li>\n<li>src<\/li>\n<\/ul>\n<p>And files:<\/p>\n<ul>\n<li>.gitignore<\/li>\n<li>data.dvc<\/li>\n<\/ul>\n<p>The <code>data.dvc<\/code> file currently points to all the contents of the data folder. The src folder contains a <code>prepare_data.py<\/code> file that takes in images from both the data\/train and data\/test folders as inputs and then outputs four files:<\/p>\n<ul>\n<li>data\/train\/imgs_train.npy<\/li>\n<li>data\/train\/train_labels.pkl<\/li>\n<li>data\/test\/imgs_test.npy<\/li>\n<li>data\/test\/test_labels.pkl<\/li>\n<\/ul>\n<p>Now, I want to create a reproducible stage for <code>src\/prepare_data.py<\/code>. To do this, I ran the following command:<\/p>\n<pre><code class=\"lang-auto\">dvc run -f prepare_data.dvc \\\n        -d src\/prepare_data.py -d data\/train -d data\/test \\\n        -o data\/train\/imgs_train.npy -o data\/train\/train_labels.pkl \\\n        -o data\/test\/imgs_test.npy -o data\/test\/test_labels.pkl \\\n        python src\/prepare_data.py\n<\/code><\/pre>\n<p>However, I received the following error message:<\/p>\n<pre><code class=\"lang-auto\">ERROR: failed to run command - Paths for outs:\n'data'('data.dvc')\n'data\\train\\imgs_train.npy'('prepare_data.dvc')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>I can see that there is a problem with referencing data\/train and data\/test when the data folder has already been tracked using the <code>data.dvc<\/code> file. However, I still want to create the reproducible stage, so any suggestions?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC integration with AZURE ML Pipeline and versioning IOT data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-integration-with-azure-ml-pipeline-and-versioning-iot-data\/364",
        "Question_created_time":1587729946039,
        "Question_answer_count":12,
        "Question_score_count":0,
        "Question_view_count":2122,
        "Question_body":"<p>I have two questions.<\/p>\n<ol>\n<li>My data storage is Azure blob which contain parquet files. These parquet files are output of preprocessing IOT data which means data keep coming in storage blobs. How to version data with dvc because with every change dvc will create a new version and then i will end up in many many data versions.<br>\nPlease help me in understanding how people integrate dvc with continuous data.<\/li>\n<li>I am using Azure ML env. for ML pipeline. How i can integrate dvc with Azure ML env.<br>\nThanks in advance.<\/li>\n<\/ol>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking changes in params.yml",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-changes-in-params-yml\/394",
        "Question_created_time":1589955027350,
        "Question_answer_count":2,
        "Question_score_count":4,
        "Question_view_count":561,
        "Question_body":"<p>I\u2019m trying to better understand how <code>params.yaml<\/code> works and in particular how changes in it are tracked. <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> suggested <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/563406153334128681\/710529764628103188\" rel=\"nofollow noopener\">here<\/a> to track <code>params.yaml<\/code> using <code>git<\/code>. So far so go. Now, when inspecting a <code>.dvc<\/code> generated by <code>dvc run -p ...<\/code> I see that the parameters from the <code>params.yaml<\/code> are <em>not<\/em> associated with <code>MD5<\/code>. This made me wonder: if I\u2019m changing a parameter in the YAML (and commit the change to git), how <code>dvc<\/code> will know about it? How will a <code>dvc repro ...<\/code> know that a parameter changed?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dealing with large numbers of small files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dealing-with-large-numbers-of-small-files\/362",
        "Question_created_time":1587658350621,
        "Question_answer_count":8,
        "Question_score_count":5,
        "Question_view_count":1923,
        "Question_body":"<p>A project I\u2019m working on has many small files, which don\u2019t work well with DVC. Instead, I zip up all the files and push them remotely. However, it would be nice to know if I\u2019ve modified any of the files in the zipped folder and diff them with the remote. Am I supposed to be using a pipeline to accomplish this? Is this just a silly goal to have?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Re-run changed final stage on previous model versions",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/re-run-changed-final-stage-on-previous-model-versions\/389",
        "Question_created_time":1589717438826,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":382,
        "Question_body":"<p>Apologies if this has been asked before. Lets say I have a pipeline which finally outputs some metrics. I have used <code>git tag<\/code> to checkpoint several model versions. Then I decide I am not using the correct metric so change the final stage which generates the metrics. I would then like to run this updated final stage for previous model versions, without having to retrain those models. What would be the best way achieve this?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dvc metrics diff on metrics stored in dvc-cache?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-metrics-diff-on-metrics-stored-in-dvc-cache\/386",
        "Question_created_time":1589548601677,
        "Question_answer_count":2,
        "Question_score_count":4,
        "Question_view_count":806,
        "Question_body":"<p>Hello!<\/p>\n<p>Does the <code>dvc metrics diff ref_a ref_b<\/code> command work on metrics which are tracked <em>not<\/em> by git, but instead by dvc?<\/p>\n<p>Our current setup is the following:<\/p>\n<ul>\n<li>We have a dvc-pipeline-stage, which produces a <code>eval.json<\/code>, which is cached by dvc<br>\n<code> dvc run -d dep1 -d dep2 -m eval.json  -f data\/evaluation.dvc python my_script.py<\/code>\n<ul>\n<li>According to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#options\" rel=\"nofollow noopener\">docs<\/a>, the lowercase <code>-m<\/code> puts <code>eval.json<\/code> on the gitignore and into the dvc cache<\/li>\n<\/ul>\n<\/li>\n<li>Our model is trained remotely via a CI-Pipeline, which after a successful run executes a <code>git add . &amp;&amp; git commit &amp;&amp; git push &amp;&amp; dvc push<\/code>\n<\/li>\n<li>On our local machines then we execute a <code>git pull &amp;&amp; dvc pull<\/code>. We now have the trained model and our <code>eval.json<\/code> locally available<\/li>\n<li>When we now execute <code>dvc metrics diff some_other_commit_hash <\/code> the command does not find the <code>eval.json<\/code> for the <code>some_other_commit_hash<\/code>:<pre><code class=\"lang-auto\">$ dvc metrics diff 5f036012bc9d35b7240eab3b7a42792093612ba8 \nWARNING: Metrics file 'data\/evaluation\/eval.json' does not exist at the reference '5f036012bc9d35b7240eab3b7a42792093612ba8'.\n        Path               Metric          Value                Change      \ndvc_pipeline\/data\/evaluation   f1       0.11549247896660152   diff not supported\n\/eval.json                                                                      \ndvc_pipeline\/data\/evaluation   acc      0.6784822940826416    diff not supported\n\/eval.json                                                                      \ndvc_pipeline\/data\/evaluation   loss     1.1063932177428895    diff not supported\n\/eval.json                                                                      \ndata\/evaluation\/eval.json      f1       0.43913782532829426   diff not supported\ndata\/evaluation\/eval.json      acc      0.750374436378479     diff not supported\ndata\/evaluation\/eval.json      loss     0.6777220140143865    diff not supported\n<\/code><\/pre>\n<\/li>\n<li>I\u2019m assuming this command fails, since (I\u2019m guessing) dvc looks for the <code>eval.json<\/code> in the git-cache, but not in the dvc-cache (where it is stored)<\/li>\n<\/ul>\n<p>Is there a flag, to tell <code>dvc metrics diff ...<\/code> that the metrics-file is stored in the dvc cache? Or is this expected to work, and we have an error somewhere else?<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How cache is maintained for big data size locally",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-cache-is-maintained-for-big-data-size-locally\/217",
        "Question_created_time":1568253347446,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":930,
        "Question_body":"<p>Hi,<br>\nI have a VM on which my local git repo exists. On top of that i have installed dvc on same machine. Now when i add data to dvc it will be in dvc cache and on git push, commit same data will go to git repo as well. Is my understanding correct? If yes then their will be two copies of data and size will keep increasing as data grows. I am not using any remote repo for data.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Move a .dvc stage without re-run",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/move-a-dvc-stage-without-re-run\/384",
        "Question_created_time":1589374711255,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":486,
        "Question_body":"<p>I just ran a <code>dvc run ...<\/code> but forgot to specify the <code>-f<\/code> flag so it created a <code>.dvc<\/code> file in an unintended location. Is there a smart way of moving it without re-running the stage? This method should take care of location and relative paths <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sync S3 remote bucket auth w\/GitHub repo access settings",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/sync-s3-remote-bucket-auth-w-github-repo-access-settings\/365",
        "Question_created_time":1587768239117,
        "Question_answer_count":4,
        "Question_score_count":3,
        "Question_view_count":587,
        "Question_body":"<p>Hi folks,<\/p>\n<p>My org is a happy GitHub user and we\u2019re looking to adopt DVC. Question: has anyone attempted a way to sync a GitHub repo\u2019s access settings with an S3 bucket? I\u2019d like to avoid having to replicate the same access settings used on every GitHub repo w\/its associated DVC S3 remote AWS permissions.<\/p>\n<p><strong>edit: similar to how <a href=\"https:\/\/help.github.com\/en\/github\/managing-large-files\/collaboration-with-git-large-file-storage\" rel=\"nofollow noopener\">LFS auth appears to \u201cjust work\u201d<\/a> when used w\/Gitub.<\/strong><\/p>\n<p>I\u2019m referring to the \u201cManage Access\u201d area of a GitHub repo:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a.png\" data-download-href=\"\/uploads\/short-url\/foQOK0UABlsex4GkRdJJ5CU3UT0.png?dl=1\" title=\"Manage_access\" rel=\"nofollow noopener\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_690x467.png\" alt=\"Manage_access\" data-base62-sha1=\"foQOK0UABlsex4GkRdJJ5CU3UT0\" width=\"690\" height=\"467\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_690x467.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_1035x700.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_1380x934.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Manage_access<\/span><span class=\"informations\">1504\u00d71018 70.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to manage files on remote repository",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-manage-files-on-remote-repository\/372",
        "Question_created_time":1588035461752,
        "Question_answer_count":3,
        "Question_score_count":3,
        "Question_view_count":744,
        "Question_body":"<p>Hi there,<\/p>\n<p>First of all thanks for the great tool that really help manage ML practice!<br>\nRight now I\u2019m setup repo in my local pc share folder and tried dvc for a while, my main usage now is to datafile versioning<\/p>\n<p>My problem is how to handle the growth of files in the repository. It is one of showstopper for me to promote to team or larger scale usage \u2026<\/p>\n<p>If there is any mechanism to support keeping track latest 5 versions of file \u201cX\u201d or any statistic could tell how many files we have, how many version we save in repo? Checked tracked data files on repo they are compressed with hashed filename. It is hard to convert to back filename for analytics.<\/p>\n<p>Any tool or better way to handle that? thanks in advance<\/p>\n<p>Donald<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Processing data in place",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/processing-data-in-place\/361",
        "Question_created_time":1587642959337,
        "Question_answer_count":8,
        "Question_score_count":5,
        "Question_view_count":957,
        "Question_body":"<p>Hi I\u2019m new to dvc (nice project!)<br>\nWe have a larger dataset and have several preprocessing scripts.<br>\nThese scripts alter data in place.<br>\nIt seems when I try to register it with <code>dvc run<\/code> it complains about cyclic dependencies (input is the same as output).<br>\nI would assume this is a very common use case.<br>\nWhat is the best practice here ?<br>\nTried to google around but i did not see any solution to this (besides creating another folder for the output).<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practice for python package dependency?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-python-package-dependency\/358",
        "Question_created_time":1587641178959,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":695,
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m figuring out how to structure my project and found your tutorials a very helpful introduction.<\/p>\n<p>In them, however, you only rely on a single script for each stage, for example:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d code\/xml_to_tsv.py -d data\/Posts.xml -o data\/Posts.tsv \\\n          -f prepare.dvc \\\n          python code\/xml_to_tsv.py data\/Posts.xml data\/Posts.tsv\n<\/code><\/pre>\n<p><a href=\"https:\/\/dvc.org\/doc\/tutorials\/pipelines\" rel=\"nofollow noopener\">Source<\/a><\/p>\n<p>Now my understanding is the following:<br>\nIf inside of <code>xml_to_tsv.py<\/code> are imports of some other of my libraries (e.g., a <code>my_special_xml_importer.py<\/code>), and I make changes to <code>my_special_xml_importer.py<\/code>, these changes would not be picked up by dvc, since <code>my_special_xml_importer.py<\/code> is not an explicit dependency of the stage, correct?<\/p>\n<p>What\u2019s the best practice here for bigger projects, where each DVC stage is not just contained in a single script?<\/p>\n<p>Our use case will be the following: For each stage we\u2019ll be having a jupyter notebook, which will import some of our python packages. I\u2019m assuming I should create a stage like this:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d my_notebook.ipynb -d code\/my_lib.py -d data\/Posts.xml -o data\/Posts.tsv\n  -f prepare.dvc\n   papermill my_notebook.ipynb my_notebook_out.ipynb\n<\/code><\/pre>\n<p>Is this a good way, are there other ways, how are people with bigger projects dealing with this issue?<\/p>\n<p>Thanks in advance,<br>\nFabi<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Public read-only s3 remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/public-read-only-s3-remote\/355",
        "Question_created_time":1587508457422,
        "Question_answer_count":4,
        "Question_score_count":8,
        "Question_view_count":1845,
        "Question_body":"<p>Hey folks,<\/p>\n<p>The DVC <a href=\"https:\/\/github.com\/iterative\/example-get-started\" rel=\"nofollow noopener\">getting started example<\/a> has a read-only HTTP remote.<\/p>\n<p>Is it possible to do the same with S3 so the public can have read-only permission? I believe I\u2019ve set my bucket permissions correctly* (I can download w\/o AWS creds), but when I try <code>dvc pull<\/code> w\/o creds I get an error:<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull\nERROR: unexpected error - Unable to locate credentials \n<\/code><\/pre>\n<p>It looks like <code>dvc pull<\/code> is checking for credentials regardless.<\/p>\n<ul>\n<li>\n<ul>\n<li>allowing s3:GetObject and s3:GetObject<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>If this isn\u2019t possible, does anyone have a suggestion on the easiest approach to allow read-only access to DVC data analogous to a public GitHub repo?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to set up DVC for hive dataset",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-set-up-dvc-for-hive-dataset\/345",
        "Question_created_time":1585938484370,
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":538,
        "Question_body":"<p>Hi Team,<br>\nCould you some help how to setup DVC on hive tables. its static data, refreshed daily basis.<\/p>\n<p>Thanks<br>\nJones J<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Run pipeline from a stage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/run-pipeline-from-a-stage\/332",
        "Question_created_time":1582834241776,
        "Question_answer_count":2,
        "Question_score_count":3,
        "Question_view_count":490,
        "Question_body":"<p>How do I force dvc repro to run the pipeline from a specific stage?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"API call to read DVC file fails",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/api-call-to-read-dvc-file-fails\/327",
        "Question_created_time":1582067621620,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":1004,
        "Question_body":"<p>How do programmatically get a file URI so I can read it? I\u2019ve created a simple git repo with following steps from  Katacode tutorial:<\/p>\n<blockquote>\n<p>dvc init<br>\ndvc add file.txt<br>\ngit add file.txt.dvc<br>\ngit add .gitignore<\/p>\n<\/blockquote>\n<p>Call to api.get_url  returns a non-existent URI with <em><a href=\"https:\/\/github.com\/amesar\/dbws2\" rel=\"nofollow noopener\">https:\/\/github.com\/amesar\/dbws2<\/a><\/em> and <em>file.txt<\/em> as arguments.<br>\nBut call with <em><a href=\"https:\/\/github.com\/iterative\/example-get-started\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/example-get-started<\/a><\/em> and <em>model.pkl<\/em> works.<\/p>\n<p>What am I missing? Would be super useful to add doc-strings to api.py. For example, what is the \u201crev\u201d argument? Git commit hash?<\/p>\n<pre><code>repo = sys.argv[1]\npath = sys.argv[2]\nuri = api.get_url(path, repo=repo)\nrsp = requests.get(uri)\nwith open(f\"out\/{path}\", 'wb') as f:\n    f.write(rsp.content)<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Partial download data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/partial-download-data\/322",
        "Question_created_time":1582013387314,
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":673,
        "Question_body":"<p>I pushed N images and meta info to remote repository, then remove all data locally.<\/p>\n<p>After some time my colleague want download this dataset, but not fully - he filter meta info and he knows paths to necessary images.<\/p>\n<p>Is there a way download part of data from remote repository?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Importing from a git repo, then pulling",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/importing-from-a-git-repo-then-pulling\/320",
        "Question_created_time":1581796222088,
        "Question_answer_count":2,
        "Question_score_count":5,
        "Question_view_count":662,
        "Question_body":"<p>Hi all!<\/p>\n<p>I\u2019ve got a dvc repo which <code>dvc import<\/code>s a directory from another normal git repo. This works fine.<\/p>\n<p>However if I clone the dvc repo (or do a <code>git clean -xdf<\/code> to simulate) then run <code>dvc pull<\/code>, I can\u2019t pull the imports properly. I notice that they\u2019re not in my remote: presumably that\u2019s because dvc is getting them from the original git repo url instead which is fine.<\/p>\n<p>The error I get is:<\/p>\n<pre><code>$ dvc pull imported_steps\/step_A.dvc \nERROR: unexpected error - 'ExternalGitRepo' object has no attribute 'cache'\n<\/code><\/pre>\n<p>The associated .dvc file is<\/p>\n<pre><code>md5: 89ee29a257a80e53398e0927103f9c40\nlocked: true\ndeps:\n- path: src\n  repo:\n    url: \/path\/to\/original\/repo\/step_A\n    rev_lock: 4244a25681d4e9dcbd9795e9406c9d0734dac3f9\nouts:\n- md5: d3a51ab352355f4f39f5adf02bcd698a.dir\n  path: step_A\n  cache: true\n  metric: false\n  persist: false<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can and how DVC work with shared network storage?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-and-how-dvc-work-with-shared-network-storage\/26",
        "Question_created_time":1526441315809,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":1161,
        "Question_body":"<p>We have developers working with data shared on a local network, and I\u2019d like to understand whether\/how dvc could integrate with this pipeline. I think I\u2019m asking whether its possible (or even makes sense) to have a single, shared cache \u2013 kinda like the dvc cloud workflow you describe but without push\/pull. The code just reads\/writes data outside the git repo.<\/p>\n<p>Another reason I\u2019d like to leave data outside the repo is because many projects have the same (large) dataset as a dependency.<\/p>\n<p><s>Is the answer hardlinks? Worried 'cause there\u2019s already a lot of linking going on \u2026<\/s> (Duh. Not across filesystems!)<\/p>\n<p>To be clear, this looks like an awesome tool that I\u2019d like to adapt to if possible.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What are the possible values for `dvc config` command?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-are-the-possible-values-for-dvc-config-command\/46",
        "Question_created_time":1530525747567,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":680,
        "Question_body":"<p>Is there a place listing all the properties that can be configured using the <code>dvc config<\/code> command? I couldn\u2019t find such info in the documentation, for instance.<\/p>\n<p>This would be very helpful for people starting out, to understand the possibilities of DVC.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does DVC actually require Git, or would Mercurial work just as well?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-actually-require-git-or-would-mercurial-work-just-as-well\/55",
        "Question_created_time":1532508768537,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":1031,
        "Question_body":"<p>My question is as the title states really, does DVC actually require Git, or would Mercurial work just as well? Does DVC actually call Git to do anything, or does it just store information in normal files which the user commits.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to output the stage file in a different directory than CWD?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-output-the-stage-file-in-a-different-directory-than-cwd\/58",
        "Question_created_time":1533304349308,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":1353,
        "Question_body":"<p>I typically run scripts from my project\u2019s root directory, and use parameters to point to inputs and outputs as needed. Wrapping with <code>dvc run<\/code> generates a stage file in the root directory, which clutters the project.<\/p>\n<p>I tried using the <code>-f<\/code> option in <code>dvc run<\/code> to specify a different path for the stage file, but it ended up causing problems with the data paths. In particular it appears that DVC considers the dependencies and output paths to be relative to the stage file.<\/p>\n<p>My current solution is to make a <code>runs<\/code> directory from where I execute all <code>dvc run<\/code> commands. All the stage files end up in there. But this is a bit clunky and isn\u2019t the pattern I usually use.<\/p>\n<p>I\u2019m wondering if I\u2019m missing something here? Is there a way to store stage files to somewhere other than where the <code>dvc run<\/code> command was executed?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Compliance - administering data that is under DVC control",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174",
        "Question_created_time":1556203634652,
        "Question_answer_count":9,
        "Question_score_count":8,
        "Question_view_count":1558,
        "Question_body":"<p>Hi there<\/p>\n<p>We are currently looking to implement DVC on our stack.<\/p>\n<p>One thing that isn\u2019t clear from the available documentation, however, is how our data compliance team would go about administering and managing our data store using DVC. What kind of user access control is available to us? Or is this based on our existing Git\/Bitbucket setup?<\/p>\n<p>We\u2019ve been through the tutorials available to us and we understand how to add data to the repository, branch off, update etc, but what we\u2019re looking for clarity on is how we would centrally manage this for multiple users.<\/p>\n<p>Any advice or guidance would be appreciated.<\/p>\n<p>EDIT: To add clarity to the question, our requirement is for all of our incoming data sources to be handled by a specialist compliance team who will act as gatekeepers to the data and make it available to the data scientists within the firm as needed. The intention is for our DS team to pull data as is needed but not be able to update it without approval from the compliance team. Thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Procedure for changing cache type",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/procedure-for-changing-cache-type\/204",
        "Question_created_time":1566354562099,
        "Question_answer_count":11,
        "Question_score_count":3,
        "Question_view_count":1009,
        "Question_body":"<p>Because I\u2019m using large files which are tens of gigabytes, I want to use hardlinks as my filesystem doesn\u2019t support reflinks.<br>\nI executed the following commands<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type = hardlink\ndvc config cache.protected = true\ndvc checkout\n<\/code><\/pre>\n<p>However when I checked using <code>ls -i<\/code> the inodes of the file in .dvc\/cache and the file in my working directory, they are still different.<br>\nAre there any further steps I need to take to get hardlinks to be used?<\/p>\n<p>Also I noticed that both the file in the working directory and cache were still writable, when I thought that they should have been made read-only by using <code>cache.protected = true<\/code><br>\nIs my understanding here mistaken?<\/p>\n<p>Update:<br>\nI mistyped the commands above. I actually had executed them correctly, i.e.:<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type hardlink\ndvc config cache.protected true\ndvc checkout\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to change metrics computation and recompute",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-change-metrics-computation-and-recompute\/278",
        "Question_created_time":1575979302220,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":452,
        "Question_body":"<p>Hi all,<br>\nI trained several models and computed a simple metrics summary for each of them. Now, those models (and their corresponding metrics.json files) are versioned by dvc and git and there is a git tag associated with each model.<br>\nHaving the models trained and stored, I would like to enrich the metrics summary and go back and recompute it for each model \u2013 and store it in git as with the simple summary. This means the last part of the pipeline, say compute_metrics.py, will be changed and I would like to run dvc repro again. Is there a simple way of recomputing the metrics for all models (that is for all tags)?<\/p>\n<p>Thank you,<br>\nMichal<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I remove files which are no longer tracked by DVC from DVC remote SSH storage",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-do-i-remove-files-which-are-no-longer-tracked-by-dvc-from-dvc-remote-ssh-storage\/113",
        "Question_created_time":1548846841252,
        "Question_answer_count":6,
        "Question_score_count":2,
        "Question_view_count":3140,
        "Question_body":"<p>Hi,<\/p>\n<p>I had added a folder inputs to remote ssh dvc storage.<br>\nAfter some time, i decided to remove this folder<br>\nso i did<\/p>\n<ul>\n<li><code>dvc remove inputs.dvc<\/code><\/li>\n<li>did a git commit and push of the dvc changes<br>\nBut if i use an older commit where the inputs folder was tracked by dvc i am still able to pull inputs folder from remote dvc storage<\/li>\n<\/ul>\n<p>How can i remove the file from the remote ssh dvc storage?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can the command `dvc add` just add new and changed files?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-the-command-dvc-add-just-add-new-and-changed-files\/262",
        "Question_created_time":1575200338304,
        "Question_answer_count":4,
        "Question_score_count":5,
        "Question_view_count":1079,
        "Question_body":"<p>I have just started playing with dvc to investigate whether it can help me adding version control to large sets of images used for deep learning.<\/p>\n<p>I made my own <em>small<\/em> toy example to try to understand how dvc works.<\/p>\n<p>I have a data folder with 10 images of approx 10 MB each, in total ~100 MB<\/p>\n<p>When I run \u201cdvc add data\u201d I can see that the dvc cache grows with ~100 MB<\/p>\n<p>I now replace one of the images in my data folder with another one, i.e. remove one 10 MB image and add another 10 MB image. The total size is still 100 MB<\/p>\n<p>When I run \u201cdvc add data\u201d again to track the changes, the dvc cache grows by another 100 MB! I was thinking that dvc would automatically add the difference between the data sets, but that does not seems to be the case? Does it <em>copy<\/em> all the content for each \u201cdvc add\u201d command?<\/p>\n<p>If yes, is there any practically convenient way to make dvc detect and store the <em>difference<\/em> between versions of data sets?<\/p>\n<p>Thanks,<br>\nSven<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to remove old versions of data files?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-remove-old-versions-of-data-files\/256",
        "Question_created_time":1574756258176,
        "Question_answer_count":1,
        "Question_score_count":4,
        "Question_view_count":1123,
        "Question_body":"<p>My dvc repository got big, and I do not need the old versions of data files.<\/p>\n<p>Is there any way to remove them from the repository, to save the disk space?<\/p>\n<p>In fact, if there are no other options, I would remove my dvc repository and start it again.<\/p>\n<p>However, I do not want to recreate my dvc files (pipelines), keeping the existing ones.<\/p>\n<p>Is this possible?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practice for hyperparameters sweep",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-hyperparameters-sweep\/244",
        "Question_created_time":1570444543780,
        "Question_answer_count":8,
        "Question_score_count":9,
        "Question_view_count":2026,
        "Question_body":"<p>Hi,<\/p>\n<p>I wonder what is best practice for hyperparameter optimization, to do e.g. a parameter sweep, to capture both the models and the metrics. I\u2019d like to try 10 different values for the number of clusters (and also a few other parameters), and then plot figures visualizing each clustering.<\/p>\n<p>Do you just execute <code>dvc run<\/code> in a loop from a script? Then that script essentially becomes the \u201cMakefile\u201d?<\/p>\n<p>Also, do you create a new branch for each experiment, or just a new folder with a unique name?<\/p>\n<p>Thanks!<br>\nAlex<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Retrieve data after using dvc import, but deleting original git repo",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/retrieve-data-after-using-dvc-import-but-deleting-original-git-repo\/252",
        "Question_created_time":1572630007806,
        "Question_answer_count":2,
        "Question_score_count":3,
        "Question_view_count":1327,
        "Question_body":"<p>We ran into a problem, where we had original data inside of a particular original git repo. The repo was connected to a bucket on google cloud storage. We then imported the data into a new repo. We just realized that we accidentally deleted the original git repo at some point. The data is still floating around in our bucket, but we no longer have the original git repo\u2019s dvc files to point to it. We do have the new git repo\u2019s files, but doing a dvc pull now gives this error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: failed to fetch data for 'split' - Failed to clone repo '\/mnt\/myfolder' to '\/tmp\/tmp_m8bnoq_dvc-repo': Cmd('git') failed due to: exit code(128)\n  cmdline: git clone --no-single-branch -v \/mnt\/myfolder \/tmp\/tmp_m8bnoq_dvc-repo\n  stderr: 'fatal: repository '\/mnt\/myfolder' does not exist\n'\n<\/code><\/pre>\n<p>Can we do anything now to retrieve the data?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Whole directory as input or output",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/whole-directory-as-input-or-output\/232",
        "Question_created_time":1569494799270,
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":1334,
        "Question_body":"<p>I have tried to list a whole directory as input or output to a stage but this doesn\u2019t seem to work as I would hope. I would like, ideally, to have the directory contents monitored for changes when reproducing the pipeline as the files within could change and are too many to list individually as inputs or outputs.<\/p>\n<p>Is this already possible (if so how?) or something that may be implemented in future? Or is there some other process to deal with this kind of situation? Am I trying to use the system in the wrong way?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Upgrade to .60 produces .dvc file dependency error -- breaks pipeline",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/upgrade-to-60-produces-dvc-file-dependency-error-breaks-pipeline\/230",
        "Question_created_time":1569421768226,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":582,
        "Question_body":"<p>I just upgraded to dvc 0.60.0 and when attempting to run the same commands I have run for ages I get the following error<\/p>\n<p>Command run:<br>\n<code>dvc run -f entropy_filter.dvc -d entropy_config.json -d ..\/..\/..\/scripts\/entropy_filter.py -O junk_clusters.txt  python ..\/..\/..\/scripts\/entropy_filter.py entropy_config.json<\/code><\/p>\n<p>output<br>\n<code>ERROR: run_model.dvc cannot be a dependency<\/code><\/p>\n<p>Note that the dependency to entropy_filter.dvc stage (entropy_config.json) is produced by run_model.dvc.<\/p>\n<p>I then attempted to rerun the base command to regenerate run_model.dvc only to get the exact same error, that run_model.dvc cannot be a dependency. I then attempted to remove run_model.dvc and run the command again only to be given the same error. Finally I attempted a <code>dvc gc<\/code> which again gave the same error\u2026<\/p>\n<p>Only way to fix was to downgrade back to 59.2. Any suggestions on how I can change my pipeline to work with 0.60.0?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multiple pipelines with single metric file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-pipelines-with-single-metric-file\/233",
        "Question_created_time":1569495407825,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":841,
        "Question_body":"<p>I am trying to define multiple pipelines sharing a single metrics file and this does not seem possible currently. I get an error when trying to add the same metric file to more than one pipeline.<\/p>\n<p>My use-case is, I am doing some ML experiments and would like to have multiple models that I work on in parallel, for example a SVM and a Naive Bayes model (maybe more models in future), both sharing the same data and much of the pre-processing steps. Then I have 2 pipelines, which are very similar, sharing many stages: model_svm.dvc and model_nb.dvc. So they are in the same repository and I would like that they both use the same metric file so that when I call \u201cdvc metrics show -T\u201d it will show past results from both models.<\/p>\n<p>Is this currently possible and if so how? Or am I trying to use the system in a way that was not intended, and if so could you recommend how I could restructure my project so that it fits better with how DVC was intended to be used?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Once a model is trained, can the DAG be re-used for prediction?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/once-a-model-is-trained-can-the-dag-be-re-used-for-prediction\/228",
        "Question_created_time":1569232040969,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":514,
        "Question_body":"<p>Hi,<\/p>\n<p>The examples I reviewed in the documentation seem to describe how to define, share and reproduce a <strong>train<\/strong> model pipeline. Once we are happy with our trained model and we want to move it into production, what would be the recommended approach to use DVC to ensure the pipeline consistency between <strong>train<\/strong> and <strong>predict<\/strong>?<\/p>\n<p>I would like to re-use the DVC pipeline defined for training (feature engineering, processing,\u2026) to ensure consistency and proper usage. On the other hand, the pipeline would also be somewhat different (each individual model script would \u201cpredict\u201d instead on \u201ctraining\u201d).<\/p>\n<p>Is there a recommended solution ? Should I create additional variables to tell each script whether to train or predict ? What to make of the metric at the end?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC and data lake",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-data-lake\/215",
        "Question_created_time":1568116726435,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":953,
        "Question_body":"<p>Hey guys, quick question.<\/p>\n<p>Following the ML flow best practice, we must store our raw data in one data lake structure, right?<\/p>\n<p>DVC can use Google Storage\/S3 and a lot of other storage support to store our code\/dataset and models in one common place.<\/p>\n<p>Is it correct to say than I have one \u201cData Lake\u201d structure just using DVC?<\/p>\n<p>I\u2019m using DVC to store my dataset and models, and now I would like to know if is necessary to implement one data lake architecture on top of that.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Copying a dvc repository",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/copying-a-dvc-repository\/213",
        "Question_created_time":1567023444597,
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":2496,
        "Question_body":"<p>I have realized that a dvc repository cannot be backed up as I would any working directory, since links cannot be backed up (except by archiving).<\/p>\n<p>I am using symlinks with <code>cache.protected true<\/code>.<\/p>\n<p>Apparently, the way to backup a dvc repository is to set up another dvc repository for this purpose and transfer all changes there.<\/p>\n<p>I have set up a backup dvc repository on a flash drive, as follows:<\/p>\n<pre><code>cd \/path\/to\/mount\/point\/\nmkdir backup\ncd backup\ngit init\ndvc init\ndvc remote add -d mainremote \/path\/to\/my\/dvc\/repository\ngit add -A\ngit commit\ngit clone --no-hardlinks \/path\/to\/my\/dvc\/repository\ndvc pull\n<\/code><\/pre>\n<p>What happened, was that git clone did not copy any symlinks and <code>dvc pull<\/code> reported:<\/p>\n<blockquote>\n<p>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<\/p>\n<\/blockquote>\n<p>And here it listed all my data files and directories, some of them twice.<\/p>\n<p>After that a few more error messages followed, saying that the cache files could not be downloaded since they do not exist on the flash repository.<\/p>\n<p>Please help.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Add'ed files are duplicated in the cache, no links",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/added-files-are-duplicated-in-the-cache-no-links\/209",
        "Question_created_time":1566732869315,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":729,
        "Question_body":"<p>I have started using dvc by running <code>dvc init<\/code> and then <code>dvc add<\/code> for two directories and one file.<\/p>\n<p>All worked fine, all filed have been copied to the cache but I noted that the total disk usage has doubled as the original files remained in addition to the cached copies.<\/p>\n<p>The original files do not show any signs of being reflinks.<\/p>\n<p>Is this right?<\/p>\n<p>I am using Ubuntu 16.04.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC in a directory that is already git-controlled",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-in-a-directory-that-is-already-git-controlled\/208",
        "Question_created_time":1566477305073,
        "Question_answer_count":2,
        "Question_score_count":5,
        "Question_view_count":514,
        "Question_body":"<p>I want to init dvc in a directory that is already under git.<\/p>\n<p>More precisely, I already have a git-controlled directory that contains all my pycharm projects, and want to initialize dvc in its subdirectory for a specific project.<\/p>\n<p>Am I going to run into trouble later?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to move DVC files",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-move-dvc-files\/205",
        "Question_created_time":1566355790691,
        "Question_answer_count":3,
        "Question_score_count":4,
        "Question_view_count":1341,
        "Question_body":"<p>Search for this topic, I found <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-heartbeat-discord-gems\/159\/2\">this summary post<\/a> but I cannot open the original question (discord tells me it cannot find it) so I\u2019m creating a new topic. Apologies in advance.<\/p>\n<p>I tried following the instructions given i.e. I ran<\/p>\n<pre><code class=\"lang-auto\">mv my_file.dvc dvc_links\/my_file.dvc\n<\/code><\/pre>\n<p>and edited the working directory in the moved file.<br>\nHowever running <code>dvc status<\/code> gives<\/p>\n<pre><code class=\"lang-auto\">dvc_links\/my_file.dvc:\n        changed checksum\n<\/code><\/pre>\n<p>How do I get dvc to update the checksum in the relocated file? I even tried removing the md5: xx line at the top of the new file, calculating the md5 sum and reinserting it but this didn\u2019t work.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC - can\u2019t I track directly an S3 remote data?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-can-t-i-track-directly-an-s3-remote-data\/188",
        "Question_created_time":1562840122440,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":840,
        "Question_body":"<p>Hi guys,<br>\nI am currently evaluating DVC for my team.<br>\nI have been trying it out and like some of its features. However there are something I am a bit lost with.<br>\nWe work on s3, we have a daily pipeline that downloads data from the internet, processes and spits out data in another format.<br>\nSo far I managed to : configure a remote s3 repo, push a local file to it, play with run and repro. However this does not fit 80% of our workflow. Can you help me understand if the following things are possible:<\/p>\n<ol>\n<li>To push data on dvc, do I necessarily need to have the data on my local machine then push it to the remotely defined location ? Can\u2019t I track directly a remote s3 parquet file ? e.g: dvc add s3:\/\/mybucket\/folder-a\/myfile.parquet -r myRemote ?<\/li>\n<li>If 1 is possible, if someone manually overwrites the s3:\/\/mybucket\/folder-a\/myfile.parquet, how will that affect the integrity of dvc ?<\/li>\n<\/ol>\n<p>Thanks for the help.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I integrate DVC with perforce?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-i-integrate-dvc-with-perforce\/121",
        "Question_created_time":1550225525646,
        "Question_answer_count":3,
        "Question_score_count":3,
        "Question_view_count":988,
        "Question_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use perforce for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versioning software that DVC can work with?<\/p>\n<p>Is it possible to integrate my DVC with perforce.<\/p>\n<p>Please let me know.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Setup DVC to work with shared data on NAS server",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/setup-dvc-to-work-with-shared-data-on-nas-server\/180",
        "Question_created_time":1557992716645,
        "Question_answer_count":10,
        "Question_score_count":8,
        "Question_view_count":12632,
        "Question_body":"<p>Hi DVC team!<\/p>\n<p>Thanks for providing these wonderful tools. Our scenario is that we have a server with NAS storage and many projects need to use data stored in NAS. The storage will be updated so the size of data will grow every day<\/p>\n<pre><code class=\"lang-auto\">\/mnt\/dataset\/project1_data\/\n<\/code><\/pre>\n<p>And the model code is listed in<\/p>\n<pre><code class=\"lang-auto\">\/home\/user\/project1\/\n<\/code><\/pre>\n<p>The project1_data takes about 100 GB, so when I used <strong>dvc add \/mnt\/dataset\/project1_data\/<\/strong>, it takes so long that I shut down it before it runs to end.<\/p>\n<p>My question is how do we use DVC to do data version control and code control based on NAS storage? I have a look into <a href=\"https:\/\/dvc.org\/doc\/use-cases\/multiple-data-scientists-on-a-single-machine\" rel=\"nofollow noopener\">Shared Development Server<\/a> but not clear for adding data from NAS.<\/p>\n<p>Thanks again!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"First run of DVC - getting a \"failed to reproduce\" error",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/first-run-of-dvc-getting-a-failed-to-reproduce-error\/171",
        "Question_created_time":1554389891804,
        "Question_answer_count":7,
        "Question_score_count":2,
        "Question_view_count":4183,
        "Question_body":"<p>Hi,<br>\nI just cloned a project from a colleague; in this project there is a <em>.dvc<\/em> file, with some <em>deps<\/em> and some <em>outs<\/em>.<br>\nWhen I run <em>dvc repro -f myfile.dvc<\/em>, I got an error<\/p>\n<blockquote>\n<p>\u2026failed to reproduce \u2018myfile.dvc\u2019: output \u2018xxx\u2019 does not exist<\/p>\n<\/blockquote>\n<p>Well, obviously the <em>output<\/em> does not exist, this is the first time I run the script, so nothing was created yet !<br>\nI got the same error when I run with the \u2018\u2013force\u2019 flag<\/p>\n<p>How do I run the pipeline for the first time ?<br>\nI am missing something ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How does DVC do version checking?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-does-dvc-do-version-checking\/168",
        "Question_created_time":1554023291346,
        "Question_answer_count":6,
        "Question_score_count":6,
        "Question_view_count":1188,
        "Question_body":"<p>Hi,<\/p>\n<p>Let\u2019s assume that we have data on an S3 Bucket. The data is downloaded locally and after processing, a TFRecord of the data is made.<\/p>\n<p>The data in the S3 Server has increased in samples and we don\u2019t want to redo the pipeline for all of the data. The new data is on the same S3 bucket as the previous version. No versioning of the data is done on the S3.<\/p>\n<ol>\n<li>How does DVC understand that the data has changed? Do we need to trigger it manually?<\/li>\n<li>How can the pipeline be only run for the new version of the data?<\/li>\n<li>How can be tag the data and\/or the TFRecord with a certain version number that is implicitly in the system? Can this tag be read from anywhere?<\/li>\n<\/ol>\n<p>I would appreciate if you can share your solutions.<\/p>\n<p>Regards,<br>\nSiavash<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"S3 remote permissions and integrity best practices",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/s3-remote-permissions-and-integrity-best-practices\/165",
        "Question_created_time":1553787278231,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":1797,
        "Question_body":"<p>Does anyone have recommendations about best practices for S3 remote set-up?<\/p>\n<p>I\u2019m specifically interested if there are thoughts around permissions and integrity of data in S3. To enable S3, remotes users would be granted read &amp; write permissions. As long as users use the dvc tooling it appears the data should relatively save. However, granting permissions would allow direct access outside of dvc tooling. This seems to open the possibility of a scenario where the data history could be corrupted.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deleting files from a directory data set",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deleting-files-from-a-directory-data-set\/134",
        "Question_created_time":1552387162914,
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":1215,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m currently tracking with dvc a directory containing the set of files of my data base. I added this on a single \u201cdvc add\u201d call, and a single entry appears on the \u201c.dvc\u201d file. I would like to erase some files from the directory without having to remove and re-add the whole directory, but I can\u2019t find information on how to do this. Is there a safe way to do it while keeping the soon-to-be-deleted files on history so I can get them back if I move to an older commit?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC requirements - how and where to install it?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-requirements-how-and-where-to-install-it\/126",
        "Question_created_time":1551703044426,
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":582,
        "Question_body":"<p>I need know the requisites to install the DVC:<\/p>\n<ul>\n<li>Server Specifications (ram, cpu, etc),<\/li>\n<li>-Type of Operative System (Linux is better of Windows?),<\/li>\n<li>Dependences of another application (as GIT)<\/li>\n<li>Others issues that we have to consider<\/li>\n<\/ul>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Gitlab: getting a Disallowed command error",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/gitlab-getting-a-disallowed-command-error\/119",
        "Question_created_time":1550059658781,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":2381,
        "Question_body":"<p>Hi,<\/p>\n<p>we are trying to configure GitLab to work with dvc. Git lfs is enabled for the repository yet. I installed dvc client on a local machine. Ran <code>dvc init<\/code>, <code>dvc add<\/code>, <code>git push<\/code> and when I try to run dvc push I\u2019ve got an error:<\/p>\n<pre><code class=\"lang-bash\">roman@vs-roman:~\/dev\/project$ dvc push\nPreparing to upload data to 'ssh:\/\/git@gitlab.cvanalytics.pro:\/home\/git\/gitlab\/shared\/lfs-objects\/box\/box.neuro.git'\nPreparing to collect status from ssh:\/\/git@gitlab.cvanalytics.pro:\/home\/git\/gitlab\/shared\/lfs-objects\/box\/box.neuro.git\n[#########                     ] 30% Collecting information\/usr\/lib\/dvc\/paramiko\/ecdsakey.py:164: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n\/usr\/lib\/dvc\/paramiko\/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n\/usr\/lib\/dvc\/paramiko\/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n\/usr\/lib\/dvc\/paramiko\/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\nError: failed to push data to the cloud - ssh command 'find \/box\/box.neuro.git -type f -follow -print' finished with non-zero return code 1': GitLab: Disallowed command\n<\/code><\/pre>\n<p>Where to find instructions for server installation?<br>\nWhat should I do to fix the problem?<\/p>\n<p>UPD: changed url after port reconfiguration according to <a href=\"https:\/\/discuss.dvc.org\/t\/remote-ssh-port-error\/84\">this topic<\/a><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using DVC for end-to-end pipeline",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-for-end-to-end-pipeline\/111",
        "Question_created_time":1545748275249,
        "Question_answer_count":6,
        "Question_score_count":7,
        "Question_view_count":1256,
        "Question_body":"<p>HI,<\/p>\n<p>I am trying to create an end-to-end pipeline ( I am new to it \u2026 ), and trying to use dvc as well.<\/p>\n<p>I have create 5 script corresponding to each phase<\/p>\n<ol>\n<li>preprocessing<\/li>\n<li>1st Feature engineering<\/li>\n<li>train-test data splite<\/li>\n<li>2nd Feature engineering<\/li>\n<li>Model training<br>\nI could do 'dvc run bla bla bla \u2019 for each program file. and I could reproduce the result.<\/li>\n<\/ol>\n<p>However if I would like to re-run the \u201cWHOLE\u201d pipeline after I have twitched something, is there any \u201cbest practice\u201d or recommendation or guide line  (e.g. use shell script to wrap up all \u2018dvc run\u2019 commands and run it ?  use dvc run -d xx.data -d yy.data \u2026 -d fe1.py -d fe2.py -d main.py \u2026, to run a very bulky command to make a dvc file ?  ) ?<\/p>\n<p>and. \u2026 do creating separate script for each phase a good idea to manage the pipeline ?<\/p>\n<p>Looking forward for answer ! Many thanks !<\/p>\n<p>Best Regards,<br>\nSolomon Leung<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dealing with Kaggle data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dealing-with-kaggle-data\/103",
        "Question_created_time":1541129343230,
        "Question_answer_count":2,
        "Question_score_count":3,
        "Question_view_count":1097,
        "Question_body":"<p>What is the best way to deal with outside data, such as a well known data set such as Imagenet or a Kaggle dataset?<\/p>\n<p>More specifically, I want to download the data from the source as the first stage in the pipeline and be able to use them as dependencies for the next stage in the pipeline. But, I don\u2019t want to upload those (potentially) huge data sets to my s3 bucket.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Creating an aggregate .dvc file",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/creating-an-aggregate-dvc-file\/93",
        "Question_created_time":1536703395229,
        "Question_answer_count":11,
        "Question_score_count":1,
        "Question_view_count":2662,
        "Question_body":"<p>I have the following use case:<\/p>\n<ul>\n<li>Launch a script which loops over multiple folders  and does some processing (each folder takes quite a bit of CPU time)<\/li>\n<li>Each folder leads to the creation of a new folder, containing the output of the processing<\/li>\n<li>Once the loop is complete, a single .dvc file is created<\/li>\n<\/ul>\n<p>This works fine, but if the processing crashes mid-way, <code>dvc repro<\/code> will start again from scratch.<br>\nOne solution would be to create a <code>.dvc<\/code> file for each folder, but then, repro is a bit tedious to run as I would need to loop over all the <code>.dvc<\/code> file.<\/p>\n<p>Is there a better way to go ? If not, would it be possible to define multiple layers of dvc files (e.g. run subtasks, create multiple <code>.dvc<\/code>, once subtask is complete, create a master <code>.dvc<\/code> which will allow to easily repro all the subtasks) ?<\/p>\n<p>Thanks in advance !<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Differences between dvc repro and dvc status",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/differences-between-dvc-repro-and-dvc-status\/101",
        "Question_created_time":1537891906524,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":763,
        "Question_body":"<p>Is it guaranteed that if <code>dvc status Dvcfile<\/code> shows: nothing to reproduce, the <code>dvc repro Dvcfile<\/code> will not do anything ?<\/p>\n<p>For my use case, it looks like <code>dvc repro<\/code> (even when there is nothing to reproduce) takes more time to run than <code>dvc status<\/code>. I could then switch to calling <code>dvc status<\/code> to find out if there\u2019s nothing to reproduce and have a faster pipeline.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Shared cache without commiting",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-without-commiting\/100",
        "Question_created_time":1537882485174,
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":582,
        "Question_body":"<p>Hi<\/p>\n<p>As far as i understand, when I setup a shared cache directory, I have to commit it and others also have to use it.<br>\nIs there a way that I can define a shared cache dir for me only?<\/p>\n<p>Use Cases:<\/p>\n<ul>\n<li>I have a project, where I want to run jobs from two branches simultaneously. So I need to clone the repo twice, but only want the cache to exist once.<\/li>\n<li>I have projects with similar data dependencies and I want to share the cache between both projects.<\/li>\n<\/ul>\n<p>Is it possible or do you have any other recommendations for my use cases?<\/p>\n<p>Thanks and Regards<br>\nMatthias<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Step with no dependency keeps running",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/step-with-no-dependency-keeps-running\/99",
        "Question_created_time":1537531855753,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":895,
        "Question_body":"<p>Hi!<\/p>\n<p>When I have a step in my pipeline that does not have any dependencies, e.g. a <code>wget<\/code> command, <code>dvc repro<\/code> will call that command every time, even if the output is already there. Is this intended behavior?<\/p>\n<p>For my case it does not really make sense as the command should only be run if the output (the downloaded file) is not there already. I know about setting up external dependencies (remotes), but they don\u2019t really work for me here\u2026<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practices for specifying dependencies",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practices-for-specifying-dependencies\/96",
        "Question_created_time":1536759423939,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":794,
        "Question_body":"<p>In a pipeline, if <code>folder1<\/code> is the output of <code>stage1<\/code> and used in <code>stage2<\/code>, then dvc will know about it if we run <code>dvc run -d folder1 -o folder2 python stage2.py<\/code><\/p>\n<p>Equivalently, since stage1 creates a dvc file, we could run: <code>dvc run -d stage1.dvc -o folder2 python stage2.py<\/code><\/p>\n<p>What is the recommended way ?<\/p>\n<p>Thanks !<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Exposing a non CLI API",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/exposing-a-non-cli-api\/95",
        "Question_created_time":1536759195852,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":535,
        "Question_body":"<p>Hi,<\/p>\n<p>Is there any plan to expose a pure python API so that DVC commands can be run directly within a python script ? (At the moment, I am building a pipeline based on a series of subprocesses that call the various DVC commands, but development \/ debugging would be smoother if I could directly call the underlying python code).<\/p>\n<p>It is probably already possible, but there are no examples in the documentation about this use case.<\/p>\n<p>Thanks !<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC and version control systems other than Git",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-version-control-systems-other-than-git\/90",
        "Question_created_time":1536316201269,
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":803,
        "Question_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use Mercurial for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versionioning software that DVC can work with? The only part that I\u2019ve spotted in the docs that really relies on Git is the <code>dvc init<\/code> command that creates <code>.dvc\/.gitignore<\/code>. Would adding contents of this file to hgignore (equivalent of gitignore in Mercurial) solve the problem? Are there other caveats that would prevent me from using Mercurial with DVC?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC conflict may arrive if multiple user working on same repository?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-conflict-may-arrive-if-multiple-user-working-on-same-repository\/86",
        "Question_created_time":1535600671377,
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":854,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. And i will push the data to S3 using dvc push. In our project multiple people are working together if another person already push the data changes to dvc if im going to push same it will through dvc conflict. In git we are getting git conflict<\/p>\n<p>please let me know<\/p>\n<p>Thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Handle cache to keep the latest version of data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/handle-cache-to-keep-the-latest-version-of-data\/87",
        "Question_created_time":1535720101051,
        "Question_answer_count":2,
        "Question_score_count":4,
        "Question_view_count":666,
        "Question_body":"<p>Hi Ruslan,<\/p>\n<p>I would like to ask the best practice on how to handle the following situation with DVC.<br>\nLet\u2019s suppose we have a big raw dataset stored in the cloud.<\/p>\n<ul>\n<li>So, we initialize a project (git + dvc) to work with this dataset. At first we would like to trace how to get the dataset locally. For this we use <code>dvc run<\/code> with commands as <code>wget url<\/code> to download the raw dataset. We commit changes etc.<\/li>\n<li>Next we develop some scripts to preprocess the raw dataset into a dataset we call v0.1.0. Then we use <code>dvc run<\/code> to run the preprocessing on the raw dataset and store the way to obtain the dataset v0.1.0. Thus, we have two folders like <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>. We commit and push with git and dvc. At this step we would like to keep only <code>dataset_v0.1.0<\/code> in the cache and remove <code>raw_data<\/code> from cache.<\/li>\n<li>We have another developer to work on the project, so he\/she uses git to clone the project and run <code>dvc checkout<\/code> or <code>dvc pull<\/code> to get the latest data version state. After the last command he\/she gets from remote both data: <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>.<\/li>\n<\/ul>\n<p>Is it possible to handle the cache in such manner that we keep only the latest version without using in-place preprocessing for a given commit ?<\/p>\n<p>Thank you<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Handling indeterministic output",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/handling-indeterministic-output\/78",
        "Question_created_time":1534925717183,
        "Question_answer_count":5,
        "Question_score_count":3,
        "Question_view_count":991,
        "Question_body":"<p>Hi,<\/p>\n<p>first of all, very nice tool, thank you for that!<\/p>\n<p>I am currently experimenting with dvc in a pytorch ML pipeline.<br>\nOne step of the pipeline is training and pickling a model.<\/p>\n<p>Usually training a model starts with random numbers, so if I do not set random seeds I will end up in a dfiferent model each training re-run, resulting in a new md5 hash for the model file each time.<br>\nEven if I set seeds, and the model ends up in the same values, I was not able (yet) with pytorch to pickle the model in a way that it exactly equals the previous run in terms of md5 hash.<\/p>\n<p>Do you recommend any best practices regarding such kind of indeterministic\/random output (for pytorch and\/or in general)?<br>\nI think that there might arise problems when sharing pipelines but not data, or when having different metrics output.<br>\nIs it a problem at all? Is dvc capable of handling these things and if yes, how?<\/p>\n<p>Thanks,<\/p>\n<p>Alex<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"DVC run and add: store command and data",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-run-and-add-store-command-and-data\/68",
        "Question_created_time":1534263237672,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":532,
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to know whether we could store run info for reproducibility and add output data to a storage that can be than pulled.<br>\nFor example, I have a raw dataset and a cleanup script<\/p>\n<pre><code class=\"lang-auto\">$ ls \nraw cleanup.py\n<\/code><\/pre>\n<p>I can run the cleanup<\/p>\n<pre><code class=\"lang-auto\">$ dvc run -d raw -o clean python cleanup.py raw clean\n$ cat clean.dvc\ncmd: python cleanup.py\ndeps:\n- md5: xxxx\n  path: raw\nmd5: yyyyy\nouts:\n- cache: true\n  md5: zzzz\n  path: clean\n<\/code><\/pre>\n<p>and I observe how clean folder is produced. However if I add clean folder with dvc in order to share it the information on how to produce clean folder is modifed<\/p>\n<pre><code class=\"lang-auto\">$ dvc add clean\n$ cat clean.dvc\nmd5: xxxx\nouts:\n- cache: true\n   md5: xxxxx\n<\/code><\/pre>\n<p>So, can we have both features : stored command on how dataset can be produced and is stored in the cache ?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Getting \"Unsupported URL\" when adding SSH remote",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/getting-unsupported-url-when-adding-ssh-remote\/64",
        "Question_created_time":1533818634630,
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":1491,
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying to add a new ssh remote.<\/p>\n<p><code>dvc remote add base ssh:\/\/elena@server_name.company.lan:\/home\/elena\/test_dvc\/data<\/code><\/p>\n<p>and I get a <code>Initialization error: Config file error: Unsupported URL<\/code> when trying <code>dvc status\/pull\/push<\/code>.<\/p>\n<p>I tried adding<br>\n<code>dvc remote modify base credentialpath ~\/.ssh\/config<\/code><br>\nbut the error persists.<\/p>\n<p>Can anyone help with an example on how to fully configure an ssh remote for dvc?<\/p>\n<p>Later edit:<br>\nSolved: Typo in the remote url (the remote path was missing \u2018\/\u2019 )<\/p>\n<p>Thanks,<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Adding private S3 buckets",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/adding-private-s3-buckets\/62",
        "Question_created_time":1533670100321,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":1020,
        "Question_body":"<p>Hey there!<\/p>\n<p>I\u2019m trying to <code>dvc push<\/code> my data file to a private s3 bucket but am getting the error:<\/p>\n<p><code>Failed to push data to the cloud: The config profile (tdobbins) could not be found<\/code><\/p>\n<p>I pointed dvc to my .config file by doing:<\/p>\n<p><code>dvc remote modify myremote credentialpath aws\/.config<\/code><\/p>\n<p>and used:<\/p>\n<p><code>dvc remote modify myremote profile tdobbins<\/code><\/p>\n<p>Is there anything obvious that I\u2019m missing?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does DVC deal well with partitioned Parquet files in Hive format?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-deal-well-with-partitioned-parquet-files-in-hive-format\/52",
        "Question_created_time":1530872478915,
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":1506,
        "Question_body":"<p>Parquet files in Hive format partitioned by <code>col1<\/code> are, in fact, directories in the format:<\/p>\n<pre><code class=\"lang-python\">\n\/myfile.parquet\n    \/_common_metadata\n    \/_metadata\n    \/col1=val1\n        \/part0.parquet\n        \/part1.parquet\n        \/...\n    \/col1=val2\n        \/part10.parquet\n        \/part12.parquet\n        \/...\n    \/col1=...\n        \/part20.parquet\n        \/part21.parquet\n        \/...\n\n\n<\/code><\/pre>\n<p>The tree structure can go even deeper if the Parquet file in partitioned by several columns rather that just <code>col1<\/code>.<\/p>\n<p><strong>Will DVC deal well with this type of files, both when they are a dependency or an output?<\/strong><\/p>\n<p>Example, where <code>myfile1.parquet<\/code> and <code>myfile2.parquet<\/code> are Parquet files in Hive format partitioned by some col(s):<\/p>\n<pre><code class=\"lang-auto\">dvc add myfile1.parquet\n\n# run script.py wich outputs myfile2.parquet\ndvc run -d myfile1.parquet -d script.py -o myfile2.parquet python script.py\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Linking multiple cloud accounts",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/linking-multiple-cloud-accounts\/51",
        "Question_created_time":1530804515195,
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":484,
        "Question_body":"<p>I would like to put some of my data in different folders in aws, is there any way to link multiple projects\/ urls and then decide which one I wan to deposit it too ?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Shared cache directory",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-directory\/31",
        "Question_created_time":1526900412793,
        "Question_answer_count":14,
        "Question_score_count":5,
        "Question_view_count":2345,
        "Question_body":"<p>Hi<\/p>\n<p>I was wondering how to setup a shared cache directory, if possible?<\/p>\n<p>Thanks<br>\nMatthias<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Batch pipeline support and multi I\/O",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/batch-pipeline-support-and-multi-i-o\/42",
        "Question_created_time":1530180340930,
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":858,
        "Question_body":"<p>This tool looks very promising. However, I am wondering about the functionality of the pipeline feature. From the documentation it seems that I can chain an input file to an output file and each logged operation will be appended to the repro argument when I commit. In other words it is not clear to me <strong>how to create a batch process<\/strong> without first defining each step separately.<\/p>\n<p>It is also not clear to me <strong>how to handle multiple input and output files.<\/strong><\/p>\n<p>I am very curious if this would be possible.<\/p>\n<p>Here is an example of an offline analysis that I am performing on human EEG data and behavioral data (using python and java):<\/p>\n<pre><code>sample size \u00d7 \"ASCII_\" + n + \".txt\" --&gt; Pandas --&gt; SPSS Syntax --&gt; Tables + Figures\n\n    --&gt; First Level Summary: Individual Participants (Diagnostics)\n   \/\n--&lt;\n   \\\n    --&gt; Second Level Summary: Group Level\n\n    + --&gt; Behavioral CSV\n\n\nEEG Raw Binary Data --&gt; EEG Manual Preprocessing --&gt; MATLAB Scripts --&gt; Figures\n\n    + --&gt; EEG CSV\n\n\n Behavioral CSV --\n                  \\\n                   &gt;--&gt; Correlations (SPSS Syntax)\n                  \/\n        EEG CSV --<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Documentation: tutorial problem?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/documentation-tutorial-problem\/40",
        "Question_created_time":1530112347491,
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":728,
        "Question_body":"<p>I understand this may not be fully related to dvc, but since the problem happens when following the tutorial at <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noopener\">https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46<\/a>. So here is my question:<\/p>\n<p>When following tutorial at the step in executing<\/p>\n<pre><code>dvc run -d data\/Posts.tsv -d code\/split_train_test.py         -d code\/conf.py         -o data\/Posts-test.tsv -o data\/Posts-train.tsv         python code\/split_train_test.py 0.33 20180319\n<\/code><\/pre>\n<p>it throws error<\/p>\n<pre><code>from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \\\nImportError: \/tmp\/_MEIUqCWxh\/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by \/usr\/lib\/python2.7\/dist-packages\/scipy\/sparse\/_sparsetools.x86_64-linux-gnu.so)\nFailed to run command: Stage 'Posts-test.tsv.dvc' cmd python code\/split_train_test.py 0.33 20180319 failed\n<\/code><\/pre>\n<p>I am not familiar with python, nor data science, but was just trying to evaluate if dvc fits our internal requirement so we can decide if going with dvc or not.<\/p>\n<p>How can I fix this error? Otherwise any even simpler version that can basically just show dataset, model are versioned so we can see the differences, say, between version 0.0.1 and 0.0.2 and its diff, or that kind of things?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Defer loading data when cloning repository?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/defer-loading-data-when-cloning-repository\/33",
        "Question_created_time":1526909062879,
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":560,
        "Question_body":"<p>We currently use a single repository for data science related projects (mainly due to limitations of GitHub private repositories). When cloning that repository, all Git LFS files are downloaded, which takes quite a while. Is there a way that using DVC would allow us to defer loading data from some of our data-science sub-projects until we are ready to work on the notebooks?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking data and code dependencies",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-and-code-dependencies\/28",
        "Question_created_time":1526466665911,
        "Question_answer_count":4,
        "Question_score_count":5,
        "Question_view_count":1589,
        "Question_body":"<p>Hello!<br>\nI am currently evaluating tools for keeping track of data in our workflow, and I am looking at how DVC works. First off, thanks for developing it, Dmitry, as it is a very nice tool.<\/p>\n<p>I have read the tutorial on your blog and there are a couple of question I have, though, which I suspect are quite critical.<\/p>\n<p>First: the use of hard links is a great idea when it comes to speed, but what happens if one changes the data file? I tried and the <code>dvc status<\/code> reports \u201ccorrupted cache data\u201d, as the new hash is different from the saved one. Apparently, there is no way of going back, is dvc assuming that data are immutable? Because it might be in some use cases, but not in general. Also, mistakes might happen.<\/p>\n<p>Second, the run functionality is extremely useful, but it appears to be as much fragile. Apparently, dvc is not tied to git when it comes to tracking which files are used for a specific run: I have to specify the dependency to source code using the -d flag to make runs reproducible. What happens if I do not specify a file that is actually being used? What happens if that file changes and a new run is executed? Apparently, dvc does not check for differences and no message or warning is raised. repro can be forced, and results change, but no warning is raised even in this case.<\/p>\n<p>So, I am interested into knowing if I am using the tool in an inappropriate way: am I missing something in DVC philosophy?<\/p>\n<p>Thanks!<br>\n~Alessandro<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does DVC and a LAN infrastructure where Git repos are not in the computing server?",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-and-a-lan-infrastructure-where-git-repos-are-not-in-the-computing-server\/24",
        "Question_created_time":1525936928519,
        "Question_answer_count":2,
        "Question_score_count":4,
        "Question_view_count":933,
        "Question_body":"<p>I\u2019m developing a Machine Learning group with a colleagues. The local area network infrastructure will be:<\/p>\n<ul>\n<li>Git server (one machine)<\/li>\n<li>Computing-data server (another machine)<\/li>\n<li>Local machines for users<\/li>\n<\/ul>\n<p>The way of working will be:<\/p>\n<ul>\n<li>Each user will have their own git-repos in their local machines (with they authentication keys in their local machines).<\/li>\n<li>Data and scripts have to be transferred to the computing server (data can be temporally in the server, only while is needed by the ML scripts)<\/li>\n<li>Users launch the ML operations from their local machines via SSH in the computing server<\/li>\n<\/ul>\n<p>We are performing some tests, and from now, we are doing synchronization operations in the DVC chain: first ,code and data are synched between the local machine and the computing server via SSH. After, the training is launch via SSH. Next, the outputs are transfered back from server to local machines, so that DVC can track the changes, and son on\u2026<\/p>\n<p>Questions and issues (among others):<\/p>\n<ol>\n<li>The path for the *.dvc file can be specified? (one path for each model)<\/li>\n<li>Can DVC be in the computing server while users have their git-repos in the local machines?<\/li>\n<\/ol>\n<p>Thank you very much for your effort, we really appreciate your work.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data versioning of databases",
        "Question_link":"https:\/\/discuss.dvc.org\/t\/data-versioning-of-databases\/20",
        "Question_created_time":1522998440046,
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":589,
        "Question_body":"<p>Hi,<\/p>\n<p>Are you thinking of supporting data versioning of databases.<br>\nTracking changes of transformations from raw data to cleaned data?<\/p>\n<p>Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    }
]