[
    {
        "Question_title":"About the Troubleshooting category",
        "Question_link":"https:\/\/my.guild.ai\/t\/about-the-troubleshooting-category\/13",
        "Question_created_time":"2020-06-04T22:08:31.920Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":432,
        "Question_body":"<p><a href=\"\/new-topic?category=troubleshooting\">Get help<\/a> with a problem you\u2019re facing. If you have a general question, use <a href=\"\/c\/general\">General<\/a>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pipenv compatibility in 0.9.0",
        "Question_link":"https:\/\/my.guild.ai\/t\/pipenv-compatibility-in-0-9-0\/1035",
        "Question_created_time":"2023-05-15T21:40:07.091Z",
        "Question_answer_count":2,
        "Question_score_count":4,
        "Question_view_count":35,
        "Question_body":"<p>I\u2019ve had some unexpected behavior using <code>pipenv<\/code> in 0.9.0 versus 0.8.2 - using guild check yields the following:<\/p>\n<ul>\n<li>in 0.9.0<\/li>\n<\/ul>\n<pre><code class=\"lang-plaintext\">\u27a4 pipenv run guild check\nguild_version:             0.9.0\nguild_install_location:    \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/lib\/python3.11\/site-packages\/guild\nguild_home:                \/home\/neelav\/.guild\nguild_resource_cache:      \/home\/neelav\/.guild\/cache\/resources\ninstalled_plugins:         config_flags, cpu, dask, disk, dvc, exec_script, gpu, ipynb, keras, memory, perf, python_script, quarto_document, queue, r_script, resource_flags, skopt\npython_version:            3.11.3 (main, Apr  5 2023, 15:52:25) [GCC 12.2.1 20230201]\npython_exe:                \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/bin\/python\nplatform:                  Linux 6.3.1-arch2-1 x86_64\npsutil_version:            5.9.5\ntensorboard_version:       2.13.0\ncuda_version:              12.1\nnvidia_smi_version:        530.41.03\nlatest_guild_version:      0.9.0\n<\/code><\/pre>\n<ul>\n<li>in 0.8.2:<\/li>\n<\/ul>\n<pre><code class=\"lang-plaintext\">\u27a4 pipenv run guild check\nguild_version:             0.8.2\nguild_install_location:    \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/lib\/python3.11\/site-packages\/guild\nguild_home:                \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\nguild_resource_cache:      \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/cache\/resources\ninstalled_plugins:         config_flags, cpu, dask, disk, dvc, exec_script, gpu, ipynb, keras, memory, perf, python_script, quarto_document, queue, r_script, skopt\npython_version:            3.11.3 (main, Apr  5 2023, 15:52:25) [GCC 12.2.1 20230201]\npython_exe:                \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/bin\/python\nplatform:                  Linux 6.3.1-arch2-1 x86_64\npsutil_version:            5.9.5\ntensorboard_version:       2.13.0\ncuda_version:              12.1\nnvidia_smi_version:        530.41.03\nlatest_guild_version:      0.9.0\nA newer version of Guild AI is available. Run 'pip install guildai --upgrade' to install it.\n<\/code><\/pre>\n<p>so in 0.9.0 the <code>guild_home<\/code> variable doesn\u2019t seem to point to the virtual environment like it does in 0.8.2, instead pointing at my user\u2019s <code>.guild<\/code> directory. Moreover, I seem to run into issues with guild moving my sourcecode files into the run directory. I have a small example of this with the following guild file:<\/p>\n<pre><code class=\"lang-plaintext\">train:\n    exec: python .guild\/sourcecode\/train.py\n    output-scalars: False \n    sourcecode:\n        - exclude: '*'\n        - '*.py'\n    requires:\n        -\n            file: guild.yml\n            target-type: copy                            \n<\/code><\/pre>\n<p>and a Hello World script.<\/p>\n<pre><code class=\"lang-plaintext\">def main():\n    print(\"Hello World\")\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>\n<p>The result:<\/p>\n<pre><code class=\"lang-plaintext\">\u27a4 pipenv shell\nLaunching subshell in virtual environment...\nWelcome to fish, the friendly interactive shell\nType help for instructions on how to use fish\n\u27a4  source \/home\/neelav\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/bin\/activate.fish\n\u27a4 guild run train\nYou are about to run train\nContinue? (Y\/n) \nResolving file:guild.yml\npython: can't open file '\/home\/neelav\/.guild\/runs\/d6d109936802465493d5ea91a3439f98\/.guild\/sourcecode\/train.py': [Errno 2] No such file or directory\n<\/code><\/pre>\n<p>and <code>ls<\/code> on the run directory gives:<\/p>\n<pre><code class=\"lang-plaintext\">\u27a4 ls -a ~\/.guild\/runs\/d6d109936802465493d5ea91a3439f98\/.guild\/\n.\/  ..\/  attrs\/  manifest  opref  output  output.index\n<\/code><\/pre>\n<p>In contrast, using version 0.8.2 with this configuration:<\/p>\n<pre><code class=\"lang-plaintext\">\u27a4 guild run train\nYou are about to run train\nContinue? (Y\/n) \nResolving file:guild.yml\nHello World\n\u27a4 guild runs\n[1:4c264831]  train  2023-05-15 17:34:11  completed  \n\u27a4 guild ls 1\n~\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/runs\/4c264831b090451d86592dae5d359f3a:\n  guild.yml\n\u27a4 ls -a ~\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/runs\/4c264831b090451d86592dae5d359f3a\n.\/  ..\/  .guild\/  guild.yml\n\u27a4 ls -a ~\/.local\/share\/virtualenvs\/guild_0.9.0_demo-UrtH4SR2\/.guild\/runs\/4c264831b090451d86592dae5d359f3a\/.guild\/\n.\/  ..\/  attrs\/  manifest  opref  output  output.index  sourcecode\/\n<\/code><\/pre>\n<p>So something seems to have changed regarding how guild interacts with <code>pipenv<\/code> between 0.8.2 and 0.9.0. Are there changes to the guild file that I need to make in 0.9.0 to get that functionality from 0.8.2?<\/p>",
        "Question_closed_time":"2023-05-23T09:20:50.774Z",
        "Answer_body":"<p>Hello! My apologies for the late reply here\u2026<\/p>\n<p>You\u2019re running into two changes in 0.9:<\/p>\n<ol>\n<li>In 0.9, Guild treats <code>.guild<\/code> directories in the current directory as \u2018Guild home\u2019 by default. To use an explicit location, set <code>GUILD_HOME<\/code>. Alternatively, modify <code>~\/.guild\/config.yml<\/code> to include:<\/li>\n<\/ol>\n<pre><code class=\"lang-yaml\"># ~\/.guild\/config.yml\n\nlegacy:\n  guild-home: pre-0.9\n<\/code><\/pre>\n<p>If this legacy behavior should be applied to specific projects, create <code>guild-config.yml<\/code> in your project directory (i.e. along side <code>guild.yml<\/code>). This file provides the same config as ~\/.guild\/config.yml` but applies to your project rather than to your system.<\/p>\n<ol start=\"2\">\n<li>In 0.9, Guild copies source code to the run directory root, rather than to <code>.guild\/sourcecode<\/code>. You can change this in your Guild file (<code>guild.yml<\/code>) using the <code>sourcecode<\/code> attribute for your operation.<\/li>\n<\/ol>\n<pre><code class=\"lang-yaml\"># guild.yml\n\ntrain:\n  sourcecode:\n    dest: .guild\/sourcecode\n    ...\n<\/code><\/pre>\n<p>However, in your case, consider changing <code>exec<\/code> simply to <code>python train.py<\/code>.<\/p>\n<p>I apologize for the breakage here! Guild should have done a better job here at surfacing the problem and pointing you to a solution. I\u2019ve opened an <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/502\">issue<\/a> to address this in 0.9.1 with additional help.<\/p>\n<p>We\u2019ll also confirm that the <code>legacy<\/code> switch above is properly documented.<\/p>",
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Confusion on multistep operations, restarting substeps, and copied files?",
        "Question_link":"https:\/\/my.guild.ai\/t\/confusion-on-multistep-operations-restarting-substeps-and-copied-files\/998",
        "Question_created_time":"2023-02-21T00:18:16.104Z",
        "Question_answer_count":7,
        "Question_score_count":3,
        "Question_view_count":128,
        "Question_body":"<p>I have a guild operation <code>main<\/code> that runs 3 steps which are other operations: <code>impute<\/code>, <code>evaluate<\/code>, and <code>predict<\/code>. The latter two require on the <code>impute<\/code> operation (specifically a model checkpoint and some data output).<br>\n(<a href=\"https:\/\/github.com\/davzaman\/autopopulus\/blob\/dev\/guild.yml\" rel=\"noopener nofollow ugc\">guild.yml<\/a> file for reference)<\/p>\n<ol>\n<li>When one of the steps fails (e.g. <code>evaluate<\/code>), the <code>main<\/code> op shows error and so does <code>evaluate<\/code>. If I fix the error in the code and restart the run with something like <code>for hash in $(guild select --operation evaluate --error --all); do guild run -y --background --restart $hash --force-sourcecode; done<\/code>,  then the <code>evaluate<\/code> op fixes to completed, but the <code>main<\/code> operation does not. It doesn\u2019t seem very possible to update it, but it is slightly unclean and annoying to keep track of what broke and what is fixed. I end up with something like:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\">[71:ec03c916]   evaluate  2023-02-20 14:43:57  completed  dvae myexperiment\n[72:957ecb30]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n[73:19493e6b]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n...\n[127:fe72a7ff]  predict   2023-02-18 20:58:56  completed  dvae \n[128:617bc8fd]  impute    2023-02-18 20:26:16  completed  dvae myexperiment\n[129:2b155ff0]  main      2023-02-18 20:26:14  error      dvae \n[130:39125144]  predict   2023-02-18 20:21:08  completed  dvae \n[131:5c4ed46a]  impute    2023-02-18 19:45:25  completed  dvae myexperiment\n[132:c542fcbe]  main      2023-02-18 19:45:24  error      dvae \n<\/code><\/pre>\n<p>It said <code>error<\/code> for <code>main<\/code> but it\u2019s really been fixed sine the <code>evaluate<\/code> op was fixed.<br>\nAnother issue is also what files are stored under each op which leads me to the next point, where ill use <code>run 132<\/code> as an example:<\/p>\n<ol start=\"2\">\n<li>If I look at what is stored under the <code>main<\/code> op I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-shell\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ ls\nevaluate  impute  options.yml  predict\n<\/code><\/pre>\n<p>If I drill into the directories I see:<\/p>\n<pre><code class=\"lang-plaintext\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ cd evaluate\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a\/evaluate$ ls\nF.O.  options.yml  serialized_models\n<\/code><\/pre>\n<p>If <code>evaluate<\/code> fails and I rerun it, does that mean that the <code>evaluate<\/code>folder will be updated too (is it a symlink)? There seems to be some redundancy too which leads me to:<\/p>\n<ol start=\"3\">\n<li>If I look at the output of the substeps <code>impute<\/code> and <code>predict<\/code> I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n<\/code><\/pre>\n<p>I also see<\/p>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n<\/code><\/pre>\n<p>It looks like it copies over everything from the <code>impute<\/code> op top the parents: <code>main<\/code>, and dependent steps: <code>predict<\/code>, and <code>evaluate<\/code>. This is a lot of redundancy especially for expensive\/large models and artifacts. This is making me run out of space on my machine.<\/p>\n<p>My questions are<br>\na) How do I avoid redundancy in stored artifacts between parent and child steps like <code>main<\/code> having substeps.<br>\nb) How do I avoid redundancy amongst sibling runs where one may be dependent on another? While <code>evaluate<\/code> relies on the artifacts from <code>impute<\/code> I don\u2019t want it to store all the artifacts all over again (including the model checkpoints, data, and the logging files), I just want <code>evaluate<\/code> to use the checkpointed data and model. <a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192#required-operation-files-14\">I know there\u2019s a <code>select:<\/code> option<\/a> but it seems to be regex, making it complicated to select the checkpointed model AND data. Also even if that solves excluding the logged files, I don\u2019t want to copy over the files it relies on to the final logged artifacts.<\/p>",
        "Question_closed_time":"2023-03-14T22:10:00.198Z",
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/davzaman\">@davzaman<\/a> It looks like there was a regression and Guild is indeed <em>copying<\/em> resolved operation dependency files. This is not the intended behavior and we\u2019ll fix that ASAP.<\/p>\n<p>As a workaround, avoid copying files by adding <code>target-type<\/code> to your dependency def like this:<\/p>\n<pre><code class=\"lang-yaml\">upstream: {}\n\ndownstream:\n  requires:\n    - operation: upstream\n      target-type: link  # tells Guild to link to the resolved files, not copy\n<\/code><\/pre>\n<p>The <code>downstream<\/code> operation is any operation that requires an upstream run.<\/p>\n<p>Sorry about that! This will make a big difference in disk space for you. We\u2019ll post here when the fix is applied, after which you can remove the explicit <code>target-type<\/code> in your dependencies.<\/p>",
        "Question_self_resolution":false
    },
    {
        "Question_title":"Specifying different target types for the same operation",
        "Question_link":"https:\/\/my.guild.ai\/t\/specifying-different-target-types-for-the-same-operation\/761",
        "Question_created_time":"2021-09-10T19:35:38.330Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":352,
        "Question_body":"<p>I am trying to link and copy different files from the same operation resource in my guild stage.  It seems that specifying an operation multiple times only results in the last operation being used.  Here is an example that matches what I\u2019m trying to do where a operation train is used as a resource.  The file model.pth is linked while config.json is copied.<\/p>\n<pre><code>test:\n  exec: \"python test.py\"\n  requires:\n    - operation: train\n      select:\n        - file: model.pth\n          target-type: link\n        - file: config.json\n          target-type: copy\n<\/code><\/pre>\n<p>Is there any way to achieve this kind of functionality? Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Flags-dest only delivers defined flags, not config file",
        "Question_link":"https:\/\/my.guild.ai\/t\/flags-dest-only-delivers-defined-flags-not-config-file\/1004",
        "Question_created_time":"2023-02-24T15:56:57.633Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":77,
        "Question_body":"<p>Hi,<\/p>\n<p>I have an issue in which <code>flags-dest<\/code> only seems to work for flags I define explicitly under <code>flags<\/code>.  I have a Hydra config with nested values and following <a href=\"https:\/\/github.com\/guildai\/guildai\/tree\/main\/examples\/hydra\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">guildai\/examples\/hydra at main \u00b7 guildai\/guildai \u00b7 GitHub<\/a> .<\/p>\n<p>Using <code>flags-dest: config:&lt;config_path&gt;<\/code> doesn\u2019t load the values into the Hydra config object when using the decorator, I have to manually load it.<\/p>\n<p>Using <code>flags-dest: globals<\/code> only delivers global variables that have been defined under <code>flags<\/code>.<\/p>\n<p>Since manually loading the config file works, with correct nesting and values were overwritten correctly  that were specified  either via a flag in  <code>guild.yaml<\/code> or as a command line argument,  then Guild is loading the config values correctly, right? It just doesn\u2019t deliver overwritten values back to the config file.<\/p>\n<p>Except with <code>flags-dest: globals<\/code>; then it doesn\u2019t deliver original config values that were left untouched.<\/p>\n<p>My <code>guild.yaml<\/code>:<\/p>\n<pre><code class=\"lang-plaintext\">train:\n      flags-dest: config:embeddings_and_difficulty\/configs_hydra\/config.yaml\n      flags-import: all\n      flags:\n        label_type: test\n      sourcecode:\n        - exclude: embeddings_and_difficulty\/data\n      requires:\n        - resource: data\n        - config: embeddings_and_difficulty\/configs_hydra\/config.yaml\n          target-path: .guild\/sourcecode\/embeddings_and_difficulty\/configs_hydra\n          replace-existing: yes\n      main: runner\n\n<\/code><\/pre>\n<p><code>runner.py<\/code>:<\/p>\n<pre><code class=\"lang-plaintext\">@hydra.main(version_base=None, config_path='configs_hydra',\n            config_name='config.yaml')\ndef main(args: DictConfig):\n   (...)\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Resolving a nested ini\/cfg file changes its format",
        "Question_link":"https:\/\/my.guild.ai\/t\/resolving-a-nested-ini-cfg-file-changes-its-format\/995",
        "Question_created_time":"2023-02-13T13:43:40.136Z",
        "Question_answer_count":5,
        "Question_score_count":3,
        "Question_view_count":152,
        "Question_body":"<p>First, thanks for the awesome tool!<\/p>\n<p>I am having some trouble with the following behaviour. I use <code>.ini<\/code> config files with nested sections, and import the flags from there, but when Guild resolves the files and write a copy to the run dir, the format changes and the nesting is lost. I would need it to keep the same formatting.<\/p>\n<p>I have a config file with nested sections, like:<\/p>\n<pre data-code-wrap=\"config\"><code class=\"lang-plaintext\">[ingredients]\ngreens = cabbage\n\n[ingredients.meat]\ntype = pork belly\nmarbled = true\n\n[ingredients.carbs]\ntype = noodles\n\n[ingredients.carbs.make]\n\n[ingredients.carbs.make.water]\nadd_miso = true\ncontainer = cooking pot\ntemp = 100\n<\/code><\/pre>\n<p>And a guild file:<\/p>\n<pre data-code-wrap=\"guild\"><code class=\"lang-plaintext\">cook:\n  exec: cat source_config.cfg\n  flags-import: all\n  flags-dest: config:source_config.cfg\n<\/code><\/pre>\n<p>Running this operation gives prints out the resulting config file in the run dir:<\/p>\n<pre><code class=\"lang-plaintext\">[ingredients]\ncarbs = {'make': {'water': {'add_miso': True, 'container': 'cooking pot', 'temp': 100}}, 'type': 'noodles'}\ngreens = paksoi\nmeat = {'marbled': True, 'type': 'pork belly'}\n<\/code><\/pre>\n<p>I can understand what happened, but for my purposes, this is a destructive change: I can\u2019t read this config file inside the run.<\/p>\n<p>Practically speaking, I use guild to run machine learning experiments backed by Spacy. I read in the created config file with <code>confection<\/code> (Spacy\u2019s config handler), which doesn\u2019t support reading those dicts. The upshot is that I can\u2019t use guild to run sweeps. Would love to see a solution to this.<\/p>",
        "Question_closed_time":"2023-03-03T22:16:13.762Z",
        "Answer_body":"<p>You can find the fix here <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/482\" class=\"inline-onebox\">Guild not preserving nested INI sections as expected \u00b7 Issue #482 \u00b7 guildai\/guildai \u00b7 GitHub<\/a>, it\u2019s merged into main, and hasn\u2019t been released, but you can run from source code (via <a href=\"https:\/\/my.guild.ai\/t\/install-guild-ai\/37\" class=\"inline-onebox\">Install Guild AI<\/a> under the \u201cFrom Source Code\u201d section). Let us know if that works for you.<\/p>",
        "Question_self_resolution":false
    },
    {
        "Question_title":"Tensorboard PermissionError due to trailing space in label",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-permissionerror-due-to-trailing-space-in-label\/1005",
        "Question_created_time":"2023-02-27T09:05:09.867Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"<p>Hello, I noticed that when the label of a run has a trailing space, <code>guild tensorboard<\/code> cannot start.<\/p>\n<h1>\n<a name=\"example-1\" class=\"anchor\" href=\"#example-1\"><\/a>Example<\/h1>\n<p>If I run my experiment like so,<\/p>\n<pre><code class=\"lang-plaintext\">guild run XGBClassifier:train --label \"TEST\"\n<\/code><\/pre>\n<p>then TensorBoard opens fine.<\/p>\n<p>But when I put a space behind the label name,<\/p>\n<pre><code class=\"lang-plaintext\">guild run XGBClassifier:train --label \"TEST \"\n<\/code><\/pre>\n<p>then <code>guild tensorboard<\/code> results in this trace:<\/p>\n<pre><code class=\"lang-plaintext\">Preparing runs for TensorBoard\nERROR: error removing C:\\Users\\user01\\AppData\\Local\\Temp\\guild-tensorboard-13xxcmby: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\user01\\\\AppData\\\\Local\\\\Temp\\\\guild-tensorboard-13xxcmby\\\\570734cd XGBClassifier_train 2023-02-27 16_14_43 TEST '\nTraceback (most recent call last):\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\util.py\", line 646, in _windows_symlink\n    subprocess.check_output(args, shell=True, stderr=subprocess.STDOUT)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\", line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\", line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['mklink', 'C:\\\\Users\\\\user01\\\\AppData\\\\Local\\\\Temp\\\\guild-tensorboard-13xxcmby\\\\570734cd XGBClassifier_train 2023-02-27 16_14_43 TEST \\\\fold0\\\\mfa_loadings.png', 'C:\\\\Users\\\\user01\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\venv\\\\proj01\\\\.guild\\\\runs\\\\570734cd4c664dec9a57db2f12b9b3b2\\\\fold0\\\\mfa_loadings.png']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\main_bootstrap.py\", line 29, in main\n    _main()\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\main_bootstrap.py\", line 54, in _main\n    guild.main.main()\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\main.py\", line 30, in main\n    main_cmd.main(standalone_mode=False)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\click\\core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\click\\core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\click_util.py\", line 235, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 130, in tensorboard\n    tensorboard_impl.main(args)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 41, in main\n    _run_tensorboard(args)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 83, in _run_tensorboard\n    monitor.run_once(exit_on_error=True)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\run_util.py\", line 96, in run_once\n    self._refresh_logdir(runs)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\run_util.py\", line 103, in _refresh_logdir\n    self.refresh_run_cb(run, run_path)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\tensorboard.py\", line 279, in f\n    return _refresh_run(run, run_logdir, state)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\tensorboard.py\", line 288, in _refresh_run\n    _sync_run_file_links(run, run_logdir)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\tensorboard.py\", line 474, in _sync_run_file_links\n    _add_missing_run_file_links(run, run_logdir)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\tensorboard.py\", line 480, in _add_missing_run_file_links\n    _ensure_run_file_link(src, link, link_dir)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\tensorboard.py\", line 526, in _ensure_run_file_link\n    util.symlink(src, link)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\util.py\", line 630, in symlink\n    _windows_symlink(target, link)\n  File \"C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\\util.py\", line 650, in _windows_symlink\n    raise OSError(e.returncode, err_msg) from e\nPermissionError: [Errno 1] The system cannot find the path specified.\n<\/code><\/pre>\n<p>I think the space is preventing <code>mklink<\/code> from parsing the path arguments correctly.<\/p>\n<h1>\n<a name=\"why-it-is-problematic-2\" class=\"anchor\" href=\"#why-it-is-problematic-2\"><\/a>Why it is problematic<\/h1>\n<p>When no label is passed to <code>guild run<\/code>, the experiment flags are automatically truncated to create a label. However, sometimes the flags just happen to be the correct length where the truncated label has a trailing space. Thus the error occurs.<\/p>\n<h1>\n<a name=\"guild-details-3\" class=\"anchor\" href=\"#guild-details-3\"><\/a>Guild details<\/h1>\n<p>Here is the output of <code>guild check<\/code><\/p>\n<pre><code class=\"lang-plaintext\">guild_version:             0.9.0\nguild_install_location:    C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\lib\\site-packages\\guild\nguild_home:                C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\.guild\nguild_resource_cache:      C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\.guild\\cache\\resources\ninstalled_plugins:         config_flags, cpu, dask, disk, dvc, exec_script, gpu, ipynb, keras, memory, perf, python_script, quarto_document, queue, r_script, resource_flags, skopt\npython_version:            3.9.13 (tags\/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\npython_exe:                C:\\Users\\user01\\AppData\\Local\\Programs\\Python\\Python39\\venv\\proj01\\Scripts\\python.exe\nplatform:                  Windows 10 AMD64\npsutil_version:            5.9.4\ntensorboard_version:       2.11.1\ncuda_version:              11.7\nnvidia_smi_version:        517.40\nlatest_guild_version:      0.9.0\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error with async",
        "Question_link":"https:\/\/my.guild.ai\/t\/error-with-async\/1001",
        "Question_created_time":"2023-02-23T15:25:39.937Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":81,
        "Question_body":"<p>Hi, I\u2019m new to guild, so I\u2019m sorry if I\u2019m asking something trivial\u2026<\/p>\n<p>I\u2019m trying to run a training operation, which it\u2019s quite simple I guess: I have a training script (training-mrcnn.py) and a couple of dependencies for tht script (a couple of python classes and functions).<br>\nAll the hyperparameters for configuration are managed by detectron2 (v0.6) and my script for now. So far, I have this guidl.yaml:<\/p>\n<pre><code class=\"lang-yaml\">train:\n  description: Sample training script\n  main: train-mrcnn\n  output-scalars: off\n  requires:\n    - file: 220928-01-200SynthBacksTM-blur-CUSTOM.yaml\n    - file: ..\/..\/custom\/rpnt.py\n    - file: ..\/..\/custom\/custom_coco_evaluation.py\n<\/code><\/pre>\n<p>But it seems it doesn\u2019t even start, because of an error with the async library (neede by detectron2 \u2192 torch). Here\u2019s the stack trace:<\/p>\n<pre><code class=\"lang-bash\">guild run train\nRefreshing flags...\nWARNING: cannot import flags from train-mrcnn.py: ModuleNotFoundError: No module named 'custom' (run with guild --debug for details)\nYou are about to run train\nContinue? (Y\/n)     \nWARNING: Skipping potential source code file \/home\/lucas\/Killme\/guild-pipeline-t4-real\/model\/src\/d2\/experiments\/220928-01-200SynthBacksTM-blur-CUSTOM\/exp-01\/inference\/coco_intances_results_0000499.json because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.\nWARNING: Skipping potential source code file \/home\/lucas\/Killme\/guild-pipeline-t4-real\/model\/src\/d2\/experiments\/220928-01-200SynthBacksTM-blur-CUSTOM\/exp-01\/inference\/coco_intances_results_0001000.json because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.\nResolving file:220928-01-200SynthBacksTM-blur-CUSTOM.yaml\nResolving file:..\/..\/custom\/rpnt.py\nResolving file:..\/..\/custom\/custom_coco_evaluation.py\nTraceback (most recent call last):\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/.guild\/runs\/02d9f1bb04ae477cb19a954b49bd2456\/.guild\/sourcecode\/train-mrcnn.py\", line 12, in &lt;module&gt;\n    from fvcore.nn.precise_bn import get_bn_modules\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/fvcore\/nn\/__init__.py\", line 2, in &lt;module&gt;\n    from .activation_count import ActivationCountAnalysis, activation_count\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/fvcore\/nn\/activation_count.py\", line 7, in &lt;module&gt;\n    import torch.nn as nn\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/torch\/__init__.py\", line 711, in &lt;module&gt;\n    from torch import hub as hub\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/torch\/hub.py\", line 18, in &lt;module&gt;\n    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if available\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/tqdm\/auto.py\", line 29, in &lt;module&gt;\n    from .asyncio import tqdm as asyncio_tqdm\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/tqdm\/asyncio.py\", line 10, in &lt;module&gt;\n    import asyncio\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/asyncio\/__init__.py\", line 21, in &lt;module&gt;\n    from .base_events import *\n  File \"\/media\/userFiles\/00.bin\/pyEnv\/p39d06-GPU\/lib\/python3.9\/site-packages\/asyncio\/base_events.py\", line 296\n    future = tasks.async(future, loop=self)\n<\/code><\/pre>\n<p>The curious thing, it\u2019s that I don\u2019t have this error when I manually run the training script \u2026<br>\nI\u2019m using phython 3.9.16<\/p>\n<p>Any help will be appreciated! Thanks in advance!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using flags-dest in pipelines",
        "Question_link":"https:\/\/my.guild.ai\/t\/using-flags-dest-in-pipelines\/971",
        "Question_created_time":"2023-01-04T08:25:29.030Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"<p>I can\u2019t seem to find a use case which describe using <code>flags-dest<\/code> in a pipeline of steps that all specifies flags using <code>flags-dest<\/code> and <code>flags-import<\/code>.<\/p>\n<p>It seems I must explicitly specify the flags that needs to be passed to individual steps in a pipeline, which is quite trivial. I wonder if there\u2019s a way I can automatically pass the flags to all the steps in the pipeline? E.g. something like this<\/p>\n<pre><code class=\"lang-yaml\">ops1:\n  flags-dest: config:config.yaml\n  flags-import: all\n  ...\n\nops2:\n  flags-dest: config:config.yaml\n  flags-import: all\n  ...\n\npipeline:\n  flags-dest: config:config.yaml\n  flags-import: all\n  steps:\n    - run: ops2\n      $include:all-flags\n    - run: ops2\n      $include:all-flags\n\n# This works less convinient obviously if I have many flags\npipeline_current:\n  flags-dest: config:config.yaml\n  flags-import: all\n  steps:\n    - ops1 flag1=${flag1} flag2=${flag2}\n    - ops2 flag1=${flag1} flag2=${flag2}\n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"__main__ has no attribute __spec__ pytorch-lightning multiGPU",
        "Question_link":"https:\/\/my.guild.ai\/t\/main-has-no-attribute-spec-pytorch-lightning-multigpu\/946",
        "Question_created_time":"2022-10-22T01:25:32.704Z",
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":230,
        "Question_body":"<p>When using guild to run experiments on multiple GPUs with pytorch-lightning I get the following error:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/main.py\", line 219, in &lt;module&gt;\n10\/21\/2022 5:54:52 PM\tmain()\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/main.py\", line 95, in main\n10\/21\/2022 5:54:52 PM\timputed_data = get_imputation_logic(args)(args, data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/task_logic\/ae_imputation.py\", line 119, in ae_imputation_logic\n10\/21\/2022 5:54:52 PM\tae_imputer = create_autoencoder(args, data, settings)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/task_logic\/tuner.py\", line 69, in create_autoencoder\n10\/21\/2022 5:54:52 PM\tae_imputer.fit(data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 148, in fit\n10\/21\/2022 5:54:52 PM\tself._fit(data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/.guild\/runs\/4d62c69c236641b8b2d384bed79b64de\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 166, in _fit\n10\/21\/2022 5:54:52 PM\tself.trainer.fit(self.ae, datamodule=data)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 770, in fit\n10\/21\/2022 5:54:52 PM\tself._call_and_handle_interrupt(\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 721, in _call_and_handle_interrupt\n10\/21\/2022 5:54:52 PM\treturn self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 92, in launch\n10\/21\/2022 5:54:52 PM\tself._call_children_scripts()\n10\/21\/2022 5:54:52 PM\tFile \"\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 109, in _call_children_scripts\n10\/21\/2022 5:54:52 PM\tif __main__.__spec__ is None: # pragma: no-cover\n10\/21\/2022 5:54:52 PM\tAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>So I went into the file <code>\/home\/davina\/mambaforge\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 109, in _call_children_scripts<\/code> and changed <code>if __main__.__spec__ is None<\/code> to <code>if True<\/code> as a stopgap, and it seemed to work. For some reason <code>__main__<\/code> is a dictionary.<\/p>\n<p>This isn\u2019t the exact same problem, but I happened to find a somewhat relevant problem <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/3859\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Use parameters with include flags",
        "Question_link":"https:\/\/my.guild.ai\/t\/use-parameters-with-include-flags\/952",
        "Question_created_time":"2022-11-03T02:31:05.957Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":108,
        "Question_body":"<p>I\u2019m trying to use parameters to set portions of flags, but it seems that the parameters don\u2019t affect flags which have been imported. Here\u2019s a simple case:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">- config: config\n  flags:\n    flag-key: \"{{param-key}}\"\n\n- model: model\n  params:\n    param-key: param-value\n  operations:\n    op:\n      main: path\n      flags:\n        $include: config\n<\/code><\/pre>\n<p>This results in flag:<br>\nflag-key: \u2018{{param-key}}\u2019<br>\nwhereas I\u2019d want it to be  the same result as if I declared the flag directly in the operation config:<br>\nflag-key: param-value<\/p>\n<p>In my specific use-case I\u2019m also getting the parameters from a config which this model extends, in case that affects the issue. Essentially the setup is that we have all parameters in a single config for organization, and there\u2019s many models\/operations which each use one of a few different sets of flags, and all flag sets reference the parameters.  Any recommendations or workarounds would be appreciated, thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Strange out-of-memory behavior on Guild with XGBoost",
        "Question_link":"https:\/\/my.guild.ai\/t\/strange-out-of-memory-behavior-on-guild-with-xgboost\/931",
        "Question_created_time":"2022-10-17T06:01:42.533Z",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":241,
        "Question_body":"<p>Hi all, I\u2019m using Guild to manage and tune XGBoost models for a binary classification problem. My dataset is around 2MB and has around 20K rows of 15 features. My XGBClassifier has around 100 estimators, the max depth is 6, and the tree method is <code>gpu-hist<\/code>.<\/p>\n<p>When I run my program in VS Code it executes with no problems. When I run the same program in the command line with Guild, it also finishes without throwing any error. But when I look at the run in <code>guild view<\/code> or <code>guild runs<\/code>, it says that the run exited with an error status <code>3221226505<\/code>.<\/p>\n<p>Online sources say this generic python error is some form of out of memory error. However, this can\u2019t be the case as I monitored the RAM and VRAM usage while executing my program and they were both very low.<\/p>\n<p>When I switch the tree method to just <code>hist<\/code> (cpu-only training) and re-run the program, <code>guild view<\/code> now shows the run as <code>completed<\/code>.<\/p>\n<p>May I know if this is a bug? My GPU is a Quadro T1000, my XGBoost version is <code>1.6.2<\/code>, and here is part of the <code>guild check<\/code> output:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild_version:             0.8.1\npython_version:            3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]                            \nplatform:                  Windows 10 AMD64                                                                               \npsutil_version:            5.9.2\ntensorboard_version:       2.10.1\ncuda_version:              11.7\nnvidia_smi_version:        516.69\nlatest_guild_version:      0.8.1   \n<\/code><\/pre>\n<p>Thank you!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pytorch Lightning, Distributed Data Parallel & remote",
        "Question_link":"https:\/\/my.guild.ai\/t\/pytorch-lightning-distributed-data-parallel-remote\/953",
        "Question_created_time":"2022-11-03T21:21:58.376Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":393,
        "Question_body":"<p>Hi,<br>\nI\u2019ve setup my model with Pytorch Lightning, and want to train it on a remote workstation (single machine, multiple GPUs) using multiple GPUs. The recommended strategy to use is DDP.<\/p>\n<p>I\u2019ve successfully run the model locally on 1 GPU, remotely on 1 GPU using DDP and DP strategies, and on multiple GPUs using DP strategy.<\/p>\n<p>However, when running DDP strategy with multiple GPUs on the remote, the processs hangs indefinitely on the first GPU initialization. The only indication in the Guild output that something is not correct is this:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">Installing package and its dependencies\nProcessing .\/gpkg.my_package-0.1-py2.py3-none-any.whl\nInstalling collected packages: gpkg.my_package\nSuccessfully installed gpkg.my_package-0.1\nStarting my_model:train on my_remote as 65f7f7f516f54c78900bcd313a6f906c\n[some file resolves]\n[some PLT info]\n \n**guild.op_main: missing required arg**\n\nINFO: [pytorch_lightning.utilities.distributed] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1\/2\n<\/code><\/pre>\n<p>The process never progresses from this stage. Running <code>nvidia-smi<\/code> shows no processes on any GPUs, and running <code>htop<\/code> shows no processes on the CPU.<\/p>\n<p>Below the training part of the script:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># Define the Pytorch Lightning trainer\ntrainer = pl.Trainer(\n        # auto_scale_batch_size='binsearch',\n        auto_lr_find=config.auto_lr,\n        fast_dev_run=config.fast_dev_run,\n        max_epochs=config.epochs,\n        accelerator=\"gpu\",\n        strategy='ddp',\n        devices=config.gpus,\n        precision=16,\n        callbacks=[\n            # pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-2),\n            pl.callbacks.EarlyStopping(monitor='val_auc', mode='max'),\n            pl.callbacks.LearningRateMonitor()\n        ]\n)\n\n# Tune the training parameters\ntrainer.tune(model)\n# Train\ntrainer.fit(model=model)\n# Test\ntrainer.test(model=model)\n<\/code><\/pre>\n<p>And the guild.yml file:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">- package: gpkg.my_package\n  description: My package\n  version: 0.1\n  data-files:\n    - '..\/datasets\/'\n    - 'my_model\/models\/model_definition.py'\n    - 'my_model\/train.py'\n    - 'my_model\/models\/loss.py'\n    - 'my_model\/models\/memory.py'\n    - 'my_model\/config.yaml'\n    - '.\/training_utils.py'\n    - '.\/model_utils.py'\n\n- model: my_model\n  description: \n  operations:\n    train:\n      description: Train my model\n      label: \"my_model:train - dataset: ${dataset_name}\"\n      sourcecode:\n        - my_model\/train.py\n        - my_model\/models\/model_definition.py\n        - my_model\/models\/loss.py\n        - my_model\/models\/memory.py\n        - training_utils.py\n        - model_utils.py\n      requires:\n        - config: my_model\/config.yaml\n        - file: ..\/datasets\/\n      main: my_model\/train\n      flags-dest: config:my_model\/config.yaml\n      flags-import: all\n      flags:\n        # Training parameters\n        auto_lr: True\n        epochs: 2\n        fast_dev_run: 100\n        gpus: (0,)\n        av: False\n        # Dataset parameters\n        dataset_name: my_dataset\n        batch_size: 2\n        num_workers: 6\n        model_input_size: (256,256)\n        data_path: '..\/..\/..\/data\/use_case\/my_dataset'\n        fraction: 1.0\n        crop_params: False\n        win_len: 16\n        # Model parameters\n        channels: 3\n        mem_dim: 2000\n        thresh: 0.0025\n        loss_weight: 0.0002\n        lr: 10e-5\n        wd: 10e-4\n<\/code><\/pre>\n<p>Perhaps there is a mistake in the package part? It\u2019s not entirely clear to me if that\u2019s the correct way to do this, and perhaps I\u2019m doing something redundant and\/or incorrect that prevents Guild from properly initializing my scripts?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dependencies problem",
        "Question_link":"https:\/\/my.guild.ai\/t\/dependencies-problem\/943",
        "Question_created_time":"2022-10-20T06:40:30.613Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":162,
        "Question_body":"<p>Hello! Pretty new to guild.ai and trying to use it on my recent project. I tried to run my code with guild.ai wrapper, but it does not run due to a dependencies issue (\u201cbpe_simple_vocab_16e6.txt.gz\u201d is missing from the running folder, despite every other file in the same folder with it being included). I tried to solve it by including requires in my guild.yml:<\/p>\n<p>requires:<\/p>\n<ul>\n<li>file: pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz<\/li>\n<\/ul>\n<p>but despite the relative path is correct, the error pops up saying<\/p>\n<p>ERROR: resolving required source \u2018file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz\u2019 in file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz resource<br>\nTraceback (most recent call last):<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/op_dep.py\u201d, line 236, in resolve_source<br>\nsource_paths = _resolve_source_for_location(<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/op_dep.py\u201d, line 267, in _resolve_source_for_location<br>\nreturn resolver.resolve(resolve_context)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 114, in resolve<br>\nresolved = self._resolve_source_files(source_path, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 138, in _resolve_source_files<br>\nreturn resolve_source_files(source_path, self.source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 553, in resolve_source_files<br>\nreturn _resolve_source_file_or_archive_files(source_path, source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 618, in _resolve_source_file_or_archive_files<br>\nreturn _resolve_archive_files(source_path, archive_type, source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 674, in _resolve_archive_files<br>\nunpacked = _ensure_unpacked(source_path, archive_type, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 686, in _ensure_unpacked<br>\nunpacked = _unpack(source_path, archive_type, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 711, in _unpack<br>\nreturn _gunzip(source_path, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 749, in _gunzip<br>\nreturn _gen_unpack(<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 806, in _gen_unpack<br>\nextract(unpack_dir, to_extract)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 781, in extract<br>\nwith open(dest, \u201cwb\u201d) as f_out:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018\/home\/hyang\/deadclip\/CyCLIP\/env\/.guild\/cache\/resources\/6d8d1935c2ed1916a4210b8f6391e63d846c5f02d88deced8f8fa21b\/bpe_simple_vocab_16e6.txt\u2019<br>\nguild: run failed because a dependency was not met: unexpected error resolving \u2018file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz\u2019 in file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz resource: FileNotFoundError(2, \u2018No such file or directory\u2019)<\/p>\n<p>I also tried to toy with the relative path with \u201c\u2026\u201d or use the absolute path, but it still didn\u2019t work<\/p>\n<p>Any help would be appreciated :))<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Torch.multiprocessing.spawn fails",
        "Question_link":"https:\/\/my.guild.ai\/t\/torch-multiprocessing-spawn-fails\/929",
        "Question_created_time":"2022-10-16T20:08:59.224Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":307,
        "Question_body":"<p>I have some code that works standalone, but fails when run from guild.  The offending line is:<\/p>\n<pre><code>torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args))\n<\/code><\/pre>\n<p>and the complaint is:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">  [...] \n  File \"\/usr\/lib\/python3.10\/multiprocessing\/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/context.py\", line 288, in _Popen\n    return Popen(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/spawn.py\", line 183, in get_preparation_data\n    main_mod_name = getattr(main_module.__spec__, \"name\", None)\nAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>Does anyone have any tips?  I\u2019m not sure I really understand what\u2019s failing in the spawn call\u2026  Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild error - Not a git repository",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-error-not-a-git-repository\/927",
        "Question_created_time":"2022-10-08T01:22:32.022Z",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":139,
        "Question_body":"<p>fatal: not a git repository (or any of the parent directories): .git<br>\nTraceback (most recent call last):<br>\nFile \u201c\/home\/sjoshi\/anaconda3\/envs\/clip\/.guild\/runs\/894a22b5465840858109bc4504a0295c\/.guild\/sourcecode\/simclr.py\u201d, line 46, in <br>\nargs.git_hash = subprocess.check_output([\u2018git\u2019, \u2018rev-parse\u2019, \u2018HEAD\u2019])<br>\nFile \u201c\/home\/sjoshi\/anaconda3\/envs\/clip\/lib\/python3.10\/subprocess.py\u201d, line 420, in check_output<br>\nreturn run(*popenargs, stdout=PIPE, timeout=timeout, check=True,<br>\nFile \u201c\/home\/sjoshi\/anaconda3\/envs\/clip\/lib\/python3.10\/subprocess.py\u201d, line 524, in run<br>\nraise CalledProcessError(retcode, process.args,<br>\nsubprocess.CalledProcessError: Command \u2018[\u2018git\u2019, \u2018rev-parse\u2019, \u2018HEAD\u2019]\u2019 returned non-zero exit status 128.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Opening source file causes File not found exception",
        "Question_link":"https:\/\/my.guild.ai\/t\/opening-source-file-causes-file-not-found-exception\/903",
        "Question_created_time":"2022-07-30T10:07:54.039Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":113,
        "Question_body":"<p>Hi, I just started using guild and I\u2019m trying to integrate it with an existing project.<br>\nI have the following directory structure:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">project\/\n\u251c\u2500 configs\/\n\u2502  \u251c\u2500 __init__.py\n\u2502  \u251c\u2500 cfg1.py\n\u2502  \u251c\u2500 cfg2.py\n\u251c\u2500 utils\/\n\u2502  \u251c\u2500 base_config.py\n<\/code><\/pre>\n<p>Where the config files contain something similar to<\/p>\n<pre><code class=\"lang-python\">from utils.base_config import BaseConfig\n\nclass Config(BaseConfig):\n    class Model:\n        architecture: MLP\n        class Parameters:\n             pass\n\nif __name__ == \"__main__\":\n    Config.init()\n    train.main()\n<\/code><\/pre>\n<p>and <code>BaseConfig.py<\/code> among other things does this<\/p>\n<pre><code class=\"lang-python\">import __main__\n\nclass BaseConfig:\n    @classmethod\n    def init(cls):\n        with Path('configs\/__init__.py').open('w+') as f:\n            f.writelines([f'from .{Path(str(__main__)).stem } import Config'])\n            f.flush()\n<\/code><\/pre>\n<p>Now, when I run <code>guild run configs\/cfg1.py<\/code> I get the following error:<\/p>\n<pre><code class=\"lang-python\">Traceback (most recent call last):\n  File \"C:\\Users\\username\\PycharmProjects\\project\\configs\\cfg1.py\", line 56, in &lt;module&gt;\n    Config.init()\n  File \"C:\\Users\\username\\PycharmProjects\\project\\utils\\base_config.py\", line 13, in init\n    with Path('configs\/__init__.py').open('w+') as f:\n  File \"C:\\Users\\username\\.conda\\envs\\project_env\\lib\\pathlib.py\", line 1252, in open\n    return io.open(self, mode, buffering, encoding, errors, newline,\n  File \"C:\\Users\\username\\.conda\\envs\\project_env\\lib\\pathlib.py\", line 1120, in _opener\n    return self._accessor.open(self, flags, mode)\nFileNotFoundError: [Errno 2] No such file or directory: 'configs\\\\__init__.py'\n<\/code><\/pre>\n<p>Then, when I look into the <code>sourcecode<\/code> directory of the run I see that only <code>__init__.py<\/code>, <code>cfg1.py<\/code> and <code>cfg2.py<\/code>are present and the directory structure is not preserved (i.e. the three files are not under <code>configs<\/code>).  Any idea on how to solve this?<\/p>\n<p>Also, I was wondering how I could log the configuration file. I tried playing with <code>flag-dest:config<\/code> but couldn\u2019t manage to make it work.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild view not working in jupyterhub",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-view-not-working-in-jupyterhub\/922",
        "Question_created_time":"2022-10-03T18:07:45.043Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":153,
        "Question_body":"<p>I am using GuildAI in a Jupyter Lab server provisioned in Jupyterhub. To view the UI, I run the command <code>guild view --host=localhost --port=5000<\/code> but am getting the error \u201cFailed to load resource: the server responded with a status of 404 (Not Found)\u201d. On the other hand, <code>guild tensorboard --host=localhost --port=5000<\/code> works fine for me. I\u2019m not sure what\u2019s the issue or how to debug it.<\/p>\n<p>Packages:<br>\nguildai==0.8.1<br>\njupyterhub==1.5.0<br>\njupyterlab==3.4.7<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dask scheduler cannot find python library",
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-cannot-find-python-library\/880",
        "Question_created_time":"2022-05-25T21:11:29.021Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":152,
        "Question_body":"<p>When I use <code>guild run dask:scheduler<\/code> with staged guild runs, those staged guild runs cannot find one of my Python libraries. The guild runs work if I run them outside of the dask scheduler.<\/p>\n<p>This particular Python library is not part of my Python virtual environment but is discovered by Python through my PYTHONPATH or PATH.<\/p>\n<p>The terminal I am running the dask scheduler in has the same environment variables.<\/p>\n<p>Any ideas?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild runs very slow",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-very-slow\/919",
        "Question_created_time":"2022-09-17T18:59:15.671Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":166,
        "Question_body":"<p>Similarly to what has been described in <a href=\"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362\">this thread<\/a>, running the following command results in hanging for a while before we see any results:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run train\n<\/code><\/pre>\n<p>The Guild file contains the following:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  main: script\n<\/code><\/pre>\n<p>The script is simply:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">print(\"Hello, world!\")\n<\/code><\/pre>\n<p>I believe the hanging is related to the fact that I have a data folder which contains thousands of files each corresponding to a sample of the dataset I\u2019m using in my experiments. This is because:<\/p>\n<ol>\n<li>When using the <code>--debug<\/code> flag, Guild hangs just after copying one <code>.csv<\/code> file within the data folder (none of the thousands of sample files are copied to the target directory, though).<\/li>\n<li>When I remove the data folder, the hanging stops.<\/li>\n<\/ol>\n<hr>\n<p>My questions are:<\/p>\n<ol>\n<li>Is this a bug? If so, how can I circumvent this issue?<\/li>\n<li>How am I supposed to make my dataset accessible by the training script, since it is run from a totally different directory and there are thousands of files it needs to access?<\/li>\n<\/ol>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Notebook copying to html error",
        "Question_link":"https:\/\/my.guild.ai\/t\/notebook-copying-to-html-error\/906",
        "Question_created_time":"2022-08-04T17:25:19.425Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":601,
        "Question_body":"<p>Hi,<\/p>\n<p>I did manage to get Guild to run my notebook, but I did get a couple warnings and 1 error:<\/p>\n<h2>\n<a name=\"h-2-warnings-1\" class=\"anchor\" href=\"#h-2-warnings-1\"><\/a>2 Warnings<\/h2>\n<p>1). \u201cWARNING: Skipping potential source code file .\/tcn_kerasAPI_gpu_data_generator.ipynb because it\u2019s too big. To control which files are copied, define \u2018sourcecode\u2019 for the operation in a Guild file.\u201d<\/p>\n<p>I\u2019m just wondering why I was even seeing that warning when my actual run command was:<\/p>\n<p>guild run tcn_kerasAPI_gpu_guild_ai_data_generator.ipynb<\/p>\n<p>So you see I wasn\u2019t even running the notebook that it says is \u201ctoo big\u201d.  That\u2019s a different notebook file entirely.  Is there some reason Guild would be \u201ccopying over\u201d a notebook file other than the one I\u2019m running?<\/p>\n<p>2). I\u2019m not sure why I got this warning:<\/p>\n<p>[NbConvertApp] WARNING | Config option <code>kernel_spec_manager_class<\/code> not recognized by <code>NbConvertApp<\/code>.<\/p>\n<p>===================<\/p>\n<h2>\n<a name=\"h-1-error-2\" class=\"anchor\" href=\"#h-1-error-2\"><\/a>1 Error<\/h2>\n<p>Tho\u2019 I did manage to get a successful run and saw expected output images (in Tensorboard) for my notebook, at the end of the run, when Guild was trying to convert the notebook to html via \u201cnbconvert\u201d, I did see this error:<\/p>\n<p>\u201cAttributeError: module \u2018jinja2\u2019 has no attribute \u2018Markup\u2019\u201d<\/p>\n<p>I guess it\u2019s not critical, but of course it\u2019s nicer for a run not to end on an error of course <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"> :).<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I tag\/label multiple runs?",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-do-i-tag-label-multiple-runs\/910",
        "Question_created_time":"2022-08-26T15:20:16.949Z",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":192,
        "Question_body":"<p>Hello,<\/p>\n<p>I\u2019m struggling to achieve the following:<br>\nIn TensorBoard I can only differenciate multiple runs by ID, as the comment is truncated, so the hyperparameters are mostly not shown.<br>\nFor example:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">42afef82 my_model:train 2022-08-23 02:02:04 architecture=DenseNet-121 batch_size=24 classes=14 comment='DenseNet\/experiments\/lightning_logs\/version_0\n<\/code><\/pre>\n<p>In this example, the hyper-parameter dropout is not visible.<br>\nNow, what I would love to do is to compare specific runs, e.g. all runs with <code>dropout=0.1<\/code>, in TensorBoard.<\/p>\n<p>Based on the docs AFAIK the only possibility is to filter the runs by tag and then run TensorBoard specifically for these runs, instead of using guild view \u2192 click on TensorBoard \u2192 filter inside TensorBoard.<\/p>\n<p>Therefore, my question is: how do I tag\/label multiple runs?<\/p>\n<p>I discovered, that I can display all <code>dropout=0.1<\/code> runs using:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild runs list --list-all -Fl=dropout=0.1\n<\/code><\/pre>\n<p>However, running <code>guild runs tag -Fl=dropout=0.1 -a dropout=0.1<\/code> tags only one run.<\/p>\n<p>Furthermore, I also want to tag runs where <code>dropout<\/code> wasn\u2019t a hyper-parameter but hardcoded. How do I select those? Is there something like a negative lookaround?<\/p>\n<p>Thanks!<\/p>\n<p>Cheers,<br>\nAlessandro<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Distributed training hanging",
        "Question_link":"https:\/\/my.guild.ai\/t\/distributed-training-hanging\/867",
        "Question_created_time":"2022-04-27T21:52:44.646Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":2230,
        "Question_body":"<p>Python: 3.9.12<br>\nPytorch: \u20181.11.0+cu102\u2019<br>\nPytorch-lightning: 1.6.1<br>\nguildai: 0.8.0<\/p>\n<p>I\u2019m trying to run distributed training (DDP) on my 1 machine with 4 GPUs. It works fine if I just run it normally with python. However, when I run with guild, it hangs.<\/p>\n<pre><code class=\"lang-python\">INFO: [pytorch_lightning.utilities.distributed] Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1\/4\nINFO: [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:1 to store for rank: 0\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\nINFO: [torch.distributed.distributed_c10d] Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\n...\nTraceback (most recent call last):\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/main.py\", line 223, in &lt;module&gt;\n    main()\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/main.py\", line 81, in main\n    results = get_imputation_logic(args)(args, data)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/task_logic\/ae_imputation.py\", line 101, in ae_imputation_logic\n    ae_imputer = create_autoencoder_with_tuning(args, data, settings)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/utils\/tuner.py\", line 43, in create_autoencoder_with_tuning\n    ae_imputer.fit(data)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 128, in fit\n    self._fit(data.longitudinal, \"longitudinal\")\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/.guild\/runs\/896d6a07fe044bb5a40b01c4e6a4064f\/.guild\/sourcecode\/autopopulus\/models\/ap.py\", line 139, in _fit\n    self.trainer[longitudinal_or_static].fit(ae, datamodule=data)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 768, in fit\n    self._call_and_handle_interrupt(\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 719, in _call_and_handle_interrupt\n    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/launchers\/subprocess_script.py\", line 93, in launch\n    return function(*args, **kwargs)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 809, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1169, in _run\n    self.strategy.setup_environment()\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/ddp.py\", line 151, in setup_environment\n    self.setup_distributed()\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/strategies\/ddp.py\", line 191, in setup_distributed\n    init_dist_connection(self.cluster_environment, self._process_group_backend)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 354, in init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/torch\/distributed\/distributed_c10d.py\", line 627, in init_process_group\n    _store_based_barrier(rank, store, timeout)\n  File \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.9\/site-packages\/torch\/distributed\/distributed_c10d.py\", line 255, in _store_based_barrier\n    raise RuntimeError(\nRuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\n<\/code><\/pre>\n<p>Some related errors:<\/p>\n<ul>\n<li><a href=\"https:\/\/discuss.pytorch.org\/t\/timed-out-initializing-process-group-in-store-based-barrier\/119803\/9\" rel=\"noopener nofollow ugc\">Timed out initializing process group in store based barrier<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/pytorch\/pytorch\/issues\/52848\" rel=\"noopener nofollow ugc\">Pytorch Issue<\/a><\/li>\n<\/ul>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pipeline depending on multiple of the same operation",
        "Question_link":"https:\/\/my.guild.ai\/t\/pipeline-depending-on-multiple-of-the-same-operation\/891",
        "Question_created_time":"2022-06-16T13:41:25.562Z",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":156,
        "Question_body":"<p>I have the following setup:<\/p>\n<pre><code class=\"lang-yaml\">- model: my_model\n  operations:\n    train: \n      main: scripts.train\n      flags:\n        a: 1\n    evaluate: \n      main: scripts.evaluate\n      requires:\n          - operation: train\n    train_evaluate:\n      flags:\n        a: 1\n      steps:\n      - run: train a=${a}\n      - run: evaluate\n    compare_evaluate:\n      main: scripts.compare\n      requires:\n        - train_evaluate_run_1\n        - train_evaluate_run_2\n    compare:\n      steps:\n      - run: train_evaluate a=1\n      - run: train_evaluate a=2\n      - run: compare_evaluate # HERE\nresources:\n  train_evaluate_run_1:\n    - operation: train_evaluate\n      name: train_evaluate_run_1\n  train_evaluate_run_2:\n    - operation: train_evaluate\n      name: train_evaluate_run_2\n<\/code><\/pre>\n<p>How to tell guild to resolve two different resources in the <code>compare_evaluate<\/code> run (with the # HERE tag)?<\/p>\n<p>The goal is to have a script\/notebook that takes as input two <code>train_evaluate<\/code> runs and compares them using custom plotting etc. and have all that specified in a single pipeline.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild does not catch scalar outputs when formatting values",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-does-not-catch-scalar-outputs-when-formatting-values\/890",
        "Question_created_time":"2022-06-16T12:01:21.913Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":193,
        "Question_body":"<p>Hi,<\/p>\n<p>Thank you very much for making this fantastic experiment manager and making it free!<\/p>\n<p>I noticed an unexpected behavior of Guild\u2019s <a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197#output-scalars-15\">output scalar<\/a>, so I report it here. Shortly, when the output strings are formatted with \u2018.e\u2019, they can not be caught by Guild. Below is a script to reproduce with Guild 0.8.1<\/p>\n<p>In scalar.py, I had<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">import argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--a\", type=float, default=1.0)\n    args = parser.parse_args()\n    print(f\"step: {100}\")\n    print(f\"value: {args.a:.2e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n<\/code><\/pre>\n<p>and in guild.yml, I had<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">scalar:\n  main: scalar\n  flags-dest: args\n  flags-import: all\n<\/code><\/pre>\n<p>Then in console, I got<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">(.venv) \u279c  gscalar guild run scalar\nYou are about to run scalar\n  a: 1.0\nContinue? (Y\/n)\nstep: 100\nvalue: 1.00e+00\n(.venv) \u279c  gscalar guild runs info\nid: 0b6abb0dc8bc4bb2914c01e504a5865e\noperation: scalar\nfrom: ~\/project\/gscalar\/guild.yml\nstatus: completed\nstarted: 2022-06-16 19:58:39\nstopped: 2022-06-16 19:58:39\nmarked: no\nlabel: a=1.0\nsourcecode_digest: 59509472e567014859244cbc4ce14ccd\nvcs_commit:\nrun_dir: ~\/project\/gscalar\/guild_home\/runs\/0b6abb0dc8bc4bb2914c01e504a5865e\ncommand: \/Users\/kyika\/project\/gscalar\/.venv\/bin\/python -um guild.op_main scalar -- --a 1.0\nexit_status: 0\npid:\ntags:\nflags:\n  a: 1.0\nscalars:\n<\/code><\/pre>\n<p>The last line above shown that Guild records no scalar. I guess this is because values was formatted with \u201ce+00\u201d, which was not recognized by Guild.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Specify guild home location on SSH remote",
        "Question_link":"https:\/\/my.guild.ai\/t\/specify-guild-home-location-on-ssh-remote\/866",
        "Question_created_time":"2022-04-27T18:17:42.913Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":147,
        "Question_body":"<p>I have a remote SSH with a OS disk and a data disk. I want to put my guild runs on the data disk, so on my ssh machine I have set my <code>GUILD_HOME=\/mnt\/guild_runs<\/code>.<\/p>\n<p>When I want to run from my local machine like <code>guild runs --remote ssh-machine<\/code> it cannot find the runs on the remote. When I run with additional debug information I see that the ssh command guild issues overwrites my <code>GUILD_HOME<\/code> env variable.<\/p>\n<p>Is there a way to tell guild where my <code>GUILD_HOME<\/code> is on my remote? I know I can do <code>venv-path<\/code>, but since my venv path and <code>GUILD_HOME<\/code> directory aren\u2019t the same, I don\u2019t think this solves it.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Process flag value in guild.yml for pipelines",
        "Question_link":"https:\/\/my.guild.ai\/t\/process-flag-value-in-guild-yml-for-pipelines\/860",
        "Question_created_time":"2022-04-19T16:55:38.552Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":146,
        "Question_body":"<p>Say I have a pipeline that takes as input a path as a flag:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">preprocess_bag:\n  flags:\n    bag_path: \"path\/to\/bag.bag\"\n  steps:\n    - decode_bag bag_path=${bag_path}\n    - dump_bag bag_bag=${bag_path_but_with_some_processed} # i.e. strip entire path down to only bag.bag\n<\/code><\/pre>\n<p>What I am essentially looking for is something like <a href=\"https:\/\/www.gnu.org\/software\/make\/manual\/html_node\/File-Name-Functions.html\" rel=\"noopener nofollow ugc\">Make File Name Functions<\/a> that I can use to manipulate the flags.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Get run ID before\/during the corresponding run",
        "Question_link":"https:\/\/my.guild.ai\/t\/get-run-id-before-during-the-corresponding-run\/844",
        "Question_created_time":"2022-03-24T16:54:50.873Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":195,
        "Question_body":"<p>Hey, my issue is the following:<br>\nDuring my run I am training several models and save their checkpoint. In the end I need to load them all for the final testing procedure. However, since the models are saved in  <code>.guild\/runs\/ID\/my_saved_model.ckpt<\/code> and I don\u2019t know the current <code>ID<\/code>, there is no way to re-load the models into my training script. Is there a way to pass the current\/future <code>ID<\/code> into my python script?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Remotes config doesn't work for Gist",
        "Question_link":"https:\/\/my.guild.ai\/t\/remotes-config-doesnt-work-for-gist\/840",
        "Question_created_time":"2022-03-22T06:16:37.879Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":197,
        "Question_body":"<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  description: train\n  main: tests\/train\n  sourcecode:\n    - exclude: '*.json'\n\nremotes:\n  remotename:\n    type: gist\n    user: username\n    gist-name: results.md\n<\/code><\/pre>\n<p>This remote doesn\u2019t show up when I run <code>guild remotes<\/code><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Issue with defining source code in guild.yml",
        "Question_link":"https:\/\/my.guild.ai\/t\/issue-with-defining-source-code-in-guild-yml\/833",
        "Question_created_time":"2022-03-15T01:09:44.584Z",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":162,
        "Question_body":"<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">sort_sv_by_score:\n  description: Runs validation of sorting singular vectors by score\n  main: tests\/sort_sv_by_score\n  sourcecode:\n    -exclude: '*.json'\n<\/code><\/pre>\n<p>This is my guild.yml file. I have a bunch of .json files in my directory (my results logs from previous experiments). When I run this operation though, despite specifying exclude, it attempts to copy over all the .json files.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"TensorBoard HPARAM not working",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-hparam-not-working\/835",
        "Question_created_time":"2022-03-17T17:48:26.680Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":515,
        "Question_body":"<p>I am trying to use the HPARAM tab in tensorboard together with guild<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild --debug tensorboard 1\n<\/code><\/pre>\n<p>When I navigate to the HPARAM tab this is what I get:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b.png\" data-download-href=\"\/uploads\/short-url\/hTCwjWGkWGpnber9a2bv5j8RbaX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_690x356.png\" alt=\"image\" data-base62-sha1=\"hTCwjWGkWGpnber9a2bv5j8RbaX\" width=\"690\" height=\"356\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_690x356.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_1035x534.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_1380x712.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/7d6e8b08fb55eac7111f7519e67f7947be7fc80b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2505\u00d71296 153 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The runs are not actually showing up. Any idea why this is happening? There are no errors in the debug output from guild.<\/p>\n<p><strong>Versions<\/strong><br>\nguildai==0.8.0rc1<br>\ntensorboard==2.8.0<br>\ntensorboard-data-server==0.6.1<br>\ntensorboard-plugin-wit==1.8.1<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best way to specify boolean argparse flags",
        "Question_link":"https:\/\/my.guild.ai\/t\/best-way-to-specify-boolean-argparse-flags\/834",
        "Question_created_time":"2022-03-17T16:26:00.656Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":264,
        "Question_body":"<p>What is the best way to combine argparse and guild with a boolean flag?<\/p>\n<p>I currently do something like this:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># In python argparse\ndata.add_argument('--enable_augmentation', dest='enable_augmentation', action='store_true',\n                    help=\"Whether or not apply augmentation on training dataset\")\n\n<\/code><\/pre>\n<p>And<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># guild.yml\n  enable_augmentation:\n    default: no\n    arg-switch: yes\n    type: boolean\n<\/code><\/pre>\n<p>Is this the preferred way?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Nb-replace with boolean value",
        "Question_link":"https:\/\/my.guild.ai\/t\/nb-replace-with-boolean-value\/830",
        "Question_created_time":"2022-03-10T19:33:48.346Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":186,
        "Question_body":"<p>I want to use guild to run a notebook as an experiment. I also want to parameterize the notebook by using guilds search and replace option with notebooks.<\/p>\n<p>One of my parameters is a boolean value. How should I define the <code>nb-replace<\/code> to work with booleans?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Local run vs remote run dependencies",
        "Question_link":"https:\/\/my.guild.ai\/t\/local-run-vs-remote-run-dependencies\/812",
        "Question_created_time":"2022-02-22T17:00:17.715Z",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":247,
        "Question_body":"<p>I have run some operations on remotes successfully in the past. However, there was always a some discrepancy between the imports for local and remote runs that I needed to fix by trial and error.<\/p>\n<p>In my current setup, I switched from flags as global variables in the training script to flags in config.yml files. And I\u2019m unable to make it work on remotes.<\/p>\n<p><strong>Project structure<\/strong><br>\nProject:<\/p>\n<ul>\n<li>[some folders]<\/li>\n<li>datasets \u2192 module, contains data loaders + their config.yml files<\/li>\n<li>zoo \u2192 guild Home for local runs<\/li>\n<li>models \u2192 model definitions<\/li>\n<li>\n<ul>\n<li>guild.yml<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>abstract_model.py<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>conv_lstm \u2192 model I want to run<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>model.py \u2192 model definition<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>train.py \u2192 training script<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>config.yml \u2192 flags<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><strong>Guild file<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># Standard convolutional LSTM\n- model: conv_lstm\n  description: Convolutional LSTM\n  operations:\n    train_local:\n      description: Train Convolutional LSTM\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        epochs: 100\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n    train_remote:\n      description: Train Convolutional LSTM on remote\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        optimizer: Adam\n        loss: mse\n        learning_rate: 0.001\n        epochs: 100\n        dev: True\n        gpus: [7]\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n            train_path: ~\/data\/ucsd\/UCSDped1\/Train\/\n            test_path: ~\/data\/ucsd\/UCSDped1\/Test\/\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n<\/code><\/pre>\n<p><strong>Training script<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">sys.path.append('..\/')\nsys.path.append('..\/..\/datasets')\n# Tensorflow logging level\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport tensorflow as tf\nimport yaml\nfrom model import ConvLSTM\nfrom datasets.data_loader import DataLoader\n\n\n# Load the model configuration\nclass Config(object):\n    def __init__(self, filename):\n        self.__dict__.update(yaml.safe_load(open(filename)))\n\n\nconfig = Config(\"config.yml\")\n\n(...)\n<\/code><\/pre>\n<p><strong>Current situation &amp; error<\/strong><br>\nI\u2019m able to run \u2018conv_lstm:train_local\u2019 without any issues, and everything works as expected. However, almost the same configuration, with a few flags changed, fails to run on remote.<\/p>\n<p>Issue 1: I cannot see any evidence of the config.yml file being copied to the remote<br>\nIssue 2: the remote run fails to find the main training script, even though it works locally.<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild -H \/home\/bleporowski\/Projects\/mad\/zoo run conv_lstm:train_remote --remote [remote_name] --gpus 7\nYou are about to run conv_lstm:train_remote as a batch (1 trial) on [remote_name]\n  dataset_args: [{batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}]\n  dev: yes\n  epochs: 100\n  gpus: [7]\n  learning_rate: 0.001\n  loss: mse\n  optimizer: Adam\nContinue? (Y\/n) y\nBuilding package\npackage src: \/home\/bleporowski\/Projects\/mad\/models\npackage dist: \/tmp\/guild-remote-stage-eq7ahi7e\nrunning clean\nremoving 'build\/lib' (and everything under it)\nremoving 'build\/bdist.linux-x86_64' (and everything under it)\n'build\/scripts-3.8' does not exist -- can't clean it\nremoving 'build'\nrunning bdist_wheel\nrunning build\nrunning build_py\npackage init file '\/home\/bleporowski\/Projects\/mad\/models\/__init__.py' not found (or not a regular file)\ncreating build\ncreating build\/lib\ncreating build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/abstract_model.py -&gt; build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/guild.yml -&gt; build\/lib\/conv_lstm\ninstalling to build\/bdist.linux-x86_64\/wheel\nrunning install\nrunning install_lib\ncreating build\/bdist.linux-x86_64\ncreating build\/bdist.linux-x86_64\/wheel\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/guild.yml -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/abstract_model.py -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\nrunning install_egg_info\nrunning egg_info\nwriting conv_lstm.egg-info\/PKG-INFO\nwriting dependency_links to conv_lstm.egg-info\/dependency_links.txt\nwriting entry points to conv_lstm.egg-info\/entry_points.txt\nwriting namespace_packages to conv_lstm.egg-info\/namespace_packages.txt\nwriting top-level names to conv_lstm.egg-info\/top_level.txt\nreading manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nwriting manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nCopying conv_lstm.egg-info to build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0-py3.8.egg-info\nrunning install_scripts\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0.dist-info\/WHEEL\ncreating '\/tmp\/guild-remote-stage-eq7ahi7e\/conv_lstm-0.0.0-py2.py3-none-any.whl' and adding 'build\/bdist.linux-x86_64\/wheel' to it\nadding 'conv_lstm\/abstract_model.py'\nadding 'conv_lstm\/guild.yml'\nadding 'conv_lstm-0.0.0.dist-info\/METADATA'\nadding 'conv_lstm-0.0.0.dist-info\/PACKAGE'\nadding 'conv_lstm-0.0.0.dist-info\/WHEEL'\nadding 'conv_lstm-0.0.0.dist-info\/entry_points.txt'\nadding 'conv_lstm-0.0.0.dist-info\/namespace_packages.txt'\nadding 'conv_lstm-0.0.0.dist-info\/top_level.txt'\nadding 'conv_lstm-0.0.0.dist-info\/RECORD'\nremoving build\/bdist.linux-x86_64\/wheel\nInitializing remote run\nCopying package\nsending incremental file list\nconv_lstm-0.0.0-py2.py3-none-any.whl\n\nsent 3,558 bytes  received 35 bytes  1,437.20 bytes\/sec\ntotal size is 3,424  speedup is 0.95\nInstalling package and its dependencies\nProcessing .\/conv_lstm-0.0.0-py2.py3-none-any.whl\nInstalling collected packages: conv-lstm\nSuccessfully installed conv-lstm-0.0.0\nStarting conv_lstm:train_remote on charybdis as 8a26ca399039412fb31c7791d293b507\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nINFO: [guild] Running trial 05afef0858f74b4198af160c6d904e2e: conv-lstm\/conv_lstm:train_remote (dataset_args={batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}, dev=yes, epochs=100, gpus=7, learning_rate=0.001, loss=mse, optimizer=Adam)\nINFO: [guild] Resolving config:conv_lstm\/config.yml dependency\nERROR: [guild] Trial 05afef0858f74b4198af160c6d904e2e exited with an error: (1) run failed because a dependency was not met: could not resolve 'config:conv_lstm\/config.yml' in config:conv_lstm\/config.yml resource: cannot find source file 'conv_lstm\/config.yml'\nRun 8a26ca399039412fb31c7791d293b507 stopped with a status of 'completed'\n\n<\/code><\/pre>\n<p>Do remote runs require everything to become a module, with \u2018<strong>init<\/strong>.py\u2019? Or should the guild file be in a different location?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dask Scheduler not utilizing all available resources",
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-not-utilizing-all-available-resources\/822",
        "Question_created_time":"2022-03-03T18:25:49.841Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":233,
        "Question_body":"<p>Hey all,<\/p>\n<p>I\u2019ve been trying to get the dask scheduler to work with my guild runs. Let\u2019s say I have 2 GPUs and I\u2019d like to put at max, 2 runs on each GPU.<\/p>\n<p>According to the guides (<a href=\"https:\/\/my.guild.ai\/t\/parallel-processing-with-dask-scheduler\/550\" class=\"inline-onebox\">Parallel processing with Dask scheduler<\/a>) I\u2019ve implemented two sets of commands. The first set is two commands to stage the set of runs across the 2 gpus. These commands look something like this:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run model:train   param1_to_sweep=[10, 20, 30]\n                        param2_to_sweep=[1,2,3,4,5]\n                        --label my_hp_runs\n                        --optimizer random\n                        --trials 5\n                        --tag dask:GPU0=1\n                        --stage-trials\n                        --gpus 0\n\nguild run model:train   param1_to_sweep=[10, 20, 30]\n                        param2_to_sweep=[1,2,3,4,5]\n                        --label my_hp_runs\n                        --optimizer random\n                        --trials 5\n                        --tag dask:GPU1=1\n                        --stage-trials\n                        --gpus 1\n<\/code><\/pre>\n<p>After the runs are staged, I spin up the scheduler with the associated resources that would allow 2 runs per GPU.<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run dask:scheduler run-once=yes\n                         workers=10\n                         resources='GPU0=2 GPU1=2'\n                         dashboard-address=8890\n<\/code><\/pre>\n<p>I\u2019m able to open up the dashboard and can see GPU0 and GPU1 resources avalible. The trouble happens when I see dask only puts 2 runs of GPU0 and none on GPU1. To be more clear, about the schedule of runs that actually happen I\u2019ll label each of the staged runs below:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">run id  0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n----------------------------------------------\nGPU     0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1\n<\/code><\/pre>\n<p>Assuming all the runs take the same amount of time the runs get ran in the following \u2018sets\u2019<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">Set  | run ids\n---------------\n0    | 0,1 - The initial two runs get put on GPU0\n1    | 2,3 - GPU0 runs 0 and 1 ended, so 2 more get put on\n2    | 4, 5, 6 - GPU0 runs 2,3 end and there is only one GPU0 run left that gets executed (4). Then 2 runs go on GPU2\n3    | 7, 8 - Similar behavior to set 1\n4    | 9   - The remaining runs in the GPU2 queue. \n<\/code><\/pre>\n<p>The expected behavior would be the following sets<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">Set  | run ids\n---------------\n0    | 0,1,5,6 - 2 runs on each gpu\n1    | 2,3,7,8 - 2 runs on each gpu\n2    | 4,9 - 1 run on each gpu, queue finishes\n<\/code><\/pre>\n<p>Does anyone know why this could be happening or have I just implemented the dask scheduler wrong?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking source code that is a python package",
        "Question_link":"https:\/\/my.guild.ai\/t\/tracking-source-code-that-is-a-python-package\/816",
        "Question_created_time":"2022-02-25T09:02:38.913Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":184,
        "Question_body":"<p>I\u2019m very sorry if this is already documented.<br>\nthe scripts I run are all part of a python package I\u2019m working on, the packages are installed using conda with.<\/p>\n<p>pip install -e<\/p>\n<p>which makes a symbolic link from conda to my package\u2019s directory.<\/p>\n<p>When I run the experiment with guild it stores the source code of all the files in the package\u2013which is good. However, when I try to restart the run, the program starts by using the main function in my<br>\nguild sourcecode folder\u2013which is what I want-- but the moment it tries to use a module imported from its parent package it starts using code in the packages main directory instead of the version of that file saved by Guild. How could i get guild to use the files saved by guild?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Remote connection error with jump host",
        "Question_link":"https:\/\/my.guild.ai\/t\/remote-connection-error-with-jump-host\/807",
        "Question_created_time":"2022-02-07T21:10:31.237Z",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":394,
        "Question_body":"<p>Hi,<br>\nmy institution has recently changed the configuration of our remote workstations.<\/p>\n<p>Now the connection goes through a jump host, and we cannot use a ssh pair here. I have a proxy configured, so manually I connect to the workstation with \u2018ssh [workstation]\u2019. The jump host requires a password on every connection, followed by an app authentication. The workstation has a ssh pairing with my local machine, so I only have to login to the jump host. That\u2019s the policy, and cannot be changed.<\/p>\n<p>I have successfully manged to run a guild check on that remote. I have configured a training script, config files etc. so that it all runs smoothly locally.<\/p>\n<p>However, when I try to run the train operation on the remote, I get the following errors:<\/p>\n<pre><code>Initializing remote run\nPassword: \nCopying package\nPassword: \nConnection timed out during banner exchange\nrsync: connection unexpectedly closed (0 bytes received so far) [sender]\nrsync error: unexplained error (code 255) at io.c(235) [sender=3.1.3]\nTraceback (most recent call last):\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/bin\/guild\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/main_bootstrap.py\", line 40, in main\n    _main()\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1659, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\n    return __callback(*args, **kwargs)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run.py\", line 649, in run\n    run_impl.main(args)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1514, in main\n    _dispatch_op(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1610, in _dispatch_op\n    _dispatch_op_cmd(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1797, in _dispatch_op_cmd\n    _confirm_and_run(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 1874, in _confirm_and_run\n    _run(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 2075, in _run\n    _run_remote(S)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/run_impl.py\", line 2082, in _run_remote\n    remote_impl_support.run(_remote_args(S))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/commands\/remote_impl_support.py\", line 125, in run\n    run_id = remote.run_op(**_run_kw(args))\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh.py\", line 243, in run_op\n    remote_run_dir = self._init_remote_run(tmp.path, opspec, restart)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh.py\", line 265, in _init_remote_run\n    self._copy_package_dist(package_dist_dir, remote_run_dir)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh.py\", line 330, in _copy_package_dist\n    ssh_util.rsync_copy_to(\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/site-packages\/guild\/remotes\/ssh_util.py\", line 129, in rsync_copy_to\n    subprocess.check_call(cmd)\n  File \"\/home\/bleporowski\/anaconda3\/envs\/marvel\/lib\/python3.8\/subprocess.py\", line 364, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['rsync', '-vr', '-e', \"ssh -oConnectTimeout=10 -o 'ProxyCommand ssh -oConnectTimeout=100  -W %h:%p [user]@[jumphost]'\", '\/tmp\/guild-remote-stage-ahx9az7p\/', '[user]@[workstation]:~\/anaconda3\/envs\/time-gop\/.guild\/runs\/5d5d24d410c648f897630ef102538a1e\/.guild\/job-packages\/']' returned non-zero exit status 255.\n<\/code><\/pre>\n<p>I\u2019m curious about two things:<\/p>\n<ul>\n<li>Why do I have to login twice, once after \u2018Initializing remote run\u2019 log, and then again after \u2018Copying package\u2019 log?<\/li>\n<li>I have set up my remotes in the guild\/config.yml to have a timeout of 100 seconds for both the jump host and the second step connection. However, from the trace it seems that the guild\/config.yml timeout is not properly read?<\/li>\n<\/ul>\n<p>This is the guild\/config.yml:<\/p>\n<pre><code>remotes:\n [remote-name]:\n  type: ssh\n  host: [workstation]\n  proxy: ssh -oConnectTimeout=100 -W %h:%p [user]@[jump host]\n  connect-time: 100\n  user: [user]\n  conda-env: ~\/anaconda3\/envs\/time-gop  \n  init: source ~\/anaconda3\/etc\/profile.d\/conda.sh | guild -H ~\/projects\/protime-gop\n<\/code><\/pre>\n<p>the obvious reason would be that the connection times out, as per the error log. However, the config timeout value doesn\u2019t seem to actually change the value invoked with the remote command.<\/p>\n<p>Have I made a mistake while creating my guild\/config.yml? Or it is a bug? Or maybe some other reason?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"WinError 1314 when trying to do a grid search",
        "Question_link":"https:\/\/my.guild.ai\/t\/winerror-1314-when-trying-to-do-a-grid-search\/800",
        "Question_created_time":"2022-01-21T09:31:03.278Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":868,
        "Question_body":"<p>Hi,<br>\nI\u2019m having trouble when trying to do a search with guild on a Windows 10 machine.<\/p>\n<p>I took the example code fromt the tutorial<\/p>\n<pre><code>import numpy as np\n\n# Hyperparameters\nx = 0.1\nnoise = 0.1\n\n# Simulated training loss\nloss = (np.sin(5 * x) * (1 - np.tanh(x ** 2)) + np.random.randn() * noise)\n\nprint(\"loss: %f\" % loss)\n<\/code><\/pre>\n<p>I\u2019m running it from console with<\/p>\n<pre><code>guild run test.py x=\"[1,0,-1]\"\n<\/code><\/pre>\n<p>When executing i get an error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"c:\\users\\x\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\users\\x\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_main.py\", line 41, in &lt;module&gt;\n    main()\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_main.py\", line 27, in main\n    batch_util.handle_trials(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 73, in handle_trials\n    _run_trials(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 135, in _run_trials\n    trial_runs = _init_trial_runs(batch_run, trials)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 144, in _init_trial_runs\n    return [init_trial_run(batch_run, trial) for trial in trials]\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 144, in &lt;listcomp&gt;\n    return [init_trial_run(batch_run, trial) for trial in trials]\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 149, in init_trial_run\n    _link_to_trial(batch_run, run)\n  File \"C:\\Users\\x\\AppData\\Roaming\\Python\\Python38\\site-packages\\guild\\batch_util.py\", line 168, in _link_to_trial\n    os.symlink(rel_trial_path, trial_link)\nOSError: [WinError 1314] A required privilege is not held by the client.\n\n '..\\\\749cba372c474a1598aac11e2d4fb902' -&gt; 'C:\\\\Users\\\\x\\\\Anaconda3\\\\.guild\\\\runs\\\\045f73f210fb463f814f3334fd30825d\\\\749cba372c474a1598aac11e2d4fb902'\n<\/code><\/pre>\n<p>I don\u2019t have administrator permissions on the machine and I\u2019m probably not going to get them.<\/p>\n<p>Installing it via pip with --User flag doesnt fix it, either.<\/p>\n<p>Can I fix this somehow without having to run as admin?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Date\/Datetime flags in YAML file are not recognized",
        "Question_link":"https:\/\/my.guild.ai\/t\/date-datetime-flags-in-yaml-file-are-not-recognized\/798",
        "Question_created_time":"2022-01-19T18:03:49.696Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":161,
        "Question_body":"<p>I have set up a YAML file with the flags for my single operation. When I run the guild run command, all flags get imported except for date and datetime flags. Will I have to write them as strings and parse them into dates myself on my main script?<\/p>\n<p>Contents of my YAML file:<\/p>\n<pre><code>N_LEADS: 30\nWINDOW_RANGE: [-395, -1]\nLAST_EOM_TRAIN: 2019-12-31\nLEAD_RANGE: [-60, -50]\nN_ESTIMATORS: 2\nEOM_PRED_BENCH: 2021-07-31 00:00:00\nAS_OF_PRED_BENCH: 2021-06-01 00:00:00\n<\/code><\/pre>\n<p>What shows up when running the guild run command:<\/p>\n<pre><code>You are about to run main\n  LEAD_RANGE: -60 -50\n  N_ESTIMATORS: 2\n  N_LEADS: 30\n  WINDOW_RANGE: -395 -1\n<\/code><\/pre>\n<p>Thank you for your help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Stage trials error",
        "Question_link":"https:\/\/my.guild.ai\/t\/stage-trials-error\/741",
        "Question_created_time":"2021-07-30T01:38:01.354Z",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":329,
        "Question_body":"<p>I\u2019ve tested the <code>--stage-trials<\/code> feature with <code>queues<\/code> today, all worked great. But now I call <code>guild run op--staged-trials<\/code>, and check with <code>guild runs<\/code>, some runs are staged, others show errors<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c.png\" data-download-href=\"\/uploads\/short-url\/kxgoN75AKZNvWS4wd90nbgU9u0I.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c.png\" alt=\"image\" data-base62-sha1=\"kxgoN75AKZNvWS4wd90nbgU9u0I\" width=\"690\" height=\"175\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/8fee18f78b819bce49fcc791997addfc1beed81c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">753\u00d7192 5.41 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>A minute later I try <code>guild runs<\/code> again, and all runs show as <code>error<\/code> without me having done anything. What could cause the staged trials to flip to errors all of a sudden? This always happens now.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241.png\" data-download-href=\"\/uploads\/short-url\/8VBYFfkCYF3cwNgAMfGh4A2J561.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241.png\" alt=\"image\" data-base62-sha1=\"8VBYFfkCYF3cwNgAMfGh4A2J561\" width=\"690\" height=\"176\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/3e9499c087f43e99cf0b7df8049e6a1fdd03a241_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">757\u00d7194 5.21 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Running jobs show as errors on cluster",
        "Question_link":"https:\/\/my.guild.ai\/t\/running-jobs-show-as-errors-on-cluster\/746",
        "Question_created_time":"2021-08-06T19:33:26.504Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":300,
        "Question_body":"<p>Not sure if this is a bug or a feature request, but here is the issue: I stage a bunch of jobs on computer 1, part of a cluster with a shared file system. When I call <code>guild runs<\/code> on any computer in the cluster, it will properly show all staged files. If I start a queue on computer 1 it will promptly launch the first staged job:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd.png\" data-download-href=\"\/uploads\/short-url\/anGnutFiNKGSAB3mEKYpbp9fAgR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd.png\" alt=\"image\" data-base62-sha1=\"anGnutFiNKGSAB3mEKYpbp9fAgR\" width=\"690\" height=\"15\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1095\u00d725 1.94 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>If I then log into computer 2, the running job will show as error:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f.png\" data-download-href=\"\/uploads\/short-url\/laL2L6avIgccXVOeb1x78OHbgIf.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f.png\" alt=\"image\" data-base62-sha1=\"laL2L6avIgccXVOeb1x78OHbgIf\" width=\"690\" height=\"25\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1158\u00d743 3.29 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Why would it show as error instead of just \u201crunning\u201d?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to combine flags from multiple operations in pipelines to enable parameter searches across multiple operations?",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-combine-flags-from-multiple-operations-in-pipelines-to-enable-parameter-searches-across-multiple-operations\/328",
        "Question_created_time":"2020-08-27T14:14:25.027Z",
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":857,
        "Question_body":"<p>Let say I have 2 operations preprocessing and training.<br>\nEach has a series of flags that I want to do a hyper parameter search over.<br>\nHere is a rough idea of what my operations would look like<\/p>\n<pre><code>preprocessing:\n  flags:\n    a:\n    b:\ntraining:\n  flags:\n    c:\n    d:\n<\/code><\/pre>\n<p>Neglecting the practicality of this scenario, I want to run my preprocessing operation every time I run train.  Is there a way to achieve this with pipelines or a similar method?  I know I could do<\/p>\n<pre><code>mypipeline:\n  flags:\n    a: [1,2,3]\n    b: [4,5,6]\n    c: [12,13,14]\n    d: [15,16,17]\n  steps:\n    - preprocessing a=${a} b=${b}\n    - train c=${c} d=${d}\n<\/code><\/pre>\n<p>and this would solve my problem, but when both operations have a lot of parameters it would end up requiring a lot of copying and pasting between the pipeline and operation flags.  Is there a better\/more efficient way to achieve this?  I basically want the pipline to \u201cinherit\u201d the flags of the steps its performing if possible.  The documentation seems to hint at something like this<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"197\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197\/1\">Guild File Reference<\/a>\n<\/div>\n<blockquote>\n<p>You can include references to step flag values as needed to pass through user-specified values.<\/p>\n<\/blockquote>\n<\/aside>\n<p>But when I create a pipeline with no flags and attempt to pass a flag which is defined by one of the steps I get an error saying that the flag does not exists.  Is what I am trying to do possible with Guild?<\/p>\n<p>Thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"TensorBoard Hparm Parallel Coordinate View Problem",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-hparm-parallel-coordinate-view-problem\/753",
        "Question_created_time":"2021-08-20T20:37:28.267Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":320,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m having an issue with the TensorBoard Hparm Parallel Coordinate View. It seems to be duplicated the axes on the lower half.<\/p>\n<p>This happens when I make a new project and just follow the get-started.ipynb file.<\/p>\n<p>I\u2019m using a Mac and Safari browser, and here are the versions info:<\/p>\n<pre><code>guild_version:             0.7.3\ntensorboard_version:       2.6.0\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb.jpeg\" data-download-href=\"\/uploads\/short-url\/oDDDcjhGMAurXRar09EHL2ltw3p.jpeg?dl=1\" title=\"Screen Shot 2021-08-20 at 4.24.55 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg\" alt=\"Screen Shot 2021-08-20 at 4.24.55 PM\" data-base62-sha1=\"oDDDcjhGMAurXRar09EHL2ltw3p\" width=\"517\" height=\"336\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_775x504.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_1034x672.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2021-08-20 at 4.24.55 PM<\/span><span class=\"informations\">1920\u00d71249 181 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Additionally, when I remove one of the variables, the entire plot grows at least 30% in size. Is there a way I can keep this constant.<\/p>\n<p>Lastly, runs in a notebook don\u2019t seem to save an output.index file and guild view throws an error from this.<\/p>\n<p>Thanks for the help! Let me know if I need to share anything else.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to specify flags-import: all with remote runs",
        "Question_link":"https:\/\/my.guild.ai\/t\/unable-to-specify-flags-import-all-with-remote-runs\/749",
        "Question_created_time":"2021-08-10T17:48:13.656Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":415,
        "Question_body":"<p>I have an operation in that looks like this<\/p>\n<pre><code>  operations:\n    prepare:\n      main: operations.prepare\n      flags-import: all\n      output-scalars: off\n<\/code><\/pre>\n<p>which has a number of different flags specified with argparse in <code>operations\/prepare.py<\/code>. I can run it just fine locally, but when I try run it remote with any flag set I get <code>guild: unsupported flag &lt;flag&gt;<\/code> for every flag. I can run it remote as long as I don\u2019t set flags. Also, if I specify the flags explicitly in <code>guild.yml<\/code> like this it works fine:<\/p>\n<pre><code>    prepare:\n      main: operations.prepare\n      flags:\n        &lt;flag1&gt;:\n        &lt;flag2&gt;:\n      output-scalars: off\n<\/code><\/pre>\n<p>Any idea what might be wrong here?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tensorboard FileNot found error on Windows-10, guild-0.7.3.dev1",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-filenot-found-error-on-windows-10-guild-0-7-3-dev1\/569",
        "Question_created_time":"2021-03-25T00:34:27.283Z",
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":491,
        "Question_body":"<pre><code>C:\\Users\\sarat.chinni\\Codes_sequencing\\biobench\\sandbox\\Sarat\\incorp_basecalling (feature\/incorp_basecalling -&gt; origin)\n(biobench-thMxqAli) \u03bb guild tensorboard\nPreparing runs for TensorBoard\nERROR: error removing C:\\Users\\SARAT~1.CHI\\AppData\\Local\\Temp\\guild-tensorboard-88wle55x: [WinError 3] The system cannot find the path specified: \"C:\\\\Users\\\\SARAT~1.CHI\\\\AppData\\\\Local\\\\Temp\\\\guild-tensorboard-88wle55x\\\\d83224a9 dnn_classifier_train 2021-03-24 17_18_48 batch_size=32 dense_activation=relu dense_units='128 128' drop_out=0.0 fileName=black_and_white_1sec.pickle l2_decay=0.001 num_epochs=100 seed=42 verify_saved=no\"\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\.virtualenvs\\biobench-thMxqAli\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 94, in _run_tensorboard\n    monitor.run_once(exit_on_error=True)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\run_util.py\", line 89, in run_once\n    self._refresh_logdir(runs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\run_util.py\", line 97, in _refresh_logdir\n    self.refresh_run_cb(run, path)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 275, in f\n    return _refresh_run(run, run_logdir, state)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 281, in _refresh_run\n    _refresh_tfevent_links(run, run_logdir, state)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 292, in _refresh_tfevent_links\n    _init_tfevent_link(tfevent_path, link, run, state)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 304, in _init_tfevent_link\n    util.ensure_dir(link_dir)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\util.py\", line 74, in ensure_dir\n    os.makedirs(d)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\os.py\", line 213, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2288.0_x64__qbz5n2kfra8p0\\lib\\os.py\", line 223, in makedirs\n    mkdir(name, mode)\nFileNotFoundError: [WinError 3] The system cannot find the path specified: \"C:\\\\Users\\\\sarat.chinni\\\\AppData\\\\Local\\\\Temp\\\\guild-tensorboard-88wle55x\\\\d83224a9 dnn_classifier_train 2021-03-24 17_18_48 batch_size=32 dense_activation=relu dense_units='128 128' drop_out=0.0 fileName=black_and_white_1sec.pickle l2_decay=0.001 num_epochs=100 seed=42 verify_saved=no\"\n\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Staged pipeline steps not given labels",
        "Question_link":"https:\/\/my.guild.ai\/t\/staged-pipeline-steps-not-given-labels\/791",
        "Question_created_time":"2022-01-03T22:36:43.629Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":254,
        "Question_body":"<p>I\u2019ve noticed that guild applies that labels I give to pipelines to each pipeline step when the pipeline stage is ran directly.  However, if I instead stage the pipeline operation and then run the staged pipeline operation this doesn\u2019t happen.  Instead only the pipeline operation receives label, not the steps of the pipeline.   For example, lets say my <code>guild.yml<\/code> is given as follows.  The contents of <code>train.py<\/code> and <code>test.py<\/code> aren\u2019t really important.<\/p>\n<pre><code>- operations:\n    mypipeline:\n        steps:\n            - train\n            - test\n\n    train:\n        sourcecode:\n            dest: .\n            select: train.py\n        exec: \"python train.py\"\n\n    test:\n        sourcecode:\n            dest: .\n            select: test.py\n        exec: \"python test.py\"\n<\/code><\/pre>\n<p>If I run <code>guild run mypipeline -y --label debug<\/code> I get<\/p>\n<pre><code>[1:b21131b7]   test                                      2022-01-03 17:27:55  completed  debug\n[2:8dd06463]   train                                     2022-01-03 17:27:55  completed  debug\n[3:092f43f2]   mypipeline                                2022-01-03 17:27:54  completed  debug\n<\/code><\/pre>\n<p>But if I run <code>guild run mypipeline -y --label debug --stage &amp;&amp; guild run queue -y<\/code> I get the following.  Note that only the <code>mypipeline<\/code> operation receives the label <code>debug<\/code> while the step operations receive no label.<\/p>\n<pre><code>[1:5b2c8b79]   test                                      2022-01-03 17:28:48  completed\n[2:85ae374c]   train                                     2022-01-03 17:28:47  completed\n[3:a0dc7ef1]   mypipeline                                2022-01-03 17:28:46  completed   debug\n<\/code><\/pre>\n<p>Note that similar commands like <code>guild run mypipeline -y --label debug --stage &amp;&amp; guild run --start $(guild select 1)<\/code> have the same effect.<\/p>\n<p>Is this intended behavior? If so how can I make it so that the steps of the staged pipeline operation receive the same label as the pipeline operation that started them?  Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Flag not recognized: No module named --batch_size",
        "Question_link":"https:\/\/my.guild.ai\/t\/flag-not-recognized-no-module-named-batch-size\/770",
        "Question_created_time":"2021-10-05T16:27:55.980Z",
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":817,
        "Question_body":"<p>Hi there,<br>\nout of the blue my guild script now claims:<\/p>\n<blockquote>\n<p>guild: No module named --batch_size<\/p>\n<\/blockquote>\n<p>Batch size is a flag, as defined in the guild.yml<\/p>\n<pre><code>flags:\n        batch_size:\n          default: 32\n<\/code><\/pre>\n<p>The batch size is an argument of the arg parser<\/p>\n<pre><code>        parser.add_argument(\"--batch_size\", type=int, default=8)\n<\/code><\/pre>\n<p>I don\u2019t understand the error message, as it worked before and it is not a syntactic error:<\/p>\n<pre><code>You are about to run model:train\n  batch_size: 32\n<\/code><\/pre>\n<p>Do you have an idea what could have gone wrong?<br>\nI did not change the code and call guild with <code>guild run train<\/code>.<\/p>\n<p>Previously I had a similar error, where the <code>accelerator<\/code> flag of the PyTorch Lightning Trainer wouldn\u2019t be recognized (same error message).<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild run can't find module\/relative import",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-run-cant-find-module-relative-import\/735",
        "Question_created_time":"2021-07-16T00:53:11.066Z",
        "Question_answer_count":18,
        "Question_score_count":0,
        "Question_view_count":1307,
        "Question_body":"<p>Hello,<\/p>\n<p>I have my project structured as follows:<\/p>\n<pre><code>classification\/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 coordinate_conv.py\n\u251c\u2500\u2500 cosine_annealing.py\n\u251c\u2500\u2500 data_generator.py\n\u251c\u2500\u2500 deformable_conv.py\n\u251c\u2500\u2500 drop_block.py\n\u251c\u2500\u2500 nnet_blocks.py\n\u251c\u2500\u2500 infer.py\n\u251c\u2500\u2500 model.py\n\u251c\u2500\u2500 train.py\n\u2514\u2500\u2500 utils.py\n<\/code><\/pre>\n<p>Where every python script is a module, and arguments are passed with <code>argparse<\/code>. So I would run <code>train.py<\/code> as<\/p>\n<pre><code>python -m classification.train \\\n    --model-name model \\\n    --train-data path\/to\/train\/data \\\n    --cycles 3 \\\n    --no-require-clean\n<\/code><\/pre>\n<ol>\n<li>\n<p>If I run <code>guild run<\/code> outside the <code>classification<\/code> folder, having configured <code>guild.yml<\/code> according to the documentation (tried both: <code>main: classification.train<\/code> and <code>main: classification\/train<\/code>), <code>guild<\/code> tells me it cannot find the <code>classification.train<\/code> module.<\/p>\n<\/li>\n<li>\n<p>if I run <code>guild run train.py<\/code> from inside the <code>classification<\/code> folder, <code>guild<\/code> tells me it cannot do relative imports with no known parent package (expected I guess).<\/p>\n<\/li>\n<\/ol>\n<p>So how would I run the training script with the above project structure, argparse, and relative imports?<\/p>\n<p>Thanks!<br>\n-fernando<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed",
        "Question_link":"https:\/\/my.guild.ai\/t\/operatornotallowedingrapherror-using-a-tf-tensor-as-a-python-bool-is-not-allowed\/739",
        "Question_created_time":"2021-07-23T00:44:44.083Z",
        "Question_answer_count":8,
        "Question_score_count":3,
        "Question_view_count":2586,
        "Question_body":"<p>When I try to run the mnist example I get the following:<\/p>\n<p>OperatorNotAllowedInGraphError: using a <code>tf.Tensor<\/code> as a Python <code>bool<\/code> is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Redirecting Standard Output",
        "Question_link":"https:\/\/my.guild.ai\/t\/redirecting-standard-output\/774",
        "Question_created_time":"2021-10-27T21:00:59.517Z",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":753,
        "Question_body":"<p>I have a guild file with the following lines:<\/p>\n<pre><code>train:\n  description: Train a model\n  exec: python -m model.utils.run &gt; logfile\n<\/code><\/pre>\n<p>The train command works correctly, however it does not redirect the standard output to \u201clogfile\u201d. I am able to see the standard output in the \u201cLog\u201d tab of the guild view server, but this also contains the output of the tqdm progress bar used in the code, which is not ideal.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Import flags from dependency",
        "Question_link":"https:\/\/my.guild.ai\/t\/import-flags-from-dependency\/765",
        "Question_created_time":"2021-09-17T16:21:18.247Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":292,
        "Question_body":"<p>I have the following guild yaml structure:<\/p>\n<pre><code>prepare-config:\n    main: model.utils.prepare-config\n    sourcecode: \n        root: \/home\/\n        dest: .\n        select: '*.py'\n\ntrain:\n    main: model.utils.run\n    sourcecode: \n        root: \/home\/\n        dest: .\n        select: '*.py'\n    requires:\n      - operation: prepare-config\n        select: prepare-config-output.json\n        target-type: copy\n        target-path: .\n    flags-dest: config:prepare-config-output.json\n    flags-import: all\n<\/code><\/pre>\n<p>In the run directory for the \u201ctrain\u201d operation I can see that the \u201cprepare-config-output.json\u201d file has been copied successfully. However when I perform \u201cguild run train --help-op\u201d I don\u2019t see any of the parameters in \u201cprepare-config-output.json\u201d. I am even able to run \u201cguild run train\u201d successfully using the parameters in \u201cprepare-config-output.json\u201d, but it seems as though the flags are not imported and I cant use something like \u201cguild run train.py lr=[0.1, 0.01, 0.001]\u201d for example.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Wrong conversion of default args leads to errors in Pytorch Lightning",
        "Question_link":"https:\/\/my.guild.ai\/t\/wrong-conversion-of-default-args-leads-to-errors-in-pytorch-lightning\/759",
        "Question_created_time":"2021-09-02T11:50:18.088Z",
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":674,
        "Question_body":"<p>Hi!<br>\nI wanted to test guild.ai in combination with pytorch lightning.<br>\nHowever I am facing the problem that default arguments are not omitted but used with an empty string <code>''<\/code>.<br>\nSmall Example:<\/p>\n<p><code>main.py<\/code><\/p>\n<pre><code>from argparse import ArgumentParser\n\n\ndef main(args):\n    model = LightningModule()\n    trainer = Trainer.from_argparse_args(args)\n    trainer.fit(model)\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser = Trainer.add_argparse_args(parser)\n    args = parser.parse_args()\n\n    main(args)\n<\/code><\/pre>\n<p>Running <code>guild run main.py deterministic=true<\/code> doesn\u2019t call <code>python main.py --deterministic true<\/code> but <code>python main.py --deterministic 1  --auto_select_gpus '' --benchmark '' ...<\/code>.<br>\nIn this case Pytorch Lightning throws <code>main.py: error: argument --auto_select_gpus: invalid str_to_bool value: ''<\/code><br>\nHow can I either suppress default flags from being sent or modify the parsing?<br>\nThe <code>str_to_bool<\/code> function is defined <a href=\"https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/bb4887368c7210ba950078e91cd7ae8b512fcaee\/pytorch_lightning\/utilities\/parsing.py#L44\" rel=\"noopener nofollow ugc\">here<\/a>, the <code>add_argparse_args<\/code> <a href=\"https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/a1264a68502ee40fe3ab1cbc2794583d3a56b989\/pytorch_lightning\/utilities\/argparse.py#L159\" rel=\"noopener nofollow ugc\">here<\/a> and the Trainer <a href=\"https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/75350938ca646efc0b4bac432ba2d5d4676662bb\/pytorch_lightning\/trainer\/trainer.py#L101\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild Init Errors on Windows 10",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-init-errors-on-windows-10\/751",
        "Question_created_time":"2021-08-13T20:25:45.302Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":291,
        "Question_body":"<p>I am working my way through the example files and have run into a bug where the guild init command fails.<\/p>\n<pre><code>You are about to initialize a Guild environment:\n  Location: C:\\Users\\{USER}\\repos\\ml_framework_testing\\guildai\\iris-svm\\venv\n  Name: iris-svm\n  Python interpreter: default\n  Use system site packages: no\n  Guild: 0.7.3\n  Python requirements:\n    .\\requirements.txt\n  Resource cache: shared\nContinue? (Y\/n) y\nInitializing Guild environment in C:\\Users\\{USER}\\repos\\ml_framework_testing\\guildai\\iris-svm\\venv\nCreating virtual environment\ncreated virtual environment CPython3.7.4.final.0-64 in 1341ms\n  creator CPython3Windows(dest=C:\\Users\\{USER}\\repos\\ml_framework_testing\\guildai\\iris-svm\\venv, clear=False, no_vcs_ignore=False, global=False)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=C:\\Users\\joeyr\\AppData\\Local\\pypa\\virtualenv)\n    added seed packages: pip==21.2.3, setuptools==57.4.0, wheel==0.37.0\n  activators BashActivator,BatchActivator,FishActivator,PowerShellActivator,PythonActivator\nUpgrading pip\nInstalling Guild guildai==0.7.3\nFatal Python error: _Py_HashRandomization_Init: failed to get random numbers to initialize Python\n\nguild: Command '['C:\\\\Users\\\\{USER}\\\\repos\\\\ml_framework_testing\\\\guildai\\\\iris-svm\\\\venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--no-warn-script-location', 'guildai==0.7.3']' returned non-zero exit status 1.\n<\/code><\/pre>\n<p>Any idea what is going on here or how to fix it? I was getting an access denied error on the Upgrading pip line for a bit until I upgraded manually ahead of time with \u201cpython -m pip install --upgrade pip\u201d.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error while running gp optimizer for Hyperparameter optimization: Cannot find objective 'loss'",
        "Question_link":"https:\/\/my.guild.ai\/t\/error-while-running-gp-optimizer-for-hyperparameter-optimization-cannot-find-objective-loss\/679",
        "Question_created_time":"2021-04-12T02:05:58.528Z",
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":597,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use Guild to do hyperparameter optimization using the gp optimizer. The first three runs started using random initializations as expected. However, even after the 3rd run, it continues to perform random initializations. On examining the output, I noticed that the following information was posted:<\/p>\n<p>INFO: [guild] Random start for optimization (cannot find objective \u2018loss\u2019)<\/p>\n<p>Is this expected? Or am I missing something?<\/p>\n<p>I looked at the scalars using guild runs info and found that all the scalars that I am logging using Tensorboard are displayed correctly.<\/p>\n<p>I would appreciate any help in this matter.<\/p>\n<p>Thanks,<br>\nVishal<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild runs on remote not found",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-on-remote-not-found\/738",
        "Question_created_time":"2021-07-22T23:22:13.100Z",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":284,
        "Question_body":"<p>I\u2019ve setup a remote in the <code>guild.yml<\/code> file. No problem running <code>guild pull --remote server<\/code> and the like. But how does one run a remote op from the local computer? There must be something simple I\u2019m missing. Running:<\/p>\n<pre><code>guild run train --remote server\n<\/code><\/pre>\n<p>Always yields <code>guild: cannot find operation train<\/code>. This kind of makes sense, how would guild know the location of the ops on the remote? My <code>guild.yml<\/code> looks something like:<\/p>\n<pre><code>  server:\n    type: ssh\n    description: Remote servers\n    user: username\n    host: server.domain\n    conda-env: \/home\/path\/to\/anaconda3\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Running multiple batches of an experiment with different hyperparameter flag values",
        "Question_link":"https:\/\/my.guild.ai\/t\/running-multiple-batches-of-an-experiment-with-different-hyperparameter-flag-values\/724",
        "Question_created_time":"2021-06-18T21:33:04.492Z",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":401,
        "Question_body":"<p>I am trying to utilize the grid search capability of guild to run an experiment multiple times with different hyperparameter flag values in a python file for SVM on the iris dataset.<\/p>\n<p>My hyperparameter flags are defined as a dictionary:<br>\nhyperparam_dict = {\u2018kernel\u2019: \u2018linear\u2019, \u2018test_split\u2019: 0.1, \u2018random_seed\u2019: 2, \u2018degree\u2019: 4, \u2018gamma\u2019: 50} and I want to be able to run the experiment with various values for each hyperparameter to test the accuracy value of every combination of flag values\u2026 e.g \u2018test_split\u2019 = [0.1, 0.2, 0.3], \u2018degree\u2019= [1, 2, 3, 4], etc.<\/p>\n<p>When I follow the example from the get-started.ipynb:<br>\n_ = guild.run(train, x=[-0.5,-0.4,-0.3,-0.2,-0.1])<br>\nwith my code:<br>\nguild.run(main, hyperparam_dict[\u2018test_split\u2019] =  [0.1, 0.2, 0.3],) I am getting an error.<\/p>\n<p>Is there any way what I am trying to achieve, be done??<\/p>\n<p>Looking for any help to try and resolve this issue!<\/p>\n<p>Thank you.<\/p>\n<p>Original guild.yml file:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/a15c24e93e41c11d4bd4960f978ec4c8b2a9631e.png\" alt=\"image\" data-base62-sha1=\"n1si1CVnP83aIsR6893PR5wXRVQ\" width=\"536\" height=\"425\"><\/p>\n<p>Notebook commands where I am trying to achieve running multiple runs of an experiment with three different  'test_split\" values to be tested.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c.png\" data-download-href=\"\/uploads\/short-url\/sjq2ufe4bGzrQn5kM5Fk1zyT0q8.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c.png\" alt=\"image\" data-base62-sha1=\"sjq2ufe4bGzrQn5kM5Fk1zyT0q8\" width=\"690\" height=\"197\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">727\u00d7208 9.91 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Flags are not being tracked\/captured during a guild run inside a docker container",
        "Question_link":"https:\/\/my.guild.ai\/t\/flags-are-not-being-tracked-captured-during-a-guild-run-inside-a-docker-container\/717",
        "Question_created_time":"2021-06-04T22:39:09.934Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":343,
        "Question_body":"<p>I am trying to execute an experiment.py file that is called upon in guild.yml. The guild.yml file contains 5 flags for an svm experiment on the iris dataset. When I execute \u2018guild run model:train\u2019 on my local machine in a terminal, there are no issues and the <code>flags<\/code> are properly captured in the \/.guild directory. However, when I execute the experiment in a Docker container, the flags are no longer being tracked by guild. Is there any way to debug this??<br>\nOr where in the source code are the flags picked up by guild to create the Flags file in \/.guild directory.<br>\nPlease help!<\/p>\n<p>guild.yml file:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9917a5497d13a1a0217004c62eaee40ae9117b5f.png\" alt=\"image\" data-base62-sha1=\"lQjIeUjvD969jrZdz9AUmi1LCwL\" width=\"542\" height=\"321\"><\/p>\n<p>terminal output display of what in inside the \/.guild directory: (notice flags is just an empty dictionary)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b0b0d9e9a6511673b88d4aa093dc95ca7a280c8d.png\" data-download-href=\"\/uploads\/short-url\/pd4W5283z3KfZvKgLj5qtTat1Pn.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b0b0d9e9a6511673b88d4aa093dc95ca7a280c8d.png\" alt=\"image\" data-base62-sha1=\"pd4W5283z3KfZvKgLj5qtTat1Pn\" width=\"690\" height=\"75\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b0b0d9e9a6511673b88d4aa093dc95ca7a280c8d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">960\u00d7105 4.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Operations as dependencies during checks",
        "Question_link":"https:\/\/my.guild.ai\/t\/operations-as-dependencies-during-checks\/701",
        "Question_created_time":"2021-05-04T22:17:15.120Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":291,
        "Question_body":"<p>I have a <code>guild<\/code> model that looks something like this:<\/p>\n<pre><code>- include: source_code_config.yml\n- model: _check\n  extends:\n    - source_code_config\n  operations:\n    _test_segmentation_training:\n      steps:\n        - run: segmentation:train dryrun=yes num_epochs=1 input_database=\"x.csv\"\n          expect:\n            - file: experiments\/best_model.pt\n    _test_segmentation_testing:\n      steps:\n        - run:segmentation:test input_database=\"x.csv\"\n          expect:\n            - output: \"Testing done.\"\n    _all:\n      steps:\n        - _test_segmentation_training\n        - _test_segmentation_testing\n<\/code><\/pre>\n<p>I use this for integration testing my training and tests scripts.<\/p>\n<p>The <code>segmentation<\/code> model looks something like this:<\/p>\n<pre><code>- model: segmentation\n  extends:\n    - source_code_config\n  operations:\n    train:\n      main: scripts\/training\/train_segmentation\n      flags:\n        $include:\n          - segmentation_flags\n          - train_flags\n          - common_flags\n      requires:\n        - prepared_data\n    test:\n      main: scripts\/training\/test_segmentation\n      flags:\n        batch_size: 1\n        $include:\n          - test_flags\n          - common_flags\n      requires:\n        - prepared_data\n        - trained_model\n  resources:\n    trained_model:\n      sources:\n        - operation: train\n          select:\n            - experiments\n            - .guild\/attrs\/flags\n          target-type: copy\n          rename:\n            - flags training-flags.yml # See https:\/\/github.com\/guildai\/guildai\/blob\/0.7.2\/examples\/upstream-flags\/guild.yml\n<\/code><\/pre>\n<p>Now when I run<\/p>\n<pre><code>guild run segmentation:train\n<\/code><\/pre>\n<p>Followed by:<\/p>\n<pre><code>guild run segmentation:test\n<\/code><\/pre>\n<p><code>guild<\/code> will automatically resolve the <code>trained_model<\/code> dependency and find the latest <code>segmentation:train<\/code> run.<\/p>\n<p>If I instead run:<\/p>\n<p><code>guild _check:_all<\/code><\/p>\n<p>The <code>_test_segmentation_training<\/code> runs successfully, but the <code>_test_segmentation_testing<\/code> operation is not able to resolve the <code>trained_model<\/code> resource.<\/p>\n<p>Ideally I would be able to run this pipeline as a step in my integration testing.<\/p>\n<p>EDIT:<\/p>\n<p>Using <code>guild<\/code> version <code>0.7.3<\/code><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error while publishing runs",
        "Question_link":"https:\/\/my.guild.ai\/t\/error-while-publishing-runs\/700",
        "Question_created_time":"2021-05-04T16:33:28.305Z",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":302,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to publish a particular run using the command <code>guild runs publish --dest &lt;path of destination&gt; &lt;run id&gt;<\/code>. However, I get the following error.<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"\/usr\/local\/bin\/guild\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/main_bootstrap.py\", line 40, in main\n    _main()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/click\/decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/commands\/runs_publish.py\", line 113, in publish_runs\n    runs_impl.publish(args, ctx)\nAttributeError: module 'guild.commands.runs_impl' has no attribute 'publish'\n<\/code><\/pre>\n<p>Any recommendations on how to solve this? I am using guild version 0.7.2.<\/p>\n<p>Thanks,<br>\nVishal<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"OperatorNotAllowedInGraphError: Error while using Guild run",
        "Question_link":"https:\/\/my.guild.ai\/t\/operatornotallowedingrapherror-error-while-using-guild-run\/697",
        "Question_created_time":"2021-05-02T17:38:49.009Z",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":939,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use a code with guild. I checked the code using python  and the code runs successfully. However, when I try to use the code using guild run  it gives me the following error. On looking at the error closely, I found that the code gives me an error in the model.fit function of keras. I am not sure what is happening when we use guild run in this function that is triggering the error?<\/p>\n<p>Any help or suggestion in this matter would be greatly appreciated.<\/p>\n<p>Thanks in advance,<br>\nVishal<\/p>\n<p>Full error below.<\/p>\n<pre><code>File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py\", line 108, in _method_wrapper\n    return method(self, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py\", line 1098, in fit\n    tmp_logs = train_function(iterator)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 780, in __call__\n    result = self._call(*args, **kwds)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 823, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 697, in _initialize\n    *args, **kwds))\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/function.py\", line 3213, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/function.py\", line 3075, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/func_graph.py\", line 986, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/def_function.py\", line 600, in wrapped_fn\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/func_graph.py\", line 973, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:\n\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py:806 train_function  *\n        return step_function(self, iterator)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py:799 step_function  **\n        write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py:2757 write_scalar_summaries\n        summary_ops_v2.scalar('batch_' + name, value, step=step)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/python_util.py:321 wrapper\n        cb(self._func, *args, **kw)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/summary_util.py:198 _handle_scalar_ops_v2\n        vals = self._summary_values(step)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/summary_util.py:179 _summary_values\n        return self._summary_cache.for_step(global_step)\n    \/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/summary_util.py:246 for_step\n        return self._val if step == self._step else None\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py:877 __bool__\n        self._disallow_bool_casting()\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py:487 _disallow_bool_casting\n        \"using a `tf.Tensor` as a Python `bool`\")\n    \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py:474 _disallow_when_autograph_enabled\n        \" indicate you are trying to use an unsupported feature.\".format(task))\n\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dynamically generated parameters in pipeline",
        "Question_link":"https:\/\/my.guild.ai\/t\/dynamically-generated-parameters-in-pipeline\/699",
        "Question_created_time":"2021-05-04T13:43:27.746Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":311,
        "Question_body":"<p>Conceptually I have a two stage pipeline. Where the first stage generates a set of flags (\u201chyper-hyper parameters\u201d). Then in the second stage I want to combine those with a set of hyper parameters to optimize. The challenge is that since they\u2019re created dynamically\u2026 I can\u2019t know ahead of time how many there are.<\/p>\n<p>I can do it manually like this<\/p>\n<p><code>guild run train x='[1,2,3] y='[1,2,3]'' @bigbatch.csv<\/code><\/p>\n<p>What I would like to do is for the train step to use a generated bigbatch.csv from the upstream pipeline<\/p>\n<p>I\u2019ve attached what I think it should look like at the guild.yml level<\/p>\n<pre><code>train:\n  description: Sample training script\n  flags-import: all\n  requires:\n    - operation: bigbatch\nbigbatch:\n  description: make file bigbatch.csv\n<\/code><\/pre>\n<p>This gives me  a symlink to the correct file called bigbatch.csv in the \u201ctrain folder\u201d after the <code>guild train<\/code> operation. However when I use the \u201c@\u201d batch notation the bigbatch.csv is taken from my cwd. Is there any way to reference batch parameters in the guild.yml?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild doesn't copy module to new source code location",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-doesnt-copy-module-to-new-source-code-location\/690",
        "Question_created_time":"2021-04-21T14:37:30.647Z",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":671,
        "Question_body":"<p>When I use the command <code>guild run train,py<\/code> I get the error <code>Error: can't important module_1<\/code>. When I look in the folder to which guildai copies the source code after I call guild run, I can see that <code>module_1<\/code> was indeed not copied. There is nothing special about that module_1 (normal code files). How can I debug this issue further?<\/p>\n<p>Here is my folder structure:<\/p>\n<p>Folder structure:<\/p>\n<pre><code>guild.yml\ntrain.py\nmodule_1\nmodule_2\nmodule_3\n<\/code><\/pre>\n<p>guild.yml<\/p>\n<pre><code>train:\n  description: Training script\n  main: train\n  # sourcecode:\n  #   - '*.py'\n  flags-dest: global:params\n  flags-import: all\n  flags:\n    ## general\n    gpu:\n      description:\n      default: 0\n    seed:\n      description:\n      default: 0\n...\n<\/code><\/pre>\n<p>Thanks for your help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"sqlite3.OperationalError: disk I\/O error when using the scratch drive on Linux cluster for storage of guild runs",
        "Question_link":"https:\/\/my.guild.ai\/t\/sqlite3-operationalerror-disk-i-o-error-when-using-the-scratch-drive-on-linux-cluster-for-storage-of-guild-runs\/684",
        "Question_created_time":"2021-04-13T02:27:04.715Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":349,
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use Guild for hyperparameter optimization. I am running max-trials of 50 and want to store these temporary models in the \/scratch drive on the Linux cluster. I checked that the drive is mounted corrected and I am able to read and write properly in the drive. However, when i submit my guild run, I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"\/usr\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.6\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_gp_main.py\", line 77, in &lt;module&gt;\n    main()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_gp_main.py\", line 30, in main\n    skopt_util.handle_seq_trials(batch_run, _suggest_x)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 210, in handle_seq_trials\n    _run_seq_trials(batch_run, suggest_x_cb)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 234, in _run_seq_trials\n    batch_flag_vals,\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 266, in _iter_seq_trials\n    prev_trials = prev_trials_cb()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/plugins\/skopt_util.py\", line 224, in &lt;lambda&gt;\n    prev_trials_cb = lambda: batch_util.trial_results(batch_run, [objective_scalar])\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/batch_util.py\", line 404, in trial_results\n    return trial_results_for_runs(trial_runs(batch_run), scalars)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/batch_util.py\", line 408, in trial_results_for_runs\n    index = _run_index_for_scalars(runs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/batch_util.py\", line 423, in _run_index_for_scalars\n    index = indexlib.RunIndex()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/index.py\", line 314, in __init__\n    self._db = self._init_db()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/index.py\", line 323, in _init_db\n    self._init_tables(db)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/guild\/index.py\", line 349, in _init_tables\n    \"\"\"\nsqlite3.OperationalError: disk I\/O error\n<\/code><\/pre>\n<p>I checked my \/scratch drive and found that the runs and cache folders are created. Also found that one folder was created inside runs. But this folder was empty.<\/p>\n<p>The same command works perfectly when I run using my GUILD_HOME as a different drive. I am not sure if I am missing anything here.<\/p>\n<p>I would appreciate any help from your side.<\/p>\n<p>Thanks,<br>\nVishal<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dask scheduler not using multiple gpus on remote",
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-not-using-multiple-gpus-on-remote\/583",
        "Question_created_time":"2021-03-29T16:51:28.225Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":499,
        "Question_body":"<p>Hi,<br>\nIt\u2019s a fresh feature and most likely I\u2019m doing something wrong, but I cannot get the Dask scheduler to work properly on a remote with 4 gpus.<\/p>\n<p>This is what I do, and apart from the remote part it is basically a copy of the steps in the How To guide:<\/p>\n<ol>\n<li>\n<p>Connect to remote. I have successfully staged runs on remote, run them directly, and also used multiple gpus by assigning runs to gpus manually using --gpus flag. So the remote works corectly.<\/p>\n<\/li>\n<li>\n<p>Start the Dask scheduler on the remote.<\/p>\n<\/li>\n<li>\n<p>Then I stage trials on remote, let\u2019s say 4, with different parameters, using a single command.<\/p>\n<\/li>\n<\/ol>\n<pre><code>guild run TABL:train window=[100,200,300,400] --remote cerberus --stage-trials\n<\/code><\/pre>\n<ol start=\"4\">\n<li>The trials are sent to remote, and the scheduler starts them. If workers is set to 4, it starts correctly 4 processes of loading data etc.<\/li>\n<\/ol>\n<p>And this is where something goes wrong. After doing the pre-procesing stage and creating 4 models concurrently, the scheduler places all 4 models and training processses on all 4 gpus (which can be seen on the screenshot - all gpus have allocated memory). And then only begins the training on 1 gpu.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png\" data-download-href=\"\/uploads\/short-url\/2PTbvO3vWfJgJUE57EqoCMII9Jc.png?dl=1\" title=\"Screenshot from 2021-03-29 18-26-25\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png\" alt=\"Screenshot from 2021-03-29 18-26-25\" data-base62-sha1=\"2PTbvO3vWfJgJUE57EqoCMII9Jc\" width=\"490\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2021-03-29 18-26-25<\/span><span class=\"informations\">731\u00d7745 103 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><strong>Expected<\/strong><br>\nI would expect the scheduler to assign the trials to available gpus and train them concurrently. Furthermore, when the number of trials is bigger than the number of gpus, I would expect the scheduler to automatically run the pending operation when a gpus becomes free.<\/p>\n<p>Did I understand what the scheduler is capable of correctly? Is there maybe some manual step somewhere that I missed?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Remote stop not working",
        "Question_link":"https:\/\/my.guild.ai\/t\/remote-stop-not-working\/584",
        "Question_created_time":"2021-03-30T15:37:34.840Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":333,
        "Question_body":"<p>After running <code>guild runs stop X -r server<\/code>, the processes are still running on the remote and GPU memory has not been released even though guild reports the run as terminated.<\/p>\n<p>I think it may be pytorch\u2019s data loader worker processes that are still running but I\u2019m not sure. I think I saw something about this subject before but I couldn\u2019t find it here or on github. Has anyone else experienced this issue?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild --remote option not recognized",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-remote-option-not-recognized\/578",
        "Question_created_time":"2021-03-26T09:15:40.636Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":345,
        "Question_body":"<p>I\u2019m running Guild AI 0.7.3 and following the instructions for remote running <a href=\"https:\/\/my.guild.ai\/t\/remotes\/171\" class=\"inline-onebox\">Remotes<\/a> I run the command to check everything is set up correctly<\/p>\n<pre><code>guild --remote dev check\n<\/code><\/pre>\n<p>guild: unrecognized option \u2018\u2013remote\u2019<br>\nTry \u2018guild --help\u2019 for more information.<\/p>\n<p>I have configured my remote and can access it using ssh, my ~\/.guild\/config.yml looks like this.<\/p>\n<pre><code>remotes:\n    dev:\n      type: ssh\n      description: DSVM on Azure for development\n      host: dl\n<\/code><\/pre>\n<p>Is the tutorial outdated? Should I be able to run things remotely?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Detect SimpleNamespace as flag",
        "Question_link":"https:\/\/my.guild.ai\/t\/detect-simplenamespace-as-flag\/567",
        "Question_created_time":"2021-03-22T18:35:28.886Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":310,
        "Question_body":"<p>Hi, I\u2019m trying to tune hyperparameters using guild. These are defined as follows:<\/p>\n<pre><code>from types import SimpleNamespace\nparams = SimpleNamespace(\n    embedding_dim = 256,\n    window_size = 5,\n    batch_size = 2048,\n    epochs = 2,\n    preprocessed = f'{DATASET_ROOT}\/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}\/{DATASET_PREFIX}',\n    modelname = f'{WORKING_ROOT}\/{DATASET_VERSION}.pt',\n    train = True\n)\n<\/code><\/pre>\n<p>Guild won\u2019t find them by default and I am having a hard time working around this. I am  new to guild so maybe there is an answer in the DOCS i haven\u2019t found.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cannot open Tensorboard with Guild - unhashable type 'Dict'",
        "Question_link":"https:\/\/my.guild.ai\/t\/cannot-open-tensorboard-with-guild-unhashable-type-dict\/563",
        "Question_created_time":"2021-03-11T13:47:11.294Z",
        "Question_answer_count":9,
        "Question_score_count":4,
        "Question_view_count":684,
        "Question_body":"<p>Hi,<br>\nFirst of all, great job, loving Guild so far.<\/p>\n<p>I have successfully created a queue, staged runs and completed them. I can run \u2018guild view\u2019 or \u2018tensorboard\u2019 standalone and see the logs.<\/p>\n<p>However, I cannot run \u2018guild tensorboard\u2019 or similary start tensorboard from guild view. In both cases I get an error with the following traceback:<\/p>\n<pre><code>(anomaly_detection) E:\\source\\repos\\anomaly_simulation\\Zoo&gt;guild -H E:\/source\/repos\/anomaly_simulation\/Results tensorboard\nPreparing runs for TensorBoard\nTraceback (most recent call last):\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\blepo\\Anaconda3\\envs\\anomaly_detection\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 94, in _run_tensorboard\n    monitor.run_once(exit_on_error=True)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\run_util.py\", line 80, in run_once\n    runs = self.list_runs_cb()\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\tensorboard.py\", line 119, in f\n    _ensure_hparam_experiment(runs, state)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\tensorboard.py\", line 134, in _ensure_hparam_experiment\n    hparams = _experiment_hparams(runs)\n  File \"c:\\users\\blepo\\anaconda3\\envs\\anomaly_detection\\lib\\site-packages\\guild\\tensorboard.py\", line 146, in _experiment_hparams\n    hparams.setdefault(name, set()).add(val)\nTypeError: unhashable type: 'dict'\n<\/code><\/pre>\n<p>Any idea what may be causing it and what is the solution?<\/p>\n<p>My system is Windows 10, Guild version is 0.7.2, and Python is 3.8.5.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Docs using -o option outdated",
        "Question_link":"https:\/\/my.guild.ai\/t\/docs-using-o-option-outdated\/568",
        "Question_created_time":"2021-03-21T09:42:08.869Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":274,
        "Question_body":"<p>In restart-batch-to-continue-optimization the \u201cguild select -o train.py+gp\u201d command actually doesn\u2019t work. I tried with \u201cguild select -Fo gp\u201d and it seemed to work.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Logging scalars when running ray[tune] tuning fails in a guild run",
        "Question_link":"https:\/\/my.guild.ai\/t\/logging-scalars-when-running-ray-tune-tuning-fails-in-a-guild-run\/557",
        "Question_created_time":"2021-03-09T02:13:14.962Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":486,
        "Question_body":"<p>In my project I have a bit of automatic tuning of my pytorch-lightning models using ray and then I also automatically apply the model. The logging that ray[tune] uses is a SummaryWriter from tensorboardX package. I am also using tensorboardX SummaryWriter for logging other things in my project. For my own logging, there is no issue with this, but for some reason guild fails with the calls to <code>add_scalar()<\/code> when it\u2019s called from the tune library.<\/p>\n<p>The trace:<\/p>\n<pre><code>3\/8\/2021 5:40:52 PM\nTraceback (most recent call last):\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 594, in _process_trial\n3\/8\/2021 5:40:52 PM\ndecision = self._process_trial_result(trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 666, in _process_trial_result\n3\/8\/2021 5:40:52 PM\nself._callbacks.on_trial_result(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/callback.py\", line 192, in on_trial_result\n3\/8\/2021 5:40:52 PM\ncallback.on_trial_result(**info)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 393, in on_trial_result\n3\/8\/2021 5:40:52 PM\nself.log_trial_result(iteration, trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 631, in log_trial_result\n3\/8\/2021 5:40:52 PM\nself._trial_writer[trial].add_scalar(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/guild\/python_util.py\", line 239, in wrapper\n3\/8\/2021 5:40:52 PM\ncb(wrapped_bound, *args, **kw)\n3\/8\/2021 5:40:52 PM\nTypeError: _handle_scalar() got an unexpected keyword argument 'global_step'\n<\/code><\/pre>\n<p>The line from tune in question in full is <code>self._trial_writer[trial].add_scalar(full_attr, value, global_step=step)<\/code>. This fails.<\/p>\n<p>In my own project I have the following line: <code> logger.add_scalar(f\"{prefix}\/{tag}\", scalar_value, global_step, walltime)<\/code> and this does not fail.<\/p>\n<p>So I went into the ray.tune library and I changed the call to <code>self._trial_writer[trial].add_scalar(full_attr, value, step)<\/code> and reran it. The failure went away.<\/p>\n<p>I dug into the <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/e9271824141583b96a6de7d1d5cebd44a04e43fe\/guild\/plugins\/summary_util.py#L181\" rel=\"noopener nofollow ugc\">github<\/a> source, and it looks like <code>_handle_scalar()<\/code> is expecting <code>step<\/code> and not <code>global_step<\/code>.<\/p>\n<p>I originally needed help with this but as I wrote this I ended up figuring out the answer. Looks like there\u2019s a potential bug here?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to view runs on all remotes?",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-view-runs-on-all-remotes\/554",
        "Question_created_time":"2021-02-28T08:14:37.203Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":416,
        "Question_body":"<p>Hi, I have concurrent runs on multiple remotes (say 4 runs on 4 different remote). Is there a way to view the status of all runs from a single interface? I would imagine that <code>guild runs<\/code> should show them, but for some reason the remote runs appear as \u201cterminated\u201d (while in fact running) and switch to \u201ccompleted\u201d when they are finished.<br>\nI am working with a cluster that has shared drives if that could be taken advantage of.<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Regular expression not detecting latest successful run for required operation dependency",
        "Question_link":"https:\/\/my.guild.ai\/t\/regular-expression-not-detecting-latest-successful-run-for-required-operation-dependency\/549",
        "Question_created_time":"2021-02-24T17:12:39.263Z",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":454,
        "Question_body":"<p>According to the docs I can specify multiple supported operations for a required statement by using regular expressions<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"197\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197\/1\">Guild File Reference<\/a>\n<\/div>\n<blockquote>\n<p>Value is a regular expression matching a suitable operation name. Multiple operations are supported by specifying the appropriate regular expression.<\/p>\n<\/blockquote>\n<\/aside>\n<p>An example is also given<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"192\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192\/1\">Guild File Cheatsheet<\/a>\n<\/div>\n<blockquote>\n<p>Require <code>model.ckpt<\/code> from any operation that starts with <code>train-<\/code> :<\/p>\n<pre><code>train:\n  requires:\n    - operation: ^train-\n      select: model\\.ckpt\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>However whenever I try to selecting the latest run using regular expressions, for example using the following guild file<\/p>\n<pre><code>\ntrain-pull:\n    main: script\n    \ntrain-extract:\n    main: script\n\ntest:\n  requires:\n  - operation: ^train-\n<\/code><\/pre>\n<p>The latest successful run is not detected<\/p>\n<pre><code>guild: run failed because a dependency was not met: could not resolve 'operation:^train-' in ^train- resource: no suitable run for ^train-\n<\/code><\/pre>\n<p>If I specify the run id then the run is detected, but this seems to work regardless of if the passed run id is compatible with the regular expression given in the guild file.<\/p>\n<p>Instead of this behavior I would expect guild to detect the latest run done for either train-pull  or train-extract and use it as the operation requirement for test.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"EDITOR with VS Code not working on Windows",
        "Question_link":"https:\/\/my.guild.ai\/t\/editor-with-vs-code-not-working-on-windows\/545",
        "Question_created_time":"2021-02-16T06:37:58.077Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":327,
        "Question_body":"<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"146\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"> guildai:<\/div>\n<blockquote>\n<p>Guild uses the editor defined in <code>VISUAL<\/code> or <code>EDITOR<\/code> environment variables.<\/p>\n<\/blockquote>\n<\/aside>\n<p>I had set the EDITOR variable to code in Windows 10. This is opening up a file in VS Code. But the file is immediately deleted and guild is auto-confirming the default values, So I am not able to change the values in the text editor.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"OSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions",
        "Question_link":"https:\/\/my.guild.ai\/t\/oserror-winerror-10013-an-attempt-was-made-to-access-a-socket-in-a-way-forbidden-by-its-access-permissions\/546",
        "Question_created_time":"2021-02-17T23:12:44.104Z",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":3568,
        "Question_body":"<p>I get forbidden access even when I run with administrative privileges. I re-run in again and it usually works. How can I fix this  behavior?<\/p>\n<pre><code>(biobench-thMxqAli) \u03bb guild tensorboard 1\nPreparing runs for TensorBoard\nWARNING: Guild took 27.80 seconds to prepare runs. To reduce startup time, try running with '--skip-images' or '--skip-hparams' options or reduce the number of runs with filters. Try 'guild tensorboard --help' for filter options.\nWARNING: webfiles.zip static assets not found: c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\plugins\\webfiles.zip\n2021-02-17 15:04:42.417335: W tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n2021-02-17 15:04:42.417554: I tensorflow\/stream_executor\/cuda\/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\.virtualenvs\\biobench-thMxqAli\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 98, in _run_tensorboard\n    tensorboard.serve_forever(\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 636, in serve_forever\n    run_simple_server(app, host, port, ready_cb)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 602, in run_simple_server\n    server, _ = make_simple_server(tb_app, host, port)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\guild\\tensorboard.py\", line 615, in make_simple_server\n    server = serving.make_server(host, port, app, threaded=True)\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\werkzeug\\serving.py\", line 847, in make_server\n    return ThreadedWSGIServer(\n  File \"c:\\users\\sarat.chinni\\.virtualenvs\\biobench-thmxqali\\lib\\site-packages\\werkzeug\\serving.py\", line 740, in __init__\n    HTTPServer.__init__(self, server_address, handler)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 452, in __init__\n    self.server_bind()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\http\\server.py\", line 138, in server_bind\n    socketserver.TCPServer.server_bind(self)\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 466, in server_bind\n    self.socket.bind(self.server_address)\nOSError: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Scalars not getting saved",
        "Question_link":"https:\/\/my.guild.ai\/t\/scalars-not-getting-saved\/527",
        "Question_created_time":"2021-01-21T17:01:32.978Z",
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":411,
        "Question_body":"<p>I\u2019m new to guild.ai and trying to use the guild.yml files to save all relevant information including scalars. The scalars are however not being saved.<\/p>\n<p>This is my code:<\/p>\n<pre><code>boston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nnamedf = ['X_train', 'X_test']\nnamenp = ['y_train', 'y_test']\n\nreg = Ridge(alpha=0.9)\nfitted = reg.fit(X, y)\n\nprint(\"score: %f\" % fitted.score(X, y))\n<\/code><\/pre>\n<p>and this is my guild.yml file:<\/p>\n<pre><code>ridge-regression:\n  description: fit ridge regression using boston data.\n  notebook: Checklist.ipynb\n  flags:\n    alpha:\n      description: alpha value in ridge regression\n      nb-replace: 'alpha=(\\d+)'\n  output-scalers:\n    score: 'score: (\\value)'\n<\/code><\/pre>\n<p>When I print out all information on the command line there are no scalars to be seen. Does anyone know why?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dependecies Problem",
        "Question_link":"https:\/\/my.guild.ai\/t\/dependecies-problem\/523",
        "Question_created_time":"2021-01-19T14:06:30.742Z",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":616,
        "Question_body":"<p>Dear all,<br>\nI am new to Guild.ai<br>\nI have a train file called train.py that needs as input a parameter (using argparse) that is called config_filepath .<br>\nI am trying over and over to run:<br>\nguild run train.py config_filepath=config_thyroid_segnet_multi_h5.json<\/p>\n<p>but nothing works. Everytime I get the following message:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018config_thyroid_segnet_multi_h5.json\u2019<\/p>\n<p>The funny part is that if I check in the runs directory (performing guild ls) the file is there. What am I doing wrong? I tried also to create a guild.yml file like following:<br>\ntrain.py:<br>\ndescription: Say hello to my friends<br>\nmain: train<br>\ndefault: yes<br>\nflags-import:<br>\n- config_filepath<\/p>\n<p>And alternatively to create a guild.yml with the tag requires.<\/p>\n<p>Nothing works\u2026<\/p>\n<p>Thanks in advance for your support<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sharing sourcecode attribute between models",
        "Question_link":"https:\/\/my.guild.ai\/t\/sharing-sourcecode-attribute-between-models\/525",
        "Question_created_time":"2021-01-20T16:06:58.181Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":266,
        "Question_body":"<p>I have a <code>guild.yml<\/code> that looks like this:<\/p>\n<pre><code>- include: guild\/model1.yml\n- include: guild\/model2.yml\n- include: guild\/model3.yml\n<\/code><\/pre>\n<p>where <code>guild\/model1.yml<\/code> and <code>guild\/model2.yml<\/code> looks something like this:<\/p>\n<pre><code>- model: model1\/model2\n  sourcecode:\n    - exclude:\n        dir:\n          - data\n          - build\n          - libs\n    - include:\n        dir:\n          - scripts\n          - module\n<\/code><\/pre>\n<p>As you notice, I have to specify the <code>sourcecode<\/code> attribute for each model, which gets a bit tedious. I am wondering if there is a better way to do this. Any suggestion?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Compare command not working with hiplot on Windows 10",
        "Question_link":"https:\/\/my.guild.ai\/t\/compare-command-not-working-with-hiplot-on-windows-10\/514",
        "Question_created_time":"2021-01-11T09:08:06.236Z",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":322,
        "Question_body":"<p>I am able to use <code>hiplot<\/code> in Jupyter notebook and also start using it via command line using <code>python -m hiplot<\/code>. But when I try to use it with <code>guild compare<\/code> I get invalid windows application error.<\/p>\n<pre><code>C:\\Users\\sarat.chinni\\Codes_sequencing\\hiplot&gt;guild compare --tool hiplot\nPreparing data for compare\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\Codes_sequencing\\biobench\\sandbox\\Sarat\\supervised_sequencing\\.venv\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare.py\", line 224, in compare\n    compare_impl.main(args, ctx)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 73, in main\n    _compare_with_tool(args)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 566, in _compare_with_tool\n    _compare_with_hiplot(args)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 579, in _compare_with_hiplot\n    hiplot.compare_runs(get_data_cb)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 36, in compare_runs\n    _handle_default(hiplot, data)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 52, in _handle_default\n    _generate_hiplot_html(hiplot, csv_path, html_path)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 100, in _generate_hiplot_html\n    html = subprocess.check_output([hiplot_include, \"--format\", \"html\", csv_path])\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 411, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 489, in run\n    with Popen(*popenargs, **kwargs) as process:\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 854, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 1307, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nOSError: [WinError 193] %1 is not a valid Win32 application\n<\/code><\/pre>\n<p>Does this has to do anything with <code>hiplot<\/code> is python script and not windows executable?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild runs showing as error instead of pending",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-showing-as-error-instead-of-pending\/518",
        "Question_created_time":"2021-01-12T21:15:30.764Z",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":448,
        "Question_body":"<p>I\u2019ve been using my guild.yml file with no issue so far. I\u2019m not sure what happened but suddenly my pending runs are showing up as errors (no logs, no error messages, but the environment somehow hasn\u2019t been resolved either).<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c780081e0276246cae200732bc38d932d3a3adef.png\" alt=\"Screen Shot 2021-01-12 at 1.07.45 PM\" data-base62-sha1=\"ssRk2IH4fTBNzGvi7VQNIuHVTL9\" width=\"357\" height=\"195\"><br>\nThis behavior also shows on <code>guild runs<\/code>:<\/p>\n<pre><code>[1:1609e5a4]   imputer   2021-01-12 13:14:13  error      none ckd\n[2:9ceadd83]   imputer   2021-01-12 13:14:12  running    none ckd\n[3:323f949e]   imputer+  2021-01-12 13:14:12  running    \n[4:b9c5ba05]   fo        2021-01-12 13:14:09  running \n<\/code><\/pre>\n<p>The pending run eventually ran and succeeded, and the error went away. But it\u2019s unclear why there was an error in the first place. I took a look at guild view and I see this, which looks strange:<\/p>\n<pre><code>ERROR: 128.97.25.29 - - [12\/Jan\/2021 05:11:03] code 400, message Bad request syntax ('\\t\\x00\\x00\\x00\\x01\\x00\\x00\\x00\u00ff\u00fe\u00ff\\x0fQ\\x00u\\x00a\\x00l\\x00y\\x00s\\x00P\\x00r\\x00o\\x00b\\x00e\\x00T\\x00e\\x00s\\x00t\\x00\\x02\\x00\\x00\\x00')\nERROR: 128.97.25.29 - - [12\/Jan\/2021 05:11:03] code 400, message Bad request syntax ('\u00ff\u00ff\u00ff\u00ff')\nERROR: 128.97.25.29 - - [12\/Jan\/2021 05:11:48] code 400, message Bad request syntax ('\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\nERROR: 128.97.25.29 - - [12\/Jan\/2021 05:12:33] code 400, message Bad request syntax ('gqw7')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:06:18] code 400, message Bad request syntax ('\\t\\x00\\x00\\x00\\x01\\x00\\x00\\x00\u00ff\u00fe\u00ff\\x0fQ\\x00u\\x00a\\x00l\\x00y\\x00s\\x00P\\x00r\\x00o\\x00b\\x00e\\x00T\\x00e\\x00s\\x00t\\x00\\x02\\x00\\x00\\x00')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:06:18] code 400, message Bad request syntax ('\u00ff\u00ff\u00ff\u00ff')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:07:03] code 400, message Bad request syntax ('\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\nERROR: 127.0.0.1 - - [12\/Jan\/2021 11:07:48] code 400, message Bad request syntax ('gqw7')\n<\/code><\/pre>\n<p>However, when I do a longer run or try to use queues, they never run. They just show as errors.<\/p>\n<p>Not sure how to debug.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Dependency mismatch between guild.ai and tensorflow",
        "Question_link":"https:\/\/my.guild.ai\/t\/dependency-mismatch-between-guild-ai-and-tensorflow\/512",
        "Question_created_time":"2021-01-08T05:34:54.576Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":687,
        "Question_body":"<p>I currently have tensorflow installed, When I try to install guild.ai There is a dependency version conflict and pipenv is not able to resolve it. The version conflicting dependency is tensorboard.<\/p>\n<pre><code>sarat@sarat-pc:~\/Codes\/guild_start $ pipenv install guildai\nInstalling guildai...\nAdding guildai to Pipfile's [packages]...\n\u2714 Installation Succeeded \nPipfile.lock (66d06e) out of date, updating to (085a73)...\nLocking [dev-packages] dependencies...\nLocking [packages] dependencies...\nBuilding requirements...\nResolving dependencies...\n\u2718 Locking Failed! \n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/resolver.py\", line 741, in _main\n[ResolutionFailure]:       resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages, dev)\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/resolver.py\", line 702, in resolve_packages\n[ResolutionFailure]:       results, resolver = resolve(\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/resolver.py\", line 684, in resolve\n[ResolutionFailure]:       return resolve_deps(\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/utils.py\", line 1395, in resolve_deps\n[ResolutionFailure]:       results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/utils.py\", line 1108, in actually_resolve_deps\n[ResolutionFailure]:       resolver.resolve()\n[ResolutionFailure]:   File \"\/home\/sarat\/.local\/lib\/python3.8\/site-packages\/pipenv\/utils.py\", line 833, in resolve\n[ResolutionFailure]:       raise ResolutionFailure(message=str(e))\n[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.\n  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.\n Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.\n  Hint: try $ pipenv lock --pre if it is a pre-release dependency.\nERROR: Could not find a version that matches tensorboard&lt;2.3.0,&gt;=2.0.0,~=2.4 (from tensorflow==2.4.0-&gt;-r \/tmp\/pipenvjhzpx270requirements\/pipenv-2yv9nm20-constraints.txt (line 2))\nTried: 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.12.0, 1.12.1, 1.12.2, 1.13.0, 1.13.1, 1.14.0, 1.15.0, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.4.0\nSkipped pre-versions: 1.6.0rc0\nThere are incompatible versions in the resolved dependencies:\n  tensorboard&lt;2.3.0,&gt;=2.0.0 (from guildai==0.7.1.post1-&gt;-r \/tmp\/pipenvjhzpx270requirements\/pipenv-2yv9nm20-constraints.txt (line 3))\n  tensorboard~=2.4 (from tensorflow==2.4.0-&gt;-r \/tmp\/pipenvjhzpx270requirements\/pipenv-2yv9nm20-constraints.txt (line 2))\n<\/code><\/pre>\n<p>So pipenv is not able to find compatible tensorflow and guild.ai versions.<\/p>\n<p>What version of tensorflow does guild.ai support?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Module 'yaml' has no attribute 'encode_val'",
        "Question_link":"https:\/\/my.guild.ai\/t\/module-yaml-has-no-attribute-encode-val\/503",
        "Question_created_time":"2020-12-16T18:46:56.901Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":453,
        "Question_body":"<p>Hi, I have just  installed guildai on a new server in a conda env (py 3.7) and get this error. I have tried reinstalling  PyYAML and pip.<br>\nLet me know if you have suggestions!<br>\nThanks, Shannon<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Debugging and profiling guild",
        "Question_link":"https:\/\/my.guild.ai\/t\/debugging-and-profiling-guild\/500",
        "Question_created_time":"2020-12-14T17:06:01.395Z",
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":584,
        "Question_body":"<p>I sometimes see <code>guild<\/code> taking a long time to start a run compared to just running the command that I get from <code>--print-cmd<\/code>. I realise this is because <code>guild<\/code> has to resolve dependencies etc., but I would like to understand if there is an easy way to debug and especially profile what steps \/ operations that is expensive in the <code>guild<\/code> command.<\/p>\n<p>I am aware of the <code>guild --debug<\/code> flag, but in my particular case it doesn\u2019t provide much info about what is taking a long time.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Rearranging columns in guild compare",
        "Question_link":"https:\/\/my.guild.ai\/t\/rearranging-columns-in-guild-compare\/496",
        "Question_created_time":"2020-12-08T19:28:31.413Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":270,
        "Question_body":"<p>Hi there!<\/p>\n<p>First off, thanks a lot for making Guild!<\/p>\n<p>I was wondering if there is a way to rearrange the columns in <code>guild compare<\/code>? It seems like by default the scalars come after the flags whereas I\u2019d like for it to be the other way round.<\/p>\n<p>Second, I\u2019m noticing that if I use <code>guild compare -cc<\/code> followed by a comma-separated list of column names which include both flags and scalars, only the scalars get outputted correctly; the values for the flag columns are empty.<\/p>\n<p>FWIW I\u2019m on Guild version 0.7.1:<\/p>\n<pre><code>% pip freeze | grep guildai                                                                                                                               20-12-08 - 14:26:45\nguildai==0.7.1\n<\/code><\/pre>\n<p>EDIT: here is a quick example to reproduce things.<\/p>\n<p>Here\u2019s my toy <code>guild.yml<\/code> file:<\/p>\n<pre><code>- operations:\n    quick_example:\n      description: \"Quick example\"\n      exec: \"echo \\\"Loss: 0.123\\\"\"\n      flags:\n        log_file:\n          type: string\n          default: \"example.log\"\n      output-scalars:\n        - loss: 'Loss: (\\value)'\n<\/code><\/pre>\n<p>If I run <code>guild run quick_example<\/code> and then <code>guild_compare<\/code>, I see<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8.png\" data-download-href=\"\/uploads\/short-url\/vezbGnqgINBlCyo0RrLJKtjFQLK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_517x42.png\" alt=\"image\" data-base62-sha1=\"vezbGnqgINBlCyo0RrLJKtjFQLK\" width=\"517\" height=\"42\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_517x42.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_775x63.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_1034x84.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1350\u00d7110 5.76 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg><\/div><\/a><\/div><\/p>\n<p>However, if I run <code>guild compare -cc log_file,loss<\/code> I get<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/30bf182e19fbc50a4b89c31a42ed85cc964b4039.png\" alt=\"image\" data-base62-sha1=\"6XekRJDBHwfTbaXShdnSESkYNMR\" width=\"377\" height=\"99\"><\/p>\n<p>Thanks a lot in advance!<br>\njks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Remove accidentally recorded flags from runs",
        "Question_link":"https:\/\/my.guild.ai\/t\/remove-accidentally-recorded-flags-from-runs\/470",
        "Question_created_time":"2020-11-30T14:10:26.615Z",
        "Question_answer_count":9,
        "Question_score_count":1,
        "Question_view_count":349,
        "Question_body":"<p>Hi the loss of every epoch has been recorded in my experiments. I dont want to delete the runs, but can these unwanted flags be removed from the overview shown in guild compare?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Status flag is \"terminated\" when experiment is still \"running\"",
        "Question_link":"https:\/\/my.guild.ai\/t\/status-flag-is-terminated-when-experiment-is-still-running\/461",
        "Question_created_time":"2020-11-25T10:30:46.699Z",
        "Question_answer_count":10,
        "Question_score_count":3,
        "Question_view_count":396,
        "Question_body":"<p>Hi,<\/p>\n<p>I currently run experiments. But their status flag says \u201cterminated\u201d instead of \u201crunning\u201d. This is quite annoying since now I can\u2019t delete all terminated runs without also deleting active runs.<\/p>\n<p>Excerpt of output when running <code>guild runs info<\/code>:<\/p>\n<pre><code>status: terminated\nstarted: 2020-11-25 05:59:38\nstopped:\n<\/code><\/pre>\n<p>Is it bug or did I cause this somehow?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Loading custom event-file summaires",
        "Question_link":"https:\/\/my.guild.ai\/t\/loading-custom-event-file-summaires\/489",
        "Question_created_time":"2020-12-02T22:48:33.280Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":255,
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using tensorboardX to store event files to disk into a folder that\u2019s different for every run.<br>\nHow do I configure guild where to find these event files?<\/p>\n<p>Thanks!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild.ipy incorrect home?",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-ipy-incorrect-home\/484",
        "Question_created_time":"2020-12-02T12:29:36.799Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":230,
        "Question_body":"<p>How does <code>guild.ipy<\/code> home get defined?<\/p>\n<p>When I run<\/p>\n<pre><code>import guild.ipy as guild\n\nguild.runs()\n<\/code><\/pre>\n<p>I get different results from the following (which feels like it should be default behavior)<\/p>\n<pre><code>import sys\nguild_home = '{}\/.guild'.format(sys.exec_prefix)\nguild.set_guild_home(guild_home)\n\nguild.runs()\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild check error",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-check-error\/466",
        "Question_created_time":"2020-11-27T09:53:17.572Z",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":353,
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>I am using guild after some time\u2026<\/p>\n<p>I have installed the guild using <code>pip install guild<\/code> and execute <code>guild check<\/code>. I got the following error:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\alphar&gt; guild check\nguild_version:             0.7.1.dev3\nguild_install_location:    c:\\programdata\\anaconda3\\lib\\site-packages\\guild\nguild_home:                C:\\ProgramData\\Anaconda3\\.guild\nguild_resource_cache:      C:\\ProgramData\\Anaconda3\\.guild\\cache\\resources\nTraceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    guild.main.main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    _main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 40, in _main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\click_util.py\", line 201, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check.py\", line 104, in check\n    check_impl.main(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 87, in main\n    _check(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 92, in _check\n    _check_impl(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 107, in _check_impl\n    _print_info(check)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 156, in _print_info\n    _print_guild_info()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 179, in _print_guild_info\n    cli.out(\"installed_plugins:         %s\" % _format_plugins())\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\check_impl.py\", line 191, in _format_plugins\n    return \", \".join([name for name, _ in sorted(plugin.iter_plugins())])\nTypeError: '&lt;' not supported between instances of 'CPUPlugin' and 'CPUPlugin'\n<\/code><\/pre>\n<p>I get the same error with <code>pip install --pre --user guildai<\/code>.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Run R script returns: guild: error running r: [WinError 193] %1 is not a valid Win32 application",
        "Question_link":"https:\/\/my.guild.ai\/t\/run-r-script-returns-guild-error-running-r-winerror-193-1-is-not-a-valid-win32-application\/467",
        "Question_created_time":"2020-11-27T11:43:08.142Z",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":647,
        "Question_body":"<p>I tried to run R script. I have guild.yml file inside my root directory (R project). Ia m not sure if guild can work for plain R scripts inside R project?<\/p>\n<p>Here is my guild.yml:<\/p>\n<pre><code>r:\n  description: Backtesting with BackCUSUM estimation of volatility structural breaks\n  # exec: Rscript .guild\/sourcecode\/train.r ${flag_args}\n  exec: C:\/Users\/Mislav\/Documents\/GitHub\/alphar\/R\/volatilityR.R ${flag_args}\n  flags:\n    contract: 'SPY5'\n    upsample: FALSE\n    std_window: 30\n    backcusum_rolling_window: 100\n    backcusum_type: 'bq'\n  output-scalars:\n    cumulatice_return: 'cumulative_return: (\\value)'\n    sharpe_ratio: 'sharpe_ratio: (\\value)'\n<\/code><\/pre>\n<p>When I execute the script with <code>guild run r<\/code> I get the error:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\alphar&gt; guild run r\nYou are about to run r\n  backcusum_rolling_window: 100\n  backcusum_type: bq\n  contract: SPY5\n  std_window: 30\n  upsample: no\nContinue? (Y\/n) y\nguild: error running r: [WinError 193] %1 is not a valid Win32 application\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tensorboard logging twice + is slow",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-logging-twice-is-slow\/423",
        "Question_created_time":"2020-10-20T05:55:53.724Z",
        "Question_answer_count":10,
        "Question_score_count":1,
        "Question_view_count":1497,
        "Question_body":"<p>I  have an operation, let\u2019s call it <code>a<\/code> that is kinda the \u201cbase operation\u201d that I could run a number ways with different flags.  Then I have another operation <code>b<\/code> that has one step  that runs operation <code>a<\/code> with the proper flags.<\/p>\n<p>What\u2019s strange to me is that I seem to get multiple of everything I logged with tensorboard. Additionally, <code>guild view<\/code>  is pretty  slow to open  my runs,  and viewing in tensorboard from  there is much slower (takes 10+s) . I saw someone had an issue with symlinks but I don\u2019t think that\u2019s the issue here since I don\u2019t have any set up. I feel like these issues are probably linked, considering I wasn\u2019t having this problem before. If I run just 1 configuration I ended up with:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png\" data-download-href=\"\/uploads\/short-url\/ppPT2noEZFMHbXQsJsQEYjoFJoF.png?dl=1\" title=\"Screen Shot 2020-10-19 at 10.35.07 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_690x162.png\" alt=\"Screen Shot 2020-10-19 at 10.35.07 PM\" data-base62-sha1=\"ppPT2noEZFMHbXQsJsQEYjoFJoF\" width=\"690\" height=\"162\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_690x162.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/b222065eb8fc000afea6c9a9eeae0cf0addca985_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2020-10-19 at 10.35.07 PM<\/span><span class=\"informations\">779\u00d7184 45.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg><\/div><\/a><\/div><\/p>\n<p>Not sure how to go about fixing this, I probably  did something wrong.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Exporting guild runs from remote sever",
        "Question_link":"https:\/\/my.guild.ai\/t\/exporting-guild-runs-from-remote-sever\/457",
        "Question_created_time":"2020-11-24T14:27:36.115Z",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":513,
        "Question_body":"<p>Hi Garret and co,<br>\nI have used guild on my last project and liked it! This time, I am running my scripts on different remote servers. I would like to import the runs to my desktop to view them all together. I am not sure how to do this. I would prefer a solution that doesnt involve the remote configuration, because I dont know much about it and my system requires often to enter passwords.<br>\nThanks for the help!<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Storing the guild artifacts on s3 using `guild run -r s3-dev`",
        "Question_link":"https:\/\/my.guild.ai\/t\/storing-the-guild-artifacts-on-s3-using-guild-run-r-s3-dev\/449",
        "Question_created_time":"2020-11-12T17:08:21.861Z",
        "Question_answer_count":5,
        "Question_score_count":4,
        "Question_view_count":569,
        "Question_body":"<p>I wanted to store all my runs in s3 bucket directly instead of the local guildai home directory. So,  I configured my <code>~\/.guild\/config.yml<\/code> in the following way which I thought will setup my s3 remote location and I can automatically save the sourcecode and artifacts that I save locally into s3 bucket.<\/p>\n<pre><code>remotes:\n  s3-dev:\n    type: s3\n    description: Production runs\n    bucket: cortex-model-data\n    region: eu-central-1\n<\/code><\/pre>\n<p>and I have my <code>guild.yml<\/code>as follows<\/p>\n<pre><code>- model: AlwaysPredictMean\n  description: A dummy model which always predicts mean\n  operations:\n    train:\n      description: Training Pipeline Sample Code\n      main: training\/train\n      flags-import: all\n      output-scalars: '(\\key): (\\value)'\n<\/code><\/pre>\n<p>when I run the script using <code>guild run --remote s3-dev<\/code> , I get the following error message<\/p>\n<pre><code>\u00b1 |feature\/guildai U:1 \u2717| \u2192 guild run -r s3-dev\nYou are about to run AlwaysPredictMean:train on s3-dev\n  comment: Description for a given training run\n  config: training\/config\/example.yml\n  data: tests\/test_df.csv\n  epochs: 10\n  model_class: training.example_model::AlwaysPredictMean\n  use_case: example_use_case\nContinue? (Y\/n) y\nguild: remote 's3-dev' does not support this operation\n<\/code><\/pre>\n<p>Can someone let me know what exactly is the problem and why can\u2019t I use guild.ai to store in the specified s3. Thanks<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"`guild export` and underlying shututil dead-slow for samba drives",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-export-and-underlying-shututil-dead-slow-for-samba-drives\/442",
        "Question_created_time":"2020-11-05T00:09:23.382Z",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":447,
        "Question_body":"<p>This is a very specific issue, but I wanted to raise it either way.<\/p>\n<p>I am using <code>guild export<\/code> to export runs to a Samba share mounted in Linux. This is incredibly slow. I see <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/9ddeb46a11e8c37a8ecb452b3b32f5c245297f2c\/guild\/commands\/runs_impl.py#L1193\" rel=\"noopener nofollow ugc\">here<\/a> that <code>guild<\/code> uses <code>shututil<\/code>. This is a known issue for <code>shututil<\/code> and they suggest a monkey patch in this issue <a href=\"https:\/\/github.com\/SickChill\/SickChill\/issues\/3292#issuecomment-289728670\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>I think I am going to experiment with ways to patch <code>guild<\/code>. Since we are using Azure, I want to see if I can utilize <code>azcopy<\/code>, which is orders of magnitude faster.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I test my Guild file?",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-do-i-test-my-guild-file\/434",
        "Question_created_time":"2020-10-27T18:15:45.054Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":473,
        "Question_body":"<p>I know guild has support for some testing and checks of the guild file setup. I just can\u2019t find the documentation for it.<\/p>\n<p>Ideally I would like to have some unit tests running that tests that the <code>guild.yml<\/code> is correctly setup and that some of the <code>operations<\/code> and <code>models<\/code> works as expected.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Source custom shell script when running in remote virtual environment",
        "Question_link":"https:\/\/my.guild.ai\/t\/source-custom-shell-script-when-running-in-remote-virtual-environment\/368",
        "Question_created_time":"2020-10-08T17:57:42.963Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":521,
        "Question_body":"<p>I am using a <code>ssh<\/code> remote with the following setup:<\/p>\n<pre><code>remotes:\n  azure-deeplearning:\n    type: ssh\n    host: deeplearning.guild.ai\n    venv-path: mypath\/.training_venv\n    user: myuser\n    use-prerelease: yes\n<\/code><\/pre>\n<p>The <code>venv<\/code> gets correctly activated. The issue is that I need to source a setup file after the <code>venv<\/code> gets activated. If I were to do it manually on the remote I would do:<\/p>\n<pre><code>source mypath\/.training-venv\/bin\/activate\nsource my_custom_bash.sh\n<\/code><\/pre>\n<p>How would this fit into the guild remote workflow?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild compare \/ view \/ tensorboard hangs",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-compare-view-tensorboard-hangs\/427",
        "Question_created_time":"2020-10-23T10:31:32.502Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":418,
        "Question_body":"<p>I have about 15 runs which where I perform about 60000 steps and log a loss for each step. When I try to view these using <code>guild runs<\/code> it works fine, but trying to extract the best run using compare or viewing the results using view \/ tensorboard it loads for a long time until I can actually view the information.<\/p>\n<p>What could be the reason for this? Am I logging too much per run?<\/p>\n<p>EDIT: I realised that <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a> has said elsewhere that the view is due for an overhaul, perhaps that will fix this problem.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Custom post-hoc analysis",
        "Question_link":"https:\/\/my.guild.ai\/t\/custom-post-hoc-analysis\/425",
        "Question_created_time":"2020-10-20T20:41:52.765Z",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":287,
        "Question_body":"<p>I would like to do some custom post-hoc analysis on various things I\u2019ve logged into tensorboard. I\u2019m doing bootstrapping and want to calculate mean and confidence interval. I know there\u2019s the <code>ipy<\/code> widget, and I\u2019ve gotten semi far with that as it gives you <code>avg_value<\/code> etc. But I\u2019d like to get access to all the values logged for, say, <code>precision<\/code> so that I can calculate the confidence intervals. I know this data is logged in tensorboard and tensorflow has some guides but they do not work with the version of tensorboard that comes  with guild. How do you advise I would do something like this?<\/p>\n<p>I  can log this separately, sure but it ends up being a pain and cluttering up all my logs. I cannot visualize them the way I\u2019d like in tensorboard, so I\u2019d rather do this analysis ad-hoc when comparing runs myself.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data Filepath Flag",
        "Question_link":"https:\/\/my.guild.ai\/t\/data-filepath-flag\/376",
        "Question_created_time":"2020-10-15T20:50:08.595Z",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":360,
        "Question_body":"<p>Hello, I\u2019m converting a project to guild and I have a question about setting a filepath as a flag.<\/p>\n<p>At the top of my training script I have a variable data_fp = \u201c\u2026\/\u2026\/data\/dtype2\/processed_data.npy\u201d<br>\nbefore this was a guild project I was just cutting and pasting different filepaths when I want to train the model on new data but now I want to be able to set it as a flag.<\/p>\n<p>When I try to run this with guild I get an error because it doesn\u2019t see that path. Is there a way to do set this up so that the script can see the data from the \/run directory?<\/p>\n<p>Also, currently my folder structure is like this and my guild file is :<\/p>\n<pre><code>proj\/\n  data\/\n    dtype1\/\n      raw_data.npy\n    dtype2\/\n      processed_data.npy\n  scripts\/\n    guild.yml\n    data_processing\/\n      process_data.py\n    model\/\n      train.py\n      ...\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Specifying guild home through -H doesn't work",
        "Question_link":"https:\/\/my.guild.ai\/t\/specifying-guild-home-through-h-doesnt-work\/369",
        "Question_created_time":"2020-10-09T20:41:35.982Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":472,
        "Question_body":"<p>I want to compare runs in a different path by using the <code>-H<\/code> option. My new path is called <code>experiments<\/code>:<\/p>\n<pre><code>$ ls experiments\na26ffe4727584d359e16f2a28af5dfab  cache\n<\/code><\/pre>\n<p>Now If I do:<\/p>\n<pre><code>$ guild -H experiments runs\n<\/code><\/pre>\n<p>There is no output. Same with other commands. Am I using the command run?<\/p>\n<p>This is the output of <code>guild check<\/code>:<\/p>\n<pre><code>guild_version:             0.7.0\nguild_install_location:    \/mypath\/.training_venv\/lib\/python3.6\/site-packages\/guild\nguild_home:                \/mypath\/.training_venv\/.guild\nguild_resource_cache:      \/mypath\/.training_venv\/.guild\/cache\/resources\ninstalled_plugins:         cpu, disk, exec_script, gpu, keras, memory, perf, python_script, queue, skopt\npython_version:            3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0]\npython_exe:                \/mypath\/.training_venv\/bin\/python3\nplatform:                  Linux 5.4.0-48-generic x86_64\npsutil_version:            5.6.3\ntensorboard_version:       2.2.2\ncuda_version:              10.0.130\nnvidia_smi_version:        440.100\nlatest_guild_version:      0.7.0.post1\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild run hangs \/ very slow",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362",
        "Question_created_time":"2020-10-03T00:33:41.068Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":445,
        "Question_body":"<p>I have successfully used guild in some older project, but in this new project I am having a hard time debugging what is going on.<\/p>\n<p>I am trying to do:<\/p>\n<pre><code>guild run model:train -y\n<\/code><\/pre>\n<p>But the operation hangs without any output to the console.<\/p>\n<p>If I do<\/p>\n<pre><code>guild run model:train -y --print-cmd\n<\/code><\/pre>\n<p>And execute it directly with python and exact same arguments it gets executed right away.<\/p>\n<p>How do I debug this behavior?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Where to look for error logs",
        "Question_link":"https:\/\/my.guild.ai\/t\/where-to-look-for-error-logs\/357",
        "Question_created_time":"2020-09-29T03:38:16.211Z",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":1198,
        "Question_body":"<p>After defining a guild operation, when I try running, I am getting the following error<br>\n<code>ERROR: [guild] trial &lt;RUNHASH&gt; exited with an error (see log for details)<\/code>.<\/p>\n<p>Presumably something went wrong in the run but I have no idea what. And I am not sure where to look for the log. If I open <code>guild view<\/code> and look for the log there, there is nothing there. Which might be expected becasue that\u2019s supposed to be the log of that the program outputs on the terminal. But then, I am still not sure where to look for the actual error log that the error message is talking about.<\/p>\n<p>How I can troubleshoot this?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error with gp: TypeError: '<' not supported between instances of 'Version' and 'tuple'",
        "Question_link":"https:\/\/my.guild.ai\/t\/error-with-gp-typeerror-not-supported-between-instances-of-version-and-tuple\/352",
        "Question_created_time":"2020-09-24T07:21:27.603Z",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":1477,
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>When I run the execute run as grid serach it works as expected (there are lots of flags):<\/p>\n<pre><code>guild run --max-trials 2 lightgbm:train num_leaves=range[40:150:10] max_depth=range[2:7:1] learning_rate=range[0.01:0.1:0.01] boosting_type=[gbdt] colsample_bytree=[0.75,0.8,0.85,0.9,0.95] subsample=[0.75,0.8,0.85,0.9,0.95] min_child_samples=[0.5,1.0,2.0,3.0,4.0,5.0,10.0] n_estimators=[100,250,500,1000]\n<\/code><\/pre>\n<p>But when I specify the gp optimizer:<\/p>\n<pre><code>guild run --max-trials 2 lightgbm:train num_leaves=range[40:150:10] max_depth=range[2:7:1] learning_rate=range[0.01:0.1:0.01] boosting_type=[gbdt] colsample_bytree=[0.75,0.8,0.85,0.9,0.95] subsample=[0.75,0.8,0.85,0.9,0.95] min_child_samples=[0.5,1.0,2.0,3.0,4.0,5.0,10.0] n_estimators=[100,250,500,1000] --maximize mean_score --optimizer gp\n<\/code><\/pre>\n<p>I get an error:<\/p>\n<pre><code>Continue? (Y\/n)\nTraceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_gp_main.py\", line 83, in &lt;module&gt;\n    main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_gp_main.py\", line 36, in main\n    skopt_util.handle_seq_trials(batch_run, _suggest_x)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 209, in handle_seq_trials\n    _run_seq_trials(batch_run, suggest_x_cb)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 225, in _run_seq_trials\n    for trial_flag_vals, is_trial_random_start, prev_trials, x0 in _iter_seq_trials(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 263, in _iter_seq_trials\n    suggested_x, random_state = _suggest_x(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_util.py\", line 370, in _suggest_x\n    return suggest_x_cb(dims, x0, y0, is_random_start, random_state, suggest_opts)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\skopt_gp_main.py\", line 40, in _suggest_x\n    res = skopt.gp_minimize(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\gp.py\", line 264, in gp_minimize\n    return base_minimize(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\base.py\", line 271, in base_minimize\n    next_x = optimizer.ask()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\", line 332, in ask\n    return self._ask()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\", line 398, in _ask\n    return self.space.rvs(random_state=self.rng)[0]\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\skopt\\space\\space.py\", line 764, in rvs\n    if sp_version &lt; (0, 16):\nTypeError: '&lt;' not supported between instances of 'Version' and 'tuple'\n<\/code><\/pre>\n<p>Here is the train script: <a href=\"https:\/\/github.com\/MislavSag\/trademl\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/MislavSag\/trademl<\/a><br>\nThe operation I run is the <code>- model: lightgbm<\/code><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tensorboard version conflict",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-version-conflict\/353",
        "Question_created_time":"2020-09-24T09:55:41.725Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":1087,
        "Question_body":"<p>Guildai tensorboard functionallity don\u0107t work for me for some time. Now I would like to make it work.<\/p>\n<p>If I execute command <code>guild tensorboard --started 'last 1 hour'<\/code>  I get:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\trademl&gt; guild tensorboard --started 'last 1 hour'\nPreparing runs for TensorBoard\n2020-09-24 11:32:59.909813: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\nTraceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    guild.main.main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    _main()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\main.py\", line 40, in _main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\external\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\click_util.py\", line 201, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\tensorboard.py\", line 108, in tensorboard\n    tensorboard_impl.main(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 46, in main\n    _run_tensorboard(args)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\commands\\tensorboard_impl.py\", line 98, in _run_tensorboard\n    tensorboard.serve_forever(\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\tensorboard.py\", line 607, in serve_forever\n    app = create_app(logdir, reload_interval, tensorboard_options=tensorboard_options)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\tensorboard.py\", line 508, in create_app\n    plugins = _tensorboard_plugins(disabled_plugins)\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\tensorboard.py\", line 522, in _tensorboard_plugins\n    base_plugins = tensorboard.base_plugins()\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\guild\\plugins\\tensorboard.py\", line 232, in base_plugins\n    return list(set(default.get_plugins() + default.get_dynamic_plugins()))\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\default.py\", line 122, in get_dynamic_plugins\n    return [\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\default.py\", line 123, in &lt;listcomp&gt;\n    entry_point.load()\n  File \"C:\\Users\\Mislav\\AppData\\Roaming\\Python\\Python38\\site-packages\\pkg_resources\\__init__.py\", line 2471, in load\n    self.require(*args, **kwargs)\n  File \"C:\\Users\\Mislav\\AppData\\Roaming\\Python\\Python38\\site-packages\\pkg_resources\\__init__.py\", line 2494, in require\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n  File \"C:\\Users\\Mislav\\AppData\\Roaming\\Python\\Python38\\site-packages\\pkg_resources\\__init__.py\", line 790, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.VersionConflict: (requests 2.18.4 (c:\\users\\mislav\\appdata\\roaming\\python\\python38\\site-packages), Requirement.parse('requests&lt;3,&gt;=2.21.0'))\n<\/code><\/pre>\n<p><code>Conda list requests<\/code> gives:<\/p>\n<pre><code>(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\trademl&gt; conda list requests\n# packages in environment at C:\\ProgramData\\Anaconda3:\n#\n# Name                    Version                   Build  Channel\nrequests                  2.24.0             pyh9f0ad1d_0    conda-forge\nrequests-oauthlib         1.3.0                    pypi_0    pypi\n<\/code><\/pre>\n<p>so it is greater than 2.21 and lower than 3?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Getting full run id",
        "Question_link":"https:\/\/my.guild.ai\/t\/getting-full-run-id\/343",
        "Question_created_time":"2020-09-14T14:26:21.539Z",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":698,
        "Question_body":"<p>Is there a way to print out the full run id when using guild compare?<\/p>\n<p>I wanted to run through the results and then inspect the folder containing the output of the run in .guild, but the output of guild compare is the shortened run id so I can\u2019t directly find the folder.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Scalar not saved if pipeline is used",
        "Question_link":"https:\/\/my.guild.ai\/t\/scalar-not-saved-if-pipeline-is-used\/339",
        "Question_created_time":"2020-09-10T11:11:53.219Z",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":399,
        "Question_body":"<p>Here is my guild file:<\/p>\n<aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/guild.yml\" target=\"_blank\" rel=\"nofollow noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/guild.yml\" target=\"_blank\" rel=\"nofollow noopener\">MislavSag\/trademl\/blob\/master\/guild.yml<\/a><\/h4>\n<pre><code class=\"lang-yml\">- config: model-base\n  resources:\n    prepared-data:\n      - operation: prepare-data\n\n- operations:\n    prepare-data:\n      main: trademl.modeling.prepare\n      flags-import: all\n      flags:\n        input_path:\n          description: Path to read data from. \n          arg_name: input_path\n          type: string\n          default: D:\/market_data\/usa\/ohlcv_features\n        output_path:\n          description: Main path where to save output\n          arg_name: output_path\n          type: string\n          default: D:\/algo_trading_files\n<\/code><\/pre>\n\n  This file has been truncated. <a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/guild.yml\" target=\"_blank\" rel=\"nofollow noopener\">show original<\/a>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>If I run prepare or random-forest operation it saves the scalars.<\/p>\n<p>But if I run the pipeline <code>pipeline-rf-opt<\/code> that includes prepare and random-forst as step, it doesn\u2019t save scalars. I call it like this:<\/p>\n<pre><code class=\"lang-command\">guild run pipeline-rf-opt \\\n  data-include_ta=1 \\\n  data-label_tuning=0 \\\n  data-label=[day_5] \\\n  data-pca=0 \\\n  data-tb_volatility_lookback=[50] \\\n  data-tb_volatility_scaler=1.0 \\\n  data-correlation_threshold=0.95 \\\n  data-scaling='none' \\\n  random-input_data_path='D:\/algo_trading_files' \\\n  random-forest-depth=4 \\\n  random-forest-maxf=10 \\\n  random-n_estimators=350 \\\n  random-min_weight_fraction_leaf=0.1\n<\/code><\/pre>\n<p>It is just one run.<\/p>\n<p>What could be the reason it doesn\u2019t save scalars?<\/p>\n<p>But it saves flags.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to view logs from trials\/batches",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-view-logs-from-trials-batches\/334",
        "Question_created_time":"2020-09-08T13:27:15.016Z",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":302,
        "Question_body":"<p>I usually use <code>guild cat --output 89549e30 | less +G<\/code> to view the stdout logs, but it seems like there is a separate logging channel for trial-level operations. How would one access these logs?<\/p>\n<p><code>ERROR: [guild] trial efb7377f334b4b5da8411e2437771e91 exited with an error (see log for details)<\/code><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Installation problems",
        "Question_link":"https:\/\/my.guild.ai\/t\/installation-problems\/295",
        "Question_created_time":"2020-08-21T08:52:17.041Z",
        "Question_answer_count":16,
        "Question_score_count":1,
        "Question_view_count":555,
        "Question_body":"<p>I have reopened the issue on GitHub. I am not sure if you see it, so I just wanted to forward the link here\u010c<br>\n<aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/133\" target=\"_blank\" rel=\"nofollow noopener\">github.com\/guildai\/guildai<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/133\" target=\"_blank\" rel=\"nofollow noopener\"> pip install guildai on macos x fails<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-03-01\" data-time=\"14:37:48\" data-timezone=\"UTC\">02:37PM - 01 Mar 20 UTC<\/span>\n      <\/div>\n\n        <div class=\"date\">\n          closed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-03-01\" data-time=\"15:15:51\" data-timezone=\"UTC\">03:15PM - 01 Mar 20 UTC<\/span>\n        <\/div>\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/mdiephuis\" target=\"_blank\" rel=\"nofollow noopener\">\n          <img alt=\"mdiephuis\" src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/ebde3a729b99195a01fadd847aa66b1763a50c8b.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          mdiephuis\n        <\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n<div class=\"github-row\">\n  <p class=\"github-content\">Runnning\npip install guildai\nERROR: Could not find a version that satisfies the requirement guildai (from versions: none)\nERROR: No matching distribution found for...<\/p>\n<\/div>\n\n<div class=\"labels\">\n<\/div>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<\/p>\n<p>For some reason, I can\u0107t install guildai on windows, after reinstalling anaconda, but I was able to install it before.<\/p>\n<p>Before update, I was able to install it.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Passing arguments, defined in guild.yml file when using Python's argparse",
        "Question_link":"https:\/\/my.guild.ai\/t\/passing-arguments-defined-in-guild-yml-file-when-using-pythons-argparse\/329",
        "Question_created_time":"2020-08-28T08:10:29.021Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":956,
        "Question_body":"<p>Guild version 0.7.0<\/p>\n<p>I noticed that flags, defined in guild.yml are not passed normally as arguments to config.yml if I use argparse in my Python code. It seems it wants to pass them python arguments even though flags-dest and flags-import are off.<\/p>\n<p>My workflow usually consists of defining main hyperparameters in the config.yml file, but for choosing which gpu to run it on (in my testing phase) I use argparse. Is there a way to combine those two?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I use variadic args?",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-do-i-use-variadic-args\/293",
        "Question_created_time":"2020-08-18T11:17:52.435Z",
        "Question_answer_count":3,
        "Question_score_count":3,
        "Question_view_count":687,
        "Question_body":"<p>I\u2019m using a Python argument parser that would like to take multiple directories as input, so I have:<\/p>\n<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--model_dirs\", action=\"store\", type=str, help=\"Path to trained models.\", nargs=\"+\", required=True\n)\n<\/code><\/pre>\n<p>It appears this is supported because I see some usage of it in an example: <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/2cc91df428d53c403962f300d3173f319dae782d\/examples\/detectron2\/demo.py#L47\" rel=\"nofollow noopener\">https:\/\/github.com\/guildai\/guildai\/blob\/2cc91df428d53c403962f300d3173f319dae782d\/examples\/detectron2\/demo.py#L47<\/a><\/p>\n<p>How do I pass multiple values in the CLI so that Guild will separate them?<\/p>\n<p><code>guild run ... model_dirs=\"foo bar\" -&gt; args.model_dirs = [\"foo bar\"]<\/code><br>\n<code>guild run ... model_dirs=foo bar -&gt; errors<\/code><br>\n<code>guild run ... \"model_dirs=foo bar\" -&gt; args.model_dirs = [\"foo bar\"]<\/code><\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to set PYTHONPATH and possible effects?",
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-set-pythonpath-and-possible-effects\/287",
        "Question_created_time":"2020-08-09T22:21:34.991Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":628,
        "Question_body":"<p>I\u2019m using Blender in my project which has it\u2019s own python implementation separate to that of my environment.  When I run guild with my project it writes to PYTHONPATH which interferes with sys.path  and causes problems with the blender installation of python.  Is there any easy way to set the PYTHONPATH in my guild files to overcome this issue?  Also are there any problems that might occur from setting PYTHONPATH while guild is running? Thanks.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"NameError: name 'argparse' is not defined",
        "Question_link":"https:\/\/my.guild.ai\/t\/nameerror-name-argparse-is-not-defined\/256",
        "Question_created_time":"2020-07-24T17:49:39.502Z",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":3760,
        "Question_body":"<p>Hi,<br>\nStarting from a fresh environment with python 3.7.3 and doing a <code>pip install guildai<\/code>, when trying to run<br>\n<code>guild tensorboard<\/code><br>\nI get this error:<br>\nFile \u201c\/Users\/louis-emmanuelmartinet\/.pyenv\/versions\/3.7.3\/envs\/ds-gathering\/lib\/python3.7\/site-packages\/tensorboard_plugin_wit\/wit_plugin_loader.py\u201d, line 73, in define_flags<br>\nexcept argparse.ArgumentError:<br>\nNameError: name \u2018argparse\u2019 is not defined<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"NameError: guild doesnt recognize defined variable?",
        "Question_link":"https:\/\/my.guild.ai\/t\/nameerror-guild-doesnt-recognize-defined-variable\/245",
        "Question_created_time":"2020-07-21T20:09:05.946Z",
        "Question_answer_count":9,
        "Question_score_count":2,
        "Question_view_count":1196,
        "Question_body":"<p>I tried to train a random forest using guildai. Here is my script:<br>\n<\/p><aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/trademl\/modeling\/train_rf.py\" target=\"_blank\" rel=\"nofollow noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/trademl\/modeling\/train_rf.py\" target=\"_blank\" rel=\"nofollow noopener\">MislavSag\/trademl\/blob\/master\/trademl\/modeling\/train_rf.py<\/a><\/h4>\n<pre><code class=\"lang-py\"># fundamental modules\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport sys\nimport os\nfrom pathlib import Path\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom mlfinlab.ensemble import SequentiallyBootstrappedBaggingClassifier\nfrom sklearn.base import clone\nimport xgboost\nimport shap\nimport mlfinlab as ml\nimport trademl as tml\n\n<\/code><\/pre>\n\n  This file has been truncated. <a href=\"https:\/\/github.com\/MislavSag\/trademl\/blob\/master\/trademl\/modeling\/train_rf.py\" target=\"_blank\" rel=\"nofollow noopener\">show original<\/a>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>It works as expected if I jsut execute the script loccaly. But if I run it through guild:<\/p>\n<pre><code>`guild run --yes --max-trials 2 random_forest:train num_threads=4 tb_volatility_scaler=[1,1.5,2] ts_look_forward_window=[1200,2400] sample_weights_type=[returns,time_decay,trend_scanning] max_depth=range[2:6:1] max_features=range[5:100:5] n_estimators=range[50:1000:50] min_weight_fraction_leaf=range[0.01:0.1:0.01] class_weight=[balanced_subsample,balanced]`\nit returns an error:\n(base) PS C:\\Users\\Mislav\\Documents\\GitHub\\trademl&gt; guild run --yes --max-trials 2 random_forest:train num_threads=4 tb_volatility_scaler=[1,1.5,2] ts_look_forward_window=[1200,2400] sample_weights_type=[returns,time_decay,trend_scanning] max_depth=range[2:6:1] max_features=range[5:100:5] n_estimators=range[50:1000:50] min_weight_fraction_leaf=range[0.01:0.1:0.01] class_weight=[balanced_subsample,balanced]\ne[33mWARNING: Could not parse requirement: -umpye[0m\ne[33mWARNING: Could not parse requirement: -illowe[0m\ne[33mWARNING: Could not parse requirement: -umpye[0m\ne[33mWARNING: Could not parse requirement: -illowe[0m\ne[33mWARNING: Skipping potential source code file C:\\Users\\Mislav\\Documents\\GitHub\\trademl\\data\\sampe_Data.csv because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.e[0m\ne[33mWARNING: Skipping potential source code file C:\\Users\\Mislav\\Documents\\GitHub\\trademl\\trademl\\modeling\\random_forest\\X_TEST.csv because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.e[0m\ne[33mWARNING: Skipping potential source code file C:\\Users\\Mislav\\Documents\\GitHub\\trademl\\trademl\\modeling\\random_forest\\nn_sample.csv because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.e[0m\ne[33mWARNING: Skipping potential source code file C:\\Users\\Mislav\\Documents\\GitHub\\trademl\\trademl\\modeling\\random_forest\\rf_model.json because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.e[0m\ne[33mWARNING: Skipping potential source code file C:\\Users\\Mislav\\Documents\\GitHub\\trademl\\trademl\\modeling\\random_forest\\rf_model_25.json because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.e[0m\ne[33mWARNING: Skipping potential source code file C:\\Users\\Mislav\\Documents\\GitHub\\trademl\\trademl\\modeling\\random_forest\\rf_model_25_ts.json because it's too big. To control which files are copied, define 'sourcecode' for the operation in a Guild file.e[0m\nINFO: [guild] Running trial c8f45a92942845fa9ef7fc8d5d8afd73: random_forest:train (class_weight=balanced, cv_number=4, labeling_technique=triple_barrier, max_depth=2, max_features=70, min_weight_fraction_leaf=0.02, n_estimators=950, num_threads=4, sample_weights_type=trend_scanning, structural_break_regime=all, tb_triplebar_min_ret=0.004, tb_triplebar_num_days=10, tb_volatility_lookback=50, tb_volatility_scaler=2, ts_look_forward_window=2400)\nINFO: [numexpr.utils] Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\nINFO: [numexpr.utils] NumExpr defaulting to 8 threads.\n2020-07-21 21:58:55.448030 100.0% apply_pt_sl_on_t1 done after 0.19 minutes. Remaining 0.0 minutes.\ndropped label:  0 0.00015123254524373645\n'fit' took 53.25 seconds to run.\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\.guild\\runs\\c8f45a92942845fa9ef7fc8d5d8afd73\\.guild\\sourcecode\\trademl\\modeling\\train_rf.py\", line 202, in &lt;module&gt;\n    sample_weight_train=sample_weigths,\nNameError: name 'sample_weigths' is not defined\n<\/code><\/pre>\n<p>It says sample_weight is not defined, but it is defined in the script and it works if I just execute it inside my IDE.<\/p>\n<p>OS: WIndows<br>\nIDE: VSCode<br>\nguild version: 0.7.0.rc11<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Random search unexpectedly failling with TypeError",
        "Question_link":"https:\/\/my.guild.ai\/t\/random-search-unexpectedly-failling-with-typeerror\/240",
        "Question_created_time":"2020-07-17T20:56:52.387Z",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":475,
        "Question_body":"<p>I was unable to replicate this in a minimalistic example so I didn\u2019t create an issue. I\u2019m guessing my guild.yml file is very incorrect or something. Might also be some package versions that are incorrect in this venv?<\/p>\n<p>Command:<\/p>\n<pre><code>guild run train batch_size=[16,32] --optimizer random\n<\/code><\/pre>\n<p>guild.yml snippet:<\/p>\n<pre><code>    train:\n      main: train\n      flags-import: all\n      output-scalars: off\n      requires:\n        - database\n<\/code><\/pre>\n<p>train.py snippet:<\/p>\n<pre><code>parser.add_argument('--batch_size', type=int, default=256)\n<\/code><\/pre>\n<p>Stacktrace:<\/p>\n<pre><code>Traceback (most recent call last):\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 192, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/guild\/plugins\/random_main.py\", li\nne 23, in &lt;module&gt;\n    from . import skopt_util\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/guild\/plugins\/skopt_util.py\", line 27, in &lt;module&gt;\n    import skopt\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/skopt\/__init__.py\", line 44, in &lt;module&gt;\n    from . import callbacks\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/skopt\/callbacks.py\", line 17, in &lt;module&gt;\n    from skopt.utils import dump\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/skopt\/utils.py\", line 3, in &lt;module&gt;\n    from sklearn.utils import check_random_state\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/__init__.py\", line 64, in &lt;module&gt;\n    from .base import clone\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/base.py\", line 14, in &lt;module&gt;\n    from .utils.fixes import signature\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/utils\/__init__.py\", line 14, in &lt;module&gt;\n    from . import _joblib\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/utils\/_joblib.py\", line 22, in &lt;module&gt;\n    from ..externals import joblib\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/__init__.py\", line 119, in &lt;module&gt;\n    from .parallel import Parallel\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/parallel.py\", line 28, in &lt;module&gt;\n    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/_parallel_backends.py\", line 22, in &lt;module&gt;\n    from .executor import get_memmapping_executor\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/executor.py\", line 14, in &lt;module&gt;\n    from .externals.loky.reusable_executor import get_reusable_executor\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/loky\/__init__.py\", line 12, in &lt;module&gt;\n    from .backend.reduction import set_loky_pickler\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/loky\/backend\/reduction.py\", line 125, in &lt;module&gt;\n    from sklearn.externals.joblib.externals import cloudpickle  # noqa: F401\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/cloudpickle\/__init__.py\", line 3, in &lt;module&gt;\n    from .cloudpickle import *\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/cloudpickle\/cloudpickle.py\", line 152, in &lt;module&gt;\n    _cell_set_template_code = _make_cell_set_template_code()\n  File \"\/home\/richard\/Documents\/league\/venv\/lib\/python3.8\/site-packages\/sklearn\/externals\/joblib\/externals\/cloudpickle\/cloudpickle.py\", line 133, in _make_cell_set_template_code\n    return types.CodeType(\nTypeError: an integer is required (got type bytes)\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Timeout error",
        "Question_link":"https:\/\/my.guild.ai\/t\/timeout-error\/196",
        "Question_created_time":"2020-06-20T12:37:23.969Z",
        "Question_answer_count":9,
        "Question_score_count":1,
        "Question_view_count":796,
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>I can\u2019t figure out why sometimes I get timeout error when I run experiments. Here is my running command:<\/p>\n<pre><code>guild run train tb_volatility_lookback=range[30:300:10]\n<\/code><\/pre>\n<p>it hangs after some time and when I click ctr + c to abort it the below error shows up.<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\runpy.py\u201d, line 193, in _run_module_as_main<br>\nINFO: [numexpr.utils] NumExpr defaulting to 8 threads.<br>\n\u201c<strong>main<\/strong>\u201d, mod_spec)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\runpy.py\u201d, line 85, in _run_code<br>\nexec(code, run_globals)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_main.py\u201d, line 38, in <br>\nmain()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_main.py\u201d, line 26, in main<br>\nbatch_util.handle_trials(batch_run, trials)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 54, in handle_trials<br>\n_run_trials(batch_run, trials)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 79, in _run_trials<br>\n_start_trial_run(run, stage)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 117, in _start_trial_run<br>\nrun_impl.run(restart=run.id, stage=stage)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1940, in run<br>\nmain(args)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1017, in main<br>\n_dispatch_op(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1101, in _dispatch_op<br>\n_dispatch_op_cmd(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1286, in _dispatch_op_cmd<br>\n_confirm_and_run(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1354, in _confirm_and_run<br>\n_run(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1544, in _run<br>\n_run_local(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1575, in _run_local<br>\n_run_op(op, S.args)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1683, in _run_op<br>\nextra_env=extra_env,<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 160, in run<br>\nexit_status = _run(run, op, quiet, stop_after, extra_env)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 195, in _run<br>\nexit_status = _op_wait_for_proc(op, proc, run, quiet, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 230, in _op_wait_for_proc<br>\nreturn _op_watch_proc(op, proc, run, quiet, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 238, in _op_watch_proc<br>\nreturn _proc_wait(proc, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 259, in <strong>exit<\/strong><br>\nself._output.wait_and_close()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 254, in wait_and_close<br>\nself.close()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 219, in close<br>\nlock = self._acquire_output_lock()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 232, in _acquire_output_lock<br>\nraise RuntimeError(\u201ctimeout\u201d)<br>\nRuntimeError: timeout<\/p>\n<\/blockquote>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Hyperopt TPE vs Skopt Gaussian Processes",
        "Question_link":"https:\/\/my.guild.ai\/t\/hyperopt-tpe-vs-skopt-gaussian-processes\/226",
        "Question_created_time":"2020-07-11T00:05:43.757Z",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":1124,
        "Question_body":"<p>In developing the <a href=\"\/examples\/hyperopt\">Hyperopt example<\/a> I wanted to compare its performance to Scikit Optimize \u2014 specifically the gp optimizer.<\/p>\n<p>I ran 50 trials for each optimizer, minimizing loss for the <a href=\"\/start\">Get Started<\/a> mock training script.<\/p>\n<p>Scatterplot results for <code>tpe<\/code>:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/8ecf41f9c817b7f95164b7b316e22df93557e59f.png\" alt=\"Screenshot from 2020-07-10 18-54-25\" data-base62-sha1=\"knlQBjfmWovvTNYOIf57tOMzfRB\" width=\"617\" height=\"483\"><\/p>\n<p>Scatterplot results for <code>gp<\/code>:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/d53d68bff805218b9b3d82c06020af617d5b30f8.png\" alt=\"Screenshot from 2020-07-10 18-53-32\" data-base62-sha1=\"uqpgIYD2oMdbIxugfwf1HAXQWdq\" width=\"627\" height=\"475\"><\/p>\n<p>Notice the concentration of trials around the minimum for <code>gp<\/code>. I would have expected this for <code>tpe<\/code>. I wonder if the example is implemented incorrectly.<\/p>\n<h3>Steps to reproduce<\/h3>\n<p>From the <a href=\"https:\/\/github.com\/guildai\/guildai\/tree\/master\/examples\/hyperopt\">example dir<\/a> generate 50 trials using <code>tpe<\/code>:<\/p>\n<pre><code class=\"lang-command\">guild run train -o tpe -m50 x=[-2.0:2.0] -t tpe\n<\/code><\/pre>\n<p>Next generate 50 trials using <code>gp<\/code>:<\/p>\n<pre><code class=\"lang-command\">guild run train -o gp -m50 x=[-2.0:2.0] -t gp\n<\/code><\/pre>\n<p>View the tpe trials in TensorBoard:<\/p>\n<pre><code class=\"lang-command\">guild tensorboard -l tpe\n<\/code><\/pre>\n<p>Click <strong>HPARAMS<\/strong> and then the scatterplot tab. Deselect all flags and metrics except x and loss.<\/p>\n<p>Do the same for the gp trials:<\/p>\n<pre><code class=\"lang-command\">guild tensorboard -l gp\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Issues with Guild file - output-scalars and sourcecode",
        "Question_link":"https:\/\/my.guild.ai\/t\/issues-with-guild-file-output-scalars-and-sourcecode\/218",
        "Question_created_time":"2020-07-04T20:49:51.379Z",
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":614,
        "Question_body":"<p>This looks like it would solve the issue but I\u2019m getting new issues with my guild file.<\/p>\n<p>It\u2019s complaining about <code>output-scalars: off<\/code><\/p>\n<pre><code>  operations:\n    search_lr:\n      main: search_lr\n      flags-import: all\n      output-scalars: off\n      requires:\n        - database\n    train:\n<\/code><\/pre>\n<pre><code>ERROR: error loading guildfile from .: error in \/home\/richard\/Documents\/league\/guild.yml: invalid value for output-scalars: False\n<\/code><\/pre>\n<p>I checked here: <a href=\"https:\/\/my.guild.ai\/t\/scalars\/160\" class=\"inline-onebox\">Scalars<\/a> and it seems like I\u2019m doing it correctly?<\/p>\n<p>If I remove that then it starts complaining about my sourcecode:<\/p>\n<pre><code>  sourcecode:\n    - '*.py'\n    - '*.json'\n    - guild.yml\n    - exclude:\n        file:\n          - riot_api_key.py\n        dir:\n          - tb\n          - checkpoints\n          - matches\n          - match_histories\n<\/code><\/pre>\n<pre><code>ERROR: error loading guildfile from .: error in \/home\/richard\/Documents\/league\/guild.yml: invalid exclude value: {'file': ['riot_api_key.py'], 'dir': ['tb', 'checkpoints', 'matches', 'match_histories']}\n<\/code><\/pre>\n<p>The list probably continues after fixing these issues so I think it\u2019s better to wait with this.<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tensorboard taking long to startup",
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-taking-long-to-startup\/212",
        "Question_created_time":"2020-07-03T10:17:07.991Z",
        "Question_answer_count":6,
        "Question_score_count":2,
        "Question_view_count":1075,
        "Question_body":"<p>I\u2019m getting this warning:<\/p>\n<p>WARNING: Guild took 9.56 seconds to prepare runs. To reduce startup time, try running with \u2018\u2013skip-images\u2019 or \u2018\u2013skip-hparams\u2019 options or reduce the number of runs with filters. Try \u2018guild tensorboard --help\u2019 for filter options.<\/p>\n<p>Adding skip-images or skip-hparams does not help.<\/p>\n<p>I suspect this happens because I have a resource (symbolic link) to a directory with a lot of files. Is it possible to configure guild tensorboard to ignore this directory?<\/p>\n<p>I found the -O option in: <a href=\"https:\/\/my.guild.ai\/t\/command-tensorboard\/127\" class=\"inline-onebox\">Command: tensorboard<\/a><br>\nbut adding -O logdir=tb did not work (had to abort because it never finished).<\/p>\n<p>Any ideas?<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Guild pull no matching run found",
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-pull-no-matching-run-found\/198",
        "Question_created_time":"2020-06-22T13:25:56.985Z",
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":378,
        "Question_body":"<p>Hi,<\/p>\n<p>I am having problems with pulling runs from a remote server.<\/p>\n<pre><code>guild runs --remote remote\n\n[1:5c07e941]  srl\/state:train-rl  2020-06-22 14:45:06  completed  env=SingleGoal-v01\n<\/code><\/pre>\n<p>When I try to pull the run it tells me there is no matching run was found:<\/p>\n<pre><code>guild pull remote 5c07e941 \n\nGetting remote run info \nguild: could not find a run matching '5c07e941'\n<\/code><\/pre>\n<p>My <code>config.yml<\/code> looks as follows:<\/p>\n<pre><code>remotes:\n  remote:\n    type: ssh\n    host: remote\n    user: username\n    guild-home: \/home\/username\/guild\n    guild-env: \/home\/username\/guild\/venv\n<\/code><\/pre>\n<p>The guild version I use is <code>0.7.0.rc11<\/code>.<br>\nPushing a run to the remote works fin.<\/p>\n<p>Have I done something wrong or is it a bug?<\/p>\n<p>Kind regards,<br>\nBart<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Problem setting up environment on Windows",
        "Question_link":"https:\/\/my.guild.ai\/t\/problem-setting-up-environment-on-windows\/190",
        "Question_created_time":"2020-06-19T08:07:31.359Z",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":1033,
        "Question_body":"<p>I tried to follow this instructions <a href=\"https:\/\/my.guild.ai\/t\/environments\/164\" class=\"inline-onebox\">Environments<\/a><\/p>\n<p>and when I set:<\/p>\n<pre><code>export GUILD_HOME=guild-rf\n<\/code><\/pre>\n<p>I get error:<\/p>\n<pre><code>export : The term 'export' is not recognized as the name of a cmdlet, function, script file, or operable program. Check\n the spelling of the name, or if a path was included, verify that the path is correct and try again.\nAt line:1 char:1\n+ export GUILD_HOME=.\\guild-rf\\\n+ ~~~~~~\n    + CategoryInfo          : ObjectNotFound: (export:String) [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException\n<\/code><\/pre>\n<p>I get similar error for:<\/p>\n<pre><code>GUILD_HOME=&lt;path&gt; guild &lt;command&gt; ...\n<\/code><\/pre>\n<p>I have also tried to define everything inside python:<\/p>\n<pre><code># confing\n\nGUILD_HOME = \"guild-rf\"\n\nDELETE_RUNS_ON_INIT = True\n\n# import packages\n\nimport guild.ipy as guild\n\nimport os\n\n# Initialize Guild Home\n\nif not os.path.exists(GUILD_HOME):\n\n    os.mkdir(GUILD_HOME)\n\n    \n\nguild.set_guild_home(GUILD_HOME)\n\n# HERE RUN MY TRAIN SAVED IN SUBMODUL (LOOK AT ABOVE GUILD YML)\n\n_ = guild.run(train)\n<\/code><\/pre>\n<p>I get:<\/p>\n<blockquote>\n<p>_ = guild.run(train)<\/p>\n<p><strong>---------------------------------------------------------------------------<\/strong> <strong>NameError<\/strong> Traceback (most recent call last) <strong>c:\\Users\\Mislav\\Documents\\GitHub\\trademl\\manage_guild.py<\/strong> in <strong>----&gt; [22](file:\/\/\/C:\/Users\/Mislav\/Documents\/GitHub\/trademl\/manage_guild.py?line=21)<\/strong> _ <strong>=<\/strong> guild <strong>.<\/strong> run <strong>(<\/strong> train <strong>)<\/strong> <strong>NameError<\/strong> : name \u2018train\u2019 is not defined<\/p>\n<\/blockquote>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ValueError: empty body on If",
        "Question_link":"https:\/\/my.guild.ai\/t\/valueerror-empty-body-on-if\/187",
        "Question_created_time":"2020-06-17T14:57:29.984Z",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":403,
        "Question_body":"<p>Hi!<\/p>\n<p>Using Guild AI version 0.6.6 inside a virtual environment, I receive the following error:<\/p>\n<pre><code>$ guild run main.py\nYou are about to run \/mnt\/volume1\/Dropbox\/X\/Y\/W\/main.py\n  DEBUG: yes\n  TRAIN: yes\n  batch_norm: yes\n  cudanum: 0\n  ...\n  \nContinue? (Y\/n) Y\nTraceback (most recent call last):\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 290, in &lt;module&gt;\n    main()\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 57, in main\n    _main()\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 84, in _main\n    _try_module(arg1, rest_args)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 134, in _try_module\n    _dispatch_module_exec(_flags_dest(args), module_info)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 215, in _dispatch_module_exec\n    _exec_module_with_globals(module_info, flags, args)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 271, in _exec_module_with_globals\n    _module_with_globals(module_info, globals)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 278, in _module_with_globals\n    _gen_exec(module_info, exec_script)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 251, in _gen_exec\n    exec_cb()\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/op_main.py\", line 277, in exec_script\n    python_util.exec_script(path, globals)\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/python_util.py\", line 353, in exec_script\n    code = _compile_script(src, filename, _node_filter(globals))\n  File \"\/home\/USER\/.virtualenvs\/my_venv\/lib\/python3.7\/site-packages\/guild\/python_util.py\", line 384, in _compile_script\n    dont_inherit=True)\nValueError: empty body on If\n<\/code><\/pre>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Precision-recall dashboard not showing",
        "Question_link":"https:\/\/my.guild.ai\/t\/precision-recall-dashboard-not-showing\/155",
        "Question_created_time":"2020-06-11T15:11:09.332Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":462,
        "Question_body":"<p>Hi Garrett,<\/p>\n<p>I think this should be fixed in rc10 (<a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/173\" rel=\"nofollow noopener\">https:\/\/github.com\/guildai\/guildai\/issues\/173<\/a>) but unfortunately I am still not seeing the PR dashboard when viewing results through Guild. I can try to make an MWE if needed but is there an obvious gotcha\u2019?<\/p>\n<p>To be clear, if I navigate to <code>...\/.guild\/runs\/{RUN_HASH}<\/code> and run <code>tensorboard --logdir .<\/code> then the PR dashboard shows up. If I call <code>guild tensorboard<\/code> there is no sign of that dashboard.<\/p>\n<p>I am explicitly saving the precision-recall event files in a scalars folder in the run directory. I can send\/upload one of these event files if that would help debugging (the example one is only 400B)? N.b. the info in event files that don\u2019t contain the PR curves do show up in <code>guild tensorboard<\/code>.<\/p>\n<p>Thanks again for the hard work.<\/p>\n<p>Chris<\/p>",
        "Question_closed_time":null,
        "Answer_body":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Parallel batch trial pipeline",
        "Question_link":"https:\/\/my.guild.ai\/t\/parallel-batch-trial-pipeline\/142",
        "Question_created_time":"2020-06-10T17:28:34.957Z",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":391,
        "Question_body":"<p>In a batched trial pipeline see <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/196\" rel=\"nofollow noopener\">196<\/a>, how can we run as fast as possible parallelizing everything we can ?<\/p>\n<p>Example Pipeline:<\/p>\n<pre><code>operation_1: # has no dependencies\n\noperation_2: \n    requires:\n        - operation: operation_1\n    flags-dest: globals\n    flags-import:\n        - some_param\n\noperation_3: \n    requires:\n        - operation: operation_2\n    flags-dest: globals\n    flags-import:\n        - some_other_param\n\npipeline:\n  steps:\n    - run: operation_1\n    - run: operation_2\n      flags:\n        some_param: [a, b]\n    - run: operation_3\n      flags:\n        some_other_param: [1, 2, 3]\n<\/code><\/pre>\n<pre><code># create 6 queues as we have 6 batch trials that can be done in parallel (a1, a2, a3, b1, b2, b3, c1, c2, c3)\n# run this command 6 times\nguild run queue --background \n\nguild run pipeline --stage\n<\/code><\/pre>\n<p>For some reason this leads to out of sequence events happening, like operation 2 being run before operation 1 resulting in an error. Is this the correct way to do this?<\/p>\n<p>Further, is there a good way of timing this to sanity check parallel works faster i.e time batched trials run?<\/p>",
        "Question_closed_time":"2020-06-10T23:26:07.272Z",
        "Answer_body":"<p>Don\u2019t use staged runs for this. Just run your pipelines in the background. At some point Guild will be optimized for parallel runs and you won\u2019t have to think about this (as you say - we want to run everything as fast as possible). But at the moment, you need to use parallel OS processes to manage parallel runs.<\/p>",
        "Question_self_resolution":false
    }
]