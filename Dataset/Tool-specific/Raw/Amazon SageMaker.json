[
    {
        "Question_title":"Deploy ML Timeseries models effectively",
        "Question_created_time":1685336152530,
        "Question_last_edit_time":1685336372738,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeCithZ5JTcaf9LyhhCF5BA\/deploy-ml-timeseries-models-effectively",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":16,
        "Question_answer_count":1,
        "Question_body":"Hi team !\nI need to deploy a ton of Machine Learning Models (Timeseries models) and I'm seeking a way that is effective.\n\nIn details, the problem is to build a platform capable of serving many time series with different frequencies from 5s to 10m (maybe beyond this, but that's it for the time being).\nThe ML models of the system are different in terms of the ML framework.\nThere are about 1000 ML models.\nML models sizes are from 2MB to 2GB. In which, the most popular size range is 2GB.\nThen how should I design the serving model system to most effective with optimal cost?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1685339863768,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1685339863768,
        "Answer_comment_count":1.0,
        "Answer_body":"Hello Quan Dang !\n\nThe following link refers to SageMaker Model Deployment and Deployment Recommendation: \n[https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deploy-model.html#deploy-model-options]()\n\nFor your problem, for each model, the processing time is not long, request payload is not large, and it\u2019s kind of real-time latency requirement, and there are about 1000 deep learning models and each\u2019s size is ~2GB. Therefore, we eliminate following options: async inference, serverless, batch transform and leaving only 1 option left : real-time inference. In Real-time inference, there are 4 options :\n \n- Host single model - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/realtime-single-model.html  :\u2028Fastest way to get inference from 1 ML model.\u2028\n- Host multi models in 1 container behind 1 endpoint - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html :\u2028https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html \u2028\u2028Multi-model endpoints are ideal for hosting a large number of models that use the same ML framework on a shared serving container. If you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings. Your application should be tolerant of occasional cold start-related latency penalties that occur when invoking infrequently used models.\u2028\n- Host multi models which use different containers behind 1 endpoint - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-container-endpoints.html :\u2028SageMaker multi-container endpoints enable customers to deploy multiple containers, that use different models or frameworks, on a single SageMaker endpoint. The containers can be run in a sequence as an inference pipeline, or each container can be accessed individually by using direct invocation to improve endpoint utilization and optimize costs.\u2028\n- (Eliminated) Host models along with pre-processing logic as serial inference pipeline behind one endpoint - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html \n- Note: What ever option we choose above, we all enable Auto-Scaling feature: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\n\nSo, we narrow it down to only 3 options for deployment, you can create a survey about your ML Models deployment details (a statistics for the following information of each model : framework, inference latency, GPU usage type). \n\nFor those models are frequently accessed (inference latency <~60s) , you can choose \u201cHost Single Model\u201d ; Otherwise, for those aren\u2019t frequently accessed, if they use the same ML framework, choose \u201cHost multi models in 1 container behind 1 endpoint\u201d, if they use different ML framework, choose \u201cHost multi models which use different containers behind 1 endpoint\u201d.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Error 500 received in AWS Lambda test when get starting Sagemaker jupyter notebook",
        "Question_created_time":1685226445425,
        "Question_last_edit_time":1685313728986,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEFq48t6KTKSLcwktDbc9RQ\/error-500-received-in-aws-lambda-test-when-get-starting-sagemaker-jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":30,
        "Question_answer_count":0,
        "Question_body":"Hi Guys, \nI'm trying to automate Jupyther Notebook using lambda function with API Gateway and Websocket.  I used this tutorial [https:\/\/www.youtube.com\/watch?v=wSdVAa7b5Oc]()to configure my services (lambda, API Gateway, and Sagemaker\/Jupyter notebook). I make a little bit of modifications because the recent Python version, in the tutorial, works with Python 3.6 and I used 3.8.\nNote: In Python 3.8 the method request was changed \n```\nimport requests\n```\nI Attached PICs from the Error 500 printout, sagemaker and notbook configuration!\n\nMy complete Python code:\n\n```\nimport boto3\nimport time\nimport requests\nfrom websocket_client import websocket\ndef lambda_handler(event, context):\n    sm_client = boto3.client('sagemaker')\n    notebook_instance_name = 'nbCareboxstatistics'\n    url = sm_client.create_presigned_notebook_instance_url(NotebookInstanceName=notebook_instance_name)['AuthorizedUrl']\n    url_tokens = url.split('\/')\n    http_proto = url_tokens[0]\n    http_hn = url_tokens[2].split('?')[0].split('#')[0]\n    s = requests.Session()\n    r = s.get(url)\n    cookies = \"; \".join(key + \"=\" + value for key, value in s.cookies.items())\n    print(cookies)\n    ws = websocket.create_connection(\n        \"wss:\/\/{}\/terminals\/websocket\/1\".format(http_hn),\n        cookie=cookies,\n        host=http_hn,\n        origin=http_proto + \"\/\/\" + http_hn\n    )\n    print(ws)\n    ws.send(\"\"\"[ \"stdin\", \"jupyter nbconvert --execute --to notebook --inplace \/home\/ec2-user\/SageMaker\/nbCareboxstatisticsv1.ipynb --ExecutePreprocessor.kernel_name=python3 --ExecutePreprocessor.timeout=1500\\\\r\" ]\"\"\")\n    \n    time.sleep(1)\n    ws.close()\n    print(\"websocket client created\")\n    return None\n```\n**I did all those configurations as a tutorial and received the error below:**\n\n![**Error 500**](\/media\/postImages\/original\/IM0iQ89kE6TnOrUGZv2tQR1A)\n![**Sagemaker configurations**](\/media\/postImages\/original\/IMv0uYB201THeT45owvu0YLA)\n![**Notebook instance active on Sagemaker**](\/media\/postImages\/original\/IMnVr-21PYTUOpwzJ1md8aDQ)\n\n\nAnybody can help how to solve it?\n\nBR\nAlex,",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Get all tags for Deep Learning Container registry",
        "Question_created_time":1685183748961,
        "Question_last_edit_time":1685357598416,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK28-msvkRMeGHBKv6E5RUg\/get-all-tags-for-deep-learning-container-registry",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":15,
        "Question_answer_count":0,
        "Question_body":"When I have a look at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md I get many different deep learning containers with an example URL, for instance, \n\n763104351884.dkr.ecr.us-east-1.amazonaws.com\/huggingface-pytorch-inference:2.0.0-transformers4.28.1-cpu-py310-ubuntu20.04\n\nMy questions is now, how I can find all tags available for some registry like the one above.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Use GroundTruth bbox labels in a TensorFlow fine tuning job",
        "Question_created_time":1685146292984,
        "Question_last_edit_time":1685319981021,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqOffgCNnTj2FSE873aQeLQ\/use-groundtruth-bbox-labels-in-a-tensorflow-fine-tuning-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":16,
        "Question_answer_count":1,
        "Question_body":"I can use the manifest file created by Ground Truth [ref](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html#sms-output-box) as input to a training job for object detection ([ref](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/396c4d204c966df8a1d5f702a6fb16789d96a9b3\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb)). \nCan also use it for Tensorflow jobs, like the one [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection-tensorflow.html)? In all the tutorials I find, the data in `annotations.json` has a different format than the one in the GroundTruth output.\n\nMy goal is to use more ad-hoc models rather than just resnet and vgg, and get information from Tensorboard and such.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker autoscaling doesn't work as expected",
        "Question_created_time":1685090255367,
        "Question_last_edit_time":1685350729187,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxqFDpGXsTfWsGTTZ5IH56w\/sagemaker-autoscaling-doesn-t-work-as-expected",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":19,
        "Question_answer_count":1,
        "Question_body":"I've configured a model for realtime inference and it's working correctly. I'm now trying to apply an autoscaling policy and using `boto3` library to put the scaling policy. The code that I've used:\n```python\nimport pprint\nimport boto3\nfrom sagemaker import get_execution_role\nimport sagemaker\nimport json\n\nendpoint_name = \"a-deployed-endpoint\"\nasg_client = boto3.client('application-autoscaling')\nresource_id='endpoint\/' + endpoint_name + '\/variant\/' + 'AllTraffic'\nasg_client.register_scalable_target(\n    ServiceNamespace='sagemaker', #\n    ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n    MinCapacity=1,\n    MaxCapacity=5\n)\n\nresponse = asg_client.put_scaling_policy(\n    PolicyName='SageMakerEndpointInvocationScalingPolicy',\n    ServiceNamespace='sagemaker',  \n    ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n    PolicyType='TargetTrackingScaling',\n    TargetTrackingScalingPolicyConfiguration={\n        'TargetValue': 20.0,\n        'PredefinedMetricSpecification': {\n            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n        },\n        'ScaleInCooldown': 60, \n        'ScaleOutCooldown': 10 \n    }\n)\n```\nTo confirm the policy has been configured correctly, I've double-checked with `describe_scaling_policies` method from ***boto3 sdk*** and on ***AWS console***.\n\nWith `describe_scaling_policies` method, we have:\n```python\nres = asg_client.describe_scaling_policies(\n    PolicyNames=['SageMakerEndpointInvocationScalingPolicy'],\n    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n    ResourceId=resource_id, # Endpoint name \n    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n)\nprint(res)\n\n###### print's result ######\n{\n    'ScalingPolicies': [\n        {\n            'PolicyARN': \n'arn:aws:autoscaling:ap-southeast-1:***:scalingPolicy:c97d566f-50f0-4ac4-ac23-765a815095e9:resource\/sagema\nker\/endpoint\/jets-gpu-endpoint\/variant\/AllTraffic:policyName\/SageMakerEndpointInvocationScalingPolicy',\n            'PolicyName': 'SageMakerEndpointInvocationScalingPolicy',\n            'ServiceNamespace': 'sagemaker',\n            'ResourceId': 'endpoint\/jets-gpu-endpoint\/variant\/AllTraffic',\n            'ScalableDimension': 'sagemaker:variant:DesiredInstanceCount',\n            'PolicyType': 'TargetTrackingScaling',\n            'TargetTrackingScalingPolicyConfiguration': {\n                'TargetValue': 20.0,\n                'PredefinedMetricSpecification': {\n                    'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n                },\n                'ScaleOutCooldown': 10,\n                'ScaleInCooldown': 60\n            },\n            'Alarms': [\n                {\n                    'AlarmName': \n'TargetTracking-endpoint\/jets-gpu-endpoint\/variant\/AllTraffic-AlarmHigh-bc06a793-369a-4515-8a7f-3cb5127455b6',\n                    'AlarmARN': \n'arn:aws:cloudwatch:ap-southeast-1:***:alarm:TargetTracking-endpoint\/jets-gpu-endpoint\/variant\/AllTraffic-\nAlarmHigh-bc06a793-369a-4515-8a7f-3cb5127455b6'\n                },\n                {\n                    'AlarmName': \n'TargetTracking-endpoint\/jets-gpu-endpoint\/variant\/AllTraffic-AlarmLow-9889edca-45f2-4f73-9f71-76cacf7b5882',\n                    'AlarmARN': \n'arn:aws:cloudwatch:ap-southeast-1:***:alarm:TargetTracking-endpoint\/jets-gpu-endpoint\/variant\/AllTraffic-\nAlarmLow-9889edca-45f2-4f73-9f71-76cacf7b5882'\n                }\n            ],\n            'CreationTime': datetime.datetime(2023, 5, 26, 7, 3, 12, 714000, tzinfo=tzlocal())\n        }\n    ],\n    'ResponseMetadata': {\n        'RequestId': '56238668-ee84-4319-b2a9-8448cac98358',\n        'HTTPStatusCode': 200,\n        'HTTPHeaders': {\n            'x-amzn-requestid': '56238668-ee84-4319-b2a9-8448cac98358',\n            'content-type': 'application\/x-amz-json-1.1',\n            'content-length': '1341',\n            'date': 'Fri, 26 May 2023 07:03:14 GMT'\n        },\n        'RetryAttempts': 0\n    }\n}\n```\n\nWith AWS console:\n![Enter image description here](\/media\/postImages\/original\/IMot2In7lQQJmvhZSsKhJy3g)\n\nEverything looks correct, right ? So I head to some load tests to make sure that the autoscaling works fine (for more information, I've used `locust` library to carry out the test). After 5 minutes, the endpoint starts encountering a bottleneck and also the high-traffic cloudwatch raises an alarm as the image below:\n\n![Enter image description here](\/media\/postImages\/original\/IMV5h9oeCbRmC2lJ3Lb2DtNQ)\n\nThe weird thing here is after 20 minutes since the cloudwatch raised an alarm, I don't see the endpoint scales out instances as expected, although I've tried to use a low target threshold and small scale out time. So, any clue or suggesstion to make the endpoint scalable?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best configuration for inferencing with PyTorch models",
        "Question_created_time":1685054186494,
        "Question_last_edit_time":1685314304500,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5vNhkpscTuCYBdW4bczvMQ\/best-configuration-for-inferencing-with-pytorch-models",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":15,
        "Question_answer_count":1,
        "Question_body":"I'm trying to make a public facing web app that allows for inferencing, with probably ten or so available models to my users.  My initial thought was that I would have a front-end basic webpage, that communicates with a REST API server on an EC2 instance. But since I started planning this out a bit more, I found a lot of info about various AWS products, and they seem interesting but it's all pretty over my head.\n\nI initially came the site because I heard about elastic inferencing.  After I researched elastic inferencing more, it seems like Amazon is encouraging people to use Inferentia2 instead. I realize that I could just do an EC2 instance, but I don't know how well that'll work for scaling if this app I'm making becomes popular. I've also read a bit about SageMaker, API Gateway, and even \"serverless\" options like Lambda, but I don't really know if those would integrate well with low cost inferencing products that AWS offers.\n\nAny advice on setting this kind of thing up?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How\/where is data copied in sagemaker pipelines?",
        "Question_created_time":1685038227025,
        "Question_last_edit_time":1685385438756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhiUhhZKyRFy_k8NkiOwL_Q\/how-where-is-data-copied-in-sagemaker-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":20,
        "Question_answer_count":1,
        "Question_body":"example contrived for this question, I understand , when we create a sagemaker pipeline, with steps to process data and then to run training, the data is copied a local instance in some opt\/ml\/.. directory i would assume. this is s3 file mode for training. so if i pull down the data manually from notebok or terminal and copy it to wherever sagemaker wants. when i run the pipeline, how can i tell sagemaker that the data is already present in the local instance such that it doesn't have to download from the s3 uri ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"custom entity recognition output coming incorrect",
        "Question_created_time":1685010908080,
        "Question_last_edit_time":1685357722053,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFQeJ-O1yQo-JfqgvbgTXnQ\/custom-entity-recognition-output-coming-incorrect",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":22,
        "Question_answer_count":1,
        "Question_body":"this is a sample pdf of my data and all the pdfs have same format:\n![Enter image description here](\/media\/postImages\/original\/IM35LB0AWPQSuhGSqkntlIhA)\nNow I am first using textract on this and then through the amazon sagemaker platform I am annotating the data. As it can be seen there is an issue date and an effective date.\nThe output from textract text is: \n![Enter image description here](\/media\/postImages\/original\/IMzAXK0z02TeSVx9LH-iY2tw)\nhere each line is considered an object so I labeled the first date as publishdate and the second date as effectivedate and trained my comprehend cer model. \nAfter passing a similar format test text into the comprehend model, the output I got had both the dates marked as effectivedate. like this:\n![Enter image description here](\/media\/postImages\/original\/IMa_v_j1_ORVey0tHM02iwew)\nbut I want the first date to be publishdate and 2nd date to be the effectivedate. I made sure to clean the data and made sure that all the labeling was done correctly.\nHow should I go about solving this problem?\nMy organization's architecture is that pdfs will come to textract and then the text extracted will be passed to the comprehend for entity recognition.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker billing",
        "Question_created_time":1684944114450,
        "Question_last_edit_time":1685290965941,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEV6lhxssQryhylFFgnFadA\/sagemaker-billing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_body":"$0.05 per Notebook ml.t3.medium hour in US East (N. Virginia)\n75.782 Hrs\tUSD 3.79\n$0.115 per Studio-Notebook ml.m5.large hour in US East (N. Virginia)\n9.096 Hrs\tUSD 1.05\n$0.239 per Training ml.c4.xlarge hour in US East (N. Virginia)\n0.059 Hrs\tUSD 0.01\n$0.24 per Hosting ml.m4.xlarge hour in US East (N. Virginia)\n84.45 Hrs\tUSD 20.27\n$0.24 per Training ml.m4.xlarge hour in US East (N. Virginia)\n0.263 Hrs\tUSD 0.06\n\nI cant seem to figure out how to disable my ml.c4 hosting instance.. any help?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Not able to create an endpoint",
        "Question_created_time":1684929438940,
        "Question_last_edit_time":1685275591492,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOMZ3zifoSReYaPAn6Dwjuw\/not-able-to-create-an-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":28,
        "Question_answer_count":0,
        "Question_body":"I am able to train and tune the model. But at the time of model deployment, the endpoint is not getting created, and it fails after some time. It gives the error as \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/train\/train_features.csv'\". The question is: at the time of model deployment, if it is not picking the train_features.csv file because the path is not correct, then how does the model training and tuning happen? The training and tuning also happen on the same train_features.csv file, and the path is same.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'nvgpu' in sagemaker batch transform",
        "Question_created_time":1684928368028,
        "Question_last_edit_time":1685275291793,
        "Question_link":"https:\/\/repost.aws\/questions\/QUc3tkDiJ7TPu-2roPoX5TWA\/modulenotfounderror-no-module-named-nvgpu-in-sagemaker-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"I am trying to do batch transform inference with a `ml.g4dn.xlarge` instance using a GPU. However, when I run the inference, I get\n`ModuleNotFoundError: No module named 'nvgpu'`\nI tried to add the nvgpu library to the requirements.txt file of the model when training, but i have not found a version that works. \nI also read here : https:\/\/github.com\/pytorch\/serve\/issues\/1813#issuecomment-1231025086 that the issue could be the framework_version, but I have tried using 1.9.0 like it suggest but the issue remains. \nAny idea what could be the issue or what version of nvgpu to add to requirements?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"FSx for lustre with AWS Sagemaker Error - Artifact upload failed:Please ensure that the subnet's route table has a route to an S3 VPC endpoint or a NAT device",
        "Question_created_time":1684781909160,
        "Question_last_edit_time":1685128680481,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7RpLD3uvSLmfA77y6uNGNg\/fsx-for-lustre-with-aws-sagemaker-error-artifact-upload-failed-please-ensure-that-the-subnet-s-route-table-has-a-route-to-an-s3-vpc-endpoint-or-a-nat-device",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":85,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI am trying to use FSx for Lustre for my Sagemaker training. I followed this tutorial: https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems\/.\n\nCode:\n\n```\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.estimator import Estimator\n\nsess = sagemaker.Session()\n\nestimator = Estimator(image_uri='image-uri',\n                      role='my-role',\n                      base_job_name='training-job',\n                      instance_count=1,\n                      sagemaker_session=sess,\n                      instance_type='ml.m5.xlarge',  # ml.p3.2xlarge\n                      subnets=['subnet-id'],\n                      security_group_ids=['sg-id', 'sg-id'],\n                      hyperparameters={...} \n                     )\n\nfrom sagemaker.inputs import FileSystemInput\n\ntrain_input = FileSystemInput(file_system_id = 'fs-id',\n                              file_system_type = 'FSxLustre',\n                              directory_path = '\/mount-idea\/dataset',\n                              file_system_access_mode = 'rw')\n\nestimator.fit(train_input)\n```\n\nFirst issue is that I don't have internet connection in the container, and second is this error:\n\n```\nUnexpectedStatusException: Error for Training job training-job: Failed. Reason: ClientError: Artifact upload failed:Please ensure that the subnet's route table has a route to an S3 VPC endpoint or a NAT device, and both the security groups and the subnet's network ACL allow uploading data to all output URIs\n```\n\nI have create a custom VPC for this with private and public subnets. I enabled NAT and S3 endpoint. I am using one of the public subnet so that I can have access to internet. I created custom security group rules (inbound\/outbound) with Custom TCP for port 988 and ports 1018 - 1023 for the security groups I use.\n\nThe code is running in a sagemaker notebook that has as VPC, the custom VPC I created and uses same subnet and security groups as the ones pass to the estimator.\n\nWhat should I do to fix this error?\n\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"install java dependency when building my own processing container",
        "Question_created_time":1684544519306,
        "Question_last_edit_time":1684891019388,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjfdp5BKmTvSdXirILemjoQ\/install-java-dependency-when-building-my-own-processing-container",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":2,
        "Question_body":"A python package I'm using requires java dependency. I put the following lines in my docker file when building processing container, and got apt-add-repository: command not found error. What is the correct commands to put in a dockerfile to install java? \n\nRUN add-apt-repository ppa:openjdk-r\/ppa && \\\n    apt-get update && \\\n    apt-get install -y openjdk-7-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;\n    \n\nRUN apt-get update && \\\n    apt-get install ca-certificates-java && \\\n    apt-get clean && \\\n    update-ca-certificates -f;\n\nENV JAVA_HOME \/usr\/lib\/jvm\/java-8-openjdk-amd64\/\nRUN export JAVA_HOME",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Autopilot Endpoint OutOfMemory",
        "Question_created_time":1684501371045,
        "Question_last_edit_time":1684848041940,
        "Question_link":"https:\/\/repost.aws\/questions\/QUgg14GO8FQHKLb_u3P1h7VA\/sagemaker-autopilot-endpoint-outofmemory",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":35,
        "Question_answer_count":0,
        "Question_body":"Good morning, I am encountering an unusual issue with Sagemaker Autopilot. After launching the training on my dataset and completing the entire process without any problems, the best model is deployed, and the endpoint appears to be functioning correctly. However, the issue arises when I attempt to call the endpoint repeatedly. I observe that the memory usage rapidly increases until it reaches 97%. At that point, it either crashes with the error message \"worker died,\" and the CloudWatch logs indicate an \"OutOfMemory\" error in Java, or it remains operational but becomes congested. Upon reviewing CloudWatch, there doesn't appear to be anything abnormal except for the recurring message:\n\"The column 'column_name' does not exist in your dataset. Please specify a different column name for the 'Input column' that actually exists in your dataset and try again.\"\nThis message repeats even before I start invoking the endpoint. I'm uncertain if this is indeed the cause of the problem.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Batch Transform MultiRecord Fail with CSV as Input",
        "Question_created_time":1684490308406,
        "Question_last_edit_time":1684836402615,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAI7z2SCLT06bUOu1ULKV-Q\/sagemaker-batch-transform-multirecord-fail-with-csv-as-input",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_body":"I am trying to do some inference on some CSV file saved on s3 using BatchTransform with `strategy ='MultiRecord'` and `assemble_with='Line'`. The same system works with `strategy ='SingleRecord'`, however I need it to be as efficient as possible. The main issue comes when I switch to MultiRecord with a small csv composed of two columns, both texts.\nWith a `max_payload = 6`, the process is succesfull with a CSV of 181 samples (609.7KB), but with the same CSV with 182 samples (621.6KB), the process fails with a `\"message\": \"Worker died.\"`. I imagined it had something to do with the memory limit of the instance I am using, so I switched to a `ml.m5.2xlarge` with 32GB of memory. \nWhen I switch to a `max_payload = 7`, suddenly the process works with the 182 samples of CSV (621.6KB), but it fails with anything bigger than that. Any ideas of what could be causing this?\n\nThe logs look like this\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,218 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/8.68M [00:00<?, ?B\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,327 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 20.0k\/8.68M [00:00<00:47, 191kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,434 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 1%| | 100k\/8.68M [00:00<00:17, 526kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,543 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 3%|\u258e | 228k\/8.68M [00:00<00:10, 841kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,651 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 6%|\u258c | 509k\/8.68M [00:00<00:05, 1.56MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,759 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 12%|\u2588\u258f | 1.04M\/8.68M [00:00<00:02, 2.91MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,867 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 25%|\u2588\u2588\u258d | 2.14M\/8.68M [00:00<00:01, 5.54MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,975 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 50%|\u2588\u2588\u2588\u2588\u2589 | 4.31M\/8.68M [00:00<00:00, 10.6MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,976 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 8.65M\/8.68M [00:00<00:00, 20.7MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.68M\/8.68M [00:00<00:00, 10.5MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG -\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,131 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/615 [00:00<?, ?B\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,894 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13647\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13459\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:15108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,897 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489793897\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13705\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,899 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:42|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,902 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13399\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:15148|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:73|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,989 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13784\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,991 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:15231|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,992 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13663\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:15249|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,010 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:103|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13293\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:15277|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,039 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:52|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13751\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:15426|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,188 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:87|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,971 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.74238204956055|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.122749328613281|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:17504.14453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13734.4453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.8|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,909 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:50:54.009+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:50:54.009+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.lang.Thread.run(Thread.java:829) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,945 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_MODEL_LOADED\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,946 [INFO ] W-9006-model_1.0 ACCESS_LOG - \/169.254.255.130:54478 \"POST \/invocations HTTP\/1.1\" 500 72597\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [INFO ] W-9006-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489779\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,948 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,949 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,995 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489853995\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,997 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489853\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,227 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded \/opt\/conda\/lib\/python3.8\/site-packages\/ts\/configs\/metrics.yaml.\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]428\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,236 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,237 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9006.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489856238\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,269 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10083\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:87564|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,352 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:30|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,971 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:46.74235534667969|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:9.12277603149414|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,973 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:17432.91796875|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:13805.67578125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:45.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,996 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 2\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:51:54.028+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:51:54.028+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:51:54.028+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1684555351305,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1684555351305,
        "Answer_comment_count":1.0,
        "Answer_body":"There could be two possible reasons for the error message : \"message\": \"Worker died.\".\n\n1. This is commonly noticed when the instance exhausts the Memory usage. (This can be verified if you can check the Cloudwatch Metrics for the job run.)\n2. Model server timeouts during the job. \n\nYou can either use a larger instance for the job or try to set the higher value of following environment variables - worker and timeout in your script. \n\n\tmodel_server_workers = int(os.environ.get(_params.MODEL_SERVER_WORKERS_ENV, num_cpus())) \n\tmodel_server_timeout = int(os.environ.get(_params.MODEL_SERVER_TIMEOUT_ENV, <Enter value here>))",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"DeepAR vs DeepVAR confusion",
        "Question_created_time":1684486028940,
        "Question_last_edit_time":1684833558302,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9AQaPdIURoOlt_kV6-7hYQ\/deepar-vs-deepvar-confusion",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":37,
        "Question_answer_count":1,
        "Question_body":"I understand that sagemaker Deepar is a deep learning-based model designed for time series forecasting and it is primarily used for univariate time series forecasting.\nHowever, it can incorporate dynamic features, which are time-varying covariates or additional time series that provide extra information to improve the forecasting accuracy. My question is: Are the patterns and relationships between the dynamic variables factored into DeepAR algorithm, and hence making it suitable for applications where the interactions between variables are important?\n\nWhat will be the main difference betwen DeepAR and DeepVAR then? \n\nMany thanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Ground Truth Labelling Job Resource Limits Exceeded",
        "Question_created_time":1684457128676,
        "Question_last_edit_time":1684804009194,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZY-ipalxTYeBpsTTjgB1fQ\/sagemaker-ground-truth-labelling-job-resource-limits-exceeded",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":41,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI have a Sagemaker Ground Truth Labelling job that has failed, in cloudwatch it displays:\n\n```\n{\"event-name\":\"HUMAN_TASK_FAILED\",\"event-log-message\":\"ERROR: Human task failed for line 3.\",\"labeling-job-name\":\"\"}\n```\n\nWhen I download the manifest file, it shows:\n\n```\n{\"metadata\":{\"retry-count\":1,\"failure-reason\":\"ClientError: Task failed to render: [ ResourceLimitsExceeded: \\u0027Resource limits exceeded\\u0027 ].\",\"human-annotated\":\"true\"}}\n```\n\nIt is a bounding box labelling job, and the other items on the job have succeeded. There are 2 labels categories on the job. \n\nHow do I find the resource that is causing this issue?\n\nThere are a lot of bounding box annotations in each image, but I cant find what resource limits there are on the number of bounding boxes that are allowed?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Java not found when running Sagemaker Studio python notebooks",
        "Question_created_time":1684388506144,
        "Question_last_edit_time":1684735019008,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIruPbWNHQ2iqZsDZEj41hA\/java-not-found-when-running-sagemaker-studio-python-notebooks",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":109,
        "Question_answer_count":2,
        "Question_body":"Here is the issue I'm currently facing when running the Python package (pypmml) within Sagemaker Studio notebooks. I am encountering a \"No such file or directory: 'java': 'java'\" error.\n\nIt seems that the error arises from Java not being installed for the Python kernel. I attempted the following solutions, but unfortunately, none of them resolved the error:\n1. I successfully used 'yum install java' in the terminal but no luck to call the java within notebooks. \n2. The 'sudo install' command inside Studio notebook is disabled. \n\nCan someone please provide guidance on how to install java or specify the java path within Sagemaker Studio notebooks?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Connect to RDS from basic SageMaker Studio domain?",
        "Question_created_time":1684357478958,
        "Question_last_edit_time":1684703683926,
        "Question_link":"https:\/\/repost.aws\/questions\/QUksj1ZyvgQbCveN6a8laexA\/connect-to-rds-from-basic-sagemaker-studio-domain",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":102,
        "Question_answer_count":2,
        "Question_body":"If you have a default SageMaker Studio domain, which has public internet access, I'm guessing you can't connect to an RDS instance that's in a private subnet? You'd have to create a new domain with the \"Standard setup\" instead of the \"Quick Setup\", right?\n\n**Or** can you actually connect from a SageMaker Studio notebook that doesn't have the \"Standard Setup\"? If so, how do you see its security group, so I can allow list that security group in the RDS instance's  inbound security group rules.\n\n![Enter image description here](\/media\/postImages\/original\/IMUKUHS7d9RYK4TdWQHFaVEw)",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error while compiling and running the LLM on Inf2 instance",
        "Question_created_time":1684333922223,
        "Question_last_edit_time":1684681382482,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoyaQUbo5RUa5-eoQ_qP4yQ\/error-while-compiling-and-running-the-llm-on-inf2-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":106,
        "Question_answer_count":2,
        "Question_body":"Hi,\nI am trying to deploy the Databricks open source LLM i.e Dolly on inf2 instance. Instance type is `inf2.24xlarge` used the AMI `Deep Learning AMI Neuron PyTorch 1.13 (Ubuntu 20.04) 2023051`.\nI am able to compile the model however while loading the compiled mode i am getting the following error:\n`Unknown opcode for unpickling at 0x59: 89`\n\n**Details are:**\nI ran the following code for the compilation which took `789.6257681846619 seconds` to compile and generated the ` *.pt` file of 11.8GB\n\n```\nimport torch\nimport torch_neuronx\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\nimport transformers\nfrom tqdm import tqdm as tqdm\nfrom transformers import pipeline\nimport time\n  \n# Create the tokenizer and model\nname = \"databricks\/dolly-v2-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(name)\ntokenizer.pad_token = tokenizer.eos_token # Define the padding token value\nmodel = AutoModelForCausalLM.from_pretrained(\"databricks\/dolly-v2-7b\", torchscript=True)\nmodel.eval()\ntext = \"Explain to me the difference between nuclear fission and fusion.\"\n\n#Encoding code\ntoken_encode_start_time = time.time()\nencoded_input =  tokenizer(text, return_tensors='pt')\ntoken_encode_completion_time = time.time()\ntoken_encoding_time = token_encode_completion_time - token_encode_start_time\nprint('Decode time:', token_encoding_time, 'seconds')\nprint(encoded_input)\n\nneuron_input =( \n    encoded_input['input_ids'],\n    encoded_input['attention_mask']\n   )\n\n#inference code on CPU\ntoken_inference_start_time = time.time()\noutput_cpu = model(*neuron_input)\ntoken_inference_end_time = time.time()\ntoken_inference_time = token_inference_end_time - token_inference_start_time\nprint('Inference time:', token_inference_time, 'seconds')\n#print(tokenizer.decode(output_cpu[0]))\nprint(output_cpu)\n\n# Compilation code\nmodel.eval()\nprint(\"evaluation done\")\ncompile_start_time = time.time()\nmodel_neuron = torch_neuronx.trace(model, neuron_input)\ncompile_end_time = time.time()\ncompile_time = compile_end_time - compile_start_time\nprint('compile_time:', compile_time, 'seconds')\n\n\n# save compiled model\nfilename = \"dolly_neuron.pt\"\ntorch.jit.save(model_neuron, filename)\nprint(encoded_input)\n\n```\n\nWhen i am trying to load the saved model for inference using the code:\n\n`neuron_model = torch.jit.load(\"dolly_neuron.pt\")`\nI am getting the following error:\n\n`Unknown opcode for unpickling at 0x59: 89`\n\nCall stack:\n\n\n```\n\/home\/ubuntu\/bert-example\/huggingface-demos\/inferentia2\/dolly\/dolly_test.py:41 in <module>       \u2502\n\u2502                                                                                                  \u2502\n\u2502   38                                                                                             \u2502\n\u2502   39                                                                                             \u2502\n\u2502   40 # Load TorchScript back                                                                     \u2502\n\u2502 \u2771 41 neuron_model = torch.jit.load(\"dolly_neuron_2.pt\")                                          \u2502\n\u2502   42                                                                                             \u2502\n\u2502   43 encoded_input =  tokenizer(text, return_tensors='pt')                                       \u2502\n\u2502   44                                                                                             \u2502\n\u2502                                                                                                  \u2502\n\u2502 \/opt\/aws_neuron_venv_pytorch\/lib\/python3.8\/site-packages\/torch\/jit\/_serialization.py:162 in load \u2502\n\u2502                                                                                                  \u2502\n\u2502   159 \u2502                                                                                          \u2502\n\u2502   160 \u2502   cu = torch._C.CompilationUnit()                                                        \u2502\n\u2502   161 \u2502   if isinstance(f, str) or isinstance(f, pathlib.Path):                                  \u2502\n162 \u2502   \u2502   cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files)     \u2502\n\u2502   163 \u2502   else:                                                                                  \u2502\n\u2502   164 \u2502   \u2502   cpp_module = torch._C.import_ir_module_from_buffer(                                \u2502\n\u2502   165 \u2502   \u2502   \u2502   cu, f.read(), map_location, _extra_files      \n```\n\n\nAny pointers will be really helpful.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1684421159419,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1684421159419,
        "Answer_comment_count":2.0,
        "Answer_body":"There is a known issue with this model type that the team are working on and we will have a fix in an upcoming release of the [Neuron SDK](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/). Since this is a large model (7B params) you will need to shard it across multiple NeuronCores and we generally recommend using the [transformers-neuronx](https:\/\/github.com\/aws-neuron\/transformers-neuronx) library which provides Tensor Parallel and other features to assist in deploying Decoder based LLMs to inf2 based instances. Soon this library will also have support for GPT-NeoX which is the underlying architecture for this model. Keep an eye out for new announcements from the Neuron SDK docs release notes page [here](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/release-notes\/index.html#latest-neuron-release).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"problem with \"pip install gcc\" in terminal",
        "Question_created_time":1684322738308,
        "Question_last_edit_time":1684670304945,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuk7qOE4pQrqKFTXZq6FYuQ\/problem-with-pip-install-gcc-in-terminal",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":79,
        "Question_answer_count":1,
        "Question_body":"So we are trying in AWS SageMaker console code to detect images and to configure it we need gcc compiler. which we added in cloud console, refreshed the studio but after running in terminal python setup.py install\" it says that the gcc isnt installed. How should we proceed?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1684327148088,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1684327148088,
        "Answer_comment_count":0.0,
        "Answer_body":"gcc doesn't install via pip. You should install  this way\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1890",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"async inference docker restart after less than 20 minutes, not helpful log found",
        "Question_created_time":1684318489587,
        "Question_last_edit_time":1684666245269,
        "Question_link":"https:\/\/repost.aws\/questions\/QUInjgvtuaRNi54eIpETzQ-Q\/async-inference-docker-restart-after-less-than-20-minutes-not-helpful-log-found",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":53,
        "Question_answer_count":1,
        "Question_body":"i have a async inference on SageMaker, with BYOC.  The job may take about 20 minutes and more. And i already set InvocationTimeoutSeconds to 3600 seconds.    \nthe problem is, when i start a new inference request, from CloudWatch i know the job is in progress,  and there is not \/ping request log in CloudWatch. but the after about 10 minute ,  \/ping log in CloudWatch show up again with error, which says service unavailable.  \n then after 6 minute, i found a new log stream in CloudWatch, and the older one is down.  \nhere is the log in CloudWatch:\n\n```\n...(\/ping log, until i send a request)\n\n2023-05-17T16:12:15.761+08:00\ttask type:file ( my job start)\n2023-05-17T16:22:58.223+08:00.     [error] 31#31: *389 connect() to unix:\/tmp\/gunicorn.sock failed (11: Resource temporarily unavailable) while connecting to upstream, client: 169.254.178.2, server: , request: \"GET \/ping HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\", host: \"169.254.180.2:8080\"\n2023-05-17T16:23:02.761+08:00\t169.254.178.2 - - [17\/May\/2023:08:22:58 +0000] \"GET \/ping HTTP\/1.1\" 502 166 \"-\" \"AHC\/2.0\"\n\n...(the error and \/ping repeat for 6 minute)\n\n2023-05-17T16:28:58.133+08:00    [error] 31#31: *449 connect() to unix:\/tmp\/gunicorn.sock failed (11: Resource temporarily unavailable) while connecting to upstream, client: 169.254.178.2, server: , request: \"GET \/ping HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\", host: \"169.254.180.2:8080\"\n```\n\nhow can i fix it?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1684462461646,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1684462461646,
        "Answer_comment_count":1.0,
        "Answer_body":"If I understand your log snippets correctly, it looks like your container is failing to respond to any `\/ping`s while processing the long-running request? Failing to respond to ping for an extended period indicates your endpoint is unhealthy so will signal SageMaker to restart the container.\n\nA likely reason for not responding might be if your request handling uses multi-processing in a way that maxes out all CPUs on the instance? This would leave no cores\/threads available to handle to incoming pings while the data is getting processed. In that case, the fix would be to identify what component(s) of your request handling might be using all available system cores at once, and re-configuring them to use `int(os.environ[\"SM_NUM_CPUS\"]) - 1` instead.\n\nA similar but less likely reason is if for some reason you're using a fully-custom serving stack or have explicitly re-configured the default one to have only one worker thread: In which case your main request handling might be blocking the server with no threads available to pick up concurrent pings (even though there are CPU resources)?",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"how to invoke a multi model endpoint in triton server?",
        "Question_created_time":1684283884921,
        "Question_last_edit_time":1684631556470,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfm4XRlj8RnmY6BDxGGe4UA\/how-to-invoke-a-multi-model-endpoint-in-triton-server",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":47,
        "Question_answer_count":0,
        "Question_body":"set up - testing a multi model endpoint in a nvidia triton server. I created the multi model endpoints based on the aws docs here (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint.html#create-multi-model-endpoint-sdk-gpu) .  the model i'm using is  a ditilbert model (https:\/\/huggingface.co\/docs\/transformers\/model_doc\/distilbert). also based on the docs here, https:\/\/aws.amazon.com\/blogs\/machine-learning\/host-ml-models-on-amazon-sagemaker-using-triton-python-backend\/, i created a model.py file for the python backend (sample below). I also have config.pbtxt file  . based on the docs here - https:\/\/raw.githubusercontent.com\/triton-inference-server\/python_backend\/main\/examples\/add_sub\/model.py , we need to implement execute method and execute method takes a list of  pb_utils.InferenceRequest argument. before this , in a single model implementation , i invoke it by passing json data as payload. so i understand i have to convert my text to pb_utils.InferenceRequest  type , as the python backend for triton server only accepts pb_utils.InferenceRequest  type. I couldn't find any examples of this. what are the steps i need, to migrate to a multi model endpoint in a nvidia triton container. I need to convert text to token via tokenizer before sending to the python backend?\n\nconfig.pbtxt\n```\nname: \u201csome_model_config\u201d\nbackend: \u201cpython\u201d\nmax_batch_size: ??\ninput: [\n    {\n         name: \u201cINPUT0\u201d\n         data_type: TYPE_STRING\n         dims: [ -1 ]\n    },\n]\noutput [\n      {\n           name: \u201coutput\u201d\n           data_type: TYPE_STRING\n           dims: [ -1 ]\n```\n\ninvoke a multi model endpoint\n```\npayload = \"some text here\"\nresponse = runtime_sm_client.invoke_endpoint(\nEndpointName=endpoint_name,\nContentType=\"application\/json\",\nBody=json.dumps(payload),\n)\n```\n\n```\nimport triton_python_backend_utils as pb_utils\nclass TritonPythonModel:\n    def initialize(self, args):\n       ....\n\n    def execute(self, requests):\n       `execute` must be implemented in every Python model. `execute`\n        function receives a list of pb_utils.InferenceRequest as the only\n        argument. This function is called when an inference is requested\n        for this model.\n\n        Parameters   \n        requests : list     A list of pb_utils.InferenceRequest\n\n        Returns :  list of pb_utils.InferenceResponse. The length of this list must be the same as `requests`\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Hyperparameter Tuning",
        "Question_created_time":1684269790144,
        "Question_last_edit_time":1684616879100,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH_IZ4_dFSCalBQF9jV3ayg\/sagemaker-hyperparameter-tuning",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_body":"Hi Everyone,\n\nContext on the problem:\nI am training an SK Learn pipeline model with custom pre-processing and an XGBRegressor model under an SKLearn Estimator image in a custom bring my own training script framework. \nI have a training and validation split of my data that I am able to load in my training script, train the pipeline model, make predictions on training and validation sets, and also able to log the evaluation metrics like train:mae, val:mae etc...\nI am also able to launch a hyper-parameter tuning job with my custom metric-definitions and the job finishes successfully. The objective I choose to minimize is validaiton:mae with \"Bayesian\" optimization strategy.\n\nQuestions:\n1. Why does each training job associated with the tuning job have a 'minimum', 'maximum', 'average', 'std-dev' of the metrics? I do not perform any cross-validation in my custom training script, so where does sagemaker get these numbers?\n2. Why does the validation error that the metrics table log the minimum value? and not the average value? How does this not lead to overfitting if we drive the tuning objective to minimum while selecting the minimum error split from internal cross-validation.\n\nPlease help me understand what's going on in sagemaker hyper-parameter tuning metric logs and objective tuning. I understand bayesian optimization and sagemaker seems to be doing the optimization correctly, but it seems that sagemaker is not using the correct numbers to optimize.\n\n![![Enter image description here](\/media\/postImages\/original\/IMgxw8ddE7TdyNZ01oZpPowg)\n![![Enter image description here](\/media\/postImages\/original\/IM-RbnS6W1T_mBkbkqvzBs7w)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Supporting Data Capture on Serverless Endpoints?",
        "Question_created_time":1684224544378,
        "Question_last_edit_time":1684572324638,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6QHYRmk1SDuw_rnxTKZA8A\/supporting-data-capture-on-serverless-endpoints",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":49,
        "Question_answer_count":0,
        "Question_body":"Hi! \n\nI have understood that the Serverless Endpoints currently do not support the Data Capture functionality (according to the feature exclusions found [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html)). This is unfortunate as my use case is primed for using Serverless Endpoints, but at the same time I was hoping to leverage the ease of the data capture functionality.\n\nAre there plans on enabling this feature for Serverless Endpoints too? If yes, what is the time plan?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"HPO training using Sagemaker SDK vs Sagemaker Jupyter notebook",
        "Question_created_time":1684221816670,
        "Question_last_edit_time":1684570236327,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBRM9QX5SRuaHz4SynGPc3w\/hpo-training-using-sagemaker-sdk-vs-sagemaker-jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":74,
        "Question_answer_count":2,
        "Question_body":"Hello,\n\nWhen training an HPO using the Sagemaker SDK It's much slower than training on Sagemaker jupyter notebook - \nBoth variants have the same:\n1. Hyperparameters (the same Model)\n2. Data - Train \/ Test (exactly same data)\n3. Resources - 'ml.m5.24xlarge' machine\n\nThe SDK is slower at training and at inference (X3 more slow).\n\nCould it be due to different tensorflow versions? \nIs it a common issue? what things should I check?\n\nThanks,\nOr",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"problem in Amazon SageMaker",
        "Question_created_time":1684171453375,
        "Question_last_edit_time":1684517857771,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7tzHX_pVSsyMiuvXq8Z5dw\/problem-in-amazon-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_body":"ValidationException\n1 validation error detected\nValue '[]' at 'subnetIds' failed to satisfy constraint: Member must have length greater than or equal to 1",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to get a summary from a URL - AI21Labs",
        "Question_created_time":1684135251481,
        "Question_last_edit_time":1684483151081,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEiitUl1PQBWVmkT4LQ7mIQ\/how-to-get-a-summary-from-a-url-ai21labs",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":67,
        "Question_answer_count":2,
        "Question_body":"I have a customer request to test summarizing data from URL links and I believe AI21Labs can do that but cannot find an example with the model deployed via Jumstart. Is this possible?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"describe-instance-type-offerings\" for sagemaker instances?",
        "Question_created_time":1684135191251,
        "Question_last_edit_time":1684482941009,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMmf7T4ZpR1aG997nAKVuhQ\/describe-instance-type-offerings-for-sagemaker-instances",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"I found that ec2 instance type has available AZ and to find which AZ as available is to check describe-instance-type-offerings command.\n\nJust like same I want to deploy my AI\/ML model to custom VPC with following code.\n```\nsubnets = ['subnet-1', 'subnet-2']\nsecurity_group_ids = ['sg-1']\nother_vpc_config = {'Subnets': subnets,\n                    'SecurityGroupIds': security_group_ids}\n\npredict = mnist_estimator.deploy(initial_instance_count=1,\n                                instance_type='ml.m5.12xlarge',\n                                vpc_config_override=other_vpc_config)\n```\nBut I can't find out which AZ can use instance type that I want(for here ml.m5.12xlarge).\nIs there any command that I can find something like \"describe-instance-type-offerings\" for sagemaker?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Batch transform step not working",
        "Question_created_time":1684095824527,
        "Question_last_edit_time":1684442540742,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSdnvugC7SqSzfZBx3bL7oQ\/batch-transform-step-not-working",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":0,
        "Question_body":"I am trying to build two SageMaker pipelines. One is for training and the other is for inference. Training pipeline works and successfully registered my trained model in registry. There, I have trained a scikit-learn model with my own training script. In my inference pipeline, I am loading the model and retrieving both model_url and image_uri as follows:\n\n```\ntraining_model = Model(\n    image_uri=step_latest_model_fetch.properties.Outputs[\"ImageUri\"],\n    entry_point=\".\/train_new.py\",\n    model_data=step_latest_model_fetch.properties.Outputs[\"ModelUrl\"],\n    sagemaker_session=pipeline_session,\n    role=role,\n    env={'SAGEMAKER_PROGRAM': 'train_new.py',\n        'SAGEMAKER_CONTAINER_LOG_LEVEL':'20',\n        'SAGEMAKER_REGION':'eu-central-1'}\n)\n\nstep_model_creation = ModelStep(name=\"ModelCreationStep\",\n                          step_args=training_model.create(instance_type=\"ml.m5.xlarge\"))\n```\n\nthen transform as follows:\n\n\n```\ntransformer = Transformer(\n    model_name=step_model_creation.properties.ModelName,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    output_path=f\"s3:\/\/{bucket}\/sagemaker\/batch_transform_results\"\n)\n\n\nstep_transform = TransformStep(\n    name=\"TransformStep\",\n    transformer=transformer,\n    inputs=TransformInput(data=batch_data)\n)\n```\n\nMy cloud watch error log is as below:\n\n\n```\nTraceback (most recent call last):\n  File \"\/miniconda3\/bin\/serve\", line 8, in <module>\n    sys.exit(serving_entrypoint())\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 146, in serving_entrypoint\n    server.start(env.ServingEnv().framework_module)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/sagemaker_containers\/_server.py\", line 86, in start\n    _modules.import_module(env.module_dir, env.module_name)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/sagemaker_containers\/_modules.py\", line 253, in import_module\n    _files.download_and_extract(uri, _env.code_dir)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/sagemaker_containers\/_files.py\", line 129, in download_and_extract\n    s3_download(uri, dst)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/sagemaker_containers\/_files.py\", line 165, in s3_download\n    s3.Bucket(bucket).download_file(key, dst)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/boto3\/s3\/inject.py\", line 277, in bucket_download_file\n    return self.meta.client.download_file(\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/boto3\/s3\/inject.py\", line 190, in download_file\n    return transfer.download_file(\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/boto3\/s3\/transfer.py\", line 320, in download_file\n    future.result()\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/s3transfer\/futures.py\", line 103, in result\n    return self._coordinator.result()\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/s3transfer\/futures.py\", line 266, in result\n    raise self._exception\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/s3transfer\/tasks.py\", line 269, in _main\n    self._submit(transfer_future=transfer_future, **kwargs)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/s3transfer\/download.py\", line 354, in _submit\n    response = client.head_object(\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/botocore\/client.py\", line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"\/miniconda3\/lib\/python3.8\/site-packages\/botocore\/client.py\", line 915, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n```\n\n\n```\nbotocore.exceptions.clienterror: An error occurred (404) when calling the HeadObject operation: Not Found\n```\n\nPlease note that I am only using one script for training\/sreving that is named \"train_new.py\" and already contains the below functions:\n\n\n```\n\n# inference functions ---------------\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n\ndef input_fn(input_data, content_type):\n    \"\"\"Parse input data payload\n    We currently only take csv input. Since we need to process both labelled\n    and unlabelled data we first determine whether the label column is present\n    by looking at how many columns were provided.\n    \"\"\"\n    df = pd.read_csv(StringIO(input_data), header=None, names=[\"lemma\"])\n\n    return df[\"lemma\"].astype(str).values\n\n\ndef predict_fn(input_data, model):\n    prediction = model.predict(input_data)\n    return prediction\n\n\ndef output_fn(prediction, accept):\n    \"\"\"Format prediction output\n    The default accept\/content-type between containers for serial inference is JSON.\n    We also want to set the ContentType or mimetype as the same value as accept so the next\n    container can read the response payload correctly.\n    \"\"\"\n    if accept == \"application\/json\":\n        json_output = {\"instances\": prediction.tolist()}\n        return worker.Response(json.dumps(json_output), mimetype=accept)\n    elif accept == \"text\/csv\":\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    else:\n        raise RuntimeException(\n            \"{} accept type is not supported by this script.\".format(accept)\n        )\n\n```\n\nI would highly appreciate your help, I guess I have something wrong with my model container.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I count custom object in single image with Amazon Rekognition Custom Labels?",
        "Question_created_time":1683878073976,
        "Question_last_edit_time":1684224576822,
        "Question_link":"https:\/\/repost.aws\/questions\/QUc_tlNGe2RGSN4k_2WXBQ7g\/can-i-count-custom-object-in-single-image-with-amazon-rekognition-custom-labels",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":67,
        "Question_answer_count":2,
        "Question_body":"I need to count custom object in single image using Amazon Rekognition Custom Labels.\nFor example I have image that containing multiple balls inside. I need to count how many balls in the image?\nCan I do it with Amazon Rekognition Custom Labels?\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Models Console Error Box",
        "Question_created_time":1683867826754,
        "Question_last_edit_time":1684214637213,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzhoxPW2iQ1OgrkrZQOl4aQ\/sagemaker-models-console-error-box",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":0,
        "Question_body":"Hi!\nI found an error box in SageMaker Models, just like in the images below...\nEvery model shows that error...\n\n![Enter image description here](\/media\/postImages\/original\/IM5Llbid-9QlSYDROcQYq2ng)\n\nHowever, creating an endpoint with that model works just fine...\nIs that just a console bug or something?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Issues Deploying Models from HuggingFace Hub via Sagemaker",
        "Question_created_time":1683864707602,
        "Question_last_edit_time":1684211764310,
        "Question_link":"https:\/\/repost.aws\/questions\/QUia0nYsU7R4OLmmsJRoWsYQ\/issues-deploying-models-from-huggingface-hub-via-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":80,
        "Question_answer_count":0,
        "Question_body":"I have recently made an AWS account, and stood up a sagemaker notebook instance. I wanted to deploy a foundation model from huggingface model hub to an endpoint. I followed the code directly from the HF hub:\n\n```\nfrom sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\n# Hub Model configuration. https:\/\/huggingface.co\/models\nhub = {\n\t'HF_MODEL_ID':'databricks\/dolly-v2-7b',\n\t'HF_TASK':'text-generation'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\ttransformers_version='4.17.0',\n\tpytorch_version='1.10.2',\n\tpy_version='py38',\n\tenv=hub,\n\trole=role, \n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n\tinitial_instance_count=1, # number of instances\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\n)\n\npredictor.predict({\n\t'inputs': \"Can you please let us know more details about your \"\n})\n```\nThe above is for the dolly 2 model. No matter what model I try, though, I get a similar error after deployment. The deployment shows no errors, and the endpoint appears in my list of active endpoint. But when I query it with:\n\n```\npredictor.predict({\n\t'inputs': \"Can you please let us know more details about your \"\n})\n```\n\nI always get an error similar to this:\n\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n```\nIt seems to depend on the underlying model; dolly2 was trained on gpt-neox, and I get above error. If I used a fine-tuned version of a Meta Llama model, I get the same error but with the message: \"\\u0027llama\\u0027\". Help!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Keep sagemaker notebook up during model training?",
        "Question_created_time":1683739235526,
        "Question_last_edit_time":1684085641957,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT7JnzaUJRs-s13xiawMEFA\/keep-sagemaker-notebook-up-during-model-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_body":"I start a training job from a sagemaker notebook using boto3.client():\n\nclient = boto3.client(service_name='sagemaker')\nclient.create_training_job(**training_params)\n\nI see the training job in progress on the console. Is it then an error to stop and delete the notebook even if the model has not finished training?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1683838814257,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1683838814257,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi @fascani, no, once the training job has been created, you don't need to keep the notebook running. You can continue other explorations\/model building on the notebook, check the progress of the training job using APIs, but that's not required.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"What to do after my training job fails with \"InternalServerError\"?",
        "Question_created_time":1683705087280,
        "Question_last_edit_time":1684052807022,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnR7GJqwcT5a6T2Do_7uQOA\/what-to-do-after-my-training-job-fails-with-internalservererror",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":49,
        "Question_answer_count":0,
        "Question_body":"I have a training job with resnet-50 on a 50GB\/ml.p2.xlarge (1 instance), Pipe mode, object detection model with 1073 images in training and 268 images in validation. According to CloudWatch, it ran up to epoch 77 (for about 4 hours) but then failed with no specific message recorded in CloudWatch. I only get the dreaded \"InternalServerError: We encountered an internal error. Please try again.\" which is not okay because it costs money and I need to know what is failing.\n\nThe CPU utilization is stable at around 270% (a number that needs to be divided by the number of vCPUs which is 4 so really this is about 68% per vCPU), GPU Utilization is constant at under 60%, GPU Memory Utilization is constant at around 18%, Memory Utilization is constant at 3.2%, Disk Utilization is stable at 0.22%.\n\nIs there an obvious mistake I am doing? Thanks for the help!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"cuDNN error ml.g5.24xlarge",
        "Question_created_time":1683685764836,
        "Question_last_edit_time":1684032272555,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPzVOAmELRyOH88RvI1hF3A\/cudnn-error-ml-g5-24xlarge",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":47,
        "Question_answer_count":0,
        "Question_body":"I'm training a machine learning model on ml.g5.24xlarge:\n\nThe following code:\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Seq2Seq(encoder, decoder, device).to(device)\n\nwhen run through the model training code:\n\nmodel.train()\nfor epoch in range(10):\n    epoch_loss = 0\n    for context_batch, question_batch, answer_batch in train_loader:\n        padded_contexts = pad_sequences(context_batch, max_seq_len).to(device)\n        padded_questions = pad_sequences(question_batch, max_seq_len).to(device)\n        padded_answers = pad_sequences(answer_batch, max_seq_len).to(device)\n        optimizer.zero_grad()\n        output = model(padded_contexts, padded_questions)\n        output_dim = output.shape[-1]\n        output = output[:, 1:, :].reshape(-1, output_dim)\n        padded_answers = padded_answers[:, 1:].reshape(-1)\n        loss = criterion(output, padded_answers)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(f'Epoch {epoch+1} loss: {epoch_loss\/len(train_loader):.4f}')\n\nreturns the dreading following error:\n\/opt\/conda\/conda-bld\/pytorch_1670525539683\/work\/aten\/src\/ATen\/native\/cuda\/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n\nRuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n\nI'm running I'm running the conda_pytorch_p39 enviroment on Jupyter lab\n\nAny ideas welcome!\n\nThanks :)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Only one CPU used while parallel processing in Sagemaker",
        "Question_created_time":1683653980525,
        "Question_last_edit_time":1684001838637,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJ5lSxQ8OT6mLBGvSUxmPgw\/only-one-cpu-used-while-parallel-processing-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":49,
        "Question_answer_count":1,
        "Question_body":"I am using a Sagemaker instance a 24xlarge to process a DataFrame of about 23 GB. I tried to use Pandarallel's `parallel_apply()` function on a Pandas DataFrame with 48 workers.    \nHowever, when I run the `top` command from the Terminal, I only see one process working with around 100% CPU utilization, but nothing happens. Eventually the favicon turns to the 'Done' icon, which usually means the code is done running, but CPU util stays high and the code cell still has the asterisk next to it.   \nI would expect multiple processes each with high CPU utilization, or one process with close to %9600 utilization. The same thing happens when I try the `swifter` package and `modin` which are also supposed to parallelize Pandas functions.\nWhat's going on here and how do I fix this problem?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Serverless Inference metrics not recorded\/displayed",
        "Question_created_time":1683553604566,
        "Question_last_edit_time":1684153971153,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkvM0tvN_R3a95J-s-aPqOQ\/sagemaker-serverless-inference-metrics-not-recorded-displayed",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_body":"Sagemaker serverless inference metrics are not shown in the UI:\n![Enter image description here](\/media\/postImages\/original\/IMgBgDARVlRkCSOtJ_-BCv0w)\n\nThe container is huge 103860px in hight and if scrolled down it shows \"No widget on this dashboard.\"\n\nData appears in cloudwatch though.\n\nSeems to be an issue on AWS side here...",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"why sagemaker is not giving free micro instance , when only micor instance is free for free tier? then how to use sagemaker  free as promised ?",
        "Question_created_time":1683489968100,
        "Question_last_edit_time":1683837335991,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMdvhHykiQtahCShvyTQjDw\/why-sagemaker-is-not-giving-free-micro-instance-when-only-micor-instance-is-free-for-free-tier-then-how-to-use-sagemaker-free-as-promised",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":72,
        "Question_answer_count":1,
        "Question_body":"why sagemaker is not giving free micro instance , when only micor instance is free for free tier?\nthen how to use sagemaker  free as promised ?\nFree tier\nAs part of the AWS Free Tier, you can get started with Amazon EC2 for free. This includes 750 hours of Linux and Windows t2.micro instances (t3.micro for the regions in which t2.micro is unavailable), each month for one year. To stay within the Free Tier, use only EC2 Micro instances\nhttps:\/\/aws.amazon.com\/ec2\/pricing\/",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"is sagemaker today broken - domain can not be created more then 1 hour",
        "Question_created_time":1683324178726,
        "Question_last_edit_time":1683672431506,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeVsa1GAHQWenTTySX6JVQg\/is-sagemaker-today-broken-domain-can-not-be-created-more-then-1-hour",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":0,
        "Question_body":"at \nConfigure SageMaker Domain\n\nthere is rotating circle , and there is no access to sagemkeer coding",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"free tier sagemaket domian creation fails with 2 errors about defaultSpaceSettings.executionRole",
        "Question_created_time":1683320373803,
        "Question_last_edit_time":1683667257602,
        "Question_link":"https:\/\/repost.aws\/questions\/QU71Nxw6mmReioJlTlb2g_Lw\/free-tier-sagemaket-domian-creation-fails-with-2-errors-about-defaultspacesettings-executionrole",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":45,
        "Question_answer_count":0,
        "Question_body":"free tier sagemaket domian creation fails with 2 errors about defaultSpaceSettings.executionRole\nValidationException\n2 validation errors detected\n\u2022\tValue '' at 'defaultSpaceSettings.executionRole' failed to satisfy constraint: Member must have length greater than or equal to 20\n\u2022\tValue '' at 'defaultSpaceSettings.executionRole' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does SageMaker with Triton Inference Server support ensembles or BLS + automatic memory time sharing? How does model loading and unloading work in this case?",
        "Question_created_time":1683320228175,
        "Question_last_edit_time":1683666240385,
        "Question_link":"https:\/\/repost.aws\/questions\/QUC5sFF5iVTUi3WXvGKYpDAA\/does-sagemaker-with-triton-inference-server-support-ensembles-or-bls-automatic-memory-time-sharing-how-does-model-loading-and-unloading-work-in-this-case",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":9,
        "Question_answer_count":0,
        "Question_body":"I am wondering how SageMaker handles model loading and unloading when dealing with Triton ensemble models or Triton BLS. The model actually being invoked is the Triton ensemble, so will SageMaker automatically tell Triton to load each of the models called by the ensemble? Similarly, is Sagemaker able to tell which models are called from a particular BLS model? Is load balancing\/memory time-sharing negatively affected in this case?\n\nSecond question: if multiple ensembles have a model in common, will the number of model instances still be the number in Triton config? How does Sagemaker deal with model loading when multiple ensembles or BLS models use the same model?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Process data from the manifest file in a specific order for custom labeling",
        "Question_created_time":1683314265803,
        "Question_last_edit_time":1683660893982,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2TzPm-V5TTezr5ewmUBSxQ\/process-data-from-the-manifest-file-in-a-specific-order-for-custom-labeling",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":58,
        "Question_answer_count":0,
        "Question_body":"Hi, I am using the Sagemaker to custom a data labeling workflow, which requires to process data based on the default order from the manifest file. I found that currently the Sagemaker fetch source from the manifest file randomly. I was wondering, is there a way to process source in a specific order?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How can I tell if my Notebook instance is frozen?",
        "Question_created_time":1683306015132,
        "Question_last_edit_time":1683644479959,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK_hTODBBTeWX_c-lcO34mg\/how-can-i-tell-if-my-notebook-instance-is-frozen",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":72,
        "Question_answer_count":2,
        "Question_body":"I am running a Sagemaker Notebook instance. How can I tell if my Notebook is frozen or just taking a long time? I am using a 24xlarge and querying from Athena in parallel and it seems to be stuck on the same query for a long time. How can I tell if I need more Memory or more VCPUs?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1683362949783,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1683362949783,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi there,\n\nGreetings for the day!\n\nI understand that you wanted to know how can you determine if you need more VCPU or memory when your SageMaker Notebook Instance is frozen or just taking a long time?\n\nI\u2019d like to inform you that If your Sagemaker Notebook instance is taking a long time, you can check if it is frozen or still running by monitoring the CPU and memory usage. If the CPU usage is low or zero, it may be frozen. \n\nIf the CPU usage is high but the memory usage is low, you may need more VCPUs and If the memory usage is high, you may need more memory. \n\nYou can check it via SageMaker Notebook Instance terminal:\n\nTo see the memory and CPU information in detail , kindly follow the below instructions:-\n\n[1] Start Your Notebook Instance\n[2] Go to the  Jupyter Home Page \n[3] Right hand side ,Click on DropDown Option \u201cNew\u201d \n[4] Select \u201cTerminal\u201d.\n\nIn the Jupyter terminal, Run the below commands to see the information of Memory and CPUs.\n\n[+] To see the memory information:\n\n$ free -h\n\n=> output of \u201cfree -h\u201d will provide the information of total memory, used memory, free memory, shared memory etc in human readable form.\n\n[+] To see the CPU information, you can run any of the commands:\n\n$ mpstat -u\n\n=> Output of \u201cmpstat -u\u201d consists of different fields like %guest, %gnice, %steal etc.\n\n%steal\nShow the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hyper\u2010\nvisor was servicing another virtual processor.\n\n%guest\nShow the percentage of time spent by the CPU or CPUs to run a virtual processor.\n\n%idle\nShow the percentage of time that the CPU or CPUs were idle and the system did not have an out\u2010\nstanding disk I\/O request.\n\nmany more.. You can find more detail about the each field of mpstat command by visiting the manual page of it. To see the manual page of mpstat command, use \u201c$ man mpstst\u201d.\n\nAlong with \u201cmpstat -u\u201d, you can also try the below listed commands to get information about the cpu:\n\n$ lscpu\n$ cat \/proc\/cpuinfo\n$ top\n\nAdditionally, You can also check the cloudwatch logs for any errors or warnings that may indicate the cause of the issue. Most of the time, Cloud watch logs helps to find out the root cause of the issue.\n\nYou can find the CloudWatch logs under CloudWatch \u2192 Log Groups \u2192 \/aws\/sagemaker\/NotebookInstances -> Notebook Instance Name\n\nBased on the analysis , you can select different Notebook Instance type, You can find more detail of SageMaker Instance Here [1].\n\nI request you to kindly follow the above suggested workarounds. \n\nIf you have any difficulty or run into any issue, Please reach out to AWS Support[+] (Sagemaker), along with your issue\/use case in details and we would be happy to assist you further.\n\n[+] Creating support cases and case management - https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case \n\nI hope this information would be useful to you.\n\nThank you!\n\nREFERENCES:\n\n[1] https:\/\/aws.amazon.com\/sagemaker\/pricing\/",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":1.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Redshift ML Instance Types",
        "Question_created_time":1683292120516,
        "Question_last_edit_time":1683868274950,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfC7d9EmJTKWFUqWjSCLTyw\/redshift-ml-instance-types",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_body":"When creating a model with Redshift ML which instances are actually being used and can I specify the instance type?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1683474638197,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1683474638197,
        "Answer_comment_count":0.0,
        "Answer_body":"You cannot specify the instance type with Redshift ML. It leverages Amazon SageMaker behind the scenes which is a fully managed machine learning service. Amazon SageMaker will choose the appropriate instance type based on the model being created.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Redshift ML SageMaker Domain",
        "Question_created_time":1683292000016,
        "Question_last_edit_time":1683639542431,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHf0f_0SuS5KOSSctyn37xA\/redshift-ml-sagemaker-domain",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":47,
        "Question_answer_count":1,
        "Question_body":"Which Sagemaker Domain is used when using Redshift ML CREATE MODEL",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1683298611293,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1683298611293,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi @jkrice, Redshift ML does not use SageMaker Studio (domains). When you run a CREATE MODEL statement, it calls SageMaker to create an Autopilot job (https:\/\/aws.amazon.com\/sagemaker\/autopilot\/) to train a model. Autopilot usually does preprocessing, and trains on a variety of suitable models for your use case and finally returns the best model, that's then deployed for inference in Redshift. You can see more information here - https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/machine_learning.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Decrease SageMaker Service Quota Limit",
        "Question_created_time":1683239659022,
        "Question_last_edit_time":1683586100365,
        "Question_link":"https:\/\/repost.aws\/questions\/QUf0qiP-qpRsWd-7Ed1FuohA\/decrease-sagemaker-service-quota-limit",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_body":"Hi, can anyone advise how to decrease the SageMaker service quota? I've requested increasing the SageMaker endpoint a few months ago, but I don't need it anymore. Checked several webpages on how to set the service quota, but only find instruction to increase service quota, and didn't find documentation on how to reset the service quota to default or decrease. \n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to read a \"application\/x-recordio-protobuf\" response from our Sagemaker inference endpoint?",
        "Question_created_time":1683232388146,
        "Question_last_edit_time":1683580727368,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKtHneE44QmWX1go3t5DhqQ\/how-to-read-a-application-x-recordio-protobuf-response-from-our-sagemaker-inference-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":118,
        "Question_answer_count":1,
        "Question_body":"We've deployed a model from our AutoML job in Sagemaker to an endpoint.\n\nWhen we hit that endpoint, we receive a \"application\/x-recordio-protobuf\" response. How do we read this response?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how does inference work in multi model endpoint in sagemaker?",
        "Question_created_time":1683225884252,
        "Question_last_edit_time":1683572391399,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCi3paU16QbySyj_I1lOEPA\/how-does-inference-work-in-multi-model-endpoint-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":49,
        "Question_answer_count":0,
        "Question_body":"based on the docs provided here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint.html. i created a multi model endpoint and invoked it as documented here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html. I'm getting a Invalid model exception and the message => \"model version is not defined\" . my set up is , i have created two models , say modelOne and modelTwo.tar.gz file , and both models have their own custom script\/inference.py file with following directory structure. \n\nwhen we send request to a multi model endpoint, does sagemaker uncompresses the tar.gz file specified in the request, as in my case both models have save directory structure and same model.pth files inside the tar files , is it getting mixed up and not sure which one to invoke?\n\ninference.py\n```\nimport torch\nimport os\n\ndef model_fn(model_dir, context):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\n```\ndirectory structure\n```\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n```\n\n\n```\nresponse = runtime_sagemaker_client.invoke_endpoint(\n                        EndpointName = \"my-multi-model-endpoint\",\n                        ContentType  = \"text\/csv\",\n                        TargetModel  = \"modelOne.tar.gz\",\n                        Body         = body)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Autopilot - Explainability and performance",
        "Question_created_time":1683206160945,
        "Question_last_edit_time":1683552777210,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmOER3SYOSo2BDoP-9O6s5Q\/autopilot-explainability-and-performance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":20,
        "Question_answer_count":0,
        "Question_body":"Hi, everyone!\n\nAfter training the model with autopilot, metrics and best model performance are not shown so I can evaluate it. I had no problem with this in previous trainings. I receive the following message:\n\nPerformance has not been generated for at least one of the following reasons:\nThis model is not scored as the best model by Autopilot.\nThis model is associated with an Autopilot experiment that was created before the model performance feature launch on February 7, 2022.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I host multiple models using Nvidia Triton Business Logic Scripting behind a Multi-Model Endpoint?",
        "Question_created_time":1683201694332,
        "Question_last_edit_time":1683549503149,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCtjcLXk8TbqdRk2nbrcllg\/can-i-host-multiple-models-using-nvidia-triton-business-logic-scripting-behind-a-multi-model-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":33,
        "Question_answer_count":0,
        "Question_body":"I'd like to host models which need to use BLS on GPU-backed realtime inference with MME. I'd like to be able to scale to hundreds or thousands of such models behind one endpoint. Will this work out of the box? I know there is support for this for models which don't use BLS, but the documentation is not clear on whether an entire BLS pipeline can be treated as an individual model by the auto-scaler.\n\n\nBetter description of my use case: \n\nThe model consists of:\n- Model slice A: weights selected from a set of ~5 options, called once per invocation, nothing ever gets added or removed\n- Model slice B: weights selected from a set of 100s+ options, called in a loop n times per invocation, variants constantly added\n\n\nSo if you take this model alone, it seems to be a good candidate for Triton BLS, where each of the slices is a model instance and slice B instance is called n times in a loop in the pipeline file.\n\n\nMy first thought was to add all the model variants to a single BLS model repo, but I am not sure if this would work with auto-scaling and frequently added new models. \n\n\nThe other possibility is to split it into m BLS pipelines, where m is the number of variants I have for model slice B. This should work with auto-scaling, since the model target should match up with the request destination model, but I am not sure if this is supported (an entire BLS model hierarchy being loaded and unloaded in Sagemaker).\n\n\nThe next best option I guess is to not use MME? And maybe switch to MCE? But this seems like a big loss in performance.\n\n\nWhat is the best way to deploy this model using Sagemaker tools?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"multi model endpoints in sagemaker?",
        "Question_created_time":1683139621094,
        "Question_last_edit_time":1683487265310,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTeSVZgN7RlKG8JS31zlE6A\/multi-model-endpoints-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":2,
        "Question_body":"is there a list of containers documented somewhere , that support multi model endpoints ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"copy the contents of a Sagemaker notebook instance to an S3 bucket from AWS CLI",
        "Question_created_time":1683043399499,
        "Question_last_edit_time":1683389956260,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMeZxHljPS2q4V11A18PHVg\/copy-the-contents-of-a-sagemaker-notebook-instance-to-an-s3-bucket-from-aws-cli",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":92,
        "Question_answer_count":1,
        "Question_body":"How to copy the contents of a Sagemaker notebook instance to an S3 bucket from AWS CLI, before deleting the instance ?\nPlease suggest !\n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I terminate SageMaker Services",
        "Question_created_time":1683031159740,
        "Question_last_edit_time":1683376942684,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6ExVK9bDTS2GgzToIS8Szw\/how-do-i-terminate-sagemaker-services",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":37,
        "Question_answer_count":1,
        "Question_body":"Hello.\n\nI had unexpected billing due to some SageMaker instances still running (see screenshot).\n\n![SageMaker billing overview](\/media\/postImages\/original\/IMj25VhkM3TiaUiMAXWdl5KQ)\n\nI don't know how to terminate it. I have followed the documentation on how to find and terminate running instances. I've looked at my billing dashboard and found that there are still running processes associated with SageMaker.\n\nI've deleted resources associated with the respective regions including endpoints \/ models \/ notebook instances \/ s3buckets \/ cloudwatch log groups  \/  domains  \/ users\n\nI thought it would be safe to delete the domains and users, although that might have been a bad call on my part.\n\nI would really like to STOP the SageMaker instances. What should I do. Please help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemker Async inference doesn't run with large payloads and no errors getting logged",
        "Question_created_time":1682736176957,
        "Question_last_edit_time":1683083365940,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7pvwjw5cRVSffgOgjJsivw\/sagemker-async-inference-doesn-t-run-with-large-payloads-and-no-errors-getting-logged",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":87,
        "Question_answer_count":1,
        "Question_body":"I have an async inference endpoint that I'm calling with \n```\npredict_async()\n```\nWhen I call this endpoint with a small 650 KB CSV, everything works fine. But when I call the endpoint with a larger 14.1 MB CSV, the input goes to the respective S3 location, but the actual inference code doesn't seem to run at all. There is no errors in cloudwatch or anything\nI also have print statements that get printed as soon as the inference code is ran, but still, nothing in cloudwatch is getting printed. \nOnce this happens, nothing can be ran through my Async endpoint. The input will continue to get logged into S3, but my inference code will not get ran. Again this is happening with no errors in cloudwatch, so I have no idea what could be causing this.\nAny help here is appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Greengrass own component deployment stuck \"in progress\"",
        "Question_created_time":1682711108991,
        "Question_last_edit_time":1683057468614,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlDu9VAj4Qx-O7cbAXDz28w\/greengrass-own-component-deployment-stuck-in-progress",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_body":"I'm trying to deploy my own GreengrassV2 components. It's a SageMaker ML model (optimized with SageMakerNeo and packaged as a Greengrass component) and the according inference app. I was trying to deploy it to my core device with SageMaker Edge Manager component. But it is always stuck in the status \"In progress\".\n\nMy logs show this error:\ncom.aws.greengrass.tes.CredentialRequestHandler: Error in retrieving AwsCredentials from TES. {iotCredentialsPath=\/role-aliases\/edgedevicerolealias\/credentials, credentialData=TES responded with status code: 403. Caching response. {\"message\":\"Access Denied\"}}\n\nBut how do I know which policies are missing?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1682713832573,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1682713832573,
        "Answer_comment_count":0.0,
        "Answer_body":"Hello, please refer to https:\/\/docs.aws.amazon.com\/greengrass\/v2\/developerguide\/troubleshooting.html#token-exchange-service-credentials-http-403 for troubleshooting,  you'll need `iot:AssumeRoleWithCertificate` permissions on your core device's AWS IoT role alias",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"python show plots for remote or online work on AWS?",
        "Question_created_time":1682702965410,
        "Question_last_edit_time":1683050091451,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvASY04eNR7W9Hov5iHpmLg\/python-show-plots-for-remote-or-online-work-on-aws",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":71,
        "Question_answer_count":1,
        "Question_body":"for coding plain python code for VSCode  is it possible to show plots for remote or online work on AWS? \nit is known using jupyter is not professional .... professional use IDEs like VScode or Charm , etc..\nhttps:\/\/www.youtube.com\/watch?v=1ISrRp6n2Tg\nJoel Grus - The case against the jupyter notebook (TDS Podcast - CLIP)\nhttps:\/\/www.youtube.com\/watch?v=3Fa6uzHxTkQ\nI Don't Like Jupyter Notebooks \/\/ Joel Grus \/\/ Coffee Sessions #62\nhttps:\/\/www.youtube.com\/watch?v=lMlN-2W1rxE\n\nfor example \n we set break point at line number 123\nrun python code till this breakpoint \nthen write code in debugging window of terminal \nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 15))\nplt.plot( [1,2,3,4,5], 'r', label='actual - reference')\nplt.plot( [1.2,2.6,2.8,4.2,5.1],'b', label='predicted ') \nplt.legend()\nplt.title(\"vowpal wabbit predictions2\")\nplt.show(block=False)\n \nthen we want to see plot on computer screen and if needed to zoom this plot - we want interactive plot as full experience coding on local computer ?\n\ncloud9 is only limited IDE per\nhttps:\/\/stackoverflow.com\/questions\/15089174\/support-for-cloud9-ide-and-matplotlib-or-other-graphical-tool-for-python\n\nplots can not be shown",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker issue: cannot start kernel",
        "Question_created_time":1682700162487,
        "Question_last_edit_time":1683047057180,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPMLc7uWPQiak4kE059z3Hw\/sagemaker-issue-cannot-start-kernel",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":55,
        "Question_answer_count":1,
        "Question_body":"Hello, I have a problem with SageMaker: I tried to start kernel with ml.g5.12xlarge instance, but it failed with InternalFailure error. After a couple of unsuccessful attempts I started to get another error about account-level service limit. My limit is 1, and it says I already have 1 instance running. However, I do not see any running instances in the list of running instances. So now I am stuck. Could someone from AWS technical support help me please?\n\n![InternalFailure error](\/media\/postImages\/original\/IMJCFeJCj6QK-_LtXEXW7GYQ)\n\n\nFailed to start kernel\nFailed to launch app [pytorch-1-13-cpu-py-ml-g5-12xlarge-9b5704dda36ab08eb9a29af41aed]. ResourceLimitExceeded: The account-level service limit 'Studio KernelGateway Apps running on ml.g5.12xlarge instance' is 1 Apps, with current utilization of 1 Apps and a request delta of 1 Apps. Please contact AWS support to request an increase for this limit. (Context: RequestId: 03518865-dde3-493e-8751-b631cf7e80ea, TimeStamp: 1682699325.895717, Date: Fri Apr 28 16:28:45 2023)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Where to find updated manifest file after labeling a dataset using a premade manifest for a Rekognition Custom Labels project without training a model?",
        "Question_created_time":1682636174386,
        "Question_last_edit_time":1682983647105,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGJnnynrdT3SdFt_XAnDwzw\/where-to-find-updated-manifest-file-after-labeling-a-dataset-using-a-premade-manifest-for-a-rekognition-custom-labels-project-without-training-a-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_body":"Hi!\n\nI have a training set of images for which I manually created a manifest file respecting the format required to train a Rekognition Custom Labels model for object detection. Both the images and that manifest are stored in an S3 bucket. When creating a project to train the model I noticed that the manifest contained some errors which I corrected manually (\"Start labeling\" then \"Draw bounding boxes\") but the updated manifest is nowhere to be found.\n\nI made sure to save the changes and when I browse through the dataset as it appears during the creation of a project I noticed that my changes were taken into account. I want the manifest so that I can store it locally on my computer but I can't seem to find it. Any help is very much appreciated.\n\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Connect Lambda function within VPC to Sagemaker Serverless Inference Endpoint",
        "Question_created_time":1682612681713,
        "Question_last_edit_time":1682959694208,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3hIvhm-IRiG4uElDlq7jBg\/connect-lambda-function-within-vpc-to-sagemaker-serverless-inference-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":53,
        "Question_answer_count":1,
        "Question_body":"Hello! \n\nI have a Lambda function which needs to connect to a MySQL RDS instance which is within a private VPC. The Lambda function also needs to connect to a Sagemaker Serverless Inference Endpoint, taking data from the RDS instance and passing it to a Sagemaker model for inference. \n\nI have tested a vanilla Lambda function from outside of the private VPC to connect to the Sagemaker Endpoint which was successful. However, in order for Lambda to access data from the RDS database, I have associated the Lambda function with the same private VPC and subnets as the RDS instance. As a result, the Lambda function can no longer access the Sagemaker Endpoint as it now lies within the private VPC. \n\nI need advice on **how to create a VPC Interface Endpoint** to enable the Lambda function to access the Sagemaker Endpoint, and on** how to build the associated security groups** to enable the network access. I am unsure of how to implement the VPC Interface Endpoint or how to implement the correct security groups, so I would appreciate advice on the implementation. \n\nAs an alternative architecture, I could create two lambda functions: one which is associated with the private VPC and connects to the RDS instance then stores the required data in an S3 bucket; and another lambda function which is not within the VPC which pulls the data from the S3 bucket and serves it to the Sagemaker Endpoint. **Would this be a better option? **",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How can I deploy an Amazon SageMaker model to a different AWS account?",
        "Question_created_time":1682609694529,
        "Question_last_edit_time":1682955885792,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUueSXw0cT9mGNKOeu2P09A\/how-can-i-deploy-an-amazon-sagemaker-model-to-a-different-aws-account",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_body":"I'm training an Amazon SageMaker model in one AWS account. I want to deploy this model to a different AWS account and use the model arn in AWS Seller Marketplace in the (Launch option)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SSLError: SSL validation failed for https:\/s3\/endpoints\/invocations EOF occurred in violation of protocol (_ssl.c:2396) in sagemaker",
        "Question_created_time":1682608832031,
        "Question_last_edit_time":1682955799803,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpoYNwj0kSp63DSd4UdXLSw\/sslerror-ssl-validation-failed-for-https-s3-endpoints-invocations-eof-occurred-in-violation-of-protocol-ssl-c-2396-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":73,
        "Question_answer_count":1,
        "Question_body":"I trained a deep neural network using sagemaker and deployed the endpoint successfully. But, when trying to get the predictions on test data, i am getting \"SSLError: SSL validation failed for https:\/\/runtime.s3.amazonaws.com\/endpoints\/[endpointName]\/invocations EOF occurred in violation of protocol (_ssl.c:2396)\"\n\n i am able to get predictions up to 10 records, afterwards i am getting this error.please help me.\n\n\n```\nfrom sagemaker.tensorflow import TensorFlowPredictor\npredictor = TensorFlowPredictor(endpoint_name)\n\n# invoke model endpoint to make inference\npred = predictor.predict(input_vals)\n    \n# store predictions\ny_pred.extend([i[0] for i in pred['predictions']])\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to add custom metadata to a model in model registry via sagemaker pipeline step?",
        "Question_created_time":1682544198301,
        "Question_last_edit_time":1682891672338,
        "Question_link":"https:\/\/repost.aws\/questions\/QU26CweYmuQP2BNVjdC8zTUg\/how-to-add-custom-metadata-to-a-model-in-model-registry-via-sagemaker-pipeline-step",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_body":"based on sagemaker python sdk  , i can add custom\/customer metadata via (sample code below). i am planning to use sagemaker model step and register the model in a model package group and create an end to end pipeline. is it possible to add these metadata at the time of model creation or registering model ?\n\n```\nboto3.client('sagemaker').update_model_package( ... \nCustomerMetadataProperties = {'key1': 'value1', 'key2': 'value2'})\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1682634634329,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1682634634329,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, it is possible. You can add custom metadata when you use the [register](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.register) method of the Model object in the SageMaker SDK. The [Model step](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-model) from SageMaker Pipelines can then be used together with this register method. \n\nBelow is a snippet of code copied from the example provided on [this page](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-model), where I have added the `custom_metadata_properties` parameter with your example input. \n \n```\nregister_model_step_args = pipeline_model.register(\n    content_types=[\"application\/json\"],\n   response_types=[\"application\/json\"],\n   inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n   transform_instances=[\"ml.m5.xlarge\"],\n   model_package_group_name=\"sipgroup\",\n   customer_metadata_properties={\"key1\": \"value1\", \"key2\": \"value2\"}\n)\n\nstep_model_registration = ModelStep(\n   name=\"AbaloneRegisterModel\",\n   step_args=register_model_step_args,\n)\n```",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Pipeline triggered from GitHub Actions",
        "Question_created_time":1682540594854,
        "Question_last_edit_time":1682887936987,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqZ5v2P0bQsSs53WxMuNoig\/sagemaker-pipeline-triggered-from-github-actions",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":56,
        "Question_answer_count":1,
        "Question_body":"Here is the process that we are thinking of.\n1. Data scientist creates a Pull Request in the GitHub repository, suppose, https:\/\/github.com\/my-org\/ml-project.\n2. GitHub actions is triggered, finished all the unit tests and vulnerabilities testing.\n3. If everything gets passed, then the SageMaker pipeline will be triggered.\n* Pre-processing of data\n* Training job\n* Evaluation and Check\n* Registering to Model registry\n* Creating Endpoint for Model serving\n\nDoes this process look good and is this achievable? Do we have any helpful contents, use-cases, code bases for similar kind of project, which we can take reference of? Can someone layout the steps, especially for 3rd point.\nThank you in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Accessing DynamoDb data from Sagemaker",
        "Question_created_time":1682539673791,
        "Question_last_edit_time":1682886971732,
        "Question_link":"https:\/\/repost.aws\/questions\/QUowJvrZpiRjOS5v0OhDkEXQ\/accessing-dynamodb-data-from-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":76,
        "Question_answer_count":3,
        "Question_body":"I have some data in my DynamoDb database and I want to access it in my Sagemaker notebook. To my surprise having done some research it looks like I need to transfer the data from DynamoDb into an S3 bucket and access the data from a csv file in there. Is that correct? What is the best way of doing this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Need guidance on which GPU sagemaker instance is suitable for my huggingface model",
        "Question_created_time":1682400922967,
        "Question_last_edit_time":1682747713883,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTbrr1mX_SwSomEG2n5cgyg\/need-guidance-on-which-gpu-sagemaker-instance-is-suitable-for-my-huggingface-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":59,
        "Question_answer_count":1,
        "Question_body":"We want to use huggingface whisper large hindi model on amazon sagemaker instance, https:\/\/huggingface.co\/vasista22\/whisper-hindi-large-v2. The model requires atleast 16GB GPU Memory, and we want to use it along with other hugging face models so we might need additional gpu memory. We are facing Resource Limit Exceeded error. Please let us know which resource to use, and please increase my limit accordingly.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker error on \"Open Launcher\" butto",
        "Question_created_time":1682346391156,
        "Question_last_edit_time":1682693128793,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsmB83BS-QmWhaNr0W3r5cw\/sagemaker-error-on-open-launcher-butto",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":30,
        "Question_answer_count":0,
        "Question_body":"Simply using the Home\/Launcher button called \"Open Launcher\" or the Launcher \"Create notebook\" button throws an error trying the open the jupyte![Enter image description here](\/media\/postImages\/original\/IMsUW0bAfSQ5S-tnlzUXr9tw)\nr notebook. \n\nBut there is nothing wrong the the Sagemaker Domain or User profile because using the button: \nFile > New > notebook\nit does work just fine.\n\nWhen using the \"Open Launcher\" button, AWS CloudWatch logs for the \"JupyterServer\/default\" says:\n\n```\n[I 2023-04-24 14:23:30.441 ServerApp.AppManager] Creating a new App. resource_spec: ResourceSpec(image_arn='arn:aws:sagemaker:eu-west-1:470317259841:image\/python-3.6', instance_type='ml.t3.medium', lifecycle_config_arn='no-script')\n[W 2023-04-24 14:23:30.445 ServerApp.RemoteKernelManagerV2] Error encountered error during app creation for kernel. err: list index out of range\n    Traceback (most recent call last):\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_nb2kg\/managers\/kernel_manager_v2.py\", line 349, in _get_remote_app_for_kernel\n        app_details, app_ctx = await self._app_manager.get_remote_app(\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_nb2kg\/managers\/app_manager.py\", line 112, in get_remote_app\n        await self._create_app(app_details, app_ctx)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_nb2kg\/managers\/app_manager.py\", line 483, in _create_app\n        app_name = generate_app_name(\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_nb2kg\/util\/app_name.py\", line 34, in generate_app_name\n        lcc_name = lcc_arn.split(\"\/\")[1] if lcc_arn is not None else None\n    IndexError: list index out of range\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cannot compile a mixed model with Huggingface and normal Pytorch content and AWS Neuron",
        "Question_created_time":1682327309846,
        "Question_last_edit_time":1684878974849,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpBXGISe-Rci6gwnluxRRoQ\/cannot-compile-a-mixed-model-with-huggingface-and-normal-pytorch-content-and-aws-neuron",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":94,
        "Question_answer_count":1,
        "Question_body":"I am initializing an `inf1.6xlarge` Sagemaekr instance via\n\n```\nsudo tee \/etc\/yum.repos.d\/neuron.repo > \/dev\/null <<EOF\n[neuron]\nname=Neuron YUM Repository\nbaseurl=https:\/\/yum.repos.neuron.amazonaws.com\nenabled=1\nmetadata_expire=0\nEOF\n\nsudo rpm --import https:\/\/yum.repos.neuron.amazonaws.com\/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n\n# Update OS packages \nsudo yum update -y\n\n# Install OS headers \nsudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y\n\n# Install git \nsudo yum install git -y\n\n# install Neuron Driver\nsudo yum install aws-neuronx-dkms-2.* -y\n\n# Install Neuron Tools \nsudo yum install aws-neuronx-tools-2.* -y\n\n# Add PATH\nexport PATH=\/opt\/aws\/neuron\/bin:$PATH\n\n# Install Python venv \nsudo yum install -y python3.7-venv gcc-c++ \n\n# Create Python venv\nyes | conda create --name aws_neuron_venv_pytorch python=3.7\n#python3.7 -m venv aws_neuron_venv_pytorch \n\n# Activate Python venv \nsource activate aws_neuron_venv_pytorch\n#source aws_neuron_venv_pytorch\/bin\/activate \n\npython -m pip install -U pip \n\n# Install Jupyter notebook kernel\npip install ipykernel \npython3.7 -m ipykernel install --user --name aws_neuron_venv_pytorch --display-name \"Python (torch-neuron)\"\npip install jupyter notebook\npip install environment_kernels\n\n# Set pip repository pointing to the Neuron repository \npython -m pip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\n\n# Install PyTorch Neuron\npython -m pip install torch-neuron neuron-cc[tensorflow] \"protobuf\" torchvision transformers sagemaker\n\n```\n\nthen I use a Jupyter Notebook and run\n\n```\nimport os\n#import tensorflow  # to workaround a protobuf version conflict issue\nimport torch\nimport torch.neuron\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\n\nfrom transformers import BloomTokenizerFast, BloomForCausalLM\n\n\nmodel_id = \"bigscience\/bloom-560m\"\ntokenizer = BloomTokenizerFast.from_pretrained(model_id, )\nmodel = BloomForCausalLM.from_pretrained(model_id)\n\n\nclass Net(nn.Module):\n    def __init__(self, pre_trained_model):\n        super(Net, self).__init__()\n        self.pre_trained_model = pre_trained_model\n        \n\n    def forward(self, text, attention_mask):\n        token_length  = int(torch.sum(attention_mask).item())\n        inputs, out = text[-token_length:].unsqueeze(0), []\n        \n        model_out = self.pre_trained_model(input_ids=inputs)\n        logits = model_out.logits[:, -1, :]\n        logits = logits * torch.tensor([range(250880)])\n\n        log_probs = F.softmax(logits, dim=-1)\n        input_token = torch.multinomial(log_probs, 1)\n\n\n        out.append(input_token.item())\n        print(out)\n        return torch.tensor(out)\n\n\nn = Net(pre_trained_model=model)\nn.eval()\n\ndummy_input = \"Dummy input which will be padded later\"\nmax_length = 128\nembeddings = tokenizer(dummy_input, max_length=max_length, padding=\"max_length\",return_tensors=\"pt\")\n\nneuron_inputs = (embeddings['input_ids'][0], embeddings['attention_mask'][0])\n\nneuron_net = torch.neuron.trace(n, neuron_inputs, compiler_workdir=\".\/workdir\", separate_weights=True)\n```\n\nThen I get the following error\n\n```\n\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/ipykernel\/__main__.py:8: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/ipykernel\/__main__.py:13: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/ipykernel\/__main__.py:19: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/ipykernel\/__main__.py:21: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n[250068]\n[250068]\n[250068]\nINFO:Neuron:There are 2 ops of 2 different types in the TorchScript that are not compiled by neuron-cc: aten::__or__, aten::embedding, (For more information see https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/release-notes\/compiler\/neuron-cc\/neuron-cc-ops\/neuron-cc-ops-pytorch.html)\nINFO:Neuron:Number of arithmetic operators (pre-compilation) before = 1820, fused = 1774, percent fused = 97.47%\nINFO:Neuron:Compiling function _NeuronGraph$2460 with neuron-cc; log file is at \/home\/ec2-user\/SageMaker\/workdir\/129\/graph_def.neuron-cc.log\nINFO:Neuron:Compiling with command line: '\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/bin\/neuron-cc compile \/home\/ec2-user\/SageMaker\/workdir\/129\/model --framework TENSORFLOW --pipeline compile SaveTemps --output \/home\/ec2-user\/SageMaker\/workdir\/129\/graph_def.neff --verbose 35'\nhuggingface\/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nINFO:Neuron:Compile command returned: 1\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$2460; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/bin\/neuron-cc compile \/home\/ec2-user\/SageMaker\/workdir\/129\/model --framework TENSORFLOW --pipeline compile SaveTemps --output \/home\/ec2-user\/SageMaker\/workdir\/129\/graph_def.neff --verbose 35\nTraceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/torch_neuron\/convert.py\", line 414, in op_converter\n    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/torch_neuron\/decorators.py\", line 264, in trace\n    'neuron-cc failed with the following command line call:\\n{}'.format(command))\nsubprocess.SubprocessError: neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/aws_neuron_venv_pytorch\/bin\/neuron-cc compile \/home\/ec2-user\/SageMaker\/workdir\/129\/model --framework TENSORFLOW --pipeline compile SaveTemps --output \/home\/ec2-user\/SageMaker\/workdir\/129\/graph_def.neff --verbose 35\nINFO:Neuron:Number of arithmetic operators (post-compilation) before = 1820, compiled = 0, percent compiled = 0.0%\nINFO:Neuron:The neuron partitioner created 1 sub-graphs\nINFO:Neuron:Neuron successfully compiled 0 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 0.0%\nINFO:Neuron:Compiled these operators (and operator counts) to Neuron:\nINFO:Neuron:Not compiled operators (and operator counts) to Neuron:\nINFO:Neuron: => aten::Int: 444 [supported]\nINFO:Neuron: => aten::ScalarImplicit: 1 [supported]\nINFO:Neuron: => aten::__or__: 1 [not supported]\nINFO:Neuron: => aten::add: 98 [supported]\nINFO:Neuron: => aten::arange: 2 [supported]\nINFO:Neuron: => aten::baddbmm: 24 [supported]\nINFO:Neuron: => aten::bitwise_not: 1 [supported]\nINFO:Neuron: => aten::bmm: 24 [supported]\nINFO:Neuron: => aten::copy_: 1 [supported]\nINFO:Neuron: => aten::cumsum: 1 [supported]\nINFO:Neuron: => aten::detach: 2 [supported]\nINFO:Neuron: => aten::dropout: 72 [supported]\nINFO:Neuron: => aten::embedding: 1 [not supported]\nINFO:Neuron: => aten::empty: 1 [supported]\nINFO:Neuron: => aten::expand: 2 [supported]\nINFO:Neuron: => aten::floor_divide: 24 [supported]\nINFO:Neuron: => aten::layer_norm: 50 [supported]\nINFO:Neuron: => aten::linear: 97 [supported]\nINFO:Neuron: => aten::lt: 1 [supported]\nINFO:Neuron: => aten::masked_fill: 24 [supported]\nINFO:Neuron: => aten::mul: 243 [supported]\nINFO:Neuron: => aten::ones: 1 [supported]\nINFO:Neuron: => aten::permute: 48 [supported]\nINFO:Neuron: => aten::pow: 1 [supported]\nINFO:Neuron: => aten::reshape: 97 [supported]\nINFO:Neuron: => aten::select: 72 [supported]\nINFO:Neuron: => aten::size: 174 [supported]\nINFO:Neuron: => aten::slice: 83 [supported]\nINFO:Neuron: => aten::softmax: 24 [supported]\nINFO:Neuron: => aten::sub: 1 [supported]\nINFO:Neuron: => aten::tanh: 24 [supported]\nINFO:Neuron: => aten::to: 28 [supported]\nINFO:Neuron: => aten::transpose: 48 [supported]\nINFO:Neuron: => aten::unsqueeze: 9 [supported]\nINFO:Neuron: => aten::view: 96 [supported]\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_3923\/821702101.py in <module>\n      3 # Trace a module (implicitly traces `forward`) and constructs a\n      4 # `ScriptModule` with a single `forward` method\n----> 5 neuron_net = torch.neuron.trace(n, neuron_inputs, compiler_workdir=\".\/workdir\", separate_weights=True)\n      6 #neuron_net = torch.neuron.trace(n, neuron_inputs, separate_weights=True, subgraph_builder_function=subgraph_builder_function)\n\n~\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/torch_neuron\/convert.py in trace(func, example_inputs, fallback, op_whitelist, minimum_segment_size, subgraph_builder_function, subgraph_inputs_pruning, skip_compiler, debug_must_trace, allow_no_ops_on_neuron, compiler_workdir, dynamic_batch_size, compiler_timeout, single_fusion_ratio_threshold, _neuron_trace, compiler_args, optimizations, separate_weights, verbose, **kwargs)\n    215         logger.debug(\"skip_inference_context - trace with fallback at {}\".format(get_file_and_line()))\n    216         neuron_graph = cu.compile_fused_operators(neuron_graph, **compile_kwargs)\n--> 217     cu.stats_post_compiler(neuron_graph)\n    218 \n    219     # Wrap the compiled version of the model in a script module. Note that this is\n\n~\/anaconda3\/envs\/aws_neuron_venv_pytorch\/lib\/python3.7\/site-packages\/torch_neuron\/convert.py in stats_post_compiler(self, neuron_graph)\n    529         if succesful_compilations == 0 and not self.allow_no_ops_on_neuron:\n    530             raise RuntimeError(\n--> 531                 \"No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\")\n    532 \n    533         if percent_operations_compiled < 50.0:\n\nRuntimeError: No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\n```\n\nBelow in the first \"answer\" I also added some logs.\n\nIt would be great if you could tell me what I am doing wrong here. \ud83d\ude42",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Make inner queue of an async SageMaker endpoint FIFO",
        "Question_created_time":1682108119722,
        "Question_last_edit_time":1682455350800,
        "Question_link":"https:\/\/repost.aws\/questions\/QUciDy04FETl-Ye5WmRbrF7w\/make-inner-queue-of-an-async-sagemaker-endpoint-fifo",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":35,
        "Question_answer_count":1,
        "Question_body":"Hi there,\n\nI was wondering if an inner queue of an async endpoint can be FIFO, because I send a lot of invocations to the endpoint and as the amount of requests grow so does the total processing time. It's still fast, but it'll be much better if it could process in a FIFO way so the oldest requests don't time out.\n\nIf that queue can't be changed, is there another way to make it process as a FIFO?\n\n![In green the queue I'd like to modify](\/media\/postImages\/original\/IMKoRl04DbQ1SUOet7sP52Ew)\n\nBeforehand, thank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to save a model of LightGBM in sagemaker with a custom name and how to reload it using python",
        "Question_created_time":1682068950099,
        "Question_last_edit_time":1682415489159,
        "Question_link":"https:\/\/repost.aws\/questions\/QU85T5U4BFRpmkoZsSeFUPdg\/how-to-save-a-model-of-lightgbm-in-sagemaker-with-a-custom-name-and-how-to-reload-it-using-python",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"I have train a ML model in sagemaker with algorithm LightGBM and I am not able to load it locally using python and when training it on sagemaker from python (locally instance) can I decide the folder name which is created and model is saved and why these things are not mentioned in it documentation.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Inference Endpoint Creation Failure",
        "Question_created_time":1682052721804,
        "Question_last_edit_time":1682398902888,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMdwA0qHKT3m_5Jt1bM3kQw\/sagemaker-inference-endpoint-creation-failure",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":4,
        "Question_view_count":27,
        "Question_answer_count":0,
        "Question_body":"I'm trying to launch an inference endpoint on an ml.g4dn.2xlarge with a ~44GB model image. Every time I try to create the endpoint, it is in the creating state for 30-40 minutes and then fails with an error that says \"Request to service failed. If failure persists after retry, contact customer support.\" There are no logs available in Cloudwatch. I get the same error for both real time and async endpoints.\n\nI can use the same image in a Batch Transform job and the service starts up without throwing errors. \n\nI'd love to know how to solve this issue, but I'd settle for a way to get debugging information.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cannot install Python package on SageMaker",
        "Question_created_time":1682044905940,
        "Question_last_edit_time":1682390903138,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh8krHZokSheAOAhOni-aYA\/cannot-install-python-package-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":64,
        "Question_answer_count":1,
        "Question_body":"I am trying to install CKG Python library on my SageMaker Studio, following this instruction, https:\/\/ckg.readthedocs.io\/en\/latest\/intro\/getting-started-with-build.html\n\nI changed 'python setup.py install' to 'pip install .'\n\nbut I still get an error\n\n\n```\ners\n            creating build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/__init__.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/acent.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/cheshire_tomography.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/circuits.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/image_processing.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/numpy_test.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/optimal_control.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/stock_tradeoff.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/test.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            copying examples\/advanced\/xpress_example.py -> build\/lib.linux-x86_64-cpython-37\/examples\/advanced\n            creating build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/__init__.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/commodity_flow.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/create_graph.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/incidence_matrix.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/leaky_edges.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            copying examples\/flows\/max_flow.py -> build\/lib.linux-x86_64-cpython-37\/examples\/flows\n            running build_ext\n            building '_cvxcore' extension\n            creating build\/temp.linux-x86_64-cpython-37\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\/python\n            creating build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\/src\n            gcc -pthread -B \/opt\/conda\/envs\/BookEnv_01\/compiler_compat -Wl,--sysroot=\/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Icvxpy\/cvxcore\/src\/ -Icvxpy\/cvxcore\/python\/ -Icvxpy\/cvxcore\/include\/ -I\/opt\/conda\/envs\/BookEnv_01\/include\/python3.7m -I\/tmp\/pip-build-env-21op54p3\/overlay\/lib\/python3.7\/site-packages\/numpy\/core\/include -c cvxpy\/cvxcore\/python\/cvxcore_wrap.cpp -o build\/temp.linux-x86_64-cpython-37\/cvxpy\/cvxcore\/python\/cvxcore_wrap.o -O3\n            error: command 'gcc' failed: No such file or directory: 'gcc'\n            [end of output]\n      \n        note: This error originates from a subprocess, and is likely not a problem with pip.\n        ERROR: Failed building wheel for cvxpy\n      Failed to build cvxpy\n      ERROR: Could not build wheels for cvxpy, which is required to install pyproject.toml-based projects\n      Traceback (most recent call last):\n        File \"<string>\", line 36, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"\/home\/sagemaker-user\/CKG\/setup.py\", line 52, in <module>\n          include_package_data=True,\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 107, in setup\n          return distutils.core.setup(**attrs)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 185, in setup\n          return run_commands(dist)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 201, in run_commands\n          dist.run_commands()\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 969, in run_commands\n          self.run_command(cmd)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 1243, in run_command\n          super().run_command(command)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 988, in run_command\n          cmd_obj.run()\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/wheel\/bdist_wheel.py\", line 378, in run\n          self.run_command(\"install\")\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/cmd.py\", line 318, in run_command\n          self.distribution.run_command(command)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 1243, in run_command\n          super().run_command(command)\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 988, in run_command\n          cmd_obj.run()\n        File \"\/home\/sagemaker-user\/CKG\/setup.py\", line 15, in run\n          check_call(\"pip install -r requirements.txt\".split())\n        File \"\/opt\/conda\/envs\/BookEnv_01\/lib\/python3.7\/subprocess.py\", line 363, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['pip', 'install', '-r', 'requirements.txt']' returned non-zero exit status 1.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for CKG\n  Running setup.py clean for CKG\nFailed to build CKG\nERROR: Could not build wheels for CKG, which is required to install pyproject.toml-based projects\n```\nPlease kindly help me figure this one out.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to use private AWS docker images to build our custom images",
        "Question_created_time":1681994600930,
        "Question_last_edit_time":1682342011571,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHjGKGMXbRtuNxVjF0kON1A\/how-to-use-private-aws-docker-images-to-build-our-custom-images",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":2,
        "Question_body":"There are some private AWS docker images like the one used by sagemaker for environment sagemaker-sparkmagic. I know the ARN, which is arn:aws:sagemaker:eu-west-1:470317259841:image\/sagemaker-sparkmagic.\n\nI need to install extra packages in that image, I just want to extend it with my own Dockerfile. However I'm not able to docker pull that image. I tried the following command\n\n```\n- docker pull 470317259841.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-sparkmagic\n```\n\nLoging in docker using:\n\n```\n- aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 470317259841.dkr.ecr.eu-west-1.amazonaws.com\n```\n\n\nI also tried to list the available images in there using the following, but it returns ACCESS DENIED:\n\n```\ncurl -X GET -u AWS:$(AWS_PROFILE=at3w aws ecr get-login-password --region eu-west-1) https:\/\/470317259841.dkr.ecr.eu-west-1.amazonaws.com\/v2\/_catalog\n```\n\nIs it possible what I am trying to do?\n\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I compile a model for AWS Inferentia with more complex input_shape?",
        "Question_created_time":1681941989757,
        "Question_last_edit_time":1682288829318,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPtqCFfY-RmaykVF2CdDytw\/can-i-compile-a-model-for-aws-inferentia-with-more-complex-input-shape",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":26,
        "Question_answer_count":0,
        "Question_body":"I have an ML model from Huggingface, which essentially looks as follows:\n\n```\nimport torch\nfrom transformers import BloomTokenizerFast, BloomForCausalLM\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ntokenizer = BloomTokenizerFast.from_pretrained(\"bigscience\/bloom-560m\")\nmodel = BloomForCausalLM.from_pretrained(\"bigscience\/bloom-560m\").to(device)\n\ntext = tokenizer.encode(seed)\ninputs, past_key_values = torch.tensor([text[0]]), None\n\nwith torch.no_grad():\n    while #condition met:\n        model_out = model(input_ids=inputs.to(device), past_key_values=past_key_values)\n        ...\n        # Generate new inputs and go back to the start\n```\n\nNow I would like to deploy this model to Inf1 on AWS Sagemaker see [here](https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/deploy_pytorch_model_on_Inf1_instance\/pytorch_torchvision_neo_on_Inf1.html#Deploying-pre-trained-PyTorch-vision-models-with-Amazon-SageMaker-Neo-On-Inf1-Instance):\n\n```\nfrom sagemaker.pytorch.model import PyTorchModel\n\npytorch_model = PyTorchModel(\n    model_data=model_path,\n    role=role,\n    entry_point=\"my_entry_point_file.py\",\n    framework_version=\"1.5.1\",\n    py_version=\"py3\",\n)\n\nneo_model = pytorch_model.compile(\n    target_instance_family=\"ml_inf1\",\n    input_shape={\"input0\": [1, 3, 224, 224]},\n    output_path=compiled_model_path,\n    framework=\"pytorch\",\n    framework_version=\"1.5.1\",\n    role=role,\n    job_name=compilation_job_name,\n)\n```\n\nHowever, in my case I get \n\n```\nUnexpectedStatusException: Error for Compilation job bloom-compiled-inf-inf1-202304-1921-4203: Failed. Reason: ClientError: CompilationError: Unable to compile model for ml_inf1:', 'No operations were successfully partitioned and compiled to neuron for this model - aborting trace!')  For further troubleshooting common failures please visit: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-compilation.html\n```\n\nI believe the main problem is the following: Whereas `inputs` can be considered of length `[1,1]`, the variable `past_key_values` is much more complex. In this case, it is \n\n- a tuple of length 24\n- each entry is a tuple itself of length 2\n- the two entries are torch tensors of size `[16, 64, 6]` and `[16, 6, 64]`\n\nMy question is now, what can I do such that it works on Inf1?\n\nI could imagine that either \n\n- there is a way to enter the right `input_shape`, which can be something like `{\u2018var1\u2019: [1,1,28,28], \u2018var2\u2019:[1,1,28,28]}` (I do not know how to display the more complex tuple-tensor structure as outlined above)\n- or can we split `past_key_values` such that we can build `input_shape` easily?\n\nAny suggestions would be very appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Algorithm arn does not exist when trying to start training job",
        "Question_created_time":1681922783026,
        "Question_last_edit_time":1682269780586,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAqElTcvdTLmus4LJBUSSqw\/algorithm-arn-does-not-exist-when-trying-to-start-training-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":36,
        "Question_answer_count":1,
        "Question_body":"When trying to start a training job in sagemaker using AlgorithmEstimator (by inputting the algorithm arn), I get an error saying that the Algorithm arn does not exist.  I have tried this with multiple algorithm arns (both obtained from my own algorithms and algorithms from AWS Marketplace) and they all give a similar error message.\n\nHere is a sample notebook that I am trying to execute but fails: [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/aws_marketplace\/using_algorithms\/autogluon\/autogluon_tabular_marketplace.ipynb](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/aws_marketplace\/using_algorithms\/autogluon\/autogluon_tabular_marketplace.ipynb)\n\nI am using sagemaker==2.145.0, both locally and in a sagemaker notebook instance.\n\nHere is a snippet of the error log:\n```\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Algorithm arn:aws:sagemaker:us-east-2:[acc-id]:algorithm\/arn:aws:sagemaker:us-east-2:[acc-id]:algorithm\/autogluon-tabular-v3-5-cb7001bd0e8243b50adc3338deb44a48 does not exist.\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"is it possible to trigger a sagmamker pipeline execution via event bridge?",
        "Question_created_time":1681850824873,
        "Question_last_edit_time":1682197563478,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVa1pPnIUQuK_zwh8E1wHyA\/is-it-possible-to-trigger-a-sagmamker-pipeline-execution-via-event-bridge",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":47,
        "Question_answer_count":2,
        "Question_body":"I have a sagemaker pipeline code , currently run through a notebook , but i would like to hook it up to a evenbridge and trigger an execution. is it possible to do that , if yes, what permissions would i need and if there any samples of the event , that would be great",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"sagemaker xgboost estimator fail to fit",
        "Question_created_time":1681833221520,
        "Question_last_edit_time":1682180079004,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0zHwGHryQAG9K19dHUNZfQ\/sagemaker-xgboost-estimator-fail-to-fit",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":59,
        "Question_answer_count":0,
        "Question_body":"I used the training script from [https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/xgboost\/using_xgboost.html](here),and trying to train the model. And the here is the code I used for configuring the model.\n`    sess = sagemaker.Session()\n    hyperparams = {\n    \"max_depth\": \"5\",\n    \"eta\": \"0.2\",\n    \"gamma\": \"4\",\n    \"min_child_weight\": \"6\",\n    \"subsample\": \"0.7\",\n    \"objective\": \"reg:squarederror\",\n    \"num_round\": \"50\",\n    \"verbosity\": \"2\"}\n\n    instance_type = \"ml.m5.2xlarge\"\n    output_path = f's3:\/\/{bucket}\/Model-Resource-Production\/dr_training_folder'\n    content_type = \"libsvm\"\n    \n    # Create XGBoost estimator\n    xgboost_estimator = XGBoost(\n        entry_point=\"script.py\",\n        framework_version=\"1.7-1\",  # Note: framework_version is mandatory\n        hyperparameters=hyperparams,\n        role=role,\n        instance_count=1,\n        instance_type=instance_type,\n        output_path=output_path,)\n    \n    file_path = 'Model-Resource-Production\/dr_training_folder'\n\n    # Read train test file from S3\n    training_input_config = TrainingInput(s3_data=f's3:\/\/{bucket}\/{file_path}\/DR_train', content_type='csv')\n    validation_input_config = TrainingInput(s3_data=f's3:\/\/{bucket}\/{file_path}\/DR_valid', content_type='csv')\n\n    # Fit the model\n    xgboost_estimator.fit({'train': training_input_config, 'validation': validation_input_config})`\n\nHowever, it returns the error during the \"Training in progress\" and here is the error:\n`    sess = sagemaker.Session()\n    hyperparams = {\n    \"max_depth\": \"5\",\n    \"eta\": \"0.2\",\n    \"gamma\": \"4\",\n    \"min_child_weight\": \"6\",\n    \"subsample\": \"0.7\",\n    \"objective\": \"reg:squarederror\",\n    \"num_round\": \"50\",\n    \"verbosity\": \"2\"}\n\n    instance_type = \"ml.m5.2xlarge\"\n    output_path = f's3:\/\/{bucket}\/Model-Resource-Production\/dr_training_folder'\n    content_type = \"libsvm\"\n    \n    # Create XGBoost estimator\n    xgboost_estimator = XGBoost(\n        entry_point=\"script.py\",\n        framework_version=\"1.7-1\",  # Note: framework_version is mandatory\n        hyperparameters=hyperparams,\n        role=role,\n        instance_count=1,\n        instance_type=instance_type,\n        output_path=output_path,)\n    \n    file_path = 'Model-Resource-Production\/dr_training_folder'\n\n    # Read train test file from S3\n    training_input_config = TrainingInput(s3_data=f's3:\/\/{bucket}\/{file_path}\/DR_train', content_type='csv')\n    validation_input_config = TrainingInput(s3_data=f's3:\/\/{bucket}\/{file_path}\/DR_valid', content_type='csv')\n\n    # Fit the model\n    xgboost_estimator.fit({'train': training_input_config, 'validation': validation_input_config})`\n\n`---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\nCell In[4], line 71\n     68 validation_input_config = TrainingInput(s3_data=f's3:\/\/{bucket}\/{file_path}\/DR_valid', content_type='csv')\n     70 # Fit the model\n---> 71 xgboost_estimator.fit({'train': training_input_config, 'validation': validation_input_config})\n\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/sagemaker\/workflow\/pipeline_context.py:284, in runnable_by_pipeline.<locals>.wrapper(*args, **kwargs)\n    280         return context\n    282     return _StepArguments(retrieve_caller_name(self_instance), run_func, *args, **kwargs)\n--> 284 return run_func(*args, **kwargs)\n\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/sagemaker\/estimator.py:1198, in EstimatorBase.fit(self, inputs, wait, logs, job_name, experiment_config)\n   1196 self.jobs.append(self.latest_training_job)\n   1197 if wait:\n-> 1198     self.latest_training_job.wait(logs=logs)\n\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/sagemaker\/estimator.py:2344, in _TrainingJob.wait(self, logs)\n   2342 # If logs are requested, call logs_for_jobs.\n   2343 if logs != \"None\":\n-> 2344     self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2345 else:\n   2346     self.sagemaker_session.wait_for_job(self.job_name)\n\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/sagemaker\/session.py:4756, in Session.logs_for_job(self, job_name, wait, poll, log_type)\n   4753             last_profiler_rule_statuses = profiler_rule_statuses\n   4755 if wait:\n-> 4756     self._check_job_status(job_name, description, \"TrainingJobStatus\")\n   4757     if dot:\n   4758         print()\n\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/sagemaker\/session.py:4263, in Session._check_job_status(self, job, desc, status_key_name)\n   4257 if \"CapacityError\" in str(reason):\n   4258     raise exceptions.CapacityError(\n   4259         message=message,\n   4260         allowed_statuses=[\"Completed\", \"Stopped\"],\n   4261         actual_status=status,\n   4262     )\n-> 4263 raise exceptions.UnexpectedStatusException(\n   4264     message=message,\n   4265     allowed_statuses=[\"Completed\", \"Stopped\"],\n   4266     actual_status=status,\n   4267 )\n\nUnexpectedStatusException: Error for Training job sagemaker-xgboost-2023-04-18-13-58-00-099: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"\/miniconda3\/bin\/python3 -m script --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\", exit code: 1`\n\nIt seems some problems during passing the hyperparameters but I don't know what actually is going on. Thanks so much in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Autopilot price comparison with Dataiku",
        "Question_created_time":1681734065480,
        "Question_last_edit_time":1682079723252,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0-bbSGpSzWQECYPAZmN0A\/sagemaker-autopilot-price-comparison-with-dataiku",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":19,
        "Question_answer_count":0,
        "Question_body":"Hi \n\nI have some confusion to use in between Sagemaker Studio and Dataiku in terms of scalability and pricing. I didn't get any source from where I can get satisfactory answers regarding this.\n\nPlease provide me a direct comparision regarding pricing between these two?\nOr provide me a price estimate of Sagemaker Studio usage when using it to train and deploy a data of more than 100GB.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to access models built in Sagemaker from Docker",
        "Question_created_time":1681573190170,
        "Question_last_edit_time":1681920796704,
        "Question_link":"https:\/\/repost.aws\/questions\/QUchQT5pVTSPW5SFnEhO4G_g\/unable-to-access-models-built-in-sagemaker-from-docker",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":50,
        "Question_answer_count":1,
        "Question_body":"Hello, I have built a machine learning model from within Sagemaker using an inbuilt algorithm (Blazingtext). I am using a batch inference strategy in my inference code as in: \n\n```\ntransformer = Transformer(model_name=my_model_name,\n                          instance_count=1,\n                          instance_type='ml.m4.xlarge',\n                          output_path=output_pred_dir\n                            )\ntransformer.transform(data=X_batch_location,\n                         data_type='S3Prefix',\n                         content_type='application\/jsonlines',\n                         split_type='Line'\n                         )\n\n```\n\n Everything works perfectly when the code is run from within Sagemaker. But now I need to run the inference with Lambda. So I did this  by packaging the inference code in a Docker image,  saved it to ECR and deployed to Lambda. However, it is unable to do the batch transformation and gives the error:\n\n[ERROR] ValueError: Failed to fetch model information for (my_model_name). Please ensure that the model exists. Local instance types require locally created models.\n\nplease note the model is saved and available on sagemaker; I see it from the sagemaker management console. \n\nHow can I allow the dockerised code to gain access to the model in Sagemaker? \n\n\nThank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Neo compilation fails",
        "Question_created_time":1681522771007,
        "Question_last_edit_time":1681870048917,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6PejYjIATCqba0w3xtZjIg\/sagemaker-neo-compilation-fails",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":38,
        "Question_answer_count":0,
        "Question_body":"I have a PyTorch model that I've saved following [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-compilation-preparing-model.html]() into a .tar.gz file\nI uploaded it to S3, and then tried to compile it.\n\nIt fails with the error:\nClientError: InputConfiguration: Framework cannot load PyTorch model. [enforce fail at inline_container.cc:222] . file not found: neo\/version\n\nMy target platform is x86_64.  Other settings are as in the screenshot.\nHow do I fix this?\nthanks\n\n![Enter image description here](\/media\/postImages\/original\/IMxM8RlGtDRKCAIAUEJ0KQDw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tensorboard and Streamlit apps on Sagemaker Studio does not load due to Javascript error",
        "Question_created_time":1681325595597,
        "Question_last_edit_time":1681672929900,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjiMhQB3eRQuixdU4Wxogqw\/tensorboard-and-streamlit-apps-on-sagemaker-studio-does-not-load-due-to-javascript-error",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_body":"I was following the blog at https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-streamlit-apps-in-amazon-sagemaker-studio\/.\nFirst, the github code repo does not have a few files as mentioned on the blog, namely setup.sh,run.sh, and status.sh. However, I figured to install dependencies and run the streamlit app on terminal. But when I browse the link <studio_url>\/proxy\/[PORT NUMBER]\/ - it loads a blank page. \nThe web console logs shows the following message:\n<body>\n        <noscript>You need to enable JavaScript to run this app.<\/noscript>\n        <div id=\"root\"><\/div>\n<\/body>\n\nSimilarily, following the blog for Tensorboard, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html - I could follow all the steps and run it on terminal. But the browser shows the following  \n\n<noscript>\n            <h1>TensorBoard requires JavaScript<\/h1>\n            <p>Please enable JavaScript and reload this page.<\/p>\n   <\/noscript>\n\nJavascript is enabled on my browser. Please help with further troubleshooting.\n\n![Enter image description here](\/media\/postImages\/original\/IMYNt2rATeTQmtbxZlUfZuHA)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Are Graviton instance types not supported for SageMaker multimodel endpoints?",
        "Question_created_time":1681313292601,
        "Question_last_edit_time":1681660217837,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHoRNBe1XQA2srtenVsUJsg\/are-graviton-instance-types-not-supported-for-sagemaker-multimodel-endpoints",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_body":"I am trying to create a multimodel endpoint using an ml.c7g.xlarge instance type. The error I get is: \u201cMultiModel mode is not supported for instance type ml.c7g.xlarge.\u201d However, according to the documentation on AWS, \u201cMulti-model endpoints are currently supported for all CPU instances types and on single-GPU instance types.\u201d (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance). Subsequently, I tried ml.c6g.xlarge and see that that also is not supported. Are Graviton instance types not supported in general? If so, why?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How can I use python libraries spacy in aws glue jupyter notebook ?",
        "Question_created_time":1681283481231,
        "Question_last_edit_time":1681630143966,
        "Question_link":"https:\/\/repost.aws\/questions\/QU15QvgNpIT12NxHpFcQJyGw\/how-can-i-use-python-libraries-spacy-in-aws-glue-jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":30,
        "Question_answer_count":2,
        "Question_body":"I tried using !pip3 install spacy to download the library but the notebook was not able to find the module, next I realized you can use %additioanl_python_modules, but I am not able to figure out how to do so...so I have to zip them even if I am not using my own custom libraries.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to get rid of InvokeEndpoint logs in CloudWatch?",
        "Question_created_time":1681282551070,
        "Question_last_edit_time":1681629801183,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnzjGSKwJSYOlmaAtX_eufA\/how-to-get-rid-of-invokeendpoint-logs-in-cloudwatch",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":20,
        "Question_answer_count":0,
        "Question_body":"I have millions of logs in CloudWatch of the following type:\n\n    \"POST \/invocations HTTP\/1.1\" 200 5 \"-\" \"AHC\/2.0\"\n\nI am assuming these logs are generated when InvokeEndpoint is called for a particular Sagemaker Container. Is there a way how can I prevent these logs from reaching my CloudWatch log group ?\n\n\nI followed a similar discussion here : https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/83, but didn't find the correct solution.\n\nAny tips\/pointers would be highly appreciated. Thank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Sagemaker - Mobile SSD models are expected to have exactly 4 outputs, found 8",
        "Question_created_time":1681213187849,
        "Question_last_edit_time":1681560027283,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtl6zM39pQLeePlD-Fsz1WA\/aws-sagemaker-mobile-ssd-models-are-expected-to-have-exactly-4-outputs-found-8",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":35,
        "Question_answer_count":0,
        "Question_body":"Im using sagemaker for train the data \nIt has pre-trained model\n\u201ctensorflow-od1-ssd-resnet50-v1-fpn-640x640-coco17-tpu-8\u201d\n\n**Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\nfor being able to run inference through the sagemaker API.**\n\n\n```\nmodel = Model(\nimage_uri=deploy_image_uri,\nsource_dir=deploy_source_uri,\nmodel_data=base_model_uri,\nentry_point=\u201cinference.py\u201d,\nrole=aws_role,\npredictor_cls=Predictor,\nname=endpoint_name,\n)\n```\n\n\n**# deploy the Model.**\n\n```\nbase_model_predictor = model.deploy(\ninitial_instance_count=1,\ninstance_type=inference_instance_type,\nendpoint_name=endpoint_name,\n)\n```\n\n**Save the deployed model in local**\n```\nimport boto3\ns3 = boto3.client('s3')\nbucket = 'sagemaker'\nkey = 'model-tensorflow-od1-ssd-mobilenet\/model.tar.gz'\nlocal_file_path = 'new_model.tar.gz'\ns3.download_file(bucket, key, local_file_path)\n```\n\n\n**Load the saved model**\n\n```\nmodel = tf.saved_model.load(saved_model_dir)\n```\n\n\n**Convert the model to a TFLite model**\n\n```\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\nconverter.target_spec.supported_ops = [\ntf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\ntf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n]\ntflite_model = converter.convert()\n```\n\n\n**Save the TFLite model to disk**\n\n```\nwith open(tflite_model_file, \u2018wb\u2019) as f:\nf.write(tflite_model)\n```\n\n\nI trained and converting it to .tflite file and using it in my swift application i got an error\n**Mobile SSD models are expected to have exactly 4 outputs, found 8**",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to re-package sagemaker model with lambda?",
        "Question_created_time":1681182700915,
        "Question_last_edit_time":1681529527003,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDtd-nb75TZGb0QNcOUI-8w\/how-to-re-package-sagemaker-model-with-lambda",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_body":"I have a sagemaker training process which dumps a model.tar file in a s3 bucket. I don't have access to this training code . I want to download the model.tar file from that s3 location, and repackage it with my own inference code , saved in some s3 bucket. i am aware that there is a way to package this in sagemaker via pipelines but are there any examples to do it in lambda?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"is it possible to run just the model packaging step in sagemaker pipeline?",
        "Question_created_time":1681157109613,
        "Question_last_edit_time":1681503686453,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4sPPFru5RNi7NFlfflFPtA\/is-it-possible-to-run-just-the-model-packaging-step-in-sagemaker-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":0,
        "Question_body":"I have set up few steps in sagemaker pipeline , to process and train data. but in the event where i need a custom script for inference , i want to able to just run the model re-packaging step. is it possible to just run this step in the pipeline? if not, are there any other alternatives to do model packaging via lambda or something similar ? also, is it possible to view , which tool or script does sagemaker uses to package the model. is it possible to use the same tool outside the sagemaker?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Neo compiled model not using NVIDIA GPU",
        "Question_created_time":1681127066445,
        "Question_last_edit_time":1681473607035,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa7OaHGIQR96sEdJ-xKna_Q\/sagemaker-neo-compiled-model-not-using-nvidia-gpu",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":34,
        "Question_answer_count":0,
        "Question_body":"I have a YOLOv5-trained model. Exported as Tensorflow, I'm trying to compile and deploy to my device as a Greengrass Noe edge component.\n\nDevice configs are,\ndevice: NVIDIA Jetson AGX Xavier\nJetpack:5.1\nCUDA: 11.4\nTensorRT:8.5\n\nI compiled the model with different configurations.\n\n**Compilation 1. **\nmodel:tensorflow 2.4\nPlatform: Linux\nArch: ARM64\n\nCan able to load the model and run inference. But its running on CPU and the inference time is very high.\n\n**Compilation 2. **\nmodel:tensorflow 2.4\nPlatform: Linux\nArch: ARM64\nAccelerator: NVIDIA\nConfig: {\"gpu-code\":\"sm_62\",\"trt-ver\": \"8.5\", \"cuda-ver\": \"11.4\"}\n\nUnable to load the model.\nGRPC error : \"UNKNOWN:Error received from peer unix:\/tmp\/aws.greengrass.SageMakerEdgeManager.sock {grpc_message:\"LoadModel request failed\", grpc_status:13\"\n\n**Compilation 3. **\nmodel:tensorflow 2.4\nDevice: jetson_xavier\nConfig: {\"trt-ver\": \"6.0.1\", \"cuda-ver\": \"10.0\"}\n\nUnable to load the model.\nGRPC error : \"UNKNOWN:Error received from peer unix:\/tmp\/aws.greengrass.SageMakerEdgeManager.sock {grpc_message:\"LoadModel request failed\", grpc_status:13\"\n\nEither the not running on GPU or the deployed model I was not able to load.\nCloud you please help me over here.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Training XGboost error on SageMaker",
        "Question_created_time":1681118262960,
        "Question_last_edit_time":1681464859820,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4OxvmCu2T6ej1ZX_2z5XhA\/training-xgboost-error-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_body":"An error occurred (ValidationException) when calling the CreateTrainingJob operation: Access denied for repository: xgboost in registry ID: 602401143452. Please check if your ECR repository and image exist and role arn:aws:iam::866574816374:role\/service-role\/AmazonSageMaker-ExecutionRole-20230321T165519 has proper pull permissions for SageMaker: ecr:BatchCheckLayerAvailability, ecr:BatchGetImage, ecr:GetDownloadUrlForLayer\n\nCODE:https:\/\/samick-blog.s3.cn-north-1.amazonaws.com.cn\/xgboost_model_training_and_deployment.ipynb\ncontainers = { 'ap-southeast-1': '602401143452.dkr.ecr.ap-southeast-1.amazonaws.com\/xgboost:latest' }",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data import using local upload failed in Sagemaker canvas",
        "Question_created_time":1680926716728,
        "Question_last_edit_time":1681273193271,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDTjxBMfPQ6OZe9pyHXks3A\/data-import-using-local-upload-failed-in-sagemaker-canvas",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":67,
        "Question_answer_count":2,
        "Question_body":"Hi, getting upload dialed error after multiple trials while uploading data using local upload in sagemaker canvas. Please advise. Thanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best AWS Service to Put PySpark \/ ML Code into Production",
        "Question_created_time":1680817749774,
        "Question_last_edit_time":1681165754932,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfiP27nZSRHaJfUeZVtOfRA\/best-aws-service-to-put-pyspark-ml-code-into-production",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":72,
        "Question_answer_count":1,
        "Question_body":"I am very new to MLOps and seeking guidance on how to put a piece of ML python code into production on AWS.\n\nHere are my requirements:\n1. Model is run once a week\n2. Pick up very large data files from S3\n3. Models are stored in S3 and will need to be loaded from there as well\n4. Data processing is done in PySpark\n5. Processed data is then converted into Pandas for further processing before being fed into models\n6. Models are typically SkLearn or XGBoost\n7. Model results should be stored back in S3\n\nAny idea of what service is best for this? Again, I have very limited experience outside of writing ML models in python, so anything that is simple where I am not managing or worrying about infrastructure is preferable. However, I will do what I must to put the code into production.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Tensorflow Kernel Launch Issue",
        "Question_created_time":1680810606414,
        "Question_last_edit_time":1681157672861,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdxDnRhIGR7ClXORuhRX_LQ\/sagemaker-tensorflow-kernel-launch-issue",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":44,
        "Question_answer_count":0,
        "Question_body":"Hello,\n\nRegardless of the instance type I can't launch a Tensorflow 2.11.0 CPU\/GPU kernel on the Frankfurt datacenter, getting this error:\n\nStarting notebook kernel...\n\nHad error starting kernel 1 times.\n\nResponded with error: Failed to launch app [tensorflow-2-11-gpu-ml-g4dn-xlarge-d54c3ba7f07d45a5aba6519fd6c2]. InternalFailure (Context: RequestId: 1374ab56-eed6-405d-bb43-e5fe279674ff, TimeStamp: 1680809971.070621, Date: Thu Apr 6 19:39:31 2023)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cloudwatch logs not showing",
        "Question_created_time":1680782385411,
        "Question_last_edit_time":1681129275810,
        "Question_link":"https:\/\/repost.aws\/questions\/QU77HVhR9PTmCvn8rrOPqCbg\/cloudwatch-logs-not-showing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":90,
        "Question_answer_count":1,
        "Question_body":"I have implemented a sagemaker pipeline that contains processing\/training\/evaluation steps. I used to be able to see the logs of them all, but now, only evaluation and training jobs logs are shown in their cloudwatch groups. the log groups for processing jobs and training jobs are still present. I have deleted some old logs in the processing group to make room for new ones but that didnt help also. This happened before and got resolved by its own somehow, and now happened again. Could you please advise on this issue?\n\nhere is the code for my processing step:\n\n-----------------------------------------------------------------\n# script processor using custom docker image from ECR\nscript_processor = ScriptProcessor(\n    command=[\"python3\"],\n    image_uri=processing_and_splitting_image,\n    role=role,\n    instance_count=processing_instance_count,\n    instance_type=processing_instance_type,\n    base_job_name=\"data-process\",\n)\n\nstep_process = ProcessingStep(\n    name=\"DataPreprocessingStep\",\n    code=\"s3:\/\/{}\/sagemaker\/scripts\/processing.py\".format(bucket_source),\n    processor=script_processor,\n    inputs=[\n        ProcessingInput(\n            input_name=\".\/data\/raw_df.csv\",\n            source=\"s3:\/\/{}\/sagemaker\/data\/raw_df.csv\".format(bucket),\n            destination=\"\/opt\/ml\/processing\/input\",\n        )\n    ],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"train\",\n            source=\"\/opt\/ml\/processing\/train\",\n            destination=\"s3:\/\/{}\/train\".format(bucket),\n        ),\n        ProcessingOutput(\n            output_name=\"test\",\n            source=\"\/opt\/ml\/processing\/test\",\n            destination=\"s3:\/\/{}\/test\".format(bucket),\n        ),\n    ],\n    job_arguments=[\"--train-test-split-ratio\", \"0.2\"],\n)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Ground Truth completes with labelling errors",
        "Question_created_time":1680685751465,
        "Question_last_edit_time":1681033088987,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh8WSJpuNTVqQ4-gW6_e6LQ\/sagemaker-ground-truth-completes-with-labelling-errors",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":0,
        "Question_body":"Hi all,\n\nI set a Sagemaker Ground Truth labelling task. All the labelled objects results with \"Failed\" status, though it is possible retrieve the labels from S3. Eventually, nothing apart from the job status is wrong - but I am worried that I could and I would like to resolve it. There are no logs in cloudwatch with anything FAILED. Would you know how to get rid of this issue? For start, it would be good to see a specific issue in cloudwatch and then try to resolve the problem with failed jobs.\n\nThe logs show the following (I filtered out repeating events for different input objects):\n\n```\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"INPUT_REQUEST_VALIDATED\",\n    \"event-log-message\": \"Input request validated.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"CREATED_WORKER_INTERFACE\",\n    \"event-log-message\": \"Human task UI created.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"INPUT_MANIFEST_PARSED\",\n    \"event-log-message\": \"Input manifest parsed successfully.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"PRE_HUMAN_LAMBDA_INVOKED\",\n    \"event-log-message\": \"Pre-human task Lambda invoked for line 6.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"PRE_HUMAN_LAMBDA_SUCCEEDED\",\n    \"event-log-message\": \"Pre-human task Lambda returned successfully for line 6.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"HUMAN_TASK_CREATED\",\n    \"event-log-message\": \"Human task created successfully for line 6.\"\n}\n{\n    \"event-name\": \"HUMAN_TASK_COMPLETED\",\n    \"event-log-message\": \"Human task completed successfully for line 6.\",\n    \"labeling-job-name\": \"test-job51\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"LABELING_JOB_PROGRESS\",\n    \"event-log-message\": \"Labeling job progress for batch number:1 | #Total data set objects:2 | #Data set objects completed:0 | #Data set objects waiting for humans:0 | #Data set objects failed:2 | #Data set objects expired:0 | #Data set objects cancelled:0\"\n}\n```\n\n\nThe setup goes as follows\n- using VPC\n- using Private Labelling team\n- CORS set on S3 bucket (per suggestion from `https:\/\/repost.aws\/knowledge-center\/sagemaker-ground-truth-errors`)\n- the execution role has permission to use cloudwatch (per suggestion from https:\/\/repost.aws\/knowledge-center\/sagemaker-ground-truth-errors)\n- Cognito App callback url is `https:\/\/XXX.labeling.us-east-1.sagemaker.aws\/oauth2\/idpresponse` and logout URL is `https:\/\/XXX.labeling.us-east-1.sagemaker.aws\/logout` (I noticed that when the labelling job\/workforce is created from Console, there is an additional allowed callback `https:\/\/XXX.cloudfront.net`; I did not add it)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to debug invocation timeouts for Redshift ML BYOM remote inferences",
        "Question_created_time":1680622778617,
        "Question_last_edit_time":1680970665853,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFNGb6N7ESFarrUMxjk-7yw\/how-to-debug-invocation-timeouts-for-redshift-ml-byom-remote-inferences",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"I have an existing SageMaker inference endpoint that I'm successfully calling from Aurora PostgreSQL using the `aws_ml` extension's `invoke_endpoint` function. I'm now trying to use the same endpoint from Redshift.\n\nBased on [Getting started with Amazon Redshift ML](https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/getting-started-machine-learning.html), I've set up the necessary IAM policies, created a model for the endpoint in Redshift, and called it via the model's registered function. However, I'm getting an error after 370 seconds no matter what I try.\n\n```\nQuery 1 ERROR: ERROR:  Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https:\/\/us-east-\nDETAIL:  \n  -----------------------------------------------\n  error:  Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https:\/\/us-east-\n  code:      32207\n  context:   \n  query:     4076\n  location:  exfunc_client.cpp:136\n  process:   query1_125_4076 [pid=29885]\n  -----------------------------------------------\n```\n\nI can see work being performed in the endpoint containers, and there's no errors reported. One major difference between Aurora PostgreSQL and Redshift is that there's no controls for batch size from Redshift. In Aurora PostgreSQL, I typically pass a batch size of around 1000 to `invoke_endpoint`. Redshift is sending 50000 to 220000 rows per batch, which can take a couple minutes to complete.\n\nDoes anyone have any suggestions on how I can debug this? The query failure is always at 370 seconds. I'm not sure what the significance of that number is.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth Labeling of 3d Point Clouds - Center of Rotation",
        "Question_created_time":1680621259002,
        "Question_last_edit_time":1680968525708,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmNgheRVRSMe0HRUqagijdA\/ground-truth-labeling-of-3d-point-clouds-center-of-rotation",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":14,
        "Question_answer_count":0,
        "Question_body":"I'm trying to label a 3D point cloud using the Ground Truth 3d Point Cloud labeling tool.  The image is an immersive 3d object.  Therefore, in order to make the UI usable, I need to be able to rotate the image around the view position, versus the point cloud origin. For example, if you were walking through a maze, to turn left or right, you would rotate around where you were standing, not the center of the maze.\n\nIs it possible to move the axis of rotation?\n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Best practices to fine-tune large language models on Sagemaker",
        "Question_created_time":1680608834597,
        "Question_last_edit_time":1680956686486,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa7l9lEWpQCCoE-krEUTDBw\/best-practices-to-fine-tune-large-language-models-on-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":38,
        "Question_answer_count":0,
        "Question_body":"I would like to fine-tune large language models (starting with 10+B parameters) on Sagemaker.\n\nSince we are working with Pytorch and Lightning the idea would be to use DeepSpeed in combination with the Lightning trainer (https:\/\/lightning.ai\/docs\/pytorch\/stable\/advanced\/model_parallel.html).\nThe starting point will be fine-tuning some public model with the PEFT library from Huggingface.\n\nI also saw an article about Trainium but that does not seem to work out-of-the-box and require changes to the model, which will prevent us from using publicly available per-trained models (if I am not mistaken).\n\n\nI would like to know if there are resources on the topic, in particular:\n* guidance about different ways to leverage multi-GPU\n* example of successful use-cases\n* guidance about cost\/performance trade-offs",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Autopilot training jobs stopped without any logs",
        "Question_created_time":1680572149545,
        "Question_last_edit_time":1680919341692,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhcwfi7ZzSRuQjh13bem1hw\/sagemaker-autopilot-training-jobs-stopped-without-any-logs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_body":"I'm trying to use SageMaker Autopilot. I'm creating new jobs via the CreateAutoMLJob API.\n\nThe processing jobs succeed, but the training jobs just stop pretty much instantly after they start. Here's an example of the history of one of the training jobs : \n\n```\nStatus\t    Start time\t            End time\t            Description\nStarting\t4\/3\/2023, 7:03:42 PM\t4\/3\/2023, 7:04:53 PM\tPreparing the instances for training\nDownloading\t4\/3\/2023, 7:04:53 PM\t4\/3\/2023, 7:04:53 PM\tDownloading input data\nStopping\t4\/3\/2023, 7:04:53 PM\t4\/3\/2023, 7:04:53 PM\tStopping the training job\nStopped\t    4\/3\/2023, 7:04:53 PM\t4\/3\/2023, 7:04:53 PM\tTraining job stopped\n```\n\nThere are no logs available, and there are no errors in the console. When I click the `View logs` button that redirects me to CloudWatch, the Log Group does not exist.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to run additional steps in sagemaker pipelines?",
        "Question_created_time":1680451680920,
        "Question_last_edit_time":1680799987554,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTT_nCXj9QDufVUImLkzL_w\/how-to-run-additional-steps-in-sagemaker-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"I am running sagemaker experiments , based on example here -> https:\/\/aws.amazon.com\/blogs\/machine-learning\/track-your-ml-experiments-end-to-end-with-data-version-control-and-amazon-sagemaker-experiments\/, once the experiments are done , i want to create a training\/processing pipeline via sagemaker pipeline steps. similar to the sample in the link before, i want to run additioanal step\/script, to track the model output via dvc (https:\/\/dvc.org\/),  does sagemaker pipeline have any custom steps we can define , such that we can trigger additional steps to invoke dvc commands . I am aware there is a lambda step, we can add to, but not sure if this is the right way to do this. in short, the lambda will need to keep track of the output location for model, (which assume can be passed from previous step in the sagemaker pipeline) , then run dvc commands to track and then run git commands to push it back to the repo.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"NVMLError_FunctionNotFound: I was trying to deploy a PyTorch model in a ml.g4dn.xlarge instance",
        "Question_created_time":1680373339170,
        "Question_last_edit_time":1680720287658,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDoLlFS3VTRCRvYQmiZkjxg\/nvmlerror-functionnotfound-i-was-trying-to-deploy-a-pytorch-model-in-a-ml-g4dn-xlarge-instance",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":76,
        "Question_answer_count":0,
        "Question_body":"I was trying to deploy a PyTorch model in a 'ml.g4dn.xlarge' instance for real-time inference.\n\nframework_version='1.13.1'\npy_version='py39'\n\nHowever, I kept getting pynvml.nvml.NVMLError_FunctionNotFound: Function Not Found in the log when deploying it.\nIs it issue related to Nvidia GPU driver version?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cannot attach custom images to AWS SageMaker Studio. Results in \"Update_Failed\" with error message \"InternalFailure\".",
        "Question_created_time":1680294145786,
        "Question_last_edit_time":1680641839348,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUpCXF-rDRTWturwNEECsug\/cannot-attach-custom-images-to-aws-sagemaker-studio-results-in-update-failed-with-error-message-internalfailure",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":50,
        "Question_answer_count":0,
        "Question_body":"Our SageMaker Studio service is broken in one of our AWS accounts in some deep way. Our original domain encountered this issue of \"Update_Failed\" when attempting to attach a new custom docker image. Using describe-domain via the CLI, we see that the \"FailureReason\" is just \"InternalFailure\". This issue also somehow effects brand new, entirely separate SageMaker Studio domains that we create. This is only an issue in our one (data science development) AWS account. Repeating the process in other accounts works as expected.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Trying to train GPT2-large large and running out of memory",
        "Question_created_time":1680149078159,
        "Question_last_edit_time":1680496217619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUG9co-8KaRsevY3DgIKQezA\/trying-to-train-gpt2-large-large-and-running-out-of-memory",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":113,
        "Question_answer_count":1,
        "Question_body":"I am trying to train  GPT2-large model on Sagemaker Studio -- using  a 'ml.g4dn.2xlarge instance.  The training file is very small ( 13 kb). It gives the following error:\n\nExitCode 1\nErrorMessage \"RuntimeError: RESOURCE_EXHAUSTED: From \/job:localservice\/replica:0\/task:0\n 2 root error(s) found.\n (0) RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26214400 bytes.\n #011 [[{{node XRTExecute}}]]\n Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n \n #011 [[XRTExecute_G15]]\n (1) RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26214400 bytes.\n 0 successful operations.\n 0 derived errors ignored.\n Recent warning and error logs\n Allocator (GPU_0_bfc) ran out of memory trying to allocate 23.91GiB (rounded to 25677513472)requested by op\n *******************************************************************************************_________\n Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.00MiB (rounded to 26214400)requested by op\n ****************************************************************************************************\n OP_REQUIRES failed at xrt_execute_op.cc:432 : RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26214400 bytes.\n 20%|\u2588\u2588        | 1\/5 [03:23<13:33, 203.35s\/it]\"\nCommand \"\/opt\/conda\/bin\/python3.8 run_clm.py --do_train True --model_name_or_path gpt2-large --num_train_epochs 5 --output_dir \/opt\/ml\/model --per_device_train_batch_size 10 --train_file \/opt\/ml\/input\/data\/train\/train.txt\"\n2023-03-30 03:44:11,044 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n\n\nThe training config is as follows:\n\nhuggingface_estimator = HuggingFace(\n\tentry_point='run_clm.py',\n\tsource_dir='.\/examples\/pytorch\/language-modeling',\n\tinstance_type='ml.g4dn.2xlarge',\n\tinstance_count=1,\n\trole=role,\n\tgit_config=git_config,\n\ttransformers_version='4.17.0',\n\tpytorch_version='1.10.2',\n\tpy_version='py38',\n\thyperparameters = hyper_params,\n    compiler_config = TrainingCompilerConfig(),\n    environment = { 'GPU_NUM_DEVICES' : '1' },\n    disable_profiler = True,\n    debugger_hook_config = False\n)\n\nhuggingface_estimator.fit({'train': s3_training_data}, wait = True)\n\n\nSimilar errors happen for any Huggingface gpt model other than the basic GPT2( smallest).  I am using a a fairly",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to add tracking to sagemaker experiments and training jobs?",
        "Question_created_time":1680062842404,
        "Question_last_edit_time":1680409636605,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6mOnWpQWTqK3rxgdHOgtlg\/how-to-add-tracking-to-sagemaker-experiments-and-training-jobs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"regarding sagemaker experiments, we can add additonal arguments or parameters as  part of tracking for sagemaker experiments . is there anything similar for sagemaker training jobs?\n\n```\nfrom smexperiments.tracker import tracker \n\nwith Tracker.create(...) as tracker:\n   tracker.log_parameters (....\n      \n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"I deleted a sagemaker notebook without stopping it",
        "Question_created_time":1679950075455,
        "Question_last_edit_time":1680297146785,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoN4KlhnCRp-n47teJGVlWQ\/i-deleted-a-sagemaker-notebook-without-stopping-it",
        "Question_score_count":2,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":104,
        "Question_answer_count":3,
        "Question_body":"So the notebook is  still running, name of the notebook is  **Studio-Notebook:ml.t3.medium** ,please help me i am about to exceed the free tier limit . Please tell me how to fix this so that i wont be billed as i am not even using sagemaker rightnow.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1679951160328,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1679951160328,
        "Answer_comment_count":0.0,
        "Answer_body":"When you delete a notebook instance, SageMaker removes the ML compute instance, and deletes the ML storage volume as well as the network interface associated with the notebook instance.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"I have a sagemaker account setup in US-WEST-2 region. I have successfully trained and deployed my model; however, how can I chose to deploy the endpoint in a different region?",
        "Question_created_time":1679901203452,
        "Question_last_edit_time":1680247905508,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJREyfO_1Q_muUCNzmk3f8g\/i-have-a-sagemaker-account-setup-in-us-west-2-region-i-have-successfully-trained-and-deployed-my-model-however-how-can-i-chose-to-deploy-the-endpoint-in-a-different-region",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":48,
        "Question_answer_count":1,
        "Question_body":"Assume I have a sagemaker account in one region where I have all my code, data and model artifacts etc..\nI have been able to successfully deploy these models using the estimator.deploy() API after calling the estimator.fit() method. \nHowever, what If I dont want to deploy the model in the same region as my sagemaker account and want to deploy the endpoint in another region, how can I do that?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can not load GPT-J6B on 32 GB instance from HuggingFace",
        "Question_created_time":1679897704105,
        "Question_last_edit_time":1680244570211,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGqUOzFKiR_2QxZ8nyhJ2Rw\/can-not-load-gpt-j6b-on-32-gb-instance-from-huggingface",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":3,
        "Question_view_count":78,
        "Question_answer_count":0,
        "Question_body":"I have used **ml.g4dn.2xlarge** instance on SageMaker to test GPT-J6B model from HuggingFace using Transformer.\n\nI am using `revision=float16` and `low_cpu_mem_usage=True` so that the model is only of 12GB.\n\nIt is downloaded but ***after*** that it suddenly crashes the kernel.\n\nPlease share the workaround. The memory of that instance is 32 GB wit 4 vCPU.\n\n```python\n!pip install transformers\n\nfrom transformers import AutoTokenizer, AutoModelForCasualLM\n\nmodel = AutoModelForCasualLM.from_pretrained(\"EleutherAI\/gpt-j-6B\", revision=\"float16\", low_cpu_mem_usage=True) # It crashes here\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI\/gpt-j-6B\")\n```\n\nIt downloads 12GB model but after that, it crashes. \n\nI tried to follow this thread [here](https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sage-maker) but still there I can't update the sentencepiece. \n\nPlease help.\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Configuring auto-scaling for sagemaker async-inference",
        "Question_created_time":1679638211456,
        "Question_last_edit_time":1679985798951,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT4xru2SdTSqtoNyt1XV3VA\/configuring-auto-scaling-for-sagemaker-async-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":190,
        "Question_answer_count":2,
        "Question_body":"I've configured a model for async-inference, and its working correctly - I can submit a file via `invoke_endpoint_async` and download the output from s3.\n\nI'm now trying to configure auto-scaling. I'm trying experimentation with different options, but basically I want to configure 0-1 instances, have an instance created when`invoke_endpoint_async` is called, and have the instance shutdown shortly afterwards (along the lines of batch inference)\n\nI'm struggling to get it to work - I'm experiencing similar issues to https:\/\/github.com\/boto\/boto3\/issues\/2839\n\nFirst I think there's an issue with the `console` - if I  `aws register-scalable-target ...` it works but the console doesn't like the zero for `min-capacity`\n\n![Enter image description here](\/media\/postImages\/original\/IMWZdtU68ZSXSSvr46-_1nhw)\n\nI think this is just a UI nit though, I don't understand how the policy works - I have\n\n```json\n{\n    \"TargetValue\": 1.0,\n    \"CustomizedMetricSpecification\": {\n        \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n        \"Namespace\": \"AWS\/SageMaker\",\n        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": \"***-test-endpoint-2023-03-24-04-28-06-341\"}],\n        \"Statistic\": \"Average\"\n    },\n    \"ScaleInCooldown\": 60,\n    \"ScaleOutCooldown\": 60\n}\n```\n\nThe first point of confusion was the console shows a built-in and custom policy. I was initially using the name of the built-in policy (SageMakerEndpointInvocationScalingPolicy) but `put-scaling-policy` doesn't appear to edit it - it creates a new policy with the same name.\n\nWhen I monitor the scaling activity ()\n\n```console\naws application-autoscaling describe-scaling-activities \\\n    --service-namespace sagemaker\n```\n\nI can initially see \"Successfully set desired instance count to 0. Change successfully fulfilled by sagemaker.\" \n\nBut when I involve the endpoint with \n\n```python\nresponse = sm_runtime.invoke_endpoint_async(\n    EndpointName=endpoint_name, \n    InputLocation=\"***\/input\/data.json\",\n    ContentType='application\/jsonlines',\n    Accept='application\/jsonlines')\n\noutput_location = response['OutputLocation']\n```\n\nI would expect to see the instance count increase to 1, then back to zero within a space of a few minutes. I have occasionally got it to do something but not reliably. I think the main issue is I don't understand the metric and how it interacts with the target.\n\nI've seen charts but I cannot figure out how to plot the \"ApproximateBacklogSizePerInstance\"? And how does it interact with \"TargetValue\"? What is the actual trigger for a scale in\/out?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1679845259536,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1679845259536,
        "Answer_comment_count":0.0,
        "Answer_body":"A target tracking scaling policy will create 2 CloudWatch alarms (one for high and one for low usage), which you'll be able to see in the CloudWatch alarms console.  The high usage policy needs to have 3 consecutive 60 second breaching datapoints to trigger a scale-out; and the low alarm needs 15 consecutive 60 second breaching datapoints to scale-in\n\nYou may instead want to use step scaling policies, where you are able to create and control the alarms as well as the policy settings\nhttps:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/application-auto-scaling-step-scaling-policies.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"how to copy data and run training in sagemaker?",
        "Question_created_time":1679581220207,
        "Question_last_edit_time":1679928593217,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrFdu7pfeRcyG5_6GcnBSWA\/how-to-copy-data-and-run-training-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":50,
        "Question_answer_count":1,
        "Question_body":"i need to pull data from different sources and run processing then training jobs, I want to set it up such that , I spin up one or more ec2 instance , collect the data , copy it to some s3 , then use sagemaker api to create processing and training jobs. I want to do this via code, provisioning ec2 instances and the code to download data and running training and processing jobs, all of it. one question, i had was, once i download the data, can i copy the data directly to whatever data storage\/volume comes attached with the sagemaker training\/processing instance. I'm not sure what's it called . i know there are options to stream data? any thoughts. this would be my first training job , so apologies for basic question here",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker pre-signed URL timeout extension",
        "Question_created_time":1679526730196,
        "Question_last_edit_time":1679874833970,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKDk-YzA5SlK_zw3jb5Ox3g\/sagemaker-pre-signed-url-timeout-extension",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":27,
        "Question_answer_count":2,
        "Question_body":"How can I extend the default time out period for SageMaker pre-signed URL ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1679527619211,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1679527619211,
        "Answer_comment_count":0.0,
        "Answer_body":"The session timeout duration can specified using \n```\n`SessionExpirationDurationInSeconds`\n```\n in the API. Refer to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePresignedDomainUrl.html#API_CreatePresignedDomainUrl_RequestParameters.\n This value defaults to 43200 which is the max allowed value.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Does SageMaker Studio support GeoIP fencing?",
        "Question_created_time":1679526459407,
        "Question_last_edit_time":1679872514046,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5ZsU3ku7ScCJioC5KaVd7Q\/does-sagemaker-studio-support-geoip-fencing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":23,
        "Question_answer_count":1,
        "Question_body":"I want to specify a country\/region for ip verification against ip geo location for SageMaker Studio access",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to read delta lake format data for SageMaker training",
        "Question_created_time":1679525726286,
        "Question_last_edit_time":1679871523417,
        "Question_link":"https:\/\/repost.aws\/questions\/QUynCivtviQ3ieRnn4obNUnw\/how-to-read-delta-lake-format-data-for-sagemaker-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_body":"I want to read data from Databricks output and format the data for SageMaker training",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Model deployment with internal data",
        "Question_created_time":1679392536870,
        "Question_last_edit_time":1679738823811,
        "Question_link":"https:\/\/repost.aws\/questions\/QUgJlBT8V_T_i9aQz_WF6EkA\/model-deployment-with-internal-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":45,
        "Question_answer_count":3,
        "Question_body":"Is it possible to deploy a model to an endpoint along with a large amount of data for it to use? (~50GB), this can be though of as an internal database for my model to use.\nI only require such functionality temporary, since I am trying to avoid using database services (e.g DynamoDB etc) for the moment.\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"sagemaker\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5\u6570\u636e\u5728\u4e2d\u56fd\u5b81\u590f\u533a\u4e0b\u8f7d\u63d0\u793a\u62d2\u7edd\u8bbf\u95ee",
        "Question_created_time":1679313191871,
        "Question_last_edit_time":1679660142824,
        "Question_link":"https:\/\/repost.aws\/questions\/QUixnUF7E_Qseb2dO5prMJBw\/sagemaker%E7%BE%8E%E5%9B%BD%E4%BA%BA%E5%8F%A3%E6%99%AE%E6%9F%A5%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%B8%AD%E5%9B%BD%E5%AE%81%E5%A4%8F%E5%8C%BA%E4%B8%8B%E8%BD%BD%E6%8F%90%E7%A4%BA%E6%8B%92%E7%BB%9D%E8%AE%BF%E9%97%AE",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":53,
        "Question_answer_count":1,
        "Question_body":"https:\/\/aws.amazon.com\/cn\/blogs\/machine-learning\/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker\/#Comments\uff0c\u5b98\u65b9\u793a\u4f8b\u6587\u6863\u4e2d\u7684\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5\u6570\u636e\u65e0\u6cd5\u4e0b\u8f7d\uff0c\u6211\u6267\u884caws s3 ls s3:\/\/aws-ml-blog-sagemaker-census-segmentation\u3002\u63d0\u793a\u6211\u201cAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\u201d\u62d2\u7edd\u8bbf\u95ee\uff0cIAM\u8d26\u53f7\u5df2\u7ecf\u7ed9\u4e86AmazonS3FullAccess\u6743\u9650",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Xgboost hyperparameter tuning : worse metrics than RandomizedSearchCV",
        "Question_created_time":1679303972897,
        "Question_last_edit_time":1679649657133,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPtlmLuhkQ0OyjZNCQhcDBw\/xgboost-hyperparameter-tuning-worse-metrics-than-randomizedsearchcv",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":31,
        "Question_answer_count":1,
        "Question_body":"I have implemented hyperparameter tuning using the 1.5-1 sagemaker xgboost container, for a binary classifier with a F1:validation objective, loading data from a csv file. Compared to a simple RandomizedSearchCV with same parameter exploration (versions 1.5.1 or 1.6.2), the metrics are significantly worse using sagemaker (0.42 vs 0.59 for best model). I am trying to understand where such discrepancy could come from. Have you observed such a thing and how could I resolve the situation?\nThanks in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to use `initial-value` parameter of Named Entity Recognition project in Ground Truth?",
        "Question_created_time":1679251865640,
        "Question_last_edit_time":1679599141271,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9695UE4LRXWcF3qg2kVk2A\/how-to-use-initial-value-parameter-of-named-entity-recognition-project-in-ground-truth",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":79,
        "Question_answer_count":1,
        "Question_body":"In my NER Ground Truth labeling job, I want give human workers a head start by applying a regular expression or other NER model to the text. When the text is shown to the human worker, I want the text spans that matched the RE\/model result to already be highlighted with the found entity. Then, the human can accept the result or modify it.\n\nIt looks like Ground Truth's [crowd-entity-annotation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-entity-annotation.html)  template already provides a way to do this. The documentation says the template accepts an 'initial-value' attribute, which looks like exactly what I want. I have not been successful in using it though. \n\n\n- I tried adding an 'initial-value' array to the input manifest file of my project, but it didn't make a difference.\n\n- I found this example project using A2I that creates a custom template for displaying a model's annotations to the worker. However, when I followed this example, it only shows the model annotations in a separate textbox versus having them pre-highlighted.\n\n- I modified the custom UI template from that project to explicitly set initial-value to a task.input.initial-value field, but that also didn't work.\n\n\nThis would be a huge speed boost if I could get it to work. What is the correct way to set and display the initial values in an NER project? Is there a working example I can look at?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Model Monitor Data Quality on Random Cut Forest configuration issues",
        "Question_created_time":1679132698384,
        "Question_last_edit_time":1679480883548,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4b9exAY8TJOwKGMUCRfZFg\/sagemaker-model-monitor-data-quality-on-random-cut-forest-configuration-issues",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":119,
        "Question_answer_count":0,
        "Question_body":"I am trying to set up a Random Cut Forest model with a Data Quality job attached.\nI managed to train and deploy the model with the \"data_capture\" feature enabled.\n\n``` python\n# Training\nrcf = sagemaker.RandomCutForest(\n    role=role,\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    data_location=f\"s3:\/\/{BUCKET}\/random_cut_forest\/input\",\n    output_path=f's3:\/\/{BUCKET}\/random_cut_forest\/output',\n    num_sample_per_tree=1024,\n    num_trees=50,\n    serializer=JSONSerializer(),\n    deserializer=CSVDeserializer()\n)\nrs = rcf.record_set(df_multi_measurements.drop(\"datetime\", axis=1).to_numpy())\nrcf.fit(rs, wait=False)\n```\n\n``` python\n# Deploy\ndata_capture_config = DataCaptureConfig(\n    enable_capture=True, \n    sampling_percentage=100, \n    destination_s3_uri=s3_capture_upload_path\n)\nrcf_inference = rcf.deploy(\n    initial_instance_count=1, \n    instance_type='ml.m4.xlarge',\n    endpoint_name=ENDPOINT_NAME,\n    data_capture_config=data_capture_config,\n    serializer=CSVSerializer(),\n    deserializer=JSONDeserializer(),\n    )\n```\nThen, I configured and started the ModelMonitor job\n\n``` python\n# Model Monitor\nmy_default_monitor = DefaultModelMonitor(\n    role=role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    volume_size_in_gb=5,\n    max_runtime_in_seconds=3600\n)\n\nmy_default_monitor.suggest_baseline(\n    baseline_dataset=baseline_data_uri + \"\/df_multi_measurements.csv\",\n    dataset_format=DatasetFormat.csv(header=True),\n    output_s3_uri=baseline_results_uri,\n    wait=True,\n    logs=False\n)\n\nmy_default_monitor.create_monitoring_schedule(\n    monitor_schedule_name=mon_schedule_name,\n    endpoint_input=rcf_inference.endpoint,\n    output_s3_uri=s3_report_path,\n    statistics=my_default_monitor.baseline_statistics(),\n    constraints=my_default_monitor.suggested_constraints(),\n    schedule_cron_expression=CronExpressionGenerator.hourly(),\n    enable_cloudwatch_metrics=True,\n)\n```\n\nBut at the first run of the job I got this error:\n\n> Error: Encoding mismatch: Encoding is CSV for endpointInput, but Encoding is JSON for endpointOutput. We currently only support the same type of input and output encoding at the moment.\n\nData captured looked like: \n```\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"4.150000013333333,3.330000003333333,...\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"application\/json\",\"mode\":\"OUTPUT\",\"data\":\"{\\\"scores\\\":[{\\\"score\\\":0.5794829282}]}\",\"encoding\":\"JSON\"}},\"eventMetadata\":{\"eventId\":\"79add993-68cf-4903-9dfe-8275d164496f\",\"inferenceTime\":\"2023-03-17T14:10:08Z\"},\"eventVersion\":\"0\"}\n...\n```\n\nSo later I tried to force input and output to be both CSV but no luck.\n\nAfter some tuning, I managed to instruct DataCapture to only collect requests in JSON so, since I couldn't change the output, now DataCapture has both input and output in the same (JSON) form. \n\nThe JSON requests look like this:\n``` json\n{\n    \"instances\": [\n        {\n            'features': [3.8600000533333336, 3.5966666533333336...]\n        }, \n        ...\n    ]\n}\n```\nand the model correctly works, returning its predictions: \n```\nb'{\"scores\":[{\"score\":0.6015237349},...]}'\n```\n\nData captured now looks like: \n```\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"application\/json\",\"mode\":\"INPUT\",\"data\":\"{\\\"instances\\\": [{\\\"features\\\": [3.8600000533333336, 3.5966666533333336, ...]}]}\",\"encoding\":\"JSON\"},\"endpointOutput\":{\"observedContentType\":\"application\/json\",\"mode\":\"OUTPUT\",\"data\":\"{\\\"scores\\\":[{\\\"score\\\":0.6015237349},{\\\"score\\\":0.4439660733},{\\\"score\\\":0.5100689867},{\\\"score\\\":0.5456048291},{\\\"score\\\":0.5099260466}]}\",\"encoding\":\"JSON\"}},\"eventMetadata\":{\"eventId\":\"27e2c9cd-3301-419c-8d06-9ede4c6380e6\",\"inferenceTime\":\"2023-03-17T17:10:18Z\"},\"eventVersion\":\"0\"}\n```\n\nBUT... at the first run of this new configuration, the job returns an error on the data analysis part.\n\nSo, after some search, I found that model monitor only works with tabular data or plain json, so I added a preprocessing step into the ModelMonitor\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-pre-and-post-processing.html\n\nThe preprocessing script looks like this:\n\n```\nimport json\nimport random\n\n\"\"\"\n{\n    \"instances\": [\n        {\n            'features': [3.8600000533333336, 3.5966666533333336...]\n        }\n        ...\n    ]\n}\n\"\"\"\n\ndef preprocess_handler(inference_record):\n    input_record = inference_record.endpoint_input.data\n    print(input_record)\n    input_record_dict = json.loads(input_record)\n    \n    features = input_record_dict[\"instances\"][0]['features']\n    \n    return { str(i).zfill(20) : d for i, d in enumerate(features) }\n\n```\n\nAnd now, at the first run, again, I get an error that this time is absolutely NOT understandable at all:\n```\n2023-03-17 18:08:46,326 ERROR Main: No usable value for features\n2023-03-17T19:08:46.935+01:00\tNo usable value for completeness\n2023-03-17T19:08:46.935+01:00\tDid not find value which can be converted into double\n```\n\nAt this stage I feel a bit stuck. \nHow can this be fixed? RCF and ModelMonitor should be easier to be integrated in my opinion.\n\nWhat I am doing wrong?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio in VPC Only fails to create Notebook job with \"Unexpected error occurred during creation of job.\"",
        "Question_created_time":1678982226078,
        "Question_last_edit_time":1679330199394,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbLVvWJbeS42j0e1nymPYcg\/sagemaker-studio-in-vpc-only-fails-to-create-notebook-job-with-unexpected-error-occurred-during-creation-of-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":184,
        "Question_answer_count":1,
        "Question_body":"The CloudWatch logs show following error\n```\n    Traceback (most recent call last):\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post\n        job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload)))\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async\n        result = await obj\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper\n        raise excep\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper\n        return await func(*args, **kwargs)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job\n        s3_file_uploader = await self._prepare_job_artifacts(\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts\n        input_uri = S3URI(runtime_environment_parameters.s3_input)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input\n        return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value)\n    AttributeError: 'NoneType' object has no attribute 'get'\n[E 2023-03-16 15:36:01.070 SchedulerApp] 'NoneType' object has no attribute 'get' Traceback (most recent call last): File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload))) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async result = await obj File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper raise excep File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper return await func(*args, **kwargs) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job s3_file_uploader = await self._prepare_job_artifacts( File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts input_uri = S3URI(runtime_environment_parameters.s3_input) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value) AttributeError: 'NoneType' object has no attribute 'get'\n```\nAlso all the \"advanced options\" are missing in the \"create notebook job\" dialogue.\n\nThe setting is isolated VPC with permissions updated accordingly: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/scheduled-notebook-policies.html\n\nThe VPC endpoints for S3, SageMaker API and Runtime, SSM, STS, Metrics, Logs, ECR API and ECR DKR are deployed in the VPC. Notebooks are working fine.\n\nAny idea what could be wrong?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1678987295301,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1678987295301,
        "Answer_comment_count":2.0,
        "Answer_body":"Thanks for reporting the issue. \n\nCan you try also configuring following two vpc endpoints? \n1. Amazon EC2\n2. Amazon EventBridge\nAlso, if you have s3 vpc gateway endpoint policy with fine control of allowed s3 bucet, please allow s3 access for sagemakerheadlessexecution-prod-* like below. \nFYI - you can find more reference link from https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-notebook-auto-execution-advanced.html\n\n```\n{\n   \"Action\":[\n      \"s3:*\"\n   ],\n   \"Resource\":[\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\",\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\/*\"\n   ],\n   \"Effect\":\"Allow\",\n   \"Sid\":\"SCTASK14554266\"\n}\n```",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How do I remove monitoring schedules in SageMaker?",
        "Question_created_time":1678980406405,
        "Question_last_edit_time":1679326924885,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTkFoQxi0RUy3Yn4ewaj0KA\/how-do-i-remove-monitoring-schedules-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":129,
        "Question_answer_count":1,
        "Question_body":"When I performed the following hands-on with SageMaker, I was in trouble because resources (endpoints and monitoring schedules) remained and I was still being billed.\nhttps:\/\/aws.amazon.com\/jp\/getting-started\/hands-on\/build-train-deploy-monitor-machine-learning-model-sagemaker-studio\/\n\nWhen I tried to delete the endpoint from the SageMaker console, I got the following error message.\n\"The Endpoint currently has one or more Monitoring Schedules. Please delete the Monitoring Schedules before deleting the Endpoint.\"\n\nIn the hands-on \"Step 10. Cleanup\" of the above URL, there is a description of how to delete the monitoring schedule and the endpoint, but since the domain has already been deleted, I tried to create a new domain and delete it. .\nHowever, the result is an error, and it cannot be deleted successfully.\n\nSince it cannot be deleted from the SageMaker console, please let me know if there is another way to delete it.\n\nThank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Inference endpoint zero-downtime deployment",
        "Question_created_time":1678967628277,
        "Question_last_edit_time":1679314603362,
        "Question_link":"https:\/\/repost.aws\/questions\/QUha49iJ2PT8aZJoV0RIGX1A\/sagemaker-inference-endpoint-zero-downtime-deployment",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_body":"Hello, \n\nDo you know how to deploy Sagemaker endopint with updated docker image using zero-downtime approach? \nCurrently it looks like the only option to deploy new docker image is endpoint recreation\n\nThanks, \nOK",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"MultiDataModel with different inference scripts",
        "Question_created_time":1678900781216,
        "Question_last_edit_time":1679248487591,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVF03ZNbBTL28_SaJ4RotQg\/multidatamodel-with-different-inference-scripts",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_body":"I found a similar posting [here](https:\/\/repost.aws\/questions\/QUEVxelof3TmimoLt1Kd1SBA\/how-to-configure-our-own-inference-py-for-two-different-py-torch-models-in-multi-data-model-to-build-single-endpoint-and-call-both-models-from-there) but I'm hoping my situation is a little simpler.\n\n---\n\n**Q: Is there a way to provide two separate inference scripts for each model in the multi-endpoint or does some dynamic\/custom inference script need to be made to handle both?**\n\nI have two model pipeline built using SageMaker Python SDK Scikit-learn processing\/models:\n* One is a clustering model to return cluster prediction and centroid distances when requesting inferencing\n* Other is simply PCA, returning 3-components when requesting inferencing\n---\n\n\nBecause of the odd formating of the data and the way that the output needs to be provided, both models are using custom inference scripts (e.g. predicting vs. transformation). \n\n\nFrom what I can see in the examples for MultiDataModel, it only accepts a single entry_point for inference.py when passing the model information and just the model artifacts are \"added\" later:\n\n```\ncluster_model = SKLearnModel(\n    model_data=cluster_artifact,\n    role=role,\n    entry_point=\"scripts\/cluster_inference.py\",\n    sagemaker_session=sagemaker_session\n)\npca_model = SKLearnModel(\n    model_data=pca_artifact,\n    role=role,\n    entry_point=\"scripts\/pca_inference.py\",\n    sagemaker_session=sagemaker_session\n)\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n```\n---\n\n\nDeployed as separate endpoints, both perform inference as expected, but I cannot get to function as one endpoint.\n\n\nBelow is the most recent error I receive, but it is hard to understand where is failing, seeing as the serialization should be handled properly in my inference script and when invoking the endpoint:\n\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from primary with message \"{\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Unsupported model output data type.\"}\".\n```\n\nAny pointers or alternatives are appreciated!\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1679382680431,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1679382680431,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi there!\n\nIn short, our SageMaker scikit-learn container do not currently support model specific inference script.\n\nThe entry_point script that you referenced in your MultiDataModel object will be the inference script used for all the models. If you've added logging in your script, you will be able to see them in CloudWatch logs.\n\nIf you have some pre\/post-processing script that needs to be performed on a specific model, you need to write them all in one universal inference.py . You can then add some extra attributes in your data when invoking the endpoint and have the same script read the extra attribute so it knows which pre\/post-processing script to perform.\n\nOne important thing to note is that although you reference a model object in MultiDataModel, i.e.\n\n```\n\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n```\n\n\nThe only information fetched from the model object is the image_uri and entry_point, these information is needed for during endpoint deployment.\n\nAll the model.tar.gz sitting in 'model_data_prefix' should not have a inference.py as it confuses the container and forces it to go back to default handler, hence why you're probably receiving ModelError.\n\n\nTry something like below:\n\n\n```\ncluster_model = SKLearnModel(\n    model_data=cluster_artifact,\n    role=role,\n    entry_point=\"scripts\/cluster_inference.py\",\n    sagemaker_session=sagemaker_session\n)\npca_model = SKLearnModel(\n    model_data=pca_artifact,\n    role=role,\n    entry_point=\"scripts\/pca_inference.py\",\n    sagemaker_session=sagemaker_session\n)\nmme = MultiDataModel(\n    name='model',\n    model_data_prefix=model_data_prefix,\t#make sure the directory of this prefix is empty, i.e. no models in this location\n    model= cluster_model,\n    sagemaker_session=sagemaker_session,\n)\n\n\nlist(mme.list_models()) # this should be empty\n\nmme.add_model(model_data_source=cluster_artifact, model_data_path='cluster_artifact.tar.gz')\t#make sure model artifact doesn't contain inference.py\nmme.add_model(model_data_source=pca_artifact, model_data_path='pca_artifact.tar.gz') #make sure model artifact doesn't contain inference.py\n\nlist(mme.list_models()) # there should be two models listed now, if you look at the location of model_data_prefix, there should also be two model artifact\n\noutput_cluster = predictor.predict(data='<your-data>', target_model='cluster_artifact.tar.gz')\nprint(output_cluster) #this should work since it's using the inference.py from cluster_inference.py\n\noutput_pca = predictor.predict(data='<your-data>', target_model='pca_artifact.tar.gz')\nprint(output_pca) \t#this might fail since it's using cluster_inference.py, add this model's inference script into cluster_inference.py to make it work \n\n```\n\nI know this approach is not ideal since if you have a new model with new pre\/post-processing script, you'd have to redeploy your endpoint for the new script to come into effect.\n\nWe actually just added support in our tensorflow container to allow model specific inference script: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/2680\n\nYou can request for the same feature for our scikit container here: https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/issues",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Using MLOPS template with custom inference code",
        "Question_created_time":1678881226207,
        "Question_last_edit_time":1679228693272,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVOG_sJsZSk-cvJvypdXY5A\/using-mlops-template-with-custom-inference-code",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":37,
        "Question_answer_count":0,
        "Question_body":"Hi MLOps Gurus,\n\nI'd like to seek guidance on my below situation.\n\nI am currently working on a Sagemaker project where I'm using the MLOPS template for model building, training, and deployment. I trained the model using the sklearn framework and registered it in the model registry. However, while creating the model deployment pipeline, I faced an issue with the default cloudformation template resources. Specifically, when attempting to use both the ModelPackageName and custom image as parameters for the model creation, I encountered an error. I discovered that Sagemaker expects a \"ModelDataUrl\" parameter when using a custom image. \n\nDefault Clouformation template:\n\n```\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n         - ModelPackageName: !Ref ModelPackageName\n      ExecutionRoleArn: !Ref ModelExecutionRoleArn\n```\n\n\nHow I modified:\n\n```\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n         - \n           Image: !Ref ImageURI\n           ModelDataUrl: !Ref ModelData\n           Mode: SingleModel #This defaults to single model change to \"MultiModel\" for MME\n           Environment: {\"SAGEMAKER_PROGRAM\": \"inference.py\", \n                         \"SAGEMAKER_SUBMIT_DIRECTORY\": !Ref ModelData}\n      ExecutionRoleArn: !Ref ModelExecutionRoleArn\n                       \n```\n\n\n\n\nMy question is: How can I retrieve the trained model from codebuild pipeline and add \"ModelDataUrl\" parameter  and dynamically pass it to the endpoint-config cloudformation template every time I execute the pipeline?\n\nPlease guide me the steps to progress, thank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I pass images to Sagemaker inference endpoint",
        "Question_created_time":1678750653423,
        "Question_last_edit_time":1679097240310,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL7CqOfnbQvyJTWjVdjyrIw\/how-do-i-pass-images-to-sagemaker-inference-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":211,
        "Question_answer_count":1,
        "Question_body":"I have successfully deployed my Sagemaker inference endpoint (using the bring your own container method).\nThe invocations endpoint is as follows:\n\n\n```\n@app.route(\"\/invocations\", methods=[\"POST\"])\ndef transformation():\n    \"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\n    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\n    just means one prediction per line, since there's a single column).\n    \"\"\"\n\n    # Get the file from the request\n    file_list = list(flask.request.files.values())\n    if not file_list:\n        return flask.Response(response=json.dumps({'error': 'no file found'}), status=400, mimetype='application\/json')\n\n    # Get the first file in the list (assuming there is only one)\n    img = file_list[0].read()\n\n    # Process the image as needed...\n    im2 = cv2.imdecode(np.frombuffer(img, np.uint8), cv2.IMREAD_UNCHANGED)\n\n    results = ScoringService.predict(im2)\n\n    xyxy = results.pandas().xyxy[0].values\n    preds = xyxy.tolist()\n\n    response_body = json.dumps({'predictions': preds})\n    return flask.Response(response=response_body, status=200, mimetype='application\/json')\n```\n\nI am having issues with calling the endpoint, is multipart\/form-data accepted? Or do I need to pass my image as base64 and decode it in my Sagemaker endpoint?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to extend the total processing time for Asynchronous endpoint upto 60 minutes?",
        "Question_created_time":1678439351698,
        "Question_last_edit_time":1678787142156,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzMAZWe2vRY2CsKp7Xyexqg\/how-to-extend-the-total-processing-time-for-asynchronous-endpoint-upto-60-minutes",
        "Question_score_count":1,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_body":"I'm facing the below error when asynchronous endpoint is invoked in sagemaker for batch processing:\n\n```\n\"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"\n```\nI found that the issue is due to the total processing time exceeds the default 15min window. \n\nSo, I would like to know how to extend the total processing time for Asynchronous endpoint upto 60 minutes as mentioned in aws docs: [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html]()\n\nThank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1678872606500,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1678872606500,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi, I found the solution to extend the total processing time for Asynchronous endpoint upto 60 minutes. \nUse this parameter \"InvocationTimeoutSeconds=3600\" when invoking the async endpoint.\n\ncode looks like below:\n\n```\nresponse = sm_client.invoke_endpoint_async(\nEndpointName=endpoint_name,\nInputLocation=input_file_s3_path,\nContentType='text\/csv',\nAccept='application\/json',\nInvocationTimeoutSeconds=3600    \n)\n```\nThank you.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"SageMaker TensorFlow Object Detection: Null annotations raises exception",
        "Question_created_time":1678399446016,
        "Question_last_edit_time":1678746872707,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn-HbCAv8RS6MBgNr83PPrA\/sagemaker-tensorflow-object-detection-null-annotations-raises-exception",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":0,
        "Question_body":"Hi there, nice to meet you all, \n\nI've been trying to train an Object Detection Model (using Built-in Algoritms, Tensorflow) following the [jumpstart examples](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/jumpstart_object_detection\/Amazon_JumpStart_Object_Detection.ipynb) as template, but as soon as I provide a null annotation sagemaker fails to train when calling fit(), and throws the following error (at the end of the post is the entire stacktrace)\n\n\n```\nValueError: Invalid dimensions for box data: (0,)\n```\n\n\nAs I understand, according to the mentioned example the annotations.json should have a COCO structure-like in order to Sagemaker considers it has valid, quoting the same tutorial: \n\n> The annotations.json file should have information for bounding_boxes and their class labels. It should have a dictionary with keys \"images\" and \"annotations\". Value for the \"images\" key should be a list of entries, one for each image of the form {\"file_name\": image_name, \"height\": height, \"width\": width, \"id\": image_id}. Value of the 'annotations' key should be a list of entries, one for each bounding box of the form {\"image_id\": image_id, \"bbox\": [xmin, ymin, xmax, ymax], \"category_id\": bbox_label}.\n>\n\n That is, as far as I know the COCO format (it should be explicitly mentioned i think!)  \n\nGreat!, So what the issue here? \nIf I provide a dataset with equal n\u00b0 of images and annotations, thats great and I can train sucessfully my model, but if I have an image with no object in it, then i will have more images than annotations, for instance: \n\n0001.jpeg 1 has object and the corresponding annotation\n\n0002.jpeg has **no** object, so it doest have an annotation \n\nSo the annotation.json file looks like:\n\n`\n{\n  \"images\": [\n    {\n      \"file_name\": \"0001.jpeg\",\n      \"height\": 1944,\n      \"width\": 2592,\n      \"id\": \"0001\"\n    },\n    {\n      \"file_name\": \"0002.jpeg\",\n      \"height\": 1944,\n      \"width\": 2592,\n      \"id\": \"0002\"\n    }\n  ],\n  \"annotations\": [\n    {\n      \"image_id\": \"0001\",\n      \"bbox\": [\n        688,\n        371,\n        1859,\n        1581\n      ],\n      \"category_id\": 0\n    }\n  ]\n}\n`\n\nAs far as i could [investigate](https:\/\/github.com\/cocodataset\/cocoapi\/issues\/298), this is the standrad proccedure when no object is available, I've also downloaded an [entire annotation coco dataset 2017](https:\/\/cocodataset.org\/#download) to make sure of this (you can download \"2017 Train\/Val annotations [241MB]\" and search of instances_val2017.json and look for files with ID 25593, 41488, 42888 ... and you'll see that there are the images ones, but not the annotations ones)\n\nSo, I would like gently ask for your help, so I can properly train my model! \n\nThanks in advance! \n\nP.S:\n\n TraceBack\n\n```\n[Epoch 0], Speed: 0.058 samples\/sec, loss=431737.90625.\nTraceback (most recent call last):\n  File \"\/opt\/ml\/code\/transfer_learning.py\", line 246, in <module>\nrun_with_args(args)\n  File \"\/opt\/ml\/code\/transfer_learning.py\", line 201, in run_with_args\ntrain_and_save_model(\n  File \"\/opt\/ml\/code\/train.py\", line 130, in train_and_save_model\nvalidation_losses = run_validation(detection_model, validation_data, batch_size, image_size, epoch)\n  File \"\/opt\/ml\/code\/validation.py\", line 25, in run_validation\n    losses_dict = model.loss(prediction_dict, shapes)\n  File \"\/opt\/ml\/code\/object_detection\/meta_architectures\/ssd_meta_arch.py\", line 824, in loss\n) = self._assign_targets(\n  File \"\/opt\/ml\/code\/object_detection\/meta_architectures\/ssd_meta_arch.py\", line 1013, in _assign_targets\ngroundtruth_boxlists = [box_list.BoxList(boxes) for boxes in groundtruth_boxes_list]\n  File \"\/opt\/ml\/code\/object_detection\/meta_architectures\/ssd_meta_arch.py\", line 1013, in <listcomp>\ngroundtruth_boxlists = [box_list.BoxList(boxes) for boxes in groundtruth_boxes_list]\n  File \"\/opt\/ml\/code\/object_detection\/core\/box_list.py\", line 55, in __init__\nraise ValueError(\"Invalid dimensions for box data: {}\".format(boxes.shape))\nValueError: Invalid dimensions for box data: (0,)\n2023-03-09 21:59:57,046 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n2023-03-09 21:59:57,047 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n2023-03-09 21:59:57,048 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n2023-03-09 21:59:57,048 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise ValueError(\"Invalid dimensions for box data: {}\".format(boxes.shape))\n ValueError: Invalid dimensions for box data: (0,)\"\nCommand \"\/usr\/local\/bin\/python3.9 transfer_learning.py --batch_size 5 --beta_1 0.9 --beta_2 0.999 --early_stopping False --early_stopping_min_delta 0.0 --early_stopping_patience 5 --epochs 10 --epsilon 1e-07 --initial_accumulator_value 0.1 --learning_rate 0.001 --momentum 0.9 --optimizer adam --reinitialize_top_layer Auto --rho 0.95 --train_only_top_layer False\"\n2023-03-09 21:59:57,048 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n\n2023-03-09 22:00:14 Uploading - Uploading generated training model\n2023-03-09 22:00:20 Failed - Training job failed\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\nCell In[19], line 3\n      1 #with load_run(experiment_name=demo_experiment.experiment_name, run_name=demo_trial.trial_name) as run:\n      2     #run.log_parameter(\"param1\", \"value1\")\n----> 3 od_estimator.fit(\n      4         {\"training\": train_path},\n      5         # {\"training\": train_path, \"validation\": validation_path}}, \n      6         logs=True, \n      7         job_name=training_job_name, \n      8         experiment_config = {\n      9             # \"ExperimentName\"\n     10             \"TrialName\" : demo_trial.trial_name,\n     11             \"TrialComponentDisplayName\" : \"TrainingJob\",\n     12     })\n\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker\/workflow\/pipeline_context.py:272, in runnable_by_pipeline.<locals>.wrapper(*args, **kwargs)\n    268         return context\n    270     return _StepArguments(retrieve_caller_name(self_instance), run_func, *args, **kwargs)\n--> 272 return run_func(*args, **kwargs)\n\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py:1163, in EstimatorBase.fit(self, inputs, wait, logs, job_name, experiment_config)\n   1161 self.jobs.append(self.latest_training_job)\n   1162 if wait:\n-> 1163     self.latest_training_job.wait(logs=logs)\n\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py:2311, in _TrainingJob.wait(self, logs)\n   2309 # If logs are requested, call logs_for_jobs.\n   2310 if logs != \"None\":\n-> 2311     self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2312 else:\n   2313     self.sagemaker_session.wait_for_job(self.job_name)\n\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker\/session.py:4176, in Session.logs_for_job(self, job_name, wait, poll, log_type)\n   4173             last_profiler_rule_statuses = profiler_rule_statuses\n   4175 if wait:\n-> 4176     self._check_job_status(job_name, description, \"TrainingJobStatus\")\n   4177     if dot:\n   4178         print()\n\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker\/session.py:3707, in Session._check_job_status(self, job, desc, status_key_name)\n   3701 if \"CapacityError\" in str(reason):\n   3702     raise exceptions.CapacityError(\n   3703         message=message,\n   3704         allowed_statuses=[\"Completed\", \"Stopped\"],\n   3705         actual_status=status,\n   3706     )\n-> 3707 raise exceptions.UnexpectedStatusException(\n   3708     message=message,\n   3709     allowed_statuses=[\"Completed\", \"Stopped\"],\n   3710     actual_status=status,\n   3711 )\n\nUnexpectedStatusException: Error for Training job TrainNullAnnotations-tensorflow-od1-ssd-2023-03-09-21-48-59-470: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise ValueError(\"Invalid dimensions for box data: {}\".format(boxes.shape))\n ValueError: Invalid dimensions for box data: (0,)\"\nCommand \"\/usr\/local\/bin\/python3.9 transfer_learning.py --batch_size 5 --beta_1 0.9 --beta_2 0.999 --early_stopping False --early_stopping_min_delta 0.0 --early_stopping_patience 5 --epochs 10 --epsilon 1e-07 --initial_accumulator_value 0.1 --learning_rate 0.001 --momentum 0.9 --optimizer adam --reinitialize_top_layer Auto --rho 0.95 --train_only_top_layer False\", exit code: 1\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Lost changes in SageMaker Studio notebooks",
        "Question_created_time":1678312214745,
        "Question_last_edit_time":1678660150271,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7V_4NNtqSBqoS5igt4407g\/lost-changes-in-sagemaker-studio-notebooks",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":117,
        "Question_answer_count":1,
        "Question_body":"I have repeatedly encountered issue with lost recent changes in SageMaker Studio Collaborative space. It occurs when the session expires and requires reconnection. Even though the notebook instance is running, the checkpoint files are missing and the notebook does not display at all. When checking the file via terminal, previous (even 11h old) state is displayed. Any idea how to prevent the issue?\n\nI am using VpcOnly mode with private subnets only. There are two active users but only one is currently using the collaborative space. The notebooks often runs for couple of hours.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Async endpoint autoscaling - how to make it work well?",
        "Question_created_time":1678276412863,
        "Question_last_edit_time":1678624170327,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ7A1xwfwTVOEXRCB95Ns-Q\/sagemaker-async-endpoint-autoscaling-how-to-make-it-work-well",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":59,
        "Question_answer_count":0,
        "Question_body":"Hi There, \nI'm in trouble with autoscaling related to Sagemaker Async Endpoint. In Particular, I have 3 cloudwatch alarms that trigger the scaling policy: \n- ApproximateBacklogSizePerInstance < 4.5 for 15 datapoints within 15 minutes\n- ApproximateBacklogSizePerInstance > 5 for 3 datapoints within 3 minutes\n- HasBacklogWithoutCapacity >= 1 for 1 datapoints within 1 minute\n\nWhen the scaling out happens, my endpoint remains stucked in updating status until all the processing of enqueued messages is over. This implies in some errors like:\n- *The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch log for this endpoint.* -> Because of this error, several instances are created and destroyed (I checked in cloudTrail history actions). This results in some messages being \"stranded\" during processing, which unfortunately is not completed anymore. This only resumes when another scaling IN action is performed. (If, for example, I have 50 messages to process, my endpoint will reach the configured maximum number of instances and produce the above error. Than, it will not process some messages. Processing of these will only resume when a scaling IN is performed).\n\n- *Received error: \"Resource endpoint\/endpoint_name\/variant\/AllTraffic was not in a scalable state due to reason: Endpoint X is in Updating state, should be in InService state to scale.\"*;\n\nMoreover, when the scaling in to 0 instances is performed and there are some messages that are being processed (in a number less than our threshold), some of them remains \"stranded\" (similar to the first point) and are no longer processed (no errors are produced). Only when another scale activity is performed, these messages becomes again \"visible\" and so processed (it is as if there was a visibility timeout for the queue inside the Sagemaker Endpoint).\n\nHow could I solve these problems?? Seems to be a bug.\n\nThe python code I'm using to create the Endpoint is the following:\n\n```\nmodel = TensorFlowModel(\n    source_dir=f'..\/models\/{model_name}\/code',\n    entry_point='inference.py',\n    sagemaker_session=session,\n    model_data=model_data_tar_gz, \n    role=tf_model_role, \n    image_uri=image_uri,\n    name=model_sg_name,\n    code_location=final_model_output,\n    env={\n        \n        'OMP_NUM_THREADS': '1',\n        'SAGEMAKER_TFS_INTRA_OP_PARALLELISM': '1', # Setting this environment variable to the number of available physical cores is recommended. (g5.4x -> 16)\n\n    }\n    ,\n    vpc_config={\n        \"Subnets\": subnets,\n        \"SecurityGroupIds\": security_groups\n    }\n)\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type=instance_type,\n    endpoint_name=endpoint_name,\n    async_inference_config=AsyncInferenceConfig(\n        output_path=out_inference,\n        max_concurrent_invocations_per_instance=max_invocation_instance,\n        notification_config={\n            \"SuccessTopic\": success_topic,\n            \"ErrorTopic\": error_topic\n        }\n    )\n)\n\n\nclient = boto3.client('application-autoscaling', **auth_kwargs)\nresponse = client.register_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n    MinCapacity=scaling_min,\n    MaxCapacity=scaling_max\n)\n\n\nresponse = client.put_scaling_policy(\n    PolicyName='Invocations-ScalingPolicy',\n    ServiceNamespace='sagemaker', \n    ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n    PolicyType='TargetTrackingScaling',\n    TargetTrackingScalingPolicyConfiguration={\n        'TargetValue': queue_target_value,\n        'CustomizedMetricSpecification': {\n            'MetricName': 'ApproximateBacklogSize',\n            'Namespace': 'AWS\/SageMaker',\n            'Dimensions': [{'Name': 'EndpointName', 'Value': endpoint_name}],\n            'Statistic': 'Average',\n        },\n        'ScaleInCooldown': 300\n        'ScaleOutCooldown': 120\n    }\n)\n\nresponse_scaling = client.put_scaling_policy(\n    PolicyName=\"HasBacklogWithoutCapacity-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    PolicyType=\"StepScaling\",\n    StepScalingPolicyConfiguration={\n        \"AdjustmentType\": \"ChangeInCapacity\",\n        \"MetricAggregationType\": \"Average\",\n        \"Cooldown\": 300,\n        \"StepAdjustments\":[ \n            {\n              \"MetricIntervalLowerBound\": 0,\n              \"ScalingAdjustment\": 1\n            }\n        ]\n    } \n)\ncw_client = boto3.client('cloudwatch', **auth_kwargs)\nresponse = cw_client.put_metric_alarm(\n    AlarmName=f\"{endpoint_name}\/Backlog-without-capacity\",\n    MetricName='HasBacklogWithoutCapacity',\n    Namespace='AWS\/SageMaker',\n    Statistic='Average',\n    EvaluationPeriods= 1,\n    DatapointsToAlarm= 1,\n    Threshold= 1,\n    ComparisonOperator='GreaterThanOrEqualToThreshold',\n    TreatMissingData='missing',\n    Dimensions=[{ 'Name':'EndpointName', 'Value': endpoint_name }],\n    Period= 60,\n    AlarmActions=[response_scaling['PolicyARN']]\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"CreateTransformJob batching",
        "Question_created_time":1678242068792,
        "Question_last_edit_time":1678588766256,
        "Question_link":"https:\/\/repost.aws\/questions\/QUz6kbe7hcSfiB9seNmv0D1w\/createtransformjob-batching",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_body":"I'm using CreateTransformJob to submit a sagemaker inference task. I have a single input file to process consisting of 25k jsonl records, the total filesize is 2.5MB and the model is a simple PyTorch text classifier.\n\nI just want to confirm my understanding of how transform jobs work. I've configured the job to use Line batching. It seems though that because MaxPayloadInMB is an integer, setting it to 1 will result in 3 (not so mini) batches (i.e. each batch is 10k records)?\n\nSo assuming I use an instance with 4 vCPU's and MaxConcurrentTransforms=4 then it should run all 4 batches in parallel? But there doesn't seem to be any reason to use a larger instance \/ more instances to increase throughput since if my understanding is correct there's no way to explicitly set the minibatch size any smaller than MaxPayloadInMB \/\/ size_of_file_in_mb? \n\nAm I correct? If I want smaller mini-batches do I need to manually split the file myself and then manually reassemble it?\n\n    \"MaxConcurrentTransforms\": max_concurrent_transforms, \n    \"MaxPayloadInMB\": max_payload,\n    \"BatchStrategy\": \"MultiRecord\",\n    \"TransformOutput\": {\n        \"S3OutputPath\": batch_output,\n        \"AssembleWith\": \"Line\",\n        \"Accept\": \"application\/jsonlines\",\n    },",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can we connect to the instance (via ssh or other means) where a Triton Sagemaker endpoint is deployed?",
        "Question_created_time":1678225961811,
        "Question_last_edit_time":1678573626735,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8-U_XgPVRSuLTSXf8eW8fA\/can-we-connect-to-the-instance-via-ssh-or-other-means-where-a-triton-sagemaker-endpoint-is-deployed",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":92,
        "Question_answer_count":1,
        "Question_body":"I am deploying a Triton server endpoint on Sagemaker and I want to ssh into the instance where the endpoint is running for debugging purposes. I can't find a way to identify the instance (e.g. find the instance id) and connect to it. I saw this repo, https:\/\/github.com\/aws-samples\/sagemaker-ssh-helper#inference, but it seems it only works with estimator objects on Sagemaker. I was wondering if there is a way to do the same thing with Sagemaker client objects: botocore.client.SageMaker. I am using the Sagemaker client to create the endpoint with the Triton image on aws ECR.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ML Training Models - Estimate cost",
        "Question_created_time":1678074275669,
        "Question_last_edit_time":1678421639428,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq_9pbj8QSUi2uTGuN_KTPA\/ml-training-models-estimate-cost",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_body":"How can I estmate the cost for a training model for a small data set, that runs on a continuous basis ? What size comes under a small data set?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker keeps charging even after deleteing the domains",
        "Question_created_time":1678052938476,
        "Question_last_edit_time":1678399406213,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaROFLLBMRzq9WXwa6tjvJQ\/sagemaker-keeps-charging-even-after-deleteing-the-domains",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":66,
        "Question_answer_count":2,
        "Question_body":"Hi,\nI was using dataset in sagemaker data wrangler to discover how it works . I deleted the sagemaker domain without removing all the resources and i keep getting bills because of this . How can I fix this.\n\nThank you for your help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Set MaxConcurrentRuns and MaxRetries in Jupyter Notebook Job",
        "Question_created_time":1677967103348,
        "Question_last_edit_time":1678314269065,
        "Question_link":"https:\/\/repost.aws\/questions\/QUc_ISS1RySAeId7E7ccaHSQ\/set-maxconcurrentruns-and-maxretries-in-jupyter-notebook-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_body":"We have a Jupyter Notebook Glue Job and We're calling start_job_run from a Lambda with Python with boto3, we would like to change the default MaxConcurrentRuns (1) and MaxRetries (3) of the Job, as a Notebook we need to use magics, we already tried with magics and also we tried using Arguments in the Lambda that runs the Job, nothing seems to work, how should we set those settings to guarantee the Jupyter Notebook never will retry and will have a concurrency of 5, for example, we would like to have max 5 Jobs running at the same time.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to choose which Spark kernel to use in SageMaker Studio?",
        "Question_created_time":1677882889228,
        "Question_last_edit_time":1678229266306,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn8mt0IDjTaedCXHB1pqtvw\/how-to-choose-which-spark-kernel-to-use-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"Available Amazon SageMaker Kernels include [the following two Spark kernels](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-kernels.html):\n\n- PySpark (SparkMagic) with Python 3.7\n- Spark (SparkMagic) with Python 3.7\n- Spark Analytics 1.0\n- Spark Analytics 2.0\n\nAnd at re:Invent 2022 there was [an announcement](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2022\/09\/sagemaker-studio-supports-glue-interactive-sessions\/) that \"SageMaker Studio now supports Glue Interactive Sessions.\" \"The built-in Glue PySpark or Glue Spark kernel for your Studio notebook to initialize interactive, serverless Spark sessions.\"\n\nIt seems like the benefits of using one of the Glue Spark kernels are that you can \"quickly browse the Glue data catalog, run large queries, and interactively analyze and prepare data using Spark, right in your Studio notebook.\" But can't you already do all that with the existing two SageMaker kernels?\n\nIn other words, how do you choose whether to use one of the existing two SparkMagic kernels in SageMaker Studio notebooks or to use this new Glue Interactive Sessions feature?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Neo Compilation Input Error - Converting Tensflor to CoreML",
        "Question_created_time":1677871976605,
        "Question_last_edit_time":1678217803663,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHIi_MjGpQIiSBrB6GjOLFg\/sagemaker-neo-compilation-input-error-converting-tensflor-to-coreml",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":32,
        "Question_answer_count":0,
        "Question_body":"Hi all,\n\nI have a model that I've finetuned from the TF MobileResnet model available in SageMaker. The model is working well, and returns correct inference values when I host it using the SageMaker inference endpoints.\n\nI am now trying to convert the model to coreml, however when attempting to run this job via SageMaker Neo I am receiving the following error:\n\n```\nClientError: InputConfiguration: Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"mil\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed. tensorflow 2.2.0, coremltools 4.0 For further troubleshooting common failures please visit: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-compilation.html\n```\n\nI cannot find any information about this error in the docs linked in the error message. Any help would be greatly appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"h=Help():  I can't seem to shut off amazon sagemker!",
        "Question_created_time":1677807394572,
        "Question_last_edit_time":1678154362950,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhDbEJf51T6C7ilCwu6eLIw\/h-help-i-can-t-seem-to-shut-off-amazon-sagemker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_body":"There are some sage maker resources that spun up and began running two days ago. I definietely did not start any sagemaker resources in the last week. And I was assured they were shut down before then. I can't see any resources in the dashboard and I'm not sure what is running, or how to shut it down. Please help! Whatever it is, it's burning through$40 a day!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to use nbextension in custom images for SageMaker Studio Notebooks, and how?",
        "Question_created_time":1677799061470,
        "Question_last_edit_time":1678145571206,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwg9u9IB4SdezY_DAe3AyVQ\/is-it-possible-to-use-nbextension-in-custom-images-for-sagemaker-studio-notebooks-and-how",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":28,
        "Question_answer_count":0,
        "Question_body":"I'm not asking for help on debugging this particular error, but more for a general answer for Is it possible to use nbextension in custom images for SageMaker Studio Notebooks, and how?\n\n**As background:**\nI fail on [step 2](https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions#2-install-javascript-and-css-files) of the Installation of `jupyter_contrib_nbextensions`.\n\nThe reason I'm trying to install that is because it's a prerequisite to [using Black in Jupyter Notebook](https:\/\/www.freecodecamp.org\/news\/auto-format-your-python-code-with-black\/#:~:text=Black%20in%20Jupyter%20Notebook,notebook's%20code%20cell%20by%20black.&text=A%20toolbar%20button.,default%3A%20Ctrl%2DB).).\n\nWhen I run `!jupyter contrib nbextension install --user` in a cell of SageMaker Studio that's using a custom SageMaker image based on `continuumio\/miniconda3:4.9.2` and the following environment:\n```\nname: base\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - numpy\n  - awscli\n  - boto3\n  - ipykernel\n  - black\n  - jupyter_contrib_nbextensions\n```\n\nThe error I get is:\n```\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2720, in _dep_map\n    return self.__dep_map\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2835, in __getattr__\n    raise AttributeError(attr)\nAttributeError: _Distribution__dep_map\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/bin\/jupyter-contrib\", line 10, in <module>\n    sys.exit(main())\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/jupyter_core\/application.py\", line 277, in launch_instance\n    return super().launch_instance(argv=argv, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/traitlets\/config\/application.py\", line 1041, in launch_instance\n    app = cls.instance(**kwargs)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/traitlets\/config\/configurable.py\", line 551, in instance\n    inst = cls(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/jupyter_contrib_core\/application.py\", line 27, in __init__\n    self._refresh_subcommands()\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/jupyter_contrib_core\/application.py\", line 43, in _refresh_subcommands\n    get_subcommands_dict = entrypoint.load()\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2476, in load\n    self.require(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2499, in require\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 820, in resolve\n    new_requirements = dist.requires(req.extras)[::-1]\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2755, in requires\n    dm = self._dep_map\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2722, in _dep_map\n    self.__dep_map = self._filter_extras(self._build_dep_map())\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 2737, in _filter_extras\n    invalid_marker(marker) or not evaluate_marker(marker)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 1415, in invalid_marker\n    evaluate_marker(text)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/__init__.py\", line 1433, in evaluate_marker\n    return marker.evaluate()\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/_vendor\/packaging\/markers.py\", line 245, in evaluate\n    return _evaluate_markers(self._markers, current_environment)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/_vendor\/packaging\/markers.py\", line 151, in _evaluate_markers\n    groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/_vendor\/packaging\/markers.py\", line 109, in _eval_op\n    return spec.contains(lhs, prereleases=True)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/_vendor\/packaging\/specifiers.py\", line 565, in contains\n    normalized_item = _coerce_version(item)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/_vendor\/packaging\/specifiers.py\", line 36, in _coerce_version\n    version = Version(version)\n  File \"\/opt\/conda\/lib\/python3.9\/site-packages\/pkg_resources\/_vendor\/packaging\/version.py\", line 197, in __init__\n    raise InvalidVersion(f\"Invalid version: '{version}'\")\npkg_resources.extern.packaging.version.InvalidVersion: Invalid version: 'cpython'\n```",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Failed to launch app from custom SageMaker image: ResourceNotFoundError with UID\/GID in AppImageConfig",
        "Question_created_time":1677769405445,
        "Question_last_edit_time":1678115477622,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcnpBQpC8TtKPIFBWax-LvA\/failed-to-launch-app-from-custom-sagemaker-image-resourcenotfounderror-with-uid-gid-in-appimageconfig",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":223,
        "Question_answer_count":1,
        "Question_body":"I'm trying to create a [custom SageMaker image][1] and launch a kernel from it, because I want to see if I can use black, the python code formatter, in [SageMaker][2] Studio via a custom SageMaker image.\n\nSo far, I've been able to attach the image to the SageMaker domain and start launching the kernel from the custom image, following [these steps][1]. However, as soon as the notebook opens, it displays this error in the notebook:\n\n> Failed to launch app [black-conda-ml-t3-medium-7123fdb901f81ab5]. ResourceNotFoundError: SageMaker is unable to launch the app using the image [123456789012.dkr.ecr.us-east-1.amazonaws.com\/conda-sample@sha256:12345]. Ensure that the UID\/GID provided in the AppImageConfig matches the default UID\/GID defined in the image. (Context: RequestId: 21234b0f568, TimeStamp: 1677767016.2990377, Date: Thu Mar 2 14:23:36 2023)\n\n\nHere are the relevant code snippets:\nDockerfile:\n```\nFROM continuumio\/miniconda3:4.9.2\n\nCOPY environment.yml .\nRUN conda env update -f environment.yml --prune\n```\nenvironment.yml:\n```\nname: base\nchannels:\n - conda-forge\ndependencies:\n - python=3.9\n - numpy\n - awscli\n - boto3\n - ipykernel\n - black\n```\n\nand AppImageConfig:\n```\n{\n    \"AppImageConfigName\": \"conda-env-kernel-config\",\n    \"KernelGatewayImageConfig\": {\n        \"KernelSpecs\": [\n            {\n                \"Name\": \"python3\",\n                \"DisplayName\": \"Python [conda env: myenv]\"\n            }\n        ],\n        \"FileSystemConfig\": {\n            \"MountPath\": \"\/root\",\n            \"DefaultUid\": 0,\n            \"DefaultGid\": 0\n        }\n    }\n}\n```\n\nI tried following [this troubleshooting guide][3], but it doesn't seem to address my issues because all of the diagnostics worked alright. For example, when I ran `id -u` and `id -g` inside my local container, the results `0` and `0` lined up with the AppImageConfig settings of `\"DefaultUid\": 0, \"DefaultGid\": 0`.\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-byoi-create.html\n  [2]: https:\/\/github.com\/psf\/black\n  [3]: https:\/\/github.com\/aws-samples\/sagemaker-studio-custom-image-samples\/blob\/main\/DEVELOPMENT.md",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to keep track of artifacts in via sagemaker experiments ?",
        "Question_created_time":1677699193781,
        "Question_last_edit_time":1678046071303,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmg-uH2cXT9ykxcanTohxAg\/how-to-keep-track-of-artifacts-in-via-sagemaker-experiments",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":36,
        "Question_answer_count":0,
        "Question_body":"based on the docs here , https:\/\/sagemaker.readthedocs.io\/en\/stable\/experiments\/sagemaker.experiments.html. I can use the sagemaker api or experiments package to start a trial\/experiment and record it's  run , also I can choose what all i want to track - parameters , metrics ... of each run. I assume here, there are lot of different type of things we are track , such as artifacts, metrics , logs, scripts .... , and this can be all viewed in\/within sagemaker. artifacts lives in some s3 bucket , i guess, but when it comes to the scripts that we use for training, evaluations  or even preprocessing , how does it keep track of these. i assume the default is simply that there is some s3 location that saves all of the artifacts related to each run. can the scripts live in code repository, aws code commit and we can link the all the scripts associated with a run to a link in code commit? or can other git repositories , github or gitlab linked in such way?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Sagemaker Project from template Model building testing and deployment failing default pipeline.",
        "Question_created_time":1677675538986,
        "Question_last_edit_time":1678022390643,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkI4ldx2oR2OGvuMUDKiOoQ\/aws-sagemaker-project-from-template-model-building-testing-and-deployment-failing-default-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_body":"After creating the domain and adding a user for AWS Sagemaker, I opened Sagemaker Studio and created a project using the template \"Model building Training and deployment\", project got successfully created and I also cloned the files from template to local in studio, till this point it was fine,\n\nNow, when I am seeing the pipelines, I see a pipeline already runs by default after project creation, but here this pipeline is failing with the below error from output tab :-\n\"**ClientError: Failed to invoke sagemaker:CreateProcessingJob. Error Details: The account-level service limit 'ml.m5.xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.**\" \n\nHere is the screenshot - ![Failure_Screenshot_From_project_Pipeline](\/media\/postImages\/original\/IMlUFN0FJHTuyWlUe4p5YTRw)\n\nPlease help me with this, I am new to SageMaker.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to get semantic segmentation output using Batchtransform",
        "Question_created_time":1677610581602,
        "Question_last_edit_time":1677957690583,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdovmjhHrRbq-FMvQrJjBdw\/how-to-get-semantic-segmentation-output-using-batchtransform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":0,
        "Question_body":"Hello, \n\nI've been trying to use batch transform to perform inference on a semantic segmentation problem but I've been struggling to get the correct output. \n\nThe code of my estimator is:  \n\n```\nss_estimator.set_hyperparameters(\n    backbone=\"resnet-101\",  # This is the encoder. Other option is resnet-101\n    algorithm=\"fcn\",  # This is the decoder. Other options are 'psp' and 'deeplab'\n    use_pretrained_model=\"True\",  # Use the pre-trained model.\n    crop_size=320,  # Size of image random crop.\n    num_classes=3,  # Pascal has 21 classes. This is a mandatory parameter.\n    epochs=30, # Number of epochs to run.\n    learning_rate=0.0001,                             \n    optimizer='adam', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n    lr_scheduler='poly', # Other options include 'cosine' and 'step'.   \n    mini_batch_size=4,  # Setup some mini batch size.\n    validation_mini_batch_size=4,\n    early_stopping=True,  # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n    early_stopping_patience=5,  # Tolerate these many epochs if the mIoU doens't increase.\n    early_stopping_min_epochs=2,  # No matter what, run these many number of epochs.\n    num_training_samples=num_training_samples,  # This is a mandatory parameter, 1464 in this case.\n)\n```\n\n\nAfter training the model, I am configuring the batch transform as below: \n\n```\ntimestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\nbatch_job_name = \"image-segmentation-model-batch\" + timestamp\nmodel_name = \"DEMO-full-image-segmentation-model\" + time.strftime(\n    \"-%Y-%m-%d-%H-%M-%S\", time.gmtime()\n)\noutput_folder=\"s3:\/\/client\/private_datasets\/experiments\/batch_transform_output\/\"\nbatch_input = \"s3:\/\/client\/private_datasets\/experiments\/test-images-small\/\"\nrequest = {\n    \"TransformJobName\": batch_job_name,\n    \"ModelName\": model_name,\n    \"MaxConcurrentTransforms\": 16,\n    \"MaxPayloadInMB\": 6,\n    \"BatchStrategy\": \"SingleRecord\",\n    \"TransformOutput\": {\"S3OutputPath\": \"{}\/output\".format(output_folder)},\n    \"TransformInput\": {\n        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input}},\n        \"ContentType\": 'image\/jpeg',\n        \"SplitType\": \"None\",\n        \"CompressionType\": \"None\",\n    },\n    \"TransformResources\": {\"InstanceType\": \"ml.c5.xlarge\", \"InstanceCount\": 1},\n}\n\nprint(\"Transform job name: {}\".format(batch_job_name))\nprint(\"\\nInput Data Location: {}\".format(batch_input))\n```\n\nAnd I'm starting the batch transform job as:\n\n```\nsagemaker = boto3.client(\"sagemaker\")\nsagemaker.create_transform_job(**request)\n\nprint(\"Created Transform job with name: \", batch_job_name)\n\nwhile True:\n    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n    status = response[\"TransformJobStatus\"]\n    if status == \"Completed\":\n        print(\"Transform job ended with status: \" + status)\n        break\n    if status == \"Failed\":\n        message = response[\"FailureReason\"]\n        print(\"Transform failed with the following error: {}\".format(message))\n        raise Exception(\"Transform job failed\")\n    time.sleep(30)\n```\n\nMy problem is that the batch transform outputs a \"*.out\" file, which I have no idea how to open.\n\nCan you help me out? Or is there some tutorial to use batch transform for mask prediction\/image segmentation?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to select a compute type in SageMaker Studio Lab",
        "Question_created_time":1677605308755,
        "Question_last_edit_time":1677951794496,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGfsf-bZJQP-2cOtlvFcEYA\/unable-to-select-a-compute-type-in-sagemaker-studio-lab",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":46,
        "Question_answer_count":2,
        "Question_body":"I have recently started using Studio Lab for batch training a deep learning model. However, when I try to create a notebook job, I am unable to select a Compute type - the drop-down menu is just empty. If I try to create the job anyway, I get an error that says 'Unexpected error occurred during creation of job.'\nI am very new to using Studio Lab and I have not found any information about this problem\/error message anywhere, so any guidance or help of any kind would be greatly appreciated. Thanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Availability of source for AWS Sagemaker Object2Vec framework",
        "Question_created_time":1677545353071,
        "Question_last_edit_time":1677891930960,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIB0lNxXCThm5PhkYl0RqCg\/availability-of-source-for-aws-sagemaker-object2vec-framework",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":34,
        "Question_answer_count":1,
        "Question_body":"Most of the sagemaker frameworks based on open source frameworks such as PyTorch \/ scikit-learn etc are available in github (i.e. search for org:aws sagemaker-*-container). Is the source for \"in-house\" algorithms such as Obejct2Vec (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object2vec.html) also available somewhere?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker endpoints are not fast enough to get the response",
        "Question_created_time":1677499420697,
        "Question_last_edit_time":1677845082885,
        "Question_link":"https:\/\/repost.aws\/questions\/QUG7nNEX3uRTyWcBhjBdgmoA\/sagemaker-endpoints-are-not-fast-enough-to-get-the-response",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":10,
        "Question_answer_count":0,
        "Question_body":"We created a sagemaker endpoint for a keras\/tensorflow model.Invocation of this sagemaker endpoint from sagemaker notebook takes ~0.2 sec.The same code from EC2 server takes ~2.8 sec, which is not practical for our use case.\n\nThe times are averages over 50 experiments.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How Do I Make A SageMaker Studio Domain\/Model Accessible From OutSide AWS As A RestFul Endpoint?",
        "Question_created_time":1677463697386,
        "Question_last_edit_time":1677811453297,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtrpbxWwITGa3KakwiDCJlA\/how-do-i-make-a-sagemaker-studio-domain-model-accessible-from-outside-aws-as-a-restful-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":47,
        "Question_answer_count":1,
        "Question_body":"I am getting a \"Missing Auth Token\" error when attempting to POST to my Inference Model in AWS from Postman desktop.  The model works fine from the internal-to-AWS test pages in SageMarker Studio.  The model is deployed on a serverless endpoint.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to reencrypt SageMaker domain with a new KMS key?",
        "Question_created_time":1677283842660,
        "Question_last_edit_time":1677630160317,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVtgy7yVyRfqOmI3VeIFIrg\/how-to-reencrypt-sagemaker-domain-with-a-new-kms-key",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_body":"Hi, my current domain was created using a previous KMS key. Now I want to retire the key and use a new key. How should I proceed?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Download and access Sagemaker algorithm containers",
        "Question_created_time":1677191984322,
        "Question_last_edit_time":1677539617872,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-WjdbfRaRYKi_xb0JRdZ3Q\/download-and-access-sagemaker-algorithm-containers",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"I currently work with AWS Sagemaker and try to pull prebuild images from ECR.\nHowever, I ran into issues while pulling some images, and others worked.\nI successfully logged into the corresponding accounts ECR repo.\n\nE.g., I can pull the following image without issues:\n683313688378.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3\n\nAnd I run into the following issue with the next image:\ndocker pull 382416733822.dkr.ecr.us-east-1.amazonaws.com\/pca:1\n\nError response from daemon: pull access denied for 382416733822.dkr.ecr.us-east-1.amazonaws.com\/pca, repository does not exist or may require 'docker login': denied: User: XXXXXXXXXX is not authorized to perform: ecr:BatchGetImage on resource: arn:aws:ecr:us-east-1:382416733822:repository\/pca because no resource-based policy allows the ecr:BatchGetImage action\n\nIt seems like the resource based policy denies access.\n\nIs it not intended to download e.g. the PCA container image?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Feature Store Spark connector is duplicating data",
        "Question_created_time":1677163363698,
        "Question_last_edit_time":1677519025373,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDdzO0n0cRKm5fLSuPdE3Qg\/sagemaker-feature-store-spark-connector-is-duplicating-data",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":89,
        "Question_answer_count":1,
        "Question_body":"Hi! \n\nI'm using the [Feature Store Spark connector](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-ingestion-spark-connector-setup.html) to ingest data into the Sagemaker Feature Store and when we try to ingest data to a Feature Group with the online store enabled, the data is duplicated.\nIn the image bellow, \"customer_id\" is the ID feature, \"date_ref\" the event column. All the features are equal for the same ID and EventTime column, except the \"api_invocation_time\".\n\n ![Duplicated data](\/media\/postImages\/original\/IMhOq2A8JdTzm0WS21rLjyLQ)\n\nIf the feature group doesn't have the online store enabled, we ingest the data directly to the offline store without issues. But when we use the \"Ingest by default\" option in the connector (not specifying the \"target_stores\" in the connector, uses the PutRecord API), the data ingested is duplicated:\n\n```\nparams = {\n    \"input_data_frame\":dataframe,\n    \"feature_group_arn\": feature_group_arn            \n}\n\nif not online_store_enabled:\n    params[\"target_stores\"] = [\"OfflineStore\"]\n    logger.info(f\"Ingesting data to the offline store\")\n\npyspark_connector.ingest_data(**params)\nlogger.info(\"Finished the ingestion!\")\n\nfailed_records = pyspark_connector.get_failed_stream_ingestion_data_frame()\n```\n\nHow can I solve this issue using the connector?\n\n**EDIT:**\n\nApparently, the problem is in the \"get_failed_stream ingestion data frame\" method. This method, instead of just returning a dataframe, ingests the data again before returning the failed records. Removing the method from the ingestion pipeline resolves the issue, although we lose a form of validation.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1677783327874,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1677783327874,
        "Answer_comment_count":0.0,
        "Answer_body":"This issue should be patched in 1.1.1. If you upgrade from 1.1.0, get_failed_stream_ingestion_data_frame should no longer trigger any re-computation now.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to retrieve InputLocation and CustomAttributes in the container when calling invoke_endpoint_async in Sagermaker Async",
        "Question_created_time":1677139466322,
        "Question_last_edit_time":1677485612854,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3T1Tb5_6RcG0xqwoeUmvTg\/how-to-retrieve-inputlocation-and-customattributes-in-the-container-when-calling-invoke-endpoint-async-in-sagermaker-async",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":15,
        "Question_answer_count":0,
        "Question_body":"I am trying to implement sagemaker async with my own model based endpoint.\nTo invoke the service in the container, I used invoke_endpoint_async function. Here is the code:\n\n```\ninput_location = \"S3:\/\/xxxxxx\/xxx\/xxx.vtk\"\nresp = sagemaker_runtime.invoke_endpoint_async(\n    EndpointName=endpoint_name_async,\n    InputLocation=input_location,\n    CustomAttributes=json.dumps({\"test_key\": \"test_value\"})\n    )\n```\nIn the container, I am trying to retrieving those information. Here is the code:\n\n```\n@app.route(\"\/invocations\", methods=[\"POST\"])\nasync def invocation_handler(req):\n    if req.headers is not None:\n        custom_attribute = json.loads(req.headers.get(\"x-amzn-sagemaker-custom-attributes\"))\n    # ... ...\n```\nThis is the only way I can find to retrieve the CustomAttributes from the request and I don't know where is the InputLocation or where is the input file. `req` has an attribute called `req.files`. But it is empty. \n\nCan someone help me figure this out? All I want to do is retrieve the input data stored in S3 and do further process in the container as the endpoint.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker batch jobs",
        "Question_created_time":1677060099170,
        "Question_last_edit_time":1677406584411,
        "Question_link":"https:\/\/repost.aws\/questions\/QU41qCk_B9QIm5pwf28LsCLw\/sagemaker-batch-jobs",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":41,
        "Question_answer_count":2,
        "Question_body":"I have an ML workload that involves providing predictions for large datasets on demand. The model is a PyTorch text classifier and the workload involves pricing predictions for 10's of thousands of records. The jobs arrive randomly but are very infrequent.\n\nI've created a standard Sagemaker endpoint and performance is acceptable when I do client batching, i.e. \n\n\n```\noutputs = []\nfor batch in create_minibatch(inputs, batch_size=128):\n    predictions = predictor.predict(batch)\n    outputs.extend(predictions)\n```\n\n\nThis takes around a minute for 25k records using a single instance.\n\nI've considered using the AWS Batch mode, but it takes around 4-6 minutes to create the job, so any benefit from the ability to scale up to multiple instances seems to be lost due to the start-up cost. Is it possible to use Sagemaker Batch processing with a persistent endpoint? \n\nThe alternative is to use client batching (using the code above) - but if I create multiple instances can I be sure that each batch is returned in the order I request? In the example above I need to `zip` the inputs and outputs.\n\nIs there a better way of serving this workload - I feel it falls somewhere in between the API and Batch paradigm?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1677119127970,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1677119127970,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi,\n\nPlease find my answers below.\n\n**Question:** Is it possible to use Sagemaker Batch processing with a persistent endpoint?\n\n**Answer:** Currently SageMaker does not have such option.\n\n**Question:** The alternative is to use client batching (using the code above) - but if I create multiple instances can I be sure that each batch is returned in the order I request? In the example above I need to zip the inputs and outputs.\n\n**Answer**: When you send each batch to your endpoint, the request is synchronous, and your application will be waiting to get a response right away. The response will be in the order you sent. So maintaining the order is a matter of how you manage your requests. Regardless of how many instances you use in the endpoint.\n\n\n**Question:** Is there a better way of serving this workload - I feel it falls somewhere in between the API and Batch paradigm?\n\n**Answer:**\n\nThere is two main issues at play here:\n1. Cost\n2. Time\n\nDepending on your business need and which of the above is more important for you, or if you need to find a balance between the two.\n\nSageMaker has the following inference options that might be useful for your case:\n1. [Real-time inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/realtime-endpoints.html): here you provision resources for the endpoint and it stays up and running, so when you need to do predictions you use it straight away with no wait. Cost is based time the endpoint was `InService` and number and type of instance (see [pricing page](https:\/\/aws.amazon.com\/sagemaker\/pricing\/)).\n2. [Batch Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html): Here you send large number of records that you want to get predictions for. It requires resources to be provisioned first, which can take few minutes before actual prediction happens. Cost will be calculated based on the time it took to do predictions.\n\nTheoretically speaking, time took to do predictions in 1 or 2 should be very similar. However in 1, you provision the endpoint in advance, so when you invoke it, it feels faster, because your instance been there and ready. And comes with extra cost.\n\nNow if this option is good for you, you can always provision the endpoint in advance before you start predictions, and tear it down after. This way you get the benefit from both worlds. \n\nHowever, there is currently no option where the resource are persistent and you can just start predicting when you want. Just because of cost and operational considerations are in play here.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":1.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Upgrade NVIDIA drivers Sagemaker notebook instances",
        "Question_created_time":1677033974984,
        "Question_last_edit_time":1677380889413,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzBuSeZNKQym5EBiLz9cA8Q\/upgrade-nvidia-drivers-sagemaker-notebook-instances",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":13,
        "Question_answer_count":0,
        "Question_body":"Hi, Is it possible to upgrade the NVIDIA drivers on the g4dn series of instances on Sagemaker? They currently have 510 which has a max cuda of 11.6. Pytorch 2 is soon to be released and will only support >=CUDA 11.7.  Without an upgrade these instances won't be able to use pytorch 2. What is the correct command to run to upgrade these?\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to add a setting to ask password while we click on 'open jupyter' in aws sagemaker. Could you please let me know?",
        "Question_created_time":1676888446062,
        "Question_last_edit_time":1677235217626,
        "Question_link":"https:\/\/repost.aws\/questions\/QUgwDlHb3JR_-KxqaJ4G6nXA\/how-to-add-a-setting-to-ask-password-while-we-click-on-open-jupyter-in-aws-sagemaker-could-you-please-let-me-know",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":18,
        "Question_answer_count":0,
        "Question_body":"1. Once started the Amazon Sagemaker notebook instance, when we click on 'open jupyter', the screen should ask for the password before we access the contents inside.![Enter image description here](\/media\/postImages\/original\/IMRjG1E7nLTMyRAMztQ3GlsQ)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker - ValidationException",
        "Question_created_time":1676838144093,
        "Question_last_edit_time":1677184423086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzVBDaO2oTzOE8Hn_HkZqjg\/sagemaker-validationexception",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":73,
        "Question_answer_count":1,
        "Question_body":"Hello, \nI am trying to experiment with sagemaker and one of the built-in notebooks. \n\nI got to the notebook by:\n\nJumpStart > Huggingface > Question Answering\n\n Whenever I run the notebook I get the following error.\n\n'ClientError: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: Instance type ml.p2.xlarge is not supported for the chosen region. Please choose some other instance type.\"\n\nAs best as I can tell, I need to set up my domain in a region that supports the above instance type. \n\nWhere can I find out the support for Sagemaker operations by region? (By the way, I have tried various instance types, and none work.) \n\nThanks for any help!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Comprehend incremental training",
        "Question_created_time":1676615139900,
        "Question_last_edit_time":1676961564280,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuwusX1xNQO6J-M-AkCFlig\/comprehend-incremental-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":53,
        "Question_answer_count":1,
        "Question_body":"1. We trained custom model with manually annotated data set of documents but the accuracy is low, we want to annotate and train again. When I create a new version will it learn from the current set of inputs and also preserve the old training ? Do I need to give all the data for every incremental training ? \n\n2. Since there is some low accuracy issue, I want to add a2i . How to do it in console UI for batch processing ?\nWhen the people make additional annotation in a2i, will the comprehend learn incrementally ?\nOr do we need to make run the training job again ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1677047264657,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1677047264657,
        "Answer_comment_count":1.0,
        "Answer_body":"You will need to provide all of the data for each incremental training. \n\nComprehend is not integrated with A2I at this time, so you would need to re-submit a new training job with all the annotations.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Training Job stuck in Stopping",
        "Question_created_time":1676600917986,
        "Question_last_edit_time":1676948102289,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIWGDuavERN6H6eXlOeJ4bw\/sagemaker-training-job-stuck-in-stopping",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":129,
        "Question_answer_count":1,
        "Question_body":"I started the training job and there was GPU memory error but training job's status never changed to Failed status. So I stopped the training job but It is in stopping status for a day.  How can I solve this problem?\njob name:\npipelines-sx4yjpfpdg39-Training-EzzST0hXah",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Splitting data into test and train",
        "Question_created_time":1676529352231,
        "Question_last_edit_time":1676875618464,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHVqqziWJTZG8YkJmYU6piQ\/splitting-data-into-test-and-train",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":34,
        "Question_answer_count":2,
        "Question_body":"Please guide me with a tutorial for splitting the data into train and test data.\nAWS is using Sagemaker for machine learning for the new customers.I am a beginner in machine learning.\n\nPlease throw some light",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"An error occurred (AccessDenied) when calling the PutObject operation: Access Denied",
        "Question_created_time":1676494057751,
        "Question_last_edit_time":1676840704397,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuJy5bdjjTAGOR6nKChAgEA\/an-error-occurred-accessdenied-when-calling-the-putobject-operation-access-denied",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":74,
        "Question_answer_count":1,
        "Question_body":"Hi guys,\nI am uploading processed data from sagemaker to a S3 bucket but I get this error:\n`An error occurred (AccessDenied) when calling the PutObject operation: Access Denied`\nI am a little bit confused how to create IAM role. would please share a reference with detailed solution for this issue?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[SageMaker Jumpstart : Product Defect Detection ] could not launch .",
        "Question_created_time":1676449415967,
        "Question_last_edit_time":1676796158453,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8qC9VKdTRKmx2YiKlz0zlg\/sagemaker-jumpstart-product-defect-detection-could-not-launch",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":23,
        "Question_answer_count":0,
        "Question_body":"In the SageMaker Studio, I clicked the Launch button, but I couldn't start it and the status becomes 'Error'.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Studio or Notebook Instance?",
        "Question_created_time":1676440069035,
        "Question_last_edit_time":1676846125737,
        "Question_link":"https:\/\/repost.aws\/questions\/QUywQjsmRrRkK7tx8ygLhrXg\/studio-or-notebook-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":44,
        "Question_answer_count":2,
        "Question_body":"I\u2019m aware of Sagemaker Studio and notebook instance, but still confused about which to choose. Notebook instance - I find it easy but seems like it\u2019s missing some new features. Studio on the other hand seems really complicated, full with features.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to show Instance status for users inside the SageMaker Studio domain?",
        "Question_created_time":1676420486115,
        "Question_last_edit_time":1676767412552,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYl3iZLPsQzqMtRrhBVOHDg\/how-to-show-instance-status-for-users-inside-the-sagemaker-studio-domain",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":78,
        "Question_answer_count":1,
        "Question_body":"Is there a way to check with the AWS SDK or CLI whether instances are created at a specific time for users inside the SageMaker Studio domain?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Training Job CloudFormation Issue",
        "Question_created_time":1676383092273,
        "Question_last_edit_time":1676730725846,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAF30g1ibTTKBtwjaY6QfDg\/sagemaker-training-job-cloudformation-issue",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_body":"Hi AWS,\n\nI have the cloudformation code for AWS SageMaker Training Job which is provided below:\n\n```\nTensorFlowTrainingJob:\n    Type: AWS::SageMaker::TrainingJob\n    Properties:\n      TrainingJobName: my-tensorflow-training-job\n      AlgorithmSpecification:\n        TrainingImage: <ecr-uri>\n        TrainingInputMode: File\n      RoleArn: SageMakerRole.Arn\n      InputDataConfig:\n        - ChannelName: train_normal\n          DataSource:\n            S3DataSource:\n              S3DataType: S3Prefix\n              S3Uri: <s3-uri>\n        - ChannelName: train_pneumonia\n          DataSource:\n            S3DataSource:\n              S3DataType: S3Prefix\n              S3Uri: <s3-uri>\n      OutputDataConfig:\n        S3OutputPath: <s3-uri>\n      ResourceConfig:\n        InstanceCount: 1\n        InstanceType: ml.m4.xlarge\n        VolumeSizeInGB: 50\n      StoppingCondition:\n        MaxRuntimeInSeconds: 3600\n      HyperParameters:\n        epochs: \"50\"\n        learning_rate: \"0.001\"\n\n```\n\n\nBut I am experiencing this issue:\n\n**[cfn-lint] E3001: Invalid or unsupported Type AWS::SageMaker::TrainingJob for resource TensorFlowTrainingJob in us-east-1**\n\nThe linter version I am using is cfn-lint 0.70.1\n\nCan you please suggest what should be the quickfix as I have checked a lot of things on internet but have not found any valid solution for the same.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"PyTorch DDP on Sagemaker",
        "Question_created_time":1676361361394,
        "Question_last_edit_time":1676707106953,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXQ55QLd5ReaalYnaoLewaw\/pytorch-ddp-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":18,
        "Question_answer_count":0,
        "Question_body":"Hi, I am using pytorch ddp on sagemaker. It is using mpi and running 4 separate processes on each of the 4 GPUs in g4dn.12xlarge. In pytorch, this is called ddp_sapwn, right? Is there a way to force DDP instead of ddp_spawn on Sagemaker?\nMy `distribution` argument is \n```\ndistribution = { \n    \"pytorchddp\": {\n        \"enabled\": True,\n        \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION\"\n    }\n}\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Out of Memory Error SageMaker ml.p3.16xlarge",
        "Question_created_time":1676358383744,
        "Question_last_edit_time":1676705080782,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZT9-XylGRMeuzejcY-6K3w\/out-of-memory-error-sagemaker-ml-p3-16xlarge",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":97,
        "Question_answer_count":1,
        "Question_body":"[Link to the notebook.](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/training\/distributed_training\/pytorch\/model_parallel\/gpt-j\/11_train_gptj_smp_tensor_parallel_notebook.ipynb)\n\nI am trying to run the example notebook above on SageMaker using the gpt-j-xl model with the suggested instance of an ml.p3.16xlarge. However, I keep running into an out of memory error. I have tried other suggested instances (eg ml.g4dn.12xlarge) as well but get the same error. I've attached the latest error below. I've tried to set the train and val batch sizes to as low as 2 and still run into OOM issues. Any guidance would be appreciated.\n\n![Enter image description here](\/media\/postImages\/original\/IMAL-P9HZFRYGlgcCCToU59Q)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to run the tensorflow neuron in SageMaker endpoint for production",
        "Question_created_time":1675973647847,
        "Question_last_edit_time":1676320911103,
        "Question_link":"https:\/\/repost.aws\/questions\/QUgNhT1rz8Qo68w6g1lX7Mfg\/how-to-run-the-tensorflow-neuron-in-sagemaker-endpoint-for-production",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_body":"We have a huggingfacemodel with zero-shot-classification with neuron infernetia. It's based on [the pretrained huggingface pipelines distilBert with TensorFlow2 neuron](https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/src\/examples\/tensorflow\/huggingface_bert\/huggingface_bert.html) with zero-shot-classification model.\n\nWe planned to use it in production environment since it reduced the latency from 1s to 100ms.\n\nHowever, the Sagemaker Python SDK HuggingFaceModel seems not support tensorflow 2 neuron. It gave error like bellow.\n\nMy question is that how to run this tensorflow2 neuron under sageamaker?\n1. If huggingfacemodel doesn't support tensorflow2, can you provide a pytorch version for hugginface pipeline. There isn't any example of implement neuron for huggingface pipeline. \n2. Is there any other way like create dockerfile?\nThanks a lot\n\n\n```\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   model_data=\"s3:\/\/sagemaker-us-west-2-**********\/inf1\/model.tar.gz\",      # path to your model and script\n   role=role,                    # iam role with permissions to create an Endpoint\n   transformers_version=\"4.6.1\",  # transformers version used\n   tensorflow_version=\"2.4.1\",        # pytorch version used\n   py_version='py37',            # python version used\n)\nhuggingface_model._is_compiled_model = True\n# deploy the endpoint endpoint\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,      # number of instances\n    instance_type=\"ml.inf6.xlarge\" # AWS Inferentia Instance\n)\n```\nWe got response\n\n```\nDefaulting to the only supported framework\/algorithm version: 4.12.3. Ignoring framework\/algorithm version: 4.6.1.\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n\/tmp\/ipykernel_18249\/2024686093.py in <module>\n      2 predictor = huggingface_model.deploy(\n      3     initial_instance_count=1,      # number of instances\n----> 4     instance_type=\"ml.inf1.xlarge\" # AWS Inferentia Instance\n      5 )\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/huggingface\/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, **kwargs)\n    303 \n    304         return super(HuggingFaceModel, self).deploy(\n--> 305             initial_instance_count,\n    306             instance_type,\n    307             serializer,\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/huggingface\/model.py in serving_image_uri(self, region_name, instance_type, accelerator_type, serverless_inference_config)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    386 \n    387 \n--> 388 def execute_job_functions(step_args: _StepArguments):\n    389     \"\"\"Execute the job class functions during pipeline definition construction\n    390 \n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    172             )\n    173         _validate_arg(full_base_framework_version, list(version_config.keys()), \"base framework\")\n--> 174         version_config = version_config.get(full_base_framework_version)\n    175 \n    176     py_version = _validate_py_version_and_set_if_needed(py_version, version_config, framework)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    569     \"\"\"Creates a tag for the image URI.\"\"\"\n    570     if inference_tool:\n--> 571         return \"-\".join(x for x in (tag_prefix, inference_tool, py_version, container_version) if x)\n    572     return \"-\".join(x for x in (tag_prefix, processor, py_version, container_version) if x)\n    573 \n\nValueError: Unsupported base framework: tensorflow2.4.1. You may need to upgrade your SDK version (pip install -U sagemaker) for newer base frameworks. Supported base framework(s): version_aliases, pytorch1.9.1.\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to add merging steps after a ConditionStep to a Sagemaker Pipeline?",
        "Question_created_time":1675808195874,
        "Question_last_edit_time":1676155954022,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXjJBAFQySv6gkD6gwzAgUg\/how-to-add-merging-steps-after-a-conditionstep-to-a-sagemaker-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_body":"We want to create a Sagemaker pipeline with two branches based on a ConditionStep.  After either branch, we want to execute multiple common steps.  However, Sagemaker doesn't allow us to create downstream steps depend on the ConditionStep.  Is there any workaround to this except for duplicating these common steps (E and F below)?\nHere is the topology of our pipeline,\n1. Execute step A followed by a ConditionStep B.\n2. If condition B is true, then execute steps C, E and F.\n2. If condition B is false, then execute steps D, E and F.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to Visualize a custom metrics for a ProcessingStep on Sagemaker Pipeline UI",
        "Question_created_time":1675807829771,
        "Question_last_edit_time":1676154282550,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYNUBZ3ELREybZ50_n469iQ\/how-to-visualize-a-custom-metrics-for-a-processingstep-on-sagemaker-pipeline-ui",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_body":"When selecting a step from a running Sagemaker pipeline, the right-side panel has an \"Output\" tab.  We know that if the step is created from an `Estimator`, we can display all metrics from specifying `metric_definitions` parameter.  However there is no equivalent parameter for `Processor` or other types of steps.  How can we display metrics for a Sagemaker ProcessingStep then?\n![Enter image description here](\/media\/postImages\/original\/IMBajf2v2YRlaUn7mb-5dL8g)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I get class probabilities from an XGBoost model, using batch transform?",
        "Question_created_time":1675744928095,
        "Question_last_edit_time":1676092836292,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu9eMpCuYStG_Wpx_4zrCyg\/can-i-get-class-probabilities-from-an-xgboost-model-using-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":135,
        "Question_answer_count":1,
        "Question_body":"AutoML gave me an XGBoost model, which I would like to now use to run inference on a large CSV file.\n\nAfter \"deploying\" the model, I figured out how to do this using a \"batch transform\" job.\nBut the output appears to only be 0.0 or 1.0, class labels.\n\nIs there a way to get class probabilities out from the model?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Real Time Inference pricing clarification",
        "Question_created_time":1675713912571,
        "Question_last_edit_time":1676060954182,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaxWgMnwrSaq9IYZ5L0G1SA\/sagemaker-real-time-inference-pricing-clarification",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_body":"Hello,\nCan you please help me to comprehend the pricing of Sagemaker Real time Inference?\nIf I choose to go with instance ml.c7g.16xlarge(memory - 128 GiB, vCPUs - 64), suppose following is my payload expectations:\nfor a single request, 2 MB is input payload size and 2 MB is output payload size. The expected requests for a month is 2 million requests.\nSay the deployed model size is 2GB and it is deployed for 24 hours, 7 days a week.\nIf auto scaling has not been turned on, could you please tell me how much would be the charge for a month? And any restrictions on the number of requests?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Default password for Amazon SageMaker studio lab instance.",
        "Question_created_time":1675692286173,
        "Question_last_edit_time":1676038567427,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrRnl-WoDSF29NpEz_oa6Gw\/default-password-for-amazon-sagemaker-studio-lab-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":145,
        "Question_answer_count":2,
        "Question_body":"What is the default password for Amazon SageMaker studio lab instance console?\n\n![sagemaker cli](\/media\/postImages\/original\/IMz__nNherS8GaHRdL6HvM7w)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What's kind of the average launch time for a SageMaker Studio notebook?",
        "Question_created_time":1675651245777,
        "Question_last_edit_time":1675999192317,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqGiSmuPcS9WxsjybQq5d2w\/what-s-kind-of-the-average-launch-time-for-a-sagemaker-studio-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_body":"Assuming you don't add any custom lifecycle configuration scripts?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1675676899059,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1675676899059,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi Yann,\n\nTypically you should be using the fast launch instance types, which is designed to launch under 2 minutes. Generally it is 5-10 times faster than instance based notebooks.\n\nHere is to documentation which states so: [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html](link).\n\nHope it helps!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to configure RS Security Group Ingress Rule to Accept Sagemaker Stuido Notebook Connection",
        "Question_created_time":1675648531737,
        "Question_last_edit_time":1675996044720,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1UDka8XgSDyY5mtP6BWmeA\/how-to-configure-rs-security-group-ingress-rule-to-accept-sagemaker-stuido-notebook-connection",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":91,
        "Question_answer_count":1,
        "Question_body":"Hello,\nI have below resources in the same account same VPC:\n1. one redshift cluster, public accessible, internet access is controlled by security group by TCP ports range.\n2. one Sagemaker domain, use Default communication with the internet, as in https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-and-internet-access.html.\n\nMy understanding is a Sagemaker Studio notebook (not the notebook instance) in above Sagemaker domain will have its boto3 connection to Redshift initiated via Internet. Thus, the Redshift security group should have proper inbound rule configured to allow the incoming traffic.\n\nI am not able to find any information about the source (public IPs) of the sagemake domain (studio notebook) public traffic, of which the source should be owned by the Sagemaker service. How should I configure the Redshift security group in this case?\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Endpoints test inference gives error 415",
        "Question_created_time":1675495710193,
        "Question_last_edit_time":1675842228650,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5vZleUxqRTKF5GnF4d4FBQ\/endpoints-test-inference-gives-error-415",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":2,
        "Question_body":"Hi,\n\nI deploy a abalone test model, and now I want to test it through Enpoint Test Inference, it gives me an error:\n\n```\nError invoking endpoint: Received client error (415) from primary with message \"application\/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, application\/x-recordio-protobuf.\". See https:\/\/ap-northeast-1.console.aws.amazon.com\/cloudwatch\/home?region=ap-northeast-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/abaloneTest in account 556267670448 for more information.\n```\nI understand the error contentType is not accepting value \"application\/json\", but I am not able to find out how can i update the default value from \"application\/json\" to any permitted value and test endpoint with test inference ui only?\nIt will be good if ui provides a sample request format, so that we can test it fast.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to reduce launch time for SageMaker Notebooks?",
        "Question_created_time":1675452931723,
        "Question_last_edit_time":1675800546172,
        "Question_link":"https:\/\/repost.aws\/questions\/QUue_oTCZ-SUGNkYg_wviWIg\/how-to-reduce-launch-time-for-sagemaker-notebooks",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_body":"I am using SageMaker Notebooks and have added a lifecycle configuration to shut it down after inactivity. However, after a period of inactivity, it takes a long time (20 minutes) to boot back up. What can I do to reduce the launch time?\n\nI have considered creating [a custom container image](https:\/\/aws.amazon.com\/blogs\/machine-learning\/bringing-your-own-custom-container-image-to-amazon-sagemaker-studio-notebooks\/) with pre-installed packages and dependencies but I am not sure if this would help. I also wonder if using a larger instance size would reduce launch time.\n\nWhat are the strategies and best practices to reduce launch time for SageMaker Notebooks?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio API to get original s3 dataset path of an AutoMLJob",
        "Question_created_time":1675422313061,
        "Question_last_edit_time":1675769121875,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBG-5LOB7RXOHDCsaHKMHnQ\/sagemaker-studio-api-to-get-original-s3-dataset-path-of-an-automljob",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_body":"Is there any API of Sagemaker Studio that can help us get the complete path of the original dataset that was imported from s3 in Canvas for model training, in an autoMLJob?\n\nI have imported a dataset from my own s3 bucket in Canvas and trained a model using that dataset. When the training is triggered, Canvas saves the copy of that original dataset to a new path in s3, trains the model on that dataset and saves the path of that dataset in the autoMLJob of that model in Sagemaker Studio under the tag InputDataConfig. Is there any way that we can get the s3 path of the original dataset too from where we have imported the dataset, through any API of Sagemaker Studio or through the describeAutoMLJob API?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Jupyter kernel dies on SageMaker notebook instance when running join operation on large DataFrames using pd.merge",
        "Question_created_time":1675378595264,
        "Question_last_edit_time":1675726151667,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnLnPhSfBSbyKtmMaqCA8vQ\/jupyter-kernel-dies-on-sagemaker-notebook-instance-when-running-join-operation-on-large-dataframes-using-pd-merge",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":94,
        "Question_answer_count":1,
        "Question_body":"I am running a large pandas merge join operation on a `jupyter` notebook running on `SageMaker` notebook instance `ml.t3.large` i.e `8 gb` of memory. \n\n    \n```\nimport pandas as pd\n    \n    df1 = pd.DataFrame({ \n                        'ID': [1, 2, 3],\n                        'Name': ['A','B','C'],\n                        ....\n                      })\n\n    df1.shape\n    (3000000, 10)\n    \n    df2 = pd.DataFrame({\n                        'ID': [],\n                        'Name': [],\n                        ....\n                      )}\n    \n    df2.shape\n    (50000, 12)\n    \n                       \n    \n   \n    # Join data\n    \n    df_merge = pd.merge(\n                         df1,\n                         df2,\n                         left_on = ['ID','Name'],\n                         right_on = ['ID','Name'],\n                         how = 'left'\n                       )\n```\n\n\nWhen I run this operation, the kernel dies within a minute or so. How can I optimize this operation for memory efficiency?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker notebook doesn't run on Job schedule",
        "Question_created_time":1675370723349,
        "Question_last_edit_time":1675718147630,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjxD8Cf8lTc2Tknbt4MAzFQ\/sagemaker-notebook-doesn-t-run-on-job-schedule",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":90,
        "Question_answer_count":1,
        "Question_body":"Hello. I created a Notebook Job Definition, scheduled to run every hour. According to the status it is \"Active\". Whenever I manually trigger the job with the \"Run Job\" button, it creates a new Notebook Job and runs successfully. However, the notebook never runs on the schedule. It should execute every hour, but instead only executes when manually triggered.\n\nIs there anything I need to do for the notebook to obey the schedule? Thanks,",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1675405386807,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1675405386807,
        "Answer_comment_count":1.0,
        "Answer_body":"Are there any problems with IAM role settings, etc.?\n\nPlease check this document for reference only.\nhttps:\/\/aws.amazon.com\/jp\/blogs\/machine-learning\/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs\/",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AWS Rekognition and Sagemaker Running Local to automate annotation base on video streaming",
        "Question_created_time":1675364900677,
        "Question_last_edit_time":1675711649019,
        "Question_link":"https:\/\/repost.aws\/questions\/QUgd4wSBvcSAqA6tLK_0PivA\/aws-rekognition-and-sagemaker-running-local-to-automate-annotation-base-on-video-streaming",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":94,
        "Question_answer_count":2,
        "Question_body":"Hi everyone, I have a project in manufacturing area that I would like to improve by implementing computed vision capability. I already work with AWS services, but I still have no experience with Rekognition and Sagemaker, so I will explain my application below and would like to know if it is possible to make and what are the key point I need to put my attention and study.\nToday in a manufacturing plant, anytime a machine stops, the worker annotates the downtime cause on a tablet(running my app) with predefined options.\nI'd like to implement computed vision to automate this annotation, or suggest the most probable cause. I'm expecting a way for the application to auto train and improve itself given the historical data composed by: downtime cause annotated by operator + period of time when that downtime occurred + video.\nI plan to run on an edge through greengrass with a tpu processor.\n\nThanks in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Aws ground truth - labeling jobs not progressing",
        "Question_created_time":1675102352796,
        "Question_last_edit_time":1675449191958,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSQagSppgQ3KBUpDzVRdw_A\/aws-ground-truth-labeling-jobs-not-progressing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":28,
        "Question_answer_count":0,
        "Question_body":"Hello \n\nPlease i have a problem with ground truth labeling jobs , the status of jobs still shows in progress and very slow\neven though it's just small dataset with one collaborator , what's the solution",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cannot See Sagemaker Domain",
        "Question_created_time":1675099806734,
        "Question_last_edit_time":1675446561496,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh8K2moQsRjabcDgaLCtYUw\/cannot-see-sagemaker-domain",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":59,
        "Question_answer_count":2,
        "Question_body":"If I go to the Sagemaker Domain site, it says there are no domains and prompts me to make a domain. However, if my coworker goes to the Domain site she sees the domain and a list of user profiles that includes my user profile. How come I cannot see the domain?\n\nThank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"TensorFlow 2.2 in SageMaker not saving output model for deployment",
        "Question_created_time":1674966193100,
        "Question_last_edit_time":1675312650768,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlvU3YPWmTEKZl9MZCzc9zw\/tensorflow-2-2-in-sagemaker-not-saving-output-model-for-deployment",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_body":"I am experiencing an issue when using TensorFlow version 2.2 in Amazon SageMaker to train my model. The output model is not being saved and cannot be deployed (no model artifact in \/output folder). However, when using TensorFlow version 1.15, the model is trained and saved successfully. Can anyone provide insight or a solution for this issue?\nI am using : \n                     framework_version='2.2.0'\n                      py_version=\"py37\"\n\nThank you in advance",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to allowlist sagemaker IP?",
        "Question_created_time":1674683552396,
        "Question_last_edit_time":1675031393875,
        "Question_link":"https:\/\/repost.aws\/questions\/QUX7n9V0osTAmdmLYM21vmKQ\/how-to-allowlist-sagemaker-ip",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":131,
        "Question_answer_count":2,
        "Question_body":"I want to allowlist the Sagemaker studio IP so people can access certain allowlisted services from Sagemaker. I created a sagemaker domain in my private subnet of my VPC, so theoretically it should use the IP of the associated NAT gateway, right? But I see a different IP \ud83e\udd14",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1675200089343,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1675200124140,
        "Answer_comment_count":0.0,
        "Answer_body":"I should've read through my terraform code that created Sagemaker more carefully, I specified a VPC so I thought it would be *in* the VPC but it turns out I needed to specify the AppNetworkAccessType too.\n\nbad:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n```\n\ngood:\n```hcl\nresource \"aws_sagemaker_domain\" \"my_domain\" {\n  domain_name = var.domain_name\n  auth_mode   = \"IAM\"\n  vpc_id      = var.vpc_id\n  subnet_ids  = var.subnet_ids\n  app_network_access_type = \"VpcOnly\"\n```",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"How to add\/extend sagemaker pipeline parameters?",
        "Question_created_time":1674247752624,
        "Question_last_edit_time":1674594287763,
        "Question_link":"https:\/\/repost.aws\/questions\/QURUKWEs_kQ7GztS3fx5H74A\/how-to-add-extend-sagemaker-pipeline-parameters",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_body":"I am working with sagemaker pipelines and it provides following parameters. is there a way we can extend\/modify these? I want to look at the class\/data type behind these sagemaker.worflow.parameters type , and extend or modify those if possible. say for ParameterString, I want to override its behaviour such that it has to be of certain length? or add a new type to parameter , may be a list type ?\n\n```\nfrom sagemaker.workflow.parameters import (\n    ParameterInteger,\n    ParameterString,\n    ParameterFloat,\n    ParameterBoolean\n)\n\nprocessing_instance_count = ParameterString(\n    name=\"some_param\",\n    default_value=\"some_value\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to load trained model in Sagemaker",
        "Question_created_time":1674243095386,
        "Question_last_edit_time":1674591349868,
        "Question_link":"https:\/\/repost.aws\/questions\/QUU65mpZ42SXS4UXFd8pz4og\/unable-to-load-trained-model-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_body":"I have trained a few models in sagemaker however I am unable to load them for prediction.\n\nI am picking model details from: Sagemaker > Inference > Models > Container 1 section:\nImage_uri = value in image\nmodel_data = Value in model data location\n\nthen passing these values into sagemaker Model function. \n\nWhen I deploy this model, it gives error: ping health check failed for AllTraffic production variant. This error doesn't come when I train a new model and deploy it.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Model Error calling the InvokeEndpoint operation for Autogluon model.",
        "Question_created_time":1674221935817,
        "Question_last_edit_time":1674568262713,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXjEjdsU3SuK2dAEsaD7GDg\/model-error-calling-the-invokeendpoint-operation-for-autogluon-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":31,
        "Question_answer_count":0,
        "Question_body":"I have deployed an Autogluon model to enable be invoke a sagemaker endpoint on the lambda. I keep receiving this error in my CloudWatch. Any help will really be appreciated...![cloudwatch](\/media\/postImages\/original\/IMm2JGPoxcSuGm5Lew5ZaQTA)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"orchestrating Sagemaker notebook instance with AWS step function",
        "Question_created_time":1674219155385,
        "Question_last_edit_time":1674566613152,
        "Question_link":"https:\/\/repost.aws\/questions\/QULLjxU-mVSLGKHY0HMWHC_g\/orchestrating-sagemaker-notebook-instance-with-aws-step-function",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":31,
        "Question_answer_count":0,
        "Question_body":"I am building a pipeline from ETL to complete ML lifecycle. I have used AWS glue notebooks for most of the ETL Job and I have created pipeline with step functions. Rest Machine learning work completed in sagemaker notebook instances with multiple notebooks. Now I want to use step functions to build end to end pipeline. Unfortunately I can't see any sagemaker notebook instance run function in step function as like aws glue job run. Am I missing something here. please help me to complete this gap.\n\n![Enter image description here](\/media\/postImages\/original\/IMMpsITAdUQcG8v5ab4inuwg)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Annotating PDF files - SagerMakerExecutionRole can not assume role for sagemaker.amazonaws.com",
        "Question_created_time":1674126843355,
        "Question_last_edit_time":1674474010392,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0qZkPhUATbuUcqBalZNUPQ\/annotating-pdf-files-sagermakerexecutionrole-can-not-assume-role-for-sagemaker-amazonaws-com",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":75,
        "Question_answer_count":1,
        "Question_body":"I follow this tutorial :\nhttps:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/cer-annotation-pdf.html#cer-annotation-pdf-set-up\n\nAt the step \"Creating an annotation job\", I get this error :\n\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateLabelingJob operation: The role ARN arn:aws:iam::xxxxxxxxxxxx:role\/xxxxxxxx-SageMakerExecutionRole-xxxxxxxxxxxxxx isn't valid. Make sure the role exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\"\n\nSo, I went to IAM and check the trust relationships :\n        \n```\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"sagemaker.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n```\nIs that enough ?\n\nI'm using the Ubuntu app under Windows 10.\n\nI checked the version of boto3 :\n\n```\n# pip show boto3\nName: boto3\nVersion: 1.26.52\n...\n```\nWhat can I do more ?\n\nThank you in advance !",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to create a custom model tar file via create model step in sagemaker pipeline step?",
        "Question_created_time":1674099113152,
        "Question_last_edit_time":1674445424984,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_aRkG2vZSaaPaIdxO7sofA\/how-to-create-a-custom-model-tar-file-via-create-model-step-in-sagemaker-pipeline-step",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_body":"based on the sample\/docs provided here => https:\/\/www.philschmid.de\/mlops-sagemaker-huggingface-transformers, I am fine tuning a hugging face distilbert model in sagemaker studio via pipeline. this example works. when the model is created , i specify    entry_point = 'predict.py', \n   and  source_dir = 'script' (see below ) , which creates following directory structure in the model.tar file. there are other files like tokenizer.json , tokenizer_config.json. , is it possible put these files into another folder next to my script folder during the model create\/package step ? these files , i assume are downloaded from hugging face along with the pytorch model file and are put at the root of the tar model file generated. \n\nmodel directory structure\n```\nmodel.tar.gz\/\n|- pytorch_model.bin\n|- tokenizer.json\n|- tokenizer_config.json\n|- special_tokens_map.json\n|- ...\n|- script\/\n   |- predict.py\n   |- requirements.txt \n```\n\n```\n# Create Model\nmodel = Model(\n    entry_point = 'predict.py', \n    source_dir = 'script'\n```\n\n```\nhuggingface_estimator = HuggingFace(entry_point='train.py',\n                            source_dir='.\/scripts',\n                            instance_type='ml.p3.2xlarge',\n                            instance_count=1,\n                            role=role,\n                            transformers_version='4.6',\n                            pytorch_version='1.7',\n                            py_version='py36',\n                            hyperparameters = hyperparameters)\n\nstep_train = TrainingStep(\n    name=\"TrainHuggingFaceModel\",\n    estimator=huggingface_estimator,\n    inputs={\n        \"train\": TrainingInput( ...  ),\n        \"test\": TrainingInput( ...     ),\n    },\n....\n)\n\n# Create Model\nmodel = Model(\n    entry_point = 'predict.py', \n    source_dir = 'script'\n    image_uri=image_uri,\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=pipeline_session,\n    role=role,\n    ....\n)\n\nstep_create_model = ModelStep(\n    name=\"CreateModel\",\n    step_args=model.create(\"ml.m4.large\"),\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Training Job Error - \"Checkpoint hyperparameters are missing. Please check the checkpoint hyperparameters file exists on S3., exit code: 2\"",
        "Question_created_time":1674012204784,
        "Question_last_edit_time":1674359197370,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJyl6rOiLTwGR50TFdtBs_Q\/sagemaker-training-job-error-checkpoint-hyperparameters-are-missing-please-check-the-checkpoint-hyperparameters-file-exists-on-s3-exit-code-2",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":36,
        "Question_answer_count":0,
        "Question_body":"Hi,\n\nI am using SageMaker for a computer vision project. The project goal is to train an Object Detection model on SageMaker and create an Endpoint. We follow [the AWS instructions to prepare a dataset](https:\/\/www.youtube.com\/watch?v=OFlu6Gd7CrQ) having images files and *.manifest file created inside a new S3 bucket within the same region of the SageMaker notebook \n\nWe use the notebook (http:\/\/aws-tc-largeobjects.s3-us-west-2.amazonaws.com\/DIG-TF-200-MLBEES-10-EN\/demo.ipynb) which we download from a link provided by an AWS Youtube video (https:\/\/www.youtube.com\/watch?v=OFlu6Gd7CrQ).\n\nWe followed the instructions to load the images and *.manifest file provided by the notebook ran the code and then created a Training job but failed many times with the following error:\n\n\"Failure reason\nClientError: Cannot resume training. Checkpoint hyperparameters are missing. Please check the checkpoint hyperparameters file exists on S3., exit code: 2\"\n\ninstance type used is p2.xlarge\n\nI have no idea what this error means, and I have no idea what is a checkpoint hyperparameters file. I checked my S3 a hyperparameters file does not exist.\n\nI checked and all hyperparameters are set correctly during job creation and here is the list report in the report:\n\n**Hyperparameters**\nKey\tValue\nbase_network\tresnet-50\nearly_stopping\tfalse\nearly_stopping_min_epochs\t10\nearly_stopping_patience\t5\nearly_stopping_tolerance\t0.0\nepochs\t30\nfreeze_layer_pattern\tfalse\nimage_shape\t300\nlabel_width\t350\nlearning_rate\t0.001\nlr_scheduler_factor\t0.1\nmini_batch_size\t1\nmomentum\t0.9\nnms_threshold\t0.45\nnum_classes\t1\nnum_training_samples\t400\noptimizer\tadam\noverlap_threshold\t0.5\nuse_pretrained_model\t1\nweight_decay\t0.0005\n\n\nThanks for help!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Integrate Sklearn Processing Step in Inference Pipeline",
        "Question_created_time":1673967894306,
        "Question_last_edit_time":1674314919148,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1q1ppjUGQMK-uw0qbKCJag\/integrate-sklearn-processing-step-in-inference-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":36,
        "Question_answer_count":0,
        "Question_body":"Hello,\n\nI am facing a problem as I not able to integrate a fitted sklearn processor\/estimator in my sagemaker pipeline. I am defining the different steps in different functions as the follows:\n```\ndef _get_step_preprocess(\n    pipeline_session: PipelineSession,\n    processing_instance_count: ParameterInteger,\n    role: str,\n    # input_data_uri: ParameterString,\n    subnet_id: str,\n    security_group_id: str,\n) -> ProcessingStep:\n    \"\"\"\n    Step 1\n    This Step is preprocessing the data as a first step of the pipeline.\n    Args:\n        processing_instance_count (ParameterInteger): Number of instances\n        role (str): Sagemaker Execution Role\n\n    Returns:\n        ProcessingStep: Defined PreprocessingStep\n    \"\"\"\n\n    network_config = NetworkConfig(\n        enable_network_isolation=False,\n        security_group_ids=[security_group_id],\n        subnets=[subnet_id],\n        encrypt_inter_container_traffic=True,\n    )\n\n    sklearn_processor = FrameworkProcessor(\n        estimator_cls=SKLearn,\n        framework_version=\"1.0-1\",\n        instance_count=processing_instance_count,\n        instance_type=\"ml.m5.xlarge\",\n        sagemaker_session=pipeline_session,\n        base_job_name=\"name\",\n        role=role,\n        network_config=network_config,\n    )\n\n    processor_args = sklearn_processor.run(\n        inputs=[],\n        outputs=[\n            ProcessingOutput(output_name=\"train\", source=\"\/opt\/ml\/processing\/train\"),\n            ProcessingOutput(output_name=\"validation\", source=\"\/opt\/ml\/processing\/validation\"),\n            ProcessingOutput(output_name=\"test\", source=\"\/opt\/ml\/processing\/test\"),\n            ProcessingOutput(output_name=\"encoder\", source=\"\/opt\/ml\/processing\/encoder\"),\n        ],\n        code=\"main.py\",\n        source_dir=\"..\/sagemaker\/step_preprocess\",\n    )\n\n    step_preprocess = ProcessingStep(name=\"BankingSecondaryRejectionPreprocess\", step_args=processor_args)\n\n    return step_preprocess\n```\nIf seen in different examples that I am not only able to execute a script like in the given example but also fit a sklearn preprocessor which can be integrated in my final pipeline model and so in the whole inference endpoint. An example i came across was this:\n https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.html\n\nNevertheless, I am not able to integrate the sklearn estimator from the example into my whole preprocessing step defined above. How is it done the right way? Is it even possible? The ProcessingStep seems not to be able to take a fitted estimator as an argument.\n\nThanks in advance",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is there a way to configure the auto-save interval on SageMaker Studio Notebooks?",
        "Question_created_time":1673960615485,
        "Question_last_edit_time":1674306814309,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPNplairnRPWo3zA_otNpWA\/is-there-a-way-to-configure-the-auto-save-interval-on-sagemaker-studio-notebooks",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_body":"I see Autosave Documents is checked in the Settings menu of my notebook, but I'm curious what the default interval is and if that's configurable?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1673973257173,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1673973257173,
        "Answer_comment_count":0.0,
        "Answer_body":"SageMaker Studio uses the default JupyterLab configuration, i.e., auto save every 2 minutes. \n\nYou can modify it through the `%autosave` magic command, or use an extension. You can see a medium article with details [here](https:\/\/medium.com\/nabla-squared\/how-to-change-the-autosave-interval-in-jupyter-notebooks-2ab996fe4446).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Model Monitor Capture data - EndpointOutput Encoding is BASE64",
        "Question_created_time":1673956972508,
        "Question_last_edit_time":1674302800225,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGSFVfrFJS_KsdrOMeepDPg\/model-monitor-capture-data-endpointoutput-encoding-is-base64",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_body":"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-capture-endpoint.html\n\nI have followed the steps mentioned in this and it appears I cannot change the encoding for EndpointOutput in datacapture file. It's coming BASE64 for xgboost model. I am using latest version 1.2.3.\n\nFor monitor scheduler it required both EndpointOutput and EndpointInput to have the same encoding. My EndpointInput  is CSV but EndpointOutput is coming to be BASE64 and nothing can change it.\n\nThis is causing issue while run of analyzer. After baseline is generated and data is captured, when monitoring schedule runs the analyzer it throws error of Encoding mismatch. For it to run EndpointOutput and EndpointInput should have same encoding.\n\nI saw we cannot do anything to change the encoding of output. I used LightGBM, CatBoost algorithms also and found for these EndpointOuput encoding is JSON, which is readable but still not solving the purpose.\n\nIs there a way we can change EndpointOutput Encoding for DataCapture.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1675066186694,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1675066186694,
        "Answer_comment_count":1.0,
        "Answer_body":"Output encoding can be configured by using the [CaptureContentTypeHeader \nin EndpointConfig.DataCaptureConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DataCaptureConfig.html#sagemaker-Type-DataCaptureConfig-CaptureContentTypeHeader). I believe since this is not being set, default encoding i.e. base64 is being used. \n\nPlease try once with this attribute set as below:\n```\n\"CaptureContentTypeHeader\": { \n         \"CsvContentTypes\": [ \"text\/csv\" ]\n      }\n```\n> Assuming that content_type\/accept is \"text_csv\" for the concerned model.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to import\/load pretrained DeepAR model into an estimator for continuous training?",
        "Question_created_time":1673930575516,
        "Question_last_edit_time":1674277144281,
        "Question_link":"https:\/\/repost.aws\/questions\/QU48KNg_QvQmO9v0y4ST_Abg\/how-to-import-load-pretrained-deepar-model-into-an-estimator-for-continuous-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":69,
        "Question_answer_count":1,
        "Question_body":"Hi, I have a trained DeepAR model saved as model.tar.gz in some S3 location. I hope to further optimize it by training for some extra epochs with extra data having same features, like transfer learning. I have read some DeepAR documentation but cannot find a way to import the trained model. \n\nI tried using Sagemaker's Estimator's [model_uri](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.fit) config, but it reports error for deepar, saying no 'model' channel is acceptable. \n\nI am wondering does DeepAR have such functionality or it is still not developed?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Neptune Notebook Querying",
        "Question_created_time":1673858615481,
        "Question_last_edit_time":1674205962236,
        "Question_link":"https:\/\/repost.aws\/questions\/QU88PxfAaDQPOox1kqDaFQMQ\/aws-neptune-notebook-querying",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"We are not able to query the Neptune DB using AWS Sagemaker Notebook\nPlease find the attached image for the error details\n\n![Enter image description here](\/media\/postImages\/original\/IMXz_Nh9pbRSinc2SGSGI64w)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Duration of SageMaker caching of multi-model endpoints",
        "Question_created_time":1673631300477,
        "Question_last_edit_time":1673978607673,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaQ-y1bIhRrqmxSJhYwb-gA\/duration-of-sagemaker-caching-of-multi-model-endpoints",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":100,
        "Question_answer_count":1,
        "Question_body":"SageMaker offers a flag to indicate whether to cache models for a multi-model endpoint, that is enabled by default.\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_MultiModelConfig.html\n\nFor what duration does this get cached?\n\nI tried searching \"sagemaker cache duration\" and \"ModelCacheSetting\" but did not find any hits.  The use case is GDPR compliance and endpoint deletion and cleanup.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error Message when Creating SageMaker Training Report using xgboost_report",
        "Question_created_time":1673460622788,
        "Question_last_edit_time":1673808137135,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvinQxudRRoSIFlFX6w7kuQ\/error-message-when-creating-sagemaker-training-report-using-xgboost-report",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":33,
        "Question_answer_count":1,
        "Question_body":"Get the following import error message:\n\nopt\/conda\/lib\/python3.7\/site-packages\/bokeh\/core\/templates.py in <module>\n     41 \n     42 # External imports\n---> 43 from jinja2 import Environment, Markup, FileSystemLoader\n     44 \n     45 # Bokeh imports\n\nImportError: cannot import name 'Markup' from 'jinja2' (\/opt\/conda\/lib\/python3.7\/site-packages\/jinja2\/__init__.py)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker stuck in Pending state",
        "Question_created_time":1673433441819,
        "Question_last_edit_time":1673780855053,
        "Question_link":"https:\/\/repost.aws\/questions\/QULOEZ7OsPSH64l4IVSve08Q\/sagemaker-stuck-in-pending-state",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_body":"HI my sagemaker instance is stuck in pending state it shows Failure reason\nInternalFailure....but still it shows pending state only ..How to restart the machine ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Problem with reading jpg image in Sagemaker, imageclassification algorithm",
        "Question_created_time":1673348916663,
        "Question_last_edit_time":1673696451600,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj0I1epyIR1GkzTj4rpgA0A\/problem-with-reading-jpg-image-in-sagemaker-imageclassification-algorithm",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":66,
        "Question_answer_count":1,
        "Question_body":"Dears,\n\nI created a sagemaker job but I see the following error:\n\n[01\/06\/2023 11:29:21 ERROR 140033245599552] Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/dbimgraces\/output\/af\/af05.jpg\n\nAs far as I can see from my label file, the location is fine and the image also looks fine. Is any problem with the imread itself?\n\n![Enter image description here](\/media\/postImages\/original\/IML4VWT-mNRAmb7fExLcsiyw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Encrypting SageMaker Model Artifacts for the AWS Marketplace Offering",
        "Question_created_time":1673332320570,
        "Question_last_edit_time":1673680216195,
        "Question_link":"https:\/\/repost.aws\/questions\/QUapY5aChhT1iBeaPvW3-yMQ\/encrypting-sagemaker-model-artifacts-for-the-aws-marketplace-offering",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":38,
        "Question_answer_count":0,
        "Question_body":"Hi,\nWe want to sell model packages on the AWS Marketplace and saw we could provide model artifacts on S3 along with the image. \n\nOur question: \nCan we encrypt the model artifacts without giving the subscribers additional permissions to our KMS?\n(Asking because it's not specified in the documentation - it says you can encrypt your model artifacts in general using KMS, but what happens for a marketplace offering?)\n\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker on AWS Marketplace - autoscaling, parameters, pricing and S3",
        "Question_created_time":1673190912248,
        "Question_last_edit_time":1673538025193,
        "Question_link":"https:\/\/repost.aws\/questions\/QUghZxipLKTSSn45EOV-S_Yg\/sagemaker-on-aws-marketplace-autoscaling-parameters-pricing-and-s3",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":2,
        "Question_body":"Hi\nCouldn't find answers in the documentation for the following questions when selling a model package on the AWS Marketplace:\n\n1. Pricing: Can we offer only private offers? (completely disable the hourly and per inference pricing)\n\n2. Autoscaling: Is it possible to define an autoscaling policy for a hosted endpoint that runs a model package?\n\n3. Parameters: What's the interface for making an inference call? Can we pass any parameters to the inference endpoint?\n\n4. S3: Can we use S3 to load additional dependencies?\n\nThank you very much!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1673208081703,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1673208081703,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi,\n\n1. In general, AWS Marketplace uses a pay-as-you-go pricing model, which means that customers are charged for the resources they consume on an hourly or per-inference basis. I'm not aware of any way to disable this pricing model when selling a model package on AWS Marketplace. However, it's worth noting that AWS Marketplace also offers private listings, which allow you to sell your model package directly to a specific customer or group of customers. Private offers are not discoverable by other customers and are not subject to the same pricing and billing terms as public listings. You may want to consider using a private listing if you want to offer a different pricing model for your model package. Reference: https:\/\/docs.aws.amazon.com\/marketplace\/latest\/buyerguide\/buyer-private-offers.html\n\n2. Yes, it is possible to define an auto scaling policy for a hosted Amazon SageMaker endpoint that runs a model package. To define an auto scaling policy for a SageMaker endpoint, you can use the *UpdateEndpoint* API or the SageMaker console. When updating an endpoint, you can specify the desired number of instances and the minimum and maximum number of instances for the auto scaling policy. SageMaker will automatically scale the number of instances up or down based on the incoming traffic and the defined policy.\n\nHere's an example of how you can use the *UpdateEndpoint* API to update an endpoint with an auto scaling policy:\n\n```\nimport boto3\n\nsm = boto3.client('sagemaker')\n\nresponse = sm.update_endpoint(\n    EndpointName='your-endpoint-name',\n    DesiredInferenceUnits=1,\n    MinInferenceUnits=1,\n    MaxInferenceUnits=8\n)\n\n```\nMore details: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\n3. To make an inference call to a hosted Amazon SageMaker endpoint, you can use the *invoke_endpoint* method of the SageMaker runtime client. This method allows you to send an HTTP POST request to the endpoint and receive the prediction results in the response.\n\nHere's an example of how you can use the *invoke_endpoint* method to make an inference request:\n\n```\nimport boto3\n\nsm = boto3.client('sagemaker-runtime')\n\nresponse = sm.invoke_endpoint(\n    EndpointName='your-endpoint-name',\n    Body=b'your-request-data',\n    ContentType='application\/json'\n)\nprediction = response['Body'].read()\n\n```\nYou can pass any parameters that your model expects in the request body. The format of the request data and the expected parameters depend on the specific model that you are using. For example, if your model expects a JSON object with a single field called \"input\", you can pass the input data as a JSON string in the request body.\n\nMore details: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\n\n4. Yes, you can use S3 to store additional dependencies for your ML model and load them into Amazon SageMaker. SageMaker allows you to specify additional code and libraries to be included in your training or inference environment by using the CodeRepository parameter of the CreateTrainingJob or CreateEndpoint API. The CodeRepository parameter should be set to the Amazon S3 URI of a Git repository that contains the code and dependencies you want to include. SageMaker will clone the repository and build the code as part of the training or inference environment.\n\nHere's an example of how you can use the CodeRepository parameter to specify an S3-based Git repository in a CreateTrainingJob request:\n\n```\nimport boto3\n\nsm = boto3.client('sagemaker')\n\nresponse = sm.create_training_job(\n    TrainingJobName='your-training-job-name',\n    HyperParameters={...},\n    InputDataConfig=[{...}],\n    OutputDataConfig={...},\n    ResourceConfig={...},\n    RoleArn='your-role-arn',\n    CodeRepository='s3:\/\/your-bucket\/your-repository.git'\n)\n\n```\nMore details: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Serverless on AWS Marketplace?",
        "Question_created_time":1673171839659,
        "Question_last_edit_time":1673517930938,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcZwxBuy-SROI7OF3-NFslA\/sagemaker-serverless-on-aws-marketplace",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":58,
        "Question_answer_count":1,
        "Question_body":"Hi,\nCan I use serverless inference as a pricing model for selling a SageMaker Model Package on the AWS Marketplace?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1673186904128,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1673186904128,
        "Answer_comment_count":0.0,
        "Answer_body":"No, quoting from SageMaker documentation:\n\n\n```\n\u2026features currently available for SageMaker Real-time Inference are not supported for Serverless Inference, including GPUs, AWS marketplace model packages\u2026\n```\n\nReference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Batch Transform - \"upstream prematurely closed connection\" - Unable to serve requests that take longer than 30 minutes",
        "Question_created_time":1673120348969,
        "Question_last_edit_time":1673467665391,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbey_TyxSRrSsZ0XG99jnDQ\/sagemaker-batch-transform-upstream-prematurely-closed-connection-unable-to-serve-requests-that-take-longer-than-30-minutes",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":74,
        "Question_answer_count":1,
        "Question_body":"[This is a duplicate of a question I asked on stack overflow](https:\/\/stackoverflow.com\/questions\/75043118\/sagemaker-batch-transform-job-upstream-prematurely-closed-connection-when-surp)\n\nI am serving a sagemaker model through a custom docker container using [the guide that AWS provides](https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.html#When-should-I-build-my-own-algorithm-container%3F). This is a docker container that runs a simple nginx->gunicorn\/wsgi->flask server\n\nI am facing an issue where my transform requests time out around 30 minutes in all instances, despite should being able to continue to 60 minutes. I need requests to be able to go to sagemaker maximum of 60 minutes due to data intense nature of request.\n\n\n----------\n\n\nThrough experience working with this setup for some months, I know that there are 3 factors that should affect the time my server has to respond to requests:\n\n 1. Sagemaker itself will cap invocations requests according to the\n    `InvocationsTimeoutInSeconds` paremeter set when [creating the batch\n    transform\n    job](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ModelClientConfig.html#sagemaker-Type-ModelClientConfig-InvocationsTimeoutInSeconds).\n 2. The `nginx.conf` file must be configured such that `keepalive_timeout`, `proxy_read_timeout`, `proxy_send_timeout`, and `proxy_connect_timeout` are all equal or greater than maximum timeout\n 3. gunicorn server must its timeout configured to be equal or greater than maximum timeout\n\n\n----------\n\n\nI have verified that when I create my batch transform job `InvocationsTimeoutInSeconds` is set to 3600 (1 hour)\n\nMy nginx.conf looks like this:\n\n    worker_processes 1;\n    daemon off; # Prevent forking\n    \n    \n    pid \/tmp\/nginx.pid;\n    error_log \/var\/log\/nginx\/error.log;\n    \n    events {\n      # defaults\n    }\n    \n    http {\n      include \/etc\/nginx\/mime.types;\n      default_type application\/octet-stream;\n      access_log \/var\/log\/nginx\/access.log combined;\n    \n      sendfile        on;\n      client_max_body_size 30M;\n      keepalive_timeout  3920s;\n      \n      upstream gunicorn {\n        server unix:\/tmp\/gunicorn.sock;\n      }\n    \n      server {\n        listen 8080 deferred;\n        client_max_body_size 80m;\n    \n        keepalive_timeout 3920s;\n        proxy_read_timeout 3920s;\n        proxy_send_timeout 3920s;\n        proxy_connect_timeout 3920s;\n        send_timeout 3920s;\n    \n        location ~ ^\/(ping|invocations) {\n          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n          proxy_set_header Host $http_host;\n          proxy_redirect off;\n          proxy_pass http:\/\/gunicorn;\n        }\n    \n        location \/ {\n          return 404 \"{}\";\n        }\n      }\n    }`\n\nI start the gunicorn server like this:\n\n    def start_server():\n        print('Starting the inference server with {} workers.'.format(model_server_workers))\n        print('Model server timeout {}.'.format(model_server_timeout))\n    \n        # link the log streams to stdout\/err so they will be logged to the container logs\n        subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n        subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n    \n        nginx = subprocess.Popen(['nginx', '-c', '\/opt\/program\/nginx.conf'])\n        gunicorn = subprocess.Popen(['gunicorn',\n                                     '--timeout', str(3600),\n                                     '-k', 'sync',\n                                     '-b', 'unix:\/tmp\/gunicorn.sock',\n                                     '--log-level', 'debug',\n                                     '-w', str(1),\n                                     'wsgi:app'])\n    \n        signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n    \n        # If either subprocess exits, so do we.\n        pids = set([nginx.pid, gunicorn.pid])\n        while True:\n            pid, _ = os.wait()\n            if pid in pids:\n                break\n    \n        sigterm_handler(nginx.pid, gunicorn.pid)\n        print('Inference server exiting')\n\nDespite all this, whenever a transform job takes longer than approx 30 minutes I will see this message in my logs and the transform job status becomes failed: \n\n    2023\/01\/07 08:23:14 [error] 11#11: *4 upstream prematurely closed connection while reading response header from upstream, client: 169.254.255.130, server: , request: \"POST \/invocations HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/invocations\", host: \"169.254.255.131:8080\"\n\nI am close to thinking there is a bug in AWS batch transform, but perhaps I am missing some other variable (perhaps in the nginx.conf) that could lead to premature upstream termination of my request.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1673126209340,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1673126209340,
        "Answer_comment_count":0.0,
        "Answer_body":"By looking at hardware metrics was able to determine that the upstream termination only happens when the server was near its memory limit. So my guess is that the OS was killing the gunicorn worker and the 30 minute mark was just a coincidence that happened on my long running test cases.\n\nMy solution was to increase the memory available on the server",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Cloudformation parameter not visible while creating project from SM projects",
        "Question_created_time":1672899565275,
        "Question_last_edit_time":1673246365131,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhwr7BUtlRjKZHdjQR6pxdg\/cloudformation-parameter-not-visible-while-creating-project-from-sm-projects",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":58,
        "Question_answer_count":0,
        "Question_body":"Hi,\n\n After I register my custom MLOps template on SM projects; while creating the project from SM Studio am unable to see the additional parameter (SourceModelPackageGroupName). The required parameter for custom template (SageMakerProjectName) is only visible.  \n\nI've three CFTs as described below:\n\n1) Portfolio creation in Service Catalog --> worked fine\n2) Product creation in Service Catalog --> worked fine\n3) Actual project implementation CFT --> Not showing additional parameter(SourceModelPackageGroupName) from SM Projects.\n\nParameters:\n  SageMakerProjectName:\n    Type: String\n    Description: Name of the project\n    MinLength: 1\n    MaxLength: 32\n    AllowedPattern: ^[a-zA-Z](-*[a-zA-Z0-9])*\n  SageMakerProjectId:\n    Type: String\n    Description: Service generated Id of the project.\n  SourceModelPackageGroupName:\n    Type: String\n    Description: Name of the ModelPackageGroup for ML model deployment\n\nAny pointers would be really appreciated.\n\nRegards,\nNikhil",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Inference pricing not pay-as-you-go",
        "Question_created_time":1672760317014,
        "Question_last_edit_time":1673107354941,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkanXhMOzSvGGM9A56zc3Og\/sagemaker-inference-pricing-not-pay-as-you-go",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":2,
        "Question_body":"We use Sagemaker inference only and I see in CostExplorer we pay every day around ~5.90USD\/DAY but don't use Sagemaker serverless inference everyday, probably several times per month.\n\nCan someone explain what is the reason of this pricing? I expect the pricing to be \"pay-as-you-go\" , not on-demand?\n\nAs per pricing example https:\/\/aws.amazon.com\/sagemaker\/pricing\/\n\n\n> With Serverless Inference, you only pay for the compute capacity used to process inference requests, billed by the millisecond, and the amount of data processed. The compute charge depends on the memory configuration you choose.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Possible bug with random cut forest on sagemaker",
        "Question_created_time":1672710542881,
        "Question_last_edit_time":1673057941281,
        "Question_link":"https:\/\/repost.aws\/questions\/QUgnock4CoScOvlBnGJmu87Q\/possible-bug-with-random-cut-forest-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":318,
        "Question_answer_count":1,
        "Question_body":"I am following the tutorial for random cut forest on sagemaker here: https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.html\nWhen I run this chunk of code below\n```\nfrom sagemaker import RandomCutForest\n\nsession = sagemaker.Session()\n\n# specify general training job information\nrcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    data_location=f\"s3:\/\/{bucket}\/{prefix}\/\",\n    output_path=f\"s3:\/\/{bucket}\/{prefix}\/output\",\n    num_samples_per_tree=512,\n    num_trees=50,\n)\n# automatically upload the training data to S3 and run the training job\nrcf.fit(rcf.record_set(taxi_data.value.to_numpy().reshape(-1, 1)))\n```\nI get this error, due to the \"rcf = RandomCutForest()\" line: UnknownServiceError: Unknown service: 'sagemaker-metrics'. Valid service names are: .....\n\nA more detailed screenshot of the error is below\n![Enter image description here](\/media\/postImages\/original\/IM6ANv0CMjTX6UdP7Aib_Sag)\n![Enter image description here](\/media\/postImages\/original\/IMP6cS45LDRUCJ54P9QbkNmQ)\n![Enter image description here](\/media\/postImages\/original\/IMLGJmFQ5WQPC8npDpfYrGKQ)\n\nMay I ask if this is a possible bug on aws sagemaker end? Or is there something wrong with the code? It was working fine until a few days ago. Thank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"boto3 iotanalytics client has endpoint value error",
        "Question_created_time":1672564195716,
        "Question_last_edit_time":1673316552852,
        "Question_link":"https:\/\/repost.aws\/questions\/QUclBv5up1R5-rnyYatscpng\/boto3-iotanalytics-client-has-endpoint-value-error",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":2,
        "Question_body":"using sagemaker jupytor notebook instance \ncode as below\nimport boto3\n\nclient = boto3.client('iotanalytics')\n\nanomaly_client = boto3.client('iot-data', region_name='us-east-1', endpoint_url = 'a2nnb1d4l4rnib-ats.iot.us-east-1.amazonaws.com\n')\nwhen run following error\nValueError: Invalid endpoint: a2nnb1d4l4rnib-ats.iot.us-east-1.amazonaws.com\nwhile this endpoint is working fine with python file to publish data on IoT Core",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to run a batch transformation with custom inference code in sagemaker pipeline?",
        "Question_created_time":1672437782920,
        "Question_last_edit_time":1672784941453,
        "Question_link":"https:\/\/repost.aws\/questions\/QUozUe9KXKT6i5LHvOb_KFBg\/how-to-run-a-batch-transformation-with-custom-inference-code-in-sagemaker-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":88,
        "Question_answer_count":1,
        "Question_body":"based on the docs here , https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html#Define-a-Transform-Step-to-Perform-Batch-Transformation, after creating a model,  the example shows a batch transform job is run. how to include any custom inference script that is required to get an inference from the model.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"The monitoring dashboard in ProcessingJob and TrainingJob in the SageMaker console is not visible.",
        "Question_created_time":1672395821525,
        "Question_last_edit_time":1672742070032,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtRK3ALuWQtmZKSdHeqFu6A\/the-monitoring-dashboard-in-processingjob-and-trainingjob-in-the-sagemaker-console-is-not-visible",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":33,
        "Question_answer_count":0,
        "Question_body":"The monitoring dashboard in ProcessingJob and TrainingJob in the SageMaker console is not visible.\n\n![Dashboard in SageMaker console](\/media\/postImages\/original\/IMObErWwHPRlOMFhRV43WhWA)\n\nHere's what I checked:\n- Logs coming into CloudWatchLogs are all good\n- CloudWatchMetrics dashboard shows all metrics well\n- In the Experiments-Trial dashboard within SageMaker Studio, all is well displayed.\n- Tested in other browsers & secret mode, the same symptoms occur\n- The IAM User under test has Admin privileges\n\nThe suspected cause is a failed request in the browser console and network tab, and a 403 Forbidden error occurred when requesting the amzn.css static file to cf.\n- Request URL: https:\/\/djj80sfhnqgai.cloudfront.net\/948cfe73f907787e734e6bb4193359ed4c68cbc8\/cloudwatch\/amzn.css\n- Method and Response: GET\/403 Forbidden\n- Error related message in response header: \"X-Cache: Error from cloudfront\"\n\nAre there any missing settings or additional checks when executing a job?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Pipeline FailStep Error Message Not Shown",
        "Question_created_time":1672270505341,
        "Question_last_edit_time":1672618409822,
        "Question_link":"https:\/\/repost.aws\/questions\/QULfE6M9RTRqeRpKXxnlAGkg\/sagemaker-pipeline-failstep-error-message-not-shown",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":484,
        "Question_answer_count":1,
        "Question_body":"I created a Sagemaker pipeline using Python SDK containing a fail step with a custom error message, which is part of a condition step, as seen at the bottom of this post.\n \nWhenever  the execution of the pipeline fails due to the fact that the trained model's accuracy is lower than the threshold, the FailStep custom error message is not displayed anywhere: not in the stdout console where Im running the pipeline script, not in the CloudWatch logs and nowhere in the AWS Sagemaker Console. The pipeline execution simply fails with some general WaiterError message:\n> botocore.exceptions.WaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n\nTherefore, I have no way to know why did the pipeline failed.... What am I missing here? Where can I find the FailStep message **at runtime**?\n\n```\nstep_fail = FailStep(\n        name=\"AccuracyFailStep\",\n        error_message=Join(on=\" \", values=[\"Execution failed due to binary accuracy < \", accuracy_threshold]),\n)\n\nstep_cond = ConditionStep(\n        name=\"CheckAccuracyEvaluationStep\",\n        conditions=[cond_lte],\n        if_steps=[step_create_model, step_register_model, step_deploy_model],\n        else_steps=[step_fail]\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to hook up CI\/CD with sagemaker pipelines  in sagemaker sudio?",
        "Question_created_time":1672250354190,
        "Question_last_edit_time":1672598727234,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzTH1AL_WToem_Gt47y83Ug\/how-to-hook-up-ci-cd-with-sagemaker-pipelines-in-sagemaker-sudio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":101,
        "Question_answer_count":1,
        "Question_body":"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb, I have created various pipeline step and tie those together and create a pipeline at the end. once these pipelines are created , how do i link this pipeline with steps for data preprocessing, training with other pipelines, say one for evaluation , another for deployment. is there anything inbuild in sagemaker studio to do this , or do we have to write our own code for this ? how can i link\/feed output\/artifact from one pipeline to another? \n\n```\nfrom sagemaker.workflow.steps import TrainingStep\nfrom sagemaker.workflow.steps import ProcessingStep\nfrom sagemaker.workflow.pipeline import Pipeline\n\nprocessor_args = sklearn_processor.run(    inputs=  ...  ,    outputs=...  ,    code=\"code\/preprocessing.py\",)\n\nmy_processing_step = ProcessingStep(name=\"my_processing_step \", step_args=processor_args)\n\n.....\nmy_training_train = TrainingStep(    name=\"my_training_train \",    step_args=train_args,)\n...\npipeline_name = f\"experiment_Pipeline\"\npipeline = Pipeline(   name=pipeline_name,\n    parameters= .., \n    steps=[my_processing_step , my_training_train  ...],\n)\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Sagemaker splitting between training and validation and ClientError",
        "Question_created_time":1672244770081,
        "Question_last_edit_time":1672591940917,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbj-71RLaT2uChIL7hwzcSg\/aws-sagemaker-splitting-between-training-and-validation-and-clienterror",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_body":"Dears,\n\nI started working with Sagemaker! In other words, I am new with this service.\nI have a db contains some facial images for three races black\/white\/asian and two genders male\/female. I would like to use sagemaker AWS to train\/model it. I created train and validation labels and sepreated data into 6 classess for training and validation: (asian female af\/black male bf\/white male wm\/etc)\n\ns3:\/\/dbimgraces\/af\/\n\ns3:\/\/dbimgraces\/am\/\n\ns3:\/\/dbimgraces\/bf\/\n\ns3:\/\/dbimgraces\/bm\/\n\ns3:\/\/dbimgraces\/wf\/\n\ns3:\/\/dbimgraces\/wm\/\n\nThe first question is that we only have 20 channeles in sagemaker, 6 trainings+6 validations+6 train lables+6 validation lables=24 channels which wont be accepted by aws sagemaker. How can I solve this issue? Is any advice for splitting and structuring about the model?\n\nThe second question is that with every combination (train\/validation\/train-lb\/validation-lb), I get the following error? why?\n\nClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed ('validation-lb-bm', 'validation-lb-bf', 'train-lb-bf', 'train-lb-bm' were unexpected) Failed validating 'additionalProperties' in schema: {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#', 'additionalProperties': False, 'anyOf': [{'required': ['train']}, {'required': ['validation']}, {'optional': ['train_lst']}, {'optional': ['validation_lst']}, {'optional': ['model']}], 'definitions': {'data_channel': {'properties': {'ContentType': {'type': 'string'}}, 'type': 'object'}}, 'properties': {'model': {'$ref': '#\/definitions\/data_channel'}, 'train': {'$ref': '#\/definitions\/data_channel'}, 'train_lst': {'$ref': '#\/definitions\/data_channel'}, 'validation': {'$, e\n\nClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed ('lb_bf', 'valid_af', 'valid_bm', 'lb_wm', 'tr-bm', 'valid_wm', 'tr-am', 'tr-wf', 'lb_bm', 'lb_am', 'lb_wf', 'tr-af', 'tr-bf', 'valid_bf', 'valid_wf', 'lb_af', 'valid_am', 'tr-wm' were unexpected) Failed validating 'additionalProperties' in schema: {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#', 'additionalProperties': False, 'anyOf': [{'required': ['train']}, {'required': ['validation']}, {'optional': ['train_lst']}, {'optional': ['validation_lst']}, {'optional': ['model']}], 'definitions': {'data_channel': {'properties': {'ContentType': {'type': 'string'}}, 'type': 'object'}}, 'properties': {'model': {'$ref': '#\/definitions\/data_channel'}, 'train': {'$ref': '#\/definitions\/data_channel'}, , e\n\nClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed ('train-lst', 'validation-lst' were unexpected) Failed validating 'additionalProperties' in schema: {'$schema': 'http:\/\/json-schema.org\/draft-04\/schema#', 'additionalProperties': False, 'anyOf': [{'required': ['train']}, {'required': ['validation']}, {'optional': ['train_lst']}, {'optional': ['validation_lst']}, {'optional': ['model']}], 'definitions': {'data_channel': {'properties': {'ContentType': {'type': 'string'}}, 'type': 'object'}}, 'properties': {'model': {'$ref': '#\/definitions\/data_channel'}, 'train': {'$ref': '#\/definitions\/data_channel'}, 'train_lst': {'$ref': '#\/definitions\/data_channel'}, 'validation': {'$ref': '#\/definitions\/data_channel'}, , e",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Issues with exposing SKLearn model as endpoint on AWS Sagemaker",
        "Question_created_time":1672164906650,
        "Question_last_edit_time":1672512254499,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV1XEz0ZyTH-tqFLUzn1Y-Q\/issues-with-exposing-sklearn-model-as-endpoint-on-aws-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":233,
        "Question_answer_count":3,
        "Question_body":"I have tried to expose a SKLearn model as an endpoint after training it on AWS sagemaker, and I get another error related to hosting the endpoint: \n\"sagemaker.exceptions.UnexpectedStatusException: Error hosting endpoint sagemaker-scikit-learn-2022-12-26-21-19-15-169: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..\n\nProcess finished with exit code 1\"\n\nAfter taking a look at the CloudWatch logs, as this message suggests, it seems like there's an issue with a missing file related to a server that needs to be initialized for the endpoint to work : \n\"2022\/12\/26 21:23:12 [crit] 27#27: *63 connect() to unix:\/tmp\/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 169.254.178.2, server: , request: \"GET \/ping HTTP\/1.1\", upstream: \"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping \", host: \"169.254.180.2:8080\"\"\n\nDoes anyone happen to know of any way of resolving this? I have looked online for quick fixes but have not found any solutions, and I feel it has something to do with sagemaker not installing the right packages from docker or something like that - Unfortunately, I don't know of any way of reproducing this error locally.\n\nThe deployment script can be found in \"deploy.py\" in my github repo: \nhttps:\/\/github.com\/abhinavGirish\/newsy \n\nAny help\/insight is much appreciated - I've been struggling with this issue for about a week. Thank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to fix 'permissions denied' errors when installing NVIDIA driver for SageMaker Studio",
        "Question_created_time":1672144147527,
        "Question_last_edit_time":1672490084308,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIc-iwymyQfm1k63WbGOSGg\/how-to-fix-permissions-denied-errors-when-installing-nvidia-driver-for-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":37,
        "Question_answer_count":1,
        "Question_body":"I'm using a SM studio instance (ml.g4dn.4xlarge) and getting permission denied error when trying to run the needed files. I first started[ here ](https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/install-nvidia-driver.html) and selected Option 2. When I follow the [Quick start](https:\/\/docs.nvidia.com\/datacenter\/tesla\/tesla-installation-notes\/index.html)  for runfile installers I get the 'permission denied' error when I run \n```\n sudo sh NVIDIA-Linux-x86_64-$DRIVER_VERSION.run\n```\n\nI get the same error when I slow down and follow the [pre-installation actions ](https:\/\/docs.nvidia.com\/cuda\/cuda-installation-guide-linux\/index.html#pre-installation-actions)so I can't move forward\n\nMy SM role has SMFullAccess and EC2FullAccess",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Comprehend training input set",
        "Question_created_time":1671877500558,
        "Question_last_edit_time":1672223923776,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9z3uA_p1SeuxLtQ3RB24jw\/comprehend-training-input-set",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_body":"We are labelling the pdf documents. We have limited resource. To complete all the labelling task, it might take more time. I am thinking of splitting the number of documents into smaller chunks and create mulitple jobs to complete the entire labelling work. \n\nThe output of this task will be feed into comprehend for training. I observed the comprehend will take only 5 jobs as input. Is there a way to increate this input number  ? Or Is there a way to retrain comprehend with next set of 5 jobs. ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Crowd HTML Element for NER does not allow for overlapping annotations like the current Ground Truth allows",
        "Question_created_time":1671738370745,
        "Question_last_edit_time":1672084173369,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3uEu19m5TYyg7HZiknXqxw\/crowd-html-element-for-ner-does-not-allow-for-overlapping-annotations-like-the-current-ground-truth-allows",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":33,
        "Question_answer_count":1,
        "Question_body":"I am able to make an A2I UI that allows for NER annotation and classification (based off of https:\/\/github.com\/aws-samples\/amazon-sagemaker-ground-truth-task-uis\/blob\/master\/text\/named-entity-recognition-with-additional-classification.liquid.html).\nHowever the NER interface doesn't allow for overlapping annotations like Sagemaker Ground Truth's NER interface does. Is there a way to get overlapping annotations in A2I for NER?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Could not create SageMaker Domain due to S3 bucket creation failure",
        "Question_created_time":1671672088905,
        "Question_last_edit_time":1672019868725,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK1Z2qOTYTIqBRYYrLn9tjw\/could-not-create-sagemaker-domain-due-to-s3-bucket-creation-failure",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":531,
        "Question_answer_count":5,
        "Question_body":"I am trying to create a SageMaker Domain for my AWS account, following the Quick setup instructions in the [documentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-quick-start.html). Despite following the steps exactly, when I click submit, the domain creation fails with the following (unhelpful) error message:\n\"Could not create domain due to S3 bucket creation failure\"\n\nThere are no suggestions on how to proceed in the documentation or previous re:Post topics for this error message, in this context. Does anyone have any ideas?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to run an inference in sagemaker pipeline?",
        "Question_created_time":1671660466669,
        "Question_last_edit_time":1672008224227,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFQCzo_y3TdiQ5iWO4sFR-Q\/how-to-run-an-inference-in-sagemaker-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":459,
        "Question_answer_count":3,
        "Question_body":"based on the docs here https:\/\/github.com\/aws-samples\/sagemaker-pipelines-callback-step-for-batch-transform\/blob\/main\/batch_transform_with_callback.ipynb, a separate pipeline is created to perform a batch transform within sagemaker pipeline. the example utilizes a lambda and sqs to achieve this.  couldn't the batch transform job can simply be part of the training pipeline? once the model is trained, and added to model registry, one should be able to query the registry and get the latest model and run a batch transformation job on that, without the callback set up in the docs, right? any examples of running a batch transform job directly from a training pipeline?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1671913943536,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1671913943536,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi,\n\nThe solution can be various and it depends on what you are trying to achieve. In general, I believe it is a good idea to build a generic pipeline and utilize parameters for different jobs. The image below shows a typical ML pattern with stages.\n\n\n![Enter image description here](\/media\/postImages\/original\/IMgAGV22YXQ16wVlctBVCnwg)\n\nYou can use condition steps to orchestrate Sagemaker jobs, more information with code examples are below:\n\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\n\nHope it helps,",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to debug the explicability module of a model in the registry ?",
        "Question_created_time":1671619509493,
        "Question_last_edit_time":1671966145208,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbKoyR05NSzyovZPhdCwfqQ\/how-to-debug-the-explicability-module-of-a-model-in-the-registry",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":77,
        "Question_answer_count":2,
        "Question_body":"Hello, I get an error in the explicability module of the model registry (see screenshot below).\nHow can I debug this ? I would like to access logs to investigate this error.\n\n![Screenshot of the Model Registry in Sagemaker Studio](\/media\/postImages\/original\/IMgx-Xu-jQQMe7MpFtHc5zHw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Human review in Textract",
        "Question_created_time":1671618951970,
        "Question_last_edit_time":1671966031063,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpww4RCDXR4iHrHWYBBGUFA\/human-review-in-textract",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":136,
        "Question_answer_count":1,
        "Question_body":"I was exploring AWS Textract, and I want to understand that when we use the Human review function using A2I and Sage, once the human review is added, does the model learn from it? So that the next time if I upload an invoice of similar type, the model will be able to read it perfectly",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Sagemaker: Model Parallelism for NLP GPT-2 model",
        "Question_created_time":1671575053978,
        "Question_last_edit_time":1671921567274,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHD5t5NGrQBaM1BcPl4vSMw\/aws-sagemaker-model-parallelism-for-nlp-gpt-2-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_body":"Hello Dears, \n\nI am working on an NLP product based on GPT-2, I have some problems with data_pipeline.py that is included in AWS repo: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/training\/distributed_training\/pytorch\/model_parallel\/gpt2\/data_pipeline.py\n\nPlease, I need your help with how to customize this script for my own dataset in detail.\n\nBelow is the class of my dataset: \n\nclass MyDataset(Dataset):\n\n    def __init__(self, data, tokenizer, randomize=True):\n        title, text,Claims = [], [],[]\n        for k, v in data.items():\n            title.append(v[0])\n            text.append(v[1])\n            Claims.append(v[2])\n            \n        self.randomize = randomize\n        self.tokenizer = tokenizer \n        self.title     = title\n        self.text      = text\n        self.Claims      = Claims\n    #---------------------------------------------#\n    def __len__(self):\n        return len(self.text)\n    #---------------------------------------------#  \n    def __getitem__(self, i):\n\n        input = SPECIAL_TOKENS['bos_token'] + self.title[i] + \\\n                SPECIAL_TOKENS['sep_token'] + self.text[i] + SPECIAL_TOKENS['sep_token'] + self.Claims[i] + SPECIAL_TOKENS['eos_token']\n\n        encodings_dict = tokenizer(input,                                   \n                                   truncation=True, \n                                   max_length=MAXLEN, \n                                   padding=\"max_length\")   \n        \n        input_ids = encodings_dict['input_ids']\n        attention_mask = encodings_dict['attention_mask']\n        \n        return {'label': torch.tensor(input_ids),\n                'input_ids': torch.tensor(input_ids), \n                'attention_mask': torch.tensor(attention_mask)}\n\nThanks in advance!\n\nBasem",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Inference Recommendation",
        "Question_created_time":1671556563217,
        "Question_last_edit_time":1671903753373,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDPxlldhzS1GNaSLn4ebRxw\/sagemaker-inference-recommendation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":93,
        "Question_answer_count":1,
        "Question_body":"We have an application that processes customer survey responses to determine the overall sentiment of it. (negative, neutral, positive). We are leveraging SageMaker Sentiment analysis for this.\n\nBelow are some key datapoints of our current usage.\n\n1. We have 4 real time endpoints each having the below properties\n    1. Multi Model:\n        1. Model A is 6 GB\n        2. Model B is 9.4 GB\n    2. Runtime configuration:\n        1. ml.c5.2xlarge (8 vCPU, 16 GiB memory)\n    3. Image responsible for handling the inference is of size 4.5gb\n2. The reason we have 4 endpoints is so that we can make concurrent requests to SageMaker. For this we have our load balancing logic that determines which endpoint to call.\n\n\nThis pattern will not continue working for us during 2023 as we scale up our survey ingestion pipeline. From running a load test of our system, the maximum TPS to Sagemaker we can support currently is 5.333; anything beyond that causes SageMaker to send 5XX responses (because we max out the CPU on all the available cores). Furthermore, our call pattern is of batch so there is no need for us to have the endpoints running 24\/7.\n\nFor 2023, we have predicted that we want to start handling a TPS of up to 200 during peaks. Before we start designing a new workflow to support this TPS, we wanted to get the SageMaker\u2019s teams\/community's feedback on alternatives. While we could try to scale vertically and horizontally this setup, we don\u2019t believe deploying bigger hosts and creating more realtime endpoints is the right solution in terms of total costs (given our intermittent batch call pattern to Sagemaker). Specifically, we want to know which Inference Flavor would better fit our use case based on the above description of the system.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Pipelines - Using TuningStep.properties as ProcessingStep arguments",
        "Question_created_time":1671503254008,
        "Question_last_edit_time":1671849329326,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2OEhEO90Q66_dLjvBx4yGA\/sagemaker-pipelines-using-tuningstep-properties-as-processingstep-arguments",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":165,
        "Question_answer_count":1,
        "Question_body":"How can I use TuningStep's properties fields as arguments for the ProcessStep script? Some of the properties work normally, like `properties.HyperParameterTuningJobName` and` properties.BestTrainingJob.FinalHyperParameterTuningJobObjectiveMetric.Value`. However, other fields don't work and I get the following error when I try to upsert the Pipeline configuration:\n```\nClientError: An error occured (ValidationException) when calling the UpdatePipeline operation: Unknown property reference [Steps.Tuning.BestTrainingJob.TunedHyperparameters]\n```\nI've tried to use the `CreationTime `property too, with the same error.\n\nMy code is something like this:\n```\nproc_run_args = script_processor.run(\n\tcode = \"...\",\n\targuments = [\n\t\t\"--tuning-job-name\", step_tuning.properties.HyperParameterTuningJobName,\n\t\t\"--train-loss-metric\", step_tuning.properties.BestTrainingJob.FinalHyperParameterTuningJobObjectiveMetric.Value.to_string(),\n\t\t\"--hyperparameters\", step_tuning.properties.BestTrainingJob.TunedHyperparameters.to_string()\n\t],\n\tinputs = [ ... ],\n\toutputs = [ ... ],\n)\n\nstep_process_metrics = ProcessingStep(\n\tname = \"...\",\n\tstep_args = proc_run_args\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Venn diagram of permissions in SageMaker Studio?",
        "Question_created_time":1671475979923,
        "Question_last_edit_time":1671823545397,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUTo01GFWQlyRYDXAKQInyA\/venn-diagram-of-permissions-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":167,
        "Question_answer_count":1,
        "Question_body":"Is there any overlap of permissions between a space execution role and a user profile execution role, if those two are given different permissions? Does it behave the same as with service control policies (SCPs) and policies on IAM roles, where the overlap is what takes effect? For example, if the space execution role has an explicit deny (or even an implicit deny for that matter!) for `CreateApp`, but the user profile execution role has an explicit allow for `CreateApp`, then that user profile won't be able to CreateApp?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1671479342573,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1671492806348,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi Yann, the user profile's execution role is what will be used within the context of the private app of the user profile. So, once a user hits Launch -> Studio or is redirected to Studio UI through SSO, the user's execution role will allow them to launch apps such as data science app, data wrangler app, etc.\n\nThe default space execution role is what is assumed by the user once they are in a shared space. So, once the user is in the UI by clicking Launch -> Spaces, the user cannot create apps to run notebooks in, within that space.\n\nTL;DR - they are two distinct roles, each giving the user permissions on a private space or a shared space, and do not work like SCPs. Any user in the shared space within a domain, will share the same execution role as of today. However, for the private space, each user profile can have their own role if needed.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Xgboost Framework 1.7.0+ Support?",
        "Question_created_time":1671472244518,
        "Question_last_edit_time":1671819683431,
        "Question_link":"https:\/\/repost.aws\/questions\/QU74uJEUshSTODzUJ1b6croQ\/sagemaker-xgboost-framework-1-7-0-support",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_body":"Any plans for Built-in Sagemaker Xgboost to support version 1.7.0+? This version allows for spark based xgboost models.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio -Land cover segmentation - unable to upload geojson",
        "Question_created_time":1671446032404,
        "Question_last_edit_time":1671791925053,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL54hzIgST8iBenQmI5r-SA\/sagemaker-studio-land-cover-segmentation-unable-to-upload-geojson",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":19,
        "Question_answer_count":0,
        "Question_body":"I went to geojson.io and selected a rectangle over a random area on the map. I then downloaded the geojson and uploaded it to Sagemaker's Studio's spatial - Land cover segmentation portal. I then received the following error:\n\n```\n\"Invalid GeoJson format\nUploaded file cannot be a Feature or Feature Collection GeoJson. Please upload a GeoJson with a 'type' of 'Polygon'\"\n```\nHow else can I pick a piece of land from anywhere on the globe and feed it to sagemaker studio, if not with geojson.io? thanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can SageMaker notebook jobs access studio storage?",
        "Question_created_time":1671236023120,
        "Question_last_edit_time":1671582880757,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqh6oq1d6QbKzNSszEwPm0g\/can-sagemaker-notebook-jobs-access-studio-storage",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_body":"I'm using SageMaker Studio, and I have my data files as well as a requirements.txt organized under my home directory. All works fine when I run notebook kernels interactively: they can access my files just fine. However, when I create a \"notebook job\", it doesn't seem to have access to any of my files. Is there a way to give my notebook job access to the same file system as my interactive notebooks?\n\nAfter I run a job, I see that a folder for the job was created within the input S3 bucket, and within that folder there's a \"input\/\" subfolder. But I don't know how to predict the name of the temp folder created for the job, so it doesn't seem like I could myself drop additional inputs in there, even if I wanted to. And if I could, how would I find them, at run-time?\n\nCould sure use guidance on how my notebook jobs can access input files.\n\nThanks,\n\nChris",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1671466765571,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1671466784965,
        "Answer_comment_count":3.0,
        "Answer_body":"Hi Chris, the option to use input files is to directly use the S3 URIs in the notebook itself, i.e., instead of reading from an `inputs` folder in your local EFS storage (which doesn't get copied over to `inputs` folder for the training job), read the inputs directly from the S3 URI. If the inputs will be dynamic for your notebook jobs, use parameterized executions (reference - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-auto-run-troubleshoot-override.html)",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Unexpected error creating notebook job in Sagemaker",
        "Question_created_time":1671234948533,
        "Question_last_edit_time":1671582168392,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaU6ZAXCYRcC_yYuypPuSdw\/unexpected-error-creating-notebook-job-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":51,
        "Question_answer_count":0,
        "Question_body":"I have a SageMaker notebook that I want to run as a notebook job (it takes a long time). Every time I try to Create Notebook Job...Create, I get \"Unexpected error occurred during creation of job.\" Things I've tried:\n\n1. Making the IAM revisions suggested at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/scheduled-notebook-policies.html, including the \"Add additional IAM permissions\" inline policy, even though I'm using the Studio execution role.\n2. Removing all special characters from the job name.\n3. Trying different compute types, including one I know works for interactively running my notebook.\n4. Moving the notebook to my root user directory.\n5. Removing all special characters from the notebook filename.\n6. Adding a parameter, even though I don't need any parameters.\n7. Running on a schedule vs. running now.\n8. Confirming that the referenced S3 buckets in the advanced settings are there (they were auto-created).\n9. Trying a different notebook.\n10. Refreshing, closing and re-opening.\n11. Looking in the CloudWatch logs. I found \"[E 2022-12-16 23:30:13.770 SchedulerApp] [Errno 2] No such file or directory: '\/home\/sagemaker-user\/abc\/abc\/abc\/abc\/src\/notebook.ipynb'\" where the actual path includes only two \"abc\"s, not four. If I move the notebook file to the root directory, then the error has two \"abc\"s, still, and fails again because it's missing the \"src\".\n\nIn the end, I couldn't get it to stop prepending \"abc\/abc\/\" to the path, so I had to:\n\n1. Create a \"\/src\/notebook.ipynb\" file off the root.\n2. Select to create a job to run that.\n3. Observe that it actually runs \/abc\/abc\/src\/notebook.ipynb, even though the UI claims it will be running \/src\/notebook.ipynb.\n\nThis all seems completely insane, and I'm hoping there's some way to resolve this?\n\nThanks,\n\nChris",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Running tensorboard in sagemaker",
        "Question_created_time":1671106484927,
        "Question_last_edit_time":1671452300488,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0H20Tuy6Q5SowSRnli4-SQ\/running-tensorboard-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":40,
        "Question_answer_count":1,
        "Question_body":"I'm unable to view tensorboard in sagemaker notebook instance. Tried 2 methods - \n1. Running in a notebook \n\n```\n!pip install tensorboard\n%load_ext tensorboard\n%tensorboard --logdir runs --host localhost --port 6006\n```\nOpening the link - https:\/\/<instance-url>\/proxy\/6006 shows failed to load active dashboards\n![Enter image description here](\/media\/postImages\/original\/IMHIFMwGeNTi6npjJuh_TbQg)\n\n\n2. Running in notebook terminal \n\n```\npip install tensorboard\ntensorboard --logdir runs --host localhost --port 6006\n```\nOpening the link - https:\/\/<instance-url>\/proxy\/6006 shows blank image",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Lifecycle configuration",
        "Question_created_time":1671042577350,
        "Question_last_edit_time":1671389231783,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOu9L-QbgSBqsHD7AxV0kSA\/sagemaker-lifecycle-configuration",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":366,
        "Question_answer_count":2,
        "Question_body":"Trying to create a lifecycle config for notebook instance.\n![error message](\/media\/postImages\/original\/IM7NOj882NTlCWsFHLDs9mmg)\nAlso there is nothing in cloudwatch logs corresponds to this failure.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Forecasting in AWS Autopilot",
        "Question_created_time":1670920730192,
        "Question_last_edit_time":1671268065469,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlcwt3TxNSqyzhsxszvXuSw\/forecasting-in-aws-autopilot",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":2,
        "Question_body":"Hey all, \nI am used to using an AutoML solution where you can do forecasting algorithms, by uploading a time-series dataset and choose a time-period for which you want to predict (e.g. 6 months). The time-series dataset can be both multivariate, multiple time-series, include categorical\/numericla values and include parameters known in advance (e.g. time since product launch, country, product etc.). In AWS AutoPilot I can only choose regression\/multiclass classification - but not really a prediction model for a set number of months. When I do a google search, I cant find any guides to how to do forecasting in AutoPilot - even if it claims that e.g., stock predictions are possible. Any comments, thought and ideas to this? Or any link to a guide that can help we set up AutoPilot for forecasting?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Human Task Failed - Sagemaker ground truth labelling jobs",
        "Question_created_time":1670920092769,
        "Question_last_edit_time":1671266492940,
        "Question_link":"https:\/\/repost.aws\/questions\/QUirlDbmZPS8SCqnmC2RFA6A\/human-task-failed-sagemaker-ground-truth-labelling-jobs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":91,
        "Question_answer_count":1,
        "Question_body":"I have setup the pdf labelling task in Sagemaker groud truth following this link - https:\/\/github.com\/aws-samples\/amazon-comprehend-semi-structured-documents-annotation-tools\n\nAfter sometime, the job is failed saying  \"**Complete with labeling errors**\". I found the below log in cloudwatch logs\n\n```\n{\n    \"event-name\": \"HUMAN_TASK_FAILED\",\n    \"event-log-message\": \"ERROR: Human task failed for line 694.\",\n    \"labeling-job-name\": \"resume-labeling-job-20221201T182336\"\n}\n```\n\nNot sure what happened. Does anyone have any way to identify the root cause behind this failure ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1671877894556,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1671877894556,
        "Answer_comment_count":0.0,
        "Answer_body":"Since I didn't get answer here, I reached out to tech support to seek guidance. It looks like the job duration is elapsed. \n\n\"failure-reason\": \"ClientError: Annotation tasks expired. \n\nReasons are TaskAvailabilityLifetimeInSeconds parameter is too small. \n\nYou can validate this configuration by running the following AWS CLI command from your environment:\naws sagemaker describe-labeling-job --labeling-job-name resume-labeling-job-20221212T094103\n\n\u2014References\u2014\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/\/latest\/APIReference\/API_HumanTaskConfig.html#API_HumanTaskConfig_Contents",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"What is the Differences between SageMaker pytorch SDK and tensorflow SDK?",
        "Question_created_time":1670914204892,
        "Question_last_edit_time":1671260763107,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHU8LsOMtQlexXJzmf2iZ6w\/what-is-the-differences-between-sagemaker-pytorch-sdk-and-tensorflow-sdk",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_body":"Dear SageMaker makers,\n\nHi, I'm reading the docs about sagemaker pytorch, tensorflow SDK.\nBut I can't get how each framework sdks are optimized for that framework.\nLiterally, What makes using pytorch sdk more beneficial to train and deploy pytorch model than using just sagemaker sdk \n(sagemaker.pytorch.estimator.PyTorch vs. sagemaker.estimator.Estimator)\n\nthank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Attempting to use Sagemaker to generate pt file, I am given params file.",
        "Question_created_time":1670791514262,
        "Question_last_edit_time":1671138845672,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl6qkMnH0Tim-DuL5Rr8gFg\/attempting-to-use-sagemaker-to-generate-pt-file-i-am-given-params-file",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_body":"When running the tutorial code at https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.html the model.tar.gz file that is generated contains three files: \n\nhyperparams.json\n\nmodel_algo-1\n\nmodel_best.params\n\nI assume that model_best.params is the model, but I have no familiarity with params files and am attempting to load the model locally for inference with PyTorch. I have no intentions of running inference in AWS, I intend to run inference locally on an NVIDIA Jetson device. Is there any way to make AWS Sagemaker generate a .pt file? Is there any way to convert a .params file to a .pt file? Is there any way for PyTorch to natively import a .params file? I am very new to AWS and am desperately attempting to train a deeplab image segmentation model that can be used for local PyTorch inference. Is this possible with AWS?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Estimator Metrics Extraction Problem",
        "Question_created_time":1670698854852,
        "Question_last_edit_time":1671045235122,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZ2xR8P_wQx-j-Ee-5LKnxQ\/estimator-metrics-extraction-problem",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":33,
        "Question_answer_count":0,
        "Question_body":"Hello! I created an estimator with a metrics definition for the validation binary accuracy of the trained model as seen below:\n```\nestimator = TensorFlow(entry_point=code_entry,\n                   source_dir=code_dir,\n                   role=sagemaker_role,\n                   instance_type='local',\n                   instance_count=1,\n                   model_dir='s3:\/\/some-bucket\/model\/',\n                   hyperparameters=hyperparameters,\n                   output_path='s3:\/\/some-bucket\/results\/',\n                   framework_version='2.8',\n                   py_version='py39',\n                   metric_definitions=[\n                        {\n                            \"Name\": \"binary_accuracy\",\n                            \"Regex\": \"val_binary_accuracy=(\\d+\\.\\d+)?\",\n                        }\n                    ],\n                   script_mode=True)\n```\nAccording to [Sagemaker's Estimator documentation][1], by setting the metrics definition, Sagemaker will extract metrics from the training logs using a regex: \n\n> metric_definitions (list[dict[str, str] or list[dict[str, PipelineVariable]]) \u2013 A list of dictionaries that defines the metric(s) used to evaluate the training jobs. Each dictionary contains two keys: \u2018Name\u2019 for the name of the metric, and \u2018Regex\u2019 for the regular expression used to extract the metric from the logs. \n\nThe trained model metrics are then available in the properties of the training job in the `FinalMetricDataList`object.\n \nIn the training logs I can see the log below, which matches the regex in the estimator's metrics definition: \n\n    2wn9x0rkaf-algo-1-43gci | INFO:root:val_binary_accuracy=0.5\n\nBut when I try to access the metric with the code below:\n\n    step_training.properties.FinalMetricDataList['binary_accuracy'].Value\n\n... I get the following error in my conditional step that checks if the metric is higher than a threshold: \n\n     Pipeline step 'CheckAccuracyEvaluation' FAILED. Failure message is: {'Get': \"Steps.TrainingStep.FinalMetricDataList['binary_accuracy'].Value\"} is undefined.\n\nI really don't understand why the binary_accuracy metric is undefined. The training model step always conclude successfully.\n\nHow to get the metrics from a trained model using an estimator with script mode?  \n\nThank you in advance!\n\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deepracer Student League - What is happening when model training is \"Evaluating\"",
        "Question_created_time":1670657539389,
        "Question_last_edit_time":1671004025394,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0cp-XRkqQNSFLKY3LS30iA\/deepracer-student-league-what-is-happening-when-model-training-is-evaluating",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":101,
        "Question_answer_count":1,
        "Question_body":"Hello everyone,\n\nI was wondering what is happening during the \"Evaluating\" phase during training for deepracer student. I see through the simulation video that across evaluation runs, there are still big differences in deepracer performance, so I was wondering if it is comparing different policies that it formed during training. I am using the PPO algorithm. Thanks in advance!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker training job request fails from Lambda with EFS",
        "Question_created_time":1670635434784,
        "Question_last_edit_time":1670982721625,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSYaKqNkoTxWH8DVsgXimDA\/sagemaker-training-job-request-fails-from-lambda-with-efs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":34,
        "Question_answer_count":1,
        "Question_body":"I have a Lambda function with an s3 trigger to perform 2 parts:\n1. Convert the file uploaded to the trigger s3 bucket from csv to parquet and write a copy to another bucket\n2. Trigger a sagemaker training job with parameters that have contents from the csv file.\nThis lambda also as an EFS attached to it.\n\nPart 1 executes without any issue, but the part 2 goes into a timeout with out any explicit errors. \nI do not understand where to look for. The lambda has all the necessary permissions to trigger the sagemaker jobs.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do you choose the number of Studio Notebook Instances per Data Scientist?",
        "Question_created_time":1670602460560,
        "Question_last_edit_time":1670949831717,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5kNZTb_yRR6VUo1Reg6lxg\/how-do-you-choose-the-number-of-studio-notebook-instances-per-data-scientist",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":231,
        "Question_answer_count":2,
        "Question_body":"When calculating the cost of SageMaker Studio Notebooks in [the AWS Pricing Calculator](https:\/\/calculator.aws\/#\/addService\/SageMaker), it asks you for the \"Number of Studio Notebook instances per data scientist per month.\"\n\nHow do you reason about this? What would be the use case for having multiple instances for one data scientist? Would that happen if an individual is working on multiple projects, which have different kernels and library dependencies?\n\nI imagine most of the time it will be 1 Studio Notebook instance per data scientist per month, instead of 2 or more instances per data scientist?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1670809472327,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1670809472327,
        "Answer_comment_count":2.0,
        "Answer_body":"Hi @yann_stoneman, you're right. Up to 4 apps can run on the same instances, so different kernels could still be run on the same instance. For example, a data scientist could be working on a tabular use case, and an image processing use case - so they might have a CPU and GPU instance running. Or they might use a larger instance for data processing or data wrangler feature. \n\nDepending on your data scientists' projects and use cases, I'd account for at most 2 instances per data scientist running concurrently. If your users already use SageMaker Notebook Instances, you can use the commonly used resource type as the Studio instance resource type for estimates - that way you can get a closer estimate to the actual costs. \n\nIf you're allowing for shared spaces (real time collaboration), include additional instances in your estimate - the users will now be able to use a private space through their user profile (unique to one user) and a shared space (this instance can be accessed across profiles). \n\nI'd also recommend using a plugin to shut down idle instances as a best practice when your teams are onboarded to Studio, so these instances are shut down if there are no notebooks actively running (ref: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/)",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"When will g5 instances be available for batch transform?",
        "Question_created_time":1670368748045,
        "Question_last_edit_time":1670715064804,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd8Jq7t7sRiuRqzuBzGZcfQ\/when-will-g5-instances-be-available-for-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":129,
        "Question_answer_count":1,
        "Question_body":"I see the new A10 GPU G5 instances are available for sagemaker training and notebook but not Batch transform, when will it be rolled out to this service?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Exporting Model from Sagemaker Canvas to Sagemaker studio",
        "Question_created_time":1670318595716,
        "Question_last_edit_time":1670665524733,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeeQBcdzdSgyVYJFm3xnJ9A\/exporting-model-from-sagemaker-canvas-to-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":291,
        "Question_answer_count":1,
        "Question_body":"I tried to export a model trained in Canvas into Studio. The export is only allowed through a sagemaker studio link. However, when I listed available models through the CLI in Studio, I couldn't see the model imported from Canvas? Is this the intended behavior?\nAlso, is there any other way to export a model from Canvas to Studio other than the URL?\n\nThanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error Creating Endpoint",
        "Question_created_time":1670280332617,
        "Question_last_edit_time":1670628195195,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0JgbfwUoS5m6VH4TvtZmkg\/error-creating-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":71,
        "Question_answer_count":1,
        "Question_body":"Hi! The following error happens while trying to create an endpoint from a successful trained model:\n\n* In the web console: \n> The customer:primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.\n * CloudWatch logs: \n> exec: \"serve\": executable file not found in $PATH\n\nIm deploying the model using a Lambda step, just as in this [notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb). The Lambda step is successful, and I can see in the AWS web console that the model configuration is created with success. \n\nThe exact same error happens when I  create an endpoint for the registered model in the AWS web console, under Inference -> Models. In the console I can see that an inference container was created for the model, with the following characteristics:\n* Image: 763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39\n* Mode: single model\n* Environment variables (Key Value): \n> SAGEMAKER_CONTAINER_LOG_LEVEL\t20\n\n> SAGEMAKER_PROGRAM\tinference.py\n\n> SAGEMAKER_REGION\teu-west-3\n\n> SAGEMAKER_SUBMIT_DIRECTORY\t\/opt\/ml\/model\/code\n \nI absolutely have no clue what is wrong and I could not find anything relevant online about this problem. Is it necessary to provide an custom docker image for inference or something?\n\nFor more details, please find below the pipeline model steps code. Any help would be much appreciated!\n```\nmodel = Model(\n    image_uri=estimator.training_image_uri(),\n    model_data=step_training.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=sagemaker_session,\n    role=sagemaker_role,\n    source_dir='code',\n    entry_point='inference.py'\n)\nstep_model_create = ModelStep(\n        name=\"CreateModelStep\",\n        step_args=model.create(instance_type=\"ml.m5.large\")\n )\n\nregister_args = model.register(\n        content_types=[\"*\"],\n        response_types=[\"application\/json\"],\n        inference_instances=[\"ml.m5.large\"],\n        transform_instances=[\"ml.m5.large\"],\n        model_package_group_name=\"test\",\n        approval_status=\"Approved\"\n)\nstep_model_register = ModelStep(name=\"RegisterModelStep\", step_args=register_args)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1670290340637,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1670290340638,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi, the problem here is that your inference model's container URI `763104351884.dkr.ecr.eu-west-3.amazonaws.com\/tensorflow-training:2.8-cpu-py39` is using a **training** image, not an **inference** image for TensorFlow. Because the images are each optimized for their own function, the serving executable is not available in the training container in this case.\n\nUsually, the framework-specific SDK classes will handle this lookup for you (for example `TensorFlowModel(...)` as used in the notebook you linked, or when calling `sagemaker.tensorflow.TensorFlow.deploy(...)` from the Estimator class.\n\nI see here though that you're using the generic `Model`, so guess you don't know (or don't want to commit to) the framework and version at the point the Lambda function runs?\n\nMy suggestions would be:\n\n- Can you use the Pipelines `ModelStep` to create your model before calling the Lambda deployment function? Similarly to how your linked notebook uses `CreateModelStep`. This would build your framework & version into the pipeline definition itself, but should mean that the selection of inference container image gets handled properly & automatically.\n- If you really need to be dynamic, I think you might need to find a way of looking up at least the *framework* from the training job. From my testing, you can use `estimator = sagemaker.tensorflow.TensorFlow.attach(\"training-job-name\")` and then `model = estimator.create_model(...)` to correctly infer the specific inference container *version* from a training job, but it still relies on knowing that TensorFlow is the correct framework. I'm not aware of a framework-agnostic equivalent? So could e.g. try describing the training job, manually inferring which framework it uses from that information, and then using the relevant framework estimator class' [attach()](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.attach) method to figure out the specifics and create your model.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to configure our own inference.py for two different PyTorch models in MultiDataModel to build single endpoint and call both models from there?",
        "Question_created_time":1670259036956,
        "Question_last_edit_time":1670606543393,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEVxelof3TmimoLt1Kd1SBA\/how-to-configure-our-own-inference-py-for-two-different-pytorch-models-in-multidatamodel-to-build-single-endpoint-and-call-both-models-from-there",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":227,
        "Question_answer_count":2,
        "Question_body":"I have referred [this](https:\/\/github.com\/aws-samples\/sagemaker-multi-model-endpoint-tensorflow-computer-vision\/blob\/main\/multi-model-endpoint-tensorflow-cv.ipynb) notebook to deploy PyTorch model but as per this notebook they are just calling newly trained model to get predictions by passing input payload... but I want to configure my own inference.py file to pass an input there and get predictions from there...\nSo, can anyone help me achieving that?\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Ground Truth authorization errors on SOME images",
        "Question_created_time":1670109648614,
        "Question_last_edit_time":1670456456673,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHQQvrfpPRCWIPLn9ToRNCA\/aws-ground-truth-authorization-errors-on-some-images",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":54,
        "Question_answer_count":1,
        "Question_body":"I've followed these tutorials:\n\nAmazon's - [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-getting-started.html]()\n\nMedium.com - [https:\/\/medium.com\/@bruceyanghy\/end-to-end-guide-for-setting-up-aws-sagemaker-ground-truth-public-data-labeling-jobs-8732cf49b865]()\n\nI'm attempting to run a basic object detection (images with bounding boxes) Ground Truth labeling job with Mechanical Turk. I've tried multiple times, opening up any and all security on the S3 bucket containing the images. My past two attempts have been the closest, but still only manage 32\/389 images labeled (357 Failed) and 62\/389 images labeled (327 Failed). The images are exactly the same in both attempts.\n\nChecking the manifest file for clues, the only error I get is:\n\n```\n<Code>InvalidArgument<\/Code>\n<Message>Requests specifying Server Side Encryption with AWS KMS managed keys require AWS Signature Version 4.<\/Message>\n<ArgumentName>Authorization<\/ArgumentName>\n<ArgumentValue>null<\/ArgumentValue>\n```\nI can't find any references anywhere as to what I need to change to make this work. I've tried with and without server-side encryption on the S3 bucket.\n\nHow do I change this signature version?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error in SQL explorer - Athena on Sagemaker Wrangler and Sagemaker notebook",
        "Question_created_time":1669909202635,
        "Question_last_edit_time":1670256379695,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUmG0Ed8DTKudEkJwfBxk4w\/error-in-sql-explorer-athena-on-sagemaker-wrangler-and-sagemaker-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_body":"I can't solve this error.\nI have given sagemaker IAM permission within the lake formation for this table, I have errors also using awswrangles lib within sagemaker notebook\nwhat am I doing wrong?\nStudio:\n![Enter image description here](\/media\/postImages\/original\/IMB_zL50UyRjKvNJYnta9ZKQ)\n\nNotebook:\n![Enter image description here](\/media\/postImages\/original\/IM9pyYOkXNQ5urTECMAL6row)\n![Enter image description here](\/media\/postImages\/original\/IM6ihq2P_ZQJ-myFQKB3Uijw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Pipeline Deploy Model Step",
        "Question_created_time":1669850852340,
        "Question_last_edit_time":1670198562116,
        "Question_link":"https:\/\/repost.aws\/questions\/QUiGdmBa_oQJuiiewjUhe9OA\/sagemaker-pipeline-deploy-model-step",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":312,
        "Question_answer_count":1,
        "Question_body":"According to [Sagemaker's Pipeline Python SDK documenation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html), looks like there is no specific pipeline step for model deployment. \n\nCan you please confirm this and, also, if there is a plan to have such a step? \n\nWhat is the recommended way to add a pipeline step to deploy the trained model, resulting in an enpoint being created?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1669881877547,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1669881962643,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi, there is indeed no specific pipeline step for model deployment. The idea is that SageMaker Pipelines is more about \"batch mode\", but customers do ask for this feature, so it might be added. \n\nYou can implement it quite easily using Lambda Step.\n\n1st create a Lambda function to deploy\/update the model:\n```\n%%writefile deploy_model_lambda.py\n\n\n\"\"\"\nThis Lambda function deploys the model to SageMaker Endpoint. \nIf Endpoint exists, then Endpoint will be updated with new Endpoint Config.\n\"\"\"\n\nimport json\nimport boto3\nimport time\n\n\nsm_client = boto3.client(\"sagemaker\")\n\n\ndef lambda_handler(event, context):\n\n    print(f\"Received Event: {event}\")\n\n    current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n    endpoint_instance_type = event[\"endpoint_instance_type\"]\n    model_name = event[\"model_name\"]\n    endpoint_config_name = \"{}-{}\".format(event[\"endpoint_config_name\"], current_time)\n    endpoint_name = event[\"endpoint_name\"]\n\n    # Create Endpoint Configuration\n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                \"InstanceType\": endpoint_instance_type,\n                \"InitialVariantWeight\": 1,\n                \"InitialInstanceCount\": 1,\n                \"ModelName\": model_name,\n                \"VariantName\": \"AllTraffic\",\n            }\n        ],\n    )\n    print(f\"create_endpoint_config_response: {create_endpoint_config_response}\")\n\n    # Check if an endpoint exists. If no - Create new endpoint, if yes - Update existing endpoint\n    list_endpoints_response = sm_client.list_endpoints(\n        SortBy=\"CreationTime\",\n        SortOrder=\"Descending\",\n        NameContains=endpoint_name,\n    )\n    print(f\"list_endpoints_response: {list_endpoints_response}\")\n\n    if len(list_endpoints_response[\"Endpoints\"]) > 0:\n        print(\"Updating Endpoint with new Endpoint Configuration\")\n        update_endpoint_response = sm_client.update_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n        )\n        print(f\"update_endpoint_response: {update_endpoint_response}\")\n    else:\n        print(\"Creating Endpoint\")\n        create_endpoint_response = sm_client.create_endpoint(\n            EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n        )\n        print(f\"create_endpoint_response: {create_endpoint_response}\")\n\n    return {\"statusCode\": 200, \"body\": json.dumps(\"Endpoint Created Successfully\")}\n```\n\nThen create the Lambda step:\n```\ndeploy_model_lambda_function_name = \"sagemaker-deploy-model-lambda-\" + current_time\n\ndeploy_model_lambda_function = Lambda(\n    function_name=deploy_model_lambda_function_name,\n    execution_role_arn=lambda_role,\n    script=\"deploy_model_lambda.py\",\n    handler=\"deploy_model_lambda.lambda_handler\",\n)\n```\n\nYou can see a full working example in [this notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"aws sagemaker real-time endpoint, does not process requests concurrently?",
        "Question_created_time":1669800514055,
        "Question_last_edit_time":1670147398232,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdFBs1AffQjewhJ5rVeoQgw\/aws-sagemaker-real-time-endpoint-does-not-process-requests-concurrently",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":98,
        "Question_answer_count":1,
        "Question_body":"I deployed stable diffusion v2.0 by aws sagemaker, and create endpoint for real-time inference.(instance type is ml.g4dn.xlarge)\nAlso, i used aws apigateway and aws lambda.\n\nmy question is that concurrency of prediction process by invoked request.\nwhen i check cloudwatch log, i see that request is sequentially process.(one prediction finished, then next prediction proceeded)\n\ni expected that the requests are concurrently dealt with, but not.\n\nat real-time endpoint, there is no max concurrent options, then invokeendpoint always sequentially procedeed??\nno way for making requests are parellely dealt with, except increase instance number?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to pass environment variables in sagemaker tuner job",
        "Question_created_time":1669725280762,
        "Question_last_edit_time":1670071482026,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":553,
        "Question_answer_count":1,
        "Question_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n```\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n```\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1669736547973,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1669736547974,
        "Answer_comment_count":0.0,
        "Answer_body":"Thanks for raising this. Yes, as you point out the `Environment` collection is not supported in the underlying `CreateHyperparameterTuningJob` API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the [SM Py SDK GitHub issue here](https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/2934), you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore [setting the variables from your Python script](https:\/\/stackoverflow.com\/a\/5971326) (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the [ENV command](https:\/\/docs.docker.com\/engine\/reference\/builder\/#env)? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\n- Use `sagemaker.image_uris.retrieve(...)` to find the base image URI for your given framework, version, region, etc. You'll need to [authenticate Docker to this registry](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/getting-started-cli.html#cli-authenticate-registry) as well as your own Amazon ECR account.\n- Create a Dockerfile that takes this base image URI as an arg and builds `FROM` it, something like [this example](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/b95b62cd2abd304ee347cacdd3eaf2a76e8b5953\/notebooks\/custom-containers\/train-inf\/Dockerfile)\n- Add the required `ENV` commands to bake in the (static) environment variables you need\n- `docker build` your custom container (passing in the base image URI as a `--build-arg`), upload it to Amazon ECR, and use in your SageMaker training job.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Prevent boto3.client('sagemaker').create_auto_ml_job() from deploying endpoint",
        "Question_created_time":1669662766261,
        "Question_last_edit_time":1670009940311,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGEnbfLBgQJKrbdKfizHkEw\/prevent-boto3-client-sagemaker-create-auto-ml-job-from-deploying-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":73,
        "Question_answer_count":2,
        "Question_body":"When I invoke the .create_auto_ml_job() method both with and without the optional ModelDeployConfig kwarg, the autopilot job deploys an endpoint using the best model. Is there a way to prevent the .create_auto_ml_job() method from behaving this way? I do not wish to deploy the best model to an endpoint, and do not wish to have to delete this endpoint.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Model Monitor - endPointInput:contentType is of type application\/octet-stream",
        "Question_created_time":1669654298426,
        "Question_last_edit_time":1670001531583,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0i6A69kwRlelvy3hhpum3w\/sagemaker-model-monitor-endpointinput-contenttype-is-of-type-application-octet-stream",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":150,
        "Question_answer_count":1,
        "Question_body":"Calling the sagemaker model endpoint with contentType `application\/octet-stream` which is also being captured in Data Capture Logs.\nWhat would be the ideal way to transform the data such that model monitor can work properly? I understand that model monitor works with csv or json content types.\n\nAble to use a preprocessing script to transform the input and output to csv\/json from protobuf format. Can I change the contentType field also as part of pre-processing?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Pipeline strange warning message",
        "Question_created_time":1669623728809,
        "Question_last_edit_time":1669970468600,
        "Question_link":"https:\/\/repost.aws\/questions\/QUm9Ml6PX2QdOA5VIaDMblQg\/sagemaker-pipeline-strange-warning-message",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":118,
        "Question_answer_count":1,
        "Question_body":"Using Sagemaker's Python SDK 2.11 when I run my pipeline, I see this strange warning message: \n```\n\/personal_dir\/lib\/python3.8\/site-packages\/sagemaker\/workflow\/pipeline_context.py:233: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n```\n Before, I ran the exact same pipeline script with LocalPipelineSession without any problems and without any kind of weird warning messages. \n\nThis is how I am creating the PipelineSession object:\n```\ndef get_session(region, default_bucket):\n    boto_session = boto3.Session(region_name=region)\n    sagemaker_client = boto_session.client(\"sagemaker\")\n\n    return PipelineSession(\n        boto_session=boto_session,\n        sagemaker_client=sagemaker_client,\n        default_bucket=default_bucket\n    )\n```\nIm getting the region in the following way:\n```\nimport boto3\n\nregion = boto3.Session().region_name\n```\nI have tried to search the web for the meaning of that warning message, but could not find anything. What does that warning message means?? Am I doing something wrong and what can I do to make that warning disapear",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1669741153535,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1669741153536,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi, this warning is simply to clarify that running in a pipeline session will defer execution of jobs - generating pipeline step definitions instead of kicking off the jobs straight away.\n\nFor example calls such as `Estimator.fit()` or `Processor.run()` when using a pipeline session won't **start a job** (or wait for it to complete, or stream logs from CloudWatch), just prepare a definition to build up a pipeline that can be started later.\n\nIf you're already familiar with how PipelineSession works, I would say you can ignore it :-)  If not, can refer to the [SDK docs here for more details](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_model_building_pipeline.html#pipeline-session).\n\nCould be that there's an inconsistency between `LocalPipelineSession` versus `PipelineSession` in showing the message? Or that you disagree this message should be at warning level... Either way I'd suggest raising an issue on the [SageMaker Python SDK GitHub](https:\/\/github.com\/aws\/sagemaker-python-sdk) might be a good way to log that feedback with the team!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Using Lambda for data processing - Sagemaker",
        "Question_created_time":1669522752249,
        "Question_last_edit_time":1669869524785,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-EUVNgsoRViEFMwlZbcIAg\/using-lambda-for-data-processing-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":2,
        "Question_body":"I have created a docker image which has Entrypoint as processing.py. This script is taking data from \/opt\/ml\/processing\/input and after processing putting it \/opt\/ml\/processing\/output folder.\n\nFor processing the data I should put the file in \/opt\/ml\/processing\/input from s3 and then pick processed file from \/opt\/ml\/processing\/output into S3.\n\nFollowing script in sagemaker is doing it properly:\n\nfrom sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\nimport sagemaker\n\ninput_data = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Data\/Training\/Churn_Modelling.csv'\noutput_dir = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Outputs\/'\nimage_uri = '057036842446.dkr.ecr.ap-south-1.amazonaws.com\/aws-docker-repo:latest'\naws_role = sagemaker.get_execution_role()\n\nprocessor = Processor(image_uri= image_uri,\n                     role=aws_role,\n                     instance_count=1,\n                     instance_type=\"ml.m5.xlarge\")\n\nprocessor.run(inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n             outputs=[ProcessingOutput(source='\/opt\/ml\/processing\/output',\n                                       destination=output_dir)]\n                    )\n\nCould someone please guide how this can be executed with lambda function? It is not recognizing sagemaker package, second there is a challenge in placing file before the script execution and pick processed files.\n\nI am trying codepipeline to automate this operation.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"sagemakee endpoint failing with \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\"",
        "Question_created_time":1669407059790,
        "Question_last_edit_time":1669754578156,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbzR_PqclRoK5dmfEzR53wg\/sagemakee-endpoint-failing-with-an-error-occurred-modelerror-when-calling-the-invokeendpoint-operation-received-client-error-413-from-primary-and-could-not-load-the-entire-response-body",
        "Question_score_count":0,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":734,
        "Question_answer_count":1,
        "Question_body":"Hello, I have created sagemaker endpoint by following https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/20_automatic_speech_recognition_inference\/sagemaker-notebook.ipynb and this is failing with error \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\".\n\nThe predict function returning me following error but CW log does not have any error details for the endpoint.\n\n\n```\nModelError Traceback (most recent call last)\n\/tmp\/ipykernel_16248\/2846183179.py in\n2 # audio_path = \"s3:\/\/ml-backend-sales-call-audio\/sales-call-audio\/1279881599154831602.playback.mp3\"\n3 audio_path = \"\/home\/ec2-user\/SageMaker\/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face\/1279881599154831602.playback.mp3\" ## AS OF NOW have stored locally in notebook instance\n----> 4 res = predictor.predict(data=audio_path)\n5 print(res)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n159 data, initial_args, target_model, target_variant, inference_id\n160 )\n--> 161 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n162 return self._handle_response(response)\n163\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n493 )\n494 # The \"self\" in this scope is referring to the BaseClient.\n--> 495 return self._make_api_call(operation_name, kwargs)\n496\n497 _api_call.name = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n912 error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n913 error_class = self.exceptions.from_code(error_code)\n--> 914 raise error_class(parsed_response, operation_name)\n915 else:\n916 return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n`\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Getting TrainingJobName from Training step of Sagemaker Pipeline",
        "Question_created_time":1669298009010,
        "Question_last_edit_time":1669645740756,
        "Question_link":"https:\/\/repost.aws\/questions\/QU21YXYHCuRhip83ELnFlYHg\/getting-trainingjobname-from-training-step-of-sagemaker-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":107,
        "Question_answer_count":2,
        "Question_body":"I am running a sagemaker pipeline with a training step. This whole setup runs from a **Lambda function** while I pass a few parameters to the pipeline.py file. To get the TrainingJobName from the training step, \nmy code is `step_train.__dict__['step_args']['TrainingJobName']`\nThis works fine while I running it in sagemaker notebook but when I execute the same code to get the train job name from lambda, I get this error **[ERROR] TypeError: '_StepArguments' object is not subscriptable**\n`[ERROR] TypeError: '_StepArguments' object is not subscriptable\nTraceback (most recent call last):\n  File \"\/var\/task\/lambda_function.py\", line 46, in lambda_handler\n    pipeline = create_pipeline(validated_api_input)\n  File \"\/tmp\/pipeline.py\", line 105, in create_pipeline\n    \"job_name\" : training_step['step_args']['TrainingJobName']`\n\nHow do I resolve this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Column header is not showing when reading data from redshift to jupyter on Sagemaker",
        "Question_created_time":1669209147429,
        "Question_last_edit_time":1669557426839,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxjkMWSCuS2ezRSYihJWHnA\/column-header-is-not-showing-when-reading-data-from-redshift-to-jupyter-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":0,
        "Question_body":"Hello,\n\nI am reading a specific table from a redshift database using redshift connector. when i am viewing the dataframe it does not show the column headers. It shows only numbers as the column headers.\n\nCan anyone help whats wrong here? we need to view the table headers",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to keep sagemaker inference warm-up",
        "Question_created_time":1669198631982,
        "Question_last_edit_time":1669546626634,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVZqbuLPeSLWOe3TobeZHtQ\/how-to-keep-sagemaker-inference-warm-up",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":327,
        "Question_answer_count":1,
        "Question_body":"Calling sagemaker inference frequently (3-5 calls in a minute)  reduces runtime duration from ~200ms to ~50ms, so it seems there is similar warm-up behaviour like in Lambda. Do you have any suggestions how to keep sagemaker inference responsive always fast?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error when saving custom metrics in SageMaker Experiments through SageMaker Pipelines Training Job",
        "Question_created_time":1669166447689,
        "Question_last_edit_time":1669513325907,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwKF_EXaJQH2kimV1TIxI9g\/error-when-saving-custom-metrics-in-sagemaker-experiments-through-sagemaker-pipelines-training-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_body":"IHAC that I am working on enabling sagemaker experiments through a training job using SageMaker Pipelines. The below is the logic inserted into the train script which was working fine a few days ago tracking custom metrics into the trial component created by SageMaker Pipelines.\n\n```\n    try:\n        print('>>> Loading an existing trial component')\n        my_tracker = Tracker.load()\n        \n    except ValueError:\n        print('>>> Creating a new trial component')\n        my_tracker = Tracker.create()\n        \n    my_tracker.log_metric(\"mse:mse error\", mean_squared_error(valid_y, preds))\n    \n    my_tracker.close()\n```\n\nHowever, since yesterday I am facing an error with running the same code with the following error:\n\n> >>> Loading an existing trial component\nTraceback (most recent call last):\n  File \"training.py\", line 82, in <module>\n    my_tracker = Tracker.load()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/smexperiments\/tracker.py\", line 161, in load\n    _ArtifactUploader(tc.trial_component_name, artifact_bucket, artifact_prefix, boto3_session),\nAttributeError: 'NoneType' object has no attribute 'trial_component_name'\n\nI tried to change the versions of sagemaker and sagemaker-experiments to an older version but still see the same issue. This code works when I trigger just the training job out of SageMaker Pipelines but shows the above error when running through SageMaker Pipelines.  Any pointers how to fix this?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker g4 and g5 instances do not have working nvidia-drivers",
        "Question_created_time":1669082188275,
        "Question_last_edit_time":1669428704519,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBqYWuFr7SyC6P6Uae9LOww\/sagemaker-g4-and-g5-instances-do-not-have-working-nvidia-drivers",
        "Question_score_count":3,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":257,
        "Question_answer_count":2,
        "Question_body":"I am a heavy user of g4 and g5 instances on Sagemaker (notebook instances). Today when  I tried to use the same instances as I always do I was met with the following when running `nvidia-smi`\n\n`NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.`\n\nThese are all the exact same instances and workloads I have used before. The same message was found when trying to run on ec2 natively as well.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What should be suggested way to do object classification in a streamed video using pre-trained model with custom label?",
        "Question_created_time":1669043896491,
        "Question_last_edit_time":1669709266344,
        "Question_link":"https:\/\/repost.aws\/questions\/QUNQj9L8OfQOuWzjmEIhEFqQ\/what-should-be-suggested-way-to-do-object-classification-in-a-streamed-video-using-pre-trained-model-with-custom-label",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":101,
        "Question_answer_count":1,
        "Question_body":"Hello Everyone,\n\nI'm trying to create new model or with the help of pre-trained model for classification which can support video streaming data. \n\nI've tried custom label in AWS Rekognition for classification but I could evaluate that model only with image data (not in a streaming mode). Is there a way to evaluate a model for object detection or classification in a streamed video data?\n\nI've used AWS kinesis video stream to stream my laptop cam to AWS kinesis video stream. When I try to integrate Kinesis video stream with AWS Rekognition, I understood that it supports only face detection and specific object detection like pet, person or package labels with the pre-trained model. Is my understanding correct? or is there a way to use pre-trained model for a custom labels in a video streaming data.\n\nCan anyone suggest me the better solution for this requirement?\n\nAny help would be appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Pipelines - Is it possible to use a TransformStep with the Catboost Estimator ?",
        "Question_created_time":1668677758132,
        "Question_last_edit_time":1669024675576,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdkeWBFI3SXSA8QznUYgT1Q\/sagemaker-pipelines-is-it-possible-to-use-a-transformstep-with-the-catboost-estimator",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":479,
        "Question_answer_count":2,
        "Question_body":"Hi! I am trying to implement a Sagemaker Pipeline including the following steps (among other things):\n\n* **ProcessingStep**: processing script (PySparkProcessor) generating a train , validation and test dataset (csv) \n* **TrainingStep**: model training, CatBoost Estimator (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html)\n* **TransformStep**: batch inference using the model on the test dataset (csv)\n\nThe TransformStep returns the following error: **python3: can't open file 'serve': [Errno 2] No such file or directory**\n\nI wonder if I'm using TransformStep in the wrong way or if, at the moment, the use of TransformStep with the CatBoost model has not been implemented yet.\n\nCode:\n\n```\n[...]\npyspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark\",\n    framework_version=\"3.1\",\n    role=role_arn,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=12,\n    sagemaker_session=pipeline_session,\n    max_runtime_in_seconds=2400,\n)\n\nstep_process_args = pyspark_processor.run(\n    submit_app=os.path.join(\n        s3_preprocess_script_dir, \"preprocess.py\"\n    ),  # Hack to fix cache hit\n    submit_py_files=[os.path.join(\n        s3_preprocess_script_dir, \"preprocess_utils.py\"\n    ), os.path.join(\n        s3_preprocess_script_dir, \"spark_utils.py\"\n    )],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"datasets\",\n            source=\"\/opt\/ml\/processing\/output\",\n            destination=s3_preprocess_output_path,\n        )\n    ],\n    arguments=[\"--aws_account\", AWS_ACCOUNT, \"--aws_env\", AWS_ENV, \"--project_name\", PROJECT_NAME, \"--mode\", \"training\"],\n)\n\nstep_process = ProcessingStep(\n    name=\"PySparkPreprocessing\",\n    step_args=step_process_args,\n    cache_config=cache_config,\n)\n\ntrain_model_id = \"catboost-classification-model\"\ntrain_model_version = \"*\"\ntrain_scope = \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type,\n)\n\n# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n)\n\n# Retrieve the pre-trained model tarball to further fine-tune\ntrain_model_uri = model_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n)\n\ntraining_job_name = name_from_base(f\"jumpstart-{train_model_id}-training\")\n\n# Create SageMaker Estimator instance\ntabular_estimator = Estimator(\n    role=role_arn,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    max_run=360000,\n    hyperparameters=hyperparameters,\n    sagemaker_session=pipeline_session,\n    output_path=s3_training_output_path,\n    disable_profiler=True,  # The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n)\n\n# Launch a SageMaker Training job by passing s3 path of the training data\nstep_train_args = tabular_estimator.fit(\n    {\n        \"training\": TrainingInput(\n            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri\n        )\n    },\n    logs=True,\n    job_name=training_job_name,\n)\n\nstep_train = TrainingStep(\n    name=\"CatBoostTraining\",\n    step_args=step_train_args,\n    cache_config=cache_config,\n)\n\nscript_eval = ScriptProcessor(\n    image_uri=[MASKED],\n    command=[\"python3\"],\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    base_job_name=\"script-evaluation\",\n    role=role_arn,\n    sagemaker_session=pipeline_session,\n)\n\neval_args = script_eval.run(\n    inputs=[\n        ProcessingInput(\n            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            destination=\"\/opt\/ml\/processing\/model\",\n        ),\n        ProcessingInput(\n            source=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri,\n            destination=\"\/opt\/ml\/processing\/input\",\n        ),\n    ],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"evaluation\",\n            source=\"\/opt\/ml\/processing\/evaluation\",\n            destination=s3_evaluation_output_path,\n        ),\n    ],\n    code=\"common\/evaluation.py\",\n)\n\nevaluation_report = PropertyFile(\n    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\n\nstep_eval = ProcessingStep(\n    name=\"Evaluation\",\n    step_args=eval_args,\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\nmodel = Model(\n    image_uri=\"467855596088.dkr.ecr.eu-west-3.amazonaws.com\/sagemaker-catboost-image:latest\",\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=pipeline_session,\n    role=role_arn,\n)\n\nevaluation_s3_uri = \"{}\/evaluation.json\".format(\n    step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n)\n\nmodel_step_args = model.create(\n    instance_type=\"ml.m5.large\",\n)\ncreate_model = ModelStep(name=\"CatBoostModel\", step_args=model_step_args)\n\nstep_fail = FailStep(\n    name=\"FailBranch\",\n    error_message=Join(\n        on=\" \", values=[\"Execution failed due to F1-score <\", 0.8]\n    ),\n)\n\ncond_lte = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=step_eval.name,\n        property_file=evaluation_report,\n        json_path=\"classification_metrics.f1-score.value\",\n    ),\n    right=f1_threshold,\n)\n\nstep_cond = ConditionStep(\n    name=\"F1ScoreCondition\",\n    conditions=[cond_lte],\n    if_steps=[create_model],\n    else_steps=[step_fail],\n)\n\n# Transform Job\ns3_test_transform_input = os.path.join(step_process.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"], \"test\")\n\ntransformer = Transformer(model_name=create_model.properties.ModelName,\n                          instance_count=1,\n                          instance_type=\"ml.m5.xlarge\",\n                          assemble_with=\"Line\",\n                          accept=\"text\/csv\",\n                          output_path=s3_test_transform_output_path,\n                          sagemaker_session=pipeline_session)\n\ntransform_step_args = transformer.transform(\n    data=s3_test_transform_input,\n    content_type=\"text\/csv\",\n    split_type=\"Line\",\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    step_args=transform_step_args,\n)\n\n\n# Create and execute pipeline\nstep_transform.add_depends_on([step_process, create_model])\n\npipeline = Pipeline(\n    name=pipeline_name,\n    steps=[step_process, step_train, step_eval, step_cond, step_transform],\n    sagemaker_session=pipeline_session,\n)\n\npipeline.upsert(role_arn=role_arn, description=[MASKED])\nexecution = pipeline.start()\nexecution.wait(delay=60, max_attempts=120)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker studio performance issues - why so slow, or am I missing a trick?",
        "Question_created_time":1668593899489,
        "Question_last_edit_time":1668940175736,
        "Question_link":"https:\/\/repost.aws\/questions\/QULsv-qoydT_6tkC2cbdt_0w\/sagemaker-studio-performance-issues-why-so-slow-or-am-i-missing-a-trick",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":236,
        "Question_answer_count":1,
        "Question_body":"Hi all, I am fairly new to sagemaker studio, but I am concerned with the speed of the model pipeline. When I run preprocessing (Tfidf), training and evaluating models locally they take under a minute, however when using sagemaker studio it has taken at least 13 minutes.\n\nI have been running an XgBoost model based on the Abalone pipeline example and running this with my data which is around 5500 rows of text data (the text data ~1.5k characters on average per example). \n\nThe below example shows multiple pipeline runs with a HyperparameterTuner and even with the **ml.c5.18xlarge** and the below hyperparameter ranges, I was only able to complete the pipeline in 14 min run time (this included pre-processing step, hp-tuning, model register and model evaluation). FYI even without the tuning it still took around 14 min.\n\nI was wondering if I am missing a trick here or is it just Sagemaker takes awhile to start? Any help would be much appreciated! \n\n```\n xgb_train.set_hyperparameters(\n        objective='multi:softmax',\n        num_round=50,\n        max_depth=3,\n        eta=0.2,\n        gamma=4,\n        min_child_weight=6,\n        subsample=0.7,\n        silent=0,\n        num_class=18,\n        alpha=0.1904424349464699\n    )\n\n   hyperparameter_ranges = {\n                            \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n                        }\n```\n\n![Enter image description here](\/media\/postImages\/original\/IMFLkHgWI_TRyLcU1TxUNK3A)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Clone Failed SageMaker MLOps Project Using Third-party Git Repos",
        "Question_created_time":1668586359113,
        "Question_last_edit_time":1668934744625,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhIvfTpxzRtWRE-dO20bHmQ\/clone-failed-sagemaker-mlops-project-using-third-party-git-repos",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":86,
        "Question_answer_count":1,
        "Question_body":"I'm trying to use MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline in my ML project. I created the project successfully using the template and all the seed code is available in the GitHub repos I specified. But when I try to clone the repo, I am getting the below error\n\n![Enter image description here](\/media\/postImages\/original\/IM0TGphx8RTFmqI9r1zeuHxg)\n\nI see that the local path that has been specified in \n\n```\nNo such file or directory: '\/home\/sagemaker-user\/home\/sagemaker-user\/cat-ml-test-1-p-mtd5ofsbdgva\/sagemaker-p-mtd5ofsbdgva-modeldeploy'\n```\n\ndoes not sound quite right. But there's is no option to change the local path by myself also. \n\nHow can I solve this? Any leads are welcome\nTIA",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Ground Truth Plus Available Medical Expertise",
        "Question_created_time":1668530819224,
        "Question_last_edit_time":1668878326032,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1vINii7DRkGK-4klJOq7wA\/aws-ground-truth-plus-available-medical-expertise",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_body":"On Dec 1, 2021, AWS put out a press release regarding SageMaker Ground Truth Plus that contained the statement:\n\n>  To get started, customers\n    simply point Amazon SageMaker Ground Truth Plus to their data source in\n    Amazon Simple Storage Service (Amazon S3) and provide their specific\n    labeling requirements (e.g. instructions for how medical experts should\n    label anomalies in radiology images of lungs). \n\nCan AWS provide medical experts for labeling medical data?  Or am I misinterpreting this statement and the services included in this \"turnkey\" solution.  (BTW, I've already built and tested a custom segmentation task for SageMaker Ground Truth and am looking for \"expert\" labeling.)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible SageMaker HyperParameter Tuning Job without sagemaker-training tool kit",
        "Question_created_time":1668501531881,
        "Question_last_edit_time":1668848150585,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrrvbV_wJRuqG9TX1KyoZnQ\/is-it-possible-sagemaker-hyperparameter-tuning-job-without-sagemaker-training-tool-kit",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_body":"I had made my custom training image so It can be conducted through CreateTrainingJob, not sagemaker training took kit (requiring \"ContainerEntrypoint\" option).\n\nBut when I'm trying to run HyperParameter Tuning Job, it is not allowed to add \"ContainerEntrypoint\" option in \"AlgorithmSpecification\" field.\n\nIs it impossible to run HyperParameter Tuning Job with training images that can not be run without sagemaker training toolkit?\n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1668561111672,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1668561111672,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi there!\n\nFor custom training image, you can specify the entrypoint in your Dockerfile.\n\nBelow are some code snippet as well as links you can use as reference:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/rapids_bring_your_own\/code\/Dockerfile\n```\nENTRYPOINT [\".\/entrypoint.sh\"]\n```\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/keras_bring_your_own\/Dockerfile\n```\nENTRYPOINT [\"python\", \"-m\", \"trainer.start\"]\n```\n\nFurthermore, SageMaker Training Toolkit is a nicely wrapped up python package for you to use and eases the process of creating a custom training image, it's no different from implementing the logic yourself.\n\nSo it is definitely possible to run HyperParameter Tuning Job using custom containers without using our SageMaker Training Toolkit.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is any component of Sagemaker still involved in production  inferencing",
        "Question_created_time":1668489528537,
        "Question_last_edit_time":1678743019280,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVpIbYCjMTzOPTHJFfh9rnQ\/is-any-component-of-sagemaker-still-involved-in-production-inferencing",
        "Question_score_count":-1,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_body":"Are any components of the SageMaker Engine involved still during production ( inference) , I had a discussion with someone saying Sagemaker has nothing to do with production anymore because everything is deployed , containerized and executed somewhere  in a VM  or cluster.,   I think there are some instances the main engine is working in production  and please correct me  if i am wrong 1. Billing records - have to be generated -so something is connected to the engine ?  does the model help here or is it just pure EC2 etc compute measured ? Who manages the endpoints ( server less, and asynch) ?   if we need to use Sage maker monitor  this is also alive during production.  Question is What  main components are  active in production  outside model or does mode communicate to engine still?    As an addendum someone mentioned to me if it is deployed to EC2 or K8s yes it is disconnected from the main services in production you CANNOT use Model monitor in this scenario?  --- but if you deploy to SAGEMAKER INFERENCE?  you can use model monitor in prod.  You deploy to an endpoint for real time... What is SageMAker Inference - as a concept or as some physical thing because there are 71 options in Sagemaker inference ?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to access\/set up a model registry ?",
        "Question_created_time":1668477799381,
        "Question_last_edit_time":1668824461275,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW-r2Vv8-RC-qtY34c3WobQ\/how-to-access-set-up-a-model-registry",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"based on aws docs\/examples (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-registry-version.html), one can create\/register model that is generated by your training pipeline. first we need to create a model package group ( sample code below). is model registry already set up by default in our account or do we have to create it as well? can you have more than one model registry in an account.  I don't see , in the code below, where we are passing the model registry info. I am assuming , we simply create multiple training jobs and save the models in its own model group. is there a sample code\/repo, where there are multiple training steps that calls same model registry with a different group name ( i can use it as a sample). also, once the model is registered, how do i set up such that, a model can be accepted or declined, before promoting. can we set this up via code , please point me to any examples.\n\n```\nimport time\ngroup_name = \"mygroup\"\ninput_dict = { \"ModelPackageGroupName\" : group_name}\n\nresponse =  client.create_model_package_group(**input_dict)\nprint('result :  {}'.format(response['ModelPackageGroupArn']))\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Question_created_time":1668430718144,
        "Question_last_edit_time":1668778694209,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"Hello everyone, I was trying to execute the example mentioned in the docs - [https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html]().\nI was able to successfully run this example but as soon as I changed the target_device  to `jetson_tx2`, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\n```\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n```\nAccording to the troubleshoot docs [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html](), this seems to be an issue of **model_fn**() function.\nThe inference script used by this example is mentioned here [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py]() , which itself doesn't contain any model_fn() definition but it still worked for target device `ml_c5`.\nSo could anyone please help me with the following questions:\n1. What changes does SageMaker Neo do to the model depending on `target_device` type? Since it seems the same model is loaded in a different way for different target device.\n2. Is there any way to determine how the model is expected to load for a certain target_device type so that I could define the **model_fn**() function myself in the same inference script mentioned above?\n3. At-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for `jetson_tx2` device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1668766092280,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1668766092280,
        "Answer_comment_count":1.0,
        "Answer_body":"As you mentioned, you changed the Neo compiling target from `ml_c5` to `jetson_tx2`, the compiled model will require runtime from `jetson_tx2`. If you kept other code unchanged, the model will be deployed to a `ml.c5.9xlarge` EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment. \n\nIf you compile the model with `jeston_tx2` as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AWS Sagemaker model endpoint data capture",
        "Question_created_time":1668105022579,
        "Question_last_edit_time":1668530106278,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJfA77iAoT2-EkVff7sc7LA\/aws-sagemaker-model-endpoint-data-capture",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":116,
        "Question_answer_count":1,
        "Question_body":"I have deployed a sagemaker endpoint, with data capture enabled with a 100% sampling rate. But after running it multiple times, it is not storing all the inputs to the model,\n\nFor instance, after running 3 times, it saves only one time.\n\n![Enter image description here](\/media\/postImages\/original\/IMFtjw2ggMRtCWUb0H88bdyg)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth label - a word at the end of the sentence getting split into 2 parts",
        "Question_created_time":1668086625248,
        "Question_last_edit_time":1668434676659,
        "Question_link":"https:\/\/repost.aws\/questions\/QU50Od2mJZTjyEcGuGds1-qQ\/ground-truth-label-a-word-at-the-end-of-the-sentence-getting-split-into-2-parts",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_body":"We are annotating the pdf for extrating NER, The targetted entity word at the end of the sentence getting split into 2 parts. First part stay at the end of the first line and second part coming in second line. While annotating the tool doesn't allow the dragging to next line. \n\nAre there any work arounds available ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth Job Validation post completion",
        "Question_created_time":1668077364552,
        "Question_last_edit_time":1671878287287,
        "Question_link":"https:\/\/repost.aws\/questions\/QURI4l_IFoR9SZyc-6wNPqgQ\/ground-truth-job-validation-post-completion",
        "Question_score_count":2,
        "Question_favorite_count":0,
        "Question_comment_count":3,
        "Question_view_count":54,
        "Question_answer_count":1,
        "Question_body":"We annotated 10 pdf files in Ground Truth, how do I validate the annotations done by the team ? \nDo we have any metrics ?  Is there a way to get summary of daily task completion details ? \n\nSome example data that I want to get is Number of documents labelled everyday, the count of documents annotated by each person every day.\n\nThis metric is very important to track the progress and make a follow up.\n\nCan Ground truth experts provide some insights in this ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth Label Jobs - Same document repeating in the labeling job",
        "Question_created_time":1668073221576,
        "Question_last_edit_time":1668573716120,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu20cb3GSTPKBsnJsRDjbYg\/ground-truth-label-jobs-same-document-repeating-in-the-labeling-job",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":55,
        "Question_answer_count":0,
        "Question_body":"We want to extract the entities from pdf documents. We are exploring labeling the existing document using Safemaker GroundTruth. The documents are keep repeating for some reasons which we are not able to identify the reason ? When it repeats some pages coming as read only and other pages will us to annotate. When it repeats the previously read only pages are annotable .  \n\nCan some expert from this help us to understand why it is repeating ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Experiments Deletion Help Needed",
        "Question_created_time":1668037196721,
        "Question_last_edit_time":1668612841943,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sagemaker-experiments-deletion-help-needed",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":94,
        "Question_answer_count":1,
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1668070837516,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1668129579400,
        "Answer_comment_count":1.0,
        "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the [second method](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html#experiments-cleanup-boto3). Create a file (Menu File -> New -> Python File). Rename it as  `cleanup_experiments.py`(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command `python cleanup_experiments.py`",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AsyncInferenceConfig takes different parameters",
        "Question_created_time":1668012887011,
        "Question_last_edit_time":1668515096979,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUrJbuMmWTni5RT-e-GCYbA\/asyncinferenceconfig-takes-different-parameters",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":66,
        "Question_answer_count":1,
        "Question_body":"I would like to deploy an async endpoint in SageMaker. However when trying to deploy it I get the following error: \nParamValidationError: Parameter validation failed:\nUnknown parameter in input: \"AsyncInferenceConfig\"\n\nThis is the code I tried for deploying the endpoint\n```\nfrom sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n\nasync_config = AsyncInferenceConfig(\n    output_path=\"s3:\/\/poembucketus\/async_inference\/output\",\n    max_concurrent_invocations_per_instance=4,\n)\n\nasync_predictor = huggingface_estimator.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    async_inference_config=async_config,\n)\n```\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Access async endpoint created in console from notebook instance",
        "Question_created_time":1668007391842,
        "Question_last_edit_time":1668514889219,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLeSd5B8RHS7RD7KQIl5TQ\/access-async-endpoint-created-in-console-from-notebook-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_body":"In SageMaker, I've created an async endpoint from a model. How does one access this endpoint from a notebook instance so that it can be used to make predictions? \n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Question_created_time":1667961704440,
        "Question_last_edit_time":1668335812740,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sagemaker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":103,
        "Question_answer_count":1,
        "Question_body":"I'm following [this guide](https:\/\/aws.amazon.com\/blogs\/machine-learning\/migrate-your-work-to-amazon-sagemaker-notebook-instance-with-amazon-linux-2\/) for transitioning to Amazon Linux 2 provided by AWS \n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \n\"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \n`\/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory`\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found [here](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/migrate-ebs-data-backup\/on-start.sh)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1667972357492,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1668093487361,
        "Answer_comment_count":1.0,
        "Answer_body":"The extra `^M` symbol (i.e. Ctrl-M) stopped the whole scrip from being interpreted properly. \n\nThis issue is normally seen in scripts prepared in MSDOS\/Windows based system but used in Linux system due to difference of line endings.\n\nIn Unix based OS, lines end with `\\n` but MSDOS\/Win based system ends with `\\r\\n`\n\nIn Linux based system, you could show your prepared scripts by running\n```\ncat -e some-script.sh \n``` \nThe results would be something similar to\n```\n#!\/bin\/bash^M$\n... ...^M$\n```\n`$` is normal Unix end-of-line symbol. Windows uses an extra one `^M` and this symbol is not recognized by Unix system. That's why, in SageMaker Notebook Lifecycle Configuration, which is running Linux, your script was interpreted as `\/bin\/bash^M`\n\n\nTo mitigate the issue, please convert the scripts to Unix based ending and update life cycle configuration. To achieve this, you could use `Notepad++` in Windows. You can go to the `Edit` menu, select the `EOL Conversion` submenu, and from the options that come up select `UNIX\/OSX Format`. The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings. \n\nAlternatively, you could put the script in a Linux environment, e.g. EC2 instance with Amazon Linux 2 and install `dos2unix` via `sudo yum install dos2unix`. After installation, you could convert your files via\n```\ndos2unix -n file.sh  output.sh \n```\nAfter the conversion, please update LCC with the new scripts. You could verify that `^M` has been removed via \n```\ncat -e your_script.sh\n```\nThe output will print all special characters directly without hiding.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Batch Transformation Mismatch -",
        "Question_created_time":1667925703950,
        "Question_last_edit_time":1668402104437,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJo7fF1tiRQiljqizFaC-jg\/batch-transformation-mismatch",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":198,
        "Question_answer_count":2,
        "Question_body":"I performing batch transformation in a pipeline and joining it with the ground truth labels to create a ModelQuality monitoring step as per the guide here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/model-monitor-clarify-pipelines\/sagemaker-pipeline-model-monitor-clarify-steps.ipynb\n\nOverview:\n* Utilizing Sklearn pipeline tools for data handling and prediction (i.e. the sklearn pipeline module)\n* Using my own inference script (based on the build your own example provided by AWS)\n* Reading in csv file of data, which successfully loads and predicts labels (output ex: ['an01' 'hxn2' 'sv4' ... 'ngn' 'ssv' 'ssv'])\n* Transformation step is unsuccessful and provides following message:\n\n`2022-11-08T16:22:59.531:[sagemaker logs]: sagemaker-us-east-1-766029086407\/TEST-monitor-steps\/cipm6ea9musp\/projectKW-V3-Tune-TrainRegister-process\/output\/test\/test.csv: Fail to join data: mismatched line count between the input and the output`\n\nI am having trouble understanding if the configurations of *TransformInput* provided to the step are wrong or if there is an error with the inference script itself.  \n\nBelow is the code for the transformer step:\n\n```\ntransformer = Transformer(\n    model_name=step_create_model.properties.ModelName,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    accept=\"text\/csv\",\n    assemble_with=\"Line\",\n    output_path=f\"s3:\/\/{bucket}\/Transform\",\n    sagemaker_session=sagemaker_session\n)\n\nstep_transform = TransformStep(\n    name=\"TransformStep\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n        join_source=\"Input\",\n        content_type=\"text\/csv\",\n        split_type=\"Line\",\n    ),\n)\n```\n\nBelow is the code for the output_fn in the inference script:\n\n```\ndef output_fn(prediction, accept):\n    if accept == \"application\/json\":\n        instances = []\n        for row in prediction.tolist():\n            instances.append({\"features\": row})\n\n        json_output = {\"instances\": instances}\n\n        return worker.Response(json.dumps(json_output), mimetype=accept)\n    elif accept == 'application\/x-npy':\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    elif accept == 'text\/csv':\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    else:\n        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n```\n\n\nThanks for the help!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Groundtruth Named entity recognition Labeling Job Failure",
        "Question_created_time":1667860132622,
        "Question_last_edit_time":1668342407689,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwmj-aPT9StmbAjEsfAp0dA\/sagemaker-groundtruth-named-entity-recognition-labeling-job-failure",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":143,
        "Question_answer_count":1,
        "Question_body":"Hi!\nI'm unable to create a SageMaker Groundtruth Labeling Job for NER.\n\nSteps:\n\nCreated a new bucket.\n\nCreated folder for documents.\n\nCreated folder for outputs.\n\nAdded private work team\n\nCreated labeling job - Created new Role for Labeling job.\n\nHave tried  multiple times, always receiving the same error:\n\n```\n{\n    \"labeling-job-name\": \"Labelling-job-18\",\n    \"event-name\": \"PRE_HUMAN_LAMBDA_FAILED\",\n    \"event-log-message\": \"ERROR: Pre-human task Lambda failed for line 1.ClientError: The label categories file located at s3:\/\/<my-bucket>\/20221107\/output\/Labelling-job-18\/annotation-tool\/data.json can't be accessed. Make sure it exists in the us-east-1 region, that the role arn:aws:iam::<acct-id>:role\/service-role\/AmazonSageMaker-ExecutionRole-20221028T185408 has read permissions to it and try your request again.\"\n}\n```\n\nHow can I fix this? Is the role created in the labelling job missing some kind of lambda policy?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to Resolve \"ERROR execute(301) Failed to execute model:\"",
        "Question_created_time":1667853055854,
        "Question_last_edit_time":1668444077835,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwmKbCBpXSym2Vh_4Z0cW0g\/how-to-resolve-error-execute-301-failed-to-execute-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":0,
        "Question_body":"We have two applications working on the same AWS Panorama Appliance and processing different video streams.\nUnfortunately, we are catching the following error.\n\n```\n2022-10-09 21:25:32.360 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.359 ERROR execute(301) Failed to execute model:\nTVMError: \n'\"---------------------------------------------------------------\"\nAn error occurred during the execution of TVM.\nFor more information, please see: https:\/\/tvm.apache.org\/docs\/errors.html\n'\"---------------------------------------------------------------\n  Check failed: (context->execute(batch_size\n\"Stack trace:\n  File \"\/home\/nvidia\/neo-ai-dlr\/3rdparty\/tvm\/src\/runtime\/contrib\/tensorrt\/tensorrt_runtime.cc\", line 177\n  [bt] (0) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x381358) [0x7f81e66358]\n  [bt] (1) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x88) [0x7f81bb64a0]\n  [bt] (2) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::contrib::TensorRTRuntime::Run()+0x12b8) [0x7f81e243b0]\n  [bt] (3) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::json::JSONRuntimeBase::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x5c) [0x7f81e1bfc4]\n  [bt] (4) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0dc4) [0x7f81ea5dc4]\n  [bt] (5) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0e4c) [0x7f81ea5e4c]\n  [bt] (6) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(dlr::TVMModel::Run()+0xc0) [0x7f81c258e0]\n  [bt] (7) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(RunDLRModel+0x1c) [0x7f81bea304]\n  [bt] (8) \/usr\/lib\/libAwsOmniInferLib.so(awsomniinfer::CNeoModel::SNeoModel::execute()+0x3c) [0x7f887db978]\"\n2022-10-09 21:25:32.437 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.437 ERROR setData(279) Failed to set model input 'data':\n```\n\nThe error isn't persistent. It may happen once in 2-3 weeks, and I need to know which place to investigate.\nThe application logs are in the attachment. I am trying to avoid this issue.\n\nHowever, I would appreciate it if somebody knew how to cook this properly.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"List all running Sagemaker or other instances in AWS CLI",
        "Question_created_time":1667851269800,
        "Question_last_edit_time":1668613041274,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV7j_pRiGSBC4WFBAPGuY_g\/list-all-running-sagemaker-or-other-instances-in-aws-cli",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":679,
        "Question_answer_count":2,
        "Question_body":"Recently with another office account, I have seen hidden instances were running and creating unwanted cost in sagemaker, is there any way to check all the running processes\/instances using AWS CLI. I got many answers which suggest to go to Billing page, but that is not my objective. I want to immediately find out which instances are running so that I can stop them immediately.\n\nAre there any best practices for this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Question_created_time":1667756083758,
        "Question_last_edit_time":1668480117645,
        "Question_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\n![Enter image description here](\/media\/postImages\/original\/IMg3hralubRIO8ITzuV3La8Q)\n\nAny suggestion to fix this ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1667785787445,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1667797891161,
        "Answer_comment_count":1.0,
        "Answer_body":"You can try detaching the LCC script using the CLI. You can use the [CloudShell](https:\/\/aws.amazon.com\/cloudshell\/) from console, since your console role is able to perform updates on the domain. \n\nUse the [update-domain](https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/update-domain.html) CLI call, and provide an empty configuration for the default user settings, something like- \n```\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'\n```",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Tips for managing several ML project with similar framework",
        "Question_created_time":1667407640163,
        "Question_last_edit_time":1668255826964,
        "Question_link":"https:\/\/repost.aws\/questions\/QUt1SmhJXaTri_0TMwsy3wcg\/tips-for-managing-several-ml-project-with-similar-framework",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":37,
        "Question_answer_count":1,
        "Question_body":"Hi, \n\nI'm working on an end-to-end ml project which, for the moment, goes from training (it takes already processed train\/val\/test data from an S3 bucket) to deploy, passing through hyperparameter tunning. This project has been developed on SageMaker Studio and in the beginning, I decided to keep track on the project with a github's repository.\n\nSo, my work has certain degree of maturity: I was able to successfully train, tune, deploy and infer over a dataset. But the troubles came when I try to replicate this work for another dataset (a new project with a similar framework).\n\nLet's say that I had a client \"A\" for which I developed this project and now I have a new client \"B\" with similar requirements than client A. I'm looking for the best way of \"copy & paste\" the project considering the following:\n \na) I would like to keep working on the repository. The idea here is the repo been like a project's template ir order to clone the repository, make some few corrections (changing model's name, working bucket, etc) and then execute tuning, evaluation and deploy.\nb) There's a lot of changes and improvements that I should make in the future. So, I'd like those changes been reflected on both projects.\n\nIf anyone could give me some tips, guidelines, share his experience with something like this I would be very grateful.\n\nRegards! :D",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Experiment tracking with Sagemaker Pipelines",
        "Question_created_time":1667379630510,
        "Question_last_edit_time":1668613034502,
        "Question_link":"https:\/\/repost.aws\/questions\/QUttCrJyfVQYyAE-AO0vg-EA\/experiment-tracking-with-sagemaker-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":59,
        "Question_answer_count":1,
        "Question_body":"Is it possible to track only TrainingSteps in a Sagemaker Pipeline that contains multiple Processing & other steps? I don't really see big benefit of creating Trial Components for Processing Jobs or Model Repacking jobs into the experiments as they just overflow the UI.\n\nBasically could the pipeline_experiment_config parameter be used for defining which steps of the Pipeline should be tracked or should I disable automatic experiment creation and just try to create a manual experiment tracker during the Training Job.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"problem in installing library in sagemaker studio notebook",
        "Question_created_time":1667296098553,
        "Question_last_edit_time":1667974066687,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtopbA5VQSqaAI0BGv5mGsA\/problem-in-installing-library-in-sagemaker-studio-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_body":"I have a problem while installing librosa\nthis error message comes:\n\n```\nOSError: cannot load library 'libsndfile.so': libsndfile.so: cannot open shared object file: No such file or directory\n```\n\nI tried many solutions and didn't work, please help",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Damage detection on mobile",
        "Question_created_time":1667293813732,
        "Question_last_edit_time":1667858126359,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjpP7mZ07RSmhLNC0lXA3YQ\/damage-detection-on-mobile",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_body":"Hi,\nI am new to ML\/AI, Can someone help me create model for detection damage on mobile i.e. screen damage, back panel damaged etc, How sagemaker can help",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SQL Server driver issue on notebook instance running on AWS SageMaker",
        "Question_created_time":1667244851129,
        "Question_last_edit_time":1668624314543,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeC8hq97nSzKHHmMKbCPxww\/sql-server-driver-issue-on-notebook-instance-running-on-aws-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":627,
        "Question_answer_count":1,
        "Question_body":"I am using aws sagemaker notebook instance to run jupyter notebook. I have a` MS SQL Server DB 2019` db that I am trying to connect to from the notebook. Notebook instance is running on Amazon Linux 2, Jupyter Lab 1 platform.\n\n\n\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime\n    import json\n    import os\n    \n    \n    # sql database\n    import pyodbc\n    connection = pyodbc.connect(\n                                  'Driver={SQL Server};'\n                                  'Server=sname;'\n                                  'Database=dbname;'\n                                  'Trusted_Connection=True;'\n                               )\n    \n    cursor = connection.cursor()\n\n\nI get an error, likely because the driver is not installed on the instance. \n\n    Error                                     Traceback (most recent call last)\n    \/tmp\/ipykernel_20407\/3026941781.py in <cell line: 12>()\n         10 # sql database\n         11 import pyodbc\n    ---> 12 connection = pyodbc.connect(\n         13                               'Driver={SQL Server};'\n         14                               'Server=sname;'\n    \n    Error: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)\")\n\n\nhow do I install the driver on sagemaker instance and resolve this issue?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What classifier is used in SageMakers BlazingText algorithm?",
        "Question_created_time":1666879393628,
        "Question_last_edit_time":1668193555958,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhUSE7oIGTt2ZUt8bQS-SdQ\/what-classifier-is-used-in-sagemakers-blazingtext-algorithm",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":91,
        "Question_answer_count":1,
        "Question_body":"I would like to use the BlazingText algorithm in SageMaker for text classification: As I understand it we first represent the text with a word2vec algorithm to get the word embeddings, and then use these embeddings as input to a classifier. But is it known what classifier is being used and is it possible to modify it? \n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to delete Pipeline, Trial and Experiment in Sagemaker",
        "Question_created_time":1666851304944,
        "Question_last_edit_time":1668617969778,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfeScyHhkScmH7_SndJaQng\/how-to-delete-pipeline-trial-and-experiment-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_body":"I already deleted Domain, Users, Models, Endpoints, Endpoint configurations, Notebook instances, S3 Bucket instance and log groups but left with Pipeline, Trial and Experiment. Can anyone please help me with steps to followed to delete Pipeline, Trial and Experiment.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deleting SageMaker training jobs",
        "Question_created_time":1666792726682,
        "Question_last_edit_time":1668617667169,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwhvF6pP-TR-fTMXIcDfCMg\/deleting-sagemaker-training-jobs",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":806,
        "Question_answer_count":2,
        "Question_body":"Today we can't delete SageMaker training jobs.\n\nAs noted in AWS [docs](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html):  *\"Training jobs and logs cannot be deleted and are retained indefinitely.\"*, and confirmed in this Github issue: [amazon-sagemaker-examples#633](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/633)\n\nIs it planned to add this functionality ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker batch transform not loading CSV correctly",
        "Question_created_time":1666732676105,
        "Question_last_edit_time":1667925891667,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEci4LOFuRdeRj40V-koySw\/sagemaker-batch-transform-not-loading-csv-correctly",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":147,
        "Question_answer_count":1,
        "Question_body":"I am running a batch transform job that us uploading data from a CSV. The CSV is formatted as such \n```\n\"joe annes rifle accesories discount\"\n\"cute puppies for sale\"\n\"Two dudes talk about sports\"\n\"Smith & Wesson M&P 500 review\"\n\"Glock vs 1911 handgun\"\n```\nMy code for creating the batch transform is below\n\n```\nelec_model = PyTorchModel(model_data='s3:\/\/some_path\/binary-models\/tar_models\/14_10_2022__19_54_23_arms_ammunition.tar.gz',\n                         role=role,\n                         entry_point='torchserve_.py',\n                         source_dir='source_dir',\n                          framework_version='1.12.0',\n                          py_version='py38')\n```\n\n\n```\nnl_detector = elec_model.transformer(\n                     instance_count = 1,\n                     instance_type = 'ml.g4dn.xlarge', strategy=\"MultiRecord\", assemble_with=\"Line\", output_path = \"s3:\/\/some_path\/trash_output\")\n```\n\n\n```\nnl_detector.transform(\"s3:\/\/brand-safety-training-data\/trash\", content_type=\"text\/csv\", split_type=\"Line\")\n```\n When I run this code instead of the batch job taking the CSV and breaking up the examples with every space, which is what \n```\nsplit_type=\"Line\" \n```\nis telling the algorithm to do, but instead it just ingests all of the sentences in the above CSV, and outputs 1 probability. Also, if I do the same thing with the same code, but switch \n```\nstrategy=\"MultiRecord\"\n```\nto \n\n```\nstrategy=\"SingleRecord\"\n```\nso the one code block would look like this\n```\nnl_detector = elec_model.transformer(\n                     instance_count = 1,\n                     instance_type = 'ml.g4dn.xlarge', strategy=\"SingleRecord\", assemble_with=\"Line\", output_path = \"s3:\/\/some_path\/trash_output\")\n```\n\nThe algorithm works correctly, and performs inference on all of the above sentences in the CSV correctly. Any reason why this is happening?\n\nEDIT 1:\nWhen I print the input payload it looks like this\n\n\n```\n\"joe annes rifle accesories discount\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n\n\"cute puppies for sale\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n\n\"Two dudes talk about sports\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\n```\n\n\nWhere each sentence is a inference example, and is separated by this statement\n\n\n```\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\n```\nSo it seems like sagemaker is separating the inference examples. But when I try and pass these sentences into a huggingface tokenizer, the tokenizer tokenizes them like they are one inference example, when they should be 3 distinct inference examples.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to add autoscaling policy to an sagemaker endpoint via terraform?",
        "Question_created_time":1666661848109,
        "Question_last_edit_time":1668522840190,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvlNBp88AT7GtMkUbWIIZCw\/how-to-add-autoscaling-policy-to-an-sagemaker-endpoint-via-terraform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":55,
        "Question_answer_count":1,
        "Question_body":"based on the documentation here, https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb, an autoscaling policy could be added to an sagemaker async endpoint . I want to set this via terraform and have this so far (sample below) , but i'm not how to set Dimensions attribute in  TargetTrackingScalingPolicyConfiguration in terraform?\n\n```\n  TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0, SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n```\n\n```\nresource \"aws_appautoscaling_target\" \"sagemaker_target\" {\n  max_capacity       =  3\n  min_capacity       = 1\n  resource_id        = \"myendpoint\"\n  scalable_dimension = \"sagemaker:variant:DesiredInstanceCount\"\n  service_namespace  = \"sagemaker\"\n}\n\nresource \"aws_appautoscaling_policy\" \"sagemaker_policy\" {\n  name               = \"somepolicy\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.sagemaker_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.sagemaker_target.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.sagemaker_target.service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    customized_metric_specification{\n      metric_name = \"ApproximateBacklogSizePerInstance\"\n      namespace =  \"AWS\/SageMaker\"\n      Dimensions = ....?????\n      statistic =  \"Average\"\n    }\n    target_value       = 3\n    scale_in_cooldown  =300\n    scale_out_cooldown = 600\n  }\n}\n\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to add\/update autoscaling policy for an async endpoint in sagemaker?",
        "Question_created_time":1666657149491,
        "Question_last_edit_time":1668524647740,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj2WySokdQBaLNX601V6lbg\/how-to-add-update-autoscaling-policy-for-an-async-endpoint-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_body":"I have an async sagemaker endpoint, with an auto scaling policy (sample code below) . \n Everytime I update the model.tar.gz file , i delete the old endpoint and create a new one, with the same name and same setting. do i have to delete and re-create the autoscaling as well? \n\n```\nclient = boto3.client(\"application-autoscaling\") \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=  \"endpoint\/myendpoint\/variant\/test\"\n    ...\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to deploy lifecicle configuration to Sagemaker Studio via Cloudformation",
        "Question_created_time":1666655893373,
        "Question_last_edit_time":1667928934112,
        "Question_link":"https:\/\/repost.aws\/questions\/QUb1jvjl80RCClOggimxYaTQ\/how-to-deploy-lifecicle-configuration-to-sagemaker-studio-via-cloudformation",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":50,
        "Question_answer_count":0,
        "Question_body":"Hello, \n\nI need to deploy a new lifecicle configuration to Sagemaker Studio. I was searching in the Cloudformation Documentation and found nothing but this service.\n\nhttps:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-notebookinstancelifecycleconfig.html\n\nAs the name of the service says, it was deployed as a Lifecicle config for a Notebook Instance.\n\n![Sagemaker Lifecicle Config](\/media\/postImages\/original\/IMaxCL9jFuTSCgRQI-DXapSQ)\n\nIs there any way to deploy a Lifecicle configuration for Sagemaker studio via Cloudformation ?\n\nThanks.\nAnderson",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to pass credentials in  Glue Notebooks - Interactive Session using Magic Commands to override the 1 hour temporary token expiration",
        "Question_created_time":1666635956843,
        "Question_last_edit_time":1668198196990,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0wxONrToTd2vvUzwsmt3cw\/how-to-pass-credentials-in-glue-notebooks-interactive-session-using-magic-commands-to-override-the-1-hour-temporary-token-expiration",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"Hi, \nIf I start the notebook via the console the token\/credentials expire after one hour,\nGives the following error \"Exception encountered while retrieving session: An error occurred (ExpiredTokenException) when calling the GetSession operation: The security token included in the request is expired \" . \n\nI am guessing this is happening since its using temporary credentials by default. How does one pass the credentials using the magic commands such that credentials do not expire or workaround?  \n\nI can run notebooks locally using the profile in local .aws folder, but can't use TAGS for the sessions to account for costs.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"HELP!!!! Amazon SageMaker not writing best optimal route based on Genetic Algorithm to 2nd Output Dynamo Database(am stucked here&incurring dollar charges with no progress -Error Screenshot available)",
        "Question_created_time":1666611225424,
        "Question_last_edit_time":1668617718146,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDsciSjamQ5WZR_FHC1u0vQ\/help-amazon-sagemaker-not-writing-best-optimal-route-based-on-genetic-algorithm-to-2nd-output-dynamo-database-am-stucked-here-incurring-dollar-charges-with-no-progress-error-screenshot-available",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":43,
        "Question_answer_count":0,
        "Question_body":"My Challenges is this: I used CloudFormation template to deploy 2 Dynamo DB (Input and Output) and 1 IAM role to use AWS-managed Lamba Function for Genetic Algorithm, so Amazon SageMaker (using Jupyter Notebook on AWS) is meant to write the locations (X and Y coordinates into the input Dynamo DB **(Successful)**, **while the Docker file to Docker Image to Docker Container is also to be run by Amazon SageMaker to write the best Optimal Route based on Genetic Algorithm (Mutation, genomes and generation transfer mode of operation) to the 2nd Dynamo DB (Unsuccessful) and this is where I am stucked, have read a lot of materials and research a lot and even reached out to some Amazon AWS Community but they could not resolve it, Please will be glad if repost.aws can help please (Error Screenshot Available)**",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What is Validation set adjustment while the system is auto labeling",
        "Question_created_time":1666580175899,
        "Question_last_edit_time":1668011166450,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtppe1uHQTK-zEzkN4xDn4Q\/what-is-validation-set-adjustment-while-the-system-is-auto-labeling",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_body":"How is it works, and what is the purpose? \n\nINFO:samurai_science_object_detection.cli:Running validation set adjustment.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"I want to use the model monitor in the sagemaker pipeline to get the results of statistics.json and constraints.json",
        "Question_created_time":1666395801466,
        "Question_last_edit_time":1668617954264,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW13HHW8LT8O80p_ozuHk1A\/i-want-to-use-the-model-monitor-in-the-sagemaker-pipeline-to-get-the-results-of-statistics-json-and-constraints-json",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":239,
        "Question_answer_count":1,
        "Question_body":"### Summary.\n- I would like to know what the CSV format is for getting aws sagemaker model quality metrics (regression metrics).\n\n### Contents\n- The model monitor in sagemaker is used to evaluate models (own models, batch processing).\nAt this time, I believe that using the AWS mechanism, I can get statistics.json and constraints.json as a result.  \nIn the model monitor mechanism, there are three parameters for \"problem_type\" in the following script.\nI think there are three parameters for \"regression\", \"BinaryClassification\" and \"MulticlassClassification\", but I don't know what CSV format to send the content in for \"regression\", and I'm not sure if AWS will handle it appropriately. Question.  \nIncidentally, in the case of classification, 'BinaryClassification' and 'MulticlassClassification' results from the creation of appropriate CSVs.\n  \n`The CSV in the case of classification is created below.`  \n\n```py\nprobability,\tprediction,\tlabel\n[0.99,0.88], 0, 0\n[0.34,0.77], 1, 0\n\u30fb\u30fb\u30fb\n```\n\n- I don't know what kind of CSV content I should send to get the 'Regression' results.  \nHere are some excerpts from pipeline.py and the CSV content we are currently testing.  \n  \n`Here are some excerpts from 'pipeline.py`  \n\n```py\n    model_quality_check_config = ModelQualityCheckConfig(\n        baseline_dataset=step_transform.properties.TransformOutput.S3OutputPath,\n        dataset_format=DatasetFormat.csv(header=False),\n        output_s3_uri= f\"sagemaker\/monitor\/\",\n        problem_type='Regression',\n        inference_attribute= \"_c1\",\n        ground_truth_attribute= \"_c0\"\n    )\n\n    model_quality_check_step = QualityCheckStep(\n        name=\"ModelQualityCheckStep\",\n        depends_on = [\"lambdacsvtransform\"],\n        skip_check=skip_check_model_quality,\n        register_new_baseline=register_new_baseline_model_quality,\n        quality_check_config=model_quality_check_config,\n        check_job_config=check_job_config,\n        supplied_baseline_statistics=supplied_baseline_statistics_model_quality,\n        supplied_baseline_constraints=supplied_baseline_constraints_model_quality,\n        model_package_group_name=\"group\"\n    )\n```\n\n`CSV\uff08Regression\uff09`\n\n```py\n\"_c0\", \"_c0\"\n0.88, 0.99\n0.66, 0.87\n\u30fb\u30fb\u30fb\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How does sagemaker creates a model?",
        "Question_created_time":1666386495979,
        "Question_last_edit_time":1667926175018,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTsasf-AKRriQkvxUPXGlxg\/how-does-sagemaker-creates-a-model",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":63,
        "Question_answer_count":2,
        "Question_body":"I am creating sagemaker resources such as model, endpoint configuration and real time endpoint via cloudformation ( sample below) . in the template below, we provide the s3 bucket URI for the model artifact in the ModelDataUrl argument. if we update the model artifact , or delete a older version and upload a new model.tar file in the same bucket. will that work, instead of creating a new model resource everytime there is a new version of model.tar file ? when making a inference, I understand , sagemaker downloads the model.tar file in the container specified , unpack the model.tar file and call the binary file for inference ,so it doesn't matter if we update the model.tar file , right? sagemaker will simply download whatever tar file is present in the s3 URI and works with that. \n\n\n```\nSageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties: \n      Containers: \n        -\n          Image: !Ref ImageURI\n          ModelDataUrl: 's3:\/\/some-bucket\/model.tar'\n          Mode: SingleModel\n      ExecutionRoleArn: !Ref RoleArn\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Question_created_time":1666360814590,
        "Question_last_edit_time":1668547705502,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sagemaker-endpoint-to-zero",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":491,
        "Question_answer_count":1,
        "Question_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). \nFor this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used. \n\nAre there any costs associated with having an endpoint with 0 instances? \n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1666371029704,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1666796875366,
        "Answer_comment_count":1.0,
        "Answer_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\n1. Try using [SageMaker Serverless Inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\n2. You can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\n3. There is also an option of [SageMaker asynchronous inference](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html) but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"greengrass ml component",
        "Question_created_time":1666267168133,
        "Question_last_edit_time":1668619480534,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4YAAy55MQQChcPc0JhyWWA\/greengrass-ml-component",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":75,
        "Question_answer_count":0,
        "Question_body":"Hello\nI am studying Greengrass machine learning components.\nI have a question.\nI'm going to distribute Greengrass custom machine learning components to Raspberry Pi now.\nThe artifacts associated with the recipe in this component contain inference codes uploaded to S3.\nAnd this inference code has a function of determining who is by photographing the user's face in real time.\nI'm curious.\nIf the components are distributed well in Raspberry Pi, will the inference code continue to run? Or, do I have to deploy components every time I try to run an inference?\nThank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"fail to run guide demo of DeepRacer SageMaker",
        "Question_created_time":1666058970622,
        "Question_last_edit_time":1667926230750,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWyDmIyDjQu2l8OJyKBFHNw\/fail-to-run-guide-demo-of-deepracer-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":59,
        "Question_answer_count":1,
        "Question_body":"I try to follow the developer guide of training model of DeepRacer on SageMaker, but fail in \"traning_job = sagemaker.create_training_job\" part:\n![Enter image description here](\/media\/postImages\/original\/IMYEBRPbwkRgar8_li7jJvCw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Amazon SageMaker - Training Job \/ Data Wrangler",
        "Question_created_time":1665982852548,
        "Question_last_edit_time":1668618190215,
        "Question_link":"https:\/\/repost.aws\/questions\/QUg1kenySVSc2Eq9mW7Mwy1A\/amazon-sagemaker-training-job-data-wrangler",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":124,
        "Question_answer_count":2,
        "Question_body":"I have a customer who is interested in testing Amazon Sagemaker and would like to consult the following questions: \n\nQ1. While submitting training job in Amazon Sagemaker, if there is insufficient capacity occurred, would there be any auto-retry mechanism? How to set up?\nQ2. Is the underlying SQL \/ MySQL infrastructure in Data Wrangler from AWS serverless DB backend?\nQ3. What is the backend database to support Sagemaker \/ Sagemkaer Data Wrangler ? \n\nUse case: Vision ML - Object detection (self-built algorithm)\nFramework: Tensorflow 2.4.4",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to use custom functions for a model in a Sagemaker pipeline?",
        "Question_created_time":1665782248751,
        "Question_last_edit_time":1668618202657,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjEHGriWRBCoNnc8luz-6Q\/how-to-use-custom-functions-for-a-model-in-a-sagemaker-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":59,
        "Question_answer_count":0,
        "Question_body":"If I want to use a custom function transformer in preprocessing, how do I ensure that it's detected at both pipeline building and deployment?\n\nI'm building a sklearn pipeline, and in preprocessing I use a custom FunctionTransformer. In Sagemaker, I am able to train, evaluate, and register the model, but get the below error when I try to deploy it:\n`AttributeError: Can't get attribute 'truncate_function' on <module '__main__' from '\/miniconda3\/bin\/gunicorn'>`\n\nI've tried putting the functions into a helper.py file, and including it as a dependency during training, but then get the following error when evaluating in a ProcessingStep: \"ModuleNotFoundError: No module named 'helper'.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Sagemaker Notebook Not working ,how can i solve the issue?",
        "Question_created_time":1665720123752,
        "Question_last_edit_time":1668192436290,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhUbNweWRR0-oROL-jwKIRQ\/aws-sagemaker-notebook-not-working-how-can-i-solve-the-issue",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":802,
        "Question_answer_count":1,
        "Question_body":"The code failed because of a fatal error: Error sending http request and maximum retry encountered..\n\nSome things to try: a) Make sure Spark has enough available resources for Jupyter to create a Spark context. b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly. c) Restart the kernel.\n\n\nNote: There are no such logs on cloudwatch to figureout the issue.\n\n![Enter image description here](\/media\/postImages\/original\/IM_v974F51S_mfRwd5a5PcVQ)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data Wrangler: Data Flow: Export to S3 using Jupyter Notebook",
        "Question_created_time":1665658565074,
        "Question_last_edit_time":1668395130909,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskLbkfD8RW2YzO9Vr7XggA\/data-wrangler-data-flow-export-to-s3-using-jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":75,
        "Question_answer_count":1,
        "Question_body":"When I have created a Data flow using data wrangler and when I am trying to Export to export to S3 using Jupyter Notebook and when I am running the notebook, I am getting the below mentioned error every time when creating a processing job:\n \nError:  An error occurred (ResourceLimitExceeded) when calling the CreateProcessingJob operation: The account-level service limit 'ml.m5.4xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.\n \nPlease provide me with the solution for this. I have increased the service quota for running apps and Notebook instance also but then also same issue arises.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"aws textract human review flow, failed to load image",
        "Question_created_time":1665415509779,
        "Question_last_edit_time":1668416940440,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo48ev4bTTvO-GjsezfAmuQ\/aws-textract-human-review-flow-failed-to-load-image",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":131,
        "Question_answer_count":2,
        "Question_body":"Hi,\n\nI`m using aws textract to extract key-value pairs from an pdf. Because sometimes the accucary is low i use augmented AI (human review worflows) to involve a human worker.\nThat works fine with png files, but when I use pdf files (which textract supports), I get an \"Failed to load image\".\nHow do I get around this? I tried using a custom template, but can't find a way to insert the file type.\n\nBest regards,\n\nPaul",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1665462672324,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1665472433348,
        "Answer_comment_count":1.0,
        "Answer_body":"The underlying challenge here is that, while modern browsers can natively render PDFs, they require different embedding methods for PDFs vs images. To my knowledge there's no built-in [SageMaker Crowd HTML Element](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html) that's capable of handling both types interchangeably - and your experience with the pre-built UI seems to confirm this.\n\n### Displaying PDFs in A2I\/SMGT\n\n[This simple sample](https:\/\/github.com\/aws-samples\/amazon-a2i-sample-task-uis\/blob\/master\/text\/document-classification.liquid.html) suggests to use an `<iframe type=\"application\/pdf\">` to display PDFs via the browser's native renderer. You could try this approach... but as of ~March 2022, I found support was patchy because some browsers' default security policies didn't like loading a cross-origin iframe with interactive content.\n\nIf relying on the browser native renderer won't work for your users, you can use the open-source [PDF.js](https:\/\/mozilla.github.io\/pdf.js\/) renderer instead. [Here is a more complex sample template](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/0ac365f8e174d44337a5881a7dca6d61b94016cd\/notebooks\/review\/fields-validation-legacy.liquid.html#L24) that does that. PDF.js is powerful, but can be pretty tricky to get started with from my experience... Note that the basic process in this sample is:\n\n- Tag the `<script>`s and stylesheets for PDF.js in from a CDN\n- Include a [PDF viewer structure](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/0ac365f8e174d44337a5881a7dca6d61b94016cd\/notebooks\/review\/fields-validation-legacy.liquid.html#L54) in your HTML\n- Pass your A2I object URL in [through JavaScript](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/0ac365f8e174d44337a5881a7dca6d61b94016cd\/notebooks\/review\/fields-validation-legacy.liquid.html#L290) and set up your viewer there - including any interactivity you need\n- (The second inline script tag there you can probably ignore: It's specific to what that data that template collects)\n\n### Scaling template complexity\n\nAlthough the situation has improved a lot in recent years, writing direct-to-browser inline JavaScript in HTML can be tricky due to browser diversity and developer tooling limitations. If you want to build more advanced, interactive task templates, you might want to explore using front-end frameworks like React\/Angular\/Vue within A2I\/Ground Truth.\n\nThe above-mentioned PDF.js template is actually a legacy that's since been replaced by [this VueJS app](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/tree\/main\/notebooks\/review) in the sample that uses it. In that case, the switch was made because we wanted to customize the PDF viewer (rendering detection boxes over the document), and the complexity of the app justified setting up a proper toolchain. You can find discussion there about using frameworks in general and VueJS in particular with A2I, and could use the app as a starting point for building your own complex template in advance. Note if I was re-building that from scratch, I'd probably use much **less** liquid templating, and implement more within the JS framework itself as discussed [here](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/tree\/main\/notebooks\/review#can-vs-should-gaps-between-vue-and-web-components).\n\nYou can see the complex template being built\/deployed from (SageMaker) Python notebook [here](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/blob\/main\/notebooks\/3.%20Human%20Review.ipynb), and a screenshot of it in action [here](https:\/\/raw.githubusercontent.com\/aws-samples\/amazon-textract-transformer-pipeline\/main\/img\/human-review-sample.png). This end-to-end sample is discussed further in an [AWS ML blog post](https:\/\/aws.amazon.com\/blogs\/machine-learning\/bring-structure-to-diverse-documents-with-amazon-textract-and-transformer-based-models-on-amazon-sagemaker\/).\n\n### Handling mixed PDF\/Image content\n\nIf you need your template to handle both PDFs and images, this will add extra complexity. Could your JavaScript infer from the object URL (filename) which category the input object falls into, and dynamically set up either an `<img>` tag or a PDF viewer? Could you fetch the object from JS and check the `Content-Type` response header? Might it be simpler to add the file type as an input to your A2I loop, and pass it in that way? (e.g. using conditional liquid template to either render an `<img>` or not?)\n\nDepending on what points in the flow you know the file type, there are multiple different ways you might tackle this. Ultimately though, you'll probably be switching between either generating an img or a PDF viewer: Whether those HTML elements are created by static Liquid templating or by dynamic JS.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Can Sagemaker RStudio mount an NFS share?",
        "Question_created_time":1665409392840,
        "Question_last_edit_time":1668630597562,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqjjos-zZQhCU4vxTSFSDLQ\/can-sagemaker-rstudio-mount-an-nfs-share",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":128,
        "Question_answer_count":1,
        "Question_body":"Our existing Python code currently accesses data files residing on a file system. It navigates through folders to reach individual files and reads them.\n \nWe would now like to migrate the code to notebooks hosted in Sagemaker Studio. The data files will be hosted in S3 buckets.\nHowever, we do not want to modify the code to use S3 API to access files. Instead, we would like to continue using a filesystem view of the files. What is the best way to make the S3 objects visible to the Sagemaker Studio as a filesystem?\nPlease note that we would like to mount in Sagemaker studio and not on Sagemaker notebook.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Annotation of PDF document using Bounding Box",
        "Question_created_time":1665169244843,
        "Question_last_edit_time":1667925994599,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK0c3Zp1jQ8SFMF3Mfp7SmA\/annotation-of-pdf-document-using-bounding-box",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":176,
        "Question_answer_count":1,
        "Question_body":"Hi! Is it possible to develop some kind of template for SageMaker that uses bounding boxes to annotate a PDF document? I was looking for something similar to the crowd-bounding-box HTML tag, but instead of only capturing portions of the PDF's image, I also wanted to extract data regarding the text located inside that portion, along with it's coordinates and stuff like that, so I could give context to my annotation.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Amazon CloudWatch Metric ModelSetupTime not available",
        "Question_created_time":1665007033890,
        "Question_last_edit_time":1668573913560,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu2KU0TNvTu-2V6sOylZJsg\/amazon-cloudwatch-metric-modelsetuptime-not-available",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"The ModelSetupTime metric  is not available for monitoring the serverless endpoint(https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) in Amazon CloudWatch.Able to see only the below mentioned metrics\nInvocations\nInvocation5XXErrors\nInvocation4XXErrors\nModelLatency\nOverheadLatency\n\nIs there any role\/configuration changes I have to do to view the ModelSetupTime ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Submit EMR serverless jobs from SageMaker notebook",
        "Question_created_time":1664942921237,
        "Question_last_edit_time":1668530991003,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaaYMax_hTIi0UNn-7wGPIQ\/submit-emr-serverless-jobs-from-sagemaker-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":490,
        "Question_answer_count":2,
        "Question_body":"I am processing a dataset and need to submit a job to EMR serverless for the dataset to be processed in a distributed way.  I have created an application in EMR studio. I would like to submit jobs to that application. I found the command to submit jobs \n```\naws emr-serverless start-job-run \\\n    --application-id application-id \\\n    --execution-role-arn job-role-arn \\\n    --job-driver '{\n        \"sparkSubmit\": {\n            \"entryPoint\": \"s3:\/\/us-east-1.elasticmapreduce\/emr-containers\/samples\/wordcount\/scripts\/wordcount.py\",\n            \"entryPointArguments\": [\"s3:\/\/DOC-EXAMPLE-BUCKET-OUTPUT\/wordcount_output\"],\n            \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\"\n        }\n    }'\n```\nBut how can I run the above command from a Python 3 Data Science Notebook in SageMaker studio. Basically what endpoint do I need to use to submit the job.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using HuggingFace in Sagemaker Studio as part of a project",
        "Question_created_time":1664885918543,
        "Question_last_edit_time":1668487418864,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Ahf5zWZRZq63k1TOQ_48w\/using-huggingface-in-sagemaker-studio-as-part-of-a-project",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":214,
        "Question_answer_count":2,
        "Question_body":"TLDR: if we are trying to use a HuggingFaceProcessor\/Estimator in a Sagemaker Studio project, what are the requirements for the `train.py` file in terms of how it refers to the assembled training data, and where it should store the results of the operations it performs( e.g. compiled model, datae etc.)\n\n-----------------------\nFULL DETAILS\n------------------------\n\nSo our high level goal is to be able to deploy some kind of non-XGB model from a sagemaker studio project, given that the templates provided are all XGB.  As outlined in [an earlier question](https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-module-not-found-error-no-module-named-transformers) we'd started with TensorFlow, but since our TensorFlow model wraps a HuggingFace model we thought let's try something even simpler, just a HuggingFace model using the HuggingFaceProcessor.\n\nSo following docs on [HuggingFaceProcessor](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html) and a [HuggingFace Estimator](https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/02_getting_started_tensorflow\/sagemaker-notebook.ipynb) example we started to adjust the abalone (project template) pipeline.py to look like this (full code can be provided on request):\n\n```\n    # processing step for feature engineering\n    hf_processor = HuggingFaceProcessor(\n        role=role, \n        instance_count=processing_instance_count,\n        instance_type=processing_instance_type,\n        transformers_version='4.4.2',\n        pytorch_version='1.6.0', \n        base_job_name=f\"{base_job_prefix}\/frameworkprocessor-hf\",\n        sagemaker_session=pipeline_session,\n    )\n    step_args = hf_processor.run(\n        outputs=[\n            ProcessingOutput(output_name=\"train\", source=\"\/opt\/ml\/processing\/train\"),\n            ProcessingOutput(output_name=\"validation\", source=\"\/opt\/ml\/processing\/validation\"),\n            ProcessingOutput(output_name=\"test\", source=\"\/opt\/ml\/processing\/test\"),\n        ],\n        code=os.path.join(BASE_DIR, \"preprocess.py\"),\n        arguments=[\"--input-data\", input_data],\n    )\n    step_process = ProcessingStep(\n        name=\"PreprocessTopicData\",\n        step_args=step_args,\n    )\n\n    # training step for generating model artifacts\n    model_path = f\"s3:\/\/{sagemaker_session.default_bucket()}\/{base_job_prefix}\/TopicTrain\"\n\n    hf_train = HuggingFace(entry_point='train.py',\n                            source_dir=BASE_DIR,\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type=processing_instance_type,\n                            instance_count=processing_instance_count,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py36',\n                            role=role,\n                          )\n  \n    hf_train.set_hyperparameters(\n       epochs=3,\n       train_batch_size=16,\n       learning_rate=1.0e-5,\n       model_name='distilbert-base-uncased',\n    )\n                           \n    step_args = hf_train.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text\/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text\/csv\",\n            ),\n        },\n    )\n```\n\nFinding that pushing to master doesn't provide any feedback on issues arising from pipeline.py, we realised that trying to get the pipeline from a notebook was a better way of debugging these sorts of changes, assuming one remembered to restart the kernel each time to ensure changes to the pipeline.py file was available to the notebook.  \n\nSo using the following code in the notebook we worked through a series of issues trying to bash the code into shape such that it would compile:\n\n```\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n```\n\nWe needed to change the default processing and training instance types to avoid a \"cpu\" unsupported issue:\n\n```\n    processing_instance_type=\"ml.p3.xlarge\",\n    training_instance_type=\"ml.p3.xlarge\",\n```\n\nand add a train.py script:\n\n```\nfrom transformers import AutoTokenizer\nfrom transformers import TFAutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=18)\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    DistilBertTokenizerFast,\n    TFDistilBertForSequenceClassification,\n)\nDATA_COLUMN = 'text'\nLABEL_COLUMN = 'label'\nMAX_SEQUENCE_LENGTH = 512\nLEARNING_RATE = 5e-5\nBATCH_SIZE = 16\nNUM_EPOCHS = 3\nNUM_LABELS = 15\n\nif __name__ == \"__main__\":\n\n    # --------------------------------------------------------------------------------\n    # Tokenizer\n    # --------------------------------------------------------------------------------\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n    def tokenize(sentences, max_length=MAX_SEQUENCE_LENGTH, padding='max_length'):\n        \"\"\"Tokenize using the Huggingface tokenizer\n        Args:\n            sentences: String or list of string to tokenize\n            padding: Padding method ['do_not_pad'|'longest'|'max_length']\n        \"\"\"\n        return tokenizer(\n            sentences,\n            truncation=True,\n            padding=padding,\n            max_length=max_length,\n            return_tensors=\"tf\"\n        )\n    # --------------------------------------------------------------------------------\n    # Load data\n    # --------------------------------------------------------------------------------\n    from keras.utils import to_categorical\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder_Y_1 = LabelEncoder()\n    yy = labelencoder_Y_1.fit_transform(train_data[LABEL_COLUMN].tolist())\n    yy = to_categorical(yy)\n    print(len(yy))\n    print(yy.shape)\n    train_dat, validation_dat, train_label, validation_label = train_test_split(\n        train_data[DATA_COLUMN].tolist(),\n        yy,\n        test_size=0.2,\n        shuffle=True\n    )\n    # --------------------------------------------------------------------------------\n    # Prepare TF dataset\n    # --------------------------------------------------------------------------------\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(tokenize(train_dat)),  # Convert BatchEncoding instance to dictionary\n        train_label\n    )).shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n    validation_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(tokenize(validation_dat)),\n        validation_label\n    )).batch(BATCH_SIZE).prefetch(1)\n    # --------------------------------------------------------------------------------\n    # training\n    # --------------------------------------------------------------------------------\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n        'distilbert-base-uncased',\n        num_labels=NUM_LABELS\n    )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    )\n```\n\nHowever we are now stuck on this error when trying to get the pipeline from a notebook.\n\n```TypeError                                 Traceback (most recent call last)\n<ipython-input-3-be38b3dda75f> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n     11 # !conda list\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    228                     \"validation\"\n    229                 ].S3Output.S3Uri,\n--> 230                 content_type=\"text\/csv\",\n    231             ),\n    232         },\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline_context.py in wrapper(*args, **kwargs)\n    246             return self_instance.sagemaker_session.context\n    247 \n--> 248         return run_func(*args, **kwargs)\n    249 \n    250     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n   1059         self._prepare_for_training(job_name=job_name)\n   1060 \n-> 1061         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)\n   1062         self.jobs.append(self.latest_training_job)\n   1063         if wait:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in start_new(cls, estimator, inputs, experiment_config)\n   1956         train_args = cls._get_train_args(estimator, inputs, experiment_config)\n   1957 \n-> 1958         estimator.sagemaker_session.train(**train_args)\n   1959 \n   1960         return cls(estimator.sagemaker_session, estimator._current_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in train(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\n    611             self.sagemaker_client.create_training_job(**request)\n    612 \n--> 613         self._intercept_create_request(train_request, submit, self.train.__name__)\n    614 \n    615     def _get_train_request(  # noqa: C901\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _intercept_create_request(self, request, create, func_name)\n   4303             func_name (str): the name of the function needed intercepting\n   4304         \"\"\"\n-> 4305         return create(request)\n   4306 \n   4307 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in submit(request)\n    608         def submit(request):\n    609             LOGGER.info(\"Creating training-job with name: %s\", job_name)\n--> 610             LOGGER.debug(\"train request: %s\", json.dumps(request, indent=4))\n    611             self.sagemaker_client.create_training_job(**request)\n    612 \n\n\/opt\/conda\/lib\/python3.7\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237         separators=separators, default=default, sort_keys=sort_keys,\n--> 238         **kw).encode(obj)\n    239 \n    240 \n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in encode(self, o)\n    199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n--> 201             chunks = list(chunks)\n    202         return ''.join(chunks)\n    203 \n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode(o, _current_indent_level)\n    429             yield from _iterencode_list(o, _current_indent_level)\n    430         elif isinstance(o, dict):\n--> 431             yield from _iterencode_dict(o, _current_indent_level)\n    432         else:\n    433             if markers is not None:\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode_dict(dct, _current_indent_level)\n    403                 else:\n    404                     chunks = _iterencode(value, _current_indent_level)\n--> 405                 yield from chunks\n    406         if newline_indent is not None:\n    407             _current_indent_level -= 1\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode_dict(dct, _current_indent_level)\n    403                 else:\n    404                     chunks = _iterencode(value, _current_indent_level)\n--> 405                 yield from chunks\n    406         if newline_indent is not None:\n    407             _current_indent_level -= 1\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode(o, _current_indent_level)\n    436                     raise ValueError(\"Circular reference detected\")\n    437                 markers[markerid] = o\n--> 438             o = _default(o)\n    439             yield from _iterencode(o, _current_indent_level)\n    440             if markers is not None:\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in default(self, o)\n    177 \n    178         \"\"\"\n--> 179         raise TypeError(f'Object of type {o.__class__.__name__} '\n    180                         f'is not JSON serializable')\n    181 \n\nTypeError: Object of type ParameterInteger is not JSON serializable\n```\n\nWhich is telling us that some aspect of the training job (?) is not serializable, and it's not clear how to debug further.\n\nWhat would be enormously helpful is project templates for sagemaker studio showing the use of all the Processors, e.g. HuggingFace, TensorFlow and so on, but failing that we'd be most grateful is anyone could point us to documentation on what the requirements are for the `train.py` file that we need to specifiy for the HuggingFace Estimator.\n\nmany thanks in advance",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_created_time":1664542013756,
        "Question_last_edit_time":1668616022040,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sagemaker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":275,
        "Question_answer_count":2,
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\n```\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n```\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n```\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1664616480773,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1664616495636,
        "Answer_comment_count":0.0,
        "Answer_body":"Should use the Sagemaker image \n```\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n```\ninstead of ec2\n\n```\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n```",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Jumpstart in Sagemaker does not work",
        "Question_created_time":1664537965056,
        "Question_last_edit_time":1668126045107,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOGpNbAjoRASDDw1pQ5F6Fg\/jumpstart-in-sagemaker-does-not-work",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":60,
        "Question_answer_count":0,
        "Question_body":"Hello,\n\nI am new to SageMaker, I created a domain, and I have an execution role, although when I use Sagemaker studio I cannot see all functionalities, such as the resources button, jumpstart button, etc. I have enabled 'SageMaker Projects and JumpStart '. \nI am also attaching a photo of what my UI in the studio looks like. ![Enter image description here](\/media\/postImages\/original\/IM_5MgPbAMQSyuVBghpqn3gQ)\nAny help would be appreciated. \nThanks in advance",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Question_created_time":1664396753855,
        "Question_last_edit_time":1668528521211,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-modulenotfounderror-no-module-named-transformers",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":133,
        "Question_answer_count":1,
        "Question_body":"So as mentioned in my [other recent post](https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name), I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\n```\nModuleNotFoundError: No module named 'transformers'\n```\n\nNOTE: I am importing 'transformers' in `preprocess.py` not in `pipeline.py`\n\nNow I have 'transformers' listed in various places as a dependency including:\n\n* `setup.py` - `required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]`\n* `pipelines.egg-info\/requires.txt` - `transformers` (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\n------------\n------------\n------------\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see [previous post](https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name) for more details)  I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\n```\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n```\n\nthe `describe()` method gives this output:\n\n```\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n```\nWaiting for the execution I get:\n\n```\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n```\nWhich I assume is corresponding to the failure I see in the logs:\n\n![buildl pipeline error message on preprocessing step](\/media\/postImages\/original\/IMMpF6LeI6TgWxp20TnPZbUw)\n\nI did also run `python setup.py build` to ensure my build directory was up to date ... here's the terminal output of that command:\n\n```\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n```\nIt seems like the dependencies are being written to `pipelines.egg-info\/requires.txt` but are these not being picked up by the pipeline?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1664451755510,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1664531543837,
        "Answer_comment_count":14.0,
        "Answer_body":"Hi! There are two places where you need to install the dependencies \/ requirements:\n\n1. In your environment where you execute `pipeline.start()` \u2013 can be Amazon SageMaker Studio, your local machine or CI\/CD pipeline executor, e. g. AWS CodeBuild. These dependencies are installed in `setup.py`.\n2. Inside the SageMaker processing and training jobs as well as in inference endpoints. This is usually done via `requirements.txt` file that you submit as part of your `source_dir`.\n\nIn your example, I recommend you to use the `TensorFlowProcessor`. The way how to install dependencies into it is described [in the corresponding section of the documentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-tensorflow.html), in particular:\n> SageMaker Processing installs the dependencies in `requirements.txt` in the container for you.\n\nSame applies to your model training and to the `TensorFlow` estimator. See the section [Use third-party libraries](https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#use-third-party-libraries) in the TensorFlow documentation of the SageMaker Python SDK, in particular:\n> If there are other packages you want to use with your script, you can use a `requirements.txt` to install other dependencies at runtime. \n\nHope it helps!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"adjusting sagemaker xgboost project to tensorflow (or even just different folder name)",
        "Question_created_time":1664391665708,
        "Question_last_edit_time":1668489176743,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":172,
        "Question_answer_count":1,
        "Question_body":"I have sagemaker xgboost project template \"build, train, deploy\" working, but I'd like to modify if to use tensorflow instead of xgboost.  First up I was just trying to change the `abalone` folder to `topic` to reflect the data we are working with.\n\nI was experimenting with trying to change the `topic\/pipeline.py` file like so\n\n\n```\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"tensorflow\",\n        region=region,\n        version=\"1.0-1\",\n        py_version=\"py3\",\n        instance_type=training_instance_type,\n    )\n```\n\ni.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when I run the following from a notebook:\n\n\n```\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n```\n\nI get the following error\n\n```\nValueError                                Traceback (most recent call last)\n<ipython-input-5-6343f00c3471> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    188         version=\"1.0-1\",\n    189         py_version=\"py3\",\n--> 190         instance_type=training_instance_type,\n    191     )\n    192     tf_train = Estimator(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    197                 logger.warning(warning_msg_template, arg_name, func_name, type(value))\n    198                 kwargs[arg_name] = value.default_value\n--> 199         return func(*args, **kwargs)\n    200 \n    201     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    152             if inference_tool == \"neuron\":\n    153                 _framework = f\"{framework}-{inference_tool}\"\n--> 154         config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type)\n    155 \n    156     original_version = version\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type)\n    277         image_scope = available_scopes[0]\n    278 \n--> 279     _validate_arg(image_scope, available_scopes, \"image scope\")\n    280     return config if \"scope\" in config else config[image_scope]\n    281 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    443             \"Unsupported {arg_name}: {arg}. You may need to upgrade your SDK version \"\n    444             \"(pip install -U sagemaker) for newer {arg_name}s. Supported {arg_name}(s): \"\n--> 445             \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options))\n    446         )\n    447 \n\nValueError: Unsupported image scope: None. You may need to upgrade your SDK version (pip install -U sagemaker) for newer image scopes. Supported image scope(s): eia, inference, training.\n```\n\nI was skeptical that the upgrade suggested by the error message would fix this, but gave it a try:\n\n```\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npipelines 0.0.1 requires sagemaker==2.93.0, but you have sagemaker 2.110.0 which is incompatible.\n```\n\nSo that seems like I can't upgrade sagemaker without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries.\n\nBut so is it that the \"framework\" name should be different, e.g. \"tf\"?  Or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...?\n\nHowever I find that if I use the existing `abalone\/pipeline.py` file I can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook.\n\nI've searched all the files in the project to try and find any dependency on the `abalone` folder name, and the closest I came was in `codebuild-buildspec.yml` but that hasn't helped.\n\nHas anyone else successfully changed the folder name from `abalone` to something else, or am I stuck with `abalone` if I want to make progress?\n\nMany thanks in advance\n\np.s. is there a slack community for sagemaker studio anywhere?\n\np.p.s. I have tried changing all instances of the term \"Abalone\" to \"Topic\" within the `topic\/pipeline.py` file (matching case as appropriate) to no avail\n\np.p.p.s. I discovered that I can get an error free run of getting the pipeline from a unit test:\n\n\n```\nimport pytest\n\nfrom pipelines.topic.pipeline import *\n\nregion = 'eu-west-1'\nrole = 'arn:aws:iam::398371982844:role\/SageMakerExecutionRole'\ndefault_bucket = 'sagemaker-eu-west-1-398371982844'\nmodel_package_group_name = 'TopicModelPackageGroup-Example'\npipeline_name = 'TopicPipeline-Example'\n\ndef test_pipeline():\n    pipeline = get_pipeline(\n        region=region,\n        role=role,\n        default_bucket=default_bucket,\n        model_package_group_name=model_package_group_name,\n        pipeline_name=pipeline_name,\n    )\n```\nand strangely if I go to a different copy of the notebook, everything runs fine, there ... so I have two seemingly identical ipynb notebooks, and in one of them when I switch to trying to get a topic pipeline I get the above error, and in the other, I get no error at all, very strange\n\np.p.p.p.s.  I also notice that `conda list` returns very different results depending on whether I run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1664450661266,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1664450661266,
        "Answer_comment_count":4.0,
        "Answer_body":"Hi! I see two parts in your question:\n1. How to use Tensorflow in a SageMaker estimator to train and deploy a model\n2. How to adapt a SageMaker MLOps template to your data and code\n\nTensorflow estimator is slightly different from XGBoost estimator, and the easiest way to work with it is not by using `sagemaker.image_uris.retrieve(framework=\"tensorflow\",...)`, but to use [`sagemaker.tensorflow.TensorFlow` estimator instead](https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html).\n\nThese are the two examples, which will be useful for you:\n* [Train an MNIST model with TensorFlow](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/frameworks\/tensorflow\/get_started_mnist_train.ipynb)\n* [Deploy a Trained TensorFlow V2 Model](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/frameworks\/tensorflow\/get_started_mnist_deploy.ipynb)\n\nAs for updating the MLOps template, I recommend you to go through the comprehensive [self-service lab on SageMaker Pipelines](https:\/\/catalog.us-east-1.prod.workshops.aws\/workshops\/63069e26-921c-4ce1-9cc7-dd882ff62575\/en-US\/lab6).\n\nIt shows you how to update the source directory from `abalone` to `customer_churn`. In your case it will be the `topic`.\n\nP. S. As for a Slack channel, to my best knowledge, this re:Post forum now is the best place to ask any questions on Amazon SageMaker, including SageMaker Studio.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How do you find out input features required for a sagemaker model to do batch inference?",
        "Question_created_time":1664371572064,
        "Question_last_edit_time":1668618265881,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6-AhM27-RxqsbVO9bOhyMg\/how-do-you-find-out-input-features-required-for-a-sagemaker-model-to-do-batch-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":100,
        "Question_answer_count":1,
        "Question_body":"Say for example I have a trained sagemaker model artefact or model in a model registry. Now I need to  prepare the input dataset to be used in the model for batch inference. How do I know what input features the model is expecting so that I can prepare the data accordingly ? Is there a way to find out from the model artefact?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"aws.amazon.commachine-learning\/pricing link is broken",
        "Question_created_time":1664336034992,
        "Question_last_edit_time":1667926680808,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvybuFyXJRnW4Q_LvXxbEaA\/aws-amazon-commachine-learning-pricing-link-is-broken",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":2,
        "Question_body":"https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/requesting-real-time-predictions.html article\n\nlink => http:\/\/aws.amazon.commachine-learning\/pricing\/",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Killing a hung Sagemaker Job",
        "Question_created_time":1664321471237,
        "Question_last_edit_time":1668372878899,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9DABK2CjTpKk4-kT_KVdsQ\/killing-a-hung-sagemaker-job",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":0,
        "Question_body":"After running 'stop_training_job', the job is not stopping. Happy to provide the full processing name and job arn if needed. Thanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to create a sagemaker serverless endpoint via cloudformation?",
        "Question_created_time":1664305906330,
        "Question_last_edit_time":1668438999972,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1p5lKUqcTl6s6e-Oh7mKYg\/how-to-create-a-sagemaker-serverless-endpoint-via-cloudformation",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":146,
        "Question_answer_count":1,
        "Question_body":"based on the documentation, i am creating a model and trying to create an sagemaker endpoint configuration for a serverless endpoint (sample below) , I added a ServerlessConfig attribute in my endpoint configuration resource below,  but i'm get an error = \"Property validation failure: [Encountered unsupported properties in {\/} [ServerlessConfig]]. any ideas? \n```\nSageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties: \n      Containers: \n        -\n          Image: !Ref ImageURI\n          ModelDataUrl: !Ref ModelData\n          ExecutionRoleArn: !Ref RoleArn\n\nSageMakerEndpointConfig:\n    Type: \"AWS::SageMaker::EndpointConfig\"\n    Properties:\n      ServerlessConfig: \n        -\n          MaxConcurrency: 5\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"can a sagemaker endpoint be made public?",
        "Question_created_time":1664239929738,
        "Question_last_edit_time":1667925789371,
        "Question_link":"https:\/\/repost.aws\/questions\/QU96EIw32SSxmx3plPtUUcYA\/can-a-sagemaker-endpoint-be-made-public",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":161,
        "Question_answer_count":2,
        "Question_body":"is there a way to make a sagemaker endpoint be accessible publicly ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
        "Question_created_time":1664215795645,
        "Question_last_edit_time":1668485459490,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEQiFVzOhR5q1XYhjlltO7w\/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":73,
        "Question_answer_count":1,
        "Question_body":"I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1666640051311,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1666640115530,
        "Answer_comment_count":0.0,
        "Answer_body":"No , Comprehend don't support hosting multiple models with the same endpoint right now.\nThanks for your suggestions . We will take them into consideration .",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker MultiDataModel deployment error during inference. ValueError: Exactly one .pth or .pt file is required for PyTorch models: []",
        "Question_created_time":1664208836417,
        "Question_last_edit_time":1668607553806,
        "Question_link":"https:\/\/repost.aws\/questions\/QUe7ia8vHWRmukMsV_i5SzpA\/sagemaker-multidatamodel-deployment-error-during-inference-valueerror-exactly-one-pth-or-pt-file-is-required-for-pytorch-models",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":627,
        "Question_answer_count":1,
        "Question_body":"Hello,\nI've been trying to deploy multiple PyTorch models on one endpoint on SageMaker from a SageMaker Notebook. First I tested deployment of single models on single endpoints, to check if everything works smoothly and it did. I would create a PyTorchModel first:\n\n```\nimport sagemaker\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nfrom sagemaker.multidatamodel import MultiDataModel\nfrom sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\nimport boto3\n\nrole = get_execution_role()\nsagemaker_session = sagemaker.Session()\n\npytorch_model = PyTorchModel(\n            entry_point='inference.py',\n            source_dir='code',\n            role=role,\n            model_data='s3:\/\/***\/model\/model.tar.gz',\n            framework_version='1.11.0',\n            py_version='py38',\n            name='***-model',\n            sagemaker_session=sagemaker_session\n        )\n```\nMultiDataModel inherits properties from Model classes, so I used the same PyTorch model that I used for single model deployment.\nThen I would define the MultiDataModel the following way:\n\n```\nmodels = MultiDataModel(name='***-multi-model',\n                       model_data_prefix='s3:\/\/***-sagemaker\/model\/',\n                       model=pytorch_model,\n                       sagemaker_session=sagemaker_session\n                       )\n```\nAll it should need is the prefix to the S3 bucket of the model artifacts saved as tar.gz files (the same files used for single model deployment), the previously defined PyTorch model, a name and a sagemaker_session.\n\nTo deploy it:\n```\nmodels.deploy(initial_instance_count =1,\n             instance_type='ml.m4.xlarge',\n             serializer=JSONSerializer(),\n             deserializer=JSONDeserializer(),\n             endpoint_name='***-multi-model-deployment',\n             )\n```\n\nThe deployment goes well, as there are no failures and the endpoint is InService by the end of this step.\nHowever the error occurs when I try to run inference on one of the models:\n\n```\nimport json\nbody = {\"url\":\"https:\/\/***image.jpg\"} #url to an image online\npayload = json.dumps(body)\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName = \"***-multi-model-deployment\",\n    ContentType  = \"application\/json\",\n    TargetModel  = \"\/model.tar.gz\",\n    Body         = payload)\n```\n\nThis prompts an error message: \n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers for model ec1cd509c40ca81ffc3fb09deb4599e2 version: 1.0\"\n}\n\". See https:\/\/***.console.aws.amazon.com\/cloudwatch\/home?region=***#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/***-multi-model-deployment in account ***** for more information.\n```\nThe Cloudwatch logs show this error in particular:\n\n```\n22-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 210, in <module>\n2022-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\n2022-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 181, in run_server\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 139, in handle_connection\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 104, in load_model\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_loader.py\", line 151, in load\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_pytorch_serving_container\/handler_service.py\", line 51, in initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/default_handler_service.py\", line 66, in initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/transformer.py\", line 162, in validate_and_initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._model = self._model_fn(model_dir)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_pytorch_serving_container\/default_pytorch_inference_handler.py\", line 73, in default_model_fn\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     raise ValueError(\n2022-09-26T15:51:40,496 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ValueError: Exactly one .pth or .pt file is required for PyTorch models: []\n```\n\nIt seems like it's having problems loading the model, saying only one .pth file is required, however in the invocation function i point to the exact model artifact present at that S3 bucket prefix. I'm having a hard time trying to fix this issue, so it would be very helpful if anyone had some suggestions!\n\nInstead of giving the MultiDataModel a model, I also tried providing it an ECR docker image with the same inference code, but I would get the same error during invocation of the endpoint.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Excessive Memory use when deploying PipelineModel using the pre-build Scikit container in Sagemaker",
        "Question_created_time":1663945337107,
        "Question_last_edit_time":1667925669640,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2hdNVBreSFyX1iSPDRPMXw\/excessive-memory-use-when-deploying-pipelinemodel-using-the-pre-build-scikit-container-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":40,
        "Question_answer_count":0,
        "Question_body":"I am using the pre-build Scikit container in Sagemaker to deploy an endpoint based on a model that contains a 59.4 MB model.tar.gz file. The following line was used to deploy the endpoint:\n\nsm_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name)\n\nHowever, the after the endpoint was created, it fails to allocate memory to works. These error messages and warnings keep showing in the logs:\n\n[Errno 12] Cannot allocate memory\n[WARNING] Worker with pid 242 was terminated due to signal 9\n\nAs far as I know, the xlarge instance has 16 GB of memory. The endpoint memory usage is at 60% while it still fails to allocate memory to workers. May I ask if anyone has any insight on why this is happening and how to solve this issue without using an instance that has more memory?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to pass values for a \"shap_baseline\" if we have categorical values (string values) as features in classsagemaker.clarify.SHAPConfig method.",
        "Question_created_time":1663938111276,
        "Question_last_edit_time":1667926207272,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVdzi-7h5RAapCYJUehzuZw\/how-to-pass-values-for-a-shap-baseline-if-we-have-categorical-values-string-values-as-features-in-classsagemaker-clarify-shapconfig-method",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_body":"using [this documentation](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.clarify.SHAPConfig) i  passing a single row as to shap_baseline parameter to implement explainability monitoring , a similar implementation of what is done in [in this github repo implementation](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.ipynb). if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If i ignore the shap_baseline (as it is optional), the schedule is taking forever to run. Help of any kind is really appreciated.\n\nthanks for your time and effort :)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to create custom templates for model training and building in sagemaker studio?",
        "Question_created_time":1663805155971,
        "Question_last_edit_time":1668586237134,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-fW_jQSjSKiwACGXqFh1sA\/how-to-create-custom-templates-for-model-training-and-building-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_body":"I am going through documentation provided here , https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy and would like to create my own model building, training and deploying templates. can I download\/clone the sagemaker provided templates and start modifying to my own needs. I understand , we need to set up a aws catalog portfolio and products under it, to be able to use such templates. my question is which project do i need to clone and modify , say if i want to build my own training and model building template, which particular code base or code file do i need to change. I assume , the train.py file here -> https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy\/blob\/master\/08_projects\/modelbuild\/pipelines\/endtoendmlsm\/train\/train.py would be one i will customize. but once i change these files, how do i use them. how would i set up my custom template, create a product under my catalog? if yes, how to I link my custom train.py code to this new custom sagemaker project template. the documentation or samples only show how to use the prebuild templates under sagemaker project templates, but how do i get my own template pushed there ? can this be done via say cloudformation ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Glue notebook - Kernel fails with Glue version exception",
        "Question_created_time":1663701548390,
        "Question_last_edit_time":1668615887763,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQotxGT8SRwefmNxpd0AKOw\/aws-glue-notebook-kernel-fails-with-glue-version-exception",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":2,
        "Question_view_count":240,
        "Question_answer_count":3,
        "Question_body":"I am trying to run the notebooks locally,  I followed the instructions provided here (for Windows): https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/interactive-sessions.html \n\nI ran the pip3 install --upgrade jupyter boto3 aws-glue-sessions, which upgraded aws-glue-session to version 0.35\n\nbut the when starting the notebook the kernel fails to launch and throws the following error, \n```\n  File \"d:\\python38\\lib\\site-packages\\aws_glue_interactive_sessions_kernel\\glue_pyspark\\GlueKernel.py\", line 100, in __init__\n    self.set_glue_version(os_env_glue_version)\n  File \"d:\\python38\\lib\\site-packages\\aws_glue_interactive_sessions_kernel\\glue_pyspark\\GlueKernel.py\", line 443, in set_glue_version\n    raise Exception(f\"Valid Glue versions are {VALID_GLUE_VERSIONS}\")\nException: Valid Glue versions are {'2.0', '3.0'}\n```\nSetting the the glue_version = 2.0 in .aws\/config and environment variables, does not help either. any help on what could be causing this will be much appreciated!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"is it possible to create steps within the sagemaker pipeline via cloudformation?",
        "Question_created_time":1663606095949,
        "Question_last_edit_time":1668618307229,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3aphi0KgRNyk6AcB9c9Spg\/is-it-possible-to-create-steps-within-the-sagemaker-pipeline-via-cloudformation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":73,
        "Question_answer_count":1,
        "Question_body":"I am experimenting with sagemaker studio, while i was able to create the sagemaker studio domain and user profiles via cloudformation. I was wondering if, it was possible to create sagemaker projects and other resources like model registry group and steps for preprocessing , training ... via cloudformation? if is it possible , are there any samples, examples around this, if it is not supported or is not possible via cfn , may be help me on how can create link this to existing user and domain in studio  .",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I create a Dockerfile for BYOC training which allows me to pass in entry_point and source_dir arguments to SM Estimator?",
        "Question_created_time":1663592214344,
        "Question_last_edit_time":1667926452178,
        "Question_link":"https:\/\/repost.aws\/questions\/QUiVxzpWc9TMS0OkcqgqKPmQ\/how-do-i-create-a-dockerfile-for-byoc-training-which-allows-me-to-pass-in-entry-point-and-source-dir-arguments-to-sm-estimator",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_body":"I am following this tutorial: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/tensorflow_bring_your_own\/container\/Dockerfile\n\nby SM to create a docker image. According to this tutorial, the training scripts are copied into the docker container meaning that you have to rebuild the container if you make any changes.\n\nMy question is: How do I create a Dockerfile such that I don't have to copy my script when I am building the image? Instead, I want to pass in the entry_point and source_dir arguments to the SM Estimator which copies those files upon calling the .fit method.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is Redshift mixing up my data columns when creating a model?",
        "Question_created_time":1663314635540,
        "Question_last_edit_time":1667926615583,
        "Question_link":"https:\/\/repost.aws\/questions\/QUs7vw6wMDTUOLwMaZlqwzgQ\/is-redshift-mixing-up-my-data-columns-when-creating-a-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_body":"Hello,\n\nI'm using running:\n\n```\ncreate model predict_xxxxx\nfrom (select col1, col2, col3 from my_table)\ntarget col3\nfunction predict_xxx\niam_role 'arn:aws:iam::xxxxxxx:role\/RedshiftML'\nproblem_type regression\nobjective 'mse'\nsettings (\n    s3_bucket 'redshiftml-xxxxxxx',\n    s3_garbage_collect off,\n    max_runtime 1800\n);\n```\n\nWhich then generates input data files in CSV format in the S3 bucket I specified, but when I open up those files and look at them, all the columns in my `select` statement are present, but the column headers are mismatched with the data below them. I see `col1` data under the `col2` column and so on. I know the data is mixed up because the data types and numeric ranges are different for each column. I double-checked my table and the columns and data are matched correctly. Is Redshift\/Sagemaker then using that mismatched data to train the model? I have tried with only two column and it still gets mixed up. I've tried using a table instead of a select expression and the problem persists.\n\nAny insight is appreciated.\n\nThanks,\n- SV",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to export tresained models to ECR as container image",
        "Question_created_time":1663258467464,
        "Question_last_edit_time":1667926271895,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":72,
        "Question_answer_count":1,
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image  from ECR",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1663369533114,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1663369533114,
        "Answer_comment_count":1.0,
        "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). \nIf you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account. \n\nIf you are using a custom training image ([docs here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-training-container.html)), you can push this image to ECR and allow a [second account to pull the image](https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/secondary-account-access-ecr\/) and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use [SageMaker Notebook Instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html) instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Glue Interactive vs SageMaker Processing?",
        "Question_created_time":1663171688837,
        "Question_last_edit_time":1668623995118,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7J5WaZe3Qzi2giJaOmBFDQ\/glue-interactive-vs-sagemaker-processing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":638,
        "Question_answer_count":2,
        "Question_body":"Greetings! I'm a data scientist working in SageMaker notebooks. I'd appreciate an explanation about when should I use Glue Interactive and not SageMaker Processing jobs. \nTo my understanding, they are very similar and I can't differentiate them.\n\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1663248095278,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1663248095278,
        "Answer_comment_count":0.0,
        "Answer_body":"I would suggest that you use Sagemaker processing for the data cleansing and preparation. I have led projects where all the data cleansing, preparation and model build and testing have been done in Sagemaker and the data scientists love the tool.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Unable to compile model to Neuron: no error message, no output",
        "Question_created_time":1663166637969,
        "Question_last_edit_time":1668491030884,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_oVwSPQReCt96QyX4cz-g\/unable-to-compile-model-to-neuron-no-error-message-no-output",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":120,
        "Question_answer_count":2,
        "Question_body":"Hi. We are trying to convert all our in-house pytorch models to aws-neuron on inferentia. We successfully converted one, but the second model we tried did not compile. Unfortunately, compilation did not generate any error message nor log of any kind, so we are stuck.\nThe model is rather simple, but large, U-Net, with partial convolutions instead of regular ones, but otherwise no fancy operators.\nConversion of this model to torchscript is ok on the same instance.\nCould it be a memory problem ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1663233942678,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1663233942678,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi, in order to see more information about the error, you can enable debugging during tracing by passing 'verbose' to the tracing command like this:\n\n```\nimport torch\nimport torch.neuron\ntorch.neuron.trace(\n    model,\n    example_inputs=inp,\n    verbose=\"debug\",\n    compiler_workdir=\"logs\" # dir where debugging logs will be saved\n)\n```\nYou'll see the error messages in the console and they will also be saved to the \"logs\" dir.\n\nIt is always good to run the NeuronSDK analyzer first to make sure the model is: 1\/ torch.jit traceable; 2\/ supported by the compiler\n\n```\nimport torch\nimport torch.neuron\ntorch.neuron.analyze_model(model, example_inputs=inp)\n```\n\n\nYou can also see a sample that shows how to compile an U-net Pytorch (3rd party implementation) to Inf1 instances here:\nhttps:\/\/github.com\/samir-souza\/laboratory\/blob\/master\/05_Inferentia\/03_UnetPytorch\/03_UnetPytorch.ipynb\n\nRef: https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/neuron-guide\/neuron-frameworks\/pytorch-neuron\/api-compilation-python-api.html\n\nIf everything fails, try to look for something like this in the logs:\n```\nINFO:Neuron:Compile command returned: -11\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$647; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n```\nAnd paste here, please. With the \"Compile command returned:\" code it is possible to identify the error. You are suspecting that there is some issue related to memory, maybe Out of Memory. Normally when that is the case, you'll find the code: -9 in this part of the error.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"how to trigger sagemaker pipeline via code change in github?",
        "Question_created_time":1663125520507,
        "Question_last_edit_time":1668618293621,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaQEgFCVaTxCY_CINaOXSsw\/how-to-trigger-sagemaker-pipeline-via-code-change-in-github",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":94,
        "Question_answer_count":2,
        "Question_body":"based on the aws docs and sample code provided here, https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/6299535b80b44ef0b61b95c979b1511157965810\/sagemaker-pipelines\/tabular\/customizing_build_train_deploy_project\/modelbuild\/pipelines\/customer_churn\/pipeline.py, I can wrap different ML steps like training into a step , in a sagemaker pipeline code. after that, i can define sagemaker projects to create a CI\/CD pipeline to build and deploy models. the examples i saw builds\/deploy model in aws code commit\/pipeline. is it possible to move this part to github or gitlab for repository and then deploy from gitlab or other tools like jenkins. are there any examples or code samples to do this?? also, instead of working in sagemaker studio IDE, is it possible to set up code repo in github or gitlab and we use our own IDE to push changes to the pipeline or sagemkaer projects code to build\/deploy model? and somehow hook that into studio such that for example , we make a change to hyperparameter value in gitlab, that will trigger the pipeline in studio?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to use sagemaker-pyspark in batch inference",
        "Question_created_time":1663084288909,
        "Question_last_edit_time":1667925709142,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mfQyFCoTji_5XYrCAQEGQ\/how-to-use-sagemaker-pyspark-in-batch-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":130,
        "Question_answer_count":1,
        "Question_body":"am trying to execute the code below \n\n\n```\nENDPOINT_NAME = \"my-endpoint\"\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\nfrom pyspark.sql.types import StructType, StructField, MapType, StringType, IntegerType, ArrayType, FloatType\n\nattachedModel = SageMakerModel.fromEndpoint(\n    endpointName = ENDPOINT_NAME,\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName = \"col1\"\n    ),\n    responseRowDeserializer=ProtobufResponseRowDeserializer(schema=StructType([\n        StructField('prediction', MapType(StringType(), FloatType()))\n    ]))\n)\n\ndata=SageMakerModel.transform(attachedModel, df['col1'])\n```\nI keep getting the below error though\n\n\n```\npy4j.protocol.Py4JError: An error occurred while calling o58.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n        at py4j.Gateway.invoke(Gateway.java:274)\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\n        at java.lang.Thread.run(Thread.java:748)\n```\n\nany ideas ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ERROR while fitting RCF data : An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size.",
        "Question_created_time":1663020590535,
        "Question_last_edit_time":1668601411617,
        "Question_link":"https:\/\/repost.aws\/questions\/QUO0RgnpwvQOy3ays1fRLw4A\/error-while-fitting-rcf-data-an-error-occurred-entitytoolarge-when-calling-the-putobject-operation-your-proposed-upload-exceeds-the-maximum-allowed-size",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":145,
        "Question_answer_count":1,
        "Question_body":"I am fitting a RCF model to a dataset using **rcf.fit**(rcf.record_set(data[['Variable 1','Variable 2']].values.reshape(-1, 1))), but getting the below error : \n\nAn error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size.\n\nThe size of the ndarray is 239393964\n\nHow can I fix this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"neo compilation job failed on Yolov5\/v7 model",
        "Question_created_time":1662741786680,
        "Question_last_edit_time":1668416449980,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV00cs8DbQ6WVPa7J_LLXmw\/neo-compilation-job-failed-on-yolov5-v7-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":121,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI was trying to use SageMaker Neo compilation to convert a yolo model(trained with our custom data) to a coreml format, but got an error on input config: \n> ClientError: InputConfiguration: Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"mil\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed.\n\nI've tried both latest yolov7 model and yolov5 model, but get the same error. Seems Neo cannot recognize the Yolo model.\n\nBut when I tried to use the yolov4 model from this tutorial post: https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/, it works fine.\n\nAny idea if Neo compilation can work with Yolov7\/v5 model?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Yolo5 model deployment into SageMaker endpoint",
        "Question_created_time":1662693076387,
        "Question_last_edit_time":1668092312397,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhNfeNlTPQSOwfUg_rCnUpw\/yolo5-model-deployment-into-sagemaker-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":183,
        "Question_answer_count":1,
        "Question_body":"I have a trained yolo5 model which I deployed into Real-time end-point in SageMaker. I tried almost all gpus computes available but I could not get better than 2.6 sec for inference time. This is a light model and my target is to have < 1sec inference time. Could you please help me with any hints?\nI transformed the model from pytorch to TF format.\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Can do this? multimodel-endpoint + async inferce",
        "Question_created_time":1662629203617,
        "Question_last_edit_time":1668481467527,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFGfav6yyR6idYoGOkgH3sA\/sagemaker-can-do-this-multimodel-endpoint-async-inferce",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":398,
        "Question_answer_count":1,
        "Question_body":"Following this [document](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/multi_data_model.html), I try to async inference with Multi Model Endpoint.\n\nI try to set **kwargs  from Model().deploy() to  MultiDataModel().deploy(**kwargs) .\nFrom that document, MultiDataModel(**kwargs) pass to Model(**kwargs).\nI assume MultiDataModel().deploy(**kwargs) pass to Model().deploy(**kwargs).\n\nSo, I try to like that.\n\n```python\nfrom sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n\nasync_config = AsyncInferenceConfig(\n    output_path=f\"s3:\/\/{bucket}\/{prefix}\/output\",\n    max_concurrent_invocations_per_instance=4\n)\n\nmme = MultiDataModel(\n    name='MultiModel',\n    model_data_prefix=multi_model_s3uri,\n    model=model,  # passing our model\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.\"+ instance_type,\n    async_inference_config  = async_config\n)\n\n```\n\nBut, I realize that <class Predictor != class AsyncPredictor>.\nI want to get AsyncPredictor(not Predictor) from MultiModelEndpoint.deploy().",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker debugger built in rule CreateXgboostRule not generating report as expected",
        "Question_created_time":1662562294513,
        "Question_last_edit_time":1668544614614,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuZEDvbaeRfqOT_g2Sxlw7w\/sagemaker-debugger-built-in-rule-createxgboostrule-not-generating-report-as-expected",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":99,
        "Question_answer_count":1,
        "Question_body":"I'm currently working with a SageMaker hosted XGBoost model; I've added the built in rule \"CreateXgboostRule\" to generate a training report, however, only the ProfilerReport is generated in the S3 rule-output folder - the expected result based on the dev doc is for a CreateXGBoostRule folder as well within this same folder.\n\nThe code I'm using is based directly on the example provided in: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/debugger-training-xgboost-report.html\n\n\n```\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\nfrom sagemaker import image_uris\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nregion = boto3.Session().region_name\nxgboost_container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n\nestimator=Estimator(\n    role=sagemaker.get_execution_role()\n    image_uri=xgboost_container,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    \n    # Add the Debugger XGBoost report rule\n    rules=rules\n)\n\nestimator.fit(wait=False)\n```\nI've tried rewriting the estimator a number of ways, verified \"rules\" is receiving an array of objects, tried different versions of XGBoost within the region, but everything still results in the built in rule only creating the ProfilerReport with no CreateXGBoostRule directory under rule-output.\n\nAny ideas would be greatly appreciated! Thanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to serve a pretrained model from hugging face in sagemaker without custom script?",
        "Question_created_time":1662514340477,
        "Question_last_edit_time":1667925912414,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFjO6dWOKQW2RdY8XyeP0-A\/how-to-serve-a-pretrained-model-from-hugging-face-in-sagemaker-without-custom-script",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":55,
        "Question_answer_count":1,
        "Question_body":"I have been working with an example , where I write my own custom script ( sample below) , where i am overriding the predict_fn and other functions. I have tested my model without the custom script or inference.py. in the event when we don't provide our custom script, how is the model called? what does the default code for predict_fn look like, when I don't override it?\n\ninference.py\n```\n\nimport os\nimport json\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef model_fn(model_dir):\n   model_dir = '.\/pytorch_model.bin'\n\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n    \n    model_dict = {'model':model, 'tokenizer':tokenizer}\n    \n    return model_dict\n        \n\ndef predict_fn(input_data, model_dict):\n \n    input = input_data.pop('inputs')\n   \n    \n    tokenizer = model_dict['tokenizer']\n    model = model_dict['model']\n\n    input_ids = tokenizer(input, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n     ....\n    \n    \ndef input_fn(request_body, request_content_type):\n    return  json.loads(request_body)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does SageMaker's CreateModel API support model data from versioned S3 objects?",
        "Question_created_time":1662493921224,
        "Question_last_edit_time":1667926525314,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmyE-SweqTvazF0y_5OJtpA\/does-sagemaker-s-createmodel-api-support-model-data-from-versioned-s3-objects",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_body":"In the following AWS CLI invocation, I'm attempting to specify a `ModelDataUrl` which contains an embedded `versionId` to reference an object whose bucket has versioning enabled. Does SageMaker's CreateModel API support model data originating from a specific object version? If so, how does one specify the object version?\n\nAWS CLI Invocation:\n```\naws sagemaker create-model --model-name VersionedArtifacts \\\n  --execution-role-arn arn:aws:iam::<account-id>:role\/service-role\/<role-name> \\\n  --primary-container \"Image=<account-id>.dkr.ecr.us-east-1.amazonaws.com\/<repository>:<tag>,ModelDataUrl=https:\/\/s3.us-east-1.amazonaws.com\/<versioned-bucket>\/versioned-artifacts.tar.gz?versionId=<version-id>\n```\n\nResulting AWS CLI Error:\n\n```\nAn error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at https:\/\/s3.us-east-1.amazonaws.com\/<versioned-bucket>\/versioned-artifacts.tar.gz?versionId=<version-id>.\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is my Jupyter Notebook hung?",
        "Question_created_time":1662151013058,
        "Question_last_edit_time":1667926512076,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmBX_HRdkTjKnKzg_15etmw\/is-my-jupyter-notebook-hung",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":47,
        "Question_answer_count":0,
        "Question_body":"Is there anyway to tell if the current cell I am executing is hung? I am running an AutoGluon TabularPredictor().fit(). The cell it is executing is has the * indication that it is running, I checked the notebook running tab, and it says my notebook is running, but when I look at the AutoGluon models folder, it does not look like anything new has being written for hours.  \nAre there any other ways I can check to see if my model is still trying to fit?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to inference parameters to a huggingface model hosted in sagemaker?",
        "Question_created_time":1662145723356,
        "Question_last_edit_time":1667926388804,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeyT2jYhVQdOoGXrQxoZ4nw\/how-to-inference-parameters-to-a-huggingface-model-hosted-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":50,
        "Question_answer_count":0,
        "Question_body":"I created a model resource in sagemaker . the model is a tar file , downloaded from hugging face and fine tuned.  based on the documentation provided ( sample code below) . the code sample is passing HF_TASK inference parameter and i assume this is \nhugging face specific, but is it possible to pass other parameters like  padding or truncation and max_length ? such as \npadding : True\ntruncation: True\nmax_length = 512 ...\n\nhow do i pass these value? \n\n```\nimport sagemaker \n\nhub = { \n   'HF_TASK' : 'text2text-generation'\n}\nrole = sagemaker.get_execution_role()\n\nhuggingface_model = HuggingFaceModel( transformers_version='4.6.1', env=hub...\n\npredictor = huggingface_model.deploy( ....\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Region",
        "Question_created_time":1662026822143,
        "Question_last_edit_time":1667926719287,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyMQN5wWjQ1m_3Xp5YbhspA\/sagemaker-region",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":61,
        "Question_answer_count":0,
        "Question_body":"Hi everyone!\nIm having problems trying to connect to sagemaker on us-east-2, i have no problems with other regions but every project on sagemaker is made on us-east-2, \u00bfis anyone having same problems? thank u !",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Outgoing mail for sagemaker labeling job",
        "Question_created_time":1662016903588,
        "Question_last_edit_time":1667926589665,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\n1. in case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\n2. in case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1662130403761,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1662130403761,
        "Answer_comment_count":1.0,
        "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\n1. The IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\n2. Your Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How can I use serverless inference with models generated by SageMaker Autopilot?",
        "Question_created_time":1661981861600,
        "Question_last_edit_time":1668022848063,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJpArvoZTSkiWkDMVDW1avw\/how-can-i-use-serverless-inference-with-models-generated-by-sagemaker-autopilot",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_body":"There are a few articles about deploying SageMaker models to use serverless inference, but I am not clear on how to do that with autopilot models in particular. In other words, I do not understand which steps should be different and how to find information such as what my model ARN is. Thanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"PySpark on Sagemaker - force stop of the execution for all cells",
        "Question_created_time":1661935002270,
        "Question_last_edit_time":1667992716923,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW0rkjqaNS7aNKZV3mUnFGw\/pyspark-on-sagemaker-force-stop-of-the-execution-for-all-cells",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":71,
        "Question_answer_count":1,
        "Question_body":"Hi,\n I am using Jupyter Notebook on SageMaker with PySpark kernel. I want to implement some data checks so that the processing of the notebook is stopped if some conditions is meet. For Python kernel one can simply use raise statement as in the example below. If the notebook was run using \"Run All\" button then the code below would stop not only the execution of the error cell, but it would **not proceed to the next cells**. \n```\nif type(age) is not int:\n    raise TypeError(\"Age must be an integer\")\nelif age < 0:\n    raise ValueError(\"Sorry you can't be born in the future\")\n```\nHowever, if we use the same piece of code with PySpark kernel, execution of the error cell would be stopped but the notebook would still proceed with the execution of the next cells (as \"Run All\" was used). So in fact calculations would be proceeded even if age is not integer or is < 0. \n\nHow to force PySpark kernel to stop executing the entire notebook on error, just as it is the case for Python kernel?\n\nAdding a print screen with output notebook after \"Run All\": \n![Enter image description here](\/media\/postImages\/original\/IM4NPOObXJTXWuNvt7ZhL8aw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Reason why errors occur when starting SageMaker Studio",
        "Question_created_time":1661920764293,
        "Question_last_edit_time":1668548249549,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfTu0DsSxTf-B8n4srTzG9A\/reason-why-errors-occur-when-starting-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":507,
        "Question_answer_count":1,
        "Question_body":"Hello!\nI have a question about errors found when starting SageMaker Studio (below). \n****\n AccessDeniedException\n User: X is not auth**orized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateDomain action\n \n ValidationException\n Access denied in getting\/deleting the portfolio shared by SageMaker. Please call withservicecatalog:ListAcceptedPortfolioShares permission.\n \n AccessDeniedException\n User: X is not authorized to perform: sagemaker:CreateUserProfile on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateUserProfile action\n\n\n****\nI resolved the errors by adding some inline policies, but I cannot understand the reason why the errors occur on my user with S3 Full Access and SageMaker Full Access policies. \n\nI'd happy to tell me any information about the errors. Thank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"I want to deploy my model as a Serverless inference",
        "Question_created_time":1661852626896,
        "Question_last_edit_time":1668131071068,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGFC_kpAJTx6NcEG9_ZqUyQ\/i-want-to-deploy-my-model-as-a-serverless-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":115,
        "Question_answer_count":2,
        "Question_body":"Hey I trained a sickst learn model using python sdk and I want to deploy the model as a Serverless inference now. I am new to AWS and can't seem to make sense of the documentation. the model is fit it an estimator as follow: \n\n```\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\ninputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": 'ml.c5.xlarge',\n    \"instance_count\": 1,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\"\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n```\n\nthis works fine but now when I try to deploy it it doesn't work. I tried this code: https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploying-ml-models-using-sagemaker-serverless-inference-preview\/ \nbut i keep getting errors because somethings are not defined like the image_uri which I am not using. \n\nI used this \n\n```\nm_boto3 = boto3.client('sagemaker')\n\nestimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n```\nbut then the endpoint is not Serverless. please help",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to use sagemaker.processing.Processor run method",
        "Question_created_time":1661777469790,
        "Question_last_edit_time":1667926436164,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhZktmd-_Q3mFdOyFaNU9NA\/how-to-use-sagemaker-processing-processor-run-method",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":2,
        "Question_body":"This is sagemaker [docs](shttps:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html). What is the purpose of sagemaker.processing.Processor as its run method does not have input for code or script? then how can I use it? \n\nOf course, I can use FrameworkProcessor, ScriptProcessor, SklearnProcessor because I can provide my processing.py. But for the sagemaker.processing.Processor, how can I use it?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to get batch transform with jsonl data?",
        "Question_created_time":1661703317094,
        "Question_last_edit_time":1667903395203,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkP-cRiP3QiCAIqnwyirz1A\/how-to-get-batch-transform-with-jsonl-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_body":"I am using my own inference.py file as a entry point for inference. I have tested this pytorch model, served as a real time endpoint in amaon sagemaker. but when i try to create a batch job and use multiple json object in my input file (jsonl format) . i get the following error at the input_fn function on this line   data = json.loads(request_body), in cloudwatch logs ==> \n\n  data = json.loads(request_body)\nraise JSONDecodeError(\"Extra data\", s, end) \njson.decoder.JSONDecodeError: Extra data : line 2 column 1 (Char ..)\n\nI am not sure why am i getting extra data on line 2 error, because this is supposed to be batch job with multiple json input and each line. \n\ninference.py\n```\ndef model_fn(model_dir):\n   \/\/load the model\n\n\n\n\ndef input_fn(request_body, request_content_type):\n    input_data= json.loads(request_body)\n    return data\n\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n```\n\nset up batch job\n```\nresponse = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n```\n\ninput file \n```\n{\"input\" : \"some text here\"}\n{\"input\" : \"another\"}\n...",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN]",
        "Question_created_time":1661681019239,
        "Question_last_edit_time":1668075816187,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBY-fIUMuRDqwBmObCn8GqQ\/no-such-file-or-directory-opt-ml-input-data-test-revenue-train-csv-sagemaker-sm-channel-train",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":110,
        "Question_answer_count":1,
        "Question_body":"I am trying to deploy my RandomForestClassifier on Amazon Sagemaker using Python SDK. I have been following this example https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-script-mode\/sagemaker-script-mode.ipynb but keep getting an error that the train file was not found. I think the file were not uploaded to the correct channel. When I run the script as follows it works fine.\n```\n! python script_rf.py --model-dir .\/ \\\n                   --train .\/ \\\n                   --test .\/ \\\n```\n\nThis is my script code:\n\n```\n# inference functions ---------------\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--max_depth', type=int, default=2)\n    parser.add_argument('--n_estimators', type=int, default=100)\n    parser.add_argument('--random_state', type=int, default=0)\n    \n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='revenue_train.csv')\n    parser.add_argument('--test-file', type=str, default='revenue_test.csv')\n    \n    args, _ = parser.parse_known_args()\n    \n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    if len(train_df) == 0:\n        raise ValueError(('There are no files in {}.\\n').format(args.train, \"train\"))\n\n    print('building training and testing datasets')\n    attributes = ['available_minutes_100','ampido_slots_amount','ampido_slots_amount_100','ampido_slots_amount_200','ampido_slots_amount_300','min_dist_loc','count_event','min_dist_phouses','count_phouses','min_dist_stops','count_stops','min_dist_tickets','count_tickets','min_dist_google','min_dist_psa','count_psa']\n    X_train = train_df[attributes]\n    X_test = test_df[attributes]\n    y_train = train_df['target']\n    y_test = test_df['target']\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        max_depth =args.max_depth, n_estimators = args.n_estimators)\n    \n    model.fit(X_train, y_train)\n     \n    # persist model\n    path = os.path.join(args.model_dir, \"model_rf.joblib\")\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n    \n    # print accuracy and confusion matrix \n    print('validating model')\n    y_pred=model.predict(X_test) \n    print('Confusion Matrix:')\n    result = confusion_matrix(y_test, y_pred)\n    print(result)\n    print('Accuracy:')\n    result2 = accuracy_score(y_test, y_pred)\n    print(result2)\n```\nthe error is raised in the train_df line of the script (FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv'). \n\nI tried specifying the input parameters: \n\n```\n# change channel input dirs \ninputs = {\n    \"train\": \"ampido-exports\/production\/revenue_train\",\n    \"test\": \"ampido-exports\/production\/revenue_test\",\n}\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\nhyperparameters = {\"max_depth\": 2, 'random_state':0, \"n_estimators\": 100}\n\nif enable_local_mode_training:\n    train_instance_type = \"local\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nelse:\n    train_instance_type = \"ml.c5.xlarge\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": train_instance_type,\n    \"instance_count\": 1,\n    \"hyperparameters\": hyperparameters,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\",\n    'channel_input_dirs' : inputs\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n```\n\nbut i still get the error FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to get logs or print statements from SageMaker PyTorch deployed endpoint?",
        "Question_created_time":1661549640227,
        "Question_last_edit_time":1667925751658,
        "Question_link":"https:\/\/repost.aws\/questions\/QU74MThjkyRVCtySw-DEozrQ\/how-to-get-logs-or-print-statements-from-sagemaker-pytorch-deployed-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":105,
        "Question_answer_count":2,
        "Question_body":"I've deployed an extended Pytorch model as an endpoint and I'm trying to make inference requests to it. Problem is, the responses from the endpoint get timed out and CloudWatch logs show nothing beyond:\n| timestamp| message|\n| --- | --- |\n| 1661544743589| WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance. |\n| 1661544749569| Model server started.|\n\nNow in my `inference.py` file, which I provided as the entry point I've set logging as follows:\n\n```\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file.\")\n```\nI wish to see those logs\/prints. How can I accomplish that?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Jupyter Notebook",
        "Question_created_time":1661528978544,
        "Question_last_edit_time":1668501089233,
        "Question_link":"https:\/\/repost.aws\/questions\/QUowhjHLZvR3aS6tDhH38uZQ\/jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":94,
        "Question_answer_count":2,
        "Question_body":"I want to start a Jupyter Notebook instance, then load a ipynb file I have uploaded into a bucket. How do I this? I have never been able to initiate Sagemaker as of yet.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker GT Streaming Labelling Job Internal Server Error (Job Failed)",
        "Question_created_time":1661525322145,
        "Question_last_edit_time":1668455128898,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZD0isCgLSYqxXhdah_0jkQ\/sagemaker-gt-streaming-labelling-job-internal-server-error-job-failed",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":149,
        "Question_answer_count":1,
        "Question_body":"I launched a Sagemaker Ground Truth Labeling Job with a vendor for 3D Point Cloud Object Tracking using the API. Yesterday, my job failed with the reason for failure \n```\nInternalServerError: We encountered an internal error. Submit a new job.\n```\n\nThe last three Cloudwatch logs before I saw the failure were\n\n`{\n    \"labeling-job-name\": \"scand-trial2-seq10v2stream\",\n    \"event-name\": \"BATCH_ANNOTATION_STATUS_EVALUATED\",\n    \"event-log-message\": \"Batch of annotation tasks completed with status FAILED. \"\n}`\n\n`{\n    \"labeling-job-name\": \"scand-trial2-seq10v2stream\",\n    \"event-name\": \"EXPORTED_LABELED_MANIFEST\",\n    \"event-log-message\": \"Labeled manifest written to S3 output location.\"\n}`\n\n`{\n    \"labeling-job-name\": \"scand-trial2-seq10v2stream\",\n    \"event-name\": \"IDLE_TIMER_COUNT_INTERRUPTED\",\n    \"event-log-message\": \"System detected incoming objects. Timer to monitor idleness was reset.\"\n}`\n\nMy vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. In addition, I set the expiration time for my job to be 30 days, and this failed at 20 days. What could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"deploying previously trained model with Sagemaker Python SDK (StatusExceptionError)",
        "Question_created_time":1661503967725,
        "Question_last_edit_time":1668004558133,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXT-lr_7ASxSSzx0lRAaEvg\/deploying-previously-trained-model-with-sagemaker-python-sdk-statusexceptionerror",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_body":"I am using a pertained Random Forest Model and trying to deploy it on Amazon Sagemker using Python SDK: \n\n```\nfrom sagemaker.sklearn.estimator import SKLearn\n\nsklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit')\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n```\nI get the following StatusException Error \n\n\n```\n2022-08-25 12:03:27 Starting - Starting the training job....\n2022-08-25 12:03:52 Starting - Preparing the instances for training............\n2022-08-25 12:04:55 Downloading - Downloading input data......\n2022-08-25 12:05:31 Training - Downloading the training image.........\n2022-08-25 12:06:22 Training - Training image download completed. Training in progress..\n2022-08-25 12:06:32 Uploading - Uploading generated training model.\n2022-08-25 12:06:43 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n<ipython-input-37-628f942a78d3> in <module>\n----> 1 sklearn_estimator.latest_training_job.wait(logs='None')\n      2 artifact = m_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n      4 \n      5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2110         else:\n-> 2111             self.sagemaker_session.wait_for_job(self.job_name)\n   2112 \n   2113     def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3227         )\n-> 3228         self._check_job_status(job, desc, \"TrainingJobStatus\")\n   3229         return desc\n   3230 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3390                 message=message,\n   3391                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-> 3392                 actual_status=status,\n   3393             )\n   3394 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run\n    cwd=environment.code_dir,\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n```\n\nThe pertained model works fine and I don't know what the problem is, please help",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deploying a Random Forest Model on Amazon Sagemaker always getting a UnexpectedStatusException with Reason: AlgorithmError",
        "Question_created_time":1661503022955,
        "Question_last_edit_time":1668092002669,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMmnoFG_HQ0qiMWxlSlPlcQ\/deploying-a-random-forest-model-on-amazon-sagemaker-always-getting-a-unexpectedstatusexception-with-reason-algorithmerror",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":42,
        "Question_answer_count":0,
        "Question_body":"Hey I am trying to deploy my RandomForest Classifier on Amazon Sagemaker but get a StatusException Error even though the script worked fine before:\n\nThe script runs fine and prints out the confusion matrix and accuracy as expected. When I try to deploy the model to amazon Sagemaker using the script it does not work.\n\n\n\n\n\n>>! python script.py --n-estimators 100 \\\n                   --max_depth 2 \\\n                   --model-dir .\/ \\\n                   --train .\/ \\\n                   --test .\/ \\ \n\n\nConfusion Matrix: \n[[13 8] \n[ 1 17]]\n Accuracy: 0.7692307692307693\n\nI used the Estimator from Sagemaker Python SDK\n\n\n>>from sagemaker.sklearn.estimator import SKLearn\n>>sklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit')\n\nI launched the training job as follows\n\n>>sklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nHere I am trying to deploy the model which leads to the StatusExceptionError that I cannot seem to fix\n\n\n>>sklearn_estimator.latest_training_job.wait(logs='None')\n>>artifact = m_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts'['S3ModelArtifacts']\n\n>>print('Model artifact persisted at ' + artifact)\n\n>>2022-08-25 12:03:27 Starting - Starting the training job....\n>>2022-08-25 12:03:52 Starting - Preparing the instances for training............\n>>2022-08-25 12:04:55 Downloading - Downloading input data......\n>>2022-08-25 12:05:31 Training - Downloading the training image.........\n>>2022-08-25 12:06:22 Training - Training image download completed. Training in progress..\n>>2022-08-25 12:06:32 Uploading - Uploading generated training model.\n>>2022-08-25 12:06:43 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n<ipython-input-37-628f942a78d3> in <module>\n----> 1 sklearn_estimator.latest_training_job.wait(logs='None')\n      2 artifact = m_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n      4 \n      5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2110         else:\n-> 2111             self.sagemaker_session.wait_for_job(self.job_name)\n   2112 \n   2113     def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3227         )\n-> 3228         self._check_job_status(job, desc, \"TrainingJobStatus\")\n   3229         return desc\n   3230 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3390                 message=message,\n   3391                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-> 3392                 actual_status=status,\n   3393             )\n   3394 \n\n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run\n    cwd=environment.code_dir,\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\n> \n\nI am happy for some help",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SKLearn Processing Container - Error: \"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager.\"",
        "Question_created_time":1661439501840,
        "Question_last_edit_time":1668527409096,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVtb8YdmWSG2wFr5aMI5MaA\/sklearn-processing-container-error-warning-running-pip-as-the-root-user-can-result-in-broken-permissions-and-conflicting-behaviour-with-the-system-package-manager",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_body":"Hey all,\n\nI am trying to run the script below in the writefile titled \"vw_aws_a_bijlageprofile.py\". This code has worked for me using other data sources, but now I am getting the following error message from the CloudWatch Logs:\n\n\"***2022-08-24T20:09:19.708-05:00\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https:\/\/pip.pypa.io\/warnings\/venv***\"\n\nAny idea how I get around this error?\n\nFull code below.\n\nThank you in advance!!!!\n\n```\n%%writefile vw_aws_a_bijlageprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"intl-euro-archmcc-database\".\"vw_aws_a_bijlage\"\n    ;\n    \"\"\"\n                                        #swich table name above\n        \n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"intl-euro-archmcc-database\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated table profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('\/opt\/ml\/processing\/output\/profile_vw_aws_a_bijlage.html')\n                                    #switch profile name above\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n```\n\n```\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='vw_aws_a_bijlageprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n```\n\n\n```\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n```\n\n\n```\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/vw_aws_a_bijlageprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/IODataProfiles\/')])\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Neo with SageMaker Serverless Inference possible?",
        "Question_created_time":1661345484756,
        "Question_last_edit_time":1667925893497,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEjedWnu4Q5iwaGVNul__1w\/sagemaker-neo-with-sagemaker-serverless-inference-possible",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":55,
        "Question_answer_count":0,
        "Question_body":"Can I use SageMaker Neo to compile my model and then deploy it to SageMaker Serverless inference? Which instance type in Neo should I use in that case?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can Amazon Comprehend extract data from documents?",
        "Question_created_time":1661289910533,
        "Question_last_edit_time":1668485572666,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1g7uVKhDTfWHIZpFk-a1hA\/can-amazon-comprehend-extract-data-from-documents",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":121,
        "Question_answer_count":1,
        "Question_body":"Hi! My team and I have the following scenario: we want to extract some fields from several PDF documents, that may or may not follow the same pattern. To exemplify, let's say we want to extract these 3 fields from these documents:\n\n![Enter image description here](\/media\/postImages\/original\/IMRcmS97dmRTm4ZhZJRzLkbQ)\n\nSo, we have a Name, a Code (called CNPJ) for this person, and its Address. Obviously, these fields would vary between documents, but the CNPJ would always keep its format, only changing the sequence of numbers. During our research to solve this challenge, we came across Amazon Comprehend and its Custom Named Entity Recognition. Our idea was to create these three entities - Name, CNPJ and Address - using a Ground Truth Labeling Job.\n\nTo do this, we Textracted some of our PDF's, generating .txt files for each one of them, and then uploaded these files to an S3 Bucket. After that, we proceeded to create the Labeling Job, using an Automated data setup to generate the input manifest file so the labeling could start. And what happened was that as I inputted many .txt files, each line in these files got recognized as a separate object, resulting in more than 7700 objects to be labeled. Of course, approximately 90% of these objects didn't had any labeling to be done, resulting in me having to continuously skip these lines until I had to label one of those objects, and also in a very high money cost due to the high number of objects.\n\nSo, I have a few questions. For starters, was Amazon Comprehend a good choice for this job? If it wasn't, what would be the best solution?\nIf it was a good choice, what could I have done to optimize the labeling job? Were the \"useless\" objects really necessary?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to specify target feature in Sagemaker XGBoost?",
        "Question_created_time":1661221322082,
        "Question_last_edit_time":1667926553400,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoW_FqSbIQKW0MqNJLlA2AA\/how-to-specify-target-feature-in-sagemaker-xgboost",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_body":"I am considering migrating a data science project from Datarobot to Sagemaker. I am familiar with writing Python and have been going through one of the tutorial Jupyter notebooks to see how to explore the data and to build and deploy and estimator. But, I cannot see how to specify the target feature. I have entirely numerical data in a csv file. One of the fields in that file is the intended target for estimation, the rest are information from which the estimate is to be made. \n\nHow do I specify the column that is to be estimated?\nThe code I expect should have this is ...\n\n\n```\ncontainer = sm.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"1.5-1\")\n\nxgb = sm.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/xxxxxx001\/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    num_round=100,\n)\ns3_input_train = TrainingInput(\n    s3_data=\"s3:\/\/xxxxxx001\/data.csv\", content_type=\"csv\"\n)\nxgb.fit({\"train\": s3_input_train})\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1661298174174,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1661298174174,
        "Answer_comment_count":0.0,
        "Answer_body":"On a badly formatted page on the AWS documentation, I found a statement that - the CSV file must have no headers and the target field must be the first field. So, apparently, it is not possible to specify the target. So primitive, yeah?",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"[Solved]download image from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)",
        "Question_created_time":1660886637665,
        "Question_last_edit_time":1668577805956,
        "Question_link":"https:\/\/repost.aws\/questions\/QULB64ZDsXSPuHBjqAWik3hQ\/solved-download-image-from-s3-to-endpoint-made-by-sagemaker-with-s3url-s3",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":287,
        "Question_answer_count":1,
        "Question_body":"I try to download image(.jpg, .png.) from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)\n\nBecause At the endpoint made by sagemaker, To send s3url is faster than to send image.\n\nI can download image at sagemaker notebook, from s3 to sagemaker local.\n\nbut I can't download image from s3 to sagemaker endpoint.\n\nThat local download code can not work.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1661216110041,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1661216167531,
        "Answer_comment_count":0.0,
        "Answer_body":"I solve this! I try to download image at endpoint.\nbut endpoint can not connect outside network except Lambda.\n1. I make request with s3url\n2. Download image from s3 to lambda\n3. Transmit image from lambda to endpoint",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Ask AWS SageMaker",
        "Question_created_time":1660876307100,
        "Question_last_edit_time":1667925848878,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":50,
        "Question_answer_count":1,
        "Question_body":"1. Can we make a new code through the sagemaker studio?\n2. In my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\n3. How much GPU performance can you improve compared to before?\n4. I want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1661427331608,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1661427331608,
        "Answer_comment_count":3.0,
        "Answer_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\n1. Yes, you can write your own custom code through SageMaker studio.\n\n2. This may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\n3. Please refer to the response above for question # 2.\n4. Did you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards,\nPunya",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AWS SageMaker Notebook Instance is not continuing running the cell when I leave my Laptop to execute the cells over a period of time. Can you tell me how can I solve this?",
        "Question_created_time":1660857294691,
        "Question_last_edit_time":1668611267215,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPbfNFX-sQhmUncaRIpCw_A\/aws-sagemaker-notebook-instance-is-not-continuing-running-the-cell-when-i-leave-my-laptop-to-execute-the-cells-over-a-period-of-time-can-you-tell-me-how-can-i-solve-this",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":506,
        "Question_answer_count":2,
        "Question_body":"Hello,\nWhen I leave the SageMaker Jupyter notebook to execute the cells for a long period of time, after about 8 hours, it is signing me out of the console and when I log back in, it is **not** running\/continuing its execution of the cells. There was No error message displayed when this happened. When I left the notebook running, the laptop was ON and the screen was active. Could you tell me how can I leave the notebook to run for a long period of time without worrying about being active on the laptop all the time? Thank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Jumpstart 'Explain Credit Decisions' fails to deploy",
        "Question_created_time":1660818001611,
        "Question_last_edit_time":1667926049704,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMx_45pb_TLOfMOZINMG2JA\/sagemaker-jumpstart-explain-credit-decisions-fails-to-deploy",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":110,
        "Question_answer_count":1,
        "Question_body":"Im trying to use the Sagemaker Jumpstart 'Explain Credit Decisions' via the Studio Jumpstart menu. However, everything works as instructed until it hits the 'glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)' step in the datasets notebook. \n\nThis produces a \"failed to execute with exception Internal service error: Invalid Input Provided\" (error in the Glue console) and falls over on the job part of the glue job. \n\nDoes anyone have any ideas? This is as much information as is available in the console logs.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to check smdistributed-modelparallel version?",
        "Question_created_time":1660805972479,
        "Question_last_edit_time":1668434749756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":109,
        "Question_answer_count":1,
        "Question_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of `smdistributed-modelparallel` module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1660807962126,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1660807962126,
        "Answer_comment_count":0.0,
        "Answer_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at \nhttps:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for `PyTorch 1.10.2 with HuggingFace transformers` DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: `smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl`.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Can't Import Modules In Sagemaker Jupyter Notebook",
        "Question_created_time":1660764982506,
        "Question_last_edit_time":1668609100913,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBSiq8Lx5St-1D8CT3k328g\/can-t-import-modules-in-sagemaker-jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":334,
        "Question_answer_count":1,
        "Question_body":"I've tried to use the pip install librosa command within a jupyter notebook, yet I get a modulenotfounderror. Is there a place where I can use pip install librosa?\n\nThanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Question_created_time":1660713837979,
        "Question_last_edit_time":1668287433885,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMJK2lci1RZa81gB0P--NWg\/how-does-input-fn-predict-fn-and-output-fn-work-in-aws-sagemaker-script-mode",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":461,
        "Question_answer_count":1,
        "Question_body":"i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand  how they are called (invoked), can anyone help me understand the same",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker training instance",
        "Question_created_time":1660674061917,
        "Question_last_edit_time":1668416268136,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4_NA-4cTS8aGkpKawI0qVA\/sagemaker-training-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":208,
        "Question_answer_count":2,
        "Question_body":"I have a doubt with choosing instance for training job in sagemaker. Is ml.m5.2xlarge with count as 2 and ml.m5.4xlarge are same ? \n\nI would like to know if there is any best practice guide to choose the instance for training in sagemaker. \n\nThank you \ud83d\ude42",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Question_created_time":1660653174738,
        "Question_last_edit_time":1668605561609,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s3-that-is-created-in-a-sagemaker-processing-container",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":443,
        "Question_answer_count":1,
        "Question_body":"**Error message:**\n\"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\n**Background:**\nI am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\n\n```\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n```\n\n```\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n```\n\n```\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n```\n\n\n\n```\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n```\n\n```\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n```\n\n\nThank you in advance!!!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1660704602950,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1660704602951,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi,\n\nFirstly, you should not (usually) need to directly interact with S3 from your processing script: The fact that you've configured your `ProcessingOutput` means that any files your script saves in `\/opt\/ml\/processing\/output` should automatically get uploaded to your `s3:\/\/...` destination URL. Of course there might be particular special cases where you want to directly access S3 from your script, but in general the processing job inputs and outputs should do it for you, to keep your code nice and simple.\n\nI'm no Pandas Profiler expert, but I *think* the error might be coming from here:\n\n```python\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n```\n\nDoesn't this just save the report to `profile_case.html` in your current working directory? That's not the `\/opt\/ml\/processing\/output` directory: It's usually the folder where the script is downloaded to the container I believe. The FileNotFound error is telling you that the HTML file is not getting created in the folder you expect, I think.\n\nSo I would suggest to make your output path explicit e.g. `\/opt\/ml\/processing\/output\/profile_case.html`, and also remove the boto3\/s3 section at the end - hope that helps!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Debugger: cannot load training information of estimator",
        "Question_created_time":1660301495882,
        "Question_last_edit_time":1667926190523,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUl_ylpIuQUWy0N-CDqP_ag\/sagemaker-debugger-cannot-load-training-information-of-estimator",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":141,
        "Question_answer_count":1,
        "Question_body":"I am using a SageMaker notebook for training a ML model. When I created and trained the estimator successfully with the following script, I could load the debugging information (s3_output_path) as expected:\n\n\n```\nfrom sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\nrules = [\n    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n    Rule.sagemaker(rule_configs.vanishing_gradient()),\n    Rule.sagemaker(rule_configs.overfit()),\n    Rule.sagemaker(rule_configs.overtraining()),\n    Rule.sagemaker(rule_configs.poor_weight_initialization())]\n\ncollection_configs=[CollectionConfig(name=\"CrossEntropyLoss_output_0\", parameters={\n    \"include_regex\": \"CrossEntropyLoss_output_0\", \"train.save_interval\": \"100\",\"eval.save_interval\": \"10\"})]\n\ndebugger_config = DebuggerHookConfig(\n    collection_configs=collection_configs)\n\nestimator = PyTorch(\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\"ml.m5.xlarge\",\n#instance_type=\"ml.g4dn.2xlarge\",\nentry_point=\"train.py\",\nframework_version=\"1.8\",\npy_version=\"py36\",\nhyperparameters=hyperparameters,\ndebugger_hook_config=debugger_config,\nrules=rules,\n)\n\nestimator.fit({\"training\": inputs})\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\n```\nAfter the kernel died, I attached the estimator and tried to access the debugging information of the training:\n\n```\nestimator = sagemaker.estimator.Estimator.attach('pytorch-training-2022-06-07-11-07-09-804')\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\nrules_path = estimator.debugger_rules\n```\nThe return values of these 2 functions were None. Could this be a problem with the attach-function? And how can I access training information of the debugger after the kernel was shut down?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Question_created_time":1660209790025,
        "Question_last_edit_time":1668077202772,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internalserverexception",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":119,
        "Question_answer_count":2,
        "Question_body":"I make pytorch model with sagemaker, MMS.\nThis is my mms code.\n```python\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n```\n\n\nI try to predict with this code.\n\n```python\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n```\n\nAnd, return error message.\n\n```\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n```\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1660312923533,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1660312923533,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi ,\nI think your target model on the prediction needs to have the name of the model you have deployed - \nfor example , when you are adding the model with \nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nthe model_data_path contains the name of the model . From the sagemaker-examples:\n(https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb)\n**model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\".\nHowever, when predicting you call the model ,target_model=\"LV1\"?",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker endpoint running but constantly restarting",
        "Question_created_time":1660140840225,
        "Question_last_edit_time":1668490922842,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQi42sIDTTSW5KN3P-DT4LQ\/sagemaker-endpoint-running-but-constantly-restarting",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":229,
        "Question_answer_count":2,
        "Question_body":"I have deployed a model to a Sagemaker endpoint using BentoML\/BentoCTL. This is a tool for building APIs and containerizing models. To test, I use curl with a JSON payload to make a request. When I run the created docker container on my local machine I can successfully invoke it and get responses back. So I don't think the problem is in the docker image.\n\nWhen I deploy to sagemaker, I receive the message `{\"message\":\"Service Unavailable\"}` as a response to my curl request. I can see the endpoint running in the Sagemaker\/Endpoints dashboard. Viewing the cloudwatch logs, it appears that the the endpoint is constantly restarting. There are messages that are printed at startup (e.g. Tensorflow loading messages) that are written to the log over and over.\n\nI thought that this might be due to using an instance type with low memory (t2.medium) so I switched to m5.4xlarge as a test, but the result is the same.\n\nWhat can I do? How can I determine what's causing the endless restarts?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_created_time":1660135320930,
        "Question_last_edit_time":1668490449198,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\n### Endpoint Configuration ###\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a `ml.m4.xlarge` instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried `s3:\/\/<bucket-name>` as well as `s3:\/\/<bucket-name>\/<some-other-path>`. With the \"Capture content type\" I tried leaving everything blank, setting `text\/csv` in \"CSV\/Text\" and `application\/json` in \"JSON\".\n\n### Endpoint Invokation ###\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n```\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n```\nInternally, the endpoint uses a Flask API with an `\/invocation` path that returns the result.\n\n### Logs ###\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n```\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n```\n\n```\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1660656368966,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1660656368966,
        "Answer_comment_count":0.0,
        "Answer_body":"So the issue seemed to be related to the IAM role. The default role (`ModelEndpoint-Role`) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"How can make multi model endpoint with SageMaker?",
        "Question_created_time":1660122368052,
        "Question_last_edit_time":1667925983239,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"This is my code.\n\n```python\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n```\n\nAnd error message.\nHow can I find Ecr Image(within multi-models=true)?\n```\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1660139833476,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1660139884319,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi there - thanks for opening this thread. Multi-model endpoints are not supported on GPU instance types, see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance\n\nIn order to host a multi-model endpoint, choose a CPU instance type instead. The ECR image for CPUs will contain the required  `com.amazonaws.sagemaker.capabilities.multi-models=true` label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Installation of jupyter notebook on Deep Learning AMI GPU TensorFlow 2.9.1 (Amazon Linux 2)",
        "Question_created_time":1660090729750,
        "Question_last_edit_time":1668135363009,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQNSYLw6SgKO_0oZamQKqg\/installation-of-jupyter-notebook-on-deep-learning-ami-gpu-tensorflow-2-9-1-amazon-linux-2",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":172,
        "Question_answer_count":2,
        "Question_body":"I've been unable to install jupyter notebook seemingly regardless of the virtual environment I have set on Deep Learning AMI GPU TensorFlow 2.9.1 (Amazon Linux 2).   Has anyone successfully managed this?\nThe error is the oft occurring:\nModuleNotFoundError: No module named 'pysqlite2'\nBut as most of the solutions involved rebuilding python, I don't want to do that given this is a distribution for which jupyter is typically the go to tool for such an AMI.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"not authorized to perform: sagemaker:CreateModel on resource",
        "Question_created_time":1660056986989,
        "Question_last_edit_time":1667925651923,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1sGemgvLQQS-w46eoBoo6w\/not-authorized-to-perform-sagemaker-createmodel-on-resource",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":432,
        "Question_answer_count":1,
        "Question_body":"I have been given AmazonSagemakerFullAccess by my companie's AWS admin. No one at our company can figure out why I can't get this line to run to launch the model.\n\n***** CODE PRODUCING ERROR *****\n\nlang_id = sagemaker.Model(\n    image_uri=container, model_data=model_location, role=role, sagemaker_session=sess\n)\nlang_id.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n\n\n\n***** ERROR MESSAGE *****\n---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n<ipython-input-5-4c80ec284a4b> in <module>\n      2     image_uri=container, model_data=model_location, role=role, sagemaker_session=sess\n      3 )\n----> 4 lang_id.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n      5 \n      6 from sagemaker.deserializers import JSONDeserializer\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, **kwargs)\n   1132 \n   1133         self._create_sagemaker_model(\n-> 1134             instance_type, accelerator_type, tags, serverless_inference_config\n   1135         )\n   1136 \n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags, serverless_inference_config)\n    671             tags=tags,\n    672         )\n--> 673         self.sagemaker_session.create_model(**create_model_args)\n    674 \n    675     def _ensure_base_name_if_needed(self, image_uri, script_uri, model_uri):\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\n   2715                     raise\n   2716 \n-> 2717         self._intercept_create_request(create_model_request, submit, self.create_model.__name__)\n   2718         return name\n   2719 \n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _intercept_create_request(self, request, create, func_name)\n   4294             func_name (str): the name of the function needed intercepting\n   4295         \"\"\"\n-> 4296         return create(request)\n   4297 \n   4298 \n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in submit(request)\n   2703             LOGGER.debug(\"CreateModel request: %s\", json.dumps(request, indent=4))\n   2704             try:\n-> 2705                 self.sagemaker_client.create_model(**request)\n   2706             except ClientError as e:\n   2707                 error_code = e.response[\"Error\"][\"Code\"]\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    506                 )\n    507             # The \"self\" in this scope is referring to the BaseClient.\n--> 508             return self._make_api_call(operation_name, kwargs)\n    509 \n    510         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    909             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    910             error_class = self.exceptions.from_code(error_code)\n--> 911             raise error_class(parsed_response, operation_name)\n    912         else:\n    913             return parsed_response\n\nClientError: An error occurred (AccessDeniedException) when calling the CreateModel operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role\/sagemakeraccesstoservices\/SageMaker is not authorized to perform: sagemaker:CreateModel on resource: arn:aws:sagemaker:us-east-2:XXXXXXXXXX:model\/blazingtext-2022-08-09-13-58-21-739 because no identity-based policy allows the sagemaker:CreateModel action",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does a completed training job incurr charges?",
        "Question_created_time":1659691485459,
        "Question_last_edit_time":1667926677597,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvqrWFrV7SEG9L-GcLmU0ag\/does-a-completed-training-job-incurr-charges",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":85,
        "Question_answer_count":1,
        "Question_body":"Hello.\n\nAs you can see in the screenshot below, there are couple of completed jobs in sagemaker. (Similar in the processing jobs and training jobs menu.)\n\nIf I select one and click the actions button, the Stop menu is inactivated, as you can see.\n\n\nDoes this kind of completed job incurr charges?\n\nIf so, what do I have to do to Delete or Stop this job?\n\nThank you.\n\n![Enter image description here](https:\/\/repost.aws\/media\/postImages\/original\/IMzOJsW4w4QGWO0LFXm-KlEg)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How can I terminate Amazon SageMaker RunInstance?",
        "Question_created_time":1659686032815,
        "Question_last_edit_time":1668608150763,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5_3j9z8tSdit5HkWWenrOw\/how-can-i-terminate-amazon-sagemaker-runinstance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":778,
        "Question_answer_count":1,
        "Question_body":"Hello.\n\nI had an unexpected billing, and that was because the SageMaker RunInstance was still running. (Especailly DataWrangler ; see the screenshot below.)\n\nI didn't know how to terminate, so I contacted to the AWS support center.\n\nI followed all the instructions they gave,  which means that I deleted endpoints \/ models \/ notebook instances \/ s3buckets \/  cloudwatch log groups.\n\nBut after 24 hours of monitoring, AWS support center said that the SageMaker RunInstance is still running.\n\nThey gave me the same instructions and one additional instruction : stop the training jobs (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-stop-training-job.html)\n\nFirstly I checked  endpoints \/ models \/ notebook instances \/ s3buckets \/  cloudwatch log groups again, and I found nothing in those tabs.\n\nSecondly I tried to stop the jobs, but I have deleted the domain since I tried to delete everything - thought it would be better (Maybe that was a fault) - So I created the domain again with quick start option (to get access to the studio), and got into the studio to terminate jobs. But all of the jobs are already completed. There's nothing with activated 'stop training jobs' button.\n\nSo I followed the instruction, but there was nothing I could do.\n\nI really want to STOP this SageMaker RunInstance, but I don't know why it's still running. What should I do..? Please help me.\n\n![Enter image description here](https:\/\/repost.aws\/media\/postImages\/original\/IMCxdk5Ab-QYuhAWNl1kt6jw)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Attach more than one lifecycle configuration to a notebook instance",
        "Question_created_time":1659629781823,
        "Question_last_edit_time":1667926617747,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsFSQu_SjRwq8IsBnwFMjEg\/attach-more-than-one-lifecycle-configuration-to-a-notebook-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":89,
        "Question_answer_count":1,
        "Question_body":"I have implemented the auto-stop-idle and persistent-conda-kernels lifecycle configurations and they both work. However, I cannot seem to find how to attach both to a single notebook instance. How can I do that?\n\nLifecycle configuration scripts found in [aws sample scripts](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples).",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Trigger and\/or monitor Automated Data Labeling Job",
        "Question_created_time":1659628069730,
        "Question_last_edit_time":1667860228626,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5eZvKOO5R4y3FAXlbpUqiw\/trigger-and-or-monitor-automated-data-labeling-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":86,
        "Question_answer_count":1,
        "Question_body":"I am following the documentation [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html#sms-auto-labeling-ec2]() and I have created a Ground Truth labeling job for object detection. I also enabled automated data labeling when creating the job. I have roughly 5000 images in my dataset. I have manually labelled ( by creating myself as a worker ) 129 of these images. How many do I need to label before the automated labeling job triggers? How do I know if a job was triggered\/succeeded\/failed etc?\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Inference for Tensorflow Base64 Input Error through API Gateway",
        "Question_created_time":1659593262466,
        "Question_last_edit_time":1668619725826,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCAh0uL1NRmCcZANpWnZd7A\/sagemaker-inference-for-tensorflow-base64-input-error-through-api-gateway",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":212,
        "Question_answer_count":1,
        "Question_body":"When I am trying to call my Sagemaker TF endpoint using API Gateway -> Lambda Func by passing a Base 64 String (an image) I am getting an unsupported string error. I also tried with application\/Json but I am still getting the error. Need Suggestion.\n\nIn Notebook Instance this is how my input looks: \n<CODE>\ninput = {\n  'instances': [{\"b64\": \"iV\"}]\n}\n\nIn Lambda function I am doing this:\n<CODE>\n\ninstance = [{\"b64\": \"b64string\"}]\npleasework=json.dumps({\"instances\": instance})\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME_BASE64,ContentType='string',Accept='string' ,Body=pleasework)\n\nERROR:\nInference Error:\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message \"{\"error\": \"Unsupported Media Type: string\"}\".\n\n\nIncase if I pass application\/json I get this error:\n\nReceived client error (400) from primary with message \"{    \"error\": \"Failed to process element: 0 of 'instances' list. Error: INVALID_ARGUMENT: JSON Value: {\\n    \\\"b64\\\": \\\"iV\\\"\\n} Type: Object is not of expected type: uint8\"}\"",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Training Job. Python modules installation Error",
        "Question_created_time":1659396236435,
        "Question_last_edit_time":1668563101671,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmwxhCTLzTp2f9ePN1V2oLg\/sagemaker-training-job-python-modules-installation-error",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":456,
        "Question_answer_count":1,
        "Question_body":"I have a problem with Python module installation that requires pre-installation of another module.  Both modules were added to the `requirement.txt` file. However, the error occurs when installing main module:\n```message\n2022-07-29 01:18:26.460132: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:26.470589: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\"\n2022-07-29 01:18:26.765280: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:31,908 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\"\n\"2022-07-29 01:18:31,917 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\"\n\"2022-07-29 01:18:33,117 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\"\n\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\nCollecting Cython==0.29.31\nDownloading Cython-0.29.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0\/2.0 MB 33.1 MB\/s eta 0:00:00\nRequirement already satisfied: wheel==0.37.1 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 2)) (0.37.1)\nCollecting scikit-image==0.19.2\nDownloading scikit_image-0.19.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.0\/14.0 MB 83.1 MB\/s eta 0:00:00\nCollecting parallelbar==0.1.19\nDownloading parallelbar-0.1.19-py3-none-any.whl (5.6 kB)\nCollecting albumentations==1.0.3\nDownloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7\/98.7 kB 6.6 MB\/s eta 0:00:00\nCollecting tensorflow_addons==0.16.1\nDownloading tensorflow_addons-0.16.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1\/1.1 MB 54.4 MB\/s eta 0:00:00\nRequirement already satisfied: tensorflow-io==0.24.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 7)) (0.24.0)\nRequirement already satisfied: tensorboard==2.8.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 8)) (2.8.0)\nCollecting universal-pathlib==0.0.12\nDownloading universal_pathlib-0.0.12-py3-none-any.whl (19 kB)\nCollecting setuptools==63.2.0\nDownloading setuptools-63.2.0-py3-none-any.whl (1.2 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2\/1.2 MB 58.9 MB\/s eta 0:00:00\nCollecting pynanosvg==0.3.1\nDownloading pynanosvg-0.3.1.tar.gz (346 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 346.0\/346.0 kB 17.5 MB\/s eta 0:00:00\nPreparing metadata (setup.py): started\nPreparing metadata (setup.py): finished with status 'error'\n\"error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [6 lines of output]\n      Traceback (most recent call last):\n        File \"\"<string>\"\", line 2, in <module>\n        File \"\"<pip-setuptools-caller>\"\", line 34, in <module>\n        File \"\"\/tmp\/pip-install-1mt2gkfy\/pynanosvg_d6162ffce95948abb4262061a011908c\/setup.py\"\", line 2, in <module>\n          from Cython.Build import cythonize\n      ModuleNotFoundError: No module named 'Cython'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\"\nerror: metadata-generation-failed\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\"note: This is an issue with the package mentioned above, not pip.\"\nhint: See above for details.\n[notice] A new release of pip available: 22.1.2 -> 22.2.1\n\"[notice] To update, run: pip install --upgrade pip\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Reporting training FAILURE\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    InstallRequirementsError:\"\nExitCode 1\n\"ErrorMessage \"\"      ModuleNotFoundError: No module named 'Cython'\n       [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed  \u00d7 Encountered error while generating package metadata. \u2570\u2500> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details.\"\"\"\n\"Command \"\"\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\"\"\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Encountered exit_code 1\"",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Custom Post Annotation Lambda Function for Custom Labeling Job",
        "Question_created_time":1659025093082,
        "Question_last_edit_time":1667869288267,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYzKc8ISiT4eU9cXCsdUqEg\/custom-post-annotation-lambda-function-for-custom-labeling-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":179,
        "Question_answer_count":2,
        "Question_body":"Hello,\n\nI have implemented a post annotation lambda function for my sagemaker ground truth custom job.\n\nAfter the annotations are finished, the results come after the consolidation of the annotations are saved in a subdirectory called \"*iteration_X*\" of *\"annotations\/consolidated-annotation\/consolidation-response\/\"*.\n\nHowever, the outcome of the annotations is never successful and from the log of the lambda function used for the post annotation I always receive this type of error:\n\n\n```\n{\n    \"labeling-job-name\": \"labeling-job-full-dataset-test-giusy-10\",\n    \"event-name\": \"ANNOTATION_CONSOLIDATION_LAMBDA_SCHEMA_MATCHING_FAILED\",\n    \"event-log-message\": \"ERROR: Annotation consolidation Lambda response did not match expected data format for line 1.\"\n}\n```\n\nBased on this guide (*https:\/\/docs.aws.amazon.com\/id_id\/sagemaker\/latest\/dg\/sms-custom-templates-step3-lambda-requirements.html*) I made sure that my lambda function returns:\n\nRESPONSE: \n\n```\n\n[\n  {\n    \"datasetObjectId\": \"1\",\n    \"consolidatedAnnotation\": {\n      \"content\": {\n        \"annotations\": {\n          \"relations\": [\n            {\n              \"subj\": \"CW\",\n              \"predicate\": \"adjust\",\n              \"obj\": \"key\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"with\",\n              \"obj\": \"right_hand\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"on\",\n              \"obj\": \"lock\"\n            }\n          ],\n          \"groundings\": {\n            \"pre_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 776.5,\n                \"top\": 219.5,\n                \"width\": 282.52,\n                \"height\": 246.5\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 716.4,\n                \"top\": 255.6,\n                \"width\": 93.60000000000002,\n                \"height\": 111.6\n              }\n            ],\n            \"pnr_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 974.16,\n                \"top\": 275.14,\n                \"width\": 287.21,\n                \"height\": 215.84\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 914.4,\n                \"top\": 291.6,\n                \"width\": 97.20000000000005,\n                \"height\": 90\n              }\n            ],\n            \"post_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 858.58,\n                \"top\": 240.54,\n                \"width\": 316.28,\n                \"height\": 209.37\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 741.6,\n                \"top\": 237.6,\n                \"width\": 61.19999999999993,\n                \"height\": 169.20000000000002\n              }\n            ]\n          },\n          \"timestamp\": \"0\",\n          \"clip_uid\": \"undefined\"\n        }\n      }\n    }\n  }\n]\n```\nI can't figure out how to avoid this type of error and make the annotations go through when the job is finished.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/training\/train\/data.csv'",
        "Question_created_time":1659016644599,
        "Question_last_edit_time":1667926663724,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuxHzyW8yRNqccGuILrD3vw\/filenotfounderror-errno-2-no-such-file-or-directory-opt-ml-input-data-training-train-data-csv",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":107,
        "Question_answer_count":1,
        "Question_body":"I want to run sagemaker and I adjusted my code according to the following example:\n\nstep 3: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-preprocess-data.html\n\nand\n\nstep 4: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\n\nbut I can't finish the training. Sagemaker returns the following error:\n\nErrorMessage \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/training\/train\/data.csv \n\nTo upload data on s3, in my code I set the paths as follows: \n\nbucket = sagemaker_session.default_bucket()\nprefix = 'sagemaker_forecasting_ml' \n\ntrain_1d.to_csv('train_1d.csv', sep=',', index=False, header=False)\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'data\/train_1d.csv')).upload_file('train_1d.csv') \n\ntraining_1d_s3_path = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/train.csv\"), content_type=\"csv\"\n)\n\nWhy sagemaker can't find a path when data is on it?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)",
        "Question_created_time":1659014190621,
        "Question_last_edit_time":1668610449635,
        "Question_link":"https:\/\/repost.aws\/questions\/QULlX63PqqQ1q-W5kCzwTKow\/importerror-cannot-import-name-dataclass-transform-from-typing-extensions-home-ec2-user-anaconda3-envs-tensorflow2-p38-lib-python3-8-site-packages-typing-extensions-py",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":593,
        "Question_answer_count":1,
        "Question_body":"Hi AWS, I am running the code for dalle mini to convert a text into an image. Here is the code for the same: \n\n\n```\nimport jax\nimport jax.numpy as jnp\nfrom huggingface_hub import hf_hub_url, cached_download, hf_hub_download\nimport shutil\nfrom dalle_mini import DalleBart, DalleBartProcessor\nfrom vqgan_jax.modeling_flax_vqgan import VQModel\nfrom typing_extensions import dataclass_transform\nfrom transformers import CLIPProcessor, FlaxCLIPModel\nfrom IPython.display import display\n\n# TF_CPP_MIN_LOG_LEVEL=0\nprint(jax.local_device_count())\nprint(jax.devices())\n\ndalle_mini_files_list = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'merges.txt', 'vocab.json', 'special_tokens_map.json', 'enwiki-words-frequency.txt', 'flax_model.msgpack']\n\nvqgan_files_list = ['config.json',  'flax_model.msgpack']\n\nfor each_file in dalle_mini_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/dalle-mini\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nfor each_file in vqgan_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/vqgan_imagenet_f16_16384\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/vqgan\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/huggingface-sagemaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nmodel, params = DalleBart.from_pretrained(    \n      DALLE_MODEL_LOCATION, revision=DALLE_COMMIT_ID, dtype=jnp.float32, _do_init=False,\n)\n\nVQGAN_LOCAL_REPO = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini\/vqgan'\nVQGAN_LCOAL_COMMIT_ID = None\nvqgan, vqgan_params = VQModel.from_pretrained(\n     VQGAN_LOCAL_REPO, revision=VQGAN_LCOAL_COMMIT_ID, _do_init=False\n)\n\n\nprint(model.config)\nprint(vqgan.config)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nprocessor = DalleBartProcessor.from_pretrained(\n     DALLE_MODEL_LOCATION, \n     revision=DALLE_COMMIT_ID)\n\nprint(processor)\n\n# # Works for all available devices to replicate the module\nfrom flax.jax_utils import replicate\nimport random\n\nparams = replicate(params)\nvqgan_params = replicate(vqgan_params)\n\n@partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(3, 4, 5, 6))\ndef p_generate(\n    tokenized_prompt, key, params, top_k, top_p, temperature, condition_scale\n):\n  return model.generate(\n      **tokenized_prompt,\n      prng_key=key,\n      params=params,\n      top_k=top_k,\n      top_p=top_p,\n      temperature=temperature,\n      condition_scale=condition_scale,\n  )\n\n#decode the images\n@partial(jax.pmap, axis_name=\"batch\")\ndef p_decode(indices, params):\n    return vqgan.decode_code(indices, params=params)\n\n\n# entering the prompts\nprompts = [\n    \"sunset over a lake in the mountains\",\n    \"the Eiffel tower landing on the moon\",\n]\n\ntokenized_prompts = processor(prompts)\ntokenized_prompt = replicate(tokenized_prompts)\n\n\n\n# create a random key\nseed = random.randint(0, 2**32 - 1)\nkey = jax.random.PRNGKey(seed)\n\n\nn_predictions = 4\n\n# We can customize generation parameters (see https:\/\/huggingface.co\/blog\/how-to-generate)\ngen_top_k = None\ngen_top_p = None\ntemperature = None\ncond_scale = 10.0\n\nprint(f\"Prompts: {prompts}\\n\")\n\nimages = []\nfor i in trange(max(n_predictions \/\/ jax.device_count(), 1)):\n    # get a new key\n    key, subkey = jax.random.split(key)\n    # generate images\n    encoded_images = p_generate(\n        tokenized_prompt,\n        shard_prng_key(subkey),\n        params,\n        gen_top_k,\n        gen_top_p,\n        temperature,\n        cond_scale,\n    )\n    # remove BOS\n    encoded_images = encoded_images.sequences[..., 1:]\n    # decode images\n    decoded_images = p_decode(encoded_images, vqgan_params)\n    decoded_images = decoded_images.clip(0.0, 1.0).reshape((-1, 256, 256, 3))\n    for decoded_img in decoded_images:\n        img = Image.fromarray(np.asarray(decoded_img * 255, dtype=np.uint8))\n        images.append(img)\n        display(img)\n```\n\nand the error I am getting is:\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n~\/SageMaker\/huggingface-sagemaker\/code\/inference.py in <module>\n      5 #import DalleBart\n      6 #from dalle_mini import DalleBart, DalleBartProcessor\n----> 7 from vqgan_jax.modeling_flax_vqgan import VQModel\n      8 from typing_extensions import dataclass_transform\n      9 #from transformers import CLIPProcessor, FlaxCLIPModel\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/vqgan_jax\/modeling_flax_vqgan.py in <module>\n      8 import jax.numpy as jnp\n      9 import numpy as np\n---> 10 import flax.linen as nn\n     11 from flax.core.frozen_dict import FrozenDict\n     12 \n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/__init__.py in <module>\n     16 \"\"\"Flax API.\"\"\"\n     17 \n---> 18 from . import core as core\n     19 from . import linen as linen\n     20 from . import optim as optim\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/__init__.py in <module>\n     26 )\n     27 \n---> 28 from .scope import (\n     29   Scope as Scope,\n     30   Array as Array,\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/scope.py in <module>\n     26 from flax import config\n     27 from flax import errors\n---> 28 from flax import struct\n     29 from flax import traceback_util\n     30 from .frozen_dict import freeze\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/struct.py in <module>\n     23 \n     24 import jax\n---> 25 from typing_extensions import dataclass_transform  # pytype: disable=not-supported-yet\n     26 \n     27 \n\nImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)\n\n\nPlease help me ASAP as I need to fix it urgently.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Register MultiDataModel in Model Registry",
        "Question_created_time":1658995322336,
        "Question_last_edit_time":1667925983790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJS8TpSeOStayVIvQFvpN0A\/register-multidatamodel-in-model-registry",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":132,
        "Question_answer_count":1,
        "Question_body":"I am working with Pipelines and provided Sagemaker Project template for building, training and deploying models in Sagemaker Studio and I need to create Multi Model Endpoint. I tried to register MultiDataModel and deploy it as is, but I can not register MultiDataModel and the error is: \n```\nAttributeError: 'MultiDataModel' object has no attribute 'vpc_config'.\n```\n\nI also tried to create pipeline model that contains SKlearn model for processing input data and MultiModelData, but I can not register the pipeline model and the error I got is: \n```\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the UpdatePipeline operation: Unable to parse pipeline definition. Unknown Argument member 'Mode'.\n```\n\n**Question:** Is there any possibility to work with MultiDataModel and multi model endpoint in Sagemake Projects Pipelines, because I will need that endpoint for production, or I will need to work with Notebook Instances If I want to use multi data endpoint?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio encountered an error when creating your project(github and codepipeline template)",
        "Question_created_time":1658949790257,
        "Question_last_edit_time":1668585903418,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOCKdskABQumCC7OnzBZR4g\/sagemaker-studio-encountered-an-error-when-creating-your-project-github-and-codepipeline-template",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_body":"![Studio encountered an error when creating your project](https:\/\/repost.aws\/media\/postImages\/original\/IMWYkHCNADT7ihgQuoRgh7nQ)\nI trying tutorial on \"MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline\". But I am getting  error as shown in image",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1658994240283,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1658994240283,
        "Answer_comment_count":1.0,
        "Answer_body":"Hello. It seems like you are having permission problems according to the snapshot you provided. \nIf you head to the Cloudformation service, you will probably get a better understanding of where the tamplate is failing. \nMake sure to have followed the prerequisites and check out this [section](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-templates-sm.html#sagemaker-projects-templates-update).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Retrieve Linear Learner Weights",
        "Question_created_time":1658859121171,
        "Question_last_edit_time":1667931344867,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXh8p1bCgT8a2gLfTDssY7w\/retrieve-linear-learner-weights",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":89,
        "Question_answer_count":1,
        "Question_body":"Unable to find the correct attribute to list Linear Learner weights. I fitted the estimator with the training data and linear Learner creates a weight vector w, which is fundamental in this algorithm. How can I print the resulting weight vector after training\/fitting?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Mandate user to enable encryption while Sagemaker notebook creation?",
        "Question_created_time":1658816453567,
        "Question_last_edit_time":1667925805329,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA0H60oMZTUy1JgSwFVXV4g\/mandate-user-to-enable-encryption-while-sagemaker-notebook-creation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":70,
        "Question_answer_count":2,
        "Question_body":"1.We would like to mandate user to enable KMS encryption while creating Sagemaker notebooks, I would like to know any methods via policy or any other way?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Line magic error in SageMaker Studio",
        "Question_created_time":1658670880505,
        "Question_last_edit_time":1667926273265,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZPg9WRBtSwGvYhI9-3tscA\/line-magic-error-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":119,
        "Question_answer_count":2,
        "Question_body":"Hi AWS, I am trying to create virtual environment in SageMaker Studio but while doing so I am experiencing a line magic function error. Also I am not able to import the libraries. I am attaching the error screenshot for the same.\n\n![Error Screenshot](https:\/\/repost.aws\/media\/postImages\/original\/IMTFfS3Ob9QBa1JU8Yui8ouw)\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error for Training job catboost-classification-model , ErrorMessage \"TypeError: Cannot convert 'xxx'' to float",
        "Question_created_time":1658510281955,
        "Question_last_edit_time":1668628769209,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVfbc_AsXRzaxAl69MMFlsQ\/error-for-training-job-catboost-classification-model-errormessage-typeerror-cannot-convert-xxx-to-float",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":5,
        "Question_view_count":531,
        "Question_answer_count":2,
        "Question_body":"When I performed the following AWS tutorial, I got an error when training the model.\n**https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb**\n\nThe error that occurred is \n```\nUnexpectedStatusException: Error for Training job jumpstart-catboost-classification-model-2022-07-22-07-33-18-038: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1 ErrorMessage \"TypeError: Cannot convert 'b'BROOKLYN'' to float\n```\n**These are all the files that I have upload in S3 bucket**  : \nAmazon S3 --> Buckets---> R-sandbox-sagemaker--->ml\/---> train\/\nand in the train folder 'data.csv' and 'categorical_index.json' are uploaded based on the mentioned tutorial. \ndata point \"BROOKLYN\" is in the categorical column, its index is already included in the JSON file to tell Catboost that it is categorical data. Data has 55 categorical data columns; only two of them are integers , all other string\n\nCould you give me some advice on how to solve it?\n\nAlso here all the code and full traceback of the issue: \n```\n!pip install sagemaker ipywidgets --upgrade \u2013quiet\nimport sagemaker, boto3, json\nfrom sagemaker import get_execution_role\n\naws_role = get_execution_role()\naws_region = boto3.Session().region_name\nsess = sagemaker.Session()\n\n##2.1 Retrieve Training Artifacts-\n#retrieve the training docker container, the training algorithm source, and the tabular algorithm. Note that model_version=\"*\" fetches the latest model.\n# Currently, not all the object detection models in jumpstart support finetuning. Thus, we manually select a model\n# which supports finetuning.\n\nfrom sagemaker import image_uris, model_uris, script_uris\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type,\n)\n# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n)\n# Retrieve the pre-trained model tarball to further fine-tune\ntrain_model_uri = model_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n)\n## 2.2 Set Training Parameters\n# Sample training data is available in this bucket\ntraining_data_bucket = \"R-sandbox-sagemaker\"\ntraining_data_prefix = \"ml\"\n\ntraining_dataset_s3_path = f\"s3:\/\/{training_data_bucket}\/{training_data_prefix}\"\n\noutput_bucket = sess.default_bucket()\noutput_prefix = \"jumpstart-example-tabular-training\"\n\ns3_output_location = f\"s3:\/\/{output_bucket}\/{output_prefix}\/output\"\n\nfrom sagemaker import hyperparameters\n# Retrieve the default hyper-parameters for fine-tuning the model\nhyperparameters = hyperparameters.retrieve_default(\n    model_id=train_model_id, model_version=train_model_version\n)\n\n# [Optional] Override default hyperparameters with custom values\nhyperparameters[\n    \"iterations\"\n] = \"500\"  # The same hyperparameter is named as \"iterations\" for CatBoost\nprint(hyperparameters)\n\n## 2.3. Train with Automatic Model Tuning\nfrom sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner\n\nuse_amt = True\nif train_model_id == \"lightgbm-classification-model\":\n    hyperparameter_ranges = {\n        \"learning_rate\": ContinuousParameter(1e-4, 1, scaling_type=\"Logarithmic\"),\n        \"num_boost_round\": IntegerParameter(2, 30),\n        \"early_stopping_rounds\": IntegerParameter(2, 30),\n        \"num_leaves\": IntegerParameter(10, 50),\n        \"feature_fraction\": ContinuousParameter(0, 1),\n        \"bagging_fraction\": ContinuousParameter(0, 1),\n        \"bagging_freq\": IntegerParameter(1, 10),\n        \"max_depth\": IntegerParameter(5, 30),\n        \"min_data_in_leaf\": IntegerParameter(5, 50),\n    }\nif train_model_id == \"catboost-classification-model\":\n    hyperparameter_ranges = {\n        \"learning_rate\": ContinuousParameter(0.00001, 0.1, scaling_type=\"Logarithmic\"),\n        \"iterations\": IntegerParameter(50, 1000),\n        \"early_stopping_rounds\": IntegerParameter(1, 10),\n        \"depth\": IntegerParameter(1, 10),\n        \"l2_leaf_reg\": IntegerParameter(1, 10),\n        \"random_strength\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n    }\n## 2.4. Start Training\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.utils import name_from_base\ntraining_job_name = name_from_base(f\"jumpstart-{'catboost-classification-model'}-training\")\n\n# Create SageMaker Estimator instance\ntabular_estimator = Estimator(\n    role=aws_role,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=training_instance_type,\n    max_run=360000,\n    #hyperparameters=hyperparameters,\n    output_path=s3_output_location,\n)\n# Launch a SageMaker Training job by passing s3 path of the training data\ntabular_estimator.fit(\n        {\"training\": training_dataset_s3_path}, logs=True, job_name=training_job_name\n    )\n```\n````\n2022-07-22 07:33:18 Starting - Starting the training job...\n2022-07-22 07:33:46 Starting - Preparing the instances for trainingProfilerReport-1658475198: InProgress\n2022-07-22 07:35:06 Downloading - Downloading input data...\n2022-07-22 07:35:46 Training - Downloading the training image...\n2022-07-22 07:36:11 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\n2022-07-22 07:36:14,025 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n2022-07-22 07:36:14,027 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2022-07-22 07:36:14,036 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n2022-07-22 07:36:14,041 sagemaker_pytorch_container.training INFO     Invoking user training script.\n2022-07-22 07:36:15,901 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/opt\/conda\/bin\/python3.8 -m pip install -r requirements.txt\nProcessing .\/catboost\/tenacity-8.0.1-py3-none-any.whl\nProcessing .\/catboost\/plotly-5.1.0-py2.py3-none-any.whl\nProcessing .\/catboost\/graphviz-0.17-py3-none-any.whl\nProcessing .\/catboost\/catboost-1.0.1-cp38-none-manylinux1_x86_64.whl\nProcessing .\/sagemaker_jumpstart_script_utilities-1.0.0-py2.py3-none-any.whl\nRequirement already satisfied: six in \/opt\/conda\/lib\/python3.8\/site-packages (from plotly==5.1.0->-r requirements.txt (line 2)) (1.16.0)\nRequirement already satisfied: numpy>=1.16.0 in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (1.19.1)\nRequirement already satisfied: scipy in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (1.7.1)\nRequirement already satisfied: matplotlib in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (3.4.3)\nRequirement already satisfied: pandas>=0.24.0 in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (1.2.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in \/opt\/conda\/lib\/python3.8\/site-packages (from pandas>=0.24.0->catboost==1.0.1->-r requirements.txt (line 4)) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in \/opt\/conda\/lib\/python3.8\/site-packages (from pandas>=0.24.0->catboost==1.0.1->-r requirements.txt (line 4)) (2021.3)\nRequirement already satisfied: pillow>=6.2.0 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (8.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (1.3.2)\ntenacity is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nInstalling collected packages: plotly, graphviz, sagemaker-jumpstart-script-utilities, catboost\nAttempting uninstall: plotly\nFound existing installation: plotly 5.3.1\nUninstalling plotly-5.3.1:\nSuccessfully uninstalled plotly-5.3.1\nSuccessfully installed catboost-1.0.1 graphviz-0.17 plotly-5.1.0 sagemaker-jumpstart-script-utilities-1.0.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https:\/\/pip.pypa.io\/warnings\/venv\n2022-07-22 07:36:32,568 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2022-07-22 07:36:32,604 sagemaker-training-toolkit INFO     Invoking user script\nTraining Env:\n{\n    \"additional_framework_parameters\": {},\n    \"channel_input_dirs\": {\n        \"model\": \"\/opt\/ml\/input\/data\/model\",\n        \"training\": \"\/opt\/ml\/input\/data\/training\"\n    },\n    \"current_host\": \"algo-1\",\n    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n    \"hosts\": [\n        \"algo-1\"\n    ],\n    \"hyperparameters\": {},\n    \"input_config_dir\": \"\/opt\/ml\/input\/config\",\n    \"input_data_config\": {\n        \"model\": {\n            \"ContentType\": \"application\/x-sagemaker-model\",\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        },\n        \"training\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        }\n    },\n    \"input_dir\": \"\/opt\/ml\/input\",\n    \"is_master\": true,\n    \"job_name\": \"jumpstart-catboost-classification-model-2022-07-22-07-33-18-038\",\n    \"log_level\": 20,\n    \"master_hostname\": \"algo-1\",\n    \"model_dir\": \"\/opt\/ml\/model\",\n    \"module_dir\": \"s3:\/\/jumpstart-cache-prod-us-east-1\/source-directory-tarballs\/catboost\/transfer_learning\/classification\/v1.1.3\/sourcedir.tar.gz\",\n    \"module_name\": \"transfer_learning\",\n    \"network_interface_name\": \"eth0\",\n    \"num_cpus\": 4,\n    \"num_gpus\": 0,\n    \"output_data_dir\": \"\/opt\/ml\/output\/data\",\n    \"output_dir\": \"\/opt\/ml\/output\",\n    \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\n    \"resource_config\": {\n        \"current_host\": \"algo-1\",\n        \"current_instance_type\": \"ml.m5.xlarge\",\n        \"current_group_name\": \"homogeneousCluster\",\n        \"hosts\": [\n            \"algo-1\"\n        ],\n        \"instance_groups\": [\n            {\n                \"instance_group_name\": \"homogeneousCluster\",\n                \"instance_type\": \"ml.m5.xlarge\",\n                \"hosts\": [\n                    \"algo-1\"\n                ]\n            }\n        ],\n        \"network_interface_name\": \"eth0\"\n    },\n    \"user_entry_point\": \"transfer_learning.py\"\n}\nEnvironment variables:\nSM_HOSTS=[\"algo-1\"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={}\nSM_USER_ENTRY_POINT=transfer_learning.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\nSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application\/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[\"model\",\"training\"]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=transfer_learning\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/jumpstart-cache-prod-us-east-1\/source-directory-tarballs\/catboost\/transfer_learning\/classification\/v1.1.3\/sourcedir.tar.gz\nSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"\/opt\/ml\/input\/data\/model\",\"training\":\"\/opt\/ml\/input\/data\/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application\/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"jumpstart-catboost-classification-model-2022-07-22-07-33-18-038\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/jumpstart-cache-prod-us-east-1\/source-directory-tarballs\/catboost\/transfer_learning\/classification\/v1.1.3\/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\nSM_USER_ARGS=[]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_MODEL=\/opt\/ml\/input\/data\/model\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nPYTHONPATH=\/opt\/ml\/code:\/opt\/conda\/bin:\/opt\/conda\/lib\/python38.zip:\/opt\/conda\/lib\/python3.8:\/opt\/conda\/lib\/python3.8\/lib-dynload:\/opt\/conda\/lib\/python3.8\/site-packages\nInvoking script with the following command:\n\/opt\/conda\/bin\/python3.8 transfer_learning.py\nINFO:root:Validation data is not found. 20.0% of training data is randomly selected as validation data. The seed for random sampling is 200.\nTraceback (most recent call last):\n  File \"_catboost.pyx\", line 2167, in _catboost.get_float_feature\nFile \"_catboost.pyx\", line 1125, in _catboost._FloatOrNan\n  File \"_catboost.pyx\", line 949, in _catboost._FloatOrNanFromString\nTypeError: Cannot convert 'b'BROOKLYN'' to float\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"transfer_learning.py\", line 221, in <module>\nru\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Input and Output interface for the CatBoost algorithm",
        "Question_created_time":1658463993278,
        "Question_last_edit_time":1667993528405,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0PVSBTSR4GvFO3E5FusCQ\/input-and-output-interface-for-the-catboost-algorithm",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":124,
        "Question_answer_count":2,
        "Question_body":"to set up CatBoost Classifier as a built-in algorithm, aws in this [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] suggested this notebook [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb] , my question is should I prepare inference file on top of the train.csv? if yes what is that and how it should be prepared?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1658504084534,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1658504084534,
        "Answer_comment_count":0.0,
        "Answer_body":"According to the documentation,[https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] 'The CatBoost built-in algorithm runs in script mode, but the training script is provided for you and there is no need to replace it. If you have extensive experience using script mode to create a SageMaker training job, then you can incorporate your own CatBoost training scripts.' Is the same with the Inference script, all provided artifacts.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Embedding categorical feature indices for the Catboost algorithm in AWS",
        "Question_created_time":1658411979721,
        "Question_last_edit_time":1668623458511,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8Tddi-4ZQCe_ZyuCyep7Hw\/embedding-categorical-feature-indices-for-the-catboost-algorithm-in-aws",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":287,
        "Question_answer_count":1,
        "Question_body":"According to the below link : https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb\n if the prediction includes categorical feature(s), a json-format file have to be used.\nPreparing the JSON file was confusing.\nhere is more information  provided there:\n\" If the predictors include categorical feature(s), a json-format file named 'categorical_index.json' should be included in the input directory to indicate the column index(es) of the categorical features. Within the json-format file, it should have a python directory where the key is a string of 'cat_index_list' and the value is a list of unique integer(s). Each integer in the list indicates the column index of categorical features in the 'data.csv'. The range of each integer should be more than 0 (index 0 indicates the target) and less than the total number of columns.\"\n\nCould you please advise me to create the JSON-format file for the categorical feature indices?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Build error on\u300eMLOps and integrations\u300f",
        "Question_created_time":1658392692433,
        "Question_last_edit_time":1667926554383,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoaMO0NfzTDKQ-sG2P0cbNw\/build-error-on%E3%80%8E-ml-ops-and-integrations%E3%80%8F",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":88,
        "Question_answer_count":1,
        "Question_body":"Hello, I have a question about one of the Workshops published in the AWS Workshop studio.\n\nI am going through the \"MLOps and integrations\" hands-on, but it is not working well during the process.\n\nHere is the [URL](https:\/\/catalog.us-east-1.prod.workshops.aws\/workshops\/50716cb8-6a42-427f-9eeb-0465dea6e95b\/en-US) for the hands-on.\n\nSpecifically, the build process performed when deploying the provided CloudFormation template will generate an error occurs.\nI think that the [URL](https:\/\/aws-mlops-samples.s3-eu-west-1.amazonaws.com\/mnist_output_10.png) for S3 listed in the distributed [source](https:\/\/static.us-east-1.prod.workshops.aws\/public\/3d5c4066-8fc6-4f66-ba73-d0cfe5e3c927\/static\/code\/ml-ops.zip)(source\/test.py) in the workshop is probably not valid.\n\nDo any of you know how to solve this problem?\n\n\n\n----Excerpts from the CodeBuild error log-----\n\nTraceback (most recent call last): \nFile \"source\/test.py\", line 20, in <module> \ntest_file, \n\nFile \"\/usr\/local\/lib\/python3.6\/site-packages\/wget.py\", line 526, in download \n(tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback) \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 248, in urlretrieve \nwith contextlib.closing(urlopen(url, data)) as fp: \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 223, in urlopen \nreturn opener.open(url, data, timeout) \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 532, in open \nresponse = meth(req, response) \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 642, in http_response \n'http', request, response, code, msg, hdrs) \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 570, in error \nreturn self._call_chain(*args) \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 504, in _call_chain \nresult = func(*args) \n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 650, in http_error_default\nraise HTTPError(req.full_url, code, msg, hdrs, fp) \n\nurllib.error.HTTPError: HTTP Error 403: Forbidden",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Question_created_time":1658343315554,
        "Question_last_edit_time":1668463143111,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/filenotfounderror-errno-2-no-such-file-or-directory-home-ec2-user-anaconda3-envs-python3-lib-python3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":167,
        "Question_answer_count":2,
        "Question_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1658369922829,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1658369922830,
        "Answer_comment_count":3.0,
        "Answer_body":"As illustrated [here in the docs for the algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html#catboost-modes), the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\n```\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n```\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with `sagemaker.__version__` and upgrade with `!pip install --upgrade sagemaker` if needed.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"InternalServerError with SageMaker Batch transform job",
        "Question_created_time":1658253043323,
        "Question_last_edit_time":1668053284860,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyua146pJQ0a7kJZK-JgGHw\/internalservererror-with-sagemaker-batch-transform-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":108,
        "Question_answer_count":0,
        "Question_body":"We have a few thousand models on SageMaker backed by our own containers on ECR. We have a problem with one of the models that when we start a Batch transform job with it the job is pending, then after ~20 minutes the job is marked as failed with \"InternalServerError: We encountered an internal error. Please try again.\". Attempting the job again didn't help. Is there any way to debug this?\n\nThe models execution role, container, security group and subnets are set correctly. The containers are all in the same repo which the execution role has permission to access. The container image is ~3GB, but is definitely not the largest one that we have run, and all our other models run fine with the same job parameters.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Should SageMaker Canvas region and S3 region be the same?",
        "Question_created_time":1658221373656,
        "Question_last_edit_time":1667926015744,
        "Question_link":"https:\/\/repost.aws\/questions\/QULHZtj6HwQReouXor72UuSg\/should-sagemaker-canvas-region-and-s3-region-be-the-same",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":112,
        "Question_answer_count":1,
        "Question_body":"Hi, I'm going to use the canvas by connecting to S3.\nWhen using sagemaker canvas, should the canvas region and S3 region be the same?\nThank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1658234155411,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1658234155411,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi, S3 does not have to be in the same region as SageMaker Canvas, but make sure your user has the correct permissions to access the bucket!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Notebook Kernel Dying During Training",
        "Question_created_time":1658076038858,
        "Question_last_edit_time":1667926071916,
        "Question_link":"https:\/\/repost.aws\/questions\/QUO7VnXi3jT76cpgAEPZ_k7g\/sagemaker-notebook-kernel-dying-during-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":160,
        "Question_answer_count":1,
        "Question_body":"I created a machine learning pipeline in a Sagemaker notebook instance (ml.m4.10xlarge, volume size: 16384GB EBS) where the kernel keeps restarting around 20% of the way through the process.  I want to upgrade my notebook instances to meet the requirements for my workflow but am a bit confused as to what instance type would be sufficient to complete the task and also where I can purchase notebook instances. \n\nAny help is appreciated and I am happy to provide further commentary as needed.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Policy that allows only one SSO user to access a resource",
        "Question_created_time":1658054479812,
        "Question_last_edit_time":1668442079409,
        "Question_link":"https:\/\/repost.aws\/questions\/QUruheXJHaQVu_S9LIzUyDAw\/policy-that-allows-only-one-sso-user-to-access-a-resource",
        "Question_score_count":1,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":164,
        "Question_answer_count":1,
        "Question_body":"We are in a process to move all of our IAM users to aws SSO \nwe used to have this policy for sagemaker :\n\n\n\"\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:DescribeNotebookInstance\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/${aws:username}*\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n                \"sagemaker:ListNotebookInstances\",\n                \"sagemaker:ListCodeRepositories\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n\"\n\n\nthis would give access to each user to use his\\hers own notebook\nnow on the new SSO permission set i gave this \n\n\n\n```\n\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"glue:CreateScript\",\n                \"secretsmanager:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\",\n                \"sagemaker:CreatePresignedDomainUrl\",\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\"\n```\n\n\n\n\nthis is what i tried but i cant make it work \nplease assist?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1658148377804,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1658148839830,
        "Answer_comment_count":1.0,
        "Answer_body":"Hello, \n\nI understand that you are currently trying to restrict access to Sagemaker notebook using SSO identity's UserID. \n\nCurrently, I leveraged your provided SSO Permission set and tweaked it out as you can see below, and finally tested it out on AWS SageMaker Console by logging in as an AWS SSO User, and was able to see successful start\/stop\/describing of the SageMaker notebook (with Tags - Owner:UserId) corresponding to the SSO UserId.\n\n```\n{\n\t\"Version\": \"2012-10-17\",\n\t\"Statement\": [\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"glue:CreateScript\",\n\t\t\t\t\"secretsmanager:*\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t},\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListTags\",\n\t\t\t\t\"sagemaker:DeleteNotebookInstance\",\n\t\t\t\t\"sagemaker:StopNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedNotebookInstanceUrl\",\n\t\t\t\t\"sagemaker:Describe*\",\n\t\t\t\t\"sagemaker:StartNotebookInstance\",\n\t\t\t\t\"sagemaker:UpdateNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedDomainUrl\"\n\t\t\t],\n\t\t\t\"Resource\": \"arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/*\",\n\t\t\t\"Condition\": {\n\t\t\t\t\"StringEquals\": {\n\t\t\t\t\t\"sagemaker:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"Sid\": \"VisualEditor1\",\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n\t\t\t\t\"sagemaker:ListNotebookInstances\",\n\t\t\t\t\"sagemaker:ListCodeRepositories\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t}\n\t]\n}\n```\n________________________________\n\nHowever, in case if this SSO User tried to stop any other Sagemaker notebooks, which didn't have the tags corresponding to their UserId, then the following errors were observed as expected behavior -\n\n```\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:StopNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/userachecking because no identity-based policy allows the sagemaker:StopNotebookInstance action\n\nor \n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:DescribeNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/Test1Check because no identity-based policy allows the sagemaker:DescribeNotebookInstance action\n```\n\nAlso, please note that unlike your provided IAM policy, your SSO permission set policy was missing the action - `sagemaker:ListNotebookInstances` which also raised an error for not being able to list out the notebook instances on AWS SageMaker Console in my testing. Hence, I had added the appropriate Sagemaker list actions to your permission set as well. \n\n**Additional Information** - \n\na. ${identitystore:UserId} -> Each user in the AWS SSO identity store is assigned a unique UserId. You can view the UserId for your users by using the AWS SSO console and navigating to each user or by using the DescribeUser API action. [1]\n\nb. ListNotebookInstances -> Returns a list of the SageMaker notebook instances in the requester's account in an AWS Region. [2]\n\nc. ResourceTag -> You can use the ResourceTag\/key-name condition key to determine whether to allow access to the resource based on the tags that are attached to the resource. [3][4]\n\nd. sagemaker:ResourceTag\/ -> Filters access by the preface string for a tag key and value pair attached to a resource [5]\n\ne. sagemaker:ResourceTag\/${TagKey} -> Filters access by a tag key and value pair [5]\n____________________________________\nI hope the shared information is insightful to your query. In case, if you have any other queries or concerns regarding AWS SSO or Sagemaker services or any account specific configuration that you would like to discuss, then please feel free to reach out to our team directly by creating a support case with our premium support team. \n\nHave a wonderful day ahead and stay safe. \n____________________________________\nReferences: \n\n[1] https:\/\/docs.aws.amazon.com\/singlesignon\/latest\/userguide\/using-predefined-attributes.html\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ListNotebookInstances.html\n\n[3] https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_tags.html\n\n[4] https:\/\/aws.amazon.com\/blogs\/security\/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles\/\n\n[5] https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html#amazonsagemaker-policy-keys",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":1.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Suspicious Billing with SageMaker",
        "Question_created_time":1658000861225,
        "Question_last_edit_time":1667925675733,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31OY4Rt9TVeEJBL4ctmgnA\/suspicious-billing-with-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_body":"Hi, I'm being charged up to USD 16,000 with AWS SageMaker when I'm not using anything. I keep receiving suspicious activity notification with my account. What should I do to terminate this SageMaker billing? Should I close my account?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Lambda Function to invoke sagemaker endpoint",
        "Question_created_time":1657965474513,
        "Question_last_edit_time":1667925676194,
        "Question_link":"https:\/\/repost.aws\/questions\/QU33wE3pnRS9Om2yfVt4EIAg\/lambda-function-to-invoke-sagemaker-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":349,
        "Question_answer_count":2,
        "Question_body":"Hi AWS, I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image. \n\nThe endpoint is generated using HuggingFace model.\n\nI need your help with the code and the steps to obtain it.\n\nThanks\nArjun Goel",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Endpoint is not created when deploying HuggingFace Model using it.",
        "Question_created_time":1657903012949,
        "Question_last_edit_time":1668301342434,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT4ywRDmOTO-8YSR4MBrVKg\/sagemaker-endpoint-is-not-created-when-deploying-huggingface-model-using-it",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":121,
        "Question_answer_count":1,
        "Question_body":"I am trying to deploy the HuggingFace model onto sagemaker. Here is the link for the model: https:\/\/huggingface.co\/dalle-mini\/dalle-mini\n\nI am testing in my personal account and here is the code for the same:\n\n\n```\nfrom sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nsess = sagemaker.Session()\n# sagemaker session bucket -> used for uploading data, models and logs\n# sagemaker will automatically create this bucket if it not exists\nsagemaker_session_bucket='sagemaker-hugging-face-model-demo'\nif sagemaker_session_bucket == 'sagemaker-hugging-face-model-demo' and sess is not None:\n    # set to default bucket if a bucket name is not given\n    sagemaker_session_bucket = sess.default_bucket()\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n\n\nhub = {\n    'HF_MODEL_ID':'dalle-mini\/dalle-mini',\n    'HF_TASK':'Text-to-image'\n}\n\nhuggingface_model = HuggingFaceModel(\n  env=hub,\n  role=role,\n  #image_uri=\"428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face\",\n  transformers_version=\"4.6.1\",     # transformers version used\n  pytorch_version=\"1.7\",          # pytorch version used\n  py_version='py36'\n)\n\n# deploy model to Sagemaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\n```\n\n\n\nWhen I am trying to create the sagemaker endpoint I am experiencing the error: `ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Requested image 428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face not found.`\n\n\nAlso I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image.\nE.g. --> The text `Sun is shining` should be transformed to image after the lambda function invokes the sagemaker endpoint.\n\nAlso need to know what should be the ContentType for image.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"3d Ground truth labelling job issue in bbox",
        "Question_created_time":1657799060933,
        "Question_last_edit_time":1667926537729,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnfsUXUBLT0i2HEn3-CW3jQ\/3d-ground-truth-labelling-job-issue-in-bbox",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":0,
        "Question_body":"Hi, I have created a 3d labelling job for 100 frames and got the job running. When I started annotating the objects with cuboid I am facing 2 issues,\n\n1. The size of the bounding box for respective object is changing (increased or decreased) as I move to the next frames and drawing the bounding box (with increased or decreased size to the earlier frames). I believe this could be because of autofill functionality. Please confirm me whether I have an option to off the autofill functionality.\n2. The bounding boxes are getting scattered or displaced each time I login and start working. If I annotate the objects for 10 frames, save it and logout for today, later when I login back the bounding boxes were scattered for couple of frames. Can you please let me know what could be done to reduce this issue.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[bug report] Sagemaker data wrangler: An error occurred loading this view",
        "Question_created_time":1657768857538,
        "Question_last_edit_time":1668247449186,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr_PSwJDeTxG5mPrjSYKZrA\/bug-report-sagemaker-data-wrangler-an-error-occurred-loading-this-view",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":104,
        "Question_answer_count":1,
        "Question_body":"Hello,\n\nI import my data from Athena, then add a new custom data transform. As soon as I click on the \"Custom transform\" option, the error occurs with message: **An error occurred loading this view**. There is no other useful message to find out the problem. Please tell me how to troubleshot or fix this problem.\n\n![Enter image description here](https:\/\/repost.aws\/media\/postImages\/original\/IMT74o2F2nS8KYUj3jxJ4mjA)\n\nThank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Increase Limit on Lineage Tracking entities for sagemaker",
        "Question_created_time":1657755893027,
        "Question_last_edit_time":1667926686273,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwNhM5g6qRN2orm9ttoPV6w\/increase-limit-on-lineage-tracking-entities-for-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_body":"My Team is trying to onboard to sagemaker lineage tracking entities and we basically track models, datasets, associations between these entities all the way to endpoints. Currently, we have been using another system for the same. We currently require that we create dataset entities prior to our training job so that we can use this to reference during our training jobs. The problem comes with the constraints on the amount of manual entities that can be created. As per the doc, the limits are \n\n\nMaximum number of manually created lineage entities\n    Actions: 3000\n    Artifacts: 6000\n    Associations: 6000\n    Contexts: 500\n\nOur current system holds about 1500 datasets and 1000 models which means that we might hit the limit in the near future if we onboard to sagemaker. Is there a provision to increase the limits on these? I am not sure why these limits are placed. These entities must be pretty cheap to store. Please let me know if there is any way to get this limit increased",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Question_created_time":1657750167416,
        "Question_last_edit_time":1667926330662,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging.\nWhen this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible.\nI was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists.\nApproaches I have tried so far:\n- Shutdow and start kernel\n- Restart kernel\n- Restart kernel and clear outputs\n- Log out and Login (from Sagemaker)\n- Log out and Login (from AWS)\n- Change region\n- Trying a different browser (I tried Chrome and Firefox)\n- Trying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1657857148682,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1657857148683,
        "Answer_comment_count":1.0,
        "Answer_body":"The most likely cause of this from my experience is a **(very) large number of active git changes**.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example [in this GitHub issue](https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/667) - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to [gitignore](https:\/\/git-scm.com\/docs\/gitignore) a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\n- Use the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\n- Now the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\n    - You could use a System Terminal, `cd` to the affected folder and run `git status` to see where the many changes are hiding, if you're not sure already\n    - Add a `.gitignore` file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, `.gitignore` is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. `cp myrepo\/.gitignore gitignore.txt` to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then `mv gitignore.txt myrepo\/.gitignore` to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like `node_modules`) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without `.gitignore`.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker XGBoost batch transform AttributeError",
        "Question_created_time":1657717396655,
        "Question_last_edit_time":1668629881052,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3Hva4yNpSfOtRQTjKVMvvg\/sagemaker-xgboost-batch-transform-attributeerror",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":129,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nAfter training XGBoost model using SageMaker inbuilt algorithm, I am trying to perform batch transform operation. I am doing the same steps as for linear learner model which worked fine there.  However in case of XGBoost I get a following error while creating a transformer:\n\n```\nAttributeError: module 'sagemaker' has no attribute 'utils'\n```\nThe piece of code causing the error is: \n```\nxgb_transformer = xgb_estimator.transformer(\n    instance_count = 1, \n    instance_type = 'ml.m4.10xlarge',\n    output_path = '{}\/{}'.format(output_path,'output')\n)\n```\nI use '1.5-1' version of XGBoost as image in training, and 2.86.2 version of SageMaker. \n\nAny help would be highly appreciated!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Export Autopilot model to GovCloud region",
        "Question_created_time":1657645831488,
        "Question_last_edit_time":1667926196941,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV6_OvmWMRjiuQIx82Z4_Eg\/export-autopilot-model-to-govcloud-region",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":66,
        "Question_answer_count":0,
        "Question_body":"Hi \nAs AWS Sagemaker autopilot is not available in GovCloud region, is it possible to export a model trained on non-GovCloud environment in GovCloud environment.\n\nWhat I have done:\n\n- Ran Autopilot on non-GovCloud environment\n- I was able to download the output model.joblib(preprocessing) and xgboost models from output bucket in S3 bucket\n- I was not able to load the model.joblib preprocessing model since the sagemaker-sklearn-extention gives symbol not found error(https:\/\/issuehint.com\/issue\/awslabs\/ml-io\/28)\n\nThanks for your help and insights in exporting the model to GovCloud environment.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"OSError: [Errno 28] No space left on device -- PyTorch, CNN, estimator",
        "Question_created_time":1657555011984,
        "Question_last_edit_time":1668581166018,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuRZhu6ZpTPSmVO-4P2GDmQ\/oserror-errno-28-no-space-left-on-device-pytorch-cnn-estimator",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1174,
        "Question_answer_count":1,
        "Question_body":"hey there, I'm training a convolutional neural network (CNN) on a large dataset (10k images - 50 GB) stored on S3 bucket using estimator(sagemaker infrastructure) . everything works well when I work with 2000 images which has the total size of almost 5-10 GB. however, I get an error when I increase number of images to 3000 or more. the error indicates that there is **no space left on device**. I also attached the estimator setup to this message, as you can see I am using ml.g4dn.12xlarge instance which has 192 GB of memory!! I also increased the volume size to 900 GB. I still don't know why I am getting space\/storage error!! I know that error is related the function \"_get_train_data_loader\" in which it is trying to download the images!! I was reading somewhere that EFS (elastic file system) might help with this issue, if so, I don't know how to specify it in the estimator. estimator = PyTorch(\n    entry_point=\"pbdl_sm.py\",\n    role=role,\n    framework_version=\"1.4.0\",\n    py_version=\"py3\",\n    instance_count=1,\n    instance_type=\"ml.g4dn.12xlarge\",\n    volume_size = 900,\n    hyperparameters={\"epochs\": 6, \"backend\": \"gloo\",\"lr\": 0.001,\"train_size\":2900,\"n_realz\":3000},\n)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[Sagemaker] - Does Sagemaker support Xpress model",
        "Question_created_time":1657298680086,
        "Question_last_edit_time":1668092327065,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOPI761YwSQWqpUusp3Mo_g\/sagemaker-does-sagemaker-support-xpress-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":98,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nWe have a use case to use Sagemaker towards running Xpress based science models. Can anyone please tell what are the pros and cons of using sagemaker towards running the Xpress models\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to feed seed code to GitHub Repository from Sagemaker Projects Organization Template created with Service Catalog?",
        "Question_created_time":1657259337697,
        "Question_last_edit_time":1668617915381,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_Y4T-A3aQySFeRr3feBscA\/how-to-feed-seed-code-to-github-repository-from-sagemaker-projects-organization-template-created-with-service-catalog",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":416,
        "Question_answer_count":1,
        "Question_body":"The objective is to replicate \"MLOps template for model building, training, and deployment with third-party Git repositories using Jenkins\" builtin Sagemaker Project template. I want to feed custom seed code to the Github repository each time a project is created using my organization custom template instead of the default seed code that the builtin template feeds.\n\nI am able to create the custom template using service catalog but I could not find a solution for feeding the seed code to github repo. So, I decided to see how the built in project template is doing this and it is using resources from this bucket \"s3:\/\/sagemaker-servicecatalog-seedcode-us-east-1\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\" but I could not access it. I am not sure how to achieve the objective?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1657560195771,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1657560195771,
        "Answer_comment_count":1.0,
        "Answer_body":"You can download the seed package using awscli s3 cp <s3_uri> <target_path> or by using this URL: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\n\nThis .zip is used by CodeBuild that is called when the template is deployed (by a lambda mapped to a CFN custom component). If you take a look in the template you'll find a component named \"SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker\". You can find some env vars defined for this component like: SEEDCODE_BUCKET_NAME and SEEDCODE_BUCKET_KEY. These vars point to an S3 uri that has another .zip file with the content of the seed for the git repo. If you get the default values defined there you can re-create the URL and download the .zip file as well:\nhttps:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/toolchain\/model-building-workflow-jenkins-v1.0.zip\n\nSo, in the end, if you want to change the content that is pushed to the git repo, you can redefine these 2 vars and point to an S3 path that contains a .zip file you created.\n\nBonus: If you're a curious person, I recommend you to take a look at the .java file (src\/main\/java\/GitRepositorySeedCodeBootStrapper.java) inside the .zip of the CodeBuild .zip for you to understand what it does to prepare the git repo like: download a .zip, unpack it, commit\/push to the git repo.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"\"Failure reason Image size 12704675783 is greater than supported size 10737418240\" when creating serverless endpoint in SageMaker.",
        "Question_created_time":1657243407002,
        "Question_last_edit_time":1668608864473,
        "Question_link":"https:\/\/repost.aws\/questions\/QU90699ONgQD2t2HUKzm9AUA\/failure-reason-image-size-12704675783-is-greater-than-supported-size-10737418240-when-creating-serverless-endpoint-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":624,
        "Question_answer_count":2,
        "Question_body":"How to reproduce the error:\nWe want to run Python Inference in SageMaker. Because our model is pre-trained out side the SageMaker and has some special logic, so we need to create customer image. \nWe see the document https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html#prebuilt-containers-extend-tutorial \nWe use the 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker to be the base image. We wrote a dockerfile and use \"docker build\" to create a new image. Also, use \"docker push\" to push new image to Amazon ECR. We pushed it to 935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage\nThen, we follow the document: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html \nThen, we went to SageMaker console https:\/\/us-east-1.console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/models  We created model. We input the \"935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage\" of our new image to \"Location of inference code image\". Then, we create Endpoint configuration. Then, we create Endpoint. But the Endpoint shows \"Failure reason Image size 12704675783 is greater than supported size 10737418240\".",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cant generate XGBoost training report in sagemaker, only profiler_report.",
        "Question_created_time":1657222531986,
        "Question_last_edit_time":1668531918342,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskI-0YIvQ_2GRSkzyGiD2A\/cant-generate-xgboost-training-report-in-sagemaker-only-profiler-report",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":90,
        "Question_answer_count":0,
        "Question_body":"I am trying to generate the XGBoost training report to see feature importances however the following code only generates the profiler report.\n\n```\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker\/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation\/val.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('..\/Data\/Base_Model_Data\/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, \"validation\": s3_input_val}, wait=True)\n```\nWhen Checking the output path via:\n```\nrule_output_path = xgb.output_path + \"\/\" + xgb.latest_training_job.job_name + \"\/rule-output\"\n! aws s3 ls {rule_output_path} --recursive\n```\nWhich Outputs:\n```\n2022-07-07 18:40:27     329715 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.html\n2022-07-07 18:40:26     171087 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.ipynb\n2022-07-07 18:40:23        191 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/BatchSize.json\n2022-07-07 18:40:23        199 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/CPUBottleneck.json\n2022-07-07 18:40:23        126 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/Dataloader.json\n2022-07-07 18:40:23        127 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/GPUMemoryIncrease.json\n2022-07-07 18:40:23        198 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/IOBottleneck.json\n2022-07-07 18:40:23        119 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LoadBalancing.json\n2022-07-07 18:40:23        151 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LowGPUUtilization.json\n2022-07-07 18:40:23        179 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/MaxInitializationTime.json\n2022-07-07 18:40:23        133 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallFrameworkMetrics.json\n2022-07-07 18:40:23        465 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallSystemUsage.json\n2022-07-07 18:40:23        156 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/StepOutlier.json\n```\nAs you can see only the profiler report in created which does not interest me. Why isn't there a CreateXGBoostReport folder generated with the training report? How do I generate this\/what am I missing?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to resolve the private dns name of a Sagemaker runtime VPC interface endpoint",
        "Question_created_time":1657193804686,
        "Question_last_edit_time":1668594825292,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL78EiZhMTeq3v_FVsB-kXQ\/unable-to-resolve-the-private-dns-name-of-a-sagemaker-runtime-vpc-interface-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":476,
        "Question_answer_count":2,
        "Question_body":"I deployed interface endpoints for multiple AWS services into a dedicated subnet in my VPC. Besides a Sagemaker runtime interface endpoint I also created endpoints for CloudWatch logs, KMS and more.\n\nThe resolving of the service domain name (e.g. kms.eu-central-1.amazonaws.com) works for all endpoints, except for the Sagemaker runtime (e.g. runtime.sagemaker.eu-central-1.amazonaws.com).The endpoint specific domain names of the Sagemaker runtime endpoint also work, only the resolving of runtime.sagemaker.eu-central-1.amazonaws.com fails with no answers from the DNS server.\n\nI tried multiple times to recreate the endpoint, but that didn't help either. The resolving of the domain works if I deploy the endpoint into another test VPC.\n\nAny ideas on what could be wrong? Thanks in advance!\nBert",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Not able to convert Hugging Face fine-tuned BERT model into AWS Neuron",
        "Question_created_time":1657030067879,
        "Question_last_edit_time":1668512600335,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzDs6ITDqQYegTLVfTmNKOA\/not-able-to-convert-hugging-face-fine-tuned-bert-model-into-aws-neuron",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":333,
        "Question_answer_count":1,
        "Question_body":"Hi Team,\n\nI have a fine-tuned BERT model which was trained using following libraries.\ntorch == 1.8.1+cu111\ntransformers == 4.19.4\n\nAnd not able to convert that fine-tuned BERT model into AWS neuron and getting following compilation errors. Could you please help me to resolve this issue?\n\n**Note:** Trying to compile BERT model on SageMaker notebook instance and with \"conda_python3\" conda environment.\n\n**Installation:**\n\n#### Set Pip repository  to point to the Neuron repository\n!pip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\n\n#### Install Neuron PyTorch - Note: Tried both options below.\n\"#!pip install torch-neuron==1.8.1.* neuron-cc[tensorflow] \"protobuf<4\" torchvision sagemaker>=2.79.0 transformers==4.17.0 --upgrade\"\n!pip install --upgrade torch-neuron neuron-cc[tensorflow] \"protobuf<4\" torchvision\n\n---------------------------------------------------------------------------------------------------------------------------------------------------\n\n**Model compilation:**\n```\nimport os\nimport tensorflow  # to workaround a protobuf version conflict issue\nimport torch\nimport torch.neuron\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_path = 'model\/' # Model artifacts are stored in 'model\/' directory\n\n# load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, torchscript=True)\n\n# create dummy input for max length 128\ndummy_input = \"dummy input which will be padded later\"\nmax_length = 128\nembeddings = tokenizer(dummy_input, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\nneuron_inputs = tuple(embeddings.values())\n\n# compile model with torch.neuron.trace and update config\nmodel_neuron = torch.neuron.trace(model, neuron_inputs)\nmodel.config.update({\"traced_sequence_length\": max_length})\n\n# save tokenizer, neuron model and config for later use\nsave_dir=\"tmpd\"\nos.makedirs(\"tmpd\",exist_ok=True)\nmodel_neuron.save(os.path.join(save_dir,\"neuron_model.pt\"))\ntokenizer.save_pretrained(save_dir)\nmodel.config.save_pretrained(save_dir)\n```\n---------------------------------------------------------------------------------------------------------------------------------------------------\n**Model artifacts:** We have got this model artifacts from multi-label topic classification model.\n\nconfig.json\nmodel.tar.gz\npytorch_model.bin\nspecial_tokens_map.json\ntokenizer_config.json\ntokenizer.json\n\n---------------------------------------------------------------------------------------------------------------------------------------------------\n**Error logs:**\n\n```\nINFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https:\/\/github.com\/aws\/aws-neuron-sdk\/blob\/master\/release-notes\/neuron-cc-ops\/neuron-cc-ops-pytorch.md)\nINFO:Neuron:Number of arithmetic operators (pre-compilation) before = 565, fused = 548, percent fused = 96.99%\nINFO:Neuron:Number of neuron graph operations 1601 did not match traced graph 1323 - using heuristic matching of hierarchical information\nWARNING:tensorflow:From \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/ops\/aten.py:2022: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nINFO:Neuron:Compiling function _NeuronGraph$698 with neuron-cc\nINFO:Neuron:Compiling with command line: '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]} --verbose 35'\nINFO:Neuron:Compile command returned: -9\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$698; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]}' --verbose 35\nTraceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py\", line 382, in op_converter\n    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/decorators.py\", line 220, in trace\n    'neuron-cc failed with the following command line call:\\n{}'.format(command))\nsubprocess.SubprocessError: neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]}' --verbose 35\nINFO:Neuron:Number of arithmetic operators (post-compilation) before = 565, compiled = 0, percent compiled = 0.0%\nINFO:Neuron:The neuron partitioner created 1 sub-graphs\nINFO:Neuron:Neuron successfully compiled 0 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 0.0%\nINFO:Neuron:Compiled these operators (and operator counts) to Neuron:\nINFO:Neuron:Not compiled operators (and operator counts) to Neuron:\nINFO:Neuron: => aten::Int: 97 [supported]\nINFO:Neuron: => aten::add: 39 [supported]\nINFO:Neuron: => aten::contiguous: 12 [supported]\nINFO:Neuron: => aten::div: 12 [supported]\nINFO:Neuron: => aten::dropout: 38 [supported]\nINFO:Neuron: => aten::embedding: 3 [not supported]\nINFO:Neuron: => aten::gelu: 12 [supported]\nINFO:Neuron: => aten::layer_norm: 25 [supported]\nINFO:Neuron: => aten::linear: 74 [supported]\nINFO:Neuron: => aten::matmul: 24 [supported]\nINFO:Neuron: => aten::mul: 1 [supported]\nINFO:Neuron: => aten::permute: 48 [supported]\nINFO:Neuron: => aten::rsub: 1 [supported]\nINFO:Neuron: => aten::select: 1 [supported]\nINFO:Neuron: => aten::size: 97 [supported]\nINFO:Neuron: => aten::slice: 5 [supported]\nINFO:Neuron: => aten::softmax: 12 [supported]\nINFO:Neuron: => aten::tanh: 1 [supported]\nINFO:Neuron: => aten::to: 1 [supported]\nINFO:Neuron: => aten::transpose: 12 [supported]\nINFO:Neuron: => aten::unsqueeze: 2 [supported]\nINFO:Neuron: => aten::view: 48 [supported]\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-1-97bba321d013> in <module>\n     18 \n     19 # compile model with torch.neuron.trace and update config\n---> 20 model_neuron = torch.neuron.trace(model, neuron_inputs)\n     21 model.config.update({\"traced_sequence_length\": max_length})\n     22 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py in trace(func, example_inputs, fallback, op_whitelist, minimum_segment_size, subgraph_builder_function, subgraph_inputs_pruning, skip_compiler, debug_must_trace, allow_no_ops_on_neuron, compiler_workdir, dynamic_batch_size, compiler_timeout, _neuron_trace, compiler_args, optimizations, verbose, **kwargs)\n    182         logger.debug(\"skip_inference_context - trace with fallback at {}\".format(get_file_and_line()))\n    183         neuron_graph = cu.compile_fused_operators(neuron_graph, **compile_kwargs)\n--> 184     cu.stats_post_compiler(neuron_graph)\n    185 \n    186     # Wrap the compiled version of the model in a script module. Note that this is\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py in stats_post_compiler(self, neuron_graph)\n    491         if succesful_compilations == 0 and not self.allow_no_ops_on_neuron:\n    492             raise RuntimeError(\n--> 493                 \"No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\")\n    494 \n    495         if percent_operations_compiled < 50.0:\n\nRuntimeError: No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\n```\n---------------------------------------------------------------------------------------------------------------------------------------------------\n\nThanks a lot.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to search for Amazon SageMaker Models using Tags?",
        "Question_created_time":1656956445714,
        "Question_last_edit_time":1667926403593,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfe51Pe1zRKqts-P7SZVuSA\/how-to-search-for-amazon-sagemaker-models-using-tags",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":170,
        "Question_answer_count":2,
        "Question_body":"When I open the Model details in the Amazon SageMaker console, the details clearly show Tags that have been added to the model during it's creation.\n\n| Tags |\n| Key |\n| sagemaker:project-name |\n| aws:cloudformation:stack-name|\n| sagemaker:deployment-stage|\n| sagemaker:deployment-stage|\n\nBut the Search API provided by Amazon SageMaker, [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html]()\n\nmentions only the following resources can be searched for using Tags: \nValid Values: TrainingJob | Experiment | ExperimentTrial | ExperimentTrialComponent | Endpoint | ModelPackage | ModelPackageGroup | Pipeline | PipelineExecution | FeatureGroup | Project | FeatureMetadata\n\nI wish to obtain Model details, not the ModelPackageGroup\/ModelPackage details, using Tags so if there is a way to do that, please share. Also if there is no way to obtain it using Tags, like the Search API Documentation suggests, what is the purpose of the Tags still present in the Model details?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to set spark configuration parameters in PySparkProcessor() in sagemaker processing job?",
        "Question_created_time":1656608387774,
        "Question_last_edit_time":1668533410958,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhyE6RyH-QwaUsVFnslKjlg\/how-to-set-spark-configuration-parameters-in-pysparkprocessor-in-sagemaker-processing-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":474,
        "Question_answer_count":1,
        "Question_body":"Hi folks,\nI'm trying to set the spark executor instances & memory, driver memory and switch of dynamic allocation. What is the correct way to do it?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS sagemaker abalone example pipeline endpoint json rejected",
        "Question_created_time":1656607883243,
        "Question_last_edit_time":1668519247941,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJRDfNUpdR-WQgjsw_UOi5g\/aws-sagemaker-abalone-example-pipeline-endpoint-json-rejected",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":775,
        "Question_answer_count":1,
        "Question_body":"We've just created a train\/build\/deploy template in AWS SageMaker which provides a deployment of an Abalone model.  We're trying to test it via the Test Inference endpoint, but the JSON there is rejected with the following message:\n\n> Error invoking endpoint: Received client error (415) from model with message \"application\/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, application\/x-recordio-protobuf.\". See https:\/\/eu-west-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/USEngProbOfConversion-staging in account 607522716587 for more information.\n\nHowever the Test Inference endpoint only allows us to hit the endpoint with JSON - what can we do?  Here's a screenshot (if this dropbox embed works):\n\n![](https:\/\/www.dropbox.com\/s\/x79l8xxqklgbxx4\/Screenshot%202022-06-30%20at%2017.48.35.png?raw=1)\n\nbut that's not working so here's the request dump:\n\n```\n{\n  \"body\": {\n    \"s-x\": \"M\",\n    \"length\": 3,\n    \"diameter\": 5,\n    \"height\": 7,\n    \"whole_weight\": 45,\n    \"shucked_weight\": 34,\n    \"viscera_weight\": 23,\n    \"shell_weight\": 76\n  },\n  \"contentType\": \"application\/json\",\n  \"endpointName\": \"USEngProbOfConversion-staging\",\n  \"customURL\": \"\",\n  \"customHeaders\": [\n    {\n      \"Key\": \"sm_endpoint_name\",\n      \"Value\": \"USEngProbOfConversion-staging\"\n    },\n    {\n      \"Key\": \"\",\n      \"Value\": \"\"\n    }\n  ]\n}\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Use S3 as a git repo",
        "Question_created_time":1656607653458,
        "Question_last_edit_time":1668543470116,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUeFHB_qvQw67d9knOyw1Ig\/use-s3-as-a-git-repo",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":1357,
        "Question_answer_count":2,
        "Question_body":"I have a sagemaker notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. I would like to use that 1 s3 bucket as a place to house git repos, ideally I would like to be able to pull\/push to repos in that bucket from other sagemaker notebooks or ec2 instances that have connections to that bucket. Has anyone tried this before?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Question_created_time":1656580942341,
        "Question_last_edit_time":1667931210106,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":128,
        "Question_answer_count":1,
        "Question_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours  ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you!\nRuoy",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1656585271689,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1656585271689,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi Ruoy!\nMy advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern.\nIf you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support!\nHope this helps",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Studio JupyterServer App does not load",
        "Question_created_time":1656519218822,
        "Question_last_edit_time":1667925716597,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5Da9xot8TwST0MP_8uRV2A\/sagemaker-studio-jupyterserver-app-does-not-load",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":144,
        "Question_answer_count":2,
        "Question_body":"After months of seamless work in SageMaker Studio, the JupyterServer App won't load for the last 4 days. The Control Panel shows that the JupyterServer is in \"Pending\" or \"Failed\" state after I try to launch the app. When clicking \"Launch app\", the screen shows that: \n* \"The JupyterServer app default encountered a problem and was stopped.\"\n* The \"Restart Now\" button is visible, but pressing this results in the same behaviour.\nI created a new JupyterServer App and it experiences the same problem under that account. I use a different account for another project and the JupyterServer under that account works perfectly. I even mounted the EFS associated with the App on an EC2 instance and deleted some files to reduce the EFS volume but it did not help (it was 995 MB and as far as I know, 5GB is the default limit).\n\nI found a post stating the same problem from 2 years ago, but could not follow the advice to delete the app and create a new one, since the Delete option is not available in the Action dropdown ([https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load]()). All apps that I create is \"default\".\n\nPlease help, how could I overcome this and access Jupyter Lab again?\nThank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I connect a Sagemaker \"Studio\" instance to a private github repository?",
        "Question_created_time":1656504273482,
        "Question_last_edit_time":1667925937845,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH33ZXpiAQ_aV2TesXBNOBw\/can-i-connect-a-sagemaker-studio-instance-to-a-private-github-repository",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":440,
        "Question_answer_count":1,
        "Question_body":"I've successfully connected a Sagemaker \"notebook\" to a private github repository, but wondering if it isn't possible for a studio instance?  Failing that is there an easy way to get the remote codecommit git url for an existing \"studio\" instance so that code there can at least be pulled to my local machine?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Studio Jupyterlab 3.0 working poorly with SM Resources UI",
        "Question_created_time":1656420408518,
        "Question_last_edit_time":1668556155068,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOmPLv2iwRyuEcQom58DgbA\/sagemaker-studio-jupyterlab-3-0-working-poorly-with-sm-resources-ui",
        "Question_score_count":3,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":444,
        "Question_answer_count":1,
        "Question_body":"Hi all,\n\nSince Jupyterlab 3.0 was finally released on SM Studio, we have been super happy with it, however, for reasons unknown to us, the jupterlab interface works very poorly with SM resources, the following phenomenon have been observes:\n1. It takes FOREVER to load the page for SM pipelines, and half the time it reports error (\"Error listing pipeline executions: Rate exceeded\")\n2. Changing instance type and size for a notebook is now super laggy, and do not work half the time \n\nAnyone knows if this is merely a lack of optimisation on the service team's part or is there something I can do to stop this behaviour? it's making our work very slow and unbearable, we don't want to revert back to 1.0 so any help would be greatly appreciated!\n\nBest,\nRUoy",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how can I use sagemaker_sklearn_extension in Sagemaker job?",
        "Question_created_time":1656223637583,
        "Question_last_edit_time":1668577031882,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMUk4WTgJRD68w1PvBudSow\/how-can-i-use-sagemaker-sklearn-extension-in-sagemaker-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":145,
        "Question_answer_count":1,
        "Question_body":"I'm creating a data processing job in sagemaker notebook:\n\n\n```\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsklearn_processor = SKLearnProcessor(role=role,\n                                     base_job_name='end-to-end-ml-sm-proc',\n                                     instance_type='ml.m5.large',\n                                     instance_count=1,\n                                     framework_version='0.23-1')\n```\n\n\nmy processing script uses :\n\n```\nfrom sagemaker_sklearn_extension.decomposition import RobustPCA\n```\n\n\nand I get an error during the job exectution:\n\n```\nTraceback (most recent call last):\n  File \"\/opt\/ml\/processing\/input\/code\/preprocessor.py\", line 14, in <module>\n    from sagemaker_sklearn_extension.decomposition import RobustPCA\nModuleNotFoundError: No module named 'sagemaker_sklearn_extension'\n```\n\n\nas far as I understrand :\nframework_version='0.23-1' should make sagemaker create docker image based on image from that repo:\nhttps:\/\/github.com\/aws\/sagemaker-scikit-learn-container\nand the  0.23-1 branch handles extensions installation (if extenssion\/Dockerfile.cpu file is executed), but I don't see how I can make Sagemaker run that script when creating the job.\n\nhow can I use sagemaker_sklearn_extension in Sagemaker job?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to register a multi container model to a model registry?",
        "Question_created_time":1655990380836,
        "Question_last_edit_time":1668594613743,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwYjH1ZG2SNqPAB9VGmXT1A\/how-to-register-a-multi-container-model-to-a-model-registry",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":321,
        "Question_answer_count":1,
        "Question_body":"I have created a multi-container model in SageMaker notebook  and deployed it through an endpoint. But while attempting to do the same through a SageMaker Studio Project (build, train and deploy model template), I need to register the multi-container model through a 'sagemaker.workflow.step_collections.RegisterModel' step, which I am unable to do. \nWhat I understand till now is multi-container model is created through boto3 api call. I haven't found a way to create it using 'sagemaker.model.Model' and hence not being able to register it. Please help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to configure SageMaker execution Role with access to S3 bucket in another AWS account",
        "Question_created_time":1655804671957,
        "Question_last_edit_time":1668607010044,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQgytxULcQxqXZVqFzklfBg\/unable-to-configure-sagemaker-execution-role-with-access-to-s3-bucket-in-another-aws-account",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":939,
        "Question_answer_count":2,
        "Question_body":"**Requirement:** Create SakeMaker GroundTruth labeling job with input\/output location pointing to S3 bucket in another AWS account\n\n**High Level Steps Followed:**  Lets say, *Account_A:* SageMaker GroundTruth labeling job and *Account_B*: S3 bucket\n\n1. Create role *AmazonSageMaker-ExecutionRole* in *Account_A* with 3 policies attached:\n* AmazonSageMakerFullAccess\n* Account_B_S3_AccessPolicy: Policy with necessary S3 permissions to access S3 bucket in Account_B\n* AssumeRolePolicy: Assume role policy for *arn:aws:iam::Account_B:role\/Cross-Account-S3-Access-Role*\n\n2. Create role *Cross-Account-S3-Access-Role*  in *Account_B* with 1 policy and 1 trust relationship attached:\n* S3_AccessPolicy: Policy with necessary S3 permissions to access S3 bucket in the this Account_B\n* TrustRelationship: For principal *arn:aws:iam::Account_A:role\/AmazonSageMaker-ExecutionRole*\n\n**Error:** While trying to create SakeMaker GroundTruth labeling job with IAM role as *AmazonSageMaker-ExecutionRole*, it throws error *AccessDenied: Access Denied - The S3 bucket 'Account_B_S3_bucket_name' you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.*",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Input Manifest Errors in Sagemaker Ground Truth for Custom Labeling Job",
        "Question_created_time":1655745668932,
        "Question_last_edit_time":1668552459228,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn7gIM_MkSHmd9IzuV4_pmw\/input-manifest-errors-in-sagemaker-ground-truth-for-custom-labeling-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":4,
        "Question_view_count":248,
        "Question_answer_count":1,
        "Question_body":"I am attempting to create a native PDF annotation labeling job for use with Comprehend to identify entities within similar documents. I have around 20 pdf files, all of them around 100-300 pages long. \n\nI used the tools and followed the directions from [this blog post.](https:\/\/aws.amazon.com\/blogs\/machine-learning\/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend\/) I've struggled a little with the tools but ultimately got everything working. \n\nMy problem comes from the labeling job itself. When I open the labeling job in a private workforce that I've created, I find only a blank page. I did some research and found that there is something wrong with the input manifest as it seems AWS isn't able to parse it for some reason.\n\nI checked my manifest and found that it was generated as multiple objects. Each object was a single page from my PDFs. This seems normal, however the objects were not put into a list or 'top level' object, which does not fit JSON Lines guidelines. I attempted a quick fix of placing these objects all within a list (which satisfies JSON Lines) but it does not seem to help.\n\nAny suggestions or advice would be greatly appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Why does my kernel keep restarting when I try to download pre-trained Hugging Face BERT models weights to Amazon SageMaker?",
        "Question_created_time":1655735296435,
        "Question_last_edit_time":1667926114751,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQYlSFOh6TjSDwR4if_vqOQ\/why-does-my-kernel-keep-restarting-when-i-try-to-download-pre-trained-hugging-face-bert-models-weights-to-amazon-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":121,
        "Question_answer_count":1,
        "Question_body":"When I try to download the pre-trained Hugging Face BERT models weights to the conda_pytorch_p36 kernel of my Amazon SageMaker Notebook instance using the following command, the kernel always restarts:\n\n```\nPRE_TRAINED_MODEL_NAME2='sshleifer\/distilbart-cnn-12-6'\nmodel2 = BartForConditionalGeneration.from_pretrained(PRE_TRAINED_MODEL_NAME2, cache_dir='hf_cache_dir\/')\n```\nNote I have installed following libraries using pip commands.\n\n```\n!pip install transformers==4.17.0\n```\n\nThe result is the same for Hugging Face \"facebook\/bart-large-cnn\" models.\n\nWhy is this happening, and how do I resolve the issue?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Cloudformation Deployment for Serverless Sagemaker Model Endpoint",
        "Question_created_time":1655512915811,
        "Question_last_edit_time":1668568356320,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo7p2DuabQaapi9C4_nPVZg\/cloudformation-deployment-for-serverless-sagemaker-model-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":107,
        "Question_answer_count":2,
        "Question_body":"Hi, i want to deploy a serverless model using CloudFormation.\n\nI've created the model, the endpoint configuration, and when I try to create the endpoint the script times out, because it can't find a saved model to attach to the endpoint (because I never trained one for this instance).\n\nI've tried to look around for a trainingjob cloudformation API, but there doesn't seem to be one.\n\nHow do I solve this issue?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2",
        "Question_created_time":1655494107206,
        "Question_last_edit_time":1667925778672,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8UWsqxW8RbejgzLHYIFduA\/inconsistent-keras-model-summary-output-shapes-on-aws-sagemaker-and-ec2",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_body":"I have the following model in a jupyter notebook:\n\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nSIZE = (549, 549)\nSHUFFLE = False \nBATCH = 32\nEPOCHS = 20\n\ntrain_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\ntest_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\n\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=549, kernel_size=(5,5), padding=\"same\", activation=\"relu\")(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nmodel.summary()\n```\nSagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.\n\nnvidia-smi output on SageMaker:\n```\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   37C    P0    24W \/ 300W |      0MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n```\n\n\nnvidia-smi output on EC2:\n```\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   42C    P0    51W \/ 300W |   2460MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N\/A  N\/A     11802      C   \/bin\/python3.8                    537MiB |\n|    0   N\/A  N\/A     26391      C   python3.8                        1921MiB |\n+-----------------------------------------------------------------------------+\n```\n\nThe model.summary() output on SageMaker is:\n\n```python\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 1)       7535574   \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 1)      4         \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n=================================================================\nTotal params: 13,263,852\nTrainable params: 13,263,846\nNon-trainable params: 6\n\n```\n\nThe model.summary() output on EC2 is (notice the shape change):\n\n```python\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 549)     14274     \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 549)    2196      \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n=================================================================\nTotal params: 5,749,128\nTrainable params: 5,745,834\nNon-trainable params: 3,294\n_________________________________________________________________\n```\n\nOne other thing that is interesting, if I change my model on the EC2 instance to:\n\n```python\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=1, kernel_size=(5,5), padding=\"same\", activation=\"relu\")(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n```\n\nMy model.summary() output becomes:\n\n```python\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        \n                                                                 \n batch_normalization_6 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        \n                                                                 \n batch_normalization_7 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         \n                                                                 \n batch_normalization_8 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        \n                                                                 \n=================================================================\nTotal params: 60\nTrainable params: 54\nNon-trainable params: 6\n_________________________________________________________________\n```\nIn the last model the shape is similar to SageMaker but the trainable parameters are very low.\n\nAny ideas as to why the output shape is different and why this is happening with the filters? When I run this model on my personal computer, the shape is the same as EC2. I think there might be an issue with SageMaker.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Call last Sagemaker Model in Batch Transform Jobs",
        "Question_created_time":1655467756412,
        "Question_last_edit_time":1667925901874,
        "Question_link":"https:\/\/repost.aws\/questions\/QUF0u2FxOyTqK8-9hQG40i7g\/call-last-sagemaker-model-in-batch-transform-jobs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":318,
        "Question_answer_count":1,
        "Question_body":"Hi Dears, \n\nHope this message finds you well\n\nI have a sagemaker model, buit by on demand notebook. \nI have been used batch transform jobs using lambda function, It take input inference json from s3 to create batch transform job and have finally predictions. \n\nThe question how can I make lambda to use last trained model automaticity ? \n      model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n\nLambda code : \n\n if 'input_data_4' in file:\n\n                def batch_transform():\n                    transformer = Transformer(\n                        model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n                        instance_count = 2,\n                        instance_type = 'ml.m5.xlarge',\n                        assemble_with = 'Line',\n                        output_path = output_data_path,\n                        base_transform_job_name ='daily-output-predictions-to-s3',\n                        #sagemaker_session = sagemaker.session.Session,\n                        accept = 'application\/jsonlines')\n                    transformer.transform(data = input_data_path, \n                                        content_type = 'application\/jsonlines', \n                                        split_type = 'Line',\n                                        wait=False, \n                                        logs=True)\n                        #Waits for the Pipeline Transform Job to finish.\n                    print('Batch Transform Job Created successfully!')\n                batch_transform()\n\nThanks\nBasem",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1655971465907,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1655971465908,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi Basem,\n\nIf I understood correctly you'd like your Lambda function to automatically choose the latest SageMaker model when it runs, instead of hard-coding the model name.\n\nAlthough you *could* do this simply with [boto3.client(\"sagemaker\").list_models(...)](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.list_models) (which can sort by creation time), I would not recommend it. The reason is that in general this lists *all* models present in SageMaker - which might include some for different use cases in future, even if you only have the one DeepAR forecasting use-case today. You'd have to manually filter after the API call.\n\nA **better approach** would probably be to register your forecasting models in [SageMaker Model Registry](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-registry.html) - which will allow you to register different versions and track extra metadata like metrics and approval status for each version if you need.\n\n- First (e.g. from your notebook) you can [create a model package group](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model_package_group) to track your forecasting models.\n- Then (when you create your SageMaker Model) you can **register** it as a new version in the group - via [Model.register()](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.register).\n- At the point you want to look up which model to use, you can then [list_model_packages](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.list_model_packages) which can filter to your specific group of models, and also by approval status if you like.\n\nSo for example you could set the model package group name as a configuration environment variable for your Lambda function, and have the function dynamically look up the latest version to use from the group when needed.\n\nOf course there are also many more custom ways to do this such as creating an SSM Parameter to track the name of your current accepted model, or creating your own model registry using a data store like DynamoDB... But SageMaker Model Registry seems like the most purpose-built tool for the job here to me.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Pipeline with lambda step: unable to update\/upsert pipeline",
        "Question_created_time":1655388150771,
        "Question_last_edit_time":1668623284939,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0AGShN-8TqKCFzM5FBI-ig\/sagemaker-pipeline-with-lambda-step-unable-to-update-upsert-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":247,
        "Question_answer_count":1,
        "Question_body":"Hi there,\n\nI have a SM pipeline which contains a lambda step.\n\nI have recently run into problems with the pipeline\/lambda function, everytime I call the `pipeline.upsert()` method, I receive the following error message(s):\n\n```\nResourceConflictException: An error occurred (ResourceConflictException) when calling the UpdateFunctionCode operation: The operation cannot be performed at this time. An update is in progress for resource: arn:aws:lambda:eu-west-x:XXXXXXXXXXXX:function:evaluation-input-generator\n\nDuring handling of the above exception, another exception occurred: \n```\n```\nValueError: {'Message': 'The operation cannot be performed at this time. An update is in progress for resource: arn:aws:lambda:eu-west-X:XXXXXXXXXXXX:function:evaluation-input-generator', 'Code': 'ResourceConflictException'}\n```\nFor some reasons, the lambda seems to be permanently stuck in `An update is in progress for resource: ...`, however, I suspect that this is not the case, I am able to call the lambda helper's update method for the function and it will return status 200.\n\nthis behaviour is observed in multiple SM pipelines, any help would be greatly appreciated!\n\nBest,\nRuoy",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"FSxLustre FileSystemInput in Sagemaker TrainingJob leads to: InternalServerError",
        "Question_created_time":1655298419776,
        "Question_last_edit_time":1668481511373,
        "Question_link":"https:\/\/repost.aws\/questions\/QUignDqPc7QqCIBvB3r4g3Vw\/fsxlustre-filesysteminput-in-sagemaker-trainingjob-leads-to-internalservererror",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":158,
        "Question_answer_count":1,
        "Question_body":"We are submitting a Sagemaker Training job with Sagemaker SDK with a custom docker image. The job finishes successfully for EFS [FileSystemInput](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/inputs.html#sagemaker.inputs.FileSystemInput) or [TrainingInput](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/inputs.html#sagemaker.inputs.TrainingInput). Trying to use the FIleSystemInput with FSxLustre configuration leads to the training job dying during the `Preparing the instances for training` stage:\n```\nInternalServerError: We encountered an internal error. Please try again.\n```\nThis error is persistent upon re-submission.\n\nWhat we figured out until now:\n- the job errors before the training image is downloaded.\n- specifying an invalid mount point leads to a proper error: ```ClientError: Unable to mount file system: xxx directory path: yyy. Incorrect mount path. Please ensure the mount path specified exists on the filesystem.```\n- the job finishes successfully when running locally with docker-compose ([Estimator](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase) with `instance_type=\"local\"`).\n- we can mount the FSx file system on an EC2 instance with the TrainingJob's VPC and security group.\n\nHow can we narrow the problem down further and get more information about the failure reason? Can you suggest likely problems that could cause this behavior?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Determining the \"right\" instance type running Jupyter notebook in Sagemaker when reading\/writing a huge parquet file?",
        "Question_created_time":1655268194182,
        "Question_last_edit_time":1667926666288,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvBSWuAqZSru0kQHeulQqLw\/determining-the-right-instance-type-running-jupyter-notebook-in-sagemaker-when-reading-writing-a-huge-parquet-file",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":108,
        "Question_answer_count":1,
        "Question_body":"I am unclear as o how to determine the \"right\" instance type running Jupyter notebook in Sagemaker. When reading\/writing a small size parquet file, no problem; but when I try to read\/write a huge parquet file,  the program stops and gives an error, \"Job aborted due to stage failure: Task 21 in stage 33.0 failed 1 times, most recent failure: Lost task 21.0 in stage 33.0 (TID 1755, localhost, executor driver\"\nI would appreciate any insights please... thanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deploy YOLOv5 in sagemaker - ModelError: InvokeEndpoint operation: Received server error (0)",
        "Question_created_time":1655212102106,
        "Question_last_edit_time":1668599479716,
        "Question_link":"https:\/\/repost.aws\/questions\/QULAis68RtShua1Wg8A5EFXg\/deploy-yolov5-in-sagemaker-modelerror-invokeendpoint-operation-received-server-error-0",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":133,
        "Question_answer_count":2,
        "Question_body":"I'm trying to deploy custom trained Yolov5 model in Sagemaker for inference. (Note : The model was not trained in sagemaker).\n\nFollowed this doc for deploying the model and inference script - [Sagemaker docs](https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#bring-your-own-model)\n\n```\nModelError                                Traceback (most recent call last)\n<ipython-input-7-063ca701eab7> in <module>\n----> 1 result1=predictor.predict(\"FILE0032.JPG\")\n      2 print(result1)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n    159             data, initial_args, target_model, target_variant, inference_id\n    160         )\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    162         return self._handle_response(response)\n    163 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    399                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    400             # The \"self\" in this scope is referring to the BaseClient.\n--> 401             return self._make_api_call(operation_name, kwargs)\n    402 \n    403         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    729             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    730             error_class = self.exceptions.from_code(error_code)\n--> 731             raise error_class(parsed_response, operation_name)\n    732         else:\n    733             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https:\/\/ap-south-1.console.aws.amazon.com\/cloudwatch\/home?region=ap-south-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/pytorch-inference-2022-06-14-11-58-04-086 in account 772044684908 for more information.\n```\n\n\nAfter researching about `InvokeEndpoint`, tried this\n\n```\nimport boto3\n\nsagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name='ap-south-1')\nendpoint_name='pytorch-inference-2022-06-14-11-58-04-086'\nresponse = sagemaker_runtime.invoke_endpoint(\n                            EndpointName=endpoint_name, \n                            Body=bytes('{\"features\": [\"This is great!\"]}', 'utf-8') # Replace with your own data.\n                            )\nprint(response['Body'].read().decode('utf-8'))\n```\nBut this didn't help as well,\n\ndetailed output : \n\n```\nReadTimeoutError                          Traceback (most recent call last)\n<ipython-input-8-b5ca204734c4> in <module>\n     12 response = sagemaker_runtime.invoke_endpoint(\n     13                             EndpointName=endpoint_name,\n---> 14                             Body=bytes('{\"features\": [\"This is great!\"]}', 'utf-8') # Replace with your own data.\n     15                             )\n     16 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    399                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    400             # The \"self\" in this scope is referring to the BaseClient.\n--> 401             return self._make_api_call(operation_name, kwargs)\n    402 \n    403         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    716             apply_request_checksum(request_dict)\n    717             http, parsed_response = self._make_request(\n--> 718                 operation_model, request_dict, request_context)\n    719 \n    720         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    735     def _make_request(self, operation_model, request_dict, request_context):\n    736         try:\n--> 737             return self._endpoint.make_request(operation_model, request_dict)\n    738         except Exception as e:\n    739             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    105         logger.debug(\"Making request for %s with params: %s\",\n    106                      operation_model, request_dict)\n--> 107         return self._send_request(request_dict, operation_model)\n    108 \n    109     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    182             request, operation_model, context)\n    183         while self._needs_retry(attempts, operation_model, request_dict,\n--> 184                                 success_response, exception):\n    185             attempts += 1\n    186             self._update_retries_context(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    306             event_name, response=response, endpoint=self,\n    307             operation=operation_model, attempts=attempts,\n--> 308             caught_exception=caught_exception, request_dict=request_dict)\n    309         handler_response = first_non_none_response(responses)\n    310         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    356     def emit(self, event_name, **kwargs):\n    357         aliased_event_name = self._alias_event_name(event_name)\n--> 358         return self._emitter.emit(aliased_event_name, **kwargs)\n    359 \n    360     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    227                  handlers.\n    228         \"\"\"\n--> 229         return self._emit(event_name, kwargs)\n    230 \n    231     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    210         for handler in handlers_to_call:\n    211             logger.debug('Event %s: calling handler %s', event_name, handler)\n--> 212             response = handler(**kwargs)\n    213             responses.append((handler, response))\n    214             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    192             checker_kwargs.update({'retries_context': retries_context})\n    193 \n--> 194         if self._checker(**checker_kwargs):\n    195             result = self._action(attempts=attempts)\n    196             logger.debug(\"Retry needed, action of: %s\", result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception, retries_context)\n    266 \n    267         should_retry = self._should_retry(attempt_number, response,\n--> 268                                           caught_exception)\n    269         if should_retry:\n    270             if attempt_number >= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    292             # If we've exceeded the max attempts we just let the exception\n    293             # propogate if one has occurred.\n--> 294             return self._checker(attempt_number, response, caught_exception)\n    295 \n    296 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    332         for checker in self._checkers:\n    333             checker_response = checker(attempt_number, response,\n--> 334                                        caught_exception)\n    335             if checker_response:\n    336                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    232         elif caught_exception is not None:\n    233             return self._check_caught_exception(\n--> 234                 attempt_number, caught_exception)\n    235         else:\n    236             raise ValueError(\"Both response and caught_exception are None.\")\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    374         # the MaxAttemptsDecorator is not interested in retrying the exception\n    375         # then this exception just propogates out past the retry code.\n--> 376         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model, context)\n    247             http_response = first_non_none_response(responses)\n    248             if http_response is None:\n--> 249                 http_response = self._send(request)\n    250         except HTTPClientError as e:\n    251             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    319 \n    320     def _send(self, request):\n--> 321         return self.http_session.send(request)\n    322 \n    323 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    449             raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n    450         except URLLib3ReadTimeoutError as e:\n--> 451             raise ReadTimeoutError(endpoint_url=request.url, error=e)\n    452         except ProtocolError as e:\n    453             raise ConnectionClosedError(\n\nReadTimeoutError: Read timeout on endpoint URL: \"https:\/\/runtime.sagemaker.ap-south-1.amazonaws.com\/endpoints\/pytorch-inference-2022-06-14-11-58-04-086\/invocations\"\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Extending Docker image for SageMaker Inference",
        "Question_created_time":1655198456555,
        "Question_last_edit_time":1668567499750,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxESyB86cSMCN3dOUMyi4cw\/extending-docker-image-for-sagemaker-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":724,
        "Question_answer_count":1,
        "Question_body":"I'm trying to create my own Docker image for use with SageMaker Batch Transform by extending an existing one. Following the documentation at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html, I have created the following to run Detectron 2:\n\n```\nFROM 763104351884.dkr.ecr.eu-west-2.amazonaws.com\/pytorch-inference:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\n\n############# Installing latest builds ############\nRUN pip install --upgrade torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\nENV FORCE_CUDA=\"1\"\n# Build D2 only for Turing (G4) and Volta (P3) architectures. Use P3 for batch transforms and G4 for inference on endpoints\nENV TORCH_CUDA_ARCH_LIST=\"Turing;Volta\"\n\n# Install Detectron2\nRUN pip install \\\n   --no-cache-dir pycocotools~=2.0.0 \\\n   --no-cache-dir https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu113\/torch1.10\/detectron2-0.6%2Bcu113-cp38-cp38-linux_x86_64.whl\n   \n# Set a fixed model cache directory. Detectron2 requirement\nENV FVCORE_CACHE=\"\/tmp\"\n\n############# SageMaker section ##############\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\nENV SAGEMAKER_PROGRAM inference.py\n```\n\nI then create a model (`create-model`) with this image using the following configuration:\n\n```\n{\n\"ExecutionRoleArn\": \"arn:aws:iam::[redacted]:role\/model-role\",\n\"ModelName\": \"model-test\",\n\"PrimaryContainer\": { \n  \"Environment\": {\n    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"\/opt\/ml\/code\",\n    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n    \"SAGEMAKER_REGION\": \"eu-west-2\",\n    \"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"\n   },\n  \"Image\": \"[redacted].dkr.ecr.eu-west-2.amazonaws.com\/my-image:latest\",\n  \"ModelDataUrl\": \"s3:\/\/[redacted]\/training\/output\/model.tar.gz\"\n}\n}\n```\n\nAnd submit a batch transform job (`create-transform-job`) using the following configuration:\n\n```\n{\n\"MaxPayloadInMB\": 16,\n\"ModelName\": \"model-test\",\n\"TransformInput\": { \n    \"ContentType\": \"application\/x-image\",\n    \"DataSource\": { \n      \"S3DataSource\": { \n          \"S3DataType\": \"ManifestFile\",\n          \"S3Uri\": \"s3:\/\/[redacted]\/manifests\/input.manifest\"\n      }\n    }\n},\n\"TransformJobName\": \"transform-test\",\n\"TransformOutput\": { \n    \"S3OutputPath\": \"s3:\/\/[redacted]\/predictions\/\"\n},\n\"TransformResources\": { \n    \"InstanceCount\": 1,\n    \"InstanceType\": \"ml.m5.large\"\n}\n}\n```\n\nBoth of the above commands submit fine, but the transform job doesn't complete. When I look in the logs, the errors I'm getting seem to indicate that it's not using my inference script (`inference.py`, specified above) but is instead using the default script (`default_pytorch_inference_handler.py`) and therefore can't find the model.\n\nWhat am I missing so that it uses my inference script instead, and hence my model?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Canvas Lifecycle Config",
        "Question_created_time":1654810759004,
        "Question_last_edit_time":1668442212710,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0LTBVP38RoSVvEmmmyc7xw\/sagemaker-canvas-lifecycle-config",
        "Question_score_count":0,
        "Question_favorite_count":3,
        "Question_comment_count":0,
        "Question_view_count":166,
        "Question_answer_count":1,
        "Question_body":"For our Sagemaker Notebook\/studio deployments we have used lifecycle configs to turn off the app when inactive. Is this possible to apply to Canvas? Is this a supported function? is there an alternative?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio default server failing",
        "Question_created_time":1654597460713,
        "Question_last_edit_time":1667925988503,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJhSBFYdSQ8aGHNA3DjMk-A\/sagemaker-studio-default-server-failing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":84,
        "Question_answer_count":2,
        "Question_body":"Hello,\nI am trying to start Amazon SageMaker Studio today but without success.\nThe default server is failing to start. On CloudWatch I only see SIGKILL messages as errors:\n\n```\n2022-06-07T10:55:01.523+02:00\t2022-06-07 08:55:00,466 WARN killing 'jupyterlabserver' (11) with SIGKILL\n2022-06-07T10:55:04.524+02:00\t2022-06-07 08:55:01,470 INFO waiting for jupyterlabserver-listener, jupyterlabserver to die\n```\n\nOn the Apps section I see the default server in status \"Failed\", and I cannot do anything, not even delete it.\n\nI cannot use the service right now. Is it maybe related to the new release of Jupyter Lab v3.0?\n\nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"GC overhead limit exceeded",
        "Question_created_time":1654390632353,
        "Question_last_edit_time":1668531594344,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc_WeDqcTjitN1bkIJRosg\/gc-overhead-limit-exceeded",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":176,
        "Question_answer_count":1,
        "Question_body":"I have a modest size dataset, and I am running Jupyter Notebook in Sagemaker (instance type ml.c5.xlarge with 200G instance size). I receive the error message \" GC overhead limit exceeded\" Everything ran fine with small data size. \nBTW, I need to go through the dataframe one row at a time using  df.collect(), which seems t be an expensive operation... Would you suggest another way of accomplishing this? \nI would appreciate your kind help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Studio - Jupyter proxy function does not work",
        "Question_created_time":1654087666657,
        "Question_last_edit_time":1667900303577,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuptYf-6wTNmhZaNCJWRhRA\/sagemaker-studio-jupyter-proxy-function-does-not-work",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"I am trying to run TensorBoard 2.9.0 in SageMaker Studio, as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html#studio-tensorboard-launch).\n\nHowever, after launching TensorBoard, when I navigate to the proxy URL `https:\/\/<DOMAIN>.studio.<REGION>.sagemaker.aws\/jupyter\/default\/proxy\/6006\/`, I get error 500.\n\nI am using the \"PyTorch 1.10 Python 3.8 CPU Optimized\" image. Any suggestions?\n\nI have also tried with the new JupyterLab 3 server version, but same result.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Pipelines - Batch Transform job using generated predictions as input for the model",
        "Question_created_time":1653948738611,
        "Question_last_edit_time":1668599477133,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3dJWSSmiTQaBwCCY1JQsVQ\/sagemaker-pipelines-batch-transform-job-using-generated-predictions-as-input-for-the-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":697,
        "Question_answer_count":1,
        "Question_body":"Hi all!\nSo, we're trying to implement a very simple Sagemaker Pipeline with 3 steps:\n* **ETL:** for now it only runs a simple query\n* **Batch transform:** uses the ETL's result and generates predictions with a batch transform job\n* **Report:** generates an HTML report\n\nThe thing is, when running the batch transform job alone in the Pipeline, everything runs OK. But when trying to run all the steps in a Pipeline, the batch transform job fails, and what we have seen in the logs is that the job takes the dataset which was generated in the ETL step, generates the predictions and saves them correctly in S3 (this is where we would expect the job to stop) but then it resends those predictions to the endpoint, as if they were a new input, and so the step fails as the model receives an array of 1 column thus mismatching the number of features which it was trained with.\n\nThere's not much info out there on this, and Sagemaker is painfully hard to debug. Has anyone experienced anything like this?\n\nOur model and transformer code:\n\n```python\nmodel = XGBoostModel(\n    model_data=f\"s3:\/\/{BUCKET}\/{MODEL_ARTIFACTS_PATH}\/artifacts.gzip\",\n    role=get_execution_role(),\n    entry_point=\"predict.py\",\n    framework_version=\"1.3-1\",\n)\n\ntransformer = model.transformer(\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=f\"s3:\/\/{BUCKET}\/{PREDICTIONS_PATH}\/\",\n    accept=\"text\/csv\",\n)\n\nstep = TransformStep(\n    name=\"Batch\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=etl_step.properties.ProcessingOutputConfig.Outputs[\n            \"dataset\"\n        ].S3Output.S3Uri,\n        content_type=\"text\/csv\",\n        split_type=\"Line\",\n    ),\n    depends_on=[etl_step],\n)\n```\n\nAnd our inference script:\n\n```python\ndef input_fn(request_body, content_type):\n    return pd.read_csv(StringIO(request_body), header=None).values\n\n\ndef predict_fn(input_obj, model):\n    \"\"\"\n    Function which takes the result of input_fn and generates\n    predictions.\n    \"\"\"\n    return model.predict_proba(input_obj)[:, 1]\n\n\ndef output_fn(predictions, content_type):\n    return \",\".join(str(pred) for pred in predictions)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to log error\/messages in while running a sagemaker batch transform job?",
        "Question_created_time":1653699481978,
        "Question_last_edit_time":1667926062258,
        "Question_link":"https:\/\/repost.aws\/questions\/QUNirOT1cMSfig9ANtgmZMpg\/how-to-log-error-messages-in-while-running-a-sagemaker-batch-transform-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":83,
        "Question_answer_count":0,
        "Question_body":"i'm using a hugging face model and a container to create a batch transform job in sagemaker. i have a custom inference code and in the output_fn function i'm returning json_dumps(prediction). I'm using print(prediction) to see, if i can see it in the cloudwatch logs to find out type and what prediction is. how can  log these messages . Also, the inference output i get is in the form below., i'm not sure why is it not a json object in each line instead it has square brackets. I want to use the filter to match the input and output in the batch job. I'm not sure how the output should look like , because i'm trying to associate input with output by using dataprocessing config as below. but i get an error. the documenation has example of csv not json. what should the output look like so that i can associate the input with output when they both are in json format. \n\n```\n  \"DataProcessing\": {\n        \"JoinSource\": \"Input\"\n    },\n```\n```\n[ output text 1 ]\n[output text 2 ]\n```\n\n```\n# Serialize the prediction result into the desired response content type\ndef output_fn(prediction, accept=JSON_CONTENT_TYPE):\n    logger.info(\"Serializing the generated output.\")\n    if accept == JSON_CONTENT_TYPE:\n        output = json.dumps(prediction)\n        return output, accept\n    raise Exception(\"Requested unsupported ContentType in Accept: {}\".format(accept))\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Multi Model endpoint creation fails while creating for model built on container sagemaker-scikit-learn:0.23-1-cpu-py3",
        "Question_created_time":1653574428837,
        "Question_last_edit_time":1668522403648,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHZiKPwmxRyy0C0Nc0ONwuQ\/sagemaker-multi-model-endpoint-creation-fails-while-creating-for-model-built-on-container-sagemaker-scikit-learn-0-23-1-cpu-py3",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":234,
        "Question_answer_count":1,
        "Question_body":"I am working on a use-case where I am using SageMaker multi-model endpoint for model inference and the models are trained using Databricks MLFlow platform. When I tried deploying a model trained from Databricks MLFlow platform on a single endpoint on SageMaker then it worked fine but the creation of multi-model endpoint for 'sagemaker-scikit-learn:0.23-1-cpu-py3' container is failed with the following error:\n\nCode Snippet::>> \nname = \"sample-mme\"\n\nsagemaker_client = boto3.client('sagemaker')\n\nmodel_path = \"s3:\/\/test-bucket\/multi-models\"\n\nexecution_role_arn = \"IAM:\/\/sample-role\"\n\nBASE_IMAGE = image_uris.retrieve(\n            region=region, framework=\"sklearn\",version='0.23-1',image_scope='inference'\n        )\n\ncontainer = {\n            'Image': BASE_IMAGE,\n            'ModelDataUrl': model_path,\n            'Mode':         'MultiModel',\n            'MultiModelConfig': {\n                'ModelCacheSetting': 'Enabled'\n            }\n        }\n\nmodel_response = sagemaker_client.create_model(\n            ModelName=name,\n            ExecutionRoleArn=execution_role_arn,\n            Containers=[container]\n        )\n\nconfig_response = sagemaker_client.create_endpoint_config(\n            EndpointConfigName=f'{name}-config',\n            ProductionVariants=[\n                {\n                    'InstanceType':        instance_type,\n                    'InitialInstanceCount': instance_count,\n                    'InitialVariantWeight': 1,\n                    'ModelName':            name,\n                    'VariantName':          'AllTraffic'\n                }\n            ]\n        )\n\nresponse = sagemaker_client.create_endpoint(\n            EndpointName=f'{name}-endpoint',\n            EndpointConfigName=f'{name}-config'\n        )\n\nEndpoint creation is taking a lot if time and failing with the following error message : \n\nsagemaker_containers._errors.ImportModuleError: 'NoneType' object has no attribute 'startswith'\n\nPlease provide me with some help to fix this.\n\nAlso, my understanding is that I can train a model on the DataBricks MLFlow platform using sklearn libraries, and then I can store model artifacts \"model.tar.gz\" under the s3 directory for storing all multi-models. Now I can create a multi-model endpoint in SageMaker using the same s3 directory as the model path and using the above code. Once the endpoint is ready, I can do inference by providing the target model.\nPlease let me know if my understanding is correct and share any relevant documents to follow for my use case.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker instances keep awakening and charge the credit",
        "Question_created_time":1653535822137,
        "Question_last_edit_time":1668607996615,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjCMOSHaPR4WwWP1SoFzzng\/sagemaker-instances-keep-awakening-and-charge-the-credit",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":986,
        "Question_answer_count":1,
        "Question_body":"I have tried Data Wrangler in Sagemaker last month and close the service. A few weeks later I have noticed the credit was charge $1 every hour and just realized that the Data Wranger auto-save the flow every minute. So, I deleted the unsaved flow and shut down all the services and instances according to advice on these two links :\n* https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-shutdown.html\n* https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\n\nThen, I left the Sagemaker untouched for the whole month of May, and just got back to the console yesterday. This is what I found out for May's bill:\n\nAmazon SageMaker RunInstance $531.74\n==\n| Detail | Usage | Total |\n| --- | --- | --- |\n| $0.00 for Host:ml.m5.xlarge per hour under monthly free tier | 125.000 Hrs | $0.00 |\n| $0.00 for Notebk:ml.t2.medium per hour under monthly free tier | 107.056 Hrs | $0.00 |\n| $0.00 per Data Wrangler Interactive ml.m5.4xlarge hour under monthly free tier | 25.000 Hrs | $0.00 |\n| $0.23 per Hosting ml.m5.xlarge hour in US East (N. Virginia) |  88.997 Hrs | $20.47 |\n| $0.922 per Data Wrangler Interactive ml.m5.4xlarge hour in US East (N. Virginia) |  554.521 Hrs | $511.27 |\n\n\nSo, with another attempt, I installed an extension to automatically shut down idle kernels and set the limit to 10 min from advice here:\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/ \nChecked the cost in usage report, it turns out that the service was shut down after installing the extension but then it revoked itself after 5 hours later (during my sleep time). There's still cost from Studio although with less charge than previous one. \n\n| Service | Operation | UsageType | StartTime | EndTime |UsageValue |\n| --- | --- | --- | --- | --- | --- |\n| AmazonSageMaker | RunInstance | USE1-Studio_DW:KernelGateway-ml.m5.4xlarge | 5\/24\/2022 23:00 | 5\/25\/2022 0:00 | 1 |\n| AmazonSageMaker | RunInstance | USE1-Studio_DW:KernelGateway-ml.m5.4xlarge | 5\/25\/2022 0:00 | 5\/25\/2022 1:00 | 1 |\n| AmazonSageMaker | RunInstance | USE1-Studio_DW:KernelGateway-ml.m5.4xlarge | 5\/25\/2022 1:00 | 5\/25\/2022 2:00 | 1 |\n| AmazonSageMaker | RunInstance | USE1-Studio_DW:KernelGateway-ml.m5.4xlarge | 5\/25\/2022 2:00 | 5\/25\/2022 3:00 | 0.76484417 |\n| AmazonSageMaker | RunInstance | USE1-Studio_DW:KernelGateway-ml.m5.4xlarge | 5\/25\/2022 8:00 | 5\/25\/2022 9:00 | 0.36636722 |\n| AmazonSageMaker | RunInstance | USE1-Studio_DW:KernelGateway-ml.m5.4xlarge | 5\/25\/2022 9:00 | 5\/25\/2022 10:00 | 0.38959556 |\n\nDuring this time, I'm sure that there're no running instances, running apps, kernel sessions or terminal sessions. I even deleted the user profile. Last thing I haven't tried is to set up scheduled shutdown coz I think the services should not cause difficulty to our life that much. Any advice for any effective action to completely shutdown the Sagemaker instance?  Thanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Which GPU instances are supported by the sagemaker algorithm forecasting-deepar?",
        "Question_created_time":1653437621164,
        "Question_last_edit_time":1667925624792,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0TwRR6KzRzuS5Xme3uMdEw\/which-gpu-instances-are-supported-by-the-sagemaker-algorithm-forecasting-deepar",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_body":"I previously ran a hyperparameter tuning job for SageMaker DeepAR with the instance type ml.c5.18xlarge but it seems insufficient to complete the tuning job within the max_run time specified in my account. Now, having tried to use the accelerated GPU instance ml.g4dn.16xlarge, I am prompted with an error - \"Instance type ml.g4dn.16xlarge is not supported by algorithm forecasting-deepar.\" \n\nI cannot find any documentation that indicates the list of instance types supported by deepar. What GPU\/CPU instances have more compute capacity than ml.c5.18xlarge which I could leverage for my tuning job? \n\nIf there isn't, I would appreciate any recommendations as to how I could hasten the run time of the job. I require the tuning job to complete within the max run time of 432000 seconds. Thank you in advance!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker XGBoost Forecast Horizon Perameter",
        "Question_created_time":1653415645056,
        "Question_last_edit_time":1667926712277,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxwI3shNwReWGx-T3AEhtPg\/sagemaker-xgboost-forecast-horizon-perameter",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":84,
        "Question_answer_count":1,
        "Question_body":"Which XGBoost hypertuning perameter is used to set the forecast horizon?\n\nI do not see a parameter listed here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost_hyperparameters.html",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Textract to multi column pdf files",
        "Question_created_time":1653356450043,
        "Question_last_edit_time":1668523851808,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5ETLegQmRbeaA3sRyLox8g\/textract-to-multi-column-pdf-files",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":195,
        "Question_answer_count":1,
        "Question_body":"I am using the code below that I took from an example [https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract\/](), in the example it is used only for a case of 2 columns, in the code where there is division by 2, if my file has 4 columns for example, I just change that it works. But how to detect the amount of columns automatically or some way that I don't need this manual input anymore?\nIn summary I want to use this code for cases of pdf files that have more than 2 columns, how to do it?\n\n\n```\nimport boto3\n# Document\ns3BucketName = \"amazon-textract-public-content\"\ndocumentName = \"blogs\/two-column-image.jpg\"\n\n# Amazon Textract client\ntextract = boto3.client('textract')\n\n# Call Amazon Textract\nresponse = textract.detect_document_text(\n    Document={\n        'S3Object': {\n            'Bucket': s3BucketName,\n            'Name': documentName\n        }\n    })\n\n#print(response)\n\n# Detect columns and print lines\ncolumns = []\nlines = []\nfor item in response[\"Blocks\"]:\n      if item[\"BlockType\"] == \"LINE\":\n        column_found=False\n        for index, column in enumerate(columns):\n            bbox_left = item[\"Geometry\"][\"BoundingBox\"][\"Left\"]\n            bbox_right = item[\"Geometry\"][\"BoundingBox\"][\"Left\"] + item[\"Geometry\"][\"BoundingBox\"][\"Width\"]\n            bbox_centre = item[\"Geometry\"][\"BoundingBox\"][\"Left\"] + item[\"Geometry\"][\"BoundingBox\"][\"Width\"]\/2\n            column_centre = column['left'] + column['right']\/2\n\n            if (bbox_centre > column['left'] and bbox_centre < column['right']) or (column_centre > bbox_left and column_centre < bbox_right):\n                #Bbox appears inside the column\n                lines.append([index, item[\"Text\"]])\n                column_found=True\n                break\n        if not column_found:\n            columns.append({'left':item[\"Geometry\"][\"BoundingBox\"][\"Left\"], 'right':item[\"Geometry\"][\"BoundingBox\"][\"Left\"] + item[\"Geometry\"][\"BoundingBox\"][\"Width\"]})\n            lines.append([len(columns)-1, item[\"Text\"]])\n\nlines.sort(key=lambda x: x[0])\nfor line in lines:\n    print (line[1])\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Question_created_time":1653000488230,
        "Question_last_edit_time":1668013027784,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":123,
        "Question_answer_count":1,
        "Question_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart.\nWe have attempted to configure this endpoint as 'asynchronous' via the console.\nReceiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'.\nThis was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1653023938005,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1653023938006,
        "Answer_comment_count":1.0,
        "Answer_body":"Vanilla SageMaker \"Models\" (as opposed to versioned ModelPackages) are immutable in the API with no \"UpdateModel\" action... But I think you should be able to create a new Model copying the settings of the current one.\n\nI'd suggest to:\n\n1. Use [DescribeModel](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeModel.html) (via [boto3.client(\"sagemaker\").describe_model()](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_model), assuming you're using Python) to fetch all the parameters of the existing JumpStart model such as the S3 artifact location and other settings\n2. Use [CreateModel](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html) ([create_model()](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model)) to create a new model with same configuration but network isolation disabled\n3. Use your new model to try and deploy an async endpoint\n\nProbably you'd find the low-level boto3 SDK more intuitive for this task than the high-level `sagemaker` SDK's [Model class](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html) - because the latter does some magic that makes typical build\/train\/deploy workflows easier but can be less natural for hacking around with existing model definitions. For example, creating an SMSDK `Model` object doesn't actually create a Model in the SageMaker API, because deployment instance type affects choice of container image so that gets deferred until a `.deploy()` call or similar later.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Error in Creating Project in SageMaker Studio",
        "Question_created_time":1652920054013,
        "Question_last_edit_time":1668607255616,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsvzAjgcJTzGrh0wfKRDvjQ\/error-in-creating-project-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":880,
        "Question_answer_count":1,
        "Question_body":"I encountered following erorr when I tried to create a new project in SageMaker Studio. I tried to add `AdminAccess` to `role\/service-role\/AmazonSageMakerServiceCatalogProductsLaunchRole`, but still doesn't help. What could be the problem?\n\n```\nError getting the details of Service Catalog Provisioning Parameters. Error message: ValidationException: Access denied while assuming the role arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMakerServiceCatalogProductsLaunchRole. Args: {\"productId\":\"prod-xxxxxxxx\",\"provisioningArtifactId\":\"pa-xxxxxxxxx\",\"pathId\":\"lpv2-xxxxxxxxxx\"}\n```\n\n[printscreen](https:\/\/raw.githubusercontent.com\/qinjie\/picgo-images\/main\/qerpoijsdfijsadfjaosdjfqwefasdfasdfasdfasdf.png)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Async Inference Endpoint: Linking input identifier, container log, output identifier?",
        "Question_created_time":1652877072527,
        "Question_last_edit_time":1668475942524,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPN-YPS0cTsWWi4oR3tFMgQ\/sagemaker-async-inference-endpoint-linking-input-identifier-container-log-output-identifier",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":130,
        "Question_answer_count":1,
        "Question_body":"Hi,\nbased on https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb I've created a docker image which contains the model code implemented in Python. This model image is deployed as async inference endpoint. Everything works well, the endpoint can be invoked via InvokeEndpointAsync, custom attributes are provided to the model code via \"X-Amzn-SageMaker-Custom-Attributes\" header, the input located on S3 is provided as http body. After succesful processing the result is uploaded to S3 with an automatically generated output filename (\"*.out\").\nThe container behavior is logged to CloudWatch via Python logging module, initialized as:\n\n```\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger()\n```\nHowever, the request arriving inside the container neither contains an \"X-Amzn-SageMaker-Inference-Id\" nor \"X-Amzn-SageMaker-InputLocation\" field (mentioned here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpointAsync.html). Is there a way to access the inference_id\/output filename from inside the container? Or any other opportunity to link input filename\/container log\/output filename? Otherwise it's not possible to fully track and examine requests.\n\nThanks in advance for your advice!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Question_created_time":1652797037698,
        "Question_last_edit_time":1668563929559,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":614,
        "Question_answer_count":1,
        "Question_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1652798353413,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1652798353413,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like `Kernel: Idle | Instance MEM`. You can click on that text to show the kernel and instance usage metrics.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to send own failure info in case of failed SageMaker Training Job?",
        "Question_created_time":1652720071234,
        "Question_last_edit_time":1668501646379,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW-6fYh-kQy2bgc5PSkQG3Q\/how-to-send-own-failure-info-in-case-of-failed-sagemaker-training-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":476,
        "Question_answer_count":4,
        "Question_body":"Good day! \n\n\n**My main purpose:**\n\nEasy way to collect information about different failure scenarios in SageMaker TrainingJob.\n\n**What do I use currently?**\n\nSagemaker SKLearn Estimator(TrainingJobs are inside)\n\n**Where will my model train?**\n\nDifferent datasets. So, I need control and collect all information about all training processes and their final statuses on different datasets.\n\n**Which failure scenarios do I have?**\n\nThere are plenty of them. I have create my own python Errors for them.  \nFor example:\n1. There are labels only for one class.\n2. Too small dataset(by my own criterions)\n3. Missing data for crucial columns\n4. e.t.c.\n\n\n**Where am I stuck?**\n\nAfter failed training I can't get own errors from training job response. All of them are \"ExecuteUserScriptError\"\nI can't pass my own info in FailureReason or ErrorMessage(always it's empty). I see which error was raised in CloudWatchLogs and TrainingJobTraceback(from SagemakerNotebook). So, bad solution is parse all CloudWatchLogs in case of failure. \n\n**Question:\nHow to provide my own ErrorMessage or FailureReason? **\n\nMay be I am digging in the wrong direction. Anyway, I need your advice.\nThank you so much for possibility to ask an advice here)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to fix SageMaker training job error \"SM_CHANNEL_TRAIN\"?",
        "Question_created_time":1652689524286,
        "Question_last_edit_time":1667926693340,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwxIZtOn8Qg6EHGP_Ehi2mQ\/how-to-fix-sagemaker-training-job-error-sm-channel-train",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"I am building a ml workflow using step function following [this](https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/step-functions-data-science-sdk\/machine_learning_workflow_abalone\/machine_learning_workflow_abalone.html). However, when I start the state machine, I got error \n```\nAlgorithmError: framework error ... SM_CHANNEL_TRAIN ...exit code: 1 \n```\nDoes anyone know how to fix it? or how to set SM_CHANNEL_TRAIN? \n\nThank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Built-in Algorithms",
        "Question_created_time":1652686627960,
        "Question_last_edit_time":1668094438489,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":128,
        "Question_answer_count":1,
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1652688680111,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1652688680112,
        "Answer_comment_count":2.0,
        "Answer_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\n - **BlazingText**: *[BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs](https:\/\/dl.acm.org\/doi\/10.1145\/3146347.3146354)*, Gupta et Khare\n - **[DeepAR](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_how-it-works.html)** *[DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https:\/\/arxiv.org\/abs\/1704.04110)*, Salinas et al.\n - **[Factorization Machines](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines-howitworks.html)**\n - **[IP Insights](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ip-insights-howitworks.html)**\n - **[KMeans](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-kmeans-tech-notes.html)**\n - **[KNN](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/kNN_how-it-works.html)**\n - **[LDA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda-how-it-works.html)**\n - **[Linear Learner](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html)**\n - **[NTM](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html)**\n - **[Object2Vec](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object2vec-howitworks.html)**\n - **[Object Detection](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-object-detection-tech-notes.html)** (it's an SSD model)\n - **[PCA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-pca-works.html)**\n - **[Random Cut Forest](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html)**: *[Robust Random Cut Forest Based Anomaly Detection On Streams](https:\/\/proceedings.mlr.press\/v48\/guha16.pdf)*, Guha et al\n - **[Semantic Segmentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html)**\n - **[Seq2seq](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq-howitworks.html)**\n - **[XGBoost](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-HowItWorks.html)**",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Question_created_time":1652655830031,
        "Question_last_edit_time":1668587554704,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/clienterror-an-error-occurred-unknownoperationexception-when-calling-the-createhyperparametertuningjob-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":453,
        "Question_answer_count":1,
        "Question_body":"Hi Dears, \n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : \nError : \n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n-------------------\nCode:\nfrom sagemaker.tuner import (\n    IntegerParameter,\n    CategoricalParameter,\n    ContinuousParameter,\n    HyperparameterTuner,\n)\nfrom sagemaker import image_uris\n\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    use_spot_instances=True,  # use spot instances\n    max_run=1800,  # max training time in seconds\n    max_wait=1800,  # seconds to wait for spot instance\n    output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path),\n    sagemaker_session=sess,\n)\nfreq = \"D\"\ncontext_length = 300\n\ndeepar.set_hyperparameters(\n    time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length)\n)\n\nCan you please help in solving the error? \nI have to do that in af-south-1 region. \n\nThanks \nBasem\n\nhyperparameter_ranges = {\n    \"mini_batch_size\": IntegerParameter(100, 400),\n    \"epochs\": IntegerParameter(200, 400),\n    \"num_cells\": IntegerParameter(30, 100),\n    \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]),\n    \"learning_rate\": ContinuousParameter(0.0001, 0.1),\n}\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner(\n    deepar,\n    objective_metric_name,\n    hyperparameter_ranges,\n    max_jobs=10,\n    strategy=\"Bayesian\",\n    objective_type=\"Minimize\",\n    max_parallel_jobs=10,\n    early_stopping_type=\"Auto\",\n)\n\ns3_input_train = sagemaker.inputs.TrainingInput(\n    s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\"\n)\ns3_input_test = sagemaker.inputs.TrainingInput(\n    s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\"\n)\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False)\ntuner.wait()",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1653062956983,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1653062956983,
        "Answer_comment_count":0.0,
        "Answer_body":"The error message indicates that `CreateHyperParameterTuningJob` operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker - S3 bucket access when logged into us-east-1",
        "Question_created_time":1652377507550,
        "Question_last_edit_time":1668543787065,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgOCxivDSgiiHh-HZ9aYYA\/sagemaker-s3-bucket-access-when-logged-into-us-east-1",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":116,
        "Question_answer_count":1,
        "Question_body":"My IAM works, but when I login, it logs me into us-east-1.  My project is on the S3 instance (sagemaker tool).  How can I access buckets in S3 if I am logged into us-east-1 by default.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Question_created_time":1652344291417,
        "Question_last_edit_time":1668580240092,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sagemaker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":383,
        "Question_answer_count":1,
        "Question_body":"Hi all,\n\nI am asking if it's possible to use `framework processor` inside a `sagemaker pipeline`.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a `sagemaker.workflow.steps.ProcessingStep`, there isn't an available argument to specify the `source_dir`.\n\nThank you!\nBest,\nRuoy",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1652383066859,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1652383066859,
        "Answer_comment_count":1.0,
        "Answer_body":"You can do this with the latest version of the sagemaker sdk 2.89.0\n\n```\nfrom sagemaker.workflow.pipeline_context import PipelineSession\n\nsession = PipelineSession()\n\ninputs = [\n    ProcessingInput(\n    source=\"s3:\/\/my-bucket\/sourcefile\", \n    destination=\"\/opt\/ml\/processing\/inputs\/\",),\n]\n\nprocessor = FrameworkProcessor(...)\n\nstep_args = processor.run(inputs=inputs, source_dir=\"...\")\n\nstep_sklearn = ProcessingStep(\n    name=\"MyProcessingStep\",\n    step_args=step_args,\n)\n```",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Registered for aws sagemaker studio but not able to create account",
        "Question_created_time":1652237658633,
        "Question_last_edit_time":1667926143698,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSB0wO0OiQ_irUdCx9ppjbg\/registered-for-aws-sagemaker-studio-but-not-able-to-create-account",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":141,
        "Question_answer_count":1,
        "Question_body":"Hi\nI received my aws sagemaker studio approval to create an account 1 hr ago\nWhen I go to the link to create an account it says my email has not been approved \nEven though I have an email to the contrary\n\nHow do I contact amazon to sort this out?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Studio domain lifecycle configuration for hosting VSCode not working and no error logs",
        "Question_created_time":1652176660406,
        "Question_last_edit_time":1667926408825,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfxuiAWeXShmE1q7BfMdhCw\/sagemaker-studio-domain-lifecycle-configuration-for-hosting-vscode-not-working-and-no-error-logs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":87,
        "Question_answer_count":0,
        "Question_body":"Hi all,\n\nI've followed the guide provided in this medium blog by senior AWS ML SA: https:\/\/towardsdatascience.com\/hosting-vs-code-in-sagemaker-studio-f211385e25f7\n\nI've created the lifecycle configuration (jupyterlab app) for the following bash script:\n\n\n```\n#!\/bin\/bash\n\nmkdir -p vscode\ncd vscode\nsudo su\ncurl -fsSL https:\/\/code-server.dev\/install.sh | sh\nexit\n```\n\n\nHowever, when I attach the configuration to the domain or a specific user's jupyter server in the SM Studio, the JupyterServer App fails to be created with the following message displayed on the loading page of SM Studio's jupyterlab:\n\n> \"The JupyterServer app default encountered a problem and was stopped.Details: ConfigurationError: LifecycleConfig execution failed with non zero exit code 1 for script arn:aws:sagemaker:eu-west-3:615740825886:studio-lifecycle-config\/install-vscode-on-jupyterserver. Check https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lcc-debug.html for debugging instructions.\"\n\nI checked the page for debugging instructions, specifically the part where I am instructed to check the cloudwatch logs for the log group: aws\/sagemaker\/studio\nThe problem is this log group does not exist in CloudWatch (I have admin permission).\n\nI would like to have this set up for the entire team,\nAny help would be greatly appreciated!\n\nBest,\nRuoy",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"XGBoost Error: Allreduce failed - 100GB Dask Dataframe on AWS Fargate ECS cluster dies with 1T of memory.",
        "Question_created_time":1652113324900,
        "Question_last_edit_time":1668581104889,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvXlsdbQ4R9-iX9yAleu-0A\/xgboost-error-allreduce-failed-100gb-dask-dataframe-on-aws-fargate-ecs-cluster-dies-with-1t-of-memory",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":258,
        "Question_answer_count":1,
        "Question_body":"Overview: I'm trying to run an XGboost model on a bunch of parquet files sitting in S3 using dask by setting up a fargate cluster and connecting it to a Dask cluster.\n\nTotal dataframe size totals to about 140 GB of data. I scaled up a fargate cluster with properties:\n\nWorkers: 40\nTotal threads: 160\nTotal memory: 1 TB\nSo there should be enough data to hold the data tasks. Each worker has 9+ GB with 4 Threads. I do some very basic preprocessing and then I create a DaskDMatrix which does cause the task bytes per worker to get a little high, but never above the threshold where it would fail.\n\nNext I run xgb.dask.train which utilizes the xgboost package not the dask_ml.xgboost package. Very quickly, the workers die and I get the error `XGBoostError: rabit\/internal\/utils.h:90: Allreduce failed`. When I attempted this with a single file with only 17MB of data, I would still get this error but only a couple workers die. Does anyone know why this happens since I have double the memory of the dataframe?\n\n```\nX_train = X_train.to_dask_array()\nX_test = X_test.to_dask_array()\ny_train = y_train\ny_test = y_test\n```\n\n\ndtrain = xgb.dask.DaskDMatrix(client,X_train, y_train)\n\noutput = xgb.dask.train(\nclient,\n{\"verbosity\": 1, \"tree_method\": \"hist\", \"objective\": \"reg:squarederror\"},\ndtrain,\nnum_boost_round=100,\nevals=[(dtrain, \"train\")])`",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error on DeleteEndpoint operation: Cannot update in-progress endpoint",
        "Question_created_time":1652103997301,
        "Question_last_edit_time":1668502656311,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2NawO4aWQvmytekX8xJJNQ\/error-on-deleteendpoint-operation-cannot-update-in-progress-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":62,
        "Question_answer_count":0,
        "Question_body":"Hi,\n\nI have created and endpoint in sagemaker using boto3 but never finishes creation, is stuck in Creating status for few days now. I have tried to delete it using the aws cli api but i get the message:\n\n*An error occurred (ValidationException) when calling the DeleteEndpoint operation: Cannot update in-progress endpoint*\n\nUsually endpoint fails after some time and can deleted but this time doesn't fail. Is there any way to force deletion?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Amazon SageMaker Data Wrangler now supports additional M5 and R5 instances for interactive data preparation",
        "Question_created_time":1651874584266,
        "Question_last_edit_time":1667926294375,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_YGIMTDDQt61QEJiSLsmtg\/amazon-sagemaker-data-wrangler-now-supports-additional-m5-and-r5-instances-for-interactive-data-preparation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":76,
        "Question_answer_count":0,
        "Question_body":"Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare data for machine learning (ML) from weeks to minutes in Amazon SageMaker Studio, the first fully integrated development environment (IDE) for ML. With SageMaker Data Wrangler, you can simplify the process of data preparation and feature engineering, and complete each step of the data preparation workflow, including data selection, cleansing, exploration, and visualization, from a single visual interface. SageMaker Data Wrangler runs on ml.m5.4xlarge by default. SageMaker Data Wrangler includes built-in data transforms and analyses written in PySpark so you can process large data sets (up to hundreds of gigabytes (GB) of data) efficiently on the default instance. \n\nStarting today, you can use additional M5 or R5 instance types with more CPU or memory in SageMaker Data Wrangler to improve performance for your data preparation workloads. Amazon EC2 M5 instances offer a balance of compute, memory, and networking resources for a broad range of workloads. Amazon EC2 R5 instances are the memory optimized instances. Both M5 and R5 instance types are well suited for CPU and memory intensive applications such as running built-in transforms for very large data sets (up to terabytes (TB) of data) or applying custom transforms written in Panda on medium data sets (up to tens of GBs).\n\nTo learn more about the newly supported instances with Amazon SageMaker Data Wrangler, visit the [blog ](https:\/\/aws.amazon.com\/blogs\/machine-learning\/process-larger-and-wider-datasets-with-amazon-sagemaker-data-wrangler\/) or the [AWS document](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-data-flow.html), and the[ pricing page](https:\/\/aws.amazon.com\/sagemaker\/pricing\/). To get started with SageMaker Data Wrangler, visit the [AWS documentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler.html).",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Data Wrangler Full Outer Join Not Working As Expected Nor Concatenate",
        "Question_created_time":1651799011602,
        "Question_last_edit_time":1668533866794,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4DJAYgTvQQKdl-jplyRtmw\/data-wrangler-full-outer-join-not-working-as-expected-nor-concatenate",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":170,
        "Question_answer_count":1,
        "Question_body":"I've got two CSV files that are loaded into Data Wrangler that are intended to augment each other.  The tables have some columns that are the same (in name) and some that are not, many of the rows are missing entries for many of the columns.  The two tables represent separate datasets. Consider the example below:\nTable 1:\n\n| Filename | LabelA | LabelB |\n| --- | --- | --- |\n| .\/A\/001.dat | 1 | 1 |\n| .\/A\/002.dat | 0 | 1 |\n\n\nTable 2:\n\n| Filename | LabelB | LabelC |\n| --- | --- | --- |\n| .\/B\/001.dat |  | 0 |\n| .\/B\/002.dat | 0 | 1 |\n\nI am looking to merge \/ concatenate the two table.  The problem is that neither Data Wrangler join nor concatenate seems to work (at least as expected).\n\nDesired result:\n\n| Filename | LabelA | LabelB | LabelC |\n| --- | --- | --- | --- |\n| .\/A\/001.dat | 1 | 1 | |\n| .\/A\/002.dat | 0 | 1 | |\n| .\/B\/001.dat | | | 0 |\n| .\/B\/002.dat | | 0 | 1 |\n\nWhen using a \"Full Outer\" join and ask to combine \"Filename\" and \"LabelB\" columns, it will take all the values from Table 1 OR Table 2 even if Table 1 does not have that entry (for example, some rows will have Filename = <nothing> rather than Filename = .\/B\/001.dat).\n\nWhen using concatenate, Data Wrangler errors on the fact that it cannot match EVERY column between the tables.\n\nNow in my example there are many columns and many rows which precludes a manual process of joining without merging columns and then going through a renaming and merging process one-by-one.  How do get these tables to simply merge?  I feel I must be missing something obvious.  I am about to give up on Data Wrangler and do it all in a python script using pandas, but I thought I should give Data Wrangler a try while learning the MLops process.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Service",
        "Question_created_time":1651712288852,
        "Question_last_edit_time":1668505305615,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJXLrO6tMRFm6MCHQf39nbg\/sagemaker-service",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":92,
        "Question_answer_count":1,
        "Question_body":"I am using AWS Sagemaker services and delete any instances that belong to the services. However, i still get charged per day even though I dont use the service anymore. I sent the case to the customer support center but no clear explanation yet. \nPlease any kind of help for my account to stop the cost.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Service",
        "Question_created_time":1651712288840,
        "Question_last_edit_time":1668421826068,
        "Question_link":"https:\/\/repost.aws\/questions\/QURsZ28QERR3GwqsIKNPhNDw\/sagemaker-service",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":132,
        "Question_answer_count":2,
        "Question_body":"I am using AWS Sagemaker services and delete any instances that belong to the services. However, i still get charged per day even though I dont use the service anymore. I sent the case to the customer support center but no clear explanation yet. \nPlease any kind of help for my account to stop the cost.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Canvas Integration",
        "Question_created_time":1651657499501,
        "Question_last_edit_time":1668497519357,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSu1172l7SLif49c_4j4PbQ\/sagemaker-canvas-integration",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":382,
        "Question_answer_count":1,
        "Question_body":"How can you use the functions of SageMaker Canvas with python or outside the user interface?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Why my sagemaker training job slower than notebook from studiolab.sagemaker.aws?",
        "Question_created_time":1651627134522,
        "Question_last_edit_time":1668596252415,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsEj-8jnJRTK3a3gXcH8dRw\/why-my-sagemaker-training-job-slower-than-notebook-from-studiolab-sagemaker-aws",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":250,
        "Question_answer_count":2,
        "Question_body":"I run neural network tensorflow train on studiolab. and I got:\n\n```\nEpoch 145\/4000\n1941\/1941 - 10s - ... - 10s\/epoch - 5ms\/step\n```\n\nthen I try to make a train job with script_mode with `ml.c5.xlarge`\n```\nestimator = TensorFlow(entry_point='untitled.py',\n                       source_dir='.\/training\/',\n                       instance_type='ml.c5.xlarge',\n                       instance_count=1,\n                       output_path=\"s3:\/\/sagemaker-[skip]\",\n                       role=sagemaker.get_execution_role(),\n                       framework_version='2.8.0',\n                       py_version='py39',\n                       hyperparameters={...},\n                       metric_definitions=[...],\n                       script_mode=True)\n```\n\nand its got:\n```\nEpoch 19\/4000\n1941\/1941 - 49s - ... - 49s\/epoch - 25ms\/step\n```\n\nWhy is it 5 times slower than studiolab notebook? Is it because instance type?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Notebook - SSL failed validation when Boto3 session.client(verify=False)",
        "Question_created_time":1651530941407,
        "Question_last_edit_time":1668617939529,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmNuNjDSJSSyOUJ8dxVP-hg\/sagemaker-notebook-ssl-failed-validation-when-boto3-session-client-verify-false",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2274,
        "Question_answer_count":1,
        "Question_body":"In a sagemaker notebook with an associated git repository, when I try to create a boto3 session client using verify = False, I get the following : `SSLError: SSL validation failed for {service_name }  [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)`\n\nThe error resolves if I allow the default value of verify = None (meaning SSL certs are verified). My problem is that this session is created as part of a function call on the associated git repository and I don't want to change the behavior in the repo. I don't understand why this error occurs only when I specify *not* to validate SSL certificates. Any ideas of what explains this behavior?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ECS load container image from sagemaker built-in algorithm docker path",
        "Question_created_time":1651321815480,
        "Question_last_edit_time":1668180679166,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCbp7XzQSSPSH200r45m4Uw\/ecs-load-container-image-from-sagemaker-built-in-algorithm-docker-path",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":255,
        "Question_answer_count":1,
        "Question_body":"Hello, I'd like to deploy SageMaker's built-in algorithm, BlazingText model on Fargate instead of Sagemaker endpoint. So, I tried to make an ECS task using BlazingText docker path. Here is my CDK code for it.\n\nconst loadBalancedFargateService = new ecsPatterns.ApplicationLoadBalancedFargateService(this, 'Service', {\n            memoryLimitMiB: 1024,\n            desiredCount: 1,\n            cpu: 512,\n            taskImageOptions: {\n              image: ecs.ContainerImage.fromRegistry(\"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\"),\n            },\n          });\n\nHowever, I got an error: \nCannotPullContainerError: inspect image has been retried 1 time(s): failed to resolve ref \"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\": pulling from host 811284229777.dkr.ecr.us-east-1.amazonaws.com failed with status code [manifests 1]...\n\nIs it impossible to pull docker container of sagemaker built-in algorithm from ECS?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1651328900610,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1651328900611,
        "Answer_comment_count":1.0,
        "Answer_body":"To my knowledge, no - it's not generally possible to pull the [built-in algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html) containers outside SageMaker: Your easiest route would probably just be to deploy the model on SageMaker and integrate your other containerized tasks to call the SageMaker endpoint.\n\nIt's maybe worth mentioning that the [framework containers](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-prebuilt.html) for custom\/script-mode modelling (e.g. the [AWS DLCs](https:\/\/github.com\/aws\/deep-learning-containers) for PyTorch\/HuggingFace\/etc) are not subject to this restriction (can check you should even be able to pull them locally): So if you were to use those to implement a customized text processing model I think you should be able to deploy it on ECS if needed. Of course this'd mean a more initial build and later maintenance effort though.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Does Sagemaker support private GitLab repos?",
        "Question_created_time":1650995574723,
        "Question_last_edit_time":1668605819551,
        "Question_link":"https:\/\/repost.aws\/questions\/QUI8X0yZHTQ1uiS-TxOJe4ww\/does-sagemaker-support-private-gitlab-repos",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":849,
        "Question_answer_count":1,
        "Question_body":"I have a private GitLab and would like to connect it to Sagemaker Studio and\/or Notebooks. Is that supported? What is it is through a VPC? If so, how can it be done?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Run different notebooks present in same Sagemaker notebook instance using lifecycle configurations based on different lambda triggers",
        "Question_created_time":1650980304333,
        "Question_last_edit_time":1668620670555,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGrSiVuFAS_WuZDVTUmFdlA\/run-different-notebooks-present-in-same-sagemaker-notebook-instance-using-lifecycle-configurations-based-on-different-lambda-triggers",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":340,
        "Question_answer_count":2,
        "Question_body":"I have a sagemaker notebook instance having two jupyter notebook ipynb files.\nWhen I had one jupyter notebook, I was able to run it automatically with one lambda function trigger and lifecycle configuration.\n\nNow I have two jupyter notebooks and corresponding two lambda function triggers. How can I run them based on the trigger by changing the lifecycle configuration script.\n\nThe trigger is file uploading into S3. Based on what location the file is added, the corresponding jupyter notebook should run",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Run different notebooks present in same Sagemaker notebook instance with lifecycle configurations based on different lambda triggers",
        "Question_created_time":1650980301919,
        "Question_last_edit_time":1667926712300,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxwrW6VkVQra1tlWA7UtiNQ\/run-different-notebooks-present-in-same-sagemaker-notebook-instance-with-lifecycle-configurations-based-on-different-lambda-triggers",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":74,
        "Question_answer_count":0,
        "Question_body":"I have a sagemaker notebook instance having two jupyter notebook ipynb files.\nWhen I had one jupyter notebook, I was able to run it automatically with one lambda function trigger and lifecycle configuration.\n\nNow I have two jupyter notebooks and corresponding two lambda function triggers. How can I run them based on the trigger by changing the lifecycle configuration script.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"\u00bfHow can we crate a lambda which uses a Braket D-Wave device?",
        "Question_created_time":1650979594609,
        "Question_last_edit_time":1668381309829,
        "Question_link":"https:\/\/repost.aws\/questions\/QUt5r-dbkZT1O4yQM_TszxHw\/how-can-we-crate-a-lambda-which-uses-a-braket-d-wave-device",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":166,
        "Question_answer_count":1,
        "Question_body":"We are trying to deploy a Lambda with some code which works in a Notebook. The code is rather simple and uses D-Wave \u2014 DW_2000Q_6. The problem is that when we execute the lambda (container lambda due to size problems), it give us the following error:\n\n```json\n{\n  \"errorMessage\": \"[Errno 30] Read-only file system: '\/home\/sbx_user1051'\",\n  \"errorType\": \"OSError\",\n  \"stackTrace\": [\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 234, in load_module\\n    return load_source(name, filename, file)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 171, in load_source\\n    module = _load(spec)\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 702, in _load\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 671, in _load_unlocked\\n\",\n    \"  File \\\"<frozen importlib._bootstrap_external>\\\", line 843, in exec_module\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 219, in _call_with_frames_removed\\n\",\n    \"  File \\\"\/var\/task\/lambda_function.py\\\", line 6, in <module>\\n    from dwave.system.composites import EmbeddingComposite\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/__init__.py\\\", line 15, in <module>\\n    import dwave.system.flux_bias_offsets\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/flux_bias_offsets.py\\\", line 22, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/__init__.py\\\", line 15, in <module>\\n    from dwave.system.samplers.clique import *\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/clique.py\\\", line 32, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler, _failover\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/dwave_sampler.py\\\", line 31, in <module>\\n    from dwave.cloud import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/__init__.py\\\", line 21, in <module>\\n    from dwave.cloud.client import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/__init__.py\\\", line 17, in <module>\\n    from dwave.cloud.client.base import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 89, in <module>\\n    class Client(object):\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 736, in Client\\n    @cached.ondisk(maxage=_REGIONS_CACHE_MAXAGE)\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/utils.py\\\", line 477, in ondisk\\n    directory = kwargs.pop('directory', get_cache_dir())\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/config.py\\\", line 455, in get_cache_dir\\n    return homebase.user_cache_dir(\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 150, in user_cache_dir\\n    return _get_folder(True, _FolderTypes.cache, app_name, app_author, version, False, use_virtualenv, create)[0]\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 430, in _get_folder\\n    os.makedirs(final_path)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 223, in makedirs\\n    mkdir(name, mode)\\n\"\n  ]\n}\n```\nIt seems that the library tries to write to some files which are not in \/tmp folder.\n\nI'm wondering if is possible to do this, and if not, what are the alternatives.\n\nimports used:\n\n```python\nimport boto3\nfrom braket.ocean_plugin import BraketDWaveSampler\nfrom dwave.system.composites import EmbeddingComposite\nfrom neal import SimulatedAnnealingSampler\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to create (Serverless) SageMaker Endpoint using exiting tensorflow pb (frozen model) file?",
        "Question_created_time":1650919347687,
        "Question_last_edit_time":1668416574037,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZ3v2_JXIRCSbBHVBDJUWgQ\/how-to-create-serverless-sagemaker-endpoint-using-exiting-tensorflow-pb-frozen-model-file",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":214,
        "Question_answer_count":1,
        "Question_body":"Note: I am a senior developer, but am very new to the topic of machine learning.\n\nI have two frozen TensorFlow model weight files: `weights_face_v1.0.0.pb` and `weights_plate_v1.0.0.pb`. I also have some python code using Tensorflow 2, that loads the model and handles basic inference. The models detect respectively faces and license plates, and the surrounding code converts an input image to a numpy array, and applies blurring to the images in areas that had detections.\n\nI want to get a SageMaker endpoint so that I can run inference on the model. I initially tried using a regular Lambda function (container based), but that is too slow for our use case. A SageMaker endpoint should give us GPU inference, which should be much faster.\n\nI am struggling to find out how to do this. From what I can tell reading the documentation and watching some YouTube video's, I need to create my own docker container. As a start, I can use for example `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.8.0-gpu-py39-cu112-ubuntu20.04-sagemaker`. \n\nHowever, I can't find any solid documentation on how I would implement my other code. How do I send an image to SageMaker? Who tells it to convert the image to numpy array? How does it know the tensor names? How do I install additional requirements? How can I use the detections to apply blurring on the image, and how can I return the result image?\n\nCan someone here please point me in the right direction? I searched a lot but can't find any example code or blogs that explain this process. Thank you in advance! Your help is much appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Getting stuck on SageMaker domain creation through the standard setup wizard",
        "Question_created_time":1650593854106,
        "Question_last_edit_time":1668428257770,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaiMX9ZU3SdqFnjxiNn_XRg\/getting-stuck-on-sagemaker-domain-creation-through-the-standard-setup-wizard",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":456,
        "Question_answer_count":2,
        "Question_body":"I am having difficulty in getting started on SageMaker Studio using the Standard setup wizard of SageMaker Domain creation. The wizard prevents me from moving forward at the RStudio setup step even though RStudio is indicated to be optional and which I don't need or want to pay for.  I have tried a lot of things (looked at the videos, searched the web) to no avail. Tried as a root user as well as a IAM user and have the same issue. Thanks for any help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Socket closed error when running inference using Greengrass",
        "Question_created_time":1650474528451,
        "Question_last_edit_time":1667926147223,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSJgKY9a2THi1lcmLUoAHdg\/socket-closed-error-when-running-inference-using-greengrass",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":178,
        "Question_answer_count":1,
        "Question_body":"Hello \n\nWe are using Greengrass v2 with the Sagemaker component in order to do inference on the edge.\nThe following error is seen intermettently while running our application. Could you please advise of anything we can do to avoid it?\n\nFile \"\/usr\/local\/lib\/python3.6\/dist-packages\/grpc\/_channel.py\", line 946, in __call__\nreturn _end_unary_response_blocking(state, call, False, None)\nFile \"\/usr\/local\/lib\/python3.6\/dist-packages\/grpc\/_channel.py\", line 849, in _end_unary_response_blocking\nraise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.UNAVAILABLE\ndetails = \"Socket closed\"\ndebug_error_string = \"{\"created\":\"@1650459124.460491853\",\"description\":\"Error received from peer unix:\/tmp\/aws.greengrass.SageMakerEdgeManager.sock\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":906,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\"",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[errno 28] no space left on disk",
        "Question_created_time":1650457037741,
        "Question_last_edit_time":1668510402331,
        "Question_link":"https:\/\/repost.aws\/questions\/QUInf7H_9bQpCPMROevLa2fA\/errno-28-no-space-left-on-disk",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":95,
        "Question_answer_count":0,
        "Question_body":"I am trying to install a python package called rapidsai in aws notebook instance but I am getting this error errno 28 no space left on disk. I am using g4dn.xlarge instance for it. The package is almost 3 GB. While opening the notebook instance I have tried increasing volume size of notebook upto 50 GB, I am still getting this error. Let me know the solution to it. Thanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to set up a pipe mode in sagemaker?",
        "Question_created_time":1650141519238,
        "Question_last_edit_time":1668521041514,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoMtSoBPGQpabpreLah_Fjg\/how-to-set-up-a-pipe-mode-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":226,
        "Question_answer_count":1,
        "Question_body":"what other data input types can be used pipe input mode in sagemaker? an example of implementation is here https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker-nysummit2018\/, and can this be used for inference as well as, similar to training?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to choose an instance type for  a sagemaker testing\/inference?",
        "Question_created_time":1650126925753,
        "Question_last_edit_time":1668552687320,
        "Question_link":"https:\/\/repost.aws\/questions\/QULxw59aBCRfmso_f7-VCjRQ\/how-to-choose-an-instance-type-for-a-sagemaker-testing-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":586,
        "Question_answer_count":2,
        "Question_body":"looking at few examples,  for training in sagemaker . are there some guidelines based on the model size, data to be trained , what type of instance cpu\/gpu to use? also, can one use spot instances ( may be with multiple gpu cores)?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Visualizing custom Model Monitor metrics in Sagemaker Studio",
        "Question_created_time":1650041844554,
        "Question_last_edit_time":1668626005636,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjbz0DSjCSoujr8vBAJvOOA\/visualizing-custom-model-monitor-metrics-in-sagemaker-studio",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":934,
        "Question_answer_count":2,
        "Question_body":"We are developing a custom model monitoring container to be used to interact with Sagemaker's model monitoring API, as we require additional custom metrics. One of our requirements is for these metrics to populate and visualize appropriately in Studio visualization tabs, e.g. Data Quality, Model Quality, and Explainability. **None of the built-in containers are an option for us, and we are trying to interop with the Monitoring APIs as seamlessly as possible.**\n\nThe docs specify the container contracts for output, specifically *statistics.json*, *constraints.json*, and *constraint_violations.json*. However, the docs are not clear on what JSON files to emit for specifically *custom Model Quality metrics*. There does not appear to be a provided schema.\n\nThings I have tried:\n- Emitting CloudWatch metrics to a different namespace, *sagemaker\/Endpoint\/model-quality-metrics*\n- Attempting to retrofit the statistics.json file to also include Model Quality metrics.\n\nIs there a separate JSON file I should write to *\/opt\/ml\/processing\/resultdata* for our custom model quality metrics to populate the Studio visualization tab?\n\nSpecifically, where do the base containers look for this JSON report: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Ground truth pdf annotation tool not rendering anything",
        "Question_created_time":1649874349265,
        "Question_last_edit_time":1668487286374,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzYkN4V8kRsWhT3ZSNJy9mQ\/sagemaker-ground-truth-pdf-annotation-tool-not-rendering-anything",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":450,
        "Question_answer_count":1,
        "Question_body":"Hello, I have followed these docs https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/cer-annotation-pdf.html  and have gotten to the point in which I have created the annotation task and I have uploaded several pdf's to a s3 bucket to be used for an annotation task so I can create a comprehend model. I put myself and a co-worker as annotators just so I can verify that I can set up the task properly and I only uploaded 37 pdf's. However when both of us log in and start the task, the webpage loads as the instructions tell us to however there is no pdf rendered (though I think I see it briefly flash on the screen before it goes blank) and there are also no entities to be selected as a part of the ui unlike how the documentation pictures the tool. I am trying to do named entity recognition and created this task with the full 25 entities I want to be able to label and Also another time with only 5 entities to label. However there seems to be something wrong with this native pdf annotation feature.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Without ECR being enabled in AWS account at Organization Level, what's the impact to SageMaker Studio?",
        "Question_created_time":1649820151027,
        "Question_last_edit_time":1668444527277,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXZB0Oki3QamlQ5ijtVSuzQ\/without-ecr-being-enabled-in-aws-account-at-organization-level-what-s-the-impact-to-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":183,
        "Question_answer_count":1,
        "Question_body":"I have a customer already standardized Artifactory as the centralized image registration. They disabled ECR service at Org level. Now we want to understand the potential impact on customer's day-2-day use of SageMaker Studio as a platform to support their full ML lifecycle. \n\n(If customer only use built-in SageMaker algorithm or framework and use prebuilt SageMaker container images)\n\nEspecially when customer trying to deploy the model to endpoint, does that need ECR service to be enabled in customer account?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1650300385539,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1650300385539,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi, yes, if you're restricting the user to only built-in algorithms and frameworks, and prebuilt images for Studio, you should be able to use it seamlessly (to deploy endpoints as well). That said, it severely restricts the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker requirements.txt unable to find certain packages",
        "Question_created_time":1649797399439,
        "Question_last_edit_time":1668530990017,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl7yzFGwfSEauI7yesYeXiw\/sagemaker-requirements-txt-unable-to-find-certain-packages",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":3,
        "Question_view_count":428,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI'm trying to run a sagemaker job on p3.2xlarge instance using the PyTorch estimator. My script has dependencies on several packages (possibly not pre-installed on the instance) for which I have a requirements.txt file. After loading the instance, it installs several packages but the job fails with following error:\n\n**sagemaker-training-toolkit ERROR InstallRequirementsError:\nCommand \"\/opt\/conda\/bin\/python3.6 -m pip install -r requirements.txt\"\nERROR: Could not find a version that satisfies the requirement scikit_image==0.18.3\nERROR: No matching distribution found for scikit_image==0.18.3**\n\nIt fails for other packages too, like h5py, numpy etc.\n\nAny help is greatly appreciated.\n\nThank you,\nAditya",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Question_created_time":1649794559479,
        "Question_last_edit_time":1668514048207,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sagemaker-multi-model-endpoint-and-inference-data-capture-feature",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":412,
        "Question_answer_count":1,
        "Question_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error.  See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\"\nTheoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig(\n                        enable_capture = True,\n                        sampling_percentage=100,\n                        destination_s3_uri=s3_capture_upload_path)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1649857157628,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1649857157628,
        "Answer_comment_count":1.0,
        "Answer_body":"SageMaker multi-model endpoints do not have support for SageMaker Model monitor as of writing this answer. So the error is pointing to exactly that. \n\nHowever, if you are looking to implement data drift using sagemaker model monitor then you can do that my mimicking data capture config functionality by capturing inference input and prediction output and storing it in the format supported by Model Monitor. And then setup a customer monitoring container using the instructions listed [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html]()",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Question_created_time":1649790570262,
        "Question_last_edit_time":1668627587288,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-P1Hlk4OR6K6kAug-wHT_g\/can-sagemaker-git-repositories-use-ssh-secrets-no-name-and-password",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1708,
        "Question_answer_count":1,
        "Question_body":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1649828878883,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1650645924386,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, Sagemaker can use SSH for private repos. There are multiple options on how to connect to a repo in Sagemaker.\n\n** Option 1**: Using SSH to work with a private repo\nYou can follow the same steps you do in your local machine to connect to a private repo through SSH, steps to follow:\n    \n1. Open `Terminal` and type `ssh-keygen` to create an SSH key in your Amazon Sagemaker instance. \n2. Add the public key to your Git account (Github or Gitlab)\n3. Get the SSH url of your repo and git clone\n\n**Option 2**: Using AWS Secret Manager \nYou can follow the steps in AWS official documentation [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-resource.html). \n\n\n**Option 3**: Using GitHub with Personal Access Tokens **Recommended**\n\nLet\u2019s assume you have already generated an Access Tokens through the GitHub\u2019s Settings \/ Developer Settings \/ Personal Access Tokens page.\n\nYou can just simply go ahead and clone the repository using Studio UI. When it asks your username and password, you can provide your GitHub username and the Personal Access Token. If you want to cache your credentials avoiding to type it every time when you\u2019re interacting with the GitHub server, you can cache or store it on your home folder with the following command issued in the Terminal:\n\n``` $ git config --global git credential.helper [cache|store] ```\n\nIf you choose to store your credentials, it will be written to the `~\/.git-credentials` file located in your home folder. The \u201ccache\u201d helper stores the credential in-memory only and never lands on disk. It also accepts the --timeout <seconds> option, which changes the amount of time its daemon is kept running (the default is \u201c900\u201d, or 15 minutes)\n\nBefore you make your first commit, you still need to configure the git client to use your identity when we\u2019re checking in some new code into the repository. You need to run the following two commands from the terminal:\n```\n$ git config --global user.email \u201cuser@email.com\u201d\n$ git config --global user.name \u201cUser Name\u201d\n```\n\nSagemaker Studio is fully integrated with git and you can do it through the UI.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to debug invocation timeout in sagemaker?",
        "Question_created_time":1649727502799,
        "Question_last_edit_time":1668588423842,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE4UPZjwNQveIG8zuZeXIgA\/how-to-debug-invocation-timeout-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1149,
        "Question_answer_count":1,
        "Question_body":"I am testing inference in sagemaker , by using one of the container listed here ->  https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below  and with in  inference.py file , i am overwriting functions like  model_fn method and predict_fn. I tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"Model server did not respond to \/invocations request within 3600 seconds\" . I'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch  is 6mb , the input files i'm using are way smaller than that but i still get that error. \n\nDirectory structure \n```\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n\n```\nfile : inference.py\n```\nimport torch\nimport os\n\ndef model_fn(model_dir):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\ndef predict_fn():\n    \/\/\n```\n based on docs here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker inference on inf1 no opencv",
        "Question_created_time":1649704234930,
        "Question_last_edit_time":1668603912097,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUG9fHLN3TNGbXjRmuwRBcA\/sagemaker-inference-on-inf1-no-opencv",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":255,
        "Question_answer_count":1,
        "Question_body":"I am trying to deploy Pytorch model on ml.inf1.xlarge instance.\nImage: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-neo-pytorch:1.5.1-inf-py3\nMy python code using some oepncv functions, and when I am trying to run the infernce I got the following error:\nModuleNotFoundError: No module named 'cv2'\n\nI tried to add opencv-python-headless to requirements.txt, but then I got another error \nImportError: libgthread-2.0.so.0: cannot open shared object file\n\nHow I can use opencv with the ml.inf1 instances?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to create Parallel Pipelines in Sagemaker",
        "Question_created_time":1649666083053,
        "Question_last_edit_time":1668586641818,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZtsbNf1GTAOnaTCrif-WJg\/is-it-possible-to-create-parallel-pipelines-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":862,
        "Question_answer_count":1,
        "Question_body":"I want to bind processing pipeline to multiple training pipeline. I just want to compare algorithm accuracy. Same dataset will be trained by multiple algorithms and will be predicted by them. My goal for the future is consolidate predict results of different algorithms and generate combined\/consolidated resulst. Is is possible to do in SageMaker.\n\nExample Schema:\n\n```\n            - Train_Algo1      \n Process    - Train_Algo2    - Predict Result\n            - Train_AlgoN\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker endpoint does not use latest approved model version",
        "Question_created_time":1649605467970,
        "Question_last_edit_time":1668091049728,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1qWaaww8TiWKj4MRv9DX2Q\/sagemaker-endpoint-does-not-use-latest-approved-model-version",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":69,
        "Question_answer_count":0,
        "Question_body":"I'm trying to troubleshoot why the sagemaker staging endpoint is not using the latest model version. I did the following steps:\n\n1. I started a **new Sagemaker project** and used the build, train, deploy and monitor model **template**.\n2. The pipeline automatically executes upon creating the project and I **approved that model**. The **endpoint was successfully created using this model version 1.**\n3. I cloned the repo, made a inconsequential change (just changed some training hyper params) and **pushed the changes**. Pipeline executes again successfully **creating model version 2.**\n4. I **approved the version 2** and deploy script also runs successfully it seems based on the deploy logs but the **endpoint keeps using version 1** of the model. (Also for some reason version 1 of the model appears as \"Model-xxxxxxx\" in the Models list while version 2 appears as \"Pipeline-xxxxxx-xxxxxx\"\n\nWould appreciate any help. Thank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unauthorized AWS account racked up charges on stolen credit card.",
        "Question_created_time":1649524594036,
        "Question_last_edit_time":1668539116236,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhV-lkkYyS1qaYFvsoPYiWg\/unauthorized-aws-account-racked-up-charges-on-stolen-credit-card",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":528,
        "Question_answer_count":1,
        "Question_body":"My mother was automatically signed up for an AWS account or someone used her credentials to sign up. She did not know that she had been signed up, and it sat unused for 3 years. Last month, she got an email from AWS for \"unusual activity\" and she asked me to help her look into it. Someone racked up $800+ in charges in 10 days for AWS services she has never heard of, let alone used (SageMaker, LightSail were among the services). The card on the AWS account is a credit card that was stolen years ago and has since been cancelled. So when AWS tried to charge the card, it didn't go through.\n\nMy experience with AWS customer service has been unhelpful so far. Mom changed her AWS password in time so we could get into the account and contact support. I deleted the instances so that the services incurring charges are now stopped. But now AWS is telling me to put in a \"valid payment method\" or else they will not review the fraudulent bill. They also said that I have to set up additional AWS services (Cost Management, Amazon Cloud Watch, Cloud Trail, WAF, security services) before they'll review the bill. I have clearly explained to them that this entire account is unauthorized and we want to close it ASAP, so adding further services and a payment method doesn't make sense. \n\nWhy am I being told to use more AWS services when my goal is to use zero? Why do I have to set up \"preventative services\" when the issue I'm trying to resolve is a PAST issue of fraud? They also asked me to write back and confirm that we have read and understood the AWS Customer Agreement and shared responsibility model.\" Of course we haven't, because we didn't even know the account existed! \n\nAny advice or input into this situation? It's extremely frustrating to be told that AWS won't even look into the issue unless I set up these additional AWS services and give them a payment method. This is a clear case of identity fraud. We want this account shut down. \n\nSupport Case # is xxxxxxxxxx.\n\nEdit- removed case ID -Ann D",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SKLearnProcessor Run() ConnectionTime  Out Issue",
        "Question_created_time":1649441115150,
        "Question_last_edit_time":1668559958042,
        "Question_link":"https:\/\/repost.aws\/questions\/QUylzH-GAKRj-vmGib16YyGg\/sklearnprocessor-run-connectiontime-out-issue",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":117,
        "Question_answer_count":1,
        "Question_body":"We ran into the following error when trying to call SageMaker SKlearn Container using SklearnProcess run().  Also the current boto3 version is 1.20.23 ( there is a github post indicated an issue with one boto3 release 1.16 with timeout issue).   Looks like a missing permission for STS service? Any other suggestions?\n\n- \/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/httpsession.py in send(self, request)\n    416             raise ProxyConnectionError(proxy_url=proxy_url, error=e)\n    417         except URLLib3ConnectTimeoutError as e:\n--> 418             raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n    419         except URLLib3ReadTimeoutError as e:\n    420             raise ReadTimeoutError(endpoint_url=request.url, error=e)\n\nConnectTimeoutError: Connect timeout on endpoint URL: \"https:\/\/sts.us-xxxx-x.amazonaws.com\/\"",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"NoCredentialsError: Unable to locate credentials when using dask_ml inside sagemaker",
        "Question_created_time":1649439202838,
        "Question_last_edit_time":1668165889080,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwsmuXhovR3SWjSp51P0Gtw\/nocredentialserror-unable-to-locate-credentials-when-using-dask-ml-inside-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":358,
        "Question_answer_count":1,
        "Question_body":"I'm trying to use dask in sagemaker because I have over 1B+ rows in a single dataset. Creating a dask.dataframe works fine, when I create a client through dask, it also works:\n\n```\nclient = Client(n_workers=6, threads_per_worker=20)\nclient\n```\nHowever when I try to use dask_ml.preprocessing.Categorizer, I get the error `NoCredentialsError: Unable to locate credentials`.\nI understand the issue might be dask distributed client doesn't have authorization to sagemaker client. Its confusing but how do I have them connect somehow? Code below:\n\n```\nimport dask.dataframe as dd\ndf3 = dd.read_parquet('s3:\/\/bucket\/parquetfiles2\/data_*.parquet',\n                     storage_options={'token': 'anon'})\nif __name__ == \"__main__\":\n    obj_df = df3.select_dtypes(include=['object','datetime64'])\n    num_df = df3.select_dtypes(exclude=['object','datetime64'])\n    ce = Categorizer()\n    obj_df_1 = ce.fit_transform(obj_df)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Spot instances for inference and sagemaker?",
        "Question_created_time":1649350121825,
        "Question_last_edit_time":1668581522685,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcQU2DOmNQdyI8HWeIMzzdg\/spot-instances-for-inference-and-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":576,
        "Question_answer_count":1,
        "Question_body":"Is it possible to deploy spot inf1 instances on sagemaker? We run an API 24\/7, and it's costly to keep it up, considering we only have 2 hours of peak performance a day. \n\nWe don't shut off those machines because we might have random bursts of traffic during the day that CPU instances can't hold.\nAlternatively, we could deploy spot EC2 inf machines; however, I'm unsure how I would invoke them from gateway and lambda. Does anybody have a tip or recommendation for our case?\n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Exporting Sagemaker model to local computer",
        "Question_created_time":1649274099076,
        "Question_last_edit_time":1668613197517,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgLZZhWVSg2d5XJWwbaTiA\/exporting-sagemaker-model-to-local-computer",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":894,
        "Question_answer_count":1,
        "Question_body":"I hyper-tuned an XGBoost model, deployed the model and created an endpoint. Is there a way to export the model to my local computer? That way I can test the model locally.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker endpoint creation fails for Multi Model",
        "Question_created_time":1649256875061,
        "Question_last_edit_time":1668185167767,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2bUNsPi3Rgautb0ZycrziA\/sagemaker-endpoint-creation-fails-for-multi-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":207,
        "Question_answer_count":1,
        "Question_body":"When using scikit to create multi model, it throws an exception, but when in single model it works.\n\nComplains about model_fn implementation or ping issues, any tips on how to fix this?\n\ne.g container={\\n\",\n    \"        'Image' : image_uri,\",\n    \"        'Mode': 'MultiModel',\",\n    \"        'ModelDataUrl': 's3:\/\/somepatch\/with\/all\/models\/,\",\n    \"        'Environment': {'SAGEMAKER_SUBMIT_DIRECTORY': mme_artifacts_path,\",\n    \"                               'SAGEMAKER_PROGRAM': 'inference.py'} \"\n\n\nFile \"\/miniconda3\/bin\/serve\", line 8, in <module>\nsys.exit(serving_entrypoint())\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 144, in serving_entrypoint\nstart_model_server()\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving_mms.py\", line 124, in start_model_server\nmodules.import_module(serving_env.module_dir, serving_env.module_name)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 263, in import_module\nsix.reraise(_errors.ImportModuleError, _errors.ImportModuleError(e), sys.exc_info()[2])\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 702, in reraise\nraise value.with_traceback(tb)\nFile \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 258, in import_module\nmodule = importlib.import_module(name)\nFile \"\/miniconda3\/lib\/python3.7\/importlib\/__init__.py\", line 118, in import_module\nif name.startswith('.'):",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"IncompleteSignature  error while using Sklearn SDK",
        "Question_created_time":1649218784916,
        "Question_last_edit_time":1667926634966,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtDHnpTdiTE66ph6OV5GDzA\/incompletesignature-error-while-using-sklearn-sdk",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":42,
        "Question_answer_count":0,
        "Question_body":"Currently, we are trying to SK-Learn model from a python script running in a local computer by uploading data to S3 bucket.\n\n```\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\n# container = retrieve(framework='sklearn', region='us-east-1', version=\"0.23-1\")\ncontainer = sagemaker.image_uris.get_training_image_uri('us-east-1', 'sklearn', framework_version='0.23-1') \nsklearn_estimator = SKLearn(\n    entry_point=\"script.py\",\n    # # role=get_execution_role(),\n    role = role_aws,\n    instance_count=1,\n    instance_type=\"ml.m5.4xlarge\",\n    framework_version=FRAMEWORK_VERSION,\n    base_job_name=\"rf-scikit\",\n    metric_definitions=[{\"Name\": \"median-AE\", \"Regex\": \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n    hyperparameters={\n        \"n-estimators\": 100,\n        \"min-samples-leaf\": 3,\n        \"features\": \"MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude\",\n        \"target\": \"target\",\n    },\n    sagemaker_session=session,\n    image_uri=container,\n    image_uri_region='us-east-1',\n    # output_path=model_output_path,\n)\n# launch training job, with asynchronous call\npath_train_test = 's3:\/\/'+bucket_name+'\/'+prefix\nsklearn_estimator.fit({\"train\": path_train_test, \"test\": path_train_test}, wait=False)\n```\n'ClientError: An error occurred (IncompleteSignature) when calling the GetCallerIdentity operation: \nCredential must have exactly 5 slash-delimited elements, e.g. keyid\/date\/region\/service\/term, got \n'https:\/\/elasticmapreduce.us-east-1b.amazonaws.com\/\/20220406\/us-east-1\/sts\/aws4_request'\nThe access key and the secret key are passed through the session object via a client and passed to the SK-Learn estimator. \n```\nclient_sagemaker = boto3.client('sagemaker', \n                  aws_access_key_id=accesskey , \n                  aws_secret_access_key=access_secret,\n                  )\nsession = sagemaker.Session(sagemaker_client =client_sagemaker )\n```\nThe same access key worked for Xgboost model (already available in sagemaker)\nAny ideas about the reason ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"can sagemaker batch transform process input files with new line character ?",
        "Question_created_time":1649208896916,
        "Question_last_edit_time":1668591990548,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrZKVALwvQjCcoBn4bYUayg\/can-sagemaker-batch-transform-process-input-files-with-new-line-character",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":331,
        "Question_answer_count":1,
        "Question_body":"example provided in the aws documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, see sample input csv can be structured like a sample below. is there a link, to see the actual csv file that is used in an working example  , instead of the sample posted in the docs. it is also mentioned that each record is one per line and no end of line character is allowed. what if the input type is other than csv, like json? do the same rules apply to json files as well, one record per line, or can one  record be spread across multiple lines ,what about new line character in that case?\n\n```\nRecord1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\nRecord2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n...\n...\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Question_created_time":1649205572690,
        "Question_last_edit_time":1668580983582,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-maxconcurrenttransforms-in-setting-up-a-sagemaker-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":368,
        "Question_answer_count":1,
        "Question_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1649246917948,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1649246917948,
        "Answer_comment_count":1.0,
        "Answer_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model. \n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the **memory** utilisation by your model. \n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Issue: AttributeError: 'AioClientCreator' object has no attribute '_register_lazy_block_unknown_fips_pseudo_regions'",
        "Question_created_time":1649181155664,
        "Question_last_edit_time":1668624296784,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1RBibWGuQSymNwJfXkln6A\/sagemaker-issue-attributeerror-aioclientcreator-object-has-no-attribute-register-lazy-block-unknown-fips-pseudo-regions",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":878,
        "Question_answer_count":3,
        "Question_body":"Hi, there\n\nWhen training model using sagemaker and calling panda.read_csv().. we are getting the above error.  It appears that this is a known issue and the workaround is to upgrade aiobotocore to the latest 2.20 release. However, the missing link is where to upgrade the aiobotocore .. since we are running model against SageMaker and its fully managed M5 instance... instead of an EC2 instance.  Any thoughts? \n\nError message in full:\n\/opt\/conda\/lib\/python3.7\/site-packages\/aiobotocore\/client.py in create_client(self, service_name, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, api_version, client_config)\n     43             service_client, endpoint_url, client_config\n     44         )\n---> 45         self._register_lazy_block_unknown_fips_pseudo_regions(service_client)\n     46         return service_client\n     47 \n\nAttributeError: 'AioClientCreator' object has no attribute '_register_lazy_block_unknown_fips_pseudo_regions'\n\nSee notes below under this link:\nhttps:\/\/github.com\/boto\/botocore\/pull\/2558\/files#",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is it possible to use Lambda functions along with other services to scale up and scale down(probably to 0 instances) Ec2 Deployed apps",
        "Question_created_time":1649176043559,
        "Question_last_edit_time":1668483340738,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl2PMA3JZRU2QmN7_knI7fQ\/is-it-possible-to-use-lambda-functions-along-with-other-services-to-scale-up-and-scale-down-probably-to-0-instances-ec2-deployed-apps",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":399,
        "Question_answer_count":2,
        "Question_body":"Hi there, hope you are fine.\nRecently I came across Sagemaker Async inference API, there we can scale down even to 0 instances. What I want is that I deploy my solution to EC2 instances using FastAPI, uvicorn and Celery or Rabbit-MQ(as message broker, for queuing). Then I can scale up and scale down instances based on traffic.\nAlso, if that's not the case, then I keep a minimal CPU instance on always and based on that I scale up and scale down GPU instances for handling requests.\n\n\n\nThanks  , for any help.\n\nBest Regards\nMuhammad Ali",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to automate sagemaker batch transform?",
        "Question_created_time":1649085888036,
        "Question_last_edit_time":1668627644322,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyENAstk3Q_--wYwScAIq-A\/how-to-automate-sagemaker-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1167,
        "Question_answer_count":1,
        "Question_body":"does cloudformation support sagemaker batch transform? if yes, can the jobs be triggered\/run automatically once the stack is created?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1649089461868,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1649089461869,
        "Answer_comment_count":1.0,
        "Answer_body":"While CloudFormation doesn't currently offer a resource for a SageMaker Batch Transform ([resource list here in the docs](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/AWS_SageMaker.html)), there are plenty of other integration points to automate running these jobs.\n\n### CloudFormation\n\nI'd actually argue that CloudFormation is probably not a great fit for this anyway because CloudFormation defines **resources** which can be created, updated, and deleted. I could maybe see a correspondence between \"Create\" = \"Run a job\", maybe \"Delete\" = \"Delete job outputs\", and possibly \"Update\" = \"Re-run the job\"? But these are opinionated choices that might not make sense in every case.\n\nIf you really wanted, you could create a [Custom CloudFormation resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources.html) backed by an AWS Lambda function using the [CreateTransformJob API](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html) (via whatever language you prefer e.g. [boto3 in Python](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_transform_job)).\n\nNote that:\n- If you wanted to use the [SageMaker Python SDK](https:\/\/sagemaker.readthedocs.io\/en\/stable\/) (`import sagemaker`, `Transformer`, etc) instead of the low-level boto3 interface in Python - you'd need to install this extra library in your Lambda function. Tools like AWS SAM and CDK can help with this.\n- The maximum Lambda timeout is 15 minutes, you may not want to keep your Lambda function running (billable) just waiting for the transform to complete anyway, and even the overall Custom Resource will have a longer max timeout within which it must stabilize after a create\/update\/delete request... So additional orchestration may be required beyond a single synchronous Lambda function call.\n\n### Other (better?) options\n\nAs mentioned above, you can create, describe and stop SageMaker Batch Transform jobs from any environment where you're able to call AWS APIs \/ use AWS SDKs... And you can even use the high-level open-source `sagemaker` SDK from anywhere you install it. Interesting options might include:\n\n- [Amazon SageMaker Pipelines](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html): SageMaker Pipelines have native \"steps\" for a range of SageMaker processes, including transform jobs but also training, pre-processing and more. You can define a multi-step pipeline from the SageMaker Python SDK (in your notebook or elsewhere) and then start it running on-demand (with parameters) by calling the [StartPipelineExecution API](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_StartPipelineExecution.html).\n- [AWS Step Functions](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/welcome.html): Step Functions provides general-purpose serverless orchestration so while the orchestration for SageMaker jobs in particular might be a little more complex (one step to start the job, then a polling check to check wait for completion) - the visual workflow editor and range of integrations to other services may be useful.\n- [Amazon S3 Lambda integrations](https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html) can trigger an event automatically (to start your transform job) when new data is uploaded to Amazon S3.\n- [Scheduled EventBridge Rules](https:\/\/docs.aws.amazon.com\/eventbridge\/latest\/userguide\/eb-create-rule-schedule.html) can run actions on a regular schedule (such as calling Lambda functions, kicking off these pipelines, etc) - in case you need a schedule-based execution rather than in response to some event.\n\nThe choice will depend on what the initial trigger for your workflow would be (schedule? Data upload? Some other AWS event? An API call from outside AWS?) and what other steps need to be orchestrated as well as your transform job in the overall flow.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is it possible to use smddp in notebook?",
        "Question_created_time":1649002671694,
        "Question_last_edit_time":1668434752193,
        "Question_link":"https:\/\/repost.aws\/questions\/QUselhJbg7SAShCfx3-8WU9Q\/is-it-possible-to-use-smddp-in-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":59,
        "Question_answer_count":0,
        "Question_body":"I recently tried the smddp v1.4.0 on SageMaker notebook instance (not sagemaker studio), using 8-GPU instances `ml.p3.16xlarge`, by directly using `smddp` as backend in the training scripts. I launched the estimator by setting `instance_type` to `local_gpu` and ended up with smddp error. Corresponding errors are attached below, saying an initialization error.\n\n```\n42u1m0wni0-algo-1-36bbw | Traceback (most recent call last):\n42u1m0wni0-algo-1-36bbw |   File \"true_main_notebook.py\", line 636, in <module>\n42u1m0wni0-algo-1-36bbw | main()\n42u1m0wni0-algo-1-36bbw |   File \"true_main_notebook.py\", line 178, in main\n42u1m0wni0-algo-1-36bbw | dist.init_process_group(backend=args.dist_backend)\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/distributed\/distributed_c10d.py\", line 576, in init_process_group\n42u1m0wni0-algo-1-36bbw | store, rank, world_size = next(rendezvous_iterator)\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/distributed\/rendezvous.py\", line 219, in _env_rendezvous_handler\n42u1m0wni0-algo-1-36bbw | rank = int(_get_env_or_raise(\"RANK\"))\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/distributed\/rendezvous.py\", line 203, in _get_env_or_raise\n42u1m0wni0-algo-1-36bbw |     raise _env_error(env_var)\n42u1m0wni0-algo-1-36bbw | ValueError: Error initializing torch.distributed using env:\/\/ rendezvous: environment variable RANK expected, but not set\n42u1m0wni0-algo-1-36bbw | Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n42u1m0wni0-algo-1-36bbw | Running smdistributed.dataparallel v1.4.0\n42u1m0wni0-algo-1-36bbw | Error in atexit._run_exitfuncs:\n42u1m0wni0-algo-1-36bbw | Traceback (most recent call last):\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/smdistributed\/dataparallel\/torch\/torch_smddp\/__init__.py\", line 51, in at_exit_smddp\n42u1m0wni0-algo-1-36bbw | hm.shutdown()\n42u1m0wni0-algo-1-36bbw | RuntimeError: Was this script started with smddprun? For more info on using smddprun, run smddprun -h\n42u1m0wni0-algo-1-36bbw | 2022-04-03 16:07:30,005 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n42u1m0wni0-algo-1-36bbw | 2022-04-03 16:07:30,005 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n42u1m0wni0-algo-1-36bbw | ExitCode 1\n42u1m0wni0-algo-1-36bbw | ErrorMessage \"ValueError: Error initializing torch.distributed using env:\/\/ rendezvous: environment variable RANK expected, but not set\n42u1m0wni0-algo-1-36bbw |  Environment variable SAGEMAKER_INSTANCE_TYPE is not set Error in atexit._run_exitfuncs: Traceback (most recent call last):   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/smdistributed\/dataparallel\/torch\/torch_smddp\/__init__.py\", line 51, in at_exit_smddp hm.shutdown() RuntimeError: Was this script started with smddprun? For more info on using smddprun, run smddprun -h\"\n```\n\nThe original goal is to launch a single-node smddp for debugging. \n\nDoes the smddp only support launched by AWS python SDK rather than the notebook? Or if something I've done is not correct?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker studio doesn't load and returns internal failure",
        "Question_created_time":1648852294743,
        "Question_last_edit_time":1668614502492,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ_rnD4H_REKOnpPfMp8hMg\/sagemaker-studio-doesn-t-load-and-returns-internal-failure",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":699,
        "Question_answer_count":1,
        "Question_body":"Trying to open sagemaker studio and keep getting the following error message: \n\nThe JupyterServer app default encountered a problem and was stopped. If you continue to experience issues, please contact Customer Service.\nDetails: InternalFailure\n\nHave restarted the app for couple of times and always the error message is the same with no progress. Any help would be appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to configure max concurrent transforms and instance count  parameter  in batch transform ?",
        "Question_created_time":1648673976146,
        "Question_last_edit_time":1668561703222,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWFExTH6UQsaefQYrJ0OHaQ\/how-to-configure-max-concurrent-transforms-and-instance-count-parameter-in-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":680,
        "Question_answer_count":2,
        "Question_body":"configuring a batch job for inference in sagemaker ( sample code below) . for my use case,  there are multiple input files and i'm trying to configure such that it can process files one by one or if the instance type allows process them in parallel. if value of max concurrrent transforms is greater than 1, does it processes file in parallel. as things are predefined for the job , before it starts , for exampel the instance_count is already set to 1. how does it do horizontal scaling? can it add add more instances. does the value of instance type or instance count dictate what value we can configure for max concurrent transforms parameter?\n\n\n```\nfrom sagemaker.transformer import Transformer\n\ntransformer = Transformer(model_name='my-previously-trained-model',\n                          max_concurrent_transforms=0\n                          instance_count=1,\n                          instance_type='ml.m4.xlarge')\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Is OrdinalEncoder on Scikit Learn available on AWS SageMaker SciKit Learn?",
        "Question_created_time":1648655583589,
        "Question_last_edit_time":1667887454370,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnRh2iyYuTYK_EXlYYF6QoQ\/is-ordinalencoder-on-scikit-learn-available-on-aws-sagemaker-scikit-learn",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":98,
        "Question_answer_count":2,
        "Question_body":"We'd like to use SKLearn.Preprocessing.OrdinalEncoder. However, it seems that the AWS SKLearn package version SageMaker is one that older than the SKLearn release 1.0.2 that have this encoding available. Any recommendations how to work around it?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How does sagmaker batch inference processes individual files?",
        "Question_created_time":1648600758921,
        "Question_last_edit_time":1668548598475,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKlZAPl2oRtGHPnpQrPcPXA\/how-does-sagmaker-batch-inference-processes-individual-files",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_body":"based on the documentation provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, large dataset can be structured as shown below in a csv file. is it possible to have multiple files in this format for batch inference, is there any configuration that can be set , for it to process multiple files. Also, what other formats , beside csv can the batch inference handle? \n\n\n```\nRecord1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\nRecord2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n...\n...\nRecordN-Attribute1, RecordN-Attribute2, RecordN-Attribute3, ..., RecordN-AttributeM  \n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Invoking endpoint outputs empty prediction data",
        "Question_created_time":1648559198447,
        "Question_last_edit_time":1668622013390,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9EktnduxRgmYc0dYxPBD4Q\/invoking-endpoint-outputs-empty-prediction-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":217,
        "Question_answer_count":1,
        "Question_body":"Hello,\n\nI am able to invoke my endpoint using the following command template: \n> aws --profile \u2018insert_profile_name\u2019 sagemaker-runtime invoke-endpoint --endpoint-name 'insert_endpoint_name' --body fileb:\/\/'insert_image_file_path' --region \u2018insert_region\u2019  --content-type application\/x-image output.txt\n\nHowever, this produces an output text file that contains the following:\n> {prediction\": []}\n\nAlso, this appears in the terminal after running the command:\n> {\n    \"ContentType\": \"application\/json\",\n    \"InvokedProductionVariant\": \"variant-name-1\"\n}\n\nThe image I used to invoke my endpoint was also used for training the model.\n\nHere is my training job configuration (values that I've modified or added): \n\n> **Job Settings:**\n\n> Algorithm - Object Detection | Input Mode - Pipe\n\n> **Hyperparameters:**\n\n> num_classes - 1 | mini_batch_size - 1 | num_training_samples - 1\n\n> **Input data configuration:**\n\n> *First channel:*\n\n> Name - validation | Input Mode - Pipe | Content Type - application\/x-recordio | Record Wrapper - RecordIO | S3 Data Type - AugmentedManifestFile | Attribute Names - source-ref, bounding-box\n\n> *Second channel:*\n\n> Name - train | Input Mode - Pipe | Content Type - application\/x-recordio | Record Wrapper - RecordIO | S3 Data Type - AugmentedManifestFile | Attribute Names - source-ref, bounding-box\n\nAny help would be appreciated. I can provide more information if needed. Thanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Trying Sagemaker example but getting error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job'",
        "Question_created_time":1648494191878,
        "Question_last_edit_time":1668629849076,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj8LepwTyQkq0ABgtX-nfew\/trying-sagemaker-example-but-getting-error-attributeerror-module-sagemaker-has-no-attribute-create-transform-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":599,
        "Question_answer_count":1,
        "Question_body":"Hi,\nI keep getting this error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job', when using a batch transform example that AWS graciously had in the notebook instances. Code:\n***Also, I updated Sagemaker to the newest package and its still not working.\n\n```\n%%time\nimport time\nfrom time import gmtime, strftime\n\nbatch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\ninput_location = \"s3:\/\/{}\/{}\/batch\/{}\".format(\n    bucket, prefix, batch_file\n)  # use input data without ID column\noutput_location = \"s3:\/\/{}\/{}\/output\/{}\".format(bucket, prefix, batch_job_name)\n\nrequest = {\n    \"TransformJobName\": batch_job_name,\n    \"ModelName\": 'xgboost-parquet-example-training-2022-03-28-16-02-31-model',\n    \"TransformOutput\": {\n        \"S3OutputPath\": output_location,\n        \"Accept\": \"text\/csv\",\n        \"AssembleWith\": \"Line\",\n    },\n    \"TransformInput\": {\n        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": input_location}},\n        \"ContentType\": \"text\/csv\",\n        \"SplitType\": \"Line\",\n        \"CompressionType\": \"None\",\n    },\n    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n}\n\nsagemaker.create_transform_job(**request)\nprint(\"Created Transform job with name: \", batch_job_name)\n\n# Wait until the job finishes\ntry:\n    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\nfinally:\n    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n    status = response[\"TransformJobStatus\"]\n    print(\"Transform job ended with status: \" + status)\n    if status == \"Failed\":\n        message = response[\"FailureReason\"]\n        print(\"Transform failed with the following error: {}\".format(message))\n        raise Exception(\"Transform job failed\")\n```\n\nEverything else is working well. I've had no luck with this on anyother forum.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"In Sagemaker endpoint, pip download fails when connected through VPC",
        "Question_created_time":1648489585321,
        "Question_last_edit_time":1668519768699,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7Q7e634rRAidb_GUu2ZhXw\/in-sagemaker-endpoint-pip-download-fails-when-connected-through-vpc",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":171,
        "Question_answer_count":1,
        "Question_body":"Pip download fails in my instance when sagemaker is connected through VPC. It is successful when VPC is not specified. I have internet gateway configured for my public subnets. \nI was able to pip download successfully in EC2 with same security groups and subnets. Is this any bug in sagemaker side?\n\nAnd also, can we log into EC2 instance that sagemaker created? It is more of like a blackbox testing without it.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker XGBoost Parquet Example Code Fails and Errors out. Bug?",
        "Question_created_time":1648146766576,
        "Question_last_edit_time":1668604200245,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqqbIbodsT42efRxxi1FLzw\/sagemaker-xgboost-parquet-example-code-fails-and-errors-out-bug",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":306,
        "Question_answer_count":1,
        "Question_body":"Hi, \nI'm trying to run the SageMaker XGBoost Parquet example [linked here](https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_parquet_input_training.html). I followed the exact same steps but using my own data. I uploaded my data, converted it to a pandas df. The train_df shape is (15279798, 32) while the test_df shape is (150848, 32). I then converted it to parquet files and uploaded it to an S3 bucket - per example instructions. \n\nMy error is as follows:\n\n```\nFailure reason\nAlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/data_utils.py\", line 422, in _get_parquet_dmatrix_pipe_mode data = np.vstack(examples) File \"<__array_function__ internals>\", line 6, in vstack File \"\/miniconda3\/lib\/python3.7\/site-packages\/numpy\/core\/shape_base.py\", line 283, in vstack return _nx.concatenate(arrs, 0) File \"<__array_function__ internals>\", line 6, in concatenate ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 32 and the array at index 1 has size 9 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py\", line 94, in main train(framework.tr\n\n```\nBut I'm confused because the train and test are the same shape and I added no extra code. My code below:\n\n\n```\n# requires PyArrow installed\ntrain.to_parquet(\"Xgb_train.parquet\")\ntest.to_parquet(\"Xgb_test.parquet\")\n\n%%time\nsagemaker.Session().upload_data(\n    \"Xgb_train.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptrain\"\n)\n\nsagemaker.Session().upload_data(\n    \"Xgb_test.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptest\"\n)\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\njob_name = \"xgboost-parquet-example-training-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"Training job\", job_name)\n\n# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n\ncreate_training_params = {\n    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"Pipe\"},\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\"S3OutputPath\": bucket_path + \"\/\" + prefix + \"\/single-xgboost\"},\n    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 20},\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"max_depth\": \"5\",\n        \"eta\": \"0.2\",\n        \"gamma\": \"4\",\n        \"min_child_weight\": \"6\",\n        \"subsample\": \"0.7\",\n        \"objective\": \"reg:linear\",\n        \"num_round\": \"10\",\n        \"verbosity\": \"2\",\n    },\n    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptrain\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptest\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n    ],\n}\n\n\nclient = boto3.client(\"sagemaker\", region_name=region)\nclient.create_training_job(**create_training_params)\nprint(client)\nstatus = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\nprint(status)\nwhile status != \"Completed\" and status != \"Failed\":\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n    print(status)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1648149098277,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1648149098277,
        "Answer_comment_count":0.0,
        "Answer_body":"I just changed my bucket name and file names. It worked now.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Async Inference not able to process later requests",
        "Question_created_time":1648126119561,
        "Question_last_edit_time":1668612620454,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqAl1qUyYRK-cbY3DGH-X9g\/async-inference-not-able-to-process-later-requests",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":2,
        "Question_view_count":354,
        "Question_answer_count":2,
        "Question_body":"Hi there, hope all of you are fine.\n\nI am trying to deploy a train-on-inference type model. I am done with BYOC, and it is working completely fine with real-time inference endpoints. Also, I am able to make it work with Async inference, and concurrent requests on the same instance are also being handled. \nBut, the later requests, never get processed, without any logical error. Also once the endpoint gets scaled down to 0 instance, it fails to scales up.\n\nThese are some of error and warning messages which I get intermittently:\n\n```\n\n\ndata-log:\n2022-03-23T11:23:17.723:[sagemaker logs] [5ea751c9-9271-4533-bc09-c117791e1372] Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n\n\nwarnings:\n\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n```\n\nKindly help me with this. \nThanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Training Metric logging on SageMaker experiment tracking: how to get time-series metrics with visualisation",
        "Question_created_time":1648058261935,
        "Question_last_edit_time":1668624556079,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDNp9HXW9SCqdadORoXUX9g\/training-metric-logging-on-sagemaker-experiment-tracking-how-to-get-time-series-metrics-with-visualisation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":1616,
        "Question_answer_count":2,
        "Question_body":"I am using the sagemaker python SDK to train a bespoke model. I have defined my `metric_definition` regexes and passed them to the estimator like:\n\n```python\nnum_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\nmetrics = [\n    {\"Name\": \"learning-rate\", \"Regex\": f\"lr: {num_re}\"},\n    {\"Name\": \"training:loss\", \"Regex\": f\"loss: {num_re}\"},\n    # ...\n]\nestimator = Estimator(\n    image_uri=training_image_uri,\n    # ...\n    metric_definitions=metrics,\n    enable_sagemaker_metrics=True,\n)\n```\n \nWhen I run training, these metrics are visible in my logs and I can also see them in SageMaker Studio in `Trial Components > Metrics (tab)` as a grid of numbers like:\n\n> Name | Minimum | Maximum | Standard Deviation | Average | Count | Final value \n\n> learning-rate | 8.889 | 8.907 | 0.010392304845413657 | 8.898 | 4 |8.907\n\n> ...\n\nWhich suggests that the regexes are correctly matching on the logs\n\nHowever, I am not able to visualise any graphs for my metrics. I have tried all of:\n- `Sagemaker Studio > Trial components > charts`. It is only possible to plot things like `learning-rate_min` (i.e. a point value not a time-series metric)\n- `SageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section`. Here I can see metrics like CPUUtilization over time but for my metrics there is just an empty graph for each metric that I have defined that says 'No data available' \n- `SageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section > View algorithm metrics (opens in CloudWatch) > Browse > select metric (e.g. learning-rate and 'Add to Graph' `. I filter by the correct time period and go the `Graphed metrics (1) tab`, even after updating the period to `1 second` I am not able to see anything on the graph. \n\nI'm not sure what the issue is here but any help would be much appreciated",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to use custom_attribute in sagemaker api?",
        "Question_created_time":1647897988473,
        "Question_last_edit_time":1668543284899,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyzl7vjQGSVGdbnkxv8HKxA\/how-to-use-custom-attribute-in-sagemaker-api",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":539,
        "Question_answer_count":1,
        "Question_body":"based on documentation provided here -> https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker-runtime\/invoke-endpoint-async.html\nI am passing  a custom attributes parameter when calling the invoke-endpoint-async function. \n\n```\n  invoke-endpoint-async\n--endpoint-name <value>\n[--custom-attributes <value>]\n[--inference-id <value>]\n--input-location <value>\n\n```\n\nare there any sample\/example on how can i read this in my code before\/after invoking my model for inference. \n\nI am creating a preprocessing file and have input_fn and predict_fn function, can value passed in custom_attributes during api call be read or written to the response here? also, if it can be written to the response like documentation says, where can i see it . as the async endpoint only gives the output location when invoked and later process the request, i don't see the response, i just see the dumped out file in the specified output location. how can i see the full response?\n```\nsagemaker_model = TensorFlowModel(model_data = 'model_data_path',\nrole = execution_role,\nframework_version = '1.12',\nsource_dir ='src',\nentry_point = 'preprocessing.py', \n...)\n```\nfile: preprocessing.py\n```\ndef input_fn(request_body, content_type):\n    \/\/read the custom attribute here \n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can AWS Batch jobs use SageMaker FastFile mode?",
        "Question_created_time":1647671103679,
        "Question_last_edit_time":1668600288070,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzNeAEdKTQCKDfDKqGcZojw\/can-aws-batch-jobs-use-sagemaker-fastfile-mode",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":253,
        "Question_answer_count":1,
        "Question_body":"Can a regular (non-machine learning) Python job submitted via AWS Batch make use of SageMaker [FastFile mode](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/) to stream data from S3 to the container? Or is there any equivalent of SageMaker FastFile mode in other Python libraries? The key point is about whether it's possible to avoid the need to copy data from S3 to the EBS or instance store volume before processing.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Bad RMSE when predicting Price with Linear Regression",
        "Question_created_time":1647526960709,
        "Question_last_edit_time":1667926508524,
        "Question_link":"https:\/\/repost.aws\/questions\/QUly8ruHWYTCy_MEaBQfz4ZA\/bad-rmse-when-predicting-price-with-linear-regression",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":65,
        "Question_answer_count":3,
        "Question_body":"Hi. I have a dataset of price data. It looks like this\n\n\n\n| Price| Branch| ItemCode| Discount| DateTimeOfPrice|\n| --- | --- | --- | --- | --- |\n| 10| 002| 52345436| 0.33| 2022-03-24 14:00|\n\n\n\nThe dataset has about 1M records\n\nI feature engineered it in the following way\n\n| Price| Discount| ItemCode| Year| Month| Day| Hour| Branch1| Branch2| Branch3|\n| --- | --- | --- | --- | --- |--- | --- | --- | --- | --- |\n| 10 | 0.33| 52345436| 2022 | 03 |24 | 14 | 0 | 1 | 0 |\n\n\n\nEach component of the DateTimeOfPrice got a separate column\nWe have 3 branches. To avoid the situation when algorithm may think that \"branch\" column is some kind of priority column, I created 3 new column (we have 3 branches). If the item belongs to branch2, the column will get the value 1, if not - it will be 0\n\nI run Linear Learner, XGBoost build-in algorithms and also SageMaker AutoPilot.\nIn all cases I run , the best RMSE was 60 and prediction\/ validation gives sometimes a result which is far from the actual value.\nI tried also to run XGBoost from the notebook with the following parameters\n\n```\nhyperparams = {\n    \"max_depth\": \"7\",\n    \"eta\": \"0.2\",\n    \"gamma\": \"4\",\n    \"min_child_weight\": \"6\",\n    \"subsample\": \"0.7\",\n    \"objective\": \"reg:squarederror\",\n    \"num_round\": \"100\",\n    \"eval_metric\":\"rmse\",\n    \"verbosity\": \"2\",\n}\n```\n\n\n\nStill, the RMSE is arround 60.\n\nPlease advice what can be done to improve the mertic and predication",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Question_created_time":1647503861594,
        "Question_last_edit_time":1668586573828,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-stepfunctions-sagemaker-s-invokeendpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":305,
        "Question_answer_count":1,
        "Question_body":"I have a state-machine workflow with 3 following states:  \n\n[screenshot-of-my-workflow](https:\/\/i.stack.imgur.com\/4xJTE.png)   \n\n1. A 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (*this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.*) \n2. Use map to call SageMaker endpoints dictated by the array(or list) from above result.\n3. Send the result of above 'Map' to a Lambda function and exit the workflow.\n\n\nHere's the entire workflow in .asl.json, inspired from [this aws blog](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-map-state.html).\n```\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n```\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n```\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n```\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.  \n[screenshot-of-graph-inspector](https:\/\/i.stack.imgur.com\/3gXsM.jpg)\n\nWhat's causing the error and How do I fix this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1647513967263,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1647513967264,
        "Answer_comment_count":0.0,
        "Answer_body":"In general (as mentioned [here in the parameters doc](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-parameters.html)), you also need to **end the parameter name** with `.$` when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. `\"InvocationBody.$\": \"$.body.InputData\"`), but not in others (`\"EndpointName\": \"$.EndpointName\"`), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret `$.EndpointName` as literally the name of the endpoint (which doesn't satisfy `^[a-zA-Z0-9](-*[a-zA-Z0-9])*`!)\n\nSo suggest you change to `EndpointName.$` and `Body.$` in your InvokeEndpoint parameters",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Inspection of algorithm containers for Sagemaker",
        "Question_created_time":1647409671693,
        "Question_last_edit_time":1667926089866,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPDqf4t6_TGajnssgca9qNA\/inspection-of-algorithm-containers-for-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":2,
        "Question_view_count":87,
        "Question_answer_count":2,
        "Question_body":"We plan to provide a machine learning algorithm via a [container image](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html) and are concerned about. Is it possible that other parties download the docker image for local inspection?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Batch Transform Job Failure: Timeout Issue and Job Restarted Unexpectedly",
        "Question_created_time":1647373610390,
        "Question_last_edit_time":1667925940305,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHEfg4SyhSsO3CYaqLfefUQ\/sagemaker-batch-transform-job-failure-timeout-issue-and-job-restarted-unexpectedly",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":64,
        "Question_answer_count":0,
        "Question_body":"I am using the batch transform function in SageMaker for the inference of my PyTorch model. I am using the same structure as https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/advanced_functionality\/scikit_bring_your_own\/container.\nThe error is that my job will start multiple times on different workers if I choose multiple workers. Or it will repeat after finish if I choose 1 worker.\n\nI think it should be some errors in **timeout** setup. I have tried to increase the **keepalive_timeout** and **proxy_read_timeout** in the serve file and tried the **SAGEMAKER_MODEL_SERVER_TIMEOUT** as an environment variable. But nothing worked. Could some one help? Thanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to set up autoscaling for async sagemaker endpoint?",
        "Question_created_time":1646861290947,
        "Question_last_edit_time":1668594134225,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjLx7h0TR0q1KwubwQjU9A\/how-to-set-up-autoscaling-for-async-sagemaker-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":600,
        "Question_answer_count":1,
        "Question_body":"working with an example documented here -> https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb. I was able to set up the sagemaker model, config and aync endpoint via lambda, now I'm trying to re-create the stack via terraform. based on the documentation on terraform, i was able to set up the model, config and the endpoint but couldn't find how to go about setting up the auto scaling ( sample code below). \nis this possible?\n\n\n```\nclient = boto3.client(    \"application-autoscaling\") \nresource_id = (    \"endpoint\/\" + endpoint_name + \"\/variant\/\" + \"variant1\")  \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  \n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  \nSageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,\n   ....\n    },\n)\n```\nclean up\n```\nresponse = client.deregister_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId='resource_id',\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can we run a python script in Sagemaker using boto3 from a local machine?",
        "Question_created_time":1646819726632,
        "Question_last_edit_time":1668599887299,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjnuGv6KCRaS9BCxzVgCYyA\/can-we-run-a-python-script-in-sagemaker-using-boto3-from-a-local-machine",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1780,
        "Question_answer_count":1,
        "Question_body":"Here's what I am trying to do:\nIn my application that resides outside aws, I take some user inputs, and trigger scripts that reside inside Sagemaker notebook instance. I am able to start or create a new instance using boto3, and also use lifecycle configuration to run some starter script while the instance turns on. But I want to run multiple scripts in short intervals based on user inputs, so I don't want to restart my instance each time with a new lifecycle configuration script.\nI am trying to find if there is a way to execute shell commands in sagemaker using boto3 (or any other way).",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Usage of Sagemaker Processing Job Manifest File",
        "Question_created_time":1646687511442,
        "Question_last_edit_time":1668432574778,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAKFZ3-NLSIuWWSesoqRPRQ\/usage-of-sagemaker-processing-job-manifest-file",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":932,
        "Question_answer_count":1,
        "Question_body":"I have a processing Job that uses input files saved in different folders of a S3 Bucket and use the Manifest file within the processing Job to copy it to \/opt\/ml\/processing\/input Folder.\n\nThis works perfectly fine when i have all the files in one folder but wont work when they are under the same prefix but under different folders.\n\nFollowing the steps listed in the url https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_S3DataSource.html\n\n[ {\"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\"},\n\n\"relative\/path\/to\/custdata-1\",\n\n\"relative\/path\/custdata-2\",\n\n...\n\n\"relative\/path\/custdata-N\"\n\n]\n\nIf i have all the input files in \"relative\/path1\/custdata-1\" The job works fine but if i add another one \"relative\/path2\/custdata-2\", there is no file copied and my script fails with no such file or directory.\n\nAny suggestions or advise on this will be very helpful.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Load balancing is not happening on sagemaker batch transform job",
        "Question_created_time":1646666388010,
        "Question_last_edit_time":1668561143445,
        "Question_link":"https:\/\/repost.aws\/questions\/QUniirbKJST5KVrdCgXlPv0w\/load-balancing-is-not-happening-on-sagemaker-batch-transform-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":163,
        "Question_answer_count":1,
        "Question_body":"Hi All,\n\nGreetings!!\n\nWe have two issues in sagemaker batch transform job.\n\n1. Load balancing is not happening with two instances even after CPU utilization = 200% and GPU utilization = 81% of single instance and 2nd instance was complete idle.\n\n   Transform job arguments:\n\n   MaxConcurrentTransforms: 2, \n   MaxPayloadInMB: 1,\n   BatchStrategy: MultiRecord,\n   InstanceCount=2\n\n2. Batch transform job is failing after 20 minutes without any errors in cloud watch logs but noticed that CPU utilization = 200% and GPU utilization = 81% of single instance and 2nd instance was complete idle.\n\nCould you please have a look?\n\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Setting up data for DeepAR, targets and categories for simultaneous data?",
        "Question_created_time":1646522787273,
        "Question_last_edit_time":1668530871494,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT2YRXdWxS2-ORoyS5CXF7w\/setting-up-data-for-deepar-targets-and-categories-for-simultaneous-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":227,
        "Question_answer_count":1,
        "Question_body":"I would like to try out DeepAR for an engineering problem that I have some sensor datasets for, but I am unsure how to set it up for ingestion into DeepAR to get a predictive model.\n\nThe data is essentially the positions, orientations, and a few other timeseries sensor readings of an assortment of objects (animals, in this case, actually) over time. Data is both noisy and sometimes missing.\n\nSo, in this case, there are N individuals and for each individual, there are Z variables of interest per individual. None of the variables are \"static\" (color, size, etc), they are all expected to be time-varying on the same time scale.\nUltimately, I would like to try and predict all Z targets for all N individuals.\n\nHow do I set up the timeseries to feed into DeepAR?\nThe premise is that all these individuals are implicitly interacting in the observed space, so all the target values have some interdependence on each other, which is what I would like to see if DeepAR can take into account to make predictions. \n\nShould I be using a category vector of length 2, such that the first cat variable corresponds to the individual, and the second corresponds to one of the variables associated with the individual?\nThen there would be N*Z targets in my input dataset, each with `cat = [ n , z ]`, where there are N distinct values for n, and z for Z?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to prevent disassociating SageMaker LifecycleConfig unintentionally",
        "Question_created_time":1646363723581,
        "Question_last_edit_time":1667926176497,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTvkDhX_yQXyW7WpFinO_vA\/how-to-prevent-disassociating-sagemaker-lifecycleconfig-unintentionally",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":48,
        "Question_answer_count":0,
        "Question_body":"When you go to SageMaker Notebook Instance edit screen in AWS Web Console (to change the Instance Type for example), it is sometimes the case that `Lifecycle configuration` is popped up as `No Configuration` even though the configuration is actually set earlier. This results in an unintentional disassociation of the LifecycleConfig because it's easy to save the instance change without noticing the change in Lifecycle Config. This is a serious problem for us. I was able to reproduce this issue in Chrome and Firefox (but you need to try several times to repro the issue).\n\nI am in the position of provisioning different cloud resources for the end users and I need a way to systematically prevent this disassociation to happen. I considered applying an IAM policy that denies the update operation containing the change in the LifecycleConfig of notebooks, but there seems [no condition key for LifecycleConfig](https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html) which makes me think this approach isn't feasible.\n\nWhat can I do?\n\nThanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"what are some ways\/alternative to expose sagemaker endpoints as a HTTP \/REST endpoints?",
        "Question_created_time":1646086454991,
        "Question_last_edit_time":1668621495978,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCR-voc-dSKa9Og24XXs_2A\/what-are-some-ways-alternative-to-expose-sagemaker-endpoints-as-a-http-rest-endpoints",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":2,
        "Question_view_count":467,
        "Question_answer_count":1,
        "Question_body":"I am testing out serverless sagemaker endpoints and  was  planning  to integrate it with api gateway directly, but realized there is a 29 seconds timeout limit in api gateway, which might not work if the endpoints take longer than that for inference. is there any workaround for this apart from adding a lambda in-between? I am trying to avoid lambda as it might add to the latency",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker - All metrics in statistics.json by Model Quality Monitor are \"0.0 +\/- 0.0\", but confusion matrix is built correctly for multi-class classification!!",
        "Question_created_time":1645965956086,
        "Question_last_edit_time":1667926077051,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOOz6SJnzR7-VDglJ1rAW8Q\/sagemaker-all-metrics-in-statistics-json-by-model-quality-monitor-are-0-0-0-0-but-confusion-matrix-is-built-correctly-for-multi-class-classification",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":78,
        "Question_answer_count":0,
        "Question_body":"I have scheduled an hourly model-quality-monitoring job in AWS SageMaker. both the jobs, ground-truth-merge and model-quality-monitoring completes successfully without any errors. but, all the metrics calculated by the job are \"0.0 +\/- 0.0\" while the confustion matrix gets calculated as expected. \n\nI have done everything as mentioned in [this notebook for model-quality-monitoring from sagemaker-examples](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_model_monitor\/model_quality\/model_quality_churn_sdk.ipynb) with very few changes and they are:  \n\n1. I have changed the model from xgboost churn to model trained on my data.  \n2. my input to the endpoint was csv like in the example-notebook, but output was json.  \n3. i have changed the problem-type from BinaryClassfication to MulticlassClassification wherever necessary.\n\nconfustion matrix was built successfully, but all metrics are 0 for some reason. So, I would like the monitoring job to calculate the multi-classification metrics on data properly.\n\n**All Logs**\n\nHere's the `statistics.json` file that model-quality-monitor saved to S3 with confustion matrix built, but with 0s in all the metrics:\n\n```\n{\n  \"version\" : 0.0,\n  \"dataset\" : {\n    \"item_count\" : 4432,\n    \"start_time\" : \"2022-02-23T03:00:00Z\",\n    \"end_time\" : \"2022-02-23T04:00:00Z\",\n    \"evaluation_time\" : \"2022-02-23T04:13:20.193Z\"\n  },\n  \"multiclass_classification_metrics\" : {\n    \"confusion_matrix\" : {\n      \"0\" : {\n        \"0\" : 709,\n        \"2\" : 530,\n        \"1\" : 247\n      },\n      \"2\" : {\n        \"0\" : 718,\n        \"2\" : 497,\n        \"1\" : 265\n      },\n      \"1\" : {\n        \"0\" : 700,\n        \"2\" : 509,\n        \"1\" : 257\n      }\n    },\n    \"accuracy\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_recall\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_precision\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f0_5\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f1\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f2\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"accuracy_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_recall_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_precision_best_constant_classifier\" : {\n      \"value\" : 0.1124185852154987,\n      \"standard_deviation\" : 0.0021869336610830254\n    },\n    \"weighted_f0_5_best_constant_classifier\" : {\n      \"value\" : 0.12965524348784485,\n      \"standard_deviation\" : 0.0024239410000317335\n    },\n    \"weighted_f1_best_constant_classifier\" : {\n      \"value\" : 0.16838092925822584,\n      \"standard_deviation\" : 0.0028615098045768348\n    },\n    \"weighted_f2_best_constant_classifier\" : {\n      \"value\" : 0.24009212108475822,\n      \"standard_deviation\" : 0.003326031863819311\n    }\n  }\n}\n```\n\nHere's how couple of lines of captured data looks like(*prettified for readability, but each line has no tab spaces as shown below*) :\n\n```\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,1,628,210,30\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Transfer\\\",\\\"prediction\\\":2,\\\"probabilities\\\":[0.228256680901919,0.0,0.7717433190980809]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"a7cfba60-39ee-4796-bd85-343dcadef024\",\n        \"inferenceId\": \"5875\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,3,628,286,240\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.99,0.005,0.005]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"7391ac1e-6d27-4f84-a9ad-9fbd6130498a\",\n        \"inferenceId\": \"5876\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n```\n\nHere's couple of lines from my ground-truths that I have uploaded to S3 look like(*prettified for readability, but each line has no tab spaces as shown below*):\n\n```\n{\n  \"groundTruthData\": {\n    \"data\": \"0\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"1\"\n  },\n  \"eventVersion\": \"0\"\n}\n{\n  \"groundTruthData\": {\n    \"data\": \"1\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"2\"\n  },\n  \"eventVersion\": \"0\"\n},\n```\n\nHere's couple of lines from the ground-truth-merged file look like(*prettified for readability, but each line has no tab spaces as shown below*). this file is created by the ground-truth-merge job, which is one of the two jobs that model-quality-monitoring schedule runs:\n\n```\n{\n  \"eventVersion\": \"0\",\n  \"groundTruthData\": {\n    \"data\": \"2\",\n    \"encoding\": \"CSV\"\n  },\n  \"captureData\": {\n    \"endpointInput\": {\n      \"data\": \"1,2,1050,37,1095\",\n      \"encoding\": \"CSV\",\n      \"mode\": \"INPUT\",\n      \"observedContentType\": \"text\/csv\"\n    },\n    \"endpointOutput\": {\n      \"data\": \"{\\\"label\\\":\\\"Return_to_owner\\\",\\\"prediction\\\":1,\\\"probabilities\\\":[0.14512373737373732,0.6597074314574313,0.1951688311688311]}\\n\",\n      \"encoding\": \"JSON\",\n      \"mode\": \"OUTPUT\",\n      \"observedContentType\": \"application\/json\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"c9e21f63-05f0-4dec-8f95-b8a1fa3483c1\",\n    \"inferenceId\": \"4432\",\n    \"inferenceTime\": \"2022-02-23T04:00:00Z\"\n  }\n}\n{\n    \"eventVersion\": \"0\",\n    \"groundTruthData\": {\n        \"data\": \"1\",\n        \"encoding\": \"CSV\"\n    },\n    \"captureData\": {\n        \"endpointInput\": {\n            \"data\": \"0,2,628,5,90\",\n            \"encoding\": \"CSV\",\n            \"mode\": \"INPUT\",\n            \"observedContentType\": \"text\/csv\"\n        },\n        \"endpointOutput\": {\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.7029623691085284,0.0,0.29703763089147156]}\\n\",\n            \"encoding\": \"JSON\",\n            \"mode\": \"OUTPUT\",\n            \"observedContentType\": \"application\/json\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"5f1afc30-2ffd-42cf-8f4b-df97f1c86cb1\",\n        \"inferenceId\": \"4433\",\n        \"inferenceTime\": \"2022-02-23T04:00:01Z\"\n    }\n}\n```\n\nSince, the confusion matrix was constructed properly, I presume that I fed the data to sagemaker-model-monitor the right-way. But, why are all the metrics 0.0, while confustion-matrix looks as expected?\n\n\nEDIT 1:  \nLogs for the job are available [here](https:\/\/controlc.com\/1e1781d2).",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"MLOps Query regarding SageMaker Project template and Pipelines",
        "Question_created_time":1645816119455,
        "Question_last_edit_time":1668592003555,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbOWLdrrNStmzDzTMOcdG3Q\/mlops-query-regarding-sagemaker-project-template-and-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":203,
        "Question_answer_count":1,
        "Question_body":"Hi MLOps Gurus,\n\n I'd like to seek guidance on my below situation. \n\nThis is regarding Sagemaker Project creation in AWS. The use case is to take final model (built by DS team) from S3 and do all sorts of Bias analysis using Clarify and upon acceptance of its response(Bias & Explainability reports) by Data Scientists, deploy to Model Monitor. Now, as there is no prebuilt Sagemaker template that caters to my use case, how shall I initiate this whole process and how shall I create a ML project at first place.\n\nAlso, how shall I initiate a notebook from within my cloudformation template for DS Team to kick off from Service Catalogue to automate the above process?\n\nAny pointers would be greatly appreciated.\n\nRegards,\nNikhil",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"API Gateway Resource Policy Not Working - IP Address Allow List",
        "Question_created_time":1645811510068,
        "Question_last_edit_time":1668622876229,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTc-jSxqlSXCueAqrINQF-Q\/api-gateway-resource-policy-not-working-ip-address-allow-list",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1079,
        "Question_answer_count":1,
        "Question_body":"API Gateway Experts, I want to invoke an API that triggers a lambda function to create a SageMaker instance pre-signed URL. They would like to deny access to the API to only a particular user\u2019s source IP address. I have provide the YAML used to deploy this, but it is still allowing all other IP addresses to interact with the API even with the resource policy. What am I missing?\n\n```\n  ApiGatewayRestApi:\n    Type: AWS::ApiGateway::RestApi\n    Properties:\n      ApiKeySourceType: HEADER\n      Description: An API Gateway with a Lambda Integration\n      EndpointConfiguration:\n        Types:\n          - EDGE\n      Name: lambda-sagemaker-presigned-url-api\n      Policy: !Sub |\n        {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Deny\",\n              \"Principal\": \"*\",\n              \"Action\": \"execute-api:Invoke\",\n              \"Resource\": \"arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*\/${APIGatewayStageName}\/*\/*\",\n              \"Condition\": {\n                \"NotIpAddress\": {\n                  \"aws:SourceIp\": \"${YourIPAddress}\"\n                }\n              }\n            },\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": \"*\",\n              \"Action\": \"execute-api:Invoke\",\n              \"Resource\": \"arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*\/${APIGatewayStageName}\/*\/*\"\n            }\n          ]\n        }\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"what is the model(transformer) size limitation in sagemaker serverless endpoint deployment?",
        "Question_created_time":1645726803899,
        "Question_last_edit_time":1668481535714,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsU6_idiTQIWu2Poovfpfdw\/what-is-the-model-transformer-size-limitation-in-sagemaker-serverless-endpoint-deployment",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":378,
        "Question_answer_count":1,
        "Question_body":"is there a limitation on the size of the model that we can create a model and then eventually serverless endpoint? any documentation? I did some research and ran into something similar here. https:\/\/discuss.huggingface.co\/t\/sagemaker-serverless-inference-for-layoutlmv2-model\/14186\/3\ni\nas a solution , it is advised to set MMS_DEFAULT_WORKERS_PER_MODEL=1. I'm not sure what exactly does this do? is there any aws documentation around this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to create a serverless endpoint in sagemaker?",
        "Question_created_time":1645674841041,
        "Question_last_edit_time":1668451728343,
        "Question_link":"https:\/\/repost.aws\/questions\/QULRy50Vd7SW6KT0MMzk4NeQ\/how-to-create-a-serverless-endpoint-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":519,
        "Question_answer_count":1,
        "Question_body":"I am recreating an endpoint currently working in sagemaker for inference to a serverless endpoint. I am using one of the images ( huggingface-pytorch-inference:1.9.1-transformers4.12.3-cpu-py38-ubuntu20.04)  found here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. \n\neverything works when i choose non serverless, i.e. provisioned option for endpoint configuration , but when i try to create one with serverless option it fails.  error messages are below ( from the logs in cloudwatch) . starting with python and log4j error at the end. \n\n'python: can't open file '\/user\/local\/bin\/deep_learning_container.py': [Errno 13] permission denied. \nRequirement already satisfied: transformers in \/opt\/conda\/lib\/pythong3.6 .....\n.....\nWarning: MMS is using non-default JVM parameters: -XX: -UseContainersupport\nFailed to reap children process\nlog4j: ERROR setfile(null,true) call failed. \njava.io.FileNotFoundException: logs\/mms_log.log (No Such file or directory) \n\nwhy am i getting this error ???\n\nFYI - i have set  memory to  maximum allowed memory size of 6gb. for the serverless option.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to serve a model in sagemaker?",
        "Question_created_time":1645625592189,
        "Question_last_edit_time":1668606703410,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmmN8fILJTqyJz8b0-MqoXw\/how-to-serve-a-model-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":446,
        "Question_answer_count":1,
        "Question_body":"based on documentation provided here , https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure, the model file saved from training is model.pth. I also read that it can be .pt extension or even bin extension. I have seen a example of pytorch_model.bin, but when i tried to serve the model with the pytorch_model.bin, it warns me that .pt or .pth file needs to exist. has anyone run into this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What is required in HumanLoopInput.InputContent for start_human_loop",
        "Question_created_time":1645624147204,
        "Question_last_edit_time":1667925986321,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJast4QKSTjCYWe-pcQFyHw\/what-is-required-in-humanloopinput-inputcontent-for-start-human-loop",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":110,
        "Question_answer_count":1,
        "Question_body":"I have built a custom key-value extraction workflow that leverages textract Tables and Forms. It does a whole heap of post processing using the output of Textract to extract a small number of highly important fields from documents that are of very poor quality.\n\nMy client would like a human-in-the-loop to make minor changes to the results where certain fields are missing. I think that the sagemaker_a2i_runtime.start_human_loop is the perfect tool for this.\n\nI want to send the human reviewer, the input image and the current Key-Value pairs that I have extracted and have them find any that are missing, or mark them as not there. I have setup and tested the textract.analyse_image workflow with a human reviewer and like the results.\n\nWhat structure and data fields do I need to set in the HumanLoopInput field of sagemaker_a2i_runtime.start_human_loop to get this to work. I assume that it will look something like a dictionary of current K-V pairs and the s3 image file but I cannot find any documentation on how to do this.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SM Elastic Inference Accelerators are not available during inference",
        "Question_created_time":1645620490604,
        "Question_last_edit_time":1667926446102,
        "Question_link":"https:\/\/repost.aws\/questions\/QUi78wfQkWTuaxYOjA8jGZaw\/sm-elastic-inference-accelerators-are-not-available-during-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":68,
        "Question_answer_count":0,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nWe are able to deploy on real-time endpoint with elastic inference accelerators. But SM Elastic Inference Accelerators are not available during inference, could you please have a look?\n\nNote:\n\n1. We are bringing our own trained model using Pytorch = 1.10.2 on SM.\n2. We don't find any errors in cloud watch logs.\n3. Converted our trained model to TorchScript and able to load that model as well during inference.\n4. Tried both Torchscript's trace and script modes but no luck.\n\n**Code**\n\nfrom sagemaker.pytorch import PyTorchModel\n\nfrom sagemaker import get_execution_role\n\nendpoint_name = 'ner-bert'\n\npytorch = PyTorchModel(entry_point='deploy_ei.py',\nsource_dir='code',\nmodel_data=model_data,\nrole=get_execution_role(),\nframework_version='1.3.1',\npy_version='py3',\nsagemaker_session=sagemaker_session)\n\npredictor = pytorch.deploy(initial_instance_count=1,\ninstance_type='ml.m5.large',\naccelerator_type='ml.eia2.xlarge',\nendpoint_name=endpoint_name)\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to grant permission to create a log file in sagemaker?",
        "Question_created_time":1645550175217,
        "Question_last_edit_time":1668461511402,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1Cfn6H1kQMeuWAhtGctNiA\/how-to-grant-permission-to-create-a-log-file-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":244,
        "Question_answer_count":1,
        "Question_body":"I created a serverless endpoint in sagemaker based on the documentation provided. I have my model  (artifactory) zipped up and uploaded to s3, created a model and then created an endpoint configuration , then finally created the endpoint \n```\nresponse = client.create_endpoint(\n    EndpointName=\"serverless-endpoint\",\n    EndpointConfigName=\"serverless-endpoint-config\"\n)\n```\non the endpoint invocation,  cloudwatch logs show error below - \nlog4j:ERROR setFile(null, true) called failed. \njava.io.FileNotFoundException: logs\/ts_metrics.log (No such file or directory) \nat java.base\/java.io.FileOutputStream.open0 (Native Method) \nat java.base\/java.io.File......\n\nthis looks like permission error , but i'm not sure how to give access\/permission to create file in sagemaker.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using SageMaker as a backend for a web app",
        "Question_created_time":1645453626658,
        "Question_last_edit_time":1668621505554,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA06r5EiFR0ifBd3KixHHLA\/using-sagemaker-as-a-backend-for-a-web-app",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":536,
        "Question_answer_count":1,
        "Question_body":"[Continued\/simplified from this post.](https:\/\/repost.aws\/questions\/QU-iZHXP55T_6BLKhHPET_0Q\/if-i-open-a-web-socket-connection-with-one-of-my-sage-makers-notebook-cells-how-long-will-the-connection-last)\n\nLet's say I have a web app UI and I want to connect it to a SageMaker notebook using the Churn Predictor model in SageMaker Sudio. Think of it like using that model hosted in SageMaker as a backend to my web app. \n\nI could create an API endpoint but my web app has a very simple solution for opening a web socket which I'd like to use. Question: **Whats the simplest\/best way to deploy the SageMaker notebook so that it keeps the kernel running and connection open?**\n\nThe websocket means I don't need to set up and configure individual API endpoints which is what all the documentation I can find suggests. \n\nThe desired workflow is:\n- Configure model in SageMaker notebook environment \n- Add a cell that establishes a websocket connection\n- Export notebook as a Python script (Maybe?)\n- Host & Run Python script on an AWS service\n\nWith the web socket I can simply establish the connection:\n\n`anvil.server.connect(app_key)`\n\nAnd then give each function that I want my web app to call a decorator. i.e:\n```\n@anvil.server.callable\ndef function(foo):\n  return foo\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unsupported pytorch version 1.10.0 with SM Elastic Inference Accelerators",
        "Question_created_time":1645246550155,
        "Question_last_edit_time":1667925603632,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-LyE6PRQSbq3mXEouDuGjQ\/unsupported-pytorch-version-1-10-0-with-sm-elastic-inference-accelerators",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":61,
        "Question_answer_count":0,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nWe are not able to deploy on real-time endpoint with elastic inference accelerators. Could you please have a look?\n\nSageMaker version: 2.76.0\n\nCode:\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\n\nendpoint_name = 'ner-bert'\n\nmodel = PyTorchModel(entry_point='deploy_ei.py',\nsource_dir='code',\nmodel_data=model_data,\nrole=get_execution_role(),\nframework_version='1.10.0',\npy_version='py38')\n\npredictor = model.deploy(initial_instance_count=1,\ninstance_type='ml.m5.xlarge',\naccelerator_type='ml.eia2.medium',\nendpoint_name=endpoint_name)\n\nError details: Unsupported pytorch version: 1.10.0. You may need to upgrade your SDK version (pip install -U sagemaker) for newer pytorch versions. Supported pytorch version(s): 1.3.1, 1.5.1, 1.3, 1.5.\n\nNote: We are able to deploy without elastic accelerator in above code and want to use Python 3.8 version because we have some dependency libraries which supports only Python 3.8 version.\n\nI looked at \"Available DL containers\" at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\nand by looking at this section \"SageMaker Framework Containers (SM support only)\", SM support PyTorch 1.10.0 with Python 3.8 version.\n\nBut we would like to deploy on Elastic Inference and by looking at this section \"Elastic Inference Containers\" in above URL, EI containers supports only PyTorch 1.5.1 with Python 3.6. Why these containers are so outdated?\n\nWhat could be the solution?\n\nCan we specify the latest version of Python in requirements.txt file and get it installed?\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can I use glue interactive sessions with pythonshell?",
        "Question_created_time":1645223131500,
        "Question_last_edit_time":1668500280765,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjwZtVPYTSCG96fKOGn1k4w\/can-i-use-glue-interactive-sessions-with-pythonshell",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":343,
        "Question_answer_count":1,
        "Question_body":"Can I use glue interactive sessions with `pythonshell`?\n\nIn the docs, it hints at choosing the `%job_type` or is it only available for `glueetl`. I am looking for a simple way to develop by requiring myself to use a sage maker notebook because it is overkill on half the ETL more of the time. Can I just get the 0.0625 DPU by default with a notebook? And no extras like I get with sagemaker.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Notebook stopped working. Cannot access and shows in notebook instances only intermittently.",
        "Question_created_time":1645192253179,
        "Question_last_edit_time":1668547440320,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfLJaDrf8R9KsDSp1QMZ91g\/notebook-stopped-working-cannot-access-and-shows-in-notebook-instances-only-intermittently",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":304,
        "Question_answer_count":1,
        "Question_body":"I was working on a Sagemaker notebook when suddenly it stopped working. I stopped and notebook and started it again, but now I cannot access it, getting the following error:\n\n*AccessDeniedException\nUser: arn:aws:sts::755460267215:assumed-role\/voclabs\/user1672114=9540446637 is not authorized to perform: sagemaker:ListNotebookInstances with an explicit deny in an identity-based policy*\n\nCan somebody please help? I should note that I am pretty new to the Sagemaker environment.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Invoke the SageMaker model endpoint directly from PostMan without Lambda and API gateway setup",
        "Question_created_time":1645172179513,
        "Question_last_edit_time":1668588217237,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2IGM0IeOSb-6elPnkRpd-Q\/invoke-the-sagemaker-model-endpoint-directly-from-postman-without-lambda-and-api-gateway-setup",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1789,
        "Question_answer_count":2,
        "Question_body":"Hi All,\n\nGood day!!\n\nHow to invoke the SageMaker model endpoint directly from Postman without Lambda and API gateway setup?\n\nWe have deployed the real-time endpoint on SM and data scientist (has AWS account) would like to invoke that endpoint via Postman from their local machine and test?\n\nMethod: Post\n\nURL: https:\/\/runtime.sagemaker.ap-southeast-1.amazonaws.com\/endpoints\/my-endpoint-name\/invocations\n\nWe know that we can invoke SM endpoint using SM boto3 SDK like below, but we would like to invoke the endpoint directly via Postman.\n\nimport boto3 smr = boto3.client('sagemaker-runtime')\n\nresp = smr.invoke_endpoint(EndpointName=endpoint_name, Body=b'.345,0.224414,.131102,0.042329,.279923,-0.110329,-0.099358,0.0', ContentType='text\/csv')\n\nThanks, Vinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to invoke the SageMaker model endpoint directly from PostMan without Lambda and API gateway setup",
        "Question_created_time":1645077384113,
        "Question_last_edit_time":1668630700510,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-Ef8noXsSt-hPgBubJK88A\/how-to-invoke-the-sagemaker-model-endpoint-directly-from-postman-without-lambda-and-api-gateway-setup",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1861,
        "Question_answer_count":2,
        "Question_body":"Hi All,\n\nGood day!!\n\nHow to invoke the SageMaker model endpoint directly from Postman without Lambda and API gateway setup?\n\nWe know that we can invoke SM endpoint using SM boto3 SDK like below, but we would like to invoke the endpoint directly via Postman.\n\nimport boto3\nsmr = boto3.client('sagemaker-runtime')\n\nresp = smr.invoke_endpoint(EndpointName=endpoint_name, Body=b'.345,0.224414,.131102,0.042329,.279923,-0.110329,-0.099358,0.0', \n                           ContentType='text\/csv')\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to define concurrency in SageMaker real-time inference",
        "Question_created_time":1645076515894,
        "Question_last_edit_time":1668575046931,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOW_MzWx4QWaQeIqVYMEyeQ\/how-to-define-concurrency-in-sagemaker-real-time-inference",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":613,
        "Question_answer_count":1,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nCould you please confirm, how can we define concurrency in SageMaker real-time inference?\n\nThis is how we define concurrency in SageMaker serverless inference.\n\n\u201cServerlessConfig\u201d: {\n\u201cMemorySizeInMB\u201d: 4096,\n\u201cMaxConcurrency\u201d: 10,\n}\n\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to create a serverless endpoint configuration?",
        "Question_created_time":1645067206226,
        "Question_last_edit_time":1668438755898,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":260,
        "Question_answer_count":2,
        "Question_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed \nunknown parameter in` ProductVariants [ 0 ]:` \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\n```\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1645090644602,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1645171546007,
        "Answer_comment_count":0.0,
        "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\n```\npip install --upgrade boto3\npip install --upgrade sagemaker\n```\n\n\nFor a sample notebook you can have a look [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/serverless-inference\/Serverless-Inference-Walkthrough.ipynb). More information on the documentation [page](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#sagemaker-serverless-inference).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to check\/determine image\/container size for aws managed images ?",
        "Question_created_time":1645022076554,
        "Question_last_edit_time":1668481512839,
        "Question_link":"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":254,
        "Question_answer_count":1,
        "Question_body":"I'm using one of the images listed here https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md, to create an model such that I can tie that up with a sagemaker serverless endpoint , but I keep getting \"failed reason: Image size 15136109518 is greater that suppported size 1073741824\" . this work when the endpoint configuration is not serverless. is there any documentation around image\/container size for aws managed images?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"If I open a web socket connection with one of my SageMaker's notebook cells how long will the connection last?",
        "Question_created_time":1644937024950,
        "Question_last_edit_time":1668623191107,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-iZHXP55T_6BLKhHPET_0Q\/if-i-open-a-web-socket-connection-with-one-of-my-sagemaker-s-notebook-cells-how-long-will-the-connection-last",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":365,
        "Question_answer_count":1,
        "Question_body":"If I open a web socket connection with one of my SageMaker's notebook cells how long will the connection last?\n\nI realise the time frame may not be exact and I cannot expect 100% uptime. It's more about whether there is a set cut off time i.e. every 24hrs the kernel shuts down or restarts.\n\nA secondary question:\nWhat is the best way to deploy the notebook as a web service that maintains the web socket connection indefinitely?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Question_created_time":1644862112974,
        "Question_last_edit_time":1668563778934,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deepar-in-aws-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":232,
        "Question_answer_count":1,
        "Question_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1644892187263,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1644892187264,
        "Answer_comment_count":0.0,
        "Answer_body":"(As detailed further on the [algorithm details page](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)), **yes**, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy [reference table here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/common-info-all-im-models.html) listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\n**However**, to be clear, it shouldn't be the *notebook* instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in *training jobs* - for example using the [SageMaker Python SDK](https:\/\/sagemaker.readthedocs.io\/en\/stable\/) `Estimator` class as shown in the sample notebooks for DeepAR [electricity](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb) and [synthetic](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_synthetic\/deepar_synthetic.ipynb). The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\n```python\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n```\n\nSo normally I wouldn't expect you to need to change your *notebook* instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because [DeepAR hyperparameters](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_hyperparameters.html) like `context_length`, `embedding_dimension`, and `mini_batch_size` will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU `g4dn.xlarge`, `g5.xlarge` or `p3.2xlarge` instances, perhaps starting with the lowest cost-per-hour? You can keep  an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing `mini_batch_size` should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like `learning_rate` to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like [this XGBoost notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/xgboost_random_log\/hpo_xgboost_random_log.ipynb) with the `HyperparameterTuner` class?\n\nIn that case would also mention:\n- Increasing the `max_parallel_jobs` parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\n- If you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Does the pre and post-processing need to be incorporate in SageMaker?",
        "Question_created_time":1644776794437,
        "Question_last_edit_time":1668630311577,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMhy15ZuZTNeZFajUWrFXbw\/does-the-pre-and-post-processing-need-to-be-incorporate-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":199,
        "Question_answer_count":3,
        "Question_body":"Hi All,\n\nGreetings!!\n\nPlease address my below queries.\n\n1. Does the pre and post-processing need to be incorporate in SM?\n2. Isn't SM supposed to be used for inference only?\n\nWhy I'm asking these questions because I am using PyTorch model server for pre-processing, predictions and post-processing for NER use cases.\n\n3. We use utokenize for creating tokens which supports >= Python 3.8 but PyTorch model server supports <= 3.6, what would be the solution?\nHow to install Python 3.8 in PyTorch model server?\n\n4. Don't we have basic word piece tokenization as pre-processing in SM?\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Question_created_time":1644763229468,
        "Question_last_edit_time":1668456865503,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sagemaker-canvas",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":139,
        "Question_answer_count":1,
        "Question_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1644863324414,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1644863324414,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Host a fine-tuned BERT Multilingual model on SageMaker with Serverless inference",
        "Question_created_time":1644677512763,
        "Question_last_edit_time":1668608518429,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIVk2l_3iQ1Ob4kKZq-Qw5A\/host-a-fine-tuned-bert-multilingual-model-on-sagemaker-with-serverless-inference",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":161,
        "Question_answer_count":1,
        "Question_body":"Hi All,\n\nGood day!!\n\nKey point to note here is, we have pre-processing script for the text document, deserialize which is required for prediction then we have post-processing script for generating NER (entitites).\n\nI went through SageMaker material and decided to try following options.\n\n1. Option 1: Bring our own model, write a inference script and deploy it on SM real-time endpoint using Pytorch container.\nI went through Suman video (https:\/\/www.youtube.com\/watch?v=D9Qo5OpG4p8) which is really good, need to try with our pre-processing and post-processing scripts then see if it works fine or not.\n\n2. Option 2: Bring our own model, write a inference script and deploy it on SM real-time endpoint using Huggingface container.\nI went through Huggingface docs (https:\/\/huggingface.co\/docs\/sagemaker\/inference#deploy-a-%F0%9F%A4%97-transformers-model-trained-in-sagemaker) but there is no reference for how to use own pre and post-processing scripts to setup inference pipeline.\n\nIf you know any examples on using our own pre and post-processing scripts using Huggingface container then please share it.\n\n3. Option 3: Bring our own model, write a inference script and deploy it on SM Serverless inference\/endpoint using Huggingface container.\nI went through Julien video (https:\/\/www.youtube.com\/watch?v=cUhDLoBH80o&list=PLJgojBtbsuc0E1JcQheqgHUUThahGXLJT&index=35) which is excellent but he has not shown how to use our own pre and post-processing scripts using Huggingface container.\n\nPlease share if you know any examples.\n\nCould you please help?\n\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Uncaught exception in ZMQStream callback -- trying to run Jupyter notebook with Julia kernel in SageMaker",
        "Question_created_time":1644454296572,
        "Question_last_edit_time":1668579771580,
        "Question_link":"https:\/\/repost.aws\/questions\/QUM2y8-rjDS1Wp5LUkdLMhyA\/uncaught-exception-in-zmqstream-callback-trying-to-run-jupyter-notebook-with-julia-kernel-in-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":915,
        "Question_answer_count":1,
        "Question_body":"I want to run a Jupyter notebook with Julia kernel in Amazon SageMaker. The Julia 1.7.1 icon shows up in the Jupyterlab launcher and accepts the kernel on launch, but then the kernel dies immediately after launch (it never works). I have posted about this here\n\nhttps:\/\/www.repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker\n\nand Alex_T has made some very good suggestions, but even they get stumped at this point. Log info:\n\n```\n[E 00:25:58.760 NotebookApp] Uncaught exception in ZMQStream callback\n    Traceback (most recent call last):\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 431, in _run_callback\n        callback(*args, **kwargs)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/notebook\/services\/kernels\/kernelmanager.py\", line 391, in record_activity\n        msg = session.deserialize(fed_msg_list)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/jupyter_client\/session.py\", line 929, in deserialize\n        raise ValueError(\"Invalid Signature: %r\" % signature)\n    ValueError: Invalid Signature: b'ec1d1093f3b6505658469b860c203f696bc39cf8fcea1672cb55802fc57592eef57b8db0b5cb603d1bcada6f41060f0819f6002a7a31f309fbaa1a701cc13f5b'\n```\n\nThere are some posts from 2018 recommending updating tornado and ipykernel, but 4 years later this should hardly apply (and I tried it, and it didn't work). Any suggestions? What could be going wrong here? Everything else seems to be in place. There is a white paper on running a Jupyter notebook with Julia kernel in SageMaker here [https:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card], but it is incomplete, as you can see in the other post.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Possible quota issue",
        "Question_created_time":1644304787619,
        "Question_last_edit_time":1667986875557,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnEGxZsbiQPe4oHO0CkRaRg\/possible-quota-issue",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":344,
        "Question_answer_count":1,
        "Question_body":"I opened up an AWS support case to increase my quota based on the error below, but AWS support states this is not a quota issue.  They were unwilling to help me in support but directed me to this forum.\n\nI'm getting the following error when running some AWS SageMaker batch transform jobs on m5.large instances.  I'm running 8 transforms in parallel against 8 different models in a step function map.  A few of the transforms succeed, but some fail with the following error:\n{\n  \"resourceType\": \"sagemaker\",\n  \"resource\": \"createTransformJob.sync\",\n  \"error\": \"SageMaker.AmazonSageMakerException\",\n  \"cause\": \"Rate exceeded (Service: AmazonSageMaker; Status Code: 400; Error Code: ThrottlingException; Request ID: 49a80dc1-df06-4b88-a462-24e517d13531; Proxy: null)\"\n}\n\nWhat is going on here?  Am I doing something wrong?  What kind of throttling exception is this (what is being throttled?)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Host multiple TensorFlow computer vision models using Amazon SageMaker multi-model endpoints",
        "Question_created_time":1644300080795,
        "Question_last_edit_time":1668627292271,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxAPkO75GTnSERkxpABFgSQ\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":248,
        "Question_answer_count":3,
        "Question_body":"Hi All,\n\nGreetings!!\n\nCould you please clarify on two questions below which are related to this post.\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\/\n\n1. As per AWS documentation (https:\/\/docs.aws.amazon.com...] \"Multi-model endpoints are not supported on GPU instance types\".\nI see we are using 'instance_type': 'ml.m5.2xlarge' to train both image classification and sign language digit classification models in this post.\n\nAs per instance configuration we don't have any GPU cores and GPU memory available in 'ml.m5.2xlarge' instance type.\nBut we all know that we need a GPU instance for deep learning model training, we are not getting how we are able to train both classification models using 'ml.m5.2xlarge' instance type.\n\nDoes this image - IMAGE_URI = '763104351884.dkr.ecr.us-eas... has GPU cores?\n\nml.m5.2xlarge - Instance details:\n\nCompute Type: Standard Instances\nV CPU: 8\nMemory: 32 GiB\nClock Speed: undefined\nGPU: 0\nNetwork Performance: Up to 10 Gigabit\nStorage: EBS only\nGPU Memory: 0\n\n2. As mentioned in this post, I believe we have one production variant of both CIFAR and sign-language models meaning we have v1 models.\nLet's assume that we have a new set of images for classification then we have decided to train a new variant CIFAR model and want to create that model.\n\nSo we have 2 versions of the CIFAR model and 1 version of the sign-language model.\n\nHow can we get the inference from two production variants of CIFAR models?\nHow can we do A\/B testing for CIFAR models?\n\nThanks in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Turn our endpoints off when not in use with Lambda",
        "Question_created_time":1644299069251,
        "Question_last_edit_time":1668611748858,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGU0QdOahS3SrmdOSaNOj7Q\/turn-our-endpoints-off-when-not-in-use-with-lambda",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":373,
        "Question_answer_count":4,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nIn this video (https:\/\/www.youtube.com\/watch?v=KFuc2KWrTHs&list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz&index=6&t=374s) Emily Webber mentioned that we can turn our endpoints (instances) off when not in use with Lambda.\n\nCould anyone please explain how can we do that using Lambda functions? \n\nI checked it in AWS console (Amazon SageMaker --> Endpoints), looks like we can only delete the endpoints but there is no option for turning off the endpoints.\n\n\nThanks,\nVinayak",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Deploy an model trained using Sagemaker's built-in k-NN algorithm for AWS Panorama",
        "Question_created_time":1644293159320,
        "Question_last_edit_time":1667896774511,
        "Question_link":"https:\/\/repost.aws\/questions\/QUx37woHPgTamp-7MjnpYqcA\/deploy-an-model-trained-using-sagemaker-s-built-in-k-nn-algorithm-for-aws-panorama",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":178,
        "Question_answer_count":1,
        "Question_body":"Hi, I'm trying to use a k-Nearest Neighbour model for deployment to an Edge device (AWS Panorama). I understand I need to create an optimized model to suit the target device. As I understand it the built-in algorithms all include a SageMaker Neo 'compile_model' function which I am running over my trained model - \n\nBuild model..\n```\nknn = sagemaker.estimator.Estimator(container,\n                                       role, \n                                       instance_count=1, \n                                       instance_type='ml.m5.2xlarge',\n                                       output_path=output_location,\n                                       sagemaker_session=sess,\n                                       input_mode='Pipe'\n                                   )\n# Setup the hyperparameters\nknn.set_hyperparameters(**hyperparams)\n\nknn.fit(fit_input, job_name=job_name)\n```\n\nBuild optimised model...\n```\noptimized_ic = knn.compile_model(\n    target_instance_family=\"ml_c5\",\n    target_platform_os=\"LINUX\",\n    target_platform_arch=\"ARM64\",\n    input_shape={\"data\": [1,3,512,512]},\n    output_path=s3_optimized_output_location,\n    framework=\"mxnet\",\n    framework_version=\"1.8\",\n)\n```\nWhile the initial model builds fine I get an error running the optimised model \/ compile_model function:\n\n> Failed. Reason: ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found. Please make sure the framework you select is correct.\n\nI can find a few google references for attempts at something similar with KMeans algorithm, but is k-NN algorithm dramatically different? Do I just have the wrong settings? I understand all of the built-in algorithms use were originally mxnet trained? When I extract my model I only have three files:\n- model_algo-1\n- model_algo-1.json\n- model_algo-1-labels.npy\n\nI am struggling to work out where I am going wrong in porting this model!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"make julia notebooks work in SageMaker",
        "Question_created_time":1644167031317,
        "Question_last_edit_time":1668601284657,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":238,
        "Question_answer_count":2,
        "Question_body":"I want to run a Jupyter notebook in SageMaker with a Julia kernel. There is very little documentation about this. There is this:\n\nhttps:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card\n\nI followed all the instructions, and Julia shows up in the JupyterLab launcher; but when I run it, Julia 1.17.1 shows up as the kernel and then dies. It appears to be trying, but then gives up and says \"No Kernel\" instead of \"Julia 1.17.1\" in the status line.\n\nIf I run the R kernel, all goes well. If I run the Julia kernel (which shows up in the list of available kernels!), I get the following error message:\n\nConnection failed\n\nA connection to the notebook server could not be established. \n\nThe notebook will continue trying to reconnect. \n\nCheck your network connection or notebook server configuration.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Project Run Time does not start on Sagemaker Studio Lab",
        "Question_created_time":1643885518208,
        "Question_last_edit_time":1667926243036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXiCETmDfRje5zL4WArbhRA\/project-run-time-does-not-start-on-sagemaker-studio-lab",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_body":"That is the case as of two days ago. This does not work for neither CPU nor GPU \"compute type\". \n\nBasically, after pressing the \"Start runtime\" button, it says \"Preparing project runtime...\" for about ten minutes and then stops. It shows the following error, \"There was a problem when starting the project runtime. This should be resolved shortly. Please try again later.\"\n\nI have now tried it about a dozen or more  times throughout the period.\n\nThere is no way to even access the work that is saved there. The \"project\" will not boot up.\n\nBasically it is a dud at this point.\n\nIs anyone else experiencing similar issues? What does one do? Is there a way to reset the environment to original and restart (e.g., a factory reboot)?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS SageMaker - Upload our own docker image on Amazon SageMaker",
        "Question_created_time":1643870744946,
        "Question_last_edit_time":1668117795211,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYIfUo3-0Qqyr4XunexHJXw\/aws-sagemaker-upload-our-own-docker-image-on-amazon-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":255,
        "Question_answer_count":1,
        "Question_body":"I am new to AWS SageMaker and i am using this technology for building and training the machine learning models. I have now developed a docker image which contains our custom code for tensorflow. I would like to upload this custom docker image to AWS SageMaker and make use of it.\n\nI have searched various links but could not find proper information on how to upload our own custom docker image.\n\nCan you please suggest me the process of uploading our own docker image to AWS SageMaker?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Processing environmental data with netCDF files",
        "Question_created_time":1643796556447,
        "Question_last_edit_time":1667925801984,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9sduyLViQxepK_RcYuhPwQ\/processing-environmental-data-with-netcdf-files",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":0,
        "Question_body":"I'm looking for some experience, reference architecture, best practices about the processing of environmental data stored in netCDF files.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS Sagemaker: UnexpectedStatusException: Compilation job Failed. Reason: ClientError: InputConfiguration: Please make sure input config is correct - Input 1 of node StatefulPartitionedCall was passed",
        "Question_created_time":1643795001851,
        "Question_last_edit_time":1668416797679,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvCjrBvqAQzGwEyZjXDwE7g\/aws-sagemaker-unexpectedstatusexception-compilation-job-failed-reason-clienterror-inputconfiguration-please-make-sure-input-config-is-correct-input-1-of-node-statefulpartitionedcall-was-passed",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":87,
        "Question_answer_count":1,
        "Question_body":"I am trying to convert a pre-trained (NASNETMobile) model into AWS Neo Optimized model. \nI am flowing [https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/] this link. Only difference is that I am using Tensorflow 2.6 and Python 3.7.10.  Other dependencies are \nboto                               2.49.0\nboto3                              1.20.30\nbotocore                           1.23.30\nkeras                              2.7.0\nKeras-Preprocessing                1.1.2\nnumpy                              1.19.5\nsafety                             1.10.3\nsagemaker                          2.74.0\nsagemaker-pyspark                  1.4.2\ntensorflow                         2.6.0\ntensorflow-cpu                     2.6.0\ntensorflow-estimator               2.6.0\ntensorflow-gpu                     2.6.0\ntensorflow-io-gcs-filesystem       0.23.1\ntensorflow-serving-api             2.6.0\n\nAs per link, I got stuck in step 7. Getting error->\n```\nfrom tensorflow.keras.applications import nasnet\nfrom keras.preprocessing import image\nimport h5py\nimport numpy as np\nfrom keras.models import load_model\n\n# Load h5 file\nloaded_model = load_model('myy_model.h5')\n\nmodel_version = '2'\nexport_dir = 'export\/Servo\/' + model_version\n\n# Save in pb format\nloaded_model.save(export_dir)\n\n# Save as tar file\nimport tarfile\nmodel_archive = 'model.tar.gz'\nwith tarfile.open(model_archive, mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n\nfrom sagemaker import get_execution_role\nfrom sagemaker import Session\n\nrole = get_execution_role()\nsess = Session()\nregion = sess.boto_region_name\nbucket = sess.default_bucket()\n\nmodel_data = sess.upload_data(path=model_archive, key_prefix='model')\n\nfrom sagemaker.tensorflow import TensorFlowModel\n\nsagemaker_model = TensorFlowModel(model_data=model_data, \n                      framework_version=tf_framework_version,\n                      role=role)\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n\n#Load  jpeg image from local and set target size to 224 x 224\nimg = image.load_img('eagle.jpg', target_size=(224, 224))\n\n#convert image to array\ninput_img = image.img_to_array(img)\ninput_img = np.expand_dims(input_img, axis=0)\ninput_img = nasnet.preprocess_input(input_img)\n\n\npredict_img = predictor.predict(input_img)\npredict_img\n\n\/\/ Till this point every thing is working\n\ninstance_family = 'ml_c5'\nframework = 'tensorflow'\ncompilation_job_name = 'keras-compile-16'\n# output path for compiled model artifact\ncompiled_model_path = 's3:\/\/{}\/{}\/output'.format(bucket, compilation_job_name)\n\ndata_shape = {'input_2':[1,224,224,3]}\n\n\/\/ Failing in below line\noptimized_estimator = sagemaker_model.compile(target_instance_family=instance_family,\n                                         input_shape=data_shape,\n                                         job_name=compilation_job_name,\n                                         role=role,\n                                         framework=framework,\n                                         framework_version=tf_framework_version,\n                                         output_path=compiled_model_path\n                                        )\n\n```\n\ndata_shape = {'input_2':[1,224,224,3]}\n\"input_2\" is right name. have checked in model plus [https:\/\/netron.app\/]()\n\nFull Error Logs:\n\nUnexpectedStatusException                 Traceback (most recent call last)\n<ipython-input-140-c16ae7ba24de> in <module>\n      5                                          framework=framework,\n      6                                          framework_version=tf_framework_version,\n----> 7                                          output_path=compiled_model_path\n      8                                         )\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/model.py in compile(self, target_instance_family, input_shape, output_path, role, tags, job_name, compile_max_run, framework, framework_version, target_platform_os, target_platform_arch, target_platform_accelerator, compiler_options)\n    659         )\n    660         self.sagemaker_session.compile_model(**config)\n--> 661         job_status = self.sagemaker_session.wait_for_compilation_job(job_name)\n    662         self.model_data = job_status[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n    663         if target_instance_family is not None:\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/session.py in wait_for_compilation_job(self, job, poll)\n   3224         \"\"\"\n   3225         desc = _wait_until(lambda: _compilation_job_status(self.sagemaker_client, job), poll)\n-> 3226         self._check_job_status(job, desc, \"CompilationJobStatus\")\n   3227         return desc\n   3228 \n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3341                 ),\n   3342                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-> 3343                 actual_status=status,\n   3344             )\n   3345 \n\nUnexpectedStatusException: Error for Compilation job keras-compile-14: Failed. Reason: ClientError: InputConfiguration: Please make sure input config is correct - Input 1 of node StatefulPartitionedCall was passed float from stem_conv1\/kernel:0 incompatible with expected resource.\n\nThanks in Advance",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multi-model, Multi-container and Variants - what are the possible combinations?",
        "Question_created_time":1643789695844,
        "Question_last_edit_time":1668619103303,
        "Question_link":"https:\/\/repost.aws\/questions\/QUw1jokF5cQLmWHpj0hBGgtg\/multi-model-multi-container-and-variants-what-are-the-possible-combinations",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":314,
        "Question_answer_count":1,
        "Question_body":"This question is mostly for educational purposes, but the current SageMaker documentation does not describe whether these things are allowed or not.\n\nLets suppose I have:\n*  a `XGBoost_model_1` (that needs a `XGBoost container`)\n* a `KMeans_model_1` and a `KMeans_model_2` (both require a `KMeans container`)\n\n**1.** Here's the first question - can I do the following:\n* create a `Model` with `InferenceExecutionConfig.Mode=Direct` and specify two cointainers (`XGBoost` and `KMeans` with `Mode: MultiModel`)\n\nThat would enable the client:\n* to call `invoke_endpoint(TargetContainer=\"XGBoost\")` to access the `XGBoost_model_1`\n* to call `invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\")` to access the `KMeans_model_1` \n* to call `invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\")` to access the `KMeans_model_2` \n\nI don't see a straight answer in the documentation whether combining Multi-Model containers with Multi-container endpoint is possible.\n\n**2.** The second question - how does the above idea work with `ProductionVariants`. Can I create something like this:\n* `Variant1` with `XGBoost` serving `XGBoost_model_1` having a weight of `0.5`\n* `Variant2` with a Multi-container having both `XGBoost` and `KMeans` (with a `MultiModel` setup) having a weight of `0.5`\n\nSo that the client could:\n* call `invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\")` to access the `KMeans_model_1`\n* call `invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\")` to access the `KMeans_model_2`\n* call `invoke_endpoint(TargetVariant=\"Variant1\")` to access the `XGBoost_model_1`\n* call `invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"XGBoost\")` to access the `XGBoost_model_1`\n\nIs that combination even possible? \n\nIf so, what happens when the client calls the `invoke_endpoint` without specifying the variant? For example:\n* would `invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\")` fail 50% of the time (if it hits the right variant then it works just fine, if it hits the wrong one it would most likely result with a 400\/500 error (\"incorrect payload\")?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Auto rollback with Guardrails if model accuracy is not good",
        "Question_created_time":1643721370893,
        "Question_last_edit_time":1668141681300,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgggpE9JTPuwbhCmJUKPFA\/auto-rollback-with-guardrails-if-model-accuracy-is-not-good",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":92,
        "Question_answer_count":1,
        "Question_body":"**While deploying model with guardrails for SageMaker Inference Endpoint::**\n\nWe can create CloudWatch alarms to monitor Endpoint performance for metrics like Invocation5XXErrors, ModelLatency and we can rollback the model to previous version if the threshold is matched. [Different metrics available][https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/monitoring-cloudwatch.html]\n\nPlease let me know is there any way to achieve any one of the following requirement:\n\n1. I want to the use metrics like \"accuracy\" returned by endpoint and rollback to previous version if the model performance is consistently not good in the real time.\n2. Use the metrics like \"accuracy\" returned by version2 and compare it with version1 or with the ground truth and rollback model if necessary",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[Help\/ideas wanted] Serverless Inference: Optimize cold start time",
        "Question_created_time":1643639465200,
        "Question_last_edit_time":1668599872704,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlakvrCXORXyNh7KZehiXKQ\/help-ideas-wanted-serverless-inference-optimize-cold-start-time",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":833,
        "Question_answer_count":2,
        "Question_body":"We are using Sagemaker Serverless Inference, where the endpoint is wrapped with a Lambda that has a 30sec timeout (this timeout is not adjustable). Our cold start time of the model is quite above that (around 43sec). We load a model using Huggingface transformers and have a FLASK API for serving the model.  The model size is around 1.75GB.\n\nAre there any guides on how to improve cold start and model loading time? Could we compile the weights differently beforehand for faster loading?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"[Feature Request] Serverless Inference with VPC Config",
        "Question_created_time":1643639169189,
        "Question_last_edit_time":1668567973686,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0JnCsfMHRrSUosWjOiOM9g\/feature-request-serverless-inference-with-vpc-config",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":374,
        "Question_answer_count":2,
        "Question_body":"I would like to use a Sagemaker Model with a custom VPC Configuration, which is currently not possible with Serverless Inference. Is this feature planned? More generally: Is there a roadmap somewhere for Serverless Inference?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio notebook instances restricted to 64 megabytes not allow to train Pytorch multiprocess",
        "Question_created_time":1643418800912,
        "Question_last_edit_time":1668235943772,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4A4gYrPSQWuh71w2shardQ\/sagemaker-studio-notebook-instances-restricted-to-64-megabytes-not-allow-to-train-pytorch-multiprocess",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":276,
        "Question_answer_count":1,
        "Question_body":"Sagemaker Studio notebook instances restricted to 64 megabytes not allow to train Pytorch multiprocess with the default dataloaders. **How can I add more capacity to \/dev\/shm or what kernel can I use to train with Pytorch multiprocess?**\n```\nuname -a\nLinux tensorflow-2-3-gpu--ml-g4dn-xlarge-33edf42bcb5531c041d8b56553ba 4.14.231-173.361.amzn2.x86_64 #1 SMP Mon Apr 26 20:57:08 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\ndf -h | grep -E 'shm|File'\nFilesystem Size Used Avail Use% Mounted on\nshm 64M 0 64M 0% \/dev\/shm\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Inference endpoint not responding when invoked by lambda",
        "Question_created_time":1643379618036,
        "Question_last_edit_time":1668523294718,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxDOb4CEnT1qEzTkgcOLRog\/inference-endpoint-not-responding-when-invoked-by-lambda",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":158,
        "Question_answer_count":1,
        "Question_body":"Hi fellow AWS users,\n\nI am working on an inference pipeline on AWS.\nSimply put, I have trained a PyTorch model and I deployed it (and created an inference endpoint) on Sagemaker from a notebook.\n\nOn the other hand, I have a lambda that will be triggered whenever there is a new audio that gets uploaded to my S3 bucket and pass the name of that audio to the endpoint. The endpoint downloads the audio, performs some pre-processing (super-quick) and returns predictions. The lambda then sends these predictions by email.\n\nAudios get uploaded on the S3 bucket on a non regular basis, like around 10 audios a day, at irregular intervals.\n\nThis morning, I tried manually uploading a test audio to the bucket to check if the pipeline was working.\nIt turns out that my endpoint is correctly invoked by my lambda but looking at the endpoint logs nothing happens (and I don't get any email). I tried a couple of times, without any more success. The lambda just ends up timing out after 300ms (what I set).\nHowever, invoking the endpoint from my sagemaker notebook worked perfectly fine on the first try and seemed to unblock the endpoint. After that, the endpoint was responsive to the lambda invokation.\nWas that because the endpoint was not \"cold\" anymore and it was a coincidence, I couldn't tell.\n\nMy questions are: \n- Are there any differences in endpoint invokations between the two scenarios (from the lambda or from the Sagemaker notebook)?\n- How can we see how much time after an invokation the endpoint will become \"cold\" again? Please correct me If I am wrong using the term cold here. I know it applies to lambdas as well. To what I understood, the endpoint is basically calling my inference script on a ECR container. \n- According to my use case (number of inferences a day, pre-proccesing lightness, ...), what would be the best option for my endpoint? (async, batch, ...)\n- My lambda seems to try invokation twice in total (invoke 1 - timeout 1 - invoke 2 - timeout 2). Can that be set differently? \n- Shall I increase the timeout of my lambda and let it try more times until the ECR is \"warm\"? or is there such a setting that can be modified on the endpoint side?\n\nThank you so much in advance for your support.\n\nCheers\n\nAntoine",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Bug in the Sagemaker Studio!!",
        "Question_created_time":1643281342627,
        "Question_last_edit_time":1668534311220,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcXnGbPcOSP-sOuXCZIdSRA\/bug-in-the-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":335,
        "Question_answer_count":1,
        "Question_body":"I'm trying to open the Projects or Model Registry in AWS Sagemaker Studio from the Components and registries icon in the left sidebar.\nBut when I choose either Projects or Model Registry from the dropdown menu it shows nothing except an error message:\nResponse not successful: Received status code 400\n\nThe region I'm working in is Frankfurt (eu-central-1).\n\nThey used to work fine, but it's been a few days that I see this error. Please someone help me with this, I need to approve some models as soon as possible.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_created_time":1643230196748,
        "Question_last_edit_time":1668030995402,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sagemaker-ml-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":244,
        "Question_answer_count":1,
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/r_examples).  And it doesn't look that R is fully supported currently by ML Pipelines.  Any examples and success stories are very welcome. \n\nThanks.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1643404063709,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1643404063709,
        "Answer_comment_count":0.0,
        "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3  using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker\nhttps:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"how to choose ml.g4dn.* instances in sagemaker processing jobs",
        "Question_created_time":1643215786791,
        "Question_last_edit_time":1668612186951,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXqikCZktSFywwXL14PWcYg\/how-to-choose-ml-g4dn-instances-in-sagemaker-processing-jobs",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":619,
        "Question_answer_count":1,
        "Question_body":"I have to perform some data manipulation for which the sagemaker \"processing job\" would fit perfectly. Such jobs would benefit from GPU and thus I was looking to use instances from the ml.g4dn family for cost efficiency. Unfortunately, I cant see them available in the dropdown when creating a processing job from the aws dashboard, only when creating training jobs. \nI previously requested the limit increase to the aws support, and i was told it was not necessary and up to 20 instances could be run in the chosen region.\n\nAm I missing anything? do i have to enable the instance family somewhere else?\n\nthanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Question_created_time":1643132865842,
        "Question_last_edit_time":1668578369590,
        "Question_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":376,
        "Question_answer_count":2,
        "Question_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1650994900956,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1650994900956,
        "Answer_comment_count":0.0,
        "Answer_body":"Thank you for your response. For those looking to do the same thing, according to AWS Support AWS SageMakers does NOT support GitLab yet and there is no ETA for that feature.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"How much GPU memory are available on the g4, g5, p3d, and p4d series instances?",
        "Question_created_time":1643029354176,
        "Question_last_edit_time":1668613197793,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvPdBv2rwTEiYKKHDPLUTWA\/how-much-gpu-memory-are-available-on-the-g4-g5-p3d-and-p4d-series-instances",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2195,
        "Question_answer_count":2,
        "Question_body":"Is there a link that shows how much GPU memory is available on the following GPU instances on AWS?\n\n1. g4-series instances (NVidia T4)\n2. g5-series instances (NVidia A10)\n3. p3d-series instances (NVidia V100)\n4. p4d-series instances (NVidia A100)\n\nUpdate: the information is available for the [p3d series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/) and [g5 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g5\/), though not for the [g4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g4\/) or the [p4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p4\/) instances. Is it possible to retrieve the information for the latter two instances anywhere (without having to launch the instances)?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1676386194669,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1676386194669,
        "Answer_comment_count":0.0,
        "Answer_body":"This link has a table that compares instances' GPU memory\nhttps:\/\/docs.amazonaws.cn\/en_us\/AmazonECS\/latest\/developerguide\/ecs-gpu.html",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Forecast - how to handle missing values in the dataset",
        "Question_created_time":1642832665715,
        "Question_last_edit_time":1667925884894,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEBKd6SqjSJuVY3xYuMgC5w\/forecast-how-to-handle-missing-values-in-the-dataset",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":52,
        "Question_answer_count":0,
        "Question_body":"I have a few questions regarding data preparation for Forecast.  \n\nI have a dataset with about 3,000 item_id's, the data is recorded on weekdays only (no row for weekends\/holidays), and the forecast horizon is 1 day.\nFor example:  \n[item_id | timestamp | target_value]  \nitem_A | 2022-01-19 (Wed) | 100  \nitem_A | 2022-01-20 (Thurs) | 101  \nitem_A | 2022-01-21 (Fri) | 99  \nitem_A | 2022-01-24 (Mon) | 98  \nitem_A | 2022-01-25 (Tues) | 102  \n\nQ1. Is it recommended that the weekends (1\/22, 1\/23) row is inserted to the dataset with NaN as the target_value?  \n\nQ2. If target_value for a timestamp is NaN, do the RTS attributes get ignored regardless of what value it is?  \n\nQ3. When the Forecast is training, does Forecast recognize that on Friday, the next value to predict is on Monday rather than on Saturday?  \n\nQ4. If an item_id has small time series data points (for instance, global start~end date ranges from 2018 to 2021, but a particular item_id only has data recorded for a couple months in 2020), should I front-fill & back-fill with NaN to match the global start~end range? (I only intend to use the data for training, not creating a forecast)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Use crowd-textract-analyze-document with start_human_loop",
        "Question_created_time":1642783549682,
        "Question_last_edit_time":1667926650217,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuEnsW0WnRI6HU8TvW7FEWA\/use-crowd-textract-analyze-document-with-start-human-loop",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":44,
        "Question_answer_count":0,
        "Question_body":"I'm trying to use the crowd-textract-analyze-document widget on a custom task with sagemaker.\n\nThe problem is call the human loop from \"start_human_loop\" function not from analyze_document.\n\nThe human loop tasks is not rendered, the error can be reproduced using \"render_ui_template\" function\n\n`response = sagemaker_client.render_ui_template(\n    UiTemplate={\"Content\": template_content},\n    Task={\"Input\": input_json},\n    RoleArn=role,\n)\n`\n\nI'm using the default template from the documentation.\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\nWith the minimal ranges to read the data from task.input, so I assume than the error is on the input data.\n\nThe error raised is\n\n> crowd-html-elements-without-ce-polyfill.js:689 Uncaught TypeError: Cannot read properties of undefined (reading 'text')\n\n\nThe entire json used is this one that includes the \"text\" attribute, so I don't have any idea of why means the error:\n\n```\n{\n  \"TaskObject\": \"s3:\/\/foo_bar\/foo_bar.pdf\",\n  \"Keys\": [\n    {\n      \"importantFormKey\": \"Foo bar\"\n    }\n  ],\n  \"Blocks\": [\n    {\n      \"blockType\": \"KEY_VALUE_SET\",\n      \"confidence\": 93.0,\n      \"geometry\": {\n        \"boundingBox\": {\n          \"width\": 0.09730120003223419,\n          \"height\": 0.009636270813643932,\n          \"left\": 0.5012893676757812,\n          \"top\": 0.3701384961605072\n        },\n        \"polygon\": [\n          {\n            \"x\": 0.5012893676757812,\n            \"y\": 0.3701384961605072\n          },\n          {\n            \"x\": 0.5985905528068542,\n            \"y\": 0.3701384961605072\n          },\n          {\n            \"x\": 0.5985905528068542,\n            \"y\": 0.3797747790813446\n          },\n          {\n            \"x\": 0.5012893676757812,\n            \"y\": 0.3797747790813446\n          }\n        ]\n      },\n      \"id\": \"6231be31-5f56-41db-95cf-2dad8f765cca\",\n      \"relationships\": [\n        {\n          \"type\": \"VALUE\",\n          \"ids\": [\n            \"b7697bbd-c5f5-4d28-a345-92c1f53daef7\"\n          ]\n        },\n        {\n          \"type\": \"CHILD\",\n          \"ids\": [\n            \"57c87916-0636-4d51-8d3b-8d15f4e93d73\",\n            \"f38b7a0f-2750-46e0-960f-8d6b58dcab3d\"\n          ]\n        }\n      ],\n      \"entityTypes\": [\n        \"KEY\"\n      ],\n      \"text\": \"Foo bar\",\n      \"page\": 1\n    },\n    {\n      \"blockType\": \"KEY_VALUE_SET\",\n      \"confidence\": 93.0,\n      \"geometry\": {\n        \"boundingBox\": {\n          \"width\": 0.2203546017408371,\n          \"height\": 0.019548991695046425,\n          \"left\": 0.6025875210762024,\n          \"top\": 0.3636907935142517\n        },\n        \"polygon\": [\n          {\n            \"x\": 0.6025875210762024,\n            \"y\": 0.3636907935142517\n          },\n          {\n            \"x\": 0.8229421377182007,\n            \"y\": 0.3636907935142517\n          },\n          {\n            \"x\": 0.8229421377182007,\n            \"y\": 0.3832397758960724\n          },\n          {\n            \"x\": 0.6025875210762024,\n            \"y\": 0.3832397758960724\n          }\n        ]\n      },\n      \"id\": \"b7697bbd-c5f5-4d28-a345-92c1f53daef7\",\n      \"relationships\": [\n        {\n          \"type\": \"CHILD\",\n          \"ids\": [\n            \"d803e2ba-c238-4af1-8745-0dbb17e74420\"\n          ]\n        }\n      ],\n      \"entityTypes\": [\n        \"VALUE\"\n      ],\n      \"text\": \"Foo bar\",\n      \"page\": 1\n    }\n  ]\n}\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Pros and cons of restricting user access to certain regions",
        "Question_created_time":1642700804560,
        "Question_last_edit_time":1667921325307,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":100,
        "Question_answer_count":1,
        "Question_body":"Hello,\nAre there any drawbacks I should be aware of if we restrict user access to only a single region? \n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different.  It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1642705784219,
        "Answer_score_count":4.0,
        "Answer_last_edit_time":1642705784219,
        "Answer_comment_count":1.0,
        "Answer_body":"I would take a look at [this](https:\/\/docs.aws.amazon.com\/organizations\/latest\/userguide\/orgs_manage_policies_scps_examples_general.html#example-scp-deny-region) for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the [regional endpoints](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_temp_enable-regions.html) if you aren't already.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Serverless Inference - Limit number of workers",
        "Question_created_time":1642602434394,
        "Question_last_edit_time":1668481531967,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWYP78UdYQseoErcj4kjiug\/serverless-inference-limit-number-of-workers",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":525,
        "Question_answer_count":2,
        "Question_body":"We've deployed a HuggingFace model to Sagemaker as a serverless endpoint. We set memory to be 6GB and max concurrency to be 1. With these settings, we keep getting errors when we call `invoke_endpoint`. Not all the time, but about 60% of the time...\n\nWhen we check the logs and metrics, we see that the memory has gone up to almost 100%. We also see that, since the machine has 6 CPUs, if starts 6 workers. We believe this could be the cause of the problem. How can se set the number of workers?\n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"sagemaker online and offline store question",
        "Question_created_time":1642578480105,
        "Question_last_edit_time":1668568862240,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxxGzLRsPQk6iAlG-yEv4VQ\/sagemaker-online-and-offline-store-question",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":519,
        "Question_answer_count":1,
        "Question_body":"I looked at SM feature store documentation and see a flag named is_online_Enable for ingestion.\nMy queries are - \n1. If i make it false, will it only store the data in offline store ?\n2. If I make it true will it store data both in online and offline store? if yes , both the store will have same data right?\n3. Is there any way to store my data only in online or only in offline store ? if yes, what will be the configuration?\n\nAlso is there any scenario while ingesting data there is some syn happens between online and offline store or vice versa. I see documentation mentioning  it will be available after some minutes and for our use case we want to know is there any sync happens( is there any scenario where data goes to online store first and then sync up with offline store or vice versa and some latency associated with the later store)?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"I am not able to find this solution in sagemaker jumpstart",
        "Question_created_time":1642562655390,
        "Question_last_edit_time":1668534561334,
        "Question_link":"https:\/\/repost.aws\/questions\/QULbPt5pRtT1q8sV-CK5wPhA\/i-am-not-able-to-find-this-solution-in-sagemaker-jumpstart",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":212,
        "Question_answer_count":1,
        "Question_body":"Link for the post:\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/build-custom-amazon-sagemaker-pytorch-models-for-real-time-handwriting-text-recognition\/\nI am unable to find this in sagemaker jumpstart. Please guide me through this.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker error: \"unexpected EOF\"",
        "Question_created_time":1642551770380,
        "Question_last_edit_time":1667926022833,
        "Question_link":"https:\/\/repost.aws\/questions\/QULdleKKPDQpO1CPT_VHpl-A\/sagemaker-error-unexpected-eof",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":6,
        "Question_view_count":70,
        "Question_answer_count":0,
        "Question_body":"We are trying to run a SageMaker batch transform job and we're getting some errors:\n\n> 2022-01-18T23:29:00.980:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\n2022-01-18T23:34:21.071:[sagemaker logs]: <<<path to csv file in s3>>>: Unable to get response from algorithm: unexpected EOF\n\nWe do not understand what \"Unable to get response from algorithm: unexpected EOF\" means. How can we get more details about this error? \n\nIt would help if we could get the full request and full response from the endpoint. Is this information recorded somewhere in SageMaker?\n\nWe have added extra logging in our docker image and we are not able to find an issue on that end. We also tried to log the request and response but those got truncated in CloudWatch.\n\nWe would be grateful for any pointers that you can provide. Thanks.\n\nSebastien",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"java.lang.IllegalArgumentException in SageMaker",
        "Question_created_time":1642538521034,
        "Question_last_edit_time":1667926243913,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXl66qTr3TBWJjO5td_K0jw\/java-lang-illegalargumentexception-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":54,
        "Question_answer_count":0,
        "Question_body":"I'm unable to invoke the my SageMaker endpoint. I'm seeing this error in the endpoint logs\n\n```\njava.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \\r\\n: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.11.2.\n```\n\n\n```\n\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git\n```\n\n\n\nMy Sagemaker endpoint is invoked through a lambda function. The code that calls the sagemaker endpoint is:\n\n```\nSM_ENDPOINT_NAME = \"pytorch-inference-2021-xx-xx\"\nsm_runtime= boto3.client('runtime.sagemaker')\ntxt = \"Canon SELPHY CP1300 Compact Photo Printer\"\nresponse = sm_runtime.invoke_endpoint(EndpointName=SM_ENDPOINT_NAME, ContentType='text\/plain', Body=txt)\n```\nThe response is supposed to contain a vector.\n\nIt's been working fine previously but I started seeing this exception today.\n\n\nIs this a bug in SageMaker? If not, how do I fix it?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How do I check my current SageMaker service quotas?",
        "Question_created_time":1642480202619,
        "Question_last_edit_time":1668602220822,
        "Question_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sagemaker-service-quotas",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1212,
        "Question_answer_count":2,
        "Question_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas \n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a [request for a service quota increase](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#service-limit-increase-request-procedure).",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1655317929862,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1655365526940,
        "Answer_comment_count":0.0,
        "Answer_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Use instance (NVMe) storage in SageMaker Studio notebooks",
        "Question_created_time":1642415387301,
        "Question_last_edit_time":1667926405426,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfsg0pVUAQge5kFrXBC6sZQ\/use-instance-nvme-storage-in-sagemaker-studio-notebooks",
        "Question_score_count":1,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":68,
        "Question_answer_count":0,
        "Question_body":"I'm exploring and pre-processing some raw data in SageMaker Studio which is split across many (20k+++) small files, and running into slow speed because SMStudio's main user storage is backed by EFS rather than an EBS volume as used on SageMaker Notebook Instances (NBIs). Navigating and manipulating this dataset is slower in Studio because of the extra metadata introduced by the communication being at filesystem level, rather than just a block storage device.\n\nI know there's a little ephemeral block storage available to notebooks under `\/tmp` which can help with these issues (as [used here](https:\/\/github.com\/aws-samples\/sagemaker-101-workshop\/blob\/e5bbabd866375bc1589908d6f83e16b65deb8abe\/migration_challenge_keras_image\/Local%20Notebook.ipynb), in fact), but thought it would be more scalable to make use of the proper NVMe instance storage available with `ml.m5d.*` instances in Studio to work with bigger datasets.\n\nOnly trouble is, I'm not sure how to use these instance storage volume(s) from notebooks? When I run `!df -aTh`, the NVMe device only seems to be mounted on some very specific points as shown below:\n\n```\nFilesystem        Type      Size  Used Avail Use% Mounted on\n[...]\n127.0.0.1:\/200015 nfs4      8.0E   57G  8.0E   1% \/root\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/opt\/.sagemakerinternal\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/etc\/resolv.conf\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/etc\/hostname\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/etc\/hosts\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/var\/log\/studio\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/var\/log\/apps\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/opt\/ml\/metadata\/resource-metadata.json\n[...]\n```\n\nShould I be creating a new mount somehow to access the storage? Any particular best-practices to follow?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker AutoML generates ExpiredTokenException",
        "Question_created_time":1642293400372,
        "Question_last_edit_time":1667926095123,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPU-nfYYIRbmM-tJpkG6XqA\/sagemaker-automl-generates-expiredtokenexception",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":39,
        "Question_answer_count":0,
        "Question_body":"Hi,\n\nI can train models using different AWS SageMaker estimators, but when I use SageMaker AutoML Python SDK the following error occurs about 15 minutes into the model training process:\n\n\"botocore.exceptions.ClientError: An error occurred (ExpiredTokenException) when calling the DescribeAutoMLJob operation: The security token included in the request is expired\"\n\nThe role used to create the AutoML object is associated with the following AWS pre-defined policies as well as one inline policy. Can you please let me know what I\u2019m missing that's causing this ExpiredTokenException error?\n\nAmazonS3FullAccess\nAWSCloud9Administrator\nAWSCloud9User\nAmazonSageMakerFullAccess\n\nInline policy:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:DescribeEndpointConfig\",\n                \"sagemaker:DescribeModel\",\n                \"sagemaker:InvokeEndpoint\",\n                \"sagemaker:ListTags\",\n                \"sagemaker:DescribeEndpoint\",\n                \"sagemaker:CreateModel\",\n                \"sagemaker:CreateEndpointConfig\",\n                \"sagemaker:CreateEndpoint\",\n                \"sagemaker:DeleteModel\",\n                \"sagemaker:DeleteEndpointConfig\",\n                \"sagemaker:DeleteEndpoint\",\n                \"cloudwatch:PutMetricData\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\",\n                \"logs:CreateLogGroup\",\n                \"logs:DescribeLogStreams\",\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:ListBucket\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\nThanks,\nStefan",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to specify instance type when training models using SageMaker AutoML Python SDK",
        "Question_created_time":1642292454744,
        "Question_last_edit_time":1667925856310,
        "Question_link":"https:\/\/repost.aws\/questions\/QUClX3PJmFSX-aIaTBlaGidw\/how-to-specify-instance-type-when-training-models-using-sagemaker-automl-python-sdk",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":151,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nIs there a way to specify the instance type when training models using the SageMaker AutoML Python SDK? The AutoML.deploy method takes an instance_type argument, but the AutoML.fit method does not take an instance_type argument.\n\nThanks,\nStefan",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1642381156641,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1642381156642,
        "Answer_comment_count":1.0,
        "Answer_body":"As I understand it's not currently possible to customize this instance type selection - the underlying [CreateAutoMLJob API](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateAutoMLJob.html) doesn't offer any instance type controls so there's no way for the Python SDK [AutoML class](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/automl.html) to expose them.\n\nAlthough it might be nice to have some more user control over this in future, it's worth mentioning that the options might be non-trivial: For example different stages of the autoML process encapsulated within `fit()` (like pre-processing, training, explainability analysis) might have different infrastructure needs, and different tested algorithms running in parallel might also have different optimal choices for a given dataset.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Deploying large scale ML model",
        "Question_created_time":1642203662068,
        "Question_last_edit_time":1667926736539,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzS24kMebSh2QCg4bXDvX6Q\/deploying-large-scale-ml-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":2,
        "Question_view_count":61,
        "Question_answer_count":0,
        "Question_body":"Hi, I am deploying an ML model with a retrieval component from AWS and it had two parts:\n\n1. ML model that is deployed using Sagemaker. The model isn't big, so this is simple.\n2. Retrieval: The ML model first retrieves information from a database using ANN algorithm(like Annoy or Scann). The database needs to be loaded into memory at all times for really fast inference. However, the database is big(around 500GB). What is the best way to deploy this database? Is Sagemaker the best bet?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Importing externally-trained TensorFlow v2 models to SageMaker deployment",
        "Question_created_time":1642131421895,
        "Question_last_edit_time":1667926520084,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmbEwNJTBTkSPKQvzc6R7eg\/importing-externally-trained-tensorflow-v2-models-to-sagemaker-deployment",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":64,
        "Question_answer_count":1,
        "Question_body":"Can anybody suggest nice, reasonably up-to-date example(s) for importing a previously trained TFv2 model to SageMaker? i.e. tarballing the artifact to S3, and configuring the Model & Endpoint (preferably via SageMaker Python SDK).\n\nMost of the examples I've come across so far are for TFv1. Thanks!",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Data Wrangler UI Features",
        "Question_created_time":1641943985816,
        "Question_last_edit_time":1667957958274,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sagemaker-data-wrangler-ui-features",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":126,
        "Question_answer_count":3,
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does.  When will the UI support:\n* Loading all s3 objects under a prefix?  https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\n* Loading JSON objects in addition to CSV and Parquet files?  https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1642051347466,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1642051347467,
        "Answer_comment_count":0.0,
        "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to load large amount of data from S3 onto Sagemaker?",
        "Question_created_time":1641932571368,
        "Question_last_edit_time":1668607638321,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4m2DyyJQSSCL1QqclXS6ZA\/how-to-load-large-amount-of-data-from-s3-onto-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2118,
        "Question_answer_count":3,
        "Question_body":"I have a notebook on Sagemaker Studio, I want to read data from S3, I am using the code bellow:\n\ns3_client = boto3.client('s3')\nbucket = 'bucket_name'\ndata_key = 'file_key.csv'\nobj = s3_client.get_object(Bucket=bucket, Key=data_key)\ndf = pd.read_csv(io.BytesIO(obj['Body'].read()))\ndf.head()\n\nIt works for small datasets but fails along the way with the dataset I'm trying to load which is 15GB. \nI changed the instance to ml.g4dn.xlarge ( accelerated computing, 4vCPU + 16GiB + 1 GPU), still fails. what am I missing here? Is is about the instance type, or about the code? What is the best way to import large datasets from S3 to sagemaker? \n\nThank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Train machine learning model using reserved instance",
        "Question_created_time":1641871148701,
        "Question_last_edit_time":1668610348310,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":354,
        "Question_answer_count":1,
        "Question_body":"Hi. \n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks,\nStefan",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1642148033228,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1642148033228,
        "Answer_comment_count":1.0,
        "Answer_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\n[Using the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation](https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode)\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample [Blazingtext-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html#blazingtext-instances), [Deepar-instances](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. [Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker](https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/)",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":1.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Canvas failed to import the Redshift Data",
        "Question_created_time":1641655668805,
        "Question_last_edit_time":1668612561491,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCUdYWY0gSV6W60g7ArukXw\/sagemaker-canvas-failed-to-import-the-redshift-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":233,
        "Question_answer_count":1,
        "Question_body":"Actions:\n\n1. The Redshift connection has been setup on SageMaker Canvas.\n2. The Redshift already load the sample data (users, sales, etc)\n3. Drag and drop table 'users' to import pane. \n4. Check the import preview can show the data of 'users'\n5. Click Import\n\nExpected result:\n\nThe dataset can be imported successfully\n\nActual result:\nImport failed with below details:\n\n{'message': \"Variable '$input' got invalid value None at 'input.uri'; Expected non-nullable type 'String!' not to be None.\", 'locations': [{'line': 1, 'column': 8}], 'path': None}\n\nPlease contact your admin. Request ID: 8b849887-b067-46fc-9be9-dd122e9c8874",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Canvas connect Redshift failed",
        "Question_created_time":1641573857407,
        "Question_last_edit_time":1668605964856,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvjatAJaQv-Ist96WT1IIw\/sagemaker-canvas-connect-redshift-failed",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":298,
        "Question_answer_count":1,
        "Question_body":"Try to add the Redshift connection on SageMaker Canvas to import the data\n\n- The cluster identify: redshift-cluster-1\n- database name: dev\n- database user: awsuser\n- unload IAM Role: my-reshift-role\n- connection name: redshift\n- type: IAM\n\nmy-reshift-role trust-relationship is trust the \"redshift.amazonaws.com\" and \"sagemaker.amazonaws.com\"\n\nExpectation: create connection successfully\n\nActually result: \nRedshiftCreateConnectionError\nUnable to validate connection. An error occurred when trying to list schema from Redshift",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1641634221918,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1641634221918,
        "Answer_comment_count":0.0,
        "Answer_body":"The sagemaker canvas using sagemaker domain user, so need add the Redshift permission to the IAM Role attached to domain user. After add the permission, the connection can be setup",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"How to set model custom metadata in Sagemaker ML pipeline",
        "Question_created_time":1641356351060,
        "Question_last_edit_time":1668590647005,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFtq-bSeiRDisaQaEzHr7sQ\/how-to-set-model-custom-metadata-in-sagemaker-ml-pipeline",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":975,
        "Question_answer_count":3,
        "Question_body":"Hi there, \n\nI am interested in using model custom metadata. Looks like it got released recently.  \nhttps:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/sagemaker-model-registry-endpoint-visibility-custom-metadata-model-metrics\/\n\nMetadata can be set and read successfully via cli \naws sagemaker describe-model-package --model-package-name \"arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/MODEL_PACKAGE_NAME\/1\"\n\naws --profile dev sagemaker describe-model-package --model-package-name \"arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/MODEL_PACKAGE_NAME\/1\" | jq .CustomerMetadataProperties\n{\n  \"KeyName1\": \"string2\",\n  \"KeyName2\": \"string2\"\n}\n\nHowever, it is not clear how custom metadata can be set in Sagemaker ML pipeline when model is train and registered using [RegisterModel](https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html?highlight=RegisterModel#sagemaker.workflow.step_collections.RegisterModel)\n\nThanks in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1645648805226,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1645648805226,
        "Answer_comment_count":1.0,
        "Answer_body":"Have you tried using [this](https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/26b984a2d693f95957e8555f19fa47aa36d5d5ea\/src\/sagemaker\/workflow\/step_collections.py#L78) parameter on the RegisterModel step?",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"XGBOOST inference prediction error with type",
        "Question_created_time":1640805146200,
        "Question_last_edit_time":1668519524230,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-Q8JXVUvSZaEdFwWPHmKJg\/xgboost-inference-prediction-error-with-type",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":382,
        "Question_answer_count":2,
        "Question_body":"I am trying to make a prediction in Sagemaker:\n\n#array is a nympy array variable\n\nxgb_predictor.predict(array)\n\nand I am getting this error:\n\nParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: [[0.71028037 0.7866242  0.16398714 0.88787879 0.         0.\n 0.         1.         0.         0.         0.         0.\n 0.         1.         0.         1.         0.         0.\n 1.         0.         0.         0.         0.         0.\n 0.         0.         1.         0.         0.         0.\n 0.         0.         0.         0.         1.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         1.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ]], type: <class 'numpy.ndarray'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\n\nPlease your help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Amazon SageMaker Local Mode raised boto3.exceptions.RetriesExceededError: Max Retries Exceeded",
        "Question_created_time":1640615850316,
        "Question_last_edit_time":1668151057693,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVLhj-0JzTUCLcs7yH5Kznw\/amazon-sagemaker-local-mode-raised-boto3-exceptions-retriesexceedederror-max-retries-exceeded",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_body":"I try to run the SageMaker local mode example without any modification at https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/tree\/main\/pytorch_nlp_script_mode_local_model_inference on my local machine.\n\nHowever I encountered the **boto3.exceptions.RetriesExceededError: Max Retries Exceeded\n** exception when the example tries to deploy the inference endpoint to 'local' instance type.\n\nI checked with \n```\ndocker images -a \n```\ncommand and it does not pull the expected pre-built SageMaker deep learning container image from ECR. The code example is using a dummy role for the local SageMaker session. I need help as I am blocked at this point as the exception error message is not helpful to pinpoint the actual root cause of this issue. Thanks in advance.\n\nBelow are my configurations:\n\n* Ubuntu: **20.04.3 LTS**\n* AWS CLI version: **2.4.7**\n* Python: **3.8.12**\n* Docker: **20.10.12**\n* Docker Compose: **1.29.2**\n* boto3: **1.20.26**\n* sagemaker: **2.72.1**",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"GroundTruth text labelling - hide data columns, and methods of quality control",
        "Question_created_time":1640605742768,
        "Question_last_edit_time":1667925645107,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1PUIO8wSSnqbr_9ZF_oQNQ\/groundtruth-text-labelling-hide-data-columns-and-methods-of-quality-control",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":1,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"I have a csv of sentences which I'd like labelled, and have identified GroundTruth labelling jobs as a way to do this. Having spent some time exploring the service, I have some questions:\n\n**1) **I can't find a way to display only particular columns to the labellers - e.g. if the dataset has a column of IDs for each sentence, this ideally shouldn't be shown to labellers \n\n**2)** There is either single labelling or multi labelling, but I would like a way to have two sets of single-selection labels, where one captures difficulty of assigning the label:\n\nSelect one for binary classification\na) Yes, b) No\n\nSelect one for difficulty of classification\nc) Easy, d) Medium, e) Hard\n\nCan this be done using custom HTML? Is there a guide to writing this - the template it gives you doesn't seem to render as-is.\n\n**3)** There appears to be a maximum of $1.20 payment per task. Is this the case, and why?\n\n**4)** Having not used mechanical turk before, are there ways of ensuring people take the work seriously and don't just select random answers? I can see there's an option to have x number of people answer the same question, but is there also a way to put in unambiguous questions to which we already have a 'pre_agreed_label' every nth question, and remove people from the task if they get them wrong? \n\nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker Studio notebook - no module named 'tensorflow' when chosen image type \"Tensorflow 2.6 Python 3.8 GPU optimized\"",
        "Question_created_time":1640585801507,
        "Question_last_edit_time":1668460271273,
        "Question_link":"https:\/\/repost.aws\/questions\/QUN63fjMWsT5uVUNn2AIsmhw\/sagemaker-studio-notebook-no-module-named-tensorflow-when-chosen-image-type-tensorflow-2-6-python-3-8-gpu-optimized",
        "Question_score_count":0,
        "Question_favorite_count":2,
        "Question_comment_count":0,
        "Question_view_count":358,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI have created a simple notebook on SageMaker Studio using the Image \"Tensorflow 2.6 Python 3.8 GPU optimized\". But when I try to run simple statement viz. \"import tensorflow\", I am getting the error \"no module named 'tensorflow'\".\n\nI tried to install 'tensorflow' package using pip from the terminal attached to the image. But it shows the message \"requirement already satisfied\".\n\nAm I missing anything here? Please help.\n\nThanks in advance, \nPraveen",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1640721332675,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1640721332675,
        "Answer_comment_count":2.0,
        "Answer_body":"From the question, I understand that you are trying to use a TensorFlow 2.6 Python 3.8 kernel in SageMaker Studio, but you are unable to import tensorflow.\n\nThe service team are aware of this issue and are actively working on a fix.\n\nMitigation Option\n\nA)  If your use case is version flexible, version other than 2.6 should work.\n \nB) If not, you can try the following as workaround \n\n1. Open a notebook using a Tensorflow 2.6 Python 3.8 kernel\n2. Execute the following line in a notebook cell:\n!sed -i 's|^ *\"python\",|  \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json\n3. Stop the kernel\n4. Re-attach the kernel to your notebook.\n\nHope it helps!",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":1.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_created_time":1640092026826,
        "Question_last_edit_time":1668549723952,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1149,
        "Question_answer_count":1,
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n* Choose the user.\n* On the User Details page, for each non-failed app in the Apps list, choose Delete app.\n* On the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\n* When the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1640094687751,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1640094687751,
        "Answer_comment_count":1.0,
        "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Unable to use the same HumanWorkflow within textract for more than 1 file\/call",
        "Question_created_time":1639923120530,
        "Question_last_edit_time":1667925871399,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDgM77ZgnTbWjW57_v9rCGw\/unable-to-use-the-same-humanworkflow-within-textract-for-more-than-1-file-call",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":113,
        "Question_answer_count":2,
        "Question_body":"I created the private team from the Amazon SageMaker console for labeling tasks followed by the creation of the human review workflow, which I later integrated with the Amazon Textract for Key-Value pair extraction.\n\nWhile I called the analyze_document (along with HumanLoop configuration) to extract key-value pairs for the first time it worked as expected and I was able to see the Job in the labeling project console. However, when I called it again (irrespective of the same or different file) the HumanLoop started giving the below error.\n\n\"[ERROR] InvalidParameterException: An error occurred (InvalidParameterException) when calling the AnalyzeDocument operation: HumanLoop 'textractworkflow1' already exists and it is associated with a different InputContent. Please use a new HumanLoopName and try your request again.\"\n\nDo we have to create a new Human Review Loop each time we trigger analyze_document with another file?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1640758908043,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1640758908043,
        "Answer_comment_count":1.0,
        "Answer_body":"I think you are using the same human loop name for multiple tasks and that is causing this issue. You have to make sure that within the HumanLoopConfig configuration, the HumanLoopName should be unique for each task. You can also refer to this [video](https:\/\/youtu.be\/JNuWFwhfO7E)",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Giving weights to event types in amazon personalize",
        "Question_created_time":1639825094332,
        "Question_last_edit_time":1668616469666,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSogRKFlfRzC5b8afIwPybQ\/giving-weights-to-event-types-in-amazon-personalize",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":331,
        "Question_answer_count":1,
        "Question_body":"1) For the VIDEO_ON_DEMAND domain, some use cases include multiple event types. For example, the 'Top picks for you' use case includes two event types  'watch' and 'click'. Is 'watch' given more weight than 'click' when training the model? In general, when there is more than one event type, do domain recommenders give more weight to some event types?\n\n2) In our use case, we have a platform that recommends video content. However, we have multiple event types, and some events need to be given more weight than others. Below is the list of our event types in the order of their importance:\n \nSHARE > LIKE > WATCH_COMPLETE > WATCH_PARTIAL > STARTED > SKIP\n\nSo when training the model, we would want 'SHARE' to have more weight than 'LIKE', and 'LIKE' to have more weight than 'WATCH_COMPLETE' and so on. \n\nI was looking into custom solutions. It looks like there is no way to give weights when using Personalize's custom solutions as mentioned in this [post](https:\/\/stackoverflow.com\/questions\/69456739\/any-way-to-tell-aws-personalize-that-some-interactions-count-more-than-others\/69483117#69483117)...\n\n---\n**So when using Amazon Personalize, should we use domain recommenders or build custom solutions for our use case?**\n\n**If we cannot give weights to different event types using Personalize, then what are alternatives? **Should we use Amazon SageMaker and build models from scratch? *Open to any and all suggestions.*\n\nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Aamazon SageMaker feature store throughput and latency",
        "Question_created_time":1639718714901,
        "Question_last_edit_time":1668601320841,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDX9mkJlNQzaR8lF50Z4H2Q\/aamazon-sagemaker-feature-store-throughput-and-latency",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":763,
        "Question_answer_count":1,
        "Question_body":"What is the maximum throughput of AWS feature store. Also what is the P99 value of latency of AWS feature store (online store) ?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_created_time":1639669045329,
        "Question_last_edit_time":1667926687612,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across **multiple .py files** for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing [the docs](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-enable.html#training-compiler-enable-pysdk), I added the `distributed_training_launcher.py` launcher script to my `source_dir` bundle, and passed in the true training script via a `training_script` hyperparameter.\n\n...But when the job tries to start, I get:\n\n```\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n```\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (`p3.2xlarge`) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within [TrainingArguments](https:\/\/huggingface.co\/transformers\/v3.0.2\/main_classes\/trainer.html#transformers.TrainingArguments) itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\n**EDIT: Turns out the below error can be solved by explicitly setting `n_gpus` as mentioned on the [troubleshooting doc](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-troubleshooting.html#training-compiler-troubleshooting-missing-xla-config) - but that takes me back to the error message above**\n\n```\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration\n```",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1657872107440,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1657872107441,
        "Answer_comment_count":0.0,
        "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a `_mp_fn` (which can just execute the same code as gets run `if __name__ == \"__main__\"`) and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online [here](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/pull\/14\/commits\/45fa386faa3eee527395251449e6a58e3fb5f13c). For others would also suggest referring to the [official SMTC example notebooks & scripts](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-examples-and-blogs.html)!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"SageMaker Studio PyTorch 1.8 kernel has no PyTorch, Numpy, or Matplotlib module",
        "Question_created_time":1639384597526,
        "Question_last_edit_time":1668615299839,
        "Question_link":"https:\/\/repost.aws\/questions\/QUns1rahq-ShmzYk0GJoLGWA\/sagemaker-studio-pytorch-1-8-kernel-has-no-pytorch-numpy-or-matplotlib-module",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":453,
        "Question_answer_count":2,
        "Question_body":"I'm working with SageMaker studio with the following options:\n - kernel: PyTorch 1.8 Python 3.6 GPU optimized.\n - instance: ml.g4dn.xlarge\n\nWhen running `import torch` `numpy`, `matplotlib` or `PIL`, I'm getting the `No module named 'X'` error. No matter when using `pip install` in a cell above, it will not be imported. Is this a problem only I am encountering with the new PyTorch 1.8 kernel? It also happens with the CPU-optimized version. However, PyTorch 1.6 kernel does not throw an error.\n\nWhen running `conda list`, I get the output without any of the previously mentioned modules.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Do I have to redownload dataset to training job every time I run a Sagemaker Estimator training job?",
        "Question_created_time":1639259390804,
        "Question_last_edit_time":1668600281088,
        "Question_link":"https:\/\/repost.aws\/questions\/QUleNfBVthSaGI7rAT2wsKWQ\/do-i-have-to-redownload-dataset-to-training-job-every-time-i-run-a-sagemaker-estimator-training-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":312,
        "Question_answer_count":1,
        "Question_body":"Hi, \nOver the coming weeks I'll be running some deep learning experiments using the PyTorch Sagemaker estimator, and I was wondering if it would be possible to avoid re-downloading my dataset every time I call estimator.fit()? \n\nIs there a way to do this without using FastFile mode - ie downloading the dataset once and using the same docker image? \n\nIf it's not possible to do it with online instances, would it be possible to re-use the docker instance used if I was to run it in local mode (ie instance_type='local_gpu') - if so, how?\n\nAnd just to add, I am using S3 for the input data.\n\nMany thanks,\nTim",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"AWS SageMaker Endpoint Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check",
        "Question_created_time":1639162238458,
        "Question_last_edit_time":1668618111408,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV4VuYCKHTEedPanW-keHTQ\/aws-sagemaker-endpoint-failed-reason-the-primary-container-for-production-variant-alltraffic-did-not-pass-the-ping-health-check",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2154,
        "Question_answer_count":1,
        "Question_body":"**Links to the AWS notebooks for reference**\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb \n\nhttps:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\nI am using the example from the notebooks to create and deploy an endpoint to AWS SageMaker Cloud. I have passed all the checks locally and when I attempt to deploy the endpoint I run into the issue. \n\n**Code**\n\nIn my local notebook (my personal machine NOT sagemaker notebook):\n\n```\n    import pandas\n    import xgboost\n    from xgboost import XGBRegressor\n    import numpy as np\n    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n    \n    print(xgboost.__version__)\n    1.0.1\n\n    # Fit model\n    r.fit(X_train.toarray(), y_train.values)\n\n    xgbest = r.best_estimator\n\n```\n\n**AWS SageMaker Endpoint code**\n\n```\nimport boto3\nimport pickle\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\nfrom time import gmtime, strftime\n\nregion = boto3.Session().region_name\n\nrole = 'arn:aws:iam::111:role\/xxx-sagemaker-role'\n\nbucket = 'ml-model'\nprefix = \"sagemaker\/xxx-xgboost-byo\"\nbucket_path = \"https:\/\/s3-{}.amazonaws.com\/{}\".format('us-west-1', 'ml-model')\n\nclient = boto3.client(\n    's3',\n    aws_access_key_id=xxx\n    aws_secret_access_key=xxx\n)\nclient.list_objects(Bucket=bucket)\n\n```\n\n**Save the model**\n\n```\n# save the model, either xgbest \nmodel_file_name = \"xgboost-model\"\n\n# using save_model\n# xgb_model.save_model(model_file_name)\n\npickle.dump(xgbest, open(model_file_name, 'wb'))`\n\n!tar czvf xgboost_model.tar.gz $model_file_name\n\n```\n\n**Upload to S3**\n\n```\nkey = 'xgboost_model.tar.gz'\n\nwith open('xgboost_model.tar.gz', 'rb') as f:\n    client.upload_fileobj(f, bucket, key)\n```\n\n**Import model**\n\n```\n# Import model into hosting\ncontainer = get_image_uri(boto3.Session().region_name, \"xgboost\", \"0.90-2\")\nprint(container)\n\nxxxxxx.dkr.ecr.us-west-1.amazonaws.com\/sagemaker-xgboost:0.90-2-cpu-py3\n```\n\n```\n%%time\n\nmodel_name = model_file_name + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\nmodel_url = \"https:\/\/s3-{}.amazonaws.com\/{}\/{}\".format(region, bucket, key)\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\nfrom sagemaker.local import LocalSession\n\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         region_name=\"us-west-1\",\n                         aws_access_key_id='xxxx',\n                         aws_secret_access_key='xxxx'\n                        )\n\n# Define session\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nmodels3_uri = \"s3:\/\/ml-model\/xgboost_model.tar.gz\"\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=models3_uri,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   # Cloud\n                                   sagemaker_session = sagemaker_session\n                                   # Local\n                                   # sagemaker_session = None\n           \n)\n\n#serializer = StringSerializer(content_type=\"text\/csv\")\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       # Cloud\n                                       instance_type=\"ml.t2.large\",\n                                       # Local\n                                       # instance_type = \"local\",\n                                       serializer = \"text\/csv\"\n)\n\n\nif xgb_inference_model.sagemaker_session.local_mode == True:\n    print('Deployed endpoint in local mode')\nelse:\n    print('Deployed endpoint to SageMaker AWS Cloud')\n\n\n\/Applications\/Anaconda\/anaconda3\/lib\/python3.9\/site-packages\/sagemaker\/session.py in wait_for_endpoint(self, endpoint, poll)\n   3354         if status != \"InService\":\n   3355             reason = desc.get(\"FailureReason\", None)\n-> 3356             raise exceptions.UnexpectedStatusException(\n   3357                 message=\"Error hosting endpoint {endpoint}: {status}. Reason: {reason}.\".format(\n   3358                     endpoint=endpoint, status=status, reason=reason\n\nUnexpectedStatusException: Error hosting endpoint sagemaker-xgboost-xxxx: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..\n\n```",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Question_created_time":1638914293851,
        "Question_last_edit_time":1668630486323,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Question_score_count":0,
        "Question_favorite_count":2,
        "Question_comment_count":1,
        "Question_view_count":1538,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this? \n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker. \n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models? \n\nMany thanks \n\nTim",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1638917636668,
        "Answer_score_count":2.0,
        "Answer_last_edit_time":1638917636668,
        "Answer_comment_count":2.0,
        "Answer_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to use categorical data without converting?",
        "Question_created_time":1638858572453,
        "Question_last_edit_time":1667926299269,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_mbjTldXQY2DHVM7-rMb_g\/how-to-use-categorical-data-without-converting",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":159,
        "Question_answer_count":4,
        "Question_body":"I am trying to develop a model in SageMaker, but my data is contained categorical type of data, and I would not want to convert it. In the old Machine Learning, I can use back the same data and train the model without any issue. However, when I tried the built-in algorithm, I got the error message that wanted me to convert the data. Is there anyway to do the same as the old Machine Learning without converting? Thank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"passing a numpy array to predict_fn when making inference for xgboost model",
        "Question_created_time":1638724249630,
        "Question_last_edit_time":1668612177412,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0wEAMBoQaK4s-Bsbp0qHA\/passing-a-numpy-array-to-predict-fn-when-making-inference-for-xgboost-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":970,
        "Question_answer_count":2,
        "Question_body":"I have a model that's trained locally and deployed to SageMaker to make inferences \/ invoke endpoint. When I try to make predictions, I get the following exception. \n \n    raise ValueError('Input numpy.ndarray must be 2 dimensional')\n    ValueError: Input numpy.ndarray must be 2 dimensional\n        \n\nMy `model` is a xgboost model with some pre-processing (variable encoding) and hyper-parameter tuning. Here's what `model` object looks like: \n\n    XGBRegressor(colsample_bytree=xxx, gamma=xxx,\n                 learning_rate=xxx, max_depth=x, n_estimators=xxx,\n                 subsample=xxx)\n\nMy test data is a string of float values which is turned into an array as the data must be passed as numpy array. \n \n    testdata = [........., 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2000, 200, 85, 412412, 123, 41, 552, 50000, 512, 0.1, 10.0, 2.0, 0.05]\n\n\nI have tried to reshape the numpy array from 1d to 2d, however, that doesn't work as the number of features between test data and trained model do not match. \n\nMy question is how do I pass a numpy array same as the length of # of features in trained model? I am able to make predictions by passing test data as a list locally. \n\nMore info on inference script here: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\n    Traceback (most recent call last):\n    File \"\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_functions.py\", line 93, in wrapper\n    return fn(*args, **kwargs)\n    File \"\/opt\/ml\/code\/inference.py\", line 75, in predict_fn\n    prediction = model.predict(input_data)\n    File \"\/miniconda3\/lib\/python3.6\/site-packages\/xgboost\/sklearn.py\", line 448, in predict\n    test_dmatrix = DMatrix(data, missing=self.missing, nthread=self.n_jobs)\n    File \"\/miniconda3\/lib\/python3.6\/site-packages\/xgboost\/core.py\", line 404, in __init__\n    self._init_from_npy2d(data, missing, nthread)\n    File \"\/miniconda3\/lib\/python3.6\/site-packages\/xgboost\/core.py\", line 474, in _init_from_npy2d\n    raise ValueError('Input numpy.ndarray must be 2 dimensional')\n    ValueError: Input numpy.ndarray must be 2 dimensional",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"mxnet error encountered in Lambda Function",
        "Question_created_time":1638553590411,
        "Question_last_edit_time":1668395331880,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW1vSlOVxRC2pGlTEXs0Z2w\/mxnet-error-encountered-in-lambda-function",
        "Question_score_count":0,
        "Question_favorite_count":1,
        "Question_comment_count":4,
        "Question_view_count":125,
        "Question_answer_count":1,
        "Question_body":"I trained and deployed a semantic segmentation network (mlp2.xlarge) using SageMaker. I wanted to use an AWS Lambda function to send an image to this endpoint and get a mask in return however when I use invoke_endpoint it gives an mxnet error in the logs. Funnily when I use the deployed model from a transformer object from inside the SageMaker notebook the mask is returned properly.\nHere is my Lambda function code:\n\n```\nimport json\nimport boto3\n\ns3r = boto3.resource('s3')\n\ndef lambda_handler(event, context):\n    # TODO implement\n    \n    bucket = event[\"body\"]\n    key = 'image.jpg'\n    local_file_name = '\/tmp\/'+key\n    s3r.Bucket(bucket).download_file(key, local_file_name)\n\n    runtime = boto3.Session().client('sagemaker-runtime')\n\n    with open('\/tmp\/image.jpg', 'rb') as imfile:\n        imbytes = imfile.read()\n\n    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n    response = runtime.invoke_endpoint(\n    EndpointName='semseg-2021-12-03-10-05-58-495', \n    ContentType='application\/x-image', \n    Body=bytearray(imbytes))                       # The actual image\n\n    # The response is an HTTP response whose body contains the result of our inference\n    result = response['Body'].read()\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps(result)\n    }\n```\n\nHere are the errors I see in the logs:\nmxnet.base.MXNetError: [10:26:14] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.4.x.4276.0\/AL2_x86_64\/generic-flavor\/src\/3rdparty\/dmlc-core\/src\/recordio.cc:12: Check failed: size < (1 << 29U) RecordIO only accept record less than 2^29 bytes",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Error Invoking endpoint deployed locally using SageMaker SDK for a xgboost model",
        "Question_created_time":1638547297863,
        "Question_last_edit_time":1668402122984,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnHPDXJNyTzm85yaJUFJYaA\/error-invoking-endpoint-deployed-locally-using-sagemaker-sdk-for-a-xgboost-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":348,
        "Question_answer_count":2,
        "Question_body":"I am deploying a SageMaker endpoint locally for xgboost model and running to some issues when invoking the endpoint. \n\nI am able to successfully deploy the endpoint in local model using the following code sample:\n\n\n```\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=models3_uri,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   sagemaker_session = None # sagemaker_session if cloud \/ prod mode     \n)\n\nprint('Deploying endpoint in local mode')\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       #instance_type=\"ml.m5.xlarge\",\n                                       instance_type = \"local\",\n)\n\n```\n\nI have the `inference.py` that includes the functions for accepting input, making predictions and output. \nLink here: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py \n\nThe issue I am running into is with type of data that `input_fn` accepts. I have tried passing passing a numpy array  \/ dataframe \/ bytes object as input data, but still get the error. \n\n\n```\ndef input_fn(request_body, request_content_type):\n    \"\"\"\n    The SageMaker XGBoost model server receives the request data body and the content type,\n    and invokes the `input_fn`.\n    \n    Return a DMatrix (an object that can be passed to predict_fn).\n    \"\"\"\n    # Handle numpy array type\n    if request_content_type == \"application\/x-npy\":\n        \n        print(type(request_body))\n        \n        array = np.load(BytesIO(request_body))\n        \n        return xgb.DMatrix(request_body)\n    \n    if request_content_type == \"text\/csv\":\n        \n        print(\"request body\", request_body)\n        \n        # change to request_body to Pandas DataFrame\n \n        return xgb_encoders.libsvm_to_dmatrix(request_body)\n        #perform encoding on the input data here\n    \n    else:\n        raise ValueError(\"Content type {} is not supported.\".format(request_content_type))\n\n```\n\nEncoder object. Training data is encoded before fit. Doing the same with test data. Posting here for reference. \n\n\nI tried making predictions using NumpySerializer and CSVSerializer [1]. Both don't work - \n\n```\nfrom sagemaker.serializers import NumpySerializer\npredictor.serializer = NumpySerializer()\n\ntestpoint = encoder.transform(df).toarray()\n\nprint(testpoint)\n\n[[0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00.........1.000e+01 2.000e+00 5.312e-02]]\n```\n\n\nTraceback with CSVSerializer(), when passing body of type text\/csv \n\n```\nException on \/invocations [POST]\n | Traceback (most recent call last):\n\nFile \"\/opt\/ml\/code\/inference.py\", line 35, in input_fn\nreturn xgb_encoders.libsvm_to_dmatrix(request_body)\npackages\/sagemaker_xgboost_container\/encoder.py\", line 65, in libsvm_to_dmatrix\nTypeError: a bytes-like object is required, not 'str'\n```\n\nTraceback with NumpySerializer() when passing <class 'numpy.ndarray'> type body\n\n```\nException on \/invocations [POST]\nTraceback (most recent call last):\nraise TypeError('no supported conversion for types: %r' % (args,))\nTypeError: no supported conversion for types: (dtype('O'),)\n```\n\n[1] https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Model Registry - how to set the Stage column of a Model Package?",
        "Question_created_time":1638530817367,
        "Question_last_edit_time":1668626223377,
        "Question_link":"https:\/\/repost.aws\/questions\/QUI07MHIlPSbSwALreeEJ28g\/sagemaker-model-registry-how-to-set-the-stage-column-of-a-model-package",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":902,
        "Question_answer_count":1,
        "Question_body":"Each version of a model (i.e. versioned `Model Package`) in a `Model Package Group` in SageMaker Model Registry has some metadata attached to it. You can see its `Status`, `Short description`, `Modified by` as well as `Stage` it is currently deployed on.\n\nThis can be seen in the docs [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-registry-deploy-history.html) on the screenshot below the 5. point on that list.\n\nMy question is - how do we explicitly (or implictly) set the `Stage` of a given `Model Package`? I know that changing the approval status can be done by calling [update_model_package](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.update_model_package) method. What about the `Stage`?\n\nIt magically happens when we use provided MLOps templates, but even after a thorough browsing of every generated file (SM Pipelines, CodePipeline, CodeBuild etc.) I could not find the exact API call. It probably has something to do with the `sagemaker:deployment-stage` Tag but setting it up on `Endpoint`, `Endpoint Config` or `Model` (the old construct of pre-2020 SageMaker that is still used by SageMaker Inference) did not work. You can't set tags on a `Model Package`.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to import postgresql or nosql datasets in Amazon Sagemaker?",
        "Question_created_time":1638524627821,
        "Question_last_edit_time":1668624703968,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFyL4V0gFQ3WJsCwd8ZZgrA\/how-to-import-postgresql-or-nosql-datasets-in-amazon-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":410,
        "Question_answer_count":3,
        "Question_body":"Hi, I am new to Amazon Sagemaker. I noticed that available options to import datasets are through local files, S3, Redshift, Snowflake and Athena only. Is there a way to import PostgreSQL or MySQL or any other NoSQL datasets?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Using SageMaker SDK to deploy a open source xgboost model locally",
        "Question_created_time":1638503327094,
        "Question_last_edit_time":1668492765209,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEH97qD5dSjS93XkXTdel8w\/using-sagemaker-sdk-to-deploy-a-open-source-xgboost-model-locally",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":280,
        "Question_answer_count":1,
        "Question_body":"I have a locally trained model that I am trying to debug locally on docker container before deploying \/ creating endpoint on SageMaker. I am following the documentation that AWS customer service provided, however, I am running into issue with Creating Endpoint Config. \n\nHere's the code snippet: \n\n\n```\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         aws_access_key_id='xxxxxx',\n                         aws_secret_access_key='xxxxxx'\n                        )\n\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=model_url,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   sagemaker_session = sagemaker_session     \n)\n\nprint('Deploying endpoint in local mode')\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       instance_type = \"local\"\n)\n\n\nTraceback:\n\n20 print('Deploying endpoint in local mode')\n21 predictor = xgb_inference_model.deploy(\n22                                        initial_instance_count = 1,\n\nClientError: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'local' at 'productionVariants.1.member.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.r5d.12xlarge, ml.r5.12xlarge, ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.r5d.24xlarge,\n```\n\n\n\nHere's the documentation link: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/xgboost_script_mode_local_training_and_serving.py",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"XGBoost Reports Not Generated",
        "Question_created_time":1638475570231,
        "Question_last_edit_time":1668531229210,
        "Question_link":"https:\/\/repost.aws\/questions\/QUx_M71_2nQJSDp-I1mgbjDg\/xgboost-reports-not-generated",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":3,
        "Question_view_count":305,
        "Question_answer_count":3,
        "Question_body":"Hi!\n\nI have been trying to create a model using XGBoost, and was able to successfully run\/train the model. However, I have not been able to generate the training reports. I have included the rules parameter as follows: \"rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\".\n\nI am following this tutorial, but I am using objective: \"multi:softmax\" instead of the \"binary:logistic\" used in the example.\n\nWhen I run the model everything is fine but only the Profiler Report gets generated and I do not see the XGBoostReport under the rule-output folder. According to the tutorial it should be under the same file path.\n\nHere is my code for the model if it helps any:\n\n```\ns3_output_location='s3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'xgboost_model')\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n\ntrain_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/train.csv\"), content_type=\"csv\"\n)\nvalidation_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/validation.csv\"), content_type=\"csv\"\n)\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=sagemaker.get_execution_role(),\n    instance_count=1,\n    instance_type=\"ml.c5.2xlarge\",\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=6,\n    objective='multi:softmax',\n    num_class=num_classes,\n    gamma=800,\n    num_round=250\n)\n```\n\nAny help is appreciated! Thanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Location of Huggingface SageMaker Dockerfile.",
        "Question_created_time":1636035334158,
        "Question_last_edit_time":1668483758490,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUuhJq-uDQRmUoyuagoHPsQ\/location-of-huggingface-sagemaker-dockerfile",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":287,
        "Question_answer_count":2,
        "Question_body":"Where is the github repository of the Dockerfile for Huggingface training with SageMaker? I see [this](https:\/\/github.com\/aws\/sagemaker-huggingface-inference-toolkit) repository for inference, but do not see one for training.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Specify a subnet for SageMaker endpoints",
        "Question_created_time":1634219209733,
        "Question_last_edit_time":1668574085762,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_fdJIdbDTuSL60rOV-Bv8g\/specify-a-subnet-for-sagemaker-endpoints",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":914,
        "Question_answer_count":2,
        "Question_body":"Similar to setting subnet configuration for training jobs, is it possible to specify a subnet for SageMaker endpoints? Are they deployed in any default subnets, and is there a way to find that through SDK\/CLI?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ClientError: Data download failed:Unable to create download dir",
        "Question_created_time":1633390841000,
        "Question_last_edit_time":1668620722371,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhODEUiiMT2y0cBJixQ3ofA\/clienterror-data-download-failed-unable-to-create-download-dir",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":431,
        "Question_answer_count":1,
        "Question_body":"Hello,  \nI searched for this error in the forum read through the first 6 or 7 forum pages with no luck. Im able to build models in Sagemaker Studio, but when I try in Sagemaker I get this error:  \n  \nClientError: Data download failed:Unable to create download dir \/opt\/ml\/checkpoints\/tc19\/preprocessed-data\/header  \n  \nAnybody know how to clear this ?  \n  \nThank in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models",
        "Question_created_time":1629115147000,
        "Question_last_edit_time":1668625448071,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_yXPjW3TUOinot9BJe5GA\/why-do-i-get-an-error-that-sagemaker-endpoint-does-not-have-multiple-models",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":618,
        "Question_answer_count":1,
        "Question_body":"Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.\n\n```\n    create_endpoint_config_response = client.create_endpoint_config(\r\n        EndpointConfigName=endpoint_config_name,\r\n        ProductionVariants=[\r\n            {\r\n                \"InstanceType\": \"ml.m5.large\",\r\n                \"InitialVariantWeight\": 0.5,\r\n                \"InitialInstanceCount\": 1,\r\n                \"ModelName\": model_name1,\r\n                \"VariantName\": model_name1,\r\n            },\r\n             {\r\n                \"InstanceType\": \"ml.m5.large\",\r\n                \"InitialVariantWeight\": 0.5,\r\n                \"InitialInstanceCount\": 1,\r\n                \"ModelName\": model_name2,\r\n                \"VariantName\": model_name2,\r\n            }\r\n        ]\r\n    )\n```\n\nI confirm in the GUI that it in fact has multiple models. I invoke it like this:\n\n```\n    response = client.invoke_endpoint(\r\n        EndpointName=endpoint_name, \r\n        TargetModel=model_name1,\r\n        ContentType=\"text\/x-libsvm\", \r\n        Body=payload\r\n    )\n```\n\nand get this error:  \n   \n  \n> ValidationError: An error occurred (ValidationError) when calling the  \n> InvokeEndpoint operation: Endpoint  \n> my-endpoint1 is not a multi-model endpoint  \n> and does not support target model header.  \n  \n   \nThe same problem was discussed  at https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026  with no resolution.  \n  \nHow can I invoke a multimodel endpoint?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"GPU not detected by tensorflow in SM Studio",
        "Question_created_time":1627628146000,
        "Question_last_edit_time":1668612927916,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYk1rHeoARsSfMsuiL-0JgQ\/gpu-not-detected-by-tensorflow-in-sm-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":396,
        "Question_answer_count":3,
        "Question_body":"I'm currently using SageMaker Studio with kernel \"Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)\".  \nTensorflow doesn't detect the GPU on both ml.g4dn.xlarge and ml.g4dn.2xlarge instances (with 1 GPU).  \nI would appreciate any advice.  \n  \nimport tensorflow as tf  \ntf.config.list_physical_devices('GPU')  \n: \\[]  \n  \nEdited by: haganHL on Jul 29, 2021 11:55 PM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"API Gateway returns TARGET_MODEL_HEADER_MISSING",
        "Question_created_time":1627461635000,
        "Question_last_edit_time":1667926034353,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMJ7lKY6CRl2oJOgdGVakdg\/api-gateway-returns-target-model-header-missing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":67,
        "Question_answer_count":1,
        "Question_body":"Hi everybody  \n  \nI have a deployed endpoint for a Sagemaker MultiDataModel. I can call it succesfully from my local computer using boto3.  \nI've set up an API Gateway to this Sagemaker MultiDataModel and I am trying to retrieve predictions from the model using a https request. But I keep getting a TARGET_MODEL_HEADER_MISSING-error.  \n  \nMy https request looks like this:  \nheaders = {  \n    'X-Amzn-SageMaker-Target-Model':'\/jobtitles-exact'  \n          }  \nresponse = requests.request(\"POST\"  \n                            , \"https:\/\/XXXXXXXXXX.execute-api.eu-north-1.amazonaws.com\/v1\/predicted-job-titles\"  \n                            , headers = headers  \n                            , data = data  \n                           )  \n  \nAccording to the documentation here:  \nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html  \nand the source code here:  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py  \nit seems like I am providing the header with the target model correctly. But this is obvously not the case.  \n  \nHow am I supposed to provide the target model in the header with the https-request?  \n  \nBest regards  \nElias",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"API Gateway returns TARGET_MODEL_HEADER_MISSING",
        "Question_created_time":1627461625000,
        "Question_last_edit_time":1667925687538,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3daqTW9fR5m2U-HV0U3f6A\/api-gateway-returns-target-model-header-missing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":55,
        "Question_answer_count":1,
        "Question_body":"Hi everybody  \n  \nI have a deployed endpoint for a Sagemaker MultiDataModel. I can call it succesfully from my local computer using boto3.  \nI've set up an API Gateway to this Sagemaker MultiDataModel and I am trying to retrieve predictions from the model using a https request. But I keep getting a TARGET_MODEL_HEADER_MISSING-error.  \n  \nMy https request looks like this:  \nheaders = {  \n    'X-Amzn-SageMaker-Target-Model':'\/jobtitles-exact'  \n          }  \nresponse = requests.request(\"POST\"  \n                            , \"https:\/\/XXXXXXXXXX.execute-api.eu-north-1.amazonaws.com\/v1\/predicted-job-titles\"  \n                            , headers = headers  \n                            , data = data  \n                           )  \n  \nAccording to the documentation here:  \nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html  \nand the source code here:  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py  \nit seems like I am providing the header with the target model correctly. But this is obvously not the case.  \n  \nHow am I supposed to provide the target model in the header with the https-request?  \n  \nBest regards  \nElias",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Multidatamodels and API Gateway",
        "Question_created_time":1627382816000,
        "Question_last_edit_time":1668031033818,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV-bD_HPIQ8ua0Il7RGOaPQ\/multidatamodels-and-api-gateway",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_body":"Hi forum!  \n  \nI have set up a multidatamodel with two models on it, model1 and model2.  \nI can call the model from a Sagemaker notebook or locally using boto3 and invoke_endpoint. Everything works like a charm there.  \n  \nBut I would like to set up an API with API Gateway - and that is proving troublesome. I keep getting a 403 \"Unable to determine service\/operation name to be authorized\"-error.  \n  \nIs API Gateway supporting MultiDataModels? And is there any documentation or guides on how to make them interact?  \n  \nBest regards  \nElias",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Studio:  Syntax highlighting does not work",
        "Question_created_time":1625245313000,
        "Question_last_edit_time":1668333501227,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUwwXalEUSsiJBm3joeEtnQ\/studio-syntax-highlighting-does-not-work",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":100,
        "Question_answer_count":1,
        "Question_body":"On my jupyter notebook syntax highlighting does not work.  Its also greyed out in the dropdown menu.  \nIt does work on raw python files.  \n  \nDo I need to enable a setting to turn this on?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Create endpoint from Python",
        "Question_created_time":1625083671000,
        "Question_last_edit_time":1668599351248,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":125,
        "Question_answer_count":2,
        "Question_body":"Hello,  \n  \nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.  \n  \nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.  \n  \nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?  \n  \nRegards.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1626971777000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1626971777000,
        "Answer_comment_count":0.0,
        "Answer_body":"Hello hugoflores,   \n  \nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.  \n  \nHere are a few resources towards that:  \n  \nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services  \nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/  \nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/  \n  \nHTH,   \n  \nChaitanya",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to pass data to an endpoint",
        "Question_created_time":1625081705000,
        "Question_last_edit_time":1668423337511,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":613,
        "Question_answer_count":3,
        "Question_body":"Hello,  \nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.  \n  \nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb  \n  \nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:  \n  \n1. Loading the CSV exactly the way I did it on the notebook  \n2. Parsing the CSV the same way I did on the notebook for the \"predictor.predict\" command  \n3. Instead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point  \n4. Instead of getting the same response I got on the notebook, I am getting the following message:  \n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"  \n  \nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?  \n  \nAny help will be appreciated.  \nRegards",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1626970917000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1626971072000,
        "Answer_comment_count":0.0,
        "Answer_body":"Hello,  \n  \nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().  \n  \n If you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:   \n  \nfrom sagemaker.serializers import IdentitySerializer  \nfrom sagemaker.deserializers import JSONDeserializer  \nserializer=IdentitySerializer(content_type=\"application\/json\")  \n  \nHope this helps!  \n  \nTo check out the various serializer options that can work for your different use cases check the following link.   \nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM  \n  \nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_created_time":1623820480000,
        "Question_last_edit_time":1668617546036,
        "Question_link":"https:\/\/repost.aws\/questions\/QU68gR5B3JRdqOEGlwE2-pnA\/an-error-occurred-modelerror-when-calling-the-invokeendpoint-operation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2444,
        "Question_answer_count":1,
        "Question_body":"Hello,  \nI received the following error message when I tried to send an array to my model:  \n  \nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from container-1 with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">  \n<title>500 Internal Server Error<\/title>  \n<h1>Internal Server Error<\/h1>  \n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p>  \n\". See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group  \n  \nI have created inference pipeline containing preprocessing and autoencoder model and deployed it to a single endpoint. Am trying to send raw data in text\/csv format. EX: \"39, 4, 9, 8, contact\"  \n  \nPlease help me out in this.   \n  \nMuch appreciated,  \nKarthik",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sage Maker Notebook - overcommit_memory settings - permission denied",
        "Question_created_time":1621878351000,
        "Question_last_edit_time":1668591485428,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPG-W9iG_RteLMwkPbpvEZg\/sage-maker-notebook-overcommit-memory-settings-permission-denied",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":436,
        "Question_answer_count":1,
        "Question_body":"I am doing some NLP work using spaCy in a ml.p2.xlarge notebook instance in sagemaker.  \n  \nHowever, I get the following error:  \n  \nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.10 GiB for an array with shape (2889884, 288) and data type float32  \n  \nTo fix the issue, I have tried the following commands:  \n  \n!echo 1 | sudo tee \/proc\/sys\/vm\/overcommit_memory  \n!echo 1 > \/proc\/sys\/vm\/overcommit_memory  \n  \nNeither of them work and they both return:  \n\/bin\/sh: \/proc\/sys\/vm\/overcommit_memory: Permission denied  \n  \nAny suggestions? I even added all policies with \"admin\" in them to the notebook IAM with no luck.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Can Sagemaker-trained models be deployed to non-Sagemaker endpoints?",
        "Question_created_time":1619959658000,
        "Question_last_edit_time":1668365245143,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVfuRLH2cSIifokrvWNaWdA\/can-sagemaker-trained-models-be-deployed-to-non-sagemaker-endpoints",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_body":"I am looking for a way to train model which will be deployed for inferencing, but Google AI Platform or an on-premises VM, not to a Sagemaker endpoint. Are Sagemaker-trained generic enough to be used for that? If so, could you point me to documentation?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Tracking model artifacts used in machine learning",
        "Question_created_time":1619803212000,
        "Question_last_edit_time":1668544599315,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5EvOvmhTkqTM-NYhYAJZA\/tracking-model-artifacts-used-in-machine-learning",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":349,
        "Question_answer_count":1,
        "Question_body":"What are effective working patterns and tools to ensure we can easily reproduce the model artifacts deployed in production? (Customer is using DVC and Github to get version control on all key aspects: data, training scripts, model specification, hyper parameters, etc.) I'd like to share AWS best practices and recommendations with them.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1619814328000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925575541,
        "Answer_comment_count":0.0,
        "Answer_body":"There are a few good practices that would ensure a robust model governance and tracking strategies in place. This is usually required by regulators especially in the financial industry and defined by frameworks like SR 11-7:\n\n**\u2022 Identify models, owners, and associated usage:** This is usually controlled by having a controlled landing zone for data scientists with clear authentication and authorization strategies to keep track of user activities and model owners.\n\n**\u2022 Cover all aspects of the model life cycle and MLOps:** By using an experimentation, proper documentation, feature tagging, testing, deployment environment and pipelining so that models can be independently validated by model validators without having to get back to model developers.\n\n**\u2022 maintain a centralized model inventory, and track the current validation status:** by keeping track of different model versions along with its associated risk and validation processes.\n\n**On-going monitoring for production models:** by implementing mechanisms to continuously assess accuracy, drift, building constraints on data feed used for inference and outcome analysis for different model versions.\n\nNow saying that, there are many tools out there to help in implementing and achieving all the above, which are scattered and can become challenge in implementation and integration. However, SageMaker have different modules to cover model of the practices mentioned above.\n\n- SageMaker Experiments: tracks all different steps of ML lifecycle in a construct called trial component. A bunch of trial components can form a trial and a trial belongs to an experiment. \n\n- SageMaker Pipelines: Help in building a re-producible experiment. Each stage of the ML Lifecycle fits in the pipeline and can automatically be tracked as Trial Components and tracked by its execution history.\n\n- SageMaker Model Registry: Builds a centralized catalog for models to manage different model versions, associate meta data with different version of models and manage approvals to enforce ownership.\n\n- SageMaker Model Monitoring: for managing data feed constraints, model performance analysis and drift detection.\n\n- SageMaker Feature Store: with proper tagging for feature groups and why certain features have been engineered by who is also a must.\n\n- SageMaker ML Lineage: which - from my point of view - is the most important component for model tracking, auditing and governance. SM ML Lineage is the glue that builds a graph to trace a certain model back to its origins. It can tell what Artifact contributed to which Trial Component and what data produced which model.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Waiting for SageMaker CreateTrainingJob to Finish",
        "Question_created_time":1618441857000,
        "Question_last_edit_time":1668620571120,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgs9xRKGRYaibqIaX7sdzg\/waiting-for-sagemaker-createtrainingjob-to-finish",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":350,
        "Question_answer_count":1,
        "Question_body":"I'm invoking the following task in my step function workflow:  \n  \nhttps:\/\/docs.aws.amazon.com\/cdk\/api\/latest\/docs\/@aws-cdk_aws-stepfunctions-tasks.SageMakerCreateTrainingJob.html  \n  \nSageMakerCreateTrainingJob starts the task in an asynchronous manner, i.e. the task returns immediately after I request it to start, even if the training job may take hours to finish.  \n  \nI want to have another task execute after the training job truly and completely finishes.  What is an easy way to accomplish this without having to use the two techniques below?  \n  \nI know there are two clunky ways to do this:  \n- Using a combination of a Lambda function that calls the SageMaker API to poll for the status of a training job - i.e. to detect when it is finished - and a wait state to force a poll every X number of minutes or so.  \n- Another possibility is using step function activities, but that would probably preclude me from using Lambdas as a Lambda activity worker would need to keep waiting (doing a thread wait) and constantly polling.  I would also need to worry about the 15 minute Lambda time limit.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_created_time":1615480055000,
        "Question_last_edit_time":1668618206192,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sagemaker-studio-projects-in-vpconly-mode-without-internet-access",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":678,
        "Question_answer_count":1,
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets **without** internet access, **NO** NAT gateways). \nThe all functionality is fine. However, when I try create a SageMaker projects - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-create.html), SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described [here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-studio-updates.html). The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1618085440000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925560363,
        "Answer_comment_count":0.0,
        "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for `com.amazonaws.${AWS::Region}.servicecatalog`",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Failing to create multi-model endpoint",
        "Question_created_time":1613660927000,
        "Question_last_edit_time":1668062163303,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcIYmAjUlRn-b_PqcIjk08A\/failing-to-create-multi-model-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":2,
        "Question_body":"I have been trying to create a multi-model endpoint with my own container, using the instructions here: <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html>.  \n  \nFollowing the instructions here, I am able to successfully create a model and endpoint configuration:   <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint-sdk.html>   \n  \nHowever, when I try to create the endpoint itself, it shows the status of \"Creating\" for over 2 hours, before finally stopping with the status, \"Failed\". It gives no reason for the failure or any other help.  \n  \nDoes anyone have any ideas?  \n  \nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Studio Enterprise Deployment guidelines",
        "Question_created_time":1612447142000,
        "Question_last_edit_time":1667925623706,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0QiBS-GdSIKZdXxSf1NUYA\/sagemaker-studio-enterprise-deployment-guidelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":86,
        "Question_answer_count":1,
        "Question_body":"Do we have guidelines on requirements gathering\/designing the provisioning of SageMaker Studio domains across large global enterprises with many business units? \n\nI've seen discussions where topics like number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, security and others were raised when defining requirements and boundaries. Customer is starting their first Studio deployment and they are asking for guidance on how to scope and design that so that they can have a scalable process.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1614090886000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925545476,
        "Answer_comment_count":0.0,
        "Answer_body":"You should guide your customer based on the general principles of multi account best practices that we provide for other services.\n\nHere are some high level boundaries.\n\nOne studio domain per account and region. No cross region AWS SSO configuration provided.\n\nMaximum numbers of users allowed in studio vary between 60 - 200 users. Although AWS SSO can support many more users, there are some considerations around other dependencies such as EFS among others.\n\nIf you need to isolate any model artifacts produced by SageMaker, you may want to have them use a separate account. Even if you use tag based access control, you can still technically list those artifacts.\n\nSageMaker feature store should follow the data lake pattern closely. As a general rule, you want to write in one account and can read from many other accounts perhaps using Lake formation to expose datasets into other accounts. Teams can create their own offline \/ online feature store for non production use cases.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker ValidationException",
        "Question_created_time":1612307749000,
        "Question_last_edit_time":1668594920705,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-bXA28SLQryx60CbXdb9wg\/sagemaker-validationexception",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1271,
        "Question_answer_count":2,
        "Question_body":"Hi All  \nI am brand new to AWS and find it all very overwhelming and confusing.   \nI am trying to use SageMaker and Forecast - however I am having issues with trying to get into SageMaker Studio. I am getting the following error:  \n   ValidationException  \n     1 validation error detected  \n      Value '\\[]' at 'subnetIds' failed to satisfy constraint: Member must have length greater than or equal to 1  \n  \nI have asked out internal team and they are not sure, and opened a support ticket. The support person said that I should be using the Standard setup rather than the Quick setup. However, this is still giving the same issue. To which he said that this is an advanced problem and he can't assist with it.  \nIs anyone able to help me? give me an idea of what is wrong?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"botocore.exceptions.ClientError: An error occurred (ValidationException)",
        "Question_created_time":1611849385000,
        "Question_last_edit_time":1668629395731,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSzifL4nASM60UV1hVI6e_A\/botocore-exceptions-clienterror-an-error-occurred-validationexception",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1622,
        "Question_answer_count":1,
        "Question_body":"Hi,  \nI want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in  <https:\/\/towardsdatascience.com\/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198> .  \n  \n    import mlflow.sagemaker as mfs  \n      \n    run_id = run_id # the model you want to deploy - this run_id was saved when we trained our model  \n    region = \"us-east-1\" # region of your account  \n    aws_id = \"XXXXXXXXXXX\" # from the aws-cli output  \n    arn = \"arn:aws:iam::XXXXXXXXXXX:role\/your-role\"  \n    app_name = \"iris-rf-1\"  \n    model_uri = \"mlruns\/%s\/%s\/artifacts\/random-forest-model\" % (experiment_id,run_id) # edit this path based on your working directory  \n    image_url = aws_id _ \".dkr.ecr.\" _ region + \".amazonaws.com\/mlflow-pyfunc:1.2.0\" # change to your mlflow version  \n      \n    mfs.deploy(app_name=app_name,   \n               model_uri=model_uri,   \n               region_name=region,   \n               mode=\"create\",  \n               execution_role_arn=arn,  \n               image_url=image_url)  \n  \nBut I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.  \n  \nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz. Please ensure that the role \"arn:aws:iam::xxx:role\/mlflow-sagemaker-dev\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-east-1.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Processing Job automatically created when I start a training job",
        "Question_created_time":1611570309000,
        "Question_last_edit_time":1667926525694,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmy7drPKBRH6hs3Pw_yj3tw\/processing-job-automatically-created-when-i-start-a-training-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":101,
        "Question_answer_count":1,
        "Question_body":"Hi,  \nI haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job runs (I don't even know what a processing job is). I also checked in the dashboard to be sure, this was not happening before, it's the second time (first time was in December) but I've been using sagemaker for the last two years..   \nIs this a wanted behaviour? I didn't find anything related in the documentation, but it's important to know because I don't want extra costs..  \nThis is the image used by the processing job, with a instance type of ml.m5.2xlarge which I didn't set anywhere..  \n  \n    929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest  \n  \nAnd this is how I launch my training job (the entrypoint script is basically Keras code for a MobileNetV3)  \n  \nimport sagemaker  \nfrom sagemaker.tensorflow import TensorFlow  \nfrom sagemaker import get_execution_role  \n  \nbucket = 'mybucket'  \n  \ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')  \n  \nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')  \n  \ns3_output_location = 's3:\/\/{}'.format(bucket)  \n  \nhyperparameters = {'epochs': 130, 'batch-size' : 512, 'learning-rate' : 0.0002}  \n  \nmetrics = .. some regex here  \n  \ntf_estimator = TensorFlow(entry_point='train.py',   \n                          role=get_execution_role(),  \n                          train_instance_count=1,   \n                          train_instance_type='ml.p2.xlarge',  \n                          train_max_run=172800,  \n                          output_path=s3_output_location,  \n                          framework_version='2.3.0',  \n                          py_version='py37',  \n                          metric_definitions = metrics,  \n                          hyperparameters = hyperparameters,  \n                          source_dir=\"data\")  \n  \ninputs = {'train': train_data, 'test': validation_data}  \nmyJobName = 'myname'  \ntf_estimator.fit(inputs=inputs, job_name=myJobName)  \n  \nEdited by: rokk07 on Jan 25, 2021 2:55 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Question_created_time":1611507553000,
        "Question_last_edit_time":1667925795097,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-modelbiasmonitor-and-modelexplainabilitymonitor",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links.\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1611571671000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1613663015963,
        "Answer_comment_count":0.0,
        "Answer_body":"The actual reference to the classes can be found here: \n[https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py ]()  \nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Using Hyperparameter Tuning Jobs over Training and Preprocessing",
        "Question_created_time":1610658074000,
        "Question_last_edit_time":1668500938669,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3xcWKDPHR8ylWSaX83lNKQ\/using-hyperparameter-tuning-jobs-over-training-and-preprocessing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":175,
        "Question_answer_count":1,
        "Question_body":"Some data science teams want to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.\n\nDoes AWS have a recommended approach to establish this using Sagemaker Hyperparameter tuning?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1610741862000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1613663716270,
        "Answer_comment_count":0.0,
        "Answer_body":"It depends on the dataset and the question for ML to answer. \n\nYes, it is feasible to do HPO with preprocessing. However, to run a HPO job, it is required to define to a specific target to achieve, e.g. maximize\/minimize certain values during the whole HPO process. Thus, it is important to understand what is the target during preprocessing.  If the answer is yes, they should be able to leverage Hyperparameter Tuning Jobs.\n\n\n\nHere is how HPO works in SageMaker. Firstly, we define each training Job with output in a container and specify the hyperparameters in  *\/opt\/ml\/input\/config\/hyperparameters.json*. When we run the pipeline using HyperparameterTuner in SageMaker,  the initial Job can pass the hyperparameters to the Pipeline for HPO, and return the model with *highest score*.\n\nOption 1, if there is a clear defined target for preprocessing to achieve,  we can also do HPO separately for data preprocessing through defining the function and outputs in a container and use HyperparameterTuner fit to tune the preprocessing.  \n\nOption 2.   include the preprocessing + training code in the whole SageMaker Training Job. But then you can't use separate infrastructure for training and preprocessing.\n\nSo it depends on what exactly they are looking for, but they can likely use SageMaker HPO.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?",
        "Question_created_time":1610118101000,
        "Question_last_edit_time":1668602387619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn-IyC9nySDKKibLL3Lvw3A\/sagemaker-pytorch-hosting-1-6-works-only-with-artifacts-named-model-pth",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":336,
        "Question_answer_count":1,
        "Question_body":"Hi, does SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?\nI'm trying [this sample with 1.6][1] and the deployment fails with error\n\n    FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/model.pth'\n\nthe documentation doesn't mention such a constraint\n\n  [1]: https:\/\/github.com\/aws-samples\/amazon-sagemaker-bert-classify-pytorch",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1610121735000,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1667925547320,
        "Answer_comment_count":1.0,
        "Answer_body":"Yes - from the thread on this open issue: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/86\n\nIn the issue thread they note that the new Pytorch 1.6 image requires that the model filename is `model.pth`, linking to the relevant code where this default is set: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/9a6869e\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L121\n\nAlso noted in the thread is that users have successfully adapted their code to use torchserve in Pytorch 1.6 by changing it to save their model in a file named `model.pth`. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom `model_fn`: https:\/\/github.com\/data-science-on-aws\/workshop\/blob\/374329adf15bf1810bfc4a9e73501ee5d3b4e0f5\/09_deploy\/wip\/pytorch\/code\/inference.py",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Question_created_time":1609954586000,
        "Question_last_edit_time":1668536452453,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sagemaker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":148,
        "Question_answer_count":1,
        "Question_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML? \n\nFor example, with an **Aurora ML** you can reference a SageMaker endpoint and then use it as a UDF in a `SELECT` statement. \n**Redshift ML** works a bit differently - when you call `CREATE MODEL` - the model is trained with **SageMaker Autopilot** and then deployed to the **Redshift Cluster**. \n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1609955532000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612026491936,
        "Answer_comment_count":0.0,
        "Answer_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference  sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\n1. Train and deploy your SageMaker model in a SageMaker Endpoint. \n2. Use Lambda function to [reference sagemaker endpoint](https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/). \n3. Create a [Redshift Lambda UDF](https:\/\/aws.amazon.com\/blogs\/big-data\/accessing-external-components-using-amazon-redshift-lambda-udfs\/) referring above lambda function to run predictions.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?",
        "Question_created_time":1607968009000,
        "Question_last_edit_time":1667925925013,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGJGFHP76S0izgf1xfM6aIg\/anybody-ever-successfully-ran-multi-node-gradient-boosting-on-amazon-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":85,
        "Question_answer_count":1,
        "Question_body":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?\nI'm looking for a SageMaker-compatible multi-node training solution for either Catboost, LightGBM or XGBoost. Knowing if it's ever been done would be nice, having a public demo link would be even better :)",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1607969193000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612842503289,
        "Answer_comment_count":0.0,
        "Answer_body":"We do have an example of distributed training of XGBoost in the sagemaker-examples repo. You can find it here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Custom Amazon SageMaker container registration and deployment tracking",
        "Question_created_time":1607710961000,
        "Question_last_edit_time":1668553545197,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdJqWN_WJQeuYrkZMikkibQ\/custom-amazon-sagemaker-container-registration-and-deployment-tracking",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":94,
        "Question_answer_count":1,
        "Question_body":"My customer asks that:\n\n\n* Container images must be registered and deployments tracked\n\n* Containers must be registered within a private customer-owned registry prior to deployment\n\n* Only registered containers are to be deployed.\n\t\n* Part of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities.\n\n* An inventory of all deployed containers must be maintained at all times. \n\n\n* The inventory must include:\n\t Software installed within the container\n\tversion of all software and patch level\n\t. Where the container has been deployed\n\t. Owner of the container\n\nDo we do any of these? Please provide documentation on AWS\/SageMaker vs custom container provider's responsibilities.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1608306218000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1608576929504,
        "Answer_comment_count":0.0,
        "Answer_body":"Amazon Elastic Container Registry (Amazon ECR) enables customers to store images, secure their images using AWS Identity and Access Management (IAM), and scan their containers for vulnerabilities. Open Policy Agent (OPA) is an open-source project focused on codifying policy such as the approved image registries. OPA is integrated with Kubernetes via Gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. For more details see: https:\/\/aws.amazon.com\/blogs\/containers\/designing-a-secure-container-image-registry",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Custom container not running under root account?",
        "Question_created_time":1607710724000,
        "Question_last_edit_time":1667926252792,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":206,
        "Question_answer_count":1,
        "Question_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\t\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\t\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\nHow do we ensure this?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1608306337000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1609348138779,
        "Answer_comment_count":0.0,
        "Answer_body":"SageMaker requires that Docker containers run without privileged access. See:  https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\nSageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Can you configure Amazon ECR containers to be immutable?",
        "Question_created_time":1607710511000,
        "Question_last_edit_time":1667926185084,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n* Containers can't be changed after their built.\n* Containers can't receive updates.\n* Changes in the containerized application must require the building and deployment of a new container image.\n* Runtime data and configurations must be stored outside of the container environment.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1608306412000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925587433,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, you can configure Amazon ECR containers to be immutable. Amazon ECR uses resource-based permissions to control access to repositories. The resource-based permissions let you specify which IAM users or roles have access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository. \n\nFor more information, see [Repository policies](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/repository-policies.html) and [Image tag mutability](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/image-tag-mutability.html) in the Amazon ECR user guide.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_created_time":1607681930000,
        "Question_last_edit_time":1667981435997,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sagemaker-manifest-files-enable-dataset-versioning",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":124,
        "Question_answer_count":1,
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1607684202000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925565630,
        "Answer_comment_count":0.0,
        "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Attaching custom image to user (not domain) in SageMaker Studio",
        "Question_created_time":1607576593000,
        "Question_last_edit_time":1668514383581,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhP1jmxpAQi6X0eDIUI2JKA\/attaching-custom-image-to-user-not-domain-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":338,
        "Question_answer_count":1,
        "Question_body":"Is there a way to attach a custom image to just the user (not the domain) in SageMaker Studio. \n\nDocumentation states 'To make a custom SageMaker image available to all users within a domain, you attach the image to the domain. To make an image available to a single user, you attach the image to the user's profile.'\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-byoi.html \n\nWhen I 'edit user', I dont see a way to attach a custom image. Is there a way to do this?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1607593010000,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1612932017912,
        "Answer_comment_count":0.0,
        "Answer_body":"You can attach custom images to user profiles via the APIs to create\/update user profiles.\n\nMore info:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateUserProfile.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateUserProfile.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AWS AI\/ML integration with Power BI",
        "Question_created_time":1607495476000,
        "Question_last_edit_time":1668614604512,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":360,
        "Question_answer_count":2,
        "Question_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1607528438000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1614011625614,
        "Answer_comment_count":0.0,
        "Answer_body":"PowerBI can connect to Amazon Redshift and leverage the [new SQL based ML capability in Redshift][1] that uses Sagemaker under the hood.  \n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage [Blazing Text Algorithm][2] in Sagemaker or some market place solution like [Twinword sentiment model][3] in sagemaker for sentiment analysis for Text mining after the integration.\n\n\n\n  [1]: https:\/\/aws.amazon.com\/redshift\/features\/redshift-ml\/\n  [2]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\n  [3]: https:\/\/aws.amazon.com\/marketplace\/pp\/Twinword-Inc-Sentiment-Analysis-Inference-Model\/prodview-wa74yx5z5qsvw",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Debugger and forecasting",
        "Question_created_time":1607461696000,
        "Question_last_edit_time":1668106746101,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhc2VF00VTGy1VonBHiyd9Q\/sagemaker-debugger-and-forecasting",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_body":"How to detect overfitting with Amazon Forecast. Does SageMaker Debugger work with Amazon Forecast?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1607490932000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1611775286080,
        "Answer_comment_count":0.0,
        "Answer_body":"Answer #1:  Based on the same technology used for time-series forecasting at Amazon.com, Forecast provides state-of-the-art algorithms to predict future time-series data based on historical data, and requires no machine learning experience. Amazon SageMaker Debugger profiles and debugs your training jobs to improve the performance of machine learning models on compute resource utilization and model predictions. So Debugger is available for your SageMaker training jobs but not for Amazon Forecast which is a fully managed service of its own.\n\nAnswer #2 - \nThe way to detect overfitting with forecasting is using \"backtests\", which are the \"train\/valid\" equivalent of traditional machine learning.  The reason Backtesting is used in forecasting is to keep the time order of the train\/valid data.  With careful analysis of backtest windows, you can have more confidence the model will generalize well with unseen data.\n\nAmazon Forecast now supports export of backtest forecasts as .csv files.  Customer can take the backtest forecasts and calculate whatever metric they want and\/or visualize.\n\n - Link to documentation:  https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/metrics.html\n - Link to blog and notebook about using Backtests in Amazon Forecast:  https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-forecast-now-supports-accuracy-measurements-for-individual-items\/\n\nOne common fear is backtesting will increase overfitting - it does not since each validation dataset is different from the previous backtest validation dataset.  To have more certainty about robustness of the model, use more than 1 backtest window.  If the Forecast Horizon is long, you may get by with just 1-2 backtest windows; otherwise use the maximum which is 5 backtest windows.  \n\nAnother concern is the increased training time since each backtest window is in fact another train\/forecast iteration.  It's a trade-off, if you want more model validation or not.\n\nIdeally, the wQL's of each backtest window are similar.  If you see all backtest forecasts with similar metrics (except maybe a few windows where underlying data had anomalies), that is good sign your model is not overfit.  \n\nBoth under-fitting and over-fitting for now are left to human judgement to examine backtest forecasts and decide whether model is acceptable or not.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"[AI\/ML] Data acquisition and preprocessing",
        "Question_created_time":1607357917000,
        "Question_last_edit_time":1667926383753,
        "Question_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":85,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data.\nThe e-bike sensor data are size about 4KB files each and posted in S3 buckets.\nThe sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ\ntimestamp2, sensorA, sensorB, sensorC, ..., sensorZ\ntimestamp3, sensorA, sensorB, sensorC, ..., sensorZ\n...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\n* Read S3 objects\n* Parse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\n* Put the data in DynamoDB with bike ID as primary key and timestamp as sort key.\n* Use SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\n* If we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\n* Also, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\n* Make inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently?\nWould you rather learn from S3 directly via Athena or direct S3 access?\nOr would you rather use Glue and Redshift?\nBut the data about 100MB would be sufficient to train the model we have in mind.\nGlue and Redshift maybe overkill.\nCurrently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1607362919000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925563027,
        "Answer_comment_count":0.0,
        "Answer_body":"**Thoughts about DynamoDB**\n\nPer GB, DynamoDB is around 5X more cost per GB of data stored. On top of that, you have RCU\/WCU cost.\n\nI would recommend keeping data in S3. Not only is it more cost effective, but with S3, you do not have to worry about RCU\/WCU cost or throughput of DynamoDB. \n\nSageMaker notebooks and training instances can read directly from S3, and S3 has high-throughput. I don't think you will have a problem with 100 MB datasets. \n\nIf you need to prep\/transform your data, you can do the transformations \"in place\" in S3 using Glue, Athena, Glue DataBrew, GlueStudio, etc. \n\n\n**Glue and DynamoDB**\n\n> I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support.\n\nGlue supports both Python and Spark jobs. If you use a Glue Python job, you can import the boto3 (AWS SDK) library and write to DynamoDB.\n\n**Other strategies**\n\nHow is your customer ingesting the sensor data \/ how is it being written to S3? Are they using AWS IoT Core? \n\nRegardless, the pattern you've described thus far is:\n\nDevice -> Sensor data in S3 -> Transform with Lambda -> store data in DynamoDB\n\nAn alternative approach you could consider is using Kinesis Firehose with Lambda transformations. This will allow you to do \"in-line\" parsing \/ transformation of your data before it is ever written to S3, this removing the need to re-read the data from S3 and apply transformations after the fact. Firehose also allows you to write the stored data in formats such as Parquet, which can help with cost and subsequent query performance. \n\nIf you want to store both raw data and transformed data, you can use a \"fanout\" pattern with Kinesis Streams\/Firehose, where one output is raw data to S3 and the other is a transformed stream.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Create or update Sagemaker Endpoint via CloudFormation",
        "Question_created_time":1607356793000,
        "Question_last_edit_time":1668615829186,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXiLSnlxkQHKzQVFj6GKT7w\/create-or-update-sagemaker-endpoint-via-cloudformation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":437,
        "Question_answer_count":1,
        "Question_body":"A wants to manage Sagemaker resources (such as models and endpoints) via CloudFormation. As part of their model deployment pipeline, they'd like to be able to create or update existing Sagemaker Endpoint with new model data. Customers wants to re-use the same endpoint name for a given workload. \n\n**Question:**\n\nHow to express in CF a following logic:\n1. If Sagemaker endpoint with name \"XYZ\" doesn't exist in customer account, then create a new endpoint;\n2. If Sagemaker endpoint with name \"XYZ\" already exist, then update existing endpoint with new model data.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1607357193000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925574694,
        "Answer_comment_count":0.0,
        "Answer_body":"This functionality of \"UPSERT\" type does not exist in CFn natively. You would need to use a Custom Resource to handle this logic.\nOne alternative that is not exactly what you asked for but might be a decent compromise is to use a Parameter to supply the endpoint if it does exist. Then use a condition to check the value. If the paramter is blank then create an endpoint if not use the value supplied.\nI know this is not what you asked for but it allows you to avoid the custom resource solution.\n\nSample of similiar UPSERT example for a VPC:\n\n```\nParameters :\n\n  Vpc:\n    Type: AWS::EC2::VPC::Id\n\nConditions:\n\n  VpcNotSupplied: !Equals [!Ref Vpc, '']\n\nResources:\n\n  NewVpc:\n    Type: AWS::EC2::VPC\n    Condition: VpcNotSupplied\n    Properties:\n      CidrBlock: 10.0.0.0\/16\n\n  SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Sample\n      GroupName: Sample\n      VpcId: !If [VpcNotSupplied, !Ref NewVpc, !Ref Vpc ]\n```\n\nHere the `Vpc` input parameter can be supplied if the VPC you wish to use already exists, left blank if you want to create a new one. The NewVPC resource uses the `Condition` to only create if the supplied Vpc parameter value is blank. The Security group then uses the same condition to decide whetehr to use and existing Vpc or the newly created one.\n\nHope this makes sense.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Question_created_time":1606994100000,
        "Question_last_edit_time":1668621791290,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sagemaker-pipelines-and-sagemaker-step-function-sdk",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1344,
        "Question_answer_count":2,
        "Question_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe [official Pipelines notebook][1] is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from [Julien Simon][2] I see CICD capacities mentioned, where are those? any demos?\n\n\n  [1]: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\n  [2]: https:\/\/www.youtube.com\/watch?v=Hvz2GGU3Z8g",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1607015065000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1610011923648,
        "Answer_comment_count":0.0,
        "Answer_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios.\nHaven't tried it out yet though.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Question_created_time":1606945520000,
        "Question_last_edit_time":1668512453971,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sagemaker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1054,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1606993756000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1610011918846,
        "Answer_comment_count":0.0,
        "Answer_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the **launch terminal button** in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_created_time":1606707121000,
        "Question_last_edit_time":1668525106058,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":110,
        "Question_answer_count":1,
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1606709124000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925565130,
        "Answer_comment_count":0.0,
        "Answer_body":"Some useful points:\n\n- The **typical arguments of cloud vs local** will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\n- SageMaker notebooks already run in an explicit **IAM context** (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run `sagemaker.get_execution_role()`\n- **Pre-built environments** for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and **easy wiping\/reset** of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\n- Linux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\n- If you started using **SageMaker Studio**, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Receiving consistent AccessDenied errors",
        "Question_created_time":1606179025000,
        "Question_last_edit_time":1667930464776,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ3KwMxUsREup-lSqlA44Hg\/receiving-consistent-accessdenied-errors",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":123,
        "Question_answer_count":2,
        "Question_body":"I am trying to use SageMaker Notebook Instances, but consistently receive AccessDenied errors for commands that my IAM role should have access to (and for commands that worked the last time I tried several weeks ago).  For example:  \n  \n`aws s3 ls` results in `An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied` despite my role having the AmazonS3FullAccess policy attached.  \n  \nAlso `aws ecr describe-repositories --repository-names \"sagemaker-decision-trees\"` results in `An error occurred (AccessDeniedException) when calling the DescribeRepositories operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role\/AmazonSageMaker-ExecutionRole-20201123T151452\/SageMaker is not authorized to perform: ecr:DescribeRepositories on resource: arn:aws:ecr:us-east-2:XXXXXXXXXX:repository\/sagemaker-decision-trees with an explicit deny` despite my role having the AmazonEC2ContainerRegistryFullAccess policy attached.  \n  \nOne thing that seems new is that \"SageMaker\" is appended to my user ARN.  I can't remember seeing errors with this appended before.  \n  \nNote: I've replicated these errors with several combinations of configurations:  \n* a new IAM role (which I created in the SageMaker console to have AmazonSageMakerFullAccess to any S3 bucket)  \n* fresh notebook instance  \n* with (and without) a VPC  \nAlso, these commands all work when run outside of a notebook instance (i.e. when run locally from my laptop).  \n  \nI'm guessing there's some problem with my account setup, but not sure what to try next.  \nThanks.  \n  \nEdited by: DJAIndeed on Nov 24, 2020 8:35 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Question_created_time":1605283057000,
        "Question_last_edit_time":1668165980161,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sagemaker-sdk-with-fsx-as-data-source",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":272,
        "Question_answer_count":1,
        "Question_body":"What value should I set for the **directory_path** parameter in **FileSystemInput** for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n - My FSx ID is `fs-0684xxxxxxxxxxx`.\n - My FSx has the mount name `lhskdbmv`.\n - The FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\n**Attempt 1:**\n\n    fs = FileSystemInput(\n        file_system_id='fs-0684xxxxxxxxxxx',\n        file_system_type='FSxLustre',\n        directory_path='lhskdbmv',\n        file_system_access_mode='ro')\n\n**Result:**\n\n`estimator.fit(fs)` returns `ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.`\n\n**Attempt 2:**\n\n    fs = FileSystemInput(\n        file_system_id='fs-0684xxxxxxxxxxx',\n        file_system_type='FSxLustre',\n        directory_path='\/',\n        file_system_access_mode='ro')\n\n**Result:**\n\n`ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.`\n\n**Attempt 3:**\n\n    fs = FileSystemInput(\n        file_system_id='fs-0684xxxxxxxxxxx',\n        file_system_type='FSxLustre',\n        directory_path='fsx',\n        file_system_access_mode='ro')\n\n**Result:**\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1605296508000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925582735,
        "Answer_comment_count":0.0,
        "Answer_body":"The **directory_path** parameter must point to \/**mountname**\/path\/to\/specific\/folder\/in-file-system. The value of mountname is returned in the CreateFileSystem API operation response. It is also returned in the response of the describe-file-systems AWS Command Line Interface (AWS CLI) command and the DescribeFileSystems API operation. \n\nFor your use case,  the response might look similar to the following:\n```mountName = lhskdbmv```",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_created_time":1605279993000,
        "Question_last_edit_time":1668217529291,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sagemaker-training-on-amazon-fsx-for-lustre",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":195,
        "Question_answer_count":1,
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n* SageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\n* Amazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only.\nIs this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1605281179000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925559451,
        "Answer_comment_count":0.0,
        "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\n1. FSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\n2. FSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to  communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1].\n[1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to deploy N models on N Greengrass devices with a unique Lambda for inference logic?",
        "Question_created_time":1605018964000,
        "Question_last_edit_time":1667926501335,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlVJHC1NaTTOquvDqs444oQ\/how-to-deploy-n-models-on-n-greengrass-devices-with-a-unique-lambda-for-inference-logic",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nLet's consider an ML edge inference use-case on Greengrass-managed device. The model is unique to each device, however its architecture and invocation logic are the same for all devices. In other words, the same invocation Lambda could be the same for all devices, only the model parameters would need to change across devices. We'd like to deploy a unique inference Lambda to all devices, and load device-specific artifact to each device.\n\nCan this be achieved with Greengrass ML Inference? It seems that GG MLI requires each model to be associated with a specific Lambda. \n\nOtherwise, is the recommended pattern to self-manage the inference in Lambda? E.g. by loading a specific model from S3 unique a local config file or some env variable?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1605020138000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1613588986554,
        "Answer_comment_count":0.0,
        "Answer_body":"In IoT Greengrass 1.x, the configuration is unique to each Greengrass Group. This includes Connectors, Lambdas and ML Resources.\n\nThe same Lambda can be referenced by multiple groups as a Greengrass function, which is likely what you want. This is similar to using one of the GG ML connectors (Object Detection or Image Classification).\n\nIn addition to your inference code, you'll also need to configure an ML Resource, which has a local name and a remote model. The local name would be the same for all Greengrass Groups, but in each group you will refer to a different remote object (the model) - either S3 or SageMaker job.\n\nEvery time a model changes, you will need to redeploy the corresponding Greengrass group for the changes to be deployed locally.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Relevancy of SageMaker Model Monitor for NLP?",
        "Question_created_time":1605005518000,
        "Question_last_edit_time":1668439946133,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxCKLg-eiQ1mwZvzFyczBEg\/relevancy-of-sagemaker-model-monitor-for-nlp",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":103,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nCan SageMaker Model Monitor be applied in NLP models? Is it necessary to do some preprocessing of the data? How can we use SageMaker Model Monitor?  sentence length, unseen words, language etc. Any thoughts or experience on that?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1605007442000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1611802245038,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, You can use model monitor for data capture and scheduling in your own custom container with the relevant monitoring for NLP use case.  \nFor example, there's a blog post for model monitor for computer vision classification prediction with defined *alert * of predict more than expected.  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i\/?nc1=b_rp",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Question_created_time":1604678225000,
        "Question_last_edit_time":1668069762163,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sagemaker-training-with-fsx-what-is-directory-path",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":175,
        "Question_answer_count":1,
        "Question_body":"SageMaker can train on FSx data. One [SageMaker SDK parameter for FSx training][1] is `directory_path`. Where do we find that?\n\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1604679222000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1614191724717,
        "Answer_comment_count":0.0,
        "Answer_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs:\ndirectory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system\n\/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Why does my kernal keep dying when I try to import Hugging Face BERT models to Amazon SageMaker?",
        "Question_created_time":1604517955000,
        "Question_last_edit_time":1668595385427,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sagemaker",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1148,
        "Question_answer_count":2,
        "Question_body":"When I try to import Hugging Face BERT models to the **conda_pytorch_p36** kernal of my Amazon SageMaker Notebook instance using the following pip command, the kernal always dies:\n```\n! pip install transformers\n```\nThe result is the same for Hugging Face BERT, RoBERTa, and GPT2 models on ml.c5.2xlarge and ml.c5d.4xlarge Amazon SageMaker instances.\n\nWhy is this happening, and how do I resolve the issue?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1604527535000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925552520,
        "Answer_comment_count":0.0,
        "Answer_body":"This issue occurs when the latest sentence piece breaks. The workaround is to force install sentencepiece==0.1.91. \n\n```\npip install sentencepiece==0.1.91\n\n```",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Running a request against all variants in an endpoint",
        "Question_created_time":1604486652000,
        "Question_last_edit_time":1667925743687,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":40,
        "Question_answer_count":1,
        "Question_body":"I have a customer asking me about the [Rendezvous architecture](https:\/\/towardsdatascience.com\/rendezvous-architecture-for-data-science-in-production-79c4d48f12b). What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\n- Lambda (and probably SQS) around the endpoint;\n- A custom monitoring job;\n- Step Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1604506964000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1607685345047,
        "Answer_comment_count":0.0,
        "Answer_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the `invoke_endpoint` method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Question_created_time":1604477936000,
        "Question_last_edit_time":1667926687134,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sagemaker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":78,
        "Question_answer_count":1,
        "Question_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that [in EC2 it's possible][1], however I don't see it mentioned in Amazon SageMaker documentation.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/elastic-inference\/latest\/developerguide\/basics.html",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1604506639000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612946445792,
        "Answer_comment_count":0.0,
        "Answer_body":"No, they cant be; multi-attach is only supported with EC2.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_created_time":1603454458000,
        "Question_last_edit_time":1668589944841,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":966,
        "Question_answer_count":1,
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1603455348000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925584228,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\n**Note:** This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"EnforceInstanceType\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"sagemaker:CreateTrainingJob\",\n                    \"sagemaker:CreateHyperParameterTuningJob\"\n                ],\n                \"Resource\": \"*\",\n                \"Condition\": {\n                    \"ForAllValues:StringLike\": {\n                        \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                    }\n                }\n            }\n    \n         ]\n    }",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Question_created_time":1603285551000,
        "Question_last_edit_time":1667926579047,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sagemaker-sizing-and-cost",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price *comparison* than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1603286522000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925560089,
        "Answer_comment_count":1.0,
        "Answer_body":"See the **performance efficiency** and **cost optimization** pillars in [Machine Learning Lens][1].\nAdditionally this is an [EC2 based right sizing best practices guide][2].  \nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.  \nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.\n\n\n  [1]: https:\/\/d1.awsstatic.com\/whitepapers\/architecture\/wellarchitected-Machine-Learning-Lens.pdf\n  [2]: https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/cost-optimization-right-sizing\/tips-for-right-sizing-your-workloads.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_created_time":1602770746000,
        "Question_last_edit_time":1668615616914,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sagemaker-spark-processing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":188,
        "Question_answer_count":1,
        "Question_body":"SageMaker Processing can launch [multi-instance jobs][1]. What is the underlying [cluster manager][2]? Yarn? Mesos? Something custom?\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/machine-learning\/running-on-demand-serverless-apache-spark-data-processing-jobs-using-amazon-sagemaker-managed-spark-containers-and-the-amazon-sagemaker-sdk\/\n  [2]: https:\/\/spark.apache.org\/docs\/latest\/cluster-overview.html",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1602771143000,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1609869333654,
        "Answer_comment_count":0.0,
        "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the [Dockerfile](https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/spark\/processing\/2.4\/py3\/docker\/Dockerfile.cpu) with hadoop-yarn dependencies",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Question_created_time":1602169065000,
        "Question_last_edit_time":1668628484238,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sagemaker-endpoint-url-authentication-authorization",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":750,
        "Question_answer_count":1,
        "Question_body":"Need validation:\n\n+ Once the SageMaker endpoint is deployed. It can be invoked with the *Sagemaker Runtime API* InvokeEndpoint *OR* it can be invoked using the endpoint URL+HTTP AZ headers (below). \n\n+ Successful deployment also exposes a URL (on the console) that has the format: \n\n*https:\/\/**runtime.sagemaker.us-east-1.amazonaws.com**\/endpoints\/ENDPOINT-NAME\/invocations*\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique  endpoint name!!  THAT IS  to invoke this URL it needs to have the \"HTTP Authorization headers\"   (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. *(Test with HTTP\/AZ headers pending)*",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1602171513000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925564052,
        "Answer_comment_count":0.0,
        "Answer_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Tracking the lineage between Amazon SageMaker endpoint model and Model Monitor captured data",
        "Question_created_time":1602088286000,
        "Question_last_edit_time":1668410094244,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPWBH_xFoS4aq5i41Za5qaQ\/tracking-the-lineage-between-amazon-sagemaker-endpoint-model-and-model-monitor-captured-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":173,
        "Question_answer_count":2,
        "Question_body":"I have an Amazon SageMaker endpoint with A1, a model with data capture activated, and I want to update the endpoint with A2, a new model.\n\nHow do I track the Model Monitor Data Capture that captured data in Amazon S3, and identify which data referred to model A1 and which data referred to model A2?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1602144072000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925572435,
        "Answer_comment_count":0.0,
        "Answer_body":"using boto3:    \nwhen you update the model endpoint, you need to create a new `EndpointConfig` where you specify a new `s3 uri` where data capture will be stored and thats how you can see different data captures from different versions of the model.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Question_created_time":1601671292000,
        "Question_last_edit_time":1668529103319,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sagemaker-autopilot-for-my-time-series-modeling",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":319,
        "Question_answer_count":1,
        "Question_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case.\nI understand that Amazon SageMaker Autopilot doesn't work with time series.\nIs there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1601883088000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925587778,
        "Answer_comment_count":0.0,
        "Answer_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see [Automate model development with Amazon SageMaker Autopilot][1].\n\n[1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-automate-model-development.html\n\nFor your use case, you can use [Amazon Forecast][2]. Amazon Forecast includes the [AutoML][3] feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.\n\n[2]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/what-is-forecast.html\n[3]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/automl.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Using AWS services to perform pet face recognition",
        "Question_created_time":1601645649000,
        "Question_last_edit_time":1668433979700,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2S-z85hhSr-XVecDxE3ihw\/using-aws-services-to-perform-pet-face-recognition",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":364,
        "Question_answer_count":1,
        "Question_body":"I want to build a camera that automatically recognizes a specific pet with image or video recognition. What AWS service can I use to identify an individual pet (not just the pet type). I've tried to use AWS Rekognition, but it can only differentiate between animal types, race, or color. Amazon SageMaker could be another option to create a completely new mode, but is very costly. What other AWS services can I use to identify specific pets?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1601648944000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925582225,
        "Answer_comment_count":0.0,
        "Answer_body":"You can use Amazon Rekognition Custom Labels to use single class object detection to identify or classify a specific animal. However, note that Amazon Rekognition Custom Labels do not perform animal face recognition. It only classifies the image or object.\n\nFor example, you can train your detection model to identify an animal based on the images you provide for that label. For more information about using Amazon Rekognition Custom Labels, see this blog: https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-a-custom-single-class-object-detection-model-with-amazon-rekognition-custom-labels\/.\n\nTo use frames from a video with Custom Labels, see[ Video analysis ](https:\/\/docs.aws.amazon.com\/rekognition\/latest\/customlabels-dg\/ex-video-extraction.html).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How do you analyze Autopilot results in Amazon SageMaker Studio?",
        "Question_created_time":1601332827000,
        "Question_last_edit_time":1667925765776,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7p9B6zcpSIeTG4MbzrSjKA\/how-do-you-analyze-autopilot-results-in-amazon-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":91,
        "Question_answer_count":1,
        "Question_body":"I launched an Autopilot job in SageMaker Studio, and now I'm trying to figure out how to compare autoML iterations. Is there a way to list them, see their metrics, and see the configuration of the best job?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1601339581000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925562588,
        "Answer_comment_count":0.0,
        "Answer_body":"Watch the [Choose and deploy the best model](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-videos.html#autopilot-video-choose-and-deploy-the-best-model) video tutorial in the SageMaker developer guide. The video shows how to use SageMaker Autopilot to visualize and compare model metrics.\n\nFor more SageMaker Autopilot tutorials, see [Videos: Use Autopilot to automate and explore the machine learning process](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-videos.html).",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is it possible to test locally SageMaker Inference Pipelines?",
        "Question_created_time":1600158011000,
        "Question_last_edit_time":1668454837874,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8R_MjbU1QPm66SCgld4spQ\/is-it-possible-to-test-locally-sagemaker-inference-pipelines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":340,
        "Question_answer_count":1,
        "Question_body":"Is it possible to test locally SageMaker Inference Pipelines? I would like to be able to easily troubleshoot and find the appropriate serialization between containers",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1601037561000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1609868433251,
        "Answer_comment_count":0.0,
        "Answer_body":"If you are referring to using local mode via the SM PySDK, then pipeline deployment is not supported.\n\nAs an alternative, given your three inference containers, you could manually run the services locally and then implement a kind of facade function that invokes the three services in pipeline and manages input\/output accordingly.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Question_created_time":1599771185000,
        "Question_last_edit_time":1668594259997,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":410,
        "Question_answer_count":1,
        "Question_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks.  I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row.  The result is that I'm not able to use the \"Join source\" feature with \"Input  - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1599791910000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1607690229838,
        "Answer_comment_count":0.0,
        "Answer_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using `accept=\"text\/csv\"` and `assemble_with=\"Line\"` on your `Transformer`? Is your `strategy` set to `SingleRecord` or `MultiRecord`?\n\nAnd `split_type=\"Line\"`, `content_type=\"text\/csv\"` on the `.transform()` call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to `SingleRecord` could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Automated streaming integration and multiple requests for SageMaker endpoint",
        "Question_created_time":1599656695000,
        "Question_last_edit_time":1668524155875,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6E1eYARES123bRYAe2B0Ag\/automated-streaming-integration-and-multiple-requests-for-sagemaker-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":216,
        "Question_answer_count":1,
        "Question_body":"A data scientist  is looking to host a Tensorflow model in SageMaker and process low volume streaming event data (~2-3 per second) to collect inferences about each event. Data scientist is looking at having the SageMaker inference model plugged in as a Kinesis Data Analytics Application but Kinesis Data Analytics currently only supports SQL or Flink.\n\nOne option to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message, but  if there is a more automated and optimal solution available for these kind of workflows.\n\nIt is not possible to pass multiple requests currently to a SageMaker endpoint, yet Tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. Ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an S3 based batch approach is probably too slow.\n\nIs there anything you can recommend for handling this sort of workload?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1600149573000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1611341005825,
        "Answer_comment_count":0.0,
        "Answer_body":"To build integration between SageMaker endpoints and Kinesis Data Application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis-sagemaker-flink\/. It help  to setup serverless service to invoke the SageMaker inference endpoint.\n\nTo  use batching. The Tensorflow documentation mentions the following:\n\n - [This link][1] mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your Endpoint.\n - [This link][2] mentions that you can configure SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference\n\nYou would still have to handle the logic internally in ECS\/Lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the SageMaker endpoint end based on the above.\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html#making-predictions-against-a-sagemaker-endpoint\n  [2]: https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container#enabling-batching",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":1.0
    },
    {
        "Question_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Question_created_time":1598912648000,
        "Question_last_edit_time":1668620734992,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/modulenotfounderror-when-starting-a-training-job-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":833,
        "Question_answer_count":1,
        "Question_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get `ModuleNotFoundError: No module named 'nltk'`\n\nMy code is\n\n    import sagemaker  \n    from sagemaker.pytorch import PyTorch\n\n    JOB_PREFIX   = 'pyt-ic'\n    FRAMEWORK_VERSION = '1.3.1'\n\n    estimator = PyTorch(entry_point='finetune-T5.py',\n                       source_dir='..\/src',\n                       train_instance_type='ml.p2.xlarge' ,\n                       train_instance_count=1,\n                       role=sagemaker.get_execution_role(),\n                       framework_version=FRAMEWORK_VERSION, \n                       debugger_hook_config=False,  \n                       py_version='py3',\n                       base_job_name=JOB_PREFIX)\n\n    estimator.fit()\n\n\n\n\n`finetune-T5.py` have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1598914035000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925593038,
        "Answer_comment_count":0.0,
        "Answer_body":"Check out this [link][1] (Using third-party libraries section) on how to install third-party libraries for training jobs.  You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.\n\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#using-third-party-libraries",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AWS SageMaker Studio pricing. How does the billing work?",
        "Question_created_time":1598441345000,
        "Question_last_edit_time":1668628399524,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp_yTquDhTgiweq85PA1oWg\/aws-sagemaker-studio-pricing-how-does-the-billing-work",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2543,
        "Question_answer_count":2,
        "Question_body":"Hello,  \n  \nI am currently considering using SageMaker Studio (moved from plain SageMaker Notebook Instances to try the functionality of the Studio) and am currently trying to figure out how the billing part works.   \n  \nBasically my questions are:  \n1. Do we start to pay after Notebook is initialized (in Studio) or when we initialize Notebook and Kernel (e.g. Python3) is performing tasks?  \n2. Or is the billing based upon \"Running Apps\" (if that's the case, could you please describe nuances; in particular I am interested in \"default\" App and whether we pay for it or not)  \n3. If not like in questions 1. and 2., then how?  \n  \nI would appreciate any help in this matter :)",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker python sdk installation troubles",
        "Question_created_time":1597398744000,
        "Question_last_edit_time":1668607875979,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZe9xubsHTuyRPCUGXvi40Q\/sagemaker-python-sdk-installation-troubles",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":290,
        "Question_answer_count":2,
        "Question_body":"Hi,  \n  \nI've tried following a tutorial to use AWS sagemaker in script mode from my local linux VM, but I can't even get the basics working.  \n  \nSteps:  \n- \u279c  ~> python3 --version  \nPython 3.6.9  \n  \n- \u279c  ~> pip3 install sagemaker  \n...  \nSuccessfully installed boto3-1.14.42 botocore-1.17.42 importlib-metadata-1.7.0 packaging-20.4 protobuf3-to  \n-dict-0.1.5 s3transfer-0.3.3 sagemaker-2.3.0 smdebug-rulesconfig-0.1.4 zipp-3.1.0  \n  \n\u279c  ~> cat sagemaker.py  \nimport sagemaker  \nimport boto3  \n  \nsess = sagemaker.Session()  \n  \n\u279c  ~> python3 sagemaker.py  \nTraceback (most recent call last):  \n  File \"sagemaker.py\", line 1, in <module>  \n    import sagemaker  \n  File \"~\/sagemaker.py\", line 4, in <module>  \n    sess = sagemaker.Session()  \nAttributeError: module 'sagemaker' has no attribute 'Session'  \n  \nI also have the aws cli (version 2) installed, and configured using IAM credentials that have full rights, so that's not related.  \n  \nWhat is the problem with my python sdk install? TIA",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1597425118000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1597425118000,
        "Answer_comment_count":0.0,
        "Answer_body":"You should name your script something else than sagemaker.py, since python will look in the current directory first for a module when doing an import, so the import sagemaker will not import the Sagemaker SDK, but your script.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Question_created_time":1597251203000,
        "Question_last_edit_time":1668607243322,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sagemaker-algorithms-can-only-use-gpu-for-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":275,
        "Question_answer_count":1,
        "Question_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can *only* be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both? ",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":true,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1597251737000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612350091845,
        "Answer_comment_count":0.0,
        "Answer_body":"Documentation for [Amazon SageMaker built-in algorithms][1]  provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take [Image Classification][2] as an example. Here is a excerpt from online documentation:\n\n> For image classification, we support the following GPU instances for\n> training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge,\n> ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with\n> more memory for training with large batch sizes. However, both CPU\n> (such as C4) and GPU (such as P2 and P3) instances can be used for the\n> inference. You can also run the algorithm on multi-GPU and\n> multi-machine settings for distributed training.\n\nFor more complex scenarios, such as [Script or BYO Container][3] modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\n  [2]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\n  [3]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker AutoPilot Regions",
        "Question_created_time":1596620138000,
        "Question_last_edit_time":1667926286293,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_7jk19ozQSeyjTAR_D_hEA\/sagemaker-autopilot-regions",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":89,
        "Question_answer_count":1,
        "Question_body":"Is there official documentation showing the **regions** in which **SageMaker AutoPilot** is supported? From my understanding, it should work with the SDK wherever SageMaker is supported, while in the no-code mode only where SageMaker Studio is available. Is this true?  \n\nThanks!",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1596635055000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612483656482,
        "Answer_comment_count":0.0,
        "Answer_body":"SageMaker Autopilot works in all the regions where Amazon SageMaker is available today as noted in this blog post \"[Amazon SageMaker Autopilot \u2013 Automatically Create High-Quality Machine Learning Models With Full Control And Visibility][1]\".  In addition, Autopilot is also integrated with Amazon SageMaker Studio, which is available in us-east-1, us-east-2, us-west-2 and eu-west-1. For a current list of available regions, please check the [AWS Regional Services List](https:\/\/aws.amazon.com\/about-aws\/global-infrastructure\/regional-product-services\/).\n\n\n  [1]: https:\/\/aws.amazon.com\/fr\/blogs\/aws\/amazon-sagemaker-autopilot-fully-managed-automatic-machine-learning\/",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_created_time":1596105364000,
        "Question_last_edit_time":1668622375212,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqMU2EBqGTDCMTPsB5rjNoQ\/has-sas-code-ever-been-successfully-ran-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":235,
        "Question_answer_count":1,
        "Question_body":"Has SAS code ever been successfully ran on SageMaker?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1596117765000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925587744,
        "Answer_comment_count":0.0,
        "Answer_body":"I\u2019ve helped customers run SAS on a notebook through a kernel and that was their main use case, but we also showed them how they can containerize SAS. Worked well",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Question_created_time":1596055479000,
        "Question_last_edit_time":1668591869336,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sagemaker-groundtruth-interface-option-to-skip-a-task-and-then-return",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":139,
        "Question_answer_count":1,
        "Question_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1596058119000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612484096993,
        "Answer_comment_count":0.0,
        "Answer_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n `[ ] this task was skipped` \n\nwhere the annotator could check the box for those items to be reviewed and processed at another time. ",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Question_created_time":1596036787000,
        "Question_last_edit_time":1667925777990,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sagemaker-autopilot-doing-when-in-state-inprogress-analyzingdata",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":50,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1597886708000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1609768237541,
        "Answer_comment_count":0.0,
        "Answer_body":"There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML.  There's a deep explanation what is does in our Science page https:\/\/www.amazon.science\/publications\/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Does Ground Truth Support Circles?",
        "Question_created_time":1595625000000,
        "Question_last_edit_time":1667926586932,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqVY0A3PIQsuJpcce1tjJcA\/does-ground-truth-support-circles",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":45,
        "Question_answer_count":1,
        "Question_body":"Customer has circular objects in their data. Does Ground Truth support drawing circles rather than boxes out of the box (no pun intended)? I know that it supports semantic segmentation, but that is overkill in this case.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1595626679000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1598586532461,
        "Answer_comment_count":0.0,
        "Answer_body":"As of July 2020, we currently have Crowd HTML Element support for [bounding box][1] and [polygons][2]. \n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-bounding-box.html\n  [2]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-polygon.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AutoPilot for Forecasting",
        "Question_created_time":1595154416000,
        "Question_last_edit_time":1668022225598,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu_IodOxiQ6eTu010AYb8pQ\/autopilot-for-forecasting",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":112,
        "Question_answer_count":1,
        "Question_body":"Hi there,\n\nIHAC who asked a question regarding the possible use of AutoPilot for solving Forecasting problems. They don't have the knowledge to play with DeepAR and they are running tests in parallel with Amazon Forecast. Their questions are:\n\n1. Is it possible to use AutoPilot for Forecasting problems? (my answer would be yes, since regression problems can be solved by XGBoost, which also won a bunch of competitions on Forecasting)\n2. Which kind of pre-processing should the customer do and which pre-processing is done by AutoPilot which could simplify transformation of data for solving forecasting? In particular: are there any transformation to be done on the timestamp column? Should we introduce lagged entries - or is it done by AutoPilot?\n\n\nThanks to those taking the time to answer these questions :) \n\nBest,\nDavide Gallitelli",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1595233470000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1609941033772,
        "Answer_comment_count":0.0,
        "Answer_body":"Although it is possible to model the forecasting as a regression problem in  Autopilot, there is no time-series capability built in Autopilot. So, you need to do the  preprocessing tasks such as time-series windowing, lag differencing, etc.  in order to generate the training\/test datasets  for the autopilot experiment.\n\nAdditionally, time series forecasting usually requires a model which can detect the pattern in a sequence of features. So, services such as DeepAR or Amazon forecast provide better capabilities to address this challenge.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Did the SageMaker PyTorch deployment process change?",
        "Question_created_time":1594979513000,
        "Question_last_edit_time":1668031176621,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFIru4hJ2TcWLi7CYt3mnuw\/did-the-sagemaker-pytorch-deployment-process-change",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":319,
        "Question_answer_count":1,
        "Question_body":"Did the SageMaker PyTorch deployment process change?\n\nIt use to be the case that people needed to have a model.tar.gz in s3, and an inference script locally or in git. Now, it seems that the inference script must also be part of the model.tar.gz. This is new, right?\n\nFrom the docs,  https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#for-versions-1-2-and-higher:\n\n*For PyTorch versions 1.2 and higher, the contents of model.tar.gz should be organized as follows:\n - Model files in the top-level directory\n - Inference script (and any other source files) in a directory named code\/ (for more about the inference script, see The SageMaker PyTorch Model Server)\n - Optional requirements file located at code\/requirements.txt (for more about requirements files, see Using third-party libraries)*\n\nThis may be confusing, because this new mode of deployment means that people creating the model artifact need to know in advanced how the inference is going to look like. The previous design, with separation of artifact and inference code, was more agile.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1594981418000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612485400964,
        "Answer_comment_count":0.0,
        "Answer_body":"When [AWS Sample - BERT sample using torch 1.4](https:\/\/github.com\/aws-samples\/amazon-sagemaker-bert-classify-pytorch) was published, advance knowledge of the inference seems to be necessary.  If you use the PyTorch SageMaker SDK to create or deploy the model after it is trained, it automatically **re-packages** the model.tar.gz to include the code files and the inference files. As an example, when you use the following script, the model.tar.gz is repackaged so the contents of the src directory is automatically added to the code directory model.tar.gz, which initially only contains model files. You don't need to know the inference code in advance.\n\n```python\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\n\nmodel_uri = estimator.model_data\n\nmodel = PyTorchModel(model_data=model_uri,\n                     role=role,\n                     framework_version='1.4.0',\n                     entry_point='serve.py',\n                     source_dir='src')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n```\n\nFor the older versions, you couldn't include additional files \/dependencies during inference unless you built a custom container.  The source.tar.gz was only used during training.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Rekognition Custom Labels and Ground Truth integration question",
        "Question_created_time":1594736742000,
        "Question_last_edit_time":1667925671581,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH-j5YmdZTiSEvLnm1ghztw\/rekognition-custom-labels-and-ground-truth-integration-question",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":176,
        "Question_answer_count":1,
        "Question_body":"Hello!  \n  \nI've created a semantic segmentation (SS) job in Ground Truth (GT), and it shows all objects are labeled, and the status is Complete. The manifest file appears to be syntactically correct.   \n  \nWithin Custom Labels (CL), I've created a dataset by choosing \"Import images labeled by Ground Truth\" option and using the manifest file created in GT. It parses correctly. However, when I look at the available dataset in the console, I see that there are zero labeled images. Viewing the images also displays zero labels.  \n  \nI have done the same with bounding box (BB) jobs in GT, and they import into CL correctly.   \n  \nDo CL datasets not support import of SS GT manifest files? I can't find anything in the documentation that states either way.   \n  \nThanks for your help.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Failed ping healthcheck after deploying TF2.1 model with TF-serving-contain",
        "Question_created_time":1594297614000,
        "Question_last_edit_time":1668556893886,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT86tGF5tRB2hC168_n5MAQ\/failed-ping-healthcheck-after-deploying-tf2-1-model-with-tf-serving-contain",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":312,
        "Question_answer_count":1,
        "Question_body":"Hello,  \n  \nI would appreciate any input to the following issue.  \nWe want to deploy a trained Tensorflow Model to AWS Sagemaker for inference with a tensorflow-serving-container. Tensorflow version is 2.1. Following the guide at https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container the following steps have been taken:  \n1. Build TF 2.1 AMI and publish it to AWS ECR after sucessful local testing  \n2. Setting Sagemaker Execution Role Permissions for S3 and ECR.  \n3. Pack saved TF model folder (saved_model.pb, assets, variables) into model.tar.gz  \n4a. Created endpoint with realtime predictor:\n\n```\nimport os\r\nimport sagemaker\r\nfrom sagemaker.tensorflow.serving import Model\r\nfrom sagemaker.tensorflow.model import TensorFlowModel\r\nfrom sagemaker.predictor import json_deserializer, json_serializer, RealTimePredictor\r\nfrom sagemaker.content_types import CONTENT_TYPE_JSON\r\n\r\ndef create_tfs_sagemaker_model():\r\n    sagemaker_session = sagemaker.Session()\r\n    role = 'arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXX\r\n    bucket = 'XXXXXXX-tf-serving'\r\n    prefix = 'sagemaker\/tfs-test'\r\n    s3_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n    image = 'XXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-tensorflow-serving:2.1.0-cpu'\r\n    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))\r\n    endpoint_name = 'tf-serving-ep-test-1'\r\n    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, framework_version='2.1')\r\n    tensorflow_serving_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\r\n    rt_predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_JSON)\n```\n\n4b. Create batch-transform job:\n\n```\ndef create_tfs_sagemaker_batch_transform():\r\n    sagemaker_session = sagemaker.Session()\r\n    print(sagemaker_session.boto_region_name)\r\n    role = 'arn:aws:iam::XXXXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXXX'\r\n    bucket = 'XXXXXXX-tf-serving'\r\n    prefix = 'sagemaker\/tfs-test'\r\n    image = 'XXXXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-tensorflow-serving:2.1.0-cpu'\r\n    s3_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))\r\n    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, name='deep-net-0', framework_version='2.1')\r\n    print(tensorflow_serving_model.model_data)\r\n    out_path = 's3:\/\/XXXXXX-serving-out\/'\r\n    input_path = \"s3:\/\/XXXXXX-serving-in\/\"    \r\n    tensorflow_serving_transformer = tensorflow_serving_model.transformer(instance_count=1, instance_type='ml.c4.xlarge', accept='application\/json', output_path=out_path)\r\n    tensorflow_serving_transformer.transform(input_path, content_type='application\/json')\n```\n\nBoth steps 4a and 4b are running and in the AWS Cloudwatch logs we see successful starting of the instances, loading of the model and TF-Serving entering the event loop \u2013 see below:  \n  \n_2020-07-08T17:07:16.156_02:00+  \n_INFO:__main__:starting services_  \n_2020-07-08T17:07:16.156_02:00+  \n_INFO:__main__:nginx config:_  \n_2020-07-08T17:07:16.156_02:00+  \n_load_module modules\/ngx_http_js_module.so;_  \n_2020-07-08T17:07:16.156_02:00+  \n_worker_processes auto;_  \n_2020-07-08T17:07:16.156_02:00+  \n_daemon off;_  \n_2020-07-08T17:07:16.156_02:00+  \n_pid \/tmp\/nginx.pid;_  \n_2020-07-08T17:07:16.157_02:00+  \n_error_log \/dev\/stderr error;_  \n_2020-07-08T17:07:16.157_02:00+  \n_worker_rlimit_nofile 4096;_  \n_2020-07-08T17:07:16.157_02:00+  \n_events { worker_connections 2048;_  \n_2020-07-08T17:07:16.157_02:00+  \n_}_  \n_2020-07-08T17:07:16.162_02:00+  \n_http { include \/etc\/nginx\/mime.types; default_type application\/json; access_log \/dev\/stdout combined; js_include tensorflow-serving.js; upstream tfs_upstream { server localhost:10001; } upstream gunicorn_upstream { server unix:\/tmp\/gunicorn.sock fail_timeout=1; } server { listen 8080 deferred; client_max_body_size 0; client_body_buffer_size 100m; subrequest_output_buffer_size 100m; set $tfs_version 2.1; set $default_tfs_model None; location \/tfs { rewrite ^\/tfs\/(.*) \/$1 break; proxy_redirect off; proxy_pass_request_headers off; proxy_set_header Content-Type 'application\/json'; proxy_set_header Accept 'application\/json'; proxy_pass http:\/\/tfs_upstream; } location \/ping { js_content ping; } location \/invocations { js_content invocations; } location \/models { proxy_pass http:\/\/gunicorn_upstream\/models; } location \/ { return 404 '{\"error\": \"Not Found\"}'; } keepalive_timeout 3; }_  \n_2020-07-08T17:07:16.162_02:00+  \n_}_  \n_2020-07-08T17:07:16.162_02:00+  \n_INFO:tfs_utils:using default model name: mode_  \n_2020-07-08T17:07:16.162_02:00+  \n_INFO:tfs_utils:tensorflow serving model config:_  \n_2020-07-08T17:07:16.162_02:00+  \n_model_config_list: { config: { name: \"model\", base_path: \"\/opt\/ml\/model\", model_platform: \"tensorflow\" }_  \n_2020-07-08T17:07:16.162_02:00+  \n_}_  \n_2020-07-08T17:07:16.162_02:00+  \n_INFO:__main__:using default model name: model_  \n_2020-07-08T17:07:16.162_02:00+  \n_INFO:__main__:tensorflow serving model config:_  \n_2020-07-08T17:07:16.163_02:00+  \n_model_config_list: { config: { name: \"model\", base_path: \"\/opt\/ml\/model\", model_platform: \"tensorflow\" }_  \n_2020-07-08T17:07:16.163_02:00+  \n_}_  \n_2020-07-08T17:07:16.163_02:00+  \n_INFO:__main__:tensorflow version info:_  \n_2020-07-08T17:07:16.163_02:00+  \n_TensorFlow ModelServer: 2.1.0-rc1_dev.sha.075ffcf+  \n_2020-07-08T17:07:16.163_02:00+  \n_TensorFlow Library: 2.1.0_  \n_2020-07-08T17:07:16.163_02:00+  \n_INFO:__main__:tensorflow serving command: tensorflow_model_server --port=10000 --rest_api_port=10001 --model_config_file=\/sagemaker\/model-config.cfg --max_num_load_retries=0_  \n_2020-07-08T17:07:16.163_02:00+  \n_INFO:__main__:started tensorflow serving (pid: 13)_  \n_2020-07-08T17:07:16.163_02:00+  \n_INFO:__main__:nginx version info:_  \n_2020-07-08T17:07:16.163_02:00+  \n_nginx version: nginx\/1.18.0_  \n_2020-07-08T17:07:16.163_02:00+  \n_built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)_  \n_2020-07-08T17:07:16.163_02:00+  \n_built with OpenSSL 1.1.1 11 Sep 2018_  \n_2020-07-08T17:07:16.163_02:00+  \n_TLS SNI support enabled_  \n_2020-07-08T17:07:16.163_02:00+  \n_configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=\/data\/builder\/debuild\/nginx-1.18.0\/debian\/debuild-base\/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'_  \n_2020-07-08T17:07:16.163_02:00+  \n_INFO:__main__:started nginx (pid: 15)_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.075708: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models._  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.075760: I tensorflow_serving\/model_servers\/server_core.cc:573] (Re-)adding model: model_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.180755: I tensorflow_serving\/util\/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.180887: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.180919: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: model version: 1}_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.180944: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: model version: 1}_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.180995: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/1_  \n_2020-07-08T17:07:16.163_02:00+  \n_2020-07-08 15:07:15.205712: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }_  \n_2020-07-08T17:07:16.164_02:00+  \n_2020-07-08 15:07:15.205825: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:264] Reading SavedModel debug info (if present) from: \/opt\/ml\/model\/1_  \n_2020-07-08T17:07:16.164_02:00+  \n_2020-07-08 15:07:15.208599: I external\/org_tensorflow\/tensorflow\/core\/common_runtime\/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance._  \n_2020-07-08T17:07:16.164_02:00+  \n_2020-07-08 15:07:15.328057: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:203] Restoring SavedModel bundle._  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.578796: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:152] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/1_  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.626494: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:333] SavedModel load for tags { serve }; Status: success: OK. Took 1445495 microseconds._  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.630443: I tensorflow_serving\/servables\/tensorflow\/saved_model_warmup.cc:105] No warmup data file found at \/opt\/ml\/model\/1\/assets.extra\/tf_serving_warmup_requests_  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.632461: I tensorflow_serving\/util\/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0_  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.632484: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}_  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.634727: I tensorflow_serving\/model_servers\/server.cc:362] Running gRPC ModelServer at 0.0.0.0:10000 ..._  \n_2020-07-08T17:07:17.165_02:00+  \n_warn getaddrinfo: address family for nodename not supported_  \n_2020-07-08T17:07:17.165_02:00+  \n_2020-07-08 15:07:16.635747: I tensorflow_serving\/model_servers\/server.cc:382] Exporting HTTP\/REST API at:localhost:10001 ..._  \n_2020-07-08T17:07:17.165_02:00+  \n_evhttp_server.cc : 238 NET_LOG: Entering the event loop \u2026_  \n  \n  \nBut both (endpoint and batch transform) fail the Sagemaker Ping Health check with:  \n_2020-07-08T17:07:32.169_02:00+  \n_2020\/07\/08 15:07:31 error 16#16: *1 js: failed ping{ \"error\": \"Could not find any versions of model None\" }_  \n_2020-07-08T17:07:32.170_02:00+  \n_169.254.255.130 - -08\/Jul\/2020:15:07:31 _0000 \"GET \/ping HTTP\/1.1\" 502 157 \"-\" \"Go-http-client\/1.1\"+  \n  \nAlso, when tested locally with self build tf-serving-container the model is running without problems and can be queried with curl.   \nWhat could be the issue?   \nThank you for your help.  \nBest regards  \n  \nDominik  \n  \nEdited by: DominikPanther on Jul 9, 2020 5:27 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Question_created_time":1594231696000,
        "Question_last_edit_time":1668594120152,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1061,
        "Question_answer_count":1,
        "Question_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done.  Any links or pointers would be greatly appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1594232347000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1598510602165,
        "Answer_comment_count":0.0,
        "Answer_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\n- https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n- https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E:  https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here: \n\n- https:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Question_created_time":1594209626000,
        "Question_last_edit_time":1668530670747,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":186,
        "Question_answer_count":1,
        "Question_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1594216705000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612481183581,
        "Answer_comment_count":0.0,
        "Answer_body":"The value of the DLAMI (https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/what-is-dlami.html) is ease of use and saving time to get up to speed in a development environment.  If you are developing code for ML there is a huge variety of frameworks and software that you might need to install. The DLAMI includes the more popular ones, so you may quickly deploy a machine complete with common dependencies. This results in a reduction of the time needed for installing and configuring things. It speeds up experimentation and evaluation. If you want to try a new framework, it is already there.\n\nThe second reason is that AWS keeps the AMI up to date, so you may just deploy a new AMI periodically rather than having to patch.  Again, this saves you time and lets you concentrate on the underlying development and business activities.\n\nAll that said, for running in production and at volume you might want to use a different tool, I would imagine that for most cases creating docker images to your specific requirements would make a lot of sense. No need to go over the good and bad points of containers here.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Amazon SageMaker Built-in algorithms and Spot checkpointing",
        "Question_created_time":1593596016000,
        "Question_last_edit_time":1667926133912,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbWeXcwDT8i4dvXKE4HZXg\/amazon-sagemaker-built-in-algorithms-and-spot-checkpointing",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":56,
        "Question_answer_count":1,
        "Question_body":"Which Amazon SageMaker built-in algorithms support checkpointing? In the [documentation][1] it says that:\n\n> SageMaker built-in algorithms and marketplace algorithms that do not checkpoint are currently limited to a `MaxWaitTimeInSeconds` of 3600 seconds (60 minutes).\n\nHowever, in the algorithms I don't find any pointer to \"checkpoint\" or \"spot\". Can you help me out?\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-managed-spot-training.html",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1593611388000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1614311552175,
        "Answer_comment_count":0.0,
        "Answer_body":"This is the best resource that I've found to clarify this:\n\nhttps:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\n\n> Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification). As they tend to train on large data sets and run for longer than other algorithms, they have a higher likelihood of being interrupted. Other built-in algorithms do not support checkpointing for now.\n\nAlso:\n\n> Please note that TensorFlow uses checkpoints by default. For other frameworks, you\u2019ll find examples in our sample notebooks and in the documentation.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Question_created_time":1593595381000,
        "Question_last_edit_time":1668082995577,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sagemaker-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":248,
        "Question_answer_count":1,
        "Question_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1593617691000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925593260,
        "Answer_comment_count":0.0,
        "Answer_body":" You can use the [ModelClientConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ModelClientConfig.html#sagemaker-Type-ModelClientConfig-InvocationsMaxRetries) API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.\n",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Does Amazon SageMaker RL support heterogenous clusters?",
        "Question_created_time":1593179712000,
        "Question_last_edit_time":1667987084609,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE59_Oro0SGKaOIdZNmySiw\/does-amazon-sagemaker-rl-support-heterogenous-clusters",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":49,
        "Question_answer_count":1,
        "Question_body":"Does SageMaker RL support heterogenous clusters? I'd like to have our training to run on GPU and and SageMaker RL, and our inferences to run on CPUs. ",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1593376503000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925581600,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, Amazon SageMaker RL allows you to define training workers separately from inference workers. \n\nFor more information, see [Amazon SageMaker RL \u2013 Managed reinforcement learning with Amazon SageMaker](https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-rl-managed-reinforcement-learning-with-amazon-sagemaker\/) on the AWS News Blog. Also,  the [Build and Train Reinforcement Models with Amazon SageMaker RL](https:\/\/www.youtube.com\/watch?v=xGD6rkPuQ7Q) AWS online tech talk (see minute 26).",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Notebook Instance Types for SageMaker Studio",
        "Question_created_time":1593107198000,
        "Question_last_edit_time":1668530553629,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":864,
        "Question_answer_count":1,
        "Question_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1593108137000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925564585,
        "Answer_comment_count":0.0,
        "Answer_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Question_created_time":1592989945000,
        "Question_last_edit_time":1668624634692,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":881,
        "Question_answer_count":2,
        "Question_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1593009773000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925546486,
        "Answer_comment_count":0.0,
        "Answer_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time). \n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Question_created_time":1592823369000,
        "Question_last_edit_time":1668557190488,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1292,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI want to use `awswrangler` package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use **Lifecycle configuration**. I tried to do it using the following script:\n\n    #!\/bin\/bash\n    \n    pip install awswrangler==0.2.2\n\nBut when I import that package into my Notebook:\n\n    import boto3                                      # For executing native S3 APIs\n    import pandas as pd                               # For munging tabulara data\n    import numpy as np                                # For doing some calculation\n    import awswrangler as wr\n    import io\n    from io import StringIO\n\nI still get the following error:\n\n    ---------------------------------------------------------------------------\n    ModuleNotFoundError                       Traceback (most recent call last)\n    <ipython-input-1-f3d85c7dd0f6> in <module>()\n          2 import pandas as pd                               # For munging tabulara data\n          3 import numpy as np                                # For doing some calculation\n    ----> 4 import awswrangler as wr\n          5 import io\n          6 from io import StringIO\n    \n    ModuleNotFoundError: No module named 'awswrangler'\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1592832724000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925572268,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi,\n\n example how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_created_time":1592577511000,
        "Question_last_edit_time":1667926686451,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines. ",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1592579742000,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1667925550034,
        "Answer_comment_count":0.0,
        "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: \n[1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2]\n[2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow.  For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and \n[Using Amazon SageMaker to build a machine learning platform with just three engineers][4].\n[3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/\n[4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/\n",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Custom packages in Sagemaker studio",
        "Question_created_time":1592469976000,
        "Question_last_edit_time":1668596322741,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":638,
        "Question_answer_count":1,
        "Question_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio?  When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\n\nThanks!",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1592471900000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925551743,
        "Answer_comment_count":0.0,
        "Answer_body":"**Short answer:** [Studio UI] > File > New > Terminal > sudo yum install unzip  \nThen unzip away...\n\n**Long answer:**  \nYou can open a terminal in two different types of compute environment: \n\n 1. On a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\n 2. On the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal \n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.  \nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Model Spend",
        "Question_created_time":1592312639000,
        "Question_last_edit_time":1668454706295,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sagemaker-model-spend",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":151,
        "Question_answer_count":1,
        "Question_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1592313864000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612926624243,
        "Answer_comment_count":0.0,
        "Answer_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the [SageMaker Pricing page][1]. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check [this blog post][2]).\n\n\n  [1]: https:\/\/aws.amazon.com\/sagemaker\/pricing\/?nc1=h_ls\n  [2]: https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Provide Autopilot own data",
        "Question_created_time":1591879538000,
        "Question_last_edit_time":1667926394194,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfEXvtHQmR0egOGgOSG-b0A\/provide-autopilot-own-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_body":"Is there any way to provide your own (already labeled) data to sagemaker ground truth?  \nMy plan is to use sagemaker autopilot with my own data that I have already labeled such that the folder structure represents the labels (rekognition provides such functionality).   \nThe problem is that AutoPilot can only accept manifest files or csv,txt, ect. Well I have images with labels, so my only option is to use a manifest file. HOWEVER GroundTruth create manifest file option only accepts images from only 1 folder without any option to give it labels by default, therefore forcing me to repeat the labeling on an already labeled dataset.  \n  \nAny idea how to use already labeled data as input???",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Code running slow on Sagemaker notebook instance for the first time it runs",
        "Question_created_time":1591811233000,
        "Question_last_edit_time":1668603759497,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeQJN4BFCTsikAct_m9BeZw\/code-running-slow-on-sagemaker-notebook-instance-for-the-first-time-it-runs",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1434,
        "Question_answer_count":7,
        "Question_body":"Hello!   \n  \nI've an issue running the code on SageMaker. I am running my code on SageMaker, which runs my code slowly for the first time, but runs with proper speed, the second time around (I guess there's something getting stored in the cache). Few days back, it was running with the same speed all the time. Whatever I run, be it a model (The model which took just 5 minutes for one epoch when it worked fine estimates 3 hours of running time) \/ just a code reading the data present in my files, it runs too slow. What could be a possible solution for this? I tried changing the notebook instance types as well, but in vain. I've been struggling for 2 days. It'll be great if someone could help me out a bit soon so that I progress ahead in my project. Thanks in advance!  \n  \nEdited by: vbsrinivasan on Jun 10, 2020 10:47 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Glue + SageMaker Pip Packages",
        "Question_created_time":1591210245000,
        "Question_last_edit_time":1668452450079,
        "Question_link":"https:\/\/repost.aws\/questions\/QULN3fro-LQ1umpDN831VlZg\/glue-sagemaker-pip-packages",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":151,
        "Question_answer_count":1,
        "Question_body":"My customer is looking to use Glue dev endpoints along with a SageMaker notebook. What I've noticed is that in Glue, a package, in this case scipy, will be listed as 1.4.1, but this will or won't match what you get in a sagemaker notebook dependent on kernel. \n\nconda_python3:\n\n    !pip show scipy\n    Name: scipy\n    Version: 1.1.0\n    Summary: SciPy: Scientific Library for Python\n    Home-page: https:\/\/www.scipy.org\n    Author: None\n    Author-email: None\n    License: BSD\n    Location: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\n    Requires: \n    Required-by: seaborn, scikit-learn, sagemaker\n\nconda_tensorflow_p36:\n\n    !pip show scipy\n    Name: scipy\n    Version: 1.4.1\n    Summary: SciPy: Scientific Library for Python\n    Home-page: https:\/\/www.scipy.org\n    Author: None\n    Author-email: None\n    License: BSD\n    Location: \/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\n    Requires: numpy\n    Required-by: seaborn, scikit-learn, sagemaker, Keras\n\nIs there some sort of best practice to use a kernel that corresponds directly to what's installed on Glue?\n\nSeparate not very useful question. I wasn't able activate the venv that Jupyter notebooks do via shell. Is it using a venv? How come I can't find the right activate script?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1591215618000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925566053,
        "Answer_comment_count":0.0,
        "Answer_body":"conda_python3 and conda_tensorflow_p36 are local kernels on the SageMaker notebook instance while the Spark kernels execute remotely in the Glue Spark environment.\n\nHence you are seeing different versions. The Glue Spark environment comes with 1.4.1 version of scipy. So when you use the PySpark (python) or Spark (scala) kernels and you will get the 1.4.1 version of scipy.\n\nIf you use the default LifeCycle script that Glue SageMaker notebooks already come with, the connectivity to the Glue Dev endpoint should be in place. Note that the Glue SageMaker notebooks has a tag called 'aws-glue-dev-endpoint' that is used to identify which Glue Dev endpoint that particular notebook instance communicates with.\n\nThe Spark kernels cannot be replicated via the python shell. Those kernels relay Spark commands via the Livy service to Spark on the Glue Dev endpoint using a Jupyter module called Sparkmagic.\n\nRef: https:\/\/github.com\/jupyter-incubator\/sparkmagic",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SparkR not working",
        "Question_created_time":1591029652000,
        "Question_last_edit_time":1667926595812,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/sparkr-not-working",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":75,
        "Question_answer_count":1,
        "Question_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/  and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:  \n\n    An error was encountered:\n    [1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.\n",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1591041375000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925571654,
        "Answer_comment_count":0.0,
        "Answer_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done). \n\n`sparkR.session.stop()`\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Question_created_time":1591020062000,
        "Question_last_edit_time":1668561655230,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sagemaker-notebooks-on-glue-dev-endpoints",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":311,
        "Question_answer_count":1,
        "Question_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1591030326000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925564412,
        "Answer_comment_count":0.0,
        "Answer_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\n```\nspark.sparkContext.getConf().getAll()\n```\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook. \n\n```\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n```\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n```\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n```\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_created_time":1590501108000,
        "Question_last_edit_time":1668554201782,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sagemaker-multi-model-endpoint-support-sagemaker-model-monitor",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":248,
        "Question_answer_count":1,
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1590501508000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925566336,
        "Answer_comment_count":0.0,
        "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints .  https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Pipe Mode",
        "Question_created_time":1590161458000,
        "Question_last_edit_time":1668521878124,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sagemaker-pipe-mode",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":67,
        "Question_answer_count":1,
        "Question_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1590165887000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925574687,
        "Answer_comment_count":0.0,
        "Answer_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script. \n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time. \n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"confusion about PIPE mode when using S3 shard key",
        "Question_created_time":1589363811000,
        "Question_last_edit_time":1667925675585,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s3-shard-key",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using  distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1589407880000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925572408,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi,\nSageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify *ShardedByS3Key*. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question:\nHow much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"tensorboard with custom docker image without notebook",
        "Question_created_time":1588924703000,
        "Question_last_edit_time":1668600726314,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdIJGIVDEQSm-9eX3jo0ubA\/tensorboard-with-custom-docker-image-without-notebook",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":132,
        "Question_answer_count":2,
        "Question_body":"Hi,  \nIs it possible to use tensorboard with a custom docker image without using a notebook ? Is there any other method to monitor the training process ? I&#39;m using the tensorflow object detection API and currently exposing metrics (only loss) from cloudwatch using a regex but I&#39;d like a more detailed way like tensorboar.. is that possible ??  \nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Training a classifier on parquet with SageMaker ?",
        "Question_created_time":1588841008000,
        "Question_last_edit_time":1668588105089,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":424,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1588843302000,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1667925592850,
        "Answer_comment_count":0.0,
        "Answer_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example [notebook][1]).  \nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see [source][2]) \n\nAdditionally:  \n[Uber Petastorm][3] for reading parquet into Tensorflow, Pytorch, and PySpark inputs.   \nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.\n\n\n  [1]: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/caf9363c0242d0da2de7f5765e7318fd843ce4c3\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_parquet_input_training.ipynb\n  [2]: https:\/\/github.com\/aws\/sagemaker-xgboost-container\/blob\/5e778770e009ce989e288e7bbc1255556129e75b\/src\/sagemaker_xgboost_container\/data_utils.py#L40\n  [3]: https:\/\/github.com\/uber\/petastorm",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker with multiple models",
        "Question_created_time":1587366119000,
        "Question_last_edit_time":1668583474750,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sagemaker-with-multiple-models",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":703,
        "Question_answer_count":1,
        "Question_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\n> Can one SageMaker host more than one model? Each model then share the\n> same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\n 1. Using Multi-model Inference endpoints: \nAmazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found [here](1). The sample code can be found [here](2).  Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\n 2. Using Bring your own algorithm on SageMaker\nYou can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker [here](3)\n\n 3. Using Multi-model serving container by using multi-model archive file\n      You can find a sample example here [4] for tensorflow serving\n 4. If models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint\nSagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1593677528000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925593993,
        "Answer_comment_count":0.0,
        "Answer_body":"> Customer does not want to spin-up different containers for each model due to network adding additional latency. \n\nI am assuming this is a pipeline scenario where different models need to be chained.\nIf so, it's important to keep in mind that all containers in pipeline run on the __same EC2 instance__ so that \"inferences run with low latency because the containers are co-located on the same EC2 instances.\"[1]\n\nHope this is useful.   \n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Training Job Failed \"ClientError: Mask image is not a 8-bit\"",
        "Question_created_time":1587051280000,
        "Question_last_edit_time":1667926494375,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkxj6OqkTRFaImGZYRl-ISA\/training-job-failed-clienterror-mask-image-is-not-a-8-bit",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":119,
        "Question_answer_count":1,
        "Question_body":"Hello,  \n  \nI am working on a Semantic Segmentation training job.  I have used the Ground Truth semantic segmentation labeling GUI and have labeled about 900 images.  Everything with the Ground Truth Labeling job seemed to work just fine.  However I noticed the output images in the s3 output path are of type .png.    \n  \nI am now trying to build a training job using the Pipe - augmented manifest input format.  When I kick off the job, after a few minutes I see this error: **\"ClientError: Mask image is not a 8-bit single channel image. Please check the label image dataset\"**  \n  \nI downloaded the output images or \"masked images\" to photoshop and they do appear to be 8bit \"index color\".  \n  \n1) If I used the built in Semantic Segmentation - Ground Truth Labeling job, how would I end up with the incorrect output format?  \n2) I don't remember specifying or having the option of output image format.  Did I miss that somewhere?  \n  \nHere are a couple line items from the output.manifest file\n\n```\n{\"source-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/MyImage - 20190201122850904.png\",\"potato-features-validation-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/output\/potato-features-validation\/annotations\/consolidated-annotation\/output\/0_2020-04-14T02:38:26.487862.png\",\"potato-features-validation-ref-metadata\":{\"internal-color-map\":{\"0\":{\"class-name\":\"BACKGROUND\",\"hex-color\":\"#ffffff\",\"confidence\":0.79952},\"1\":{\"class-name\":\"pressure-bruise\",\"hex-color\":\"#2ca02c\",\"confidence\":0.79952},\"2\":{\"class-name\":\"potato\",\"hex-color\":\"#1f77b4\",\"confidence\":0.79952},\"3\":{\"class-name\":\"old-bruise\",\"hex-color\":\"#ff7f0e\",\"confidence\":0.79952},\"4\":{\"class-name\":\"black-skin\",\"hex-color\":\"#d62728\",\"confidence\":0.79952},\"5\":{\"class-name\":\"green\",\"hex-color\":\"#9467bd\",\"confidence\":0.79952},\"6\":{\"class-name\":\"scab\",\"hex-color\":\"#8c564b\",\"confidence\":0.79952}},\"type\":\"groundtruth\/semantic-segmentation\",\"human-annotated\":\"yes\",\"creation-date\":\"2020-04-14T02:38:26.563488\",\"job-name\":\"labeling-job\/potato-features-validation\"}}\r\n{\"source-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/MyImage - 20190201122852921.png\",\"potato-features-validation-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/output\/potato-features-validation\/annotations\/consolidated-annotation\/output\/1_2020-04-14T02:21:27.433381.png\",\"potato-features-validation-ref-metadata\":{\"internal-color-map\":{\"0\":{\"class-name\":\"BACKGROUND\",\"hex-color\":\"#ffffff\",\"confidence\":0.82409},\"1\":{\"class-name\":\"pressure-bruise\",\"hex-color\":\"#2ca02c\",\"confidence\":0.82409},\"2\":{\"class-name\":\"potato\",\"hex-color\":\"#1f77b4\",\"confidence\":0.82409},\"3\":{\"class-name\":\"old-bruise\",\"hex-color\":\"#ff7f0e\",\"confidence\":0.82409},\"4\":{\"class-name\":\"black-skin\",\"hex-color\":\"#d62728\",\"confidence\":0.82409},\"5\":{\"class-name\":\"green\",\"hex-color\":\"#9467bd\",\"confidence\":0.82409},\"6\":{\"class-name\":\"scab\",\"hex-color\":\"#8c564b\",\"confidence\":0.82409}},\"type\":\"groundtruth\/semantic-segmentation\",\"human-annotated\":\"yes\",\"creation-date\":\"2020-04-14T02:21:30.556262\",\"job-name\":\"labeling-job\/potato-features-validation\"}}\n```\n\nThanks  \n  \nEdited by: tetontech on Apr 16, 2020 7:07 PM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Model Monitor Missing Columns Constraint Violation",
        "Question_created_time":1586974280000,
        "Question_last_edit_time":1668600874335,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8Xkelo1ARA2zcn4rHuk09w\/sagemaker-model-monitor-missing-columns-constraint-violation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":365,
        "Question_answer_count":1,
        "Question_body":"I have an Endpoint inference pipeline model deployed from an AutoPilot training job. Now that this is successful, I want to add model monitor. I have a script for online validation of the endpoint, and the F1 score is ~99%. This indicates that the endpoint interprets the call correctly. \n\nModel Monitor is recognizing the data in my jsonl files as the data not being CSV formatted. When my Model Monitor processing job runs, I receive the following constraint violation: \"There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\".\n\nGiven the results from the Endpoint and this Model Monitor constraint violation, I perceive there is a conflict between how the Endpoint is storing the data and how the Model Monitor Processing Job wants to consume the data.\n\nHere is one sample prediction from the jsonl file. The data value is comma separated. \n\n    {\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"JHB,44443000.0,-0.0334,,44264000.0,,,,-2014000.0,,-2014000.0,,,,,,,-0.04,-0.04,55872000.0,,,0.996,,,,,,,,-0.0453,,2845000.0,,2845000.0,11636000.0,,,,,,,,,,,,190000000.0,,,,,,,,-18718000.0,,,,,,,,29000000.0,,,,,,,,-33000000.0,,-4000000.0,,,,,,,,,,,,,,,0.0,,,0.995972369102,1.0,-0.045316472785366,0.0,,,,,,,0.0,,,,,,,,,95.5638,,,,,,1.0,1.0,,0.15263157894737,,,,,,0.65252120693923,0.0,0.15263157894737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18606500.0,,,95.5638,,,2.3886,,,,,-0.0326,,-1.0449,,-1.05,-1.05,,0.0,,-0.1471,,,,,,,,,,,,,,,,,-0.5451,,,,,,,Financial Services,16.67890010036862\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"c97df615-0a2e-414d-9be3-bf3a14eb6363\",\"inferenceTime\":\"2020-04-15T16:26:46Z\"},\"eventVersion\":\"0\"}\n\nHere is the point within the log that the processing job recognizes a column mismatch. I see that it pulls down the data to store locally, pulls down the statistics and constraints files, errors with this constraint, and then gracefully ends the Processing Job. If more logs are needed to analyze, I have the Processing Job logs in CloudWatch Logs.\n\n    2020-04-15 17:11:49 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/constraints\/constraints.json.\n    2020-04-15 17:11:50 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/stats\/statistics.json.\n    2020-04-15 17:11:50 ERROR DataAnalyzer:65 - There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\n    Skipping further processing because of column count mismatch.\n\nI could not find Model Monitor documentation on how to deal with column mismatch constraint violations.",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1586976185000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925546705,
        "Answer_comment_count":0.0,
        "Answer_body":"That violation fires when, for example, input to your endpoint has fewer columns than baseline input does. This is helpful to flag data quality issues. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-interpreting-violations.html\n\nIn this case, however, this is an artifact of how we perform the analysis. We concatenate output and input CSVs into a single CSV to analyze the whole thing in one go. E.g. it would look like:\n\n```\noutput_col,input_col_1,input_col_2,...,input_col_n\n```\n\nIn this case, however, your output has a trailing newline which means that after concatenating this looks like:\n\n```\noutput_col # embedded newline in your output\n,input_col_1,input_col_2,...,input_col_n\n```\n\nTriggering the code to think there is only one column in dataset and hence failing the job.\n\nWe have a fix flowing through the pipeline now, while that goes out you can add a preprocessing script to your schedule to strip out the trailing newline from the output. We will create a sample notebook for this, in the meantime docs are at\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-pre-and-post-processing.html#model-monitor-pre-processing-script",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker Studio - create domain error",
        "Question_created_time":1586796156000,
        "Question_last_edit_time":1668609159170,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1619,
        "Question_answer_count":1,
        "Question_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User:  arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:*:*:domain\/*\u201d. \nWe confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking.\nAnd aws sagemaker list-user-profiles returns no user profiles. \n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1586807470000,
        "Answer_score_count":1.0,
        "Answer_last_edit_time":1667925577936,
        "Answer_comment_count":0.0,
        "Answer_body":"A user with admin privileges would have access to `\"iam:CreateServiceLinkedRole\"` and `\"sagemaker:CreateDomain\"` actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user [least privilege](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#security_iam_service-with-iam-policy-best-practices) by reviewing [Control Access to the Amazon SageMaker API by Using Identity-based Policies](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#api-access-policy) and [Actions, Resources, and Condition Keys for Amazon SageMaker](https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/list_amazonsagemaker.html) documentation:\n\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": \"sagemaker:CreateDomain\",\n        \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n    }\n\nNOTE: An AWS account is limited to one Domain, per region, see [CreateDomain](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateDomain.html).\n\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": \"iam:CreateServiceLinkedRole\",\n        \"Resource\": \"*\",\n        \"Condition\": {\n            \"StringEquals\": {\n                \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n            }\n        }\n    }\n\nCheers!",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Question_created_time":1586331915000,
        "Question_last_edit_time":1668623326405,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sagemaker-model-artifact-during-a-training-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":403,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1586359356000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925569445,
        "Answer_comment_count":0.0,
        "Answer_body":"If you specify a checkpoint configuration (***regardless of managed spot training***) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n    \"CheckpointConfig\": { \n      \"LocalPath\": \"string\",\n      \"S3Uri\": \"string\"\n    }\n\nThe local path defaults to `\/opt\/ml\/checkpoints\/`, and then you specify the target path in S3 with `S3Uri`.\n\n***Given this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3***. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, `sagemaker-containers` implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\n**Note**: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info [here][1].\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ProcessingS3Output.html",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Question_created_time":1584346040000,
        "Question_last_edit_time":1667926338453,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sagemaker-spot-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":101,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job?\nIs there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1584346840000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925558613,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi olivier, \nIf you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the  blog on [Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs][1] also mentions the same \n\n> To avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"permissions issue with Sagemaker endpoint deployment",
        "Question_created_time":1583528380000,
        "Question_last_edit_time":1668630010465,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL3DB8x9R7ysr8KjxY4PXg\/permissions-issue-with-sagemaker-endpoint-deployment",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1043,
        "Question_answer_count":2,
        "Question_body":"Hi there,   \n  \nI am having permission issue deploying a SageMaker Endpoint, if someone could help me out here.  \n  \nIf I run this code in Sagemaker&#39;s Jupyter Notebook without providing credentials for boto3 client, it works. However if I try to invoke it with my credentials on my local machine, it gives me permission error, even though my account already was given all Sagemaker permissions.   \n  \n**ClientError: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::XXXXXXXXXXXX:user\/jenny.lien is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:249707424405:endpoint\/XXXXXXXX with an explicit deny**  \n  \nThis stackoverflow post has exactly the same issue as what I&#39;ve experienced, I tried disable MFA for my IAM user but it is still not working...does it mean I will also have to create a special IAM  user to make it work? thank you  \nhttps:\/\/stackoverflow.com\/questions\/54172907\/amazon-sagemaker-accessdeniedexception-when-calling-the-invokeendpoint-operatio",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Question_created_time":1583496984000,
        "Question_last_edit_time":1668057386501,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sagemaker-xgboost-support-parallel-training-across-multiple-machines",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":119,
        "Question_answer_count":1,
        "Question_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1583654787000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925571827,
        "Answer_comment_count":0.0,
        "Answer_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see [Docker registry paths and example code](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html) in the Amazon SageMaker developer guide. ",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Available training metrics for built-in algorithms SageMaker",
        "Question_created_time":1581632698000,
        "Question_last_edit_time":1667925629788,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0kvMbwPeRcyYcMAnnPxkng\/available-training-metrics-for-built-in-algorithms-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":104,
        "Question_answer_count":3,
        "Question_body":"Hello,  \n  \nI use built-in algorithms SageMaker and I search about the training metrics for each algorithms in SageMaker, is there a list the training metrics built-in algorithms SageMaker where I can review?  \n  \nThank you.  \nRegards.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker PIPE Mode vs FSx ?",
        "Question_created_time":1579692074000,
        "Question_last_edit_time":1668074106093,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sagemaker-pipe-mode-vs-fsx",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":261,
        "Question_answer_count":1,
        "Question_body":"Hi,\nSageMaker supports training data streaming via [PIPE mode][1], and also reading from [FSx][2] distributed file system.\nThose options seem to provide same value: low latency, high throughput.\n\n - What are the reasons for using one or the other?\n - Do we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\n  [2]: https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/08\/amazon-sagemaker-works-with-amazon-fsx-lustre-amazon-efs-model-training\/",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1579732148000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925565015,
        "Answer_comment_count":0.0,
        "Answer_body":"I can think of the following scenarios \n\nPipemode cons\n\n** UPDATED**\n\n1.  Data Shuffling -  In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use [Sagemaker data shuffle][1] to perform file level shuffle\n\n2.  Data readers -   There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a  different deep leaning  framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use [ml-io][2] to see if any of the built-in pipe mode readers work for your usecase\n\n3. PIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\n\nFSX:\n\n1. FSX works by lazy loading the  s3 file and hence it has a start up delay but gets faster during repeated training. \n\n2. There is no  dependency on the framework and your existing code will work as is..\n\n3. The only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ShuffleConfig.html\n  [2]: https:\/\/github.com\/awslabs\/ml-io#Python",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker metrics persistence",
        "Question_created_time":1578643279000,
        "Question_last_edit_time":1668414520527,
        "Question_link":"https:\/\/repost.aws\/questions\/QU197giXXuRn-4Hz56HZZpSw\/sagemaker-metrics-persistence",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":227,
        "Question_answer_count":1,
        "Question_body":"Quick questions on ML metrics persistence from sagemaker training tasks. The SageMaker regexp-over-CloudWatch is an attractive option, yet the metric retention in Cloudwatch seems to be restricted to 15 days. \n\n 1. How to persist those metrics longer? Is it common to extract them out of Cloudwatch regularly to persist them somewhere else, eg S3 or an RDS? what is the best practice for long-term persistence of those\n    metrics?\n 2. Would SageMaker Experiments allow a collection of similar data (customer-defined training metrics) over a longer retention?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1578681130000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925565860,
        "Answer_comment_count":0.0,
        "Answer_body":"1. You can now persist algorithm metrics from SageMaker Training Jobs (the ones you can collect with regexes or the ones available from built-in algorithms by default) by setting [EnableSageMakerTimeSeriesMetrics](https:\/\/alpha-docs-aws.amazon.com\/sagemaker\/latest\/dg\/API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-EnableSageMakerMetricsTimeSeries) through the AWS SDK or `enable_sagemaker_metrics=true` in the SageMaker Python SDK. These metrics are persisted long term, and available through Amazon SageMaker Studio. (Go to \"Metrics\" -> \"Add Chart\" from the detail page of a training job). These are available at no additional cost. \n\n2. Yes, SageMaker Experiments allow collection of similar data\n\nNote that system metrics (CPU\/GPU\/Memory\/Disk) are still available only through CloudWatch. ",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"how to increase the storage of host instance",
        "Question_created_time":1576741179000,
        "Question_last_edit_time":1668610792135,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeVh4VvD6R-eIhRYY6K8dsw\/how-to-increase-the-storage-of-host-instance",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":769,
        "Question_answer_count":2,
        "Question_body":"The parameters of a big neural network model can be huge. But the largest storage size of a host instance is only 30G, according to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html. Is there a way to increase the storage volume? I have a model (embeddings) that is very close to 30G and caused a no space error when deploying.   \n  \nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1577746997000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1577746997000,
        "Answer_comment_count":0.0,
        "Answer_body":"The disk size is currently not configurable for SageMaker Endpoints with EBS backed volumes. As a workaround, please use instances with ephemeral storage for your SageMaker endpoint.  \n  \nExample instance types with ephemeral storage:  \n- m5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/m5\/  \n- c5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/c5\/  \n- r5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/r5\/  \n  \nThe full list of Amazon SageMaker instance types can be accessed here: https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How should a custom SageMaker algorithm determine if checkpoints are enabled?",
        "Question_created_time":1576724302000,
        "Question_last_edit_time":1668500983136,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmOFbuaH9RPSY4kH2t1dFcQ\/how-should-a-custom-sagemaker-algorithm-determine-if-checkpoints-are-enabled",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":165,
        "Question_answer_count":1,
        "Question_body":"Per the [SageMaker Environment Variables][1] doc, algorithms should save model artifacts to the folder prescribed by `SM_MODEL_DIR`.\n\nThe [SageMaker Containers doc][2] describes additional environment variables, including `SM_OUTPUT_DATA_DIR` to write non-model training artifacts.\n\n...But how should the algorithm determine if checkpointing has been requested?\n\nThe [Using Checkpoints in Amazon SageMaker][3] doc only specifies a default local path to save them to, and I can't see any environment variables that would indicate whether or not to checkpoint. I've seen one piece of code checking for the **existence** of that default local path, but not convinced anybody's actually checked to see whether it works (is present when checkpointing is requested and absent when not).\n\nIt's good to parameterize Checkpointing to avoid wasting EBS space (and precious seconds of IO) in jobs when it's not needed; and by the conventions for other I\/O like model and data folders I would assume SageMaker to have a specific mechanism to pass this instruction, rather than just defining an algo hyperparameter?\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-container-environmental-variables-user-scripts.html\n  [2]: https:\/\/github.com\/aws\/sagemaker-containers\n  [3]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1576730886000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925559360,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi,\n\nFor custom Sagemaker containers or  deep learning frameworks, I tend to do this..and it works\n\n\nThis example is for pytorch I have tried\n\n-  entry point file: \n   \n   ```python\n   # 1. Define a custom argument, say checkpointdir\n    parser.add_argument(\"--checkpointdir\", help=\"The checkpoint dir\", type=str,\n                        default=None)\n   # 2. You can additional params for checkpoint frequency etc\n\n   # 3. Code for checkpointing\n   if checkpointdir is not None:\n      #TODO: save mode\n    \n   ```\n\n-   Sagemaker estimator in Jupyter notebook, for. e.g.\n\n```python\n# 1. Define local and remote variables for checkpoints\ncheckpoint_s3 = \"s3:\/\/{}\/{}t\/\".format(bucket, \"checkpoints\")\nlocalcheckpoint_dir=\"\/opt\/ml\/checkpoints\/\"\n\nhyperparameters = {\n\n    \"batchsize\": \"8\",\n    \"epochs\" : \"1000\",\n    \"learning_rate\":.0001,\n    \"weight_decay\":5e-5,\n    \"momentum\":.9,\n    \"patience\": 20,\n    \"log-level\" : \"INFO\",\n    \"commit_id\":commit_id,\n    \"model\" :\"FasterRcnnFactory\",\n    \"accumulation_steps\": 8,\n# 2.  define hp for checkpoint dir\n    \"checkpointdir\": localcheckpoint_dir\n}\n\n# In the Sagemaker estimator fit, specify the local and remote path\nfrom sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(\n     entry_point='experiment_train.py',\n                    source_dir = 'src',\n                    dependencies =['src\/datasets', 'src\/evaluators', 'src\/models'],\n                    role=role,\n                    framework_version =\"1.0.0\",\n                    py_version='py3',\n                    git_config= git_config,\n                    image_name= docker_repo,\n                    train_instance_count=1,\n                    train_instance_type=instance_type,\n# 3. The entrypoint file will pick up the checkpoint location from here\n                    hyperparameters =hyperparameters,\n                    output_path=s3_output_path,\n                    metric_definitions=metric_definitions,\n                    train_use_spot_instances = use_spot,\n                    train_max_run =  train_max_run_secs,\n                    train_max_wait = max_wait_time_secs,   \n                    base_job_name =\"object-detection\",\n# 4. Sagemaker knows that the checkpoints will need to be periodically copied from the localcheckpoint_dir to s3 pointed to by checkpoint_s3\n                    checkpoint_s3_uri=checkpoint_s3,\n                    checkpoint_local_path=localcheckpoint_dir)\n```",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker Studio will not load",
        "Question_created_time":1576685163000,
        "Question_last_edit_time":1668621255358,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sagemaker-studio-will-not-load",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1832,
        "Question_answer_count":8,
        "Question_body":"Morning of 12\/17 I loaded SageMaker Studio, and created an Autopilot experiment.  It ran for 2 hours and was successful.  \n  \nAfterwards, I exited SageMaker Studio.  Ever since that point, I have been unable to re-enter Studio. It is now 24 hours later.  \n  \nI either receive a response from Chrome:  \n  \nThis page isn\u2019t working  \nd-*************.studio.us-east-2.sagemaker.aws didn\u2019t send any data.  \nERR_EMPTY_RESPONSE  \n  \nOr, I get an error message:  \nThe JupyterServer app default encoutered a problem and was stopped.   \nDetails: InternalFailure  \n  \nI get the option to \"Restart Now,\" but it never works.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Question_created_time":1576675818000,
        "Question_last_edit_time":1667955285675,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sagemaker-studio-notebooks-hardware-config",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":139,
        "Question_answer_count":1,
        "Question_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1576708404000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925589626,
        "Answer_comment_count":0.0,
        "Answer_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is there any way to automatically stop the labeling job?",
        "Question_created_time":1575516599000,
        "Question_last_edit_time":1668000410519,
        "Question_link":"https:\/\/repost.aws\/questions\/QU13ldoQNRT06TJu2wWWnE0A\/is-there-any-way-to-automatically-stop-the-labeling-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_body":"Hello all,   \nI have a labeling job with about 40k images for labeling which I requested a vendor to work on, to control cost to fit within my budget, I wanted to automatically stop the job every 3k images being labeled, and may come back to get more labels more later when new budjet comes in, is there any way I can automatically stop the labeling job? or will I have to constantly monitor the labeling progress?  \n  \nThank you!  \n  \nJenny",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Incremental training with custom keras code in script mode",
        "Question_created_time":1574940710000,
        "Question_last_edit_time":1668592669454,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31VWC7mdRMCVfiCnAKiIXg\/incremental-training-with-custom-keras-code-in-script-mode",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":141,
        "Question_answer_count":1,
        "Question_body":"Hi,  \nI'm moving my first steps in sagemaker. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script\n\n```\nimport sagemaker\r\nfrom sagemaker.tensorflow import TensorFlow\r\nfrom sagemaker import get_execution_role\r\n\r\nbucket = 'sagemaker-blablabla'\r\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\r\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\r\n\r\ns3_output_location = 's3:\/\/{}'.format(bucket)\r\n\r\ntf_estimator = TensorFlow(entry_point='main.py', \r\n                          role=get_execution_role(),\r\n                          train_instance_count=1, \r\n                          train_instance_type='ml.p2.xlarge',\r\n                          framework_version='1.12', \r\n                          py_version='py3',\r\n                          output_path=s3_output_location)\r\n\r\ninputs = {'train': train_data, 'test': validation_data}\r\ntf_estimator.fit(inputs)\n```\n\nThe entry point is my custom keras code, which I adapted to receive arguments from the script.  \nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this\n\n```\ntrained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\r\n\r\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \r\n                                              role=get_execution_role(),\r\n                                              train_instance_count=1, \r\n                                              train_instance_type='ml.p2.xlarge',\r\n                                              output_path=s3_output_location,\r\n                                              model_uri = trained_model)\r\n\r\ninputs = {'train': train_data, 'test': validation_data}\r\n\r\ntf_estimator.fit(inputs)\n```\n\nDoesn't work.. first I don't know how to retrieve the training image name (for this I looked for it in the aws console, but I guess there should be a smarter solution), second this code throws an exception about the entry point.. but it is my understanding that I shouldn't need it when I do incremental learning with a ready image..  \nI'm surely missing something important, any help? Thank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"By default, does Sagemaker endpoint handles parallel requests?",
        "Question_created_time":1573757044000,
        "Question_last_edit_time":1668607977813,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfOh3ije6SsGZ-xrzRz0xzg\/by-default-does-sagemaker-endpoint-handles-parallel-requests",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":958,
        "Question_answer_count":1,
        "Question_body":"When there are multiple concurrent InvokeEndpoint requests being called to a deployed AWS Sagemaker endpoint, how is it being handled?  \n  \nI have deployed an endpoint with a P3.2xlarge instance. Currently one job takes around ~45 seconds to process. I have tried sending 4 different InvokeEndpoint requests at the same time and I noticed from CloudWatch logs that the jobs are being done serially depending on which request arrives first. --I suspect there is some sort of queue internally within the server model itself.--  \n  \nI am aware of automatic scaling as described here: https:\/\/aws.amazon.com\/\/blogs\/machine-learning\/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling\/ but my question is by default does aws sagemaker not allow concurrent requests being handled at the same time?  \n  \n**UPDATE**  \nUpon further investigation and testing here are some additional information.   \nI have deployed an ml.m4xlarge instance that simply sleeps for 45 seconds inside the transform function. Looks somewhat like\n\n```\ndef transform_fn(model, request_body, content_type, accept_type):       \r\n    request_body_dict = json.loads(request_body)\r\n    time.sleep(45)\r\n    ...\n```\n\nFurthermore, I have set the server timeout to be 420 seconds like so. \n\n```\nsagemaker_model = MXNetModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/yolo_object_person_detector.tar.gz',\r\n                             role = role, \r\n                             entry_point = 'load_testing_entrypoint.py',\r\n                             py_version='py3',\r\n                             framework_version='1.4.1',\r\n                             sagemaker_session = sagemaker_session,\r\n                            env = {'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '420' })\r\n\r\npredictor = sagemaker_model.deploy(\r\n                            initial_instance_count=1,\r\n                            instance_type='ml.m4.xlarge',\r\n                            endpoint_name='load-testing')\n```\n\nI tried sending 9 consecutive requests and monitored how they are being executed and what I've found is that there is no specific order in which the requests are being handled.  \n  \nA few questions I have from this experiment is:  \n1) Does AWS Sagemaker not process requests concurrently? Meaning, I would expect the server being able to handle two requests at the same time?  \n2) From the client's side, how is it handling the case when the server is busy? I notice that it internally does retries for about 3 times after every 60 seconds if the request is not being handled  \n3) Within each of the 60 seconds time window, how is the client code calling the Endpoint? Is it constantly calling after every 1,2,4,6,8 seconds ?   \n  \nHere is the client side code \n\n```\nsagemaker_client = boto3.client('sagemaker-runtime')\r\nresponse = sagemaker_client.invoke_endpoint(EndpointName='load-testing',Body=request_body)\n```\n\nEdited by: ptanugraha on Nov 14, 2019 10:44 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Batch Transform local mode?",
        "Question_created_time":1571055107000,
        "Question_last_edit_time":1668610004530,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sagemaker-batch-transform-local-mode",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":336,
        "Question_answer_count":1,
        "Question_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1571303926000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925565482,
        "Answer_comment_count":0.0,
        "Answer_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.  \n\nThe request can be CSV\/JSON or binary (a parquet file in your case).  \n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Fail to create Endpoints in SageMaker--JNI and NoClassDefFound error",
        "Question_created_time":1569795079000,
        "Question_last_edit_time":1667925885107,
        "Question_link":"https:\/\/repost.aws\/questions\/QUECV9A3Q2RK6pqVpSvhRpYw\/fail-to-create-endpoints-in-sagemaker-jni-and-noclassdeffound-error",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":88,
        "Question_answer_count":3,
        "Question_body":"I was trying to deploy a model(logged as a Mleap model in Databricks and saved in a s3 bucket) to SageMaker, and got stuck at the Endpoint creation:  \n  \n\"The primary container for production variant \\[xxx] did not pass the ping health check. Please check CloudWatch logs for this endpoint.\"  \n  \nIn the log I found the following block repeating over and over again until some time later the creating Endpoint process just stopped and the status turned 'failed' in the SageMaker UI:\n\n```\nError: A JNI error has occurred, please check your installation and try again\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org\/eclipse\/jetty\/util\/thread\/ThreadPool\r\n#011at java.lang.Class.getDeclaredMethods0(Native Method)\r\n#011at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\r\n#011at java.lang.Class.privateGetMethodRecursive(Class.java:3048)\r\n#011at java.lang.Class.getMethod0(Class.java:3018)\r\n#011at java.lang.Class.getMethod(Class.java:1784)\r\n#011at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)\r\n#011at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)\r\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.thread.ThreadPool\r\n#011at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n#011at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n#011at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\r\n#011at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n#011... 7 more\r\nGot sigterm signal, exiting.\n```\n\nI coded in Python in Databricks and as far as I could tell this is a Java error so I have no idea what went wrong, anyone has any experience deploying an 'outside' model to SageMaker?  Any help or tips would be much appreciated!  \n  \nFYI: my whole s3 setup, ECR setup were in us-west-2(Oregon); the integration of AWS IAM role and Databricks Role are properly set up; in my s3,  I could see the Databricks distributed filesystem in one bucket and the trained and pickled model in another; the docker image that is supposed to hold the model is successfully registered in ECR. I also tried to change the instance type under 'Production variants' in the Endpoint creation settings, I set it to the same instance type (ml.m5.large) as the one I used to initiate the Databricks runtime cluster but it did not seem to work.  \n  \nUpdate: I successfully trained, logged and deployed a sklearn model, but still have the same issue with spark ML model; for the container, I used a image built by mlflow:\n\n ```\n mlflow sagemaker build-and-push-container\n ```\n\nEdited by: ShumZZ on Sep 29, 2019 3:12 PM  \n  \nEdited by: ShumZZ on Oct 2, 2019 2:42 PM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Question_created_time":1568641875000,
        "Question_last_edit_time":1668590352780,
        "Question_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":338,
        "Question_answer_count":3,
        "Question_body":"Hi Team,  \nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.  \nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e..  deployment package size is 50 MB.  \nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.  \nsample code for this api :   \n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.  \n  \nimport sagemaker  \nfrom sagemaker.amazon.amazon_estimator import get_image_uri  \ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')  \n  \nAny reference would be of great help. Thank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1568642184000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1568642184000,
        "Answer_comment_count":0.0,
        "Answer_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"New features of XGBoost",
        "Question_created_time":1564402802000,
        "Question_last_edit_time":1667925968565,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIfvyse5TT8qN0Y0lyommFQ\/new-features-of-xgboost",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":40,
        "Question_answer_count":1,
        "Question_body":"I have realized that the original XGBoost has several features which are not supported in the preloaded XGBoost in Sagemaker (or at least I have not been able to find them). For instance, the new metrics as aucr are not available in sagemaker. Is there any plan to add such features? Thank you",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth job stuck in \"Stopping\" state",
        "Question_created_time":1564177526000,
        "Question_last_edit_time":1668343202587,
        "Question_link":"https:\/\/repost.aws\/questions\/QUttmAZMiTT8yjLovuVkwI2g\/ground-truth-job-stuck-in-stopping-state",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":88,
        "Question_answer_count":1,
        "Question_body":"I've got a job that I tried to stop, that has been stuck in \"Stopping\" for several days. This is problematic because it shows up on the worker portal, which will confuse my workers. Anything I can do to stop it completely?  \n  \nI can provide the full ARN if needed.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"loading and deploying a previously trained sagemaker xgboost model",
        "Question_created_time":1563308650000,
        "Question_last_edit_time":1668610147410,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Cm7BTSlQ1GtLEzqVeGftQ\/loading-and-deploying-a-previously-trained-sagemaker-xgboost-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1190,
        "Question_answer_count":3,
        "Question_body":"I am trying to write an inference pipeline where I load a previously trained sagemaker xgboost model stored in s3 as a tar.gz file (following sagemaker tutorial) and deploy it as an endpoint for prediction. Here is my code:\n\n```\ntrainedmodel = sagemaker.model.Model(    \r\n    model_data='data-path-to-my-model-in-s3\/model.tar.gz',\r\n    image=container,  \r\n    role=role)  \r\n\r\nxgb_predictor = trainedmodel.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n```\n\nThe code runs fine but after that when I try to call predict() on xgb_predictor I get an error saying 'NoneType' object has no attribute 'predict'. I followed the example here to train the xgboost model:  \n  \n<https:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-with-xgboost-and-amazon-sagemaker\/>  \n  \nWhy am I getting this error? What's the correct way to load a previously trained model? Help would be appreciated.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Received server error (0) from model when hosting",
        "Question_created_time":1562557558000,
        "Question_last_edit_time":1668615041499,
        "Question_link":"https:\/\/repost.aws\/questions\/QUut1Vw-EPQUWVOy6cxmOmXw\/received-server-error-0-from-model-when-hosting",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1077,
        "Question_answer_count":1,
        "Question_body":"Hi,  \n  \nI was trying to deploy my trained model to the endpoint and I was given a ModelError.   \n \"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See Link:<https:\/\/ap-southeast-2.console.aws.amazon.com\/cloudwatch\/home?region=ap-southeast-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/ss-notebook-treemapping-2019-07-08-00-56-39-825> in account 125017970330 for more information.\"  \n  \nI'm not sure what caused this issue and couldn't figure out how latency metrics in the CloudWatch would be useful in this case. Does anyone know what the approach is to solve this issue? It would also be great to know why this happens. Thanks in advance for any help!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_created_time":1562042043000,
        "Question_last_edit_time":1668613629013,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s3-bucket-from-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":2074,
        "Question_answer_count":1,
        "Question_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.  \n  \nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:  \n  \nbucket='bucketname'  \ndata_key = 'test.csv'  \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)  \ndf.to_csv(data_location)  \nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1562042063000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1562042063000,
        "Answer_comment_count":0.0,
        "Answer_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.  \n  \nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.  \n  \n# code you already have, saving the file locally to whatever directory you wish  \nfile_name = \"mydata.csv\"   \ndf.to_csv(file_name)  \n# instantiate S3 client and upload to s3  \nimport boto3  \n  \ns3 = boto3.resource('s3')  \ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')  \nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size",
        "Question_created_time":1559545390000,
        "Question_last_edit_time":1667926579528,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq0KSPPCBT1qrotLu0NJyBw\/aws-sagemaker-either-the-training-channel-is-empty-or-the-mini-batch-size",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_body":"I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50.   \n  \nI keep on getting this error in Sagemaker.  \n  \n> Customer Error: No training data processed. Either the training  \n> channel is empty or the mini-batch size is too high. Verify that  \n> training data contains non-empty files and the mini-batch size is less  \n> than the number of records per training host.  \n  \nI am using this InputDataConfig  \n  \n    InputDataConfig=\\[  \n                {  \n                    'ChannelName': 'train',  \n                    'DataSource': {  \n                        'S3DataSource': {  \n                            'S3DataType': 'S3Prefix',  \n                            'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',  \n                            'S3DataDistributionType': 'FullyReplicated'  \n                        }  \n                    },  \n                    'ContentType': 'text\/csv',  \n                    'CompressionType': 'Gzip'  \n                }  \n            ],  \n  \nI am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got   \n  \n> Customer Error: Unable to initialize the algorithm. Failed to validate  \n> input data configuration. (caused by ValidationError)  \n  \n  \n> Caused by: {u'training': {u'TrainingInputMode': u'Pipe',  \n> u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',  \n> u'S3DistributionType': u'FullyReplicated'}} is not valid under any of  \n> the given schemas  \n  \nI went back to train as that seems to be what is needed. But what am I doing wrong with that?  \n  \nEdited by: anshbansal on Jun 3, 2019 12:06 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground truth labeling job - unable to submit annotations",
        "Question_created_time":1559512661000,
        "Question_last_edit_time":1668446459681,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOw2BnRGWTVmLfC-zcNN-RA\/ground-truth-labeling-job-unable-to-submit-annotations",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":158,
        "Question_answer_count":1,
        "Question_body":"Hi,  \nI've been experiencing this issue when I tried to submit a response on the worker portal. The browser kept popping up a message saying \"something went wrong\" and instructed me to refresh. However, nothing changed after refreshing. Does anyone know what might be causing this? Thanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker Ground Truth notification after annotation job considered \"done\"?",
        "Question_created_time":1558877970000,
        "Question_last_edit_time":1667926654327,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuT3kd2CJRcanme8YzDa73Q\/sagemaker-ground-truth-notification-after-annotation-job-considered-done",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_body":"Hi all,  \n  \nIs it possible to generate a notification after an annotation job is done? Couldn't find this info anywhere in the API docs, or by trawling the web. Being able to receive such notifications, be it SNS or something else, would be lot better than having to periodically poll the job's status.  \n  \n  \n    Joni",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Mxnet default version?",
        "Question_created_time":1558282489000,
        "Question_last_edit_time":1667925934870,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGtnafC3BQnCcDoZXeGVB4g\/mxnet-default-version",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":36,
        "Question_answer_count":2,
        "Question_body":"Am struggling to deploy a SageMaker trained object detection model on DeepLens.  When I attempt to use mo.optimize on DeepLens, I get error status 2 \"inconsistent platform versions.\".  According to the SageMaker documention I found, the default version of mxnet is 1.2.1.  Is this info current, and is it therefore safe to assume that an mxnet model trained and created in SageMaker console will use that version?  \n  \nEdited by: BigEd on May 19, 2019 9:41 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Trouble deploying SageMaker trained model in DeepLens",
        "Question_created_time":1558110894000,
        "Question_last_edit_time":1668539569362,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOklHGvFVQRiq6f24aBMWuA\/trouble-deploying-sagemaker-trained-model-in-deeplens",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":96,
        "Question_answer_count":6,
        "Question_body":"Had no problem deploying sample models to DeepLens, but really struggling to get a SageMaker trained model to deploy successfully.  Here is what I have done so far:  \n  \n- Extracted images from video with AWS MediaConnect  \n- Cropped images to 540x540 with PIL and stored in S3  \n- Ran labeling job with Mechanical Turk to draw bounding box around object  \n- Ran training job in SageMaker console using Object Detection algorithm and input size 540  \n- Created model in SageMaker console from training job  \n- Imported model into DeepLens console  \n- Copied and modified Object Detection lambda with call to mo.optimize('model_algo_1',540,540)  \n- Created and deployed project to two separate DeepLens cameras  \n  \nNot getting an output stream on either DeepLens.  One device is about a year old.  I have run all the software updates and mo.optimize steps recommended in the troubleshooting guide.  The other device is brand new out of the box, updated to latest version via DeepLens console.  I have not monkeyed with the new device, so all further information will pertain to the older device.  I have connected a monitor and keyboard to the older device and am working in a terminal window.  \n  \nWhen I ran mo.optimize in terminal, I got a warning about having MXNET version 1.4.0 and 1.0.0 being required, along with list range errors.  I rolled back MXNET to 1.0.0 per the troubleshooting guide.  I did this with both PIP and PIP3.    \n  \nWhen I import mo in Python3, I get an import error message saying \"no module named mo\".   \n  \nWhen I import mo and run mo.optimize in Python, I get an access error about the current user lacking write permission to opt\/aws\/artifacts.  When I launch Python as sudo and run mo.optimize, I get warnings about symbols saved with MXNET 1.04, and a List Index out of Range error.   \n  \nHere is my error code for mo.optimize in Python console:\n\n```\naws_cam@Deepcam:~$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import mo\r\n>>> error, model_path = mo.optimize('model_algo_1',540,540)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,540,540]\r\nModel Optimizer arguments\r\n\tBatch: \t1\r\n\tPrecision of IR: \tFP16\r\n\tEnable fusing: \tTrue\r\n\tEnable gfusing: \tTrue\r\n\tNames of input layers: \tinherited from the model\r\n\tPath to the Input Model: \t\/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n\tInput shapes: \t[1,3,540,540]\r\n\tLog level: \tERROR\r\n\tMean values: \t()\r\n\tIR output name: \tmodel_algo_1\r\n\tNames of output layers: \tinherited from the model\r\n\tPath for generated IR: \t\/opt\/awscam\/artifacts\r\n\tReverse input channels: \tTrue\r\n\tScale factor: \t1.0\r\n\tScale values: \t()\r\n\tVersion: \t0.3.31.d8b314f6\r\n\tPrefix name for args.nd and argx.nd files: \t\r\n\tName of pretrained model which will be merged with .nd files: \t\r\nERROR:mo:[ ERROR ]  Output directory \/opt\/awscam\/artifacts is not writable for current user. For more information please refer to Model Optimizer FAQ.\n```\n\nHere is my error code for mo.optimize when running Python as sudo:\n\n```\naws_cam@Deepcam:~$ sudo python\r\n[sudo] password for aws_cam: \r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import mo\r\n>>> error, model_path = mo.optimize('model_algo_1\",540,540)\r\n  File \"<stdin>\", line 1\r\n    error, model_path = mo.optimize('model_algo_1\",540,540)\r\n                                                          ^\r\nSyntaxError: EOL while scanning string literal\r\n>>> error, model_path = mo.optimize('model_algo_1',540,540)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,540,540]\r\nModel Optimizer arguments\r\n\tBatch: \t1\r\n\tPrecision of IR: \tFP16\r\n\tEnable fusing: \tTrue\r\n\tEnable gfusing: \tTrue\r\n\tNames of input layers: \tinherited from the model\r\n\tPath to the Input Model: \t\/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n\tInput shapes: \t[1,3,540,540]\r\n\tLog level: \tERROR\r\n\tMean values: \t()\r\n\tIR output name: \tmodel_algo_1\r\n\tNames of output layers: \tinherited from the model\r\n\tPath for generated IR: \t\/opt\/awscam\/artifacts\r\n\tReverse input channels: \tTrue\r\n\tScale factor: \t1.0\r\n\tScale values: \t()\r\n\tVersion: \t0.3.31.d8b314f6\r\n\tPrefix name for args.nd and argx.nd files: \t\r\n\tName of pretrained model which will be merged with .nd files: \t\r\nERROR:mo:[13:54:52] src\/nnvm\/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 10400 with lower version of MXNet v10000. May cause undefined behavior. Please update MXNet if you encounter any issue\r\n[13:54:52] src\/nnvm\/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 10400 with lower version of MXNet v10000. May cause undefined behavior. Please update MXNet if you encounter any issue\r\n\/usr\/local\/lib\/python3.5\/dist-packages\/mxnet\/module\/base_module.py:53: UserWarning: You created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\r\n\trelu4_3_scale\r\n\tdata\r\n\tlabel\r\n  warnings.warn(msg)\r\n[ ERROR ]  -------------------------------------------------\r\n[ ERROR ]  ----------------- INTERNAL ERROR ----------------\r\n[ ERROR ]  Unexpected exception happened.\r\n[ ERROR ]  Please contact Model Optimizer developers and forward the following information:\r\n[ ERROR ]  list index out of range\r\n[ ERROR ]  Traceback (most recent call last):\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/main.py\", line 222, in main\r\n    return driver(argv)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/main.py\", line 208, in driver\r\n    mean_scale_values=mean_scale)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/pipeline\/mx.py\", line 80, in driver\r\n    graph = symbol2nx(model_nodes, model_params, argv.input)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/front\/mxnet\/loader.py\", line 70, in symbol2nx\r\n    model_nodes = MxnetSsdPatternMatcher.remove_and_change_layers(model_nodes)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/front\/mxnet\/mxnet_ssd_pattern_matcher.py\", line 132, in remove_and_change_layers\r\n    MxnetSsdPatternMatcher.ssd_pattern_remove_reshape(json_layers)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/front\/mxnet\/mxnet_ssd_pattern_matcher.py\", line 92, in ssd_pattern_remove_reshape\r\n    if l2['inputs'][0][0] != i or l2['op'] != 'Reshape':\r\nIndexError: list index out of range\r\n\r\n[ ERROR ]  ---------------- END OF BUG REPORT --------------\n```\n\nEdited by: BigEd on May 17, 2019 9:37 AM  \n  \nEdited by: BigEd on May 17, 2019 11:34 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth - Active Learning Status",
        "Question_created_time":1558037074000,
        "Question_last_edit_time":1667894393073,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBS3WNK9fQtOBQ4k3PowXLQ\/ground-truth-active-learning-status",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":101,
        "Question_answer_count":2,
        "Question_body":"I currently have a Ground Truth labeling job in progress for which I enabled the active learning feature. I see the activelearning folder in the output directory within s3, but all that's in there right now is a file called active_learning_info.json. The job is on its 4th batch of images, which at 250 for each batch, means it is at almost 1,000 objects labeled by Mechanical Turk workers at this point. Based on that as well as reading https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html and https:\/\/aws.amazon.com\/blogs\/machine-learning\/annotate-data-for-less-with-amazon-sagemaker-ground-truth-and-automated-data-labeling\/ , I was expecting to see additional contents within the activelearning folder. Is that not accurate? How do I know if active learning is truly being (at least) attempted within this labeling job?  \n  \nThank you!  \n  \nEdited by: naquent on May 16, 2019 1:05 PM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to create Keras's encoder-decoder model's endpoint?",
        "Question_created_time":1557413472000,
        "Question_last_edit_time":1667959097691,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa1eV25XxRjKhLPHVXe9TKQ\/how-to-create-keras-s-encoder-decoder-model-s-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_body":"Hi,  \nI'm just started to use sagemaker.  \nNow, I'm testing encoder-decoder model for regression.  \nThe model is coded in Sagemaker's script mode, and finished learning on a Jupyter notebook.  \n  \nThe learning code is written by keras + tensorflow, and the model is based on the encoder decoder model. It is similar to the Keras's seq2seq example linked below.   \nBelow, I will take this as an example.  \nhttps:\/\/keras.io\/examples\/lstm_seq2seq\/  \n  \nIn the above model, there are \"encoder_model\" and \"decoder_model\" apart from \"model\" to be trained, and in the inference, \"encoder_model\" and \"decoder_model\" are used to generate the prediction by the function \"decode_sequence (input_seq)\".  \nI would like to deploy this function \"decode_sequence (input_seq)\" as an endpoint, but it doesn't work as usual with estimator.deploy () and I don't know how to implement it.  \n  \nIs there any sample code or resources to solve this?  \nThanks in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to enable automated data labeling for custom labeling job?",
        "Question_created_time":1557318279000,
        "Question_last_edit_time":1667861880933,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-JeOVDZER4uGxiY0l_geKg\/how-to-enable-automated-data-labeling-for-custom-labeling-job",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":90,
        "Question_answer_count":1,
        "Question_body":"Hi,  \nI need to label large dataset which has around 200000 records and this doesn't fall into any of the prebuilt labeling job categories. So trying to create custom labeling job with automated data labeling enabled. But the configuration to enable the automated data labeling doesn't appear for custom labelling job. Any reference to proceed further asap would be of great help. Thank you.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_created_time":1557130614000,
        "Question_last_edit_time":1668629494374,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sagemaker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":547,
        "Question_answer_count":1,
        "Question_body":"hi there,  \n  \nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.  \n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"  \n  \nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.  \nCould anyone tell me how to solve it?  \nI also came across the following articles, but i am not sure if it is the root cause.  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1557180251000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1557180305000,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi,  \nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance  \n  \nOne example of such as setup is  \n1. RDS DB Instance is VPC vpc-a and Subnet subnet-b.  \n2. SageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"  \n3. In the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"  \n_-- Type - Protocol - Port Range - Source_  \n_-- MYSQL\/Aurora - TCP - 3306 - sg-c_  \n  \n  \n-----  \nSample Code:\n\n```\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n```\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!  \n  \nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Private Marketplace for SageMaker algorithms",
        "Question_created_time":1556295545000,
        "Question_last_edit_time":1668245479612,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCu2f_chpRL2mDtFiGqwVNg\/private-marketplace-for-sagemaker-algorithms",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":122,
        "Question_answer_count":1,
        "Question_body":"We would like to make algorithms shareable and re-usable across teams. Is it possible to create a private Amazon SageMaker algorithm marketplace?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1556296138000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1612474519794,
        "Answer_comment_count":0.0,
        "Answer_body":"[Private Marketplace](https:\/\/aws.amazon.com\/marketplace\/privatemarketplace\/) is a feature of the AWS Marketplace platform that AWS customers can use to create a private catalog of products containing both algorithms and models which are available in AWS Marketplace. To create a private catalog containing algorithms which have not been published in AWS Marketplace, you can use AWS Service Catalog. [AWS Service Catalog](https:\/\/aws.amazon.com\/servicecatalog\/) lets users create and share algorithms packaged via a CloudFormation template. However, you do have to make the container image and model artifacts available in the destination account outside AWS Service Catalog. Here is a sample template for sharing a model - https:\/\/github.com\/aws-samples\/aws-service-catalog-reference-architectures\/blob\/master\/sagemaker\/sagemaker_vend_endpoint.yml ",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_created_time":1556295446000,
        "Question_last_edit_time":1667925988661,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sagemaker-model-to-iot-greengrass-in-different-account",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1556529654000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1611605697668,
        "Answer_comment_count":0.0,
        "Answer_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API. \n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Greengrass for data processing and ML model training",
        "Question_created_time":1556295399000,
        "Question_last_edit_time":1668529586005,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":78,
        "Question_answer_count":1,
        "Question_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1556295681000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1606141115182,
        "Answer_comment_count":0.0,
        "Answer_body":"As a native service offering, Greengrass has support for deploying models to the edge and running *inference* code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Ground Truth Text Format",
        "Question_created_time":1556263122000,
        "Question_last_edit_time":1668086015695,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzhqhRbWOReCIYtX925_VPw\/ground-truth-text-format",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":284,
        "Question_answer_count":3,
        "Question_body":"Hello  \nWe are trying to setup labeling for text with ground truth. Is it possible to format the source text to be labeled in some way e.g. html, markdown? A new line would already help a lot. I could not find any documentation on this here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html  \n  \nThanks  \nNicolas",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth - Label Consolidation, NN Models, and Accuracy Levels",
        "Question_created_time":1554731637000,
        "Question_last_edit_time":1668293653956,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVR_6Xq0sQKaRrK65qA2VOg\/ground-truth-label-consolidation-nn-models-and-accuracy-levels",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":95,
        "Question_answer_count":2,
        "Question_body":"Hello,  \n  \n(1)  \nIn https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html, it says: \"Multi-class annotation consolidation for image and text classification uses a variant of the Expectation Maximization approach to annotations. It estimates parameters for each worker and uses Bayesian inference to estimate the true class based on the class annotations from individual workers.\" Is there anywhere I can get more information and detail on this consolidation process? For now, I am trying to understand what this looks like when I have only 2 labelers per object, but I would also want to get a deeper understanding of how this works in general.  \n  \n(2)  \nIs there anywhere I can get more information on the neural networks used in the automatic labeling component of Ground Truth? Is there a way to access and customize them? Also, is there a way to export\/pickle the models and use them outside of Ground Truth?  \n  \n(3)  \nLastly, in https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html, it says: \"The potential benefit of automated data labeling also depends on the accuracy that you require. Higher accuracy levels generally reduce the number of data objects that are automatically labeled.\" Where is this accuracy level set?  \n  \nThank you!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Lifecycle scripts to access the notebook instance git repository",
        "Question_created_time":1554297310000,
        "Question_last_edit_time":1668612944722,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhTwl-BZURCGatI4Rm9fFqg\/lifecycle-scripts-to-access-the-notebook-instance-git-repository",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":262,
        "Question_answer_count":2,
        "Question_body":"Hi there,  \nIs it possible for the lifecycle scripts to access the content of the checkout-ed git repository? A use case would be to access the already available in the repository pip requirements file and to populate the notebook instance with the required python modules on start up.  \nI guess the answer to this question depends on the order of the executed events when a notebook is created. Are you executing the lifecycle scripts first and then checkout the repository or vice versa.  \n  \nThanks!  \n  \nEdit:  \n  \nWell, I did my experiment as follows.  \n  \nI added a simple 'ls -al SageMaker' in the start and create lifecycle scripts and inspected the logs.  \nIt seems that on initial notebook instance creation the git repository is checked out after the execution of the start and create scripts.  \nOn subsequent notebook starts, the start script is executed and the repository folder is present in the SageMaker folder with a timestamp indicating that the repository folder was created after the initial start\/create scripts executions.  \n  \nSo, can someone confirm that this is what's expected and that we can access the repository only on subsequent notebook starts?  \n  \nEdited by: ainkov on Apr 3, 2019 7:25 AM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Linear Learner - Multiclass classification - Prediction Scores",
        "Question_created_time":1553707649000,
        "Question_last_edit_time":1667925827383,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCgmx93mT92w6lw03c8Sqg\/linear-learner-multiclass-classification-prediction-scores",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":64,
        "Question_answer_count":2,
        "Question_body":"Hi,  \nI am using the linear learner algo for multiclass classification with 5 classes and 9 features  \n  \nI'm trying to figure out what the scores relate to? After getting the predicted_label result I get 5 scores....eg....  \n  \n{\"score\": \\[0.0003054474655073136, 0.8110334873199463, 0.0521857813000679, 0.1320497989654541, 0.004425559192895889], \"predicted_label\": 1.0}  \n  \nor  \n  \n{\"score\": \\[0.0001415203878423199, 0.11968196928501129, 0.8732749223709106, 0.006143826059997082, 0.0007577905780635774], \"predicted_label\": 2.0}  \n  \nDoes each score relate to a class? I have tried searching the internet for an explanation but so far cannot find anything, hopefully someone can enlighten me  \n  \nRegards",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Unable to create endpoint",
        "Question_created_time":1553516561000,
        "Question_last_edit_time":1668612166705,
        "Question_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":666,
        "Question_answer_count":1,
        "Question_body":"Hi,  \n  \nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:  \n  \n**Failure reason**  \nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets  \n  \nI have tried using different instance types but always the same error  \n  \nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1553556696000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1553556696000,
        "Answer_comment_count":0.0,
        "Answer_body":"Hello,  \n  \nSagemaker engineer here. I looked at the VpcConfig of your model and found only one subnet configured.   \n  \nThe error message \"Unable to locate at least 2 availability zone(s) with the requested instance type XYZ that overlap with SageMaker subnets\" usually indicates misconfigured VPCs. Sagemaker imposes mandatory requirement for at least 2 availability zones in your VPC subnets even if you only request one instance, to account for the potential use of auto-scaling in the future.   \n  \nIn order to create the endpoint, the number of subnets in your model needs to be at least 2 in distinct availability zones, and ideally as close to the total number of availability zones as possible in the region.   \n  \nHope it helps,   \nWenzhao",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Ground Truth - Error loading Image for Task",
        "Question_created_time":1552494750000,
        "Question_last_edit_time":1668443362790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfa0f2oL7SY2hRFJ05dRC6Q\/ground-truth-error-loading-image-for-task",
        "Question_score_count":1,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":149,
        "Question_answer_count":1,
        "Question_body":"Hi,   \n  \nI am creating a custom labeling job using boto3. It creates the job and the user is able to access the job using the front end tool but it does not load the images. I am using the default pre labeling lambda functions (arn:aws:lambda:us-east-1:432418664414:function:PRE-SemanticSegmentation) and the default for the annotation consolidation lambda (arn:aws:lambda:us-east-1:432418664414:function:ACS-SemanticSegmentation). I think the issue is with the name I am using for the LabelAttributeName (custom-ref) but I am not sure.   \n  \nFull error: There was an error loading the image for the task  \nThis task was created incorrectly and cannot be completed at this time. Please return\/stop this task.  \n  \nAny help would be appreciated.   \n  \nThanks,  \nHumberto",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Image Classification Algorithm Class Activation Map",
        "Question_created_time":1552316346000,
        "Question_last_edit_time":1667926186729,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUdCD7nXuTJegFm_lGunPzw\/image-classification-algorithm-class-activation-map",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":47,
        "Question_answer_count":2,
        "Question_body":"Hey!  \n  \nI am using SageMaker's built in image classification algorithm and currently have a trained model that can make predictions. I am wondering if there is an easy way to use this model to make a class activation map to better debug my classifier and see why it is making the predictions it is?  \n  \nThanks!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to deploy pre-trained model?",
        "Question_created_time":1551517462000,
        "Question_last_edit_time":1668098826335,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_grAySy_Ra6hAsekjf_tlQ\/how-to-deploy-pre-trained-model",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":180,
        "Question_answer_count":1,
        "Question_body":"I have a pre-trained model that will translate text from English to Marathi. You can find it here...  \ngit clone https:\/\/github.com\/shantanuo\/Word-Level-Eng-Mar-NMT.git  \n  \nClone and Run the notebook. I am looking for a way to deploy it so that users can use it as an API  \n  \nThe guidelines for deploying the model can be found here...  \nhttps:\/\/gitlab.com\/shantanuo\/dlnotebooks\/blob\/master\/sagemaker\/01-Image-classification-transfer-learning-cifar10.ipynb  \n  \nI will like someone to suggest me the steps to follow to deploy the model.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"invoke_endpoint error in Lambda: StreamingBody is not JSON serializable",
        "Question_created_time":1550644604000,
        "Question_last_edit_time":1668617593506,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6wwr5n1XTuWtQjkbCGbCOA\/invoke-endpoint-error-in-lambda-streamingbody-is-not-json-serializable",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1746,
        "Question_answer_count":5,
        "Question_body":"I'm writing a Lambda function that invokes an endpoint:\n\n```\nruntime= boto3.Session().client('runtime.sagemaker')\r\npayload = {\"data\": [\"McDonalds\"]}\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\n                                       ContentType='application\/json',\r\n                                       Body=json.dumps(payload))\n```\n\nIt returns this error \n\n```\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f59e40acc50> is not JSON serializable\n```\n\nI tried this exact function in SageMaker notebook and it works but it doesn't work in Lambda. Can someone please help me?  \n  \nEdited by: aurelius on Feb 19, 2019 10:38 PM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"What factors affect Sagemaker endpoint response time?",
        "Question_created_time":1550614607000,
        "Question_last_edit_time":1668548021315,
        "Question_link":"https:\/\/repost.aws\/questions\/QUraUOZYBITMGxNh339cctOA\/what-factors-affect-sagemaker-endpoint-response-time",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":797,
        "Question_answer_count":2,
        "Question_body":"I'm using Sagemaker as part of a planned deployment of a XGBoost model to production where it will be called by a customer facing mobile app (via another back end service that we also have hosted in AWS).  \n  \nI would like to understand how to improve response times. I have tested the response time of my model both when it resides locally on my own dev machine, as well as when its running in Sagemaker.  \n  \nThe wall time for local atomic predictions takes about 1 ms at 50p and 7 ms at 99p.  \n  \nThe wall time for atomic predictions (using the Python client SDK in a sagemaker notebook) takes about 20 ms at 50p and  25 ms at 99p. However, there are outliers that take as long as ~300ms.   \n  \nI am curious to know what factors affect the performance of Sagemaker calls (other than the complexity of the model itself). And I would be very grateful for any tips to get our outliers lower (preferably around 50 ms if possible).",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Ground Truth - Progress Not Updating?",
        "Question_created_time":1550529412000,
        "Question_last_edit_time":1668000554942,
        "Question_link":"https:\/\/repost.aws\/questions\/QUf9EnD7gzRlu86yy1Zlry7g\/ground-truth-progress-not-updating",
        "Question_score_count":1,
        "Question_favorite_count":1,
        "Question_comment_count":0,
        "Question_view_count":316,
        "Question_answer_count":10,
        "Question_body":"I've just configured my first Ground Truth labelling job, 1737 images assigned to a private workforce.  \n  \nWhen I first logged in as a worker I was able to label 10 images as a test run, and the Ground Truth \"Labeling job summary\" in the AWS Console shows 10\/1737 images labelled.  So far so good.  \n  \nI then returned to the labelling job and worked through approx 50 images and then used the 'Stop working' button to finish my labelling session.  \n  \nDespite that work, the progress shown in \"Labelling job summary\" has not been updated with those additional labeled images, even 48+ hours after the work was done.  \n  \nHow can I confirm whether the image labelling is happening correctly?  I don't see any error messages that might help debug the situation, and I certainly don't want to get other workers to label 1700+ images without being sure that the labelling data is being saved.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Which CUDA version exists on p2.xlarge when I use it in Sagemaker?",
        "Question_created_time":1550235092000,
        "Question_last_edit_time":1668603416275,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL74ZyThTRlKKMEN4lYGJJQ\/which-cuda-version-exists-on-p2-xlarge-when-i-use-it-in-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":955,
        "Question_answer_count":5,
        "Question_body":"Hi,  \nFrom my Host machine: **Nvidia GPU GeForce GTX 1080 Ti, CUDA 9.0, Ubuntu 16.04**:  \n  \n**I have compiled my docker image with:**  \n1) nvidia cuda 9.0 cudnn 7 devel (base image)  \n2) installed python2.7, darknet yolo (from PJReddie site)  \n3) built the darknet from source with options GPU=1, OPENMP=1, CUDNN=1  \n4) deployed Flask based RESTful service that gives prediction from Images (object labels and bounding boxes)  \n  \n**On this docker image:**  \n1) trained the Darknet model with my images data and created model weights file (yolov2.backup)  \n2) tested this model with test images, and it works  \n3) ensured that the darknet YOLO model utilizes the host machine's GPU  \n  \n**Then to use this custom image in Sagemaker, for inference:**  \n1) pushed docker image to ECR, created Model, Endpoint Config and Endpoint in Sagemaker  \n2) config = **p2.xlarge**, min instance=1, max instance=1  \n3) The\n `\/ping`\n and\n `\/invocations`\n requests ran without error E2E.  \n4) The RESTful client code was Python code from Sagemaker's Jupyter Notebook  \n  \n_**Issue I am facing is:**_  \n  \nThere are no predictions generated during execution and my webservice returns an Image in response, when requested by Python's 'requests' API. The image that I get in response, does not show any object's bounding box. I feel that the CUDA version on P2 instance is not matching the CUDA9.0 that is in my docker image. For CUDA to run properly, it is required that the host machine and docker's CUDA version should be same.  \n  \n_When I test my webservice and Darknet predictions inside my docker on my local Host machine, it runs fine, but not on Sagemaker!!_  \n  \n**Questions:**  \n1) Can you please tell me which CUDA version exists on the p2.xlarge instance? So that I can build my docker accordingly.  \n2) Please suggest some way to print debug information in CloudWatch logs from my docker image so that I am aware what is going on inside the Docker on Sagemaker env. The Flask webservice code doesn't allow any print statements. Hence each time I need to debug, I am creating a new docker image. A good way could save my time.   \n  \nThanks",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"How to pass null\/Nan values into the dataframe passed into batch transform",
        "Question_created_time":1549531218000,
        "Question_last_edit_time":1668441126052,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtKq2ZetyTUmvDwS7i3ot2Q\/how-to-pass-null-nan-values-into-the-dataframe-passed-into-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":224,
        "Question_answer_count":1,
        "Question_body":"I am trying to make inference using my Xgboost model on a dataset which has NaN values, now inherently Xgboost handles the NaN values, it does not throw any error while training with NaN values whereas the batch transform job gives the error **''could not convert string to float''** when it encounters NaN values into the dataset that is to be transformed.  \nCan anyone help me as to how can I pass NaN values into my input dataset for the batch transform job?  \nThanks a ton!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"how to version step functions for ML?",
        "Question_created_time":1549456954000,
        "Question_last_edit_time":1668624402177,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":555,
        "Question_answer_count":1,
        "Question_body":"Hi,\nStep Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers\nOlivier",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1549478317000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1601285800631,
        "Answer_comment_count":0.0,
        "Answer_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"S3 Dataset versioning with SageMaker?",
        "Question_created_time":1549396058000,
        "Question_last_edit_time":1668615929200,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s3-dataset-versioning-with-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":775,
        "Question_answer_count":2,
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning?\nBasically, what setup allows to track a given model training execution to a given dataset?\nInterested to hear about proven or state-of-the-art ideas",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1549437509000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925558875,
        "Answer_comment_count":0.0,
        "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic.\nUsing SageMaker search you'll be able to find all your jobs and compare between datasets.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Neo compilation load error",
        "Question_created_time":1548810043000,
        "Question_last_edit_time":1668416793084,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW8F63wxUTSCCdXf0MESXIg\/neo-compilation-load-error",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":45,
        "Question_answer_count":2,
        "Question_body":"Hi,  \n  \nI tried to follow the documentation to run a Neo compilation job on AWS console. I downloaded the model from <http:\/\/download.tensorflow.org\/models\/mobilenet_v1_2018_02_22\/mobilenet_v1_1.0_224.tgz> and uploaded it to S3. I did everything else the same as the documentation <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-job-compilation-console.html>. But I got this error:  \n\"Load Error: InputConfiguration: Exactly one .pb file is allowed for Tensorflow models.\"  \nI checked the tar.gz file that there is only one .pn file. What caused the error?  \nThank you very much!",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Free tier usage Sagemaker",
        "Question_created_time":1548335489000,
        "Question_last_edit_time":1668454454978,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWzc5HuoeQZOuo79E8SQwxg\/free-tier-usage-sagemaker",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":326,
        "Question_answer_count":3,
        "Question_body":"Hi, I have a question about the free tier usage of AWS Sagemaker.   \nIt says on the free tier page:   \n  \nAmazon SageMaker  \n250 Hours  \nper month of t2.medium notebook usage for the first two months  \n  \nDoes this mean that you have to start using Sagemaker right away when you start your account or your two months will be void? Or does this mean that the first two months you're using Sagemaker, up to 250 hours per month of t2.medium notebook usage is free?  \n  \nI really just want to test something but I have to know if I can still do this for free. I've had my account since september 2018 so >2 months.   \n  \nThe FAQ page that's referred to about expiring offers FREE TRIAL (What's the difference between expiring and non-expiring offers? VIEW FAQs >> https:\/\/aws.amazon.com\/free\/faqs\/?ft=n) doesn't give an answer to my question.   \n  \nThanks in advance.",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Maximising multi-instance and multi-GPU utilisation",
        "Question_created_time":1548267645000,
        "Question_last_edit_time":1667925986518,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJdZhEsN_QTWOcvWD7tfcLA\/maximising-multi-instance-and-multi-gpu-utilisation",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":25,
        "Question_answer_count":2,
        "Question_body":"Hi,  \n  \nI've been trying to distribute the MNIST example across instances and GPUs with SageMaker Tensorflow, but I'm not seeing the kind of benefit that I was hoping for. I'm not sure whether I'm just setting the job up incorrectly or whether this example is just not suited to distribution and was wondering if anyone has any ideas which it might be please?  \n  \nI'm using TensorFlow 1.9 because I think MPI\/Horovod doesn't work with 1.12 which I was using originally?  \n  \nIn my test I get these results with a batch size of 512:  \n1 Instance 1 GPU   13.5 global_step\/sec  \n2 Instance 1 GPU    7.3 global_step\/sec  \n1 Instance 8 GPU   18.8 global_step\/sec  \n  \nIf I reduce the batch size I get lower overall throughput, and see little benefit when increasing beyond 512.  \n  \nMy job specification looks as below and mnist.py is the file which comes with the examples, though I changed **batch_size=100** to **batch_size=512** in the script:\n\n```\nestimator = TensorFlow(entry_point='mnist.py',\r\n                  role=role,\r\n                  framework_version='1.9.0',\r\n                  training_steps=1250, \r\n                  evaluation_steps=10,\r\n                  train_max_run=5*60,    \r\n                  output_path=output_location,\r\n                  checkpoint_path=output_location,\r\n                  code_location=output_location,\r\n                  model_dir=output_location,\r\n                  train_instance_count=1,\r\n                  train_instance_type='ml.p3.16xlarge',\r\n                  base_job_name='PerformanceTest-p3-16xlarge-1-instance',\r\n                  distributions={\r\n                    'mpi': {\r\n                      'enabled': True,\r\n                      'processes_per_host': 8,\r\n                      'custom_mpi_options': '--NCCL_DEBUG INFO'\r\n                    }\r\n                  })\n```\n\nWhat I was hoping to see was a single 8-GPU instance hitting global_step\/sec of 70.2-97.2. Based on 8x the global_step\/sec of a single instance, scaled with 60-90% efficiency. Any help or clarification on this would be greatly appreciated!  \n  \nThanks,  \n  \nCarl",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"ClientError: object_detection_augmented_manifest_training template",
        "Question_created_time":1547563664000,
        "Question_last_edit_time":1668624099053,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/clienterror-object-detection-augmented-manifest-training-template",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":123,
        "Question_answer_count":4,
        "Question_body":"Hello,  \n  \nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n```\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n```\n\nI now have 2 output manifest files with many lines of this:\n\n```\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n```\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.  \n  \nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to _'application\/x-image'_ with _Record wrapper type:RecordIO_ : **'ClientError: train channel is not specified.'**  \n  \nI then changed the channel to train_annotation instead of train and I receive this error message: **\"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property**  \n  \nAdditional information can be provided if neccessary.  \nAny help would be much apreciated! Thank you.  \n  \nEdited by: LuciA on Jan 16, 2019 1:12 PM  \n  \nEdited by: LuciA on Jan 16, 2019 1:18 PM  \n  \nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1549563627000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1549563627000,
        "Answer_comment_count":0.0,
        "Answer_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?   \n  \nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"How to use an Augmented Manifest File for AWS SageMaker Ground Truth?",
        "Question_created_time":1546562747000,
        "Question_last_edit_time":1668535733171,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1LLbT-AYQDO-XXrjPUFl9w\/how-to-use-an-augmented-manifest-file-for-aws-sagemaker-ground-truth",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":241,
        "Question_answer_count":1,
        "Question_body":"Hey,  \n  \nI'm trying to use Ground Truth to do image classification but with a different set of label options for each image. I have the custom labeling task template and pre-\/post-labeling Lambda functions set up and I figured I could pass in the labels through the manifest file.  \n  \nMy issue is that the Ground Truth job ignores the attributes in the manifest file that are not \"source-ref\" (or \"source\"). This causes the pre-processing Lambda function to fail because the request it is passed only contains the \"source-ref\" attribute, but the Lambda function also references a different attribute. Are augmented manifest files supported for Ground Truth and if they are, how can I make use of the extra attributes?  \n  \nReferences:  \nGround Truth Input Data: <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html>  \nSageMaker Augmented Manifest Files: <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html>  \n  \nExample:  \n  \nA normal Ground Truth manifest file:\n\n```\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\"}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\"}\r\n...\n```\n\nWhat I want to be able to use:\n\n```\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\",\"labels\":[\"pen\",\"pencil\",\"stick\"]}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\",\"labels\":[\"tv\",\"laptop\",\"phone\"]}\r\n...\n```\n\n",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1546888840000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1546888840000,
        "Answer_comment_count":1.0,
        "Answer_body":"Hi sageuser, I'm an engineer at AWS. Augmented manifests are not supported for custom workflows, and so it is not possible to pass through additional parameters, e.g., \"labels\" in your example. We appreciate that you are using the service and welcome customer feedback. We can always be reached at https:\/\/aws.amazon.com\/contact-us\/.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"why SageMaker Ground Truth automated labelling?",
        "Question_created_time":1544611168000,
        "Question_last_edit_time":1667965191088,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp6wm80kUT0GZJp8meQtgTg\/why-sagemaker-ground-truth-automated-labelling",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":108,
        "Question_answer_count":1,
        "Question_body":"Hi, I'm confused with the automated labelling feature of SM. Usually when people label things it is to train their own models afterwards. Is the goal of this feature to replace the downstream ML model that would use the labelled dataset? some sort of code free computer vision system?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1544632081000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925583503,
        "Answer_comment_count":0.0,
        "Answer_body":"Automated data labeling is labeling of data using machine learning. Amazon SageMaker Ground Truth will first select a random sample of data and send it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Accessing SageMaker Notebooks without accessing the console",
        "Question_created_time":1543947347000,
        "Question_last_edit_time":1668556320118,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sagemaker-notebooks-without-accessing-the-console",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":481,
        "Question_answer_count":1,
        "Question_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\n**Do we have a best practice for that?**\nIn the [`create-presigned-notebook-instance-url`][1] command, what is the `--session-expiration-duration-in-seconds`: is it the validity duration of the URL or the max session duration once the URL has been clicked?\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1543951736000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925565195,
        "Answer_comment_count":0.0,
        "Answer_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form:\n`https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>`\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of  `[1800, 43200]`  in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Question_created_time":1541494962000,
        "Question_last_edit_time":1668438538513,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sagemaker-notebook-through-vpc-interface-endpoint",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":466,
        "Question_answer_count":1,
        "Question_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n>\"*You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.*\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1541516577000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925550451,
        "Answer_comment_count":0.0,
        "Answer_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Question_created_time":1540384039000,
        "Question_last_edit_time":1668610211874,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1128,
        "Question_answer_count":1,
        "Question_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download? \n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location,\n                      train_instance_count=1, train_instance_type='ml.p3.8xlarge',\n                     train_volume_size=300, train_max_run = 5*24*60*60 ,\n                     security_group_ids='sg-00f1529adc4076841')\n\nThe output was:\n2018-10-18 23:27:15 Starting - Starting the training job...\nLaunching requested ML instances......\nPreparing the instances for training...\n2018-10-18 23:29:15 Downloading - Downloading input data............\n....................................................................\n....................................................................\n....................................................................\n2018-10-19 00:23:50 Training - Downloading the training image..\n \nDataset download took ~54mins",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1540622900000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925589026,
        "Answer_comment_count":0.0,
        "Answer_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT?\nIf they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use [pipe input](https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/), pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Question_created_time":1539775195000,
        "Question_last_edit_time":1668613505334,
        "Question_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloudformation-with-sagemaker-lifecycleconfig-without-leaving-the-instance-running",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":479,
        "Question_answer_count":1,
        "Question_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation.  I have some initialization to do at creation time (clone a github repo, etc.).  That all works fine.  The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a `Stopped` state.  A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\n```\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n```\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige.  I would have thought it would let me do the same in the lifecycle config.  Unfortunately, SageMaker still has the notebook in the `Pending` state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1539777191000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925548143,
        "Answer_comment_count":0.0,
        "Answer_body":"One solutions will be to create a [CFN custom resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources-lambda.html) backed by lambda.\nYou can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"SageMaker MXNet local mode not working",
        "Question_created_time":1537534843000,
        "Question_last_edit_time":1668119355545,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sagemaker-mxnet-local-mode-not-working",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":259,
        "Question_answer_count":1,
        "Question_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\n    bucket = 'XXXXXXXXXXX'\n    prefix = 'sagemaker\/cifar-bench\/data'\n    \n    inputs = sagemaker_session.upload_data(\n        path='data',\n        bucket=bucket, \n        key_prefix=prefix)\n    \n    print('data sent to ' + inputs)\n    \n    \n    Inception = MXNet('gluon_cifar_net.py', \n              role=role, \n              train_instance_count=1, \n              train_instance_type='local_gpu',\n              framework_version='1.2.1',\n              base_job_name='cifar10-inception-',\n              hyperparameters={'batch_size': 256, \n                               'optimizer': 'sgd',\n                               'epochs': 100, \n                               'learning_rate': 0.1, \n                               'momentum': 0.9})\n    \n    \n    Inception.fit(inputs)\n\nwhich returns an `OSError: [Errno 2] No such file or directory`\n\nIn the error log I can see that there seems to be error at `self.latest_training_job = _TrainingJob.start_new(self, inputs)` and `self.sagemaker_client.create_training_job(**train_request)`\n\nHow can I make the local mode work?\n",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1537550695000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925587141,
        "Answer_comment_count":0.0,
        "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Problem Sagemaker and Spark",
        "Question_created_time":1536801534000,
        "Question_last_edit_time":1668622523553,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3gi1LMvEQNiiGlYooXnUQA\/problem-sagemaker-and-spark",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":1140,
        "Question_answer_count":2,
        "Question_body":"Hi there,  \n  \nI followed this tutorial to set up Sagemaker Notebook with Spark (EMR): https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/  \n  \nI launched a notebook with sparkmagic (pyspark3) and tried to call the Spark context but got the following error:  \n\"\"\"  \nThe code failed because of a fatal error:  \n\tInvalid status code '400' from http:\/\/xxx.xx.xx.xx:8998\/sessions with error payload: \"Invalid kind: pyspark3 (through reference chain: org.apache.livy.server.interactive.CreateInteractiveRequest\\[\\\"kind\\\"])\".  \n  \nSome things to try:  \na) Make sure Spark has enough available resources for Jupyter to create a Spark context.  \nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.  \nc) Restart the kernel.  \n\"\"\"  \n  \nAnyone encountered the same issue?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_last_edit_time":null,
        "Answer_comment_count":null,
        "Answer_body":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_self_resolution":null
    },
    {
        "Question_title":"Sagemaker and Data on Databases",
        "Question_created_time":1533317474000,
        "Question_last_edit_time":1668289581924,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":348,
        "Question_answer_count":2,
        "Question_body":"A customer has a question about data sources \n\n> \u201cmost of our data is stored in SQL databases, while the SageMaker docs\n> say that I have to put it all in S3. It\u2019s not obvious what the best\n> way to do this is. I can think for example of splitting my analysis\n> code in two; one pre-processing step to go from SQL queries to tabular\n> data, and e.g. store that as Parquet files. For high-dimensional\n> tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1533318766000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925552580,
        "Answer_comment_count":0.0,
        "Answer_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Question_created_time":1533133401000,
        "Question_last_edit_time":1668556657875,
        "Question_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtimeerror-giving-up-endpoint-didn-t-launch-correctly",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":431,
        "Question_answer_count":1,
        "Question_body":"Hi,\nI am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n    # get trained model from s3\n    trained_S2S = SM.model.Model(\n        image=seq2seq,\n        model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n            + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n        role=role) \n    \n    S2S = trained_S2S.deploy(1, instance_type='local')    \n\nI get the following error (several hundreds of lines repeated):\n\n    WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n`RuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly`",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1533135863000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925544815,
        "Answer_comment_count":0.0,
        "Answer_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker batch transform 415 error",
        "Question_created_time":1532625720000,
        "Question_last_edit_time":1668556023886,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":568,
        "Question_answer_count":1,
        "Question_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\n    transformer = XGB.transformer(\n        instance_count=2, instance_type='ml.c5.9xlarge',\n        output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n        max_payload=100)\n\n    transformer.transform(\n        data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n        split_type='Line')\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\n    Bad HTTP status returned from invoke: 415\n    'NoneType' object has no attribute 'lower'\n\nDid I forget something in my batch transform settings?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1532634125000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925589052,
        "Answer_comment_count":0.0,
        "Answer_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things: \n\n1. Try running the algorithm on the original data using the `\"SplitType\": \"Line\"` and `\"BatchStrategy\": \"MultiRecord\"` arguments and see if you have better luck.\n2. Look in the cloudwatch logs for your run and see if  there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Sagemaker batch transform",
        "Question_created_time":1532619204000,
        "Question_last_edit_time":1668425051631,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":1,
        "Question_view_count":376,
        "Question_answer_count":1,
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads\nI'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Poster_isAwsEmployee":true,
        "Poster_isModerator":false,
        "Poster_isExpert":true,
        "Poster_isCse":false,
        "Question_closed_time":1532628166000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925544636,
        "Answer_comment_count":0.0,
        "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use `\"SplitType\": \"Line\"` and `\"BatchStrategy\": \"MultiRecord\"`. There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_created_time":1527798496000,
        "Question_last_edit_time":1668616977193,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sagemaker-that-receive-a-non-zero-exit-status",
        "Question_score_count":0,
        "Question_favorite_count":0,
        "Question_comment_count":0,
        "Question_view_count":744,
        "Question_answer_count":1,
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\n    Warning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n    \u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\n    Making 'packages.html' ... done\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\n    yum install r-cran-rjava\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Poster_isAwsEmployee":false,
        "Poster_isModerator":false,
        "Poster_isExpert":false,
        "Poster_isCse":false,
        "Question_closed_time":1527821535000,
        "Answer_score_count":0.0,
        "Answer_last_edit_time":1667925576348,
        "Answer_comment_count":0.0,
        "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n`$ yum install r-cran-rjava`\n\nyou can try:\n\n`$ sudo yum install r-cran-rjava`\n\nwhich will allow you to impersonate the superuser (ie. root) for that command [1]\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results [2])\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n`$ sudo yum install -y R-java-devel.x86_64`\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n`$ sudo yum install -y libxml2-devel` [3]\n\nAfter which you can then open R (either as root user...)\n\n`$ sudo R`\n\nor personal\/local user\n\n`$ R`\n\nand execute the package installation:\n\n`> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")`\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n`$ sudo find \/ -iname libgomp.spec`\n\nwhich should be located at **\/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec** -- if so, you can manually create symlinks to fix this:\n\n```\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n```\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n`$ ls \/usr\/lib64\/libgomp*`\n\nOnce confirmed, you can run the install.package('rJava') command.\n\n[1]:https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/install-software.html\n[2]:https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/find-software.html\n[3]:https:\/\/stackoverflow.com\/questions\/29014062\/unable-to-install-xml-package-in-r-on-centos#29014363",
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_self_resolution":0.0
    }
]