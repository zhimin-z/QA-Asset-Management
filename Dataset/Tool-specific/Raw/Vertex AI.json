[
	{
		"Question_title": "403 error when trying to access PalmAPI",
		"Question_created_time": "2023-05-15T19:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/403-error-when-trying-to-access-PalmAPI/td-p/553667/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 32,
		"Question_body": "Dear community,I got an invite to try out PalmAPI through makesuite, but unfortunately it is showing me the following error: The account <my-email>@gmail.com doesn't have permission to see this page.\nContact your admin for access or use a different account.is there something I can do to fix this? note that I am trying to log in with the account I got the invite for. appreciate all the insights",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-20T20:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I got the same error. I was also invited but seeing the same error after logging in."
			}
		]
	},
	{
		"Question_title": "Safe Search Vision moderations API for detecting child porn",
		"Question_created_time": "2023-05-20T19:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Safe-Search-Vision-moderations-API-for-detecting-child-porn/td-p/595139/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 6,
		"Question_body": "Hello everyone,We just started using the Google Safe Search Vision API to provide image upload moderation on our platform.At first, we moderated images flagged as \"adult\", \"medical\" and \"racy\".Then, we realized \"Racy\" is a little too strict, it easily rejects images with fully clothed cleavage (perhaps because it was a drawing maybe, I don't know). So we started allowing \"racy\".I want to know what category \"child porn\" and \"inappropriately dressed children\" images are flagged as. We had one instance where a user uploaded child porn images that could have been interpreted as a scantily clad pre-teen/baby. Will images like that be flagged as \"adult\" or \"racy\"?I want to make sure that by denying images moderated as \"adult\" (VERY_LIKELY), that we will be able to detect & filter out images that are:1. Child porn that includes sex\n2. Child porn that includes no sex but inappropriately dressed children\n3. Nude photos of children in any pose (even if it's something like a baby being powdered or diapered)\n4. Clothed children in inappropriate poses\n\nFurthermore, I'm curious if there's an option to detect \"offensive\" images such as violent vomiting or literal toilet/diarrhea photos, etcetera. Which of the existing labels would detect those kind of images (if any)? If those images are not detectable with Cloud Vision Safe Search API, what are our best options?Thank you for your help, as this is very important to us.I look forward to your reply.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-20T19:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello everyone,\n\nWe just started using the Google Safe Search Vision API to provide image upload moderation on our platform.\n\nAt first, we moderated images flagged as \"adult\", \"medical\" and \"racy\".\n\nThen, we realized \"Racy\" is a little too strict, it easily rejects images with fully clothed cleavage (perhaps because it was a drawing maybe, I don't know). So we started allowing \"racy\".\n\nI want to know what category \"child porn\" and \"inappropriately dressed children\" images are flagged as. We had one instance where a user uploaded child porn images that could have been interpreted as a scantily clad pre-teen/baby. Will images like that be flagged as \"adult\" or \"racy\"?\n\nI want to make sure that by denying images moderated as \"adult\" (VERY_LIKELY), that we will be able to detect & filter out images that are:\n\n1. Child porn that includes sex\n2. Child porn that includes no sex but inappropriately dressed children\n3. Nude photos of children in any pose (even if it's something like a baby being powdered or diapered)\n4. Clothed children in inappropriate poses\n\nFurthermore, I'm curious if there's an option to detect \"offensive\" images such as violent vomiting or literal toilet/diarrhea photos, etcetera. Which of the existing labels would detect those kind of images (if any)? If those images are not detectable with Cloud Vision Safe Search API, what are our best options?\n\nThank you for your help, as this is very important to us.\n\nI look forward to your reply."
			}
		]
	},
	{
		"Question_title": "I want to build bard tools",
		"Question_created_time": "2023-05-13T05:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/I-want-to-build-bard-tools/td-p/553119/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "Hello Google \ud83d\udc4bI'm one of the first 40 approved plugin developers at gpt and am currently in talks with 5 Fortune 200 companies to bring their site online as a plugin, having made functioning demos for all use cases I am a firm believer that this is the future of the web.After Google IO, I'm eager to build for Bard tools.Please point me in the right direction. Much appreciated \ud83e\udd70Alexander",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-20T14:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you tell more about what plugin did you build for ChatGPT?\n\nI'm working on a marketplace where builders and prompt engineers could showcase, promote and sell as a service their AI apps.\n\nMaybe we could partner, I think we could list your both Bard and ChatGPT plugins."
			}
		]
	},
	{
		"Question_title": "Vertex AI custom training job never finished",
		"Question_created_time": "2023-05-16T23:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-custom-training-job-never-finished/td-p/554143/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 59,
		"Question_body": "Hi, I am trying to implement custom model training on Vertex AI.I pushed custom training job to Vertex AI and met the freezing issue at the trainer.train()  process.  Command:\n```gcloud ai custom-jobs create \\\n--region=us-west1 \\\n--display-name=test_model \\\n--worker-pool-spec=machine-type=n1-highmem-2,accelerator-type=NVIDIA_TESLA_T4,accelerator-count=1,replica-count=1,executor-image-uri=asia-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13:latest, local-package-path=src, script=task.py```Version```image: asia-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13:latesttransformers v4.28.0  (changed to other versions but got the same freeze issue)```Log in Logging is below without error messages.```Map: 0%| | 0/72 [00:00<?, ? examples/s]",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T18:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@lingshu,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, but you can try validating the following if it will solve your problem:\u00a0\n\n1. Verify if your script and dependencies are compatible with the PyTorch version as well as the libraries that you are using in the script. In your environment, verify if the versions and the required packages are specified correctly.\n\n2. Verify if you have not exceeded the quotas, you can use this link for more information about quotas and limits:\u00a0https://cloud.google.com/vertex-ai/docs/quotas#custom-trained_model_quotas\n\nBut I highly recommend that you reach out to Google Cloud Support for this inquiry as engineers have better visibility to your logs:\u00a0https://cloud.google.com/support\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "Unable to deploy model to endpoint",
		"Question_created_time": "2023-05-17T01:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-deploy-model-to-endpoint/td-p/554155/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 153,
		"Question_body": "Hi,I am trying to deploy a simple model on an endpoint in order to start making predictions.I followed these steps:1. Create the model, create a docker image, push it into Artifact Registry2. Upload the model to Vertex AI:  3. Create an endpoint on Vertex AI:  4. Deploy the model to the endpoint:  Here's the message:Model server terminated: model server container terminated: exit_code: 0 reason: \"Completed\" started_at { seconds: 1684309447 } finished_at { seconds: 1684309449 } . Model server logs can be found at xxxxxWhen i check the logs: As you can see i have no errors.What i tried:- upsized the machine-type - deployed with a python script - deployed manually using the portal  - changed my model to a simple linear regression to test  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T16:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@zied_gobji,\n\nWelcome to Google Cloud Community!\n\nThis issue might have been caused by several reasons, it might be due to the model being not deployed correctly or the endpoint was not created. You can check the following solutions if it will work on your end:\n\n1. Try creating a new endpoint and try deploying it there also verify that the model exists.\n\n2. Please note that when you create a model\u00a0resource, the size of the artifacts that you've specified\u00a0in the artifactUri must be\u00a010 GB or less.\u00a0You can check this link for more information: https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#maximum_model_size\n\n3. Make sure also that you are meeting the container requirements. You can check this link for more information:\u00a0https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#variables\n\nBut I highly suggest that you reach out to Google Cloud Support for further investigation in your case:\u00a0https://cloud.google.com/support\n\nHope this will help!"
			}
		]
	},
	{
		"Question_title": "building a custom jupyter notebook container that supports python 3.10",
		"Question_created_time": "2023-05-18T09:04:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/building-a-custom-jupyter-notebook-container-that-supports/td-p/554623/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 27,
		"Question_body": "the off the shelf notebooks only support python 3.7. This doesn't work for us as many of the libraries we use require a minimum of python 3.8 or higher.Is there a way to create a customer notebook where we can configure the correct environment for our needs?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T14:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@anannra,\n\nWelcome to Google Cloud Community!\n\nYou can try using a custom image when creating the notebook, I think this would be the ideal solution in your situation. By doing this, you can install any necessary dependencies and the version of Python that you need. You can use these guides:\u00a0https://cloud.google.com/vertex-ai/docs/workbench/managed/custom-container\nhttps://cloud.google.com/vertex-ai/docs/workbench/user-managed/custom-container\n\nYou can also check these guides as they might be helpful to your case but please note that these are third party links and Google doesn't support them.\nhttps://stackoverflow.com/questions/57322838/requirements-for-launching-google-cloud-ai-platform-not...\nhttps://stackoverflow.com/questions/58697538/how-do-you-set-up-a-python-3-8-kernel-on-gcps-ai-platfo...\n\nIn case, you haven't built your container, you can use this link as a guide:\u00a0https://cloud.google.com/build/docs/building/build-containers\n\nAdditionaly, a feature request was already submitted which is to upgrade the python version to 3.8 and above, you can track the request using this link:\u00a0https://issuetracker.google.com/issues/235826369\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "Document Ai to csv document",
		"Question_created_time": "2023-05-18T13:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-Ai-to-csv-document/td-p/554699/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 15,
		"Question_body": "Hi,I am using Document AI to extract data from a pdf file. However, the result is a json file. What can I do to get a csv file?Thank you",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T13:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@user1234,\n\nWelcome to Google Cloud Community!\n\nIf you have a response that is a table similar to HTML table structure (You can check the sample JSON representation of the table using this link:\u00a0https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#table)\nIt is possible to use the Document AI Toolbox which is an SDK for Python used to manipulate and manage the document response, it can help you convert the response to Pandas CSV file.\u00a0You can check this sample code:\u00a0https://cloud.google.com/document-ai/docs/samples/documentai-toolbox-table#code-sample\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "Google Cloud Vision API reports (x,) in boundingBox when (x,y) is expected",
		"Question_created_time": "2023-05-19T00:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Vision-API-reports-x-in-boundingBox-when-x-y-is/td-p/554811/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 19,
		"Question_body": "I have this piece of response where (x,y) information for 4-points of the bounding box is expected but you'll see a point where only (x,) is reported.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T12:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@faraday,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, you can check these solutions if it will solve your problem:\u00a0\n\n1. You are not following the right format. By default, Vision API only supports several image types:\u00a0\n\nJPEG\nPNG8\nPNG24\nGIF\nAnimated GIF (first frame only)\nBMP\nWEBP\nRAW\nICO\nPDF\nTIFF\n\nPlease note that some image formats may reduce the Vision API accuracy since they are \"lossy\" (e.g. JPEG).\nYou can check this link for more information:\u00a0https://cloud.google.com/vision/docs/supported-files#file_formats\n\u00a0\n2. The image must be at least\u00a0640 x 480 pixels in order to accurately detect the image. Vision API Features have recommended sizes. You can check this link to learn more:\u00a0https://cloud.google.com/vision/docs/supported-files#file_formats\n\n3. File Size is also a factor, it must not exceed 20 MB. You check this link for more information:\u00a0https://cloud.google.com/vision/docs/supported-files#file_size\n\n4. It is also possible that the y value may be equal to 0. If this is the case, the y value will not be included in the response.\u00a0\n\nYou can also reach out to Google Cloud Support:\u00a0https://cloud.google.com/support\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "Invoice parser \"invents\" non-existing string for supplier_name normalized value",
		"Question_created_time": "2023-05-18T06:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Invoice-parser-quot-invents-quot-non-existing-string-for/td-p/554570/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 29,
		"Question_body": "While processing a dummy invoice taken from the internet, I have found that invoice parser returns normalized value for supplier_name with a wrong string that is not found anywhere on the image.Expected normalized value: Stanford Plumbing & HeatingProposed value: Stanford University  Invoice header looks like belowI wonder if I should consider this a bug or a feature? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T12:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, Yes you are correct. Although the value is incorrect, this is a feature called Enrichment and Normalization (this is with the intention to reduce post processing for the API)\u00a0 for more details about this, please visit the link about this feature[1].Also I would recommend contacting Google Support for them to look in to this and improve the quality of capturing for this object[2].\n\n[1]https://cloud.google.com/document-ai/docs/ekg-enrichment\n\n[2]https://cloud.google.com/contact"
			}
		]
	},
	{
		"Question_title": "Submit Job But Never Ran",
		"Question_created_time": "2023-05-19T10:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Submit-Job-But-Never-Ran/td-p/554999/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 17,
		"Question_body": "I am trying to test a bash script on the cloud, but when I submit it, it repeatedly switch between SCHEDULE and QUEUE and never ran. Here is the bash script that I am trying to run:   I config my job as follows:   Both run.sh and config.json is in the same directory. I ran:\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json   ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-19T10:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am trying to test a bash script on the cloud, but when I submit it, it repeatedly switch between SCHEDULE and QUEUE and never ran. Here is the bash script that I am trying to run:\u00a0\n\n\u00a0\n\n#!/bin/bash\n\n\nsource /opt/conda/etc/profile.d/conda.sh\nconda activate WaifuDiffusion\n\nexport FOLDER=/home/kannachan/SeaSalt-Downloader\n\npython $FOLDER/main.py -u \"https://danbooru.donmai.us/posts?tags=mika_pikazo\" \\\n       --scraper danbooru \\\n       --filter tag_filter animated video \\\n       --saver folder mika_pikazo       \n\n\u00a0\n\n\nI config my job as follows:\n\n\u00a0\n\n{\n    \"taskGroups\": [\n        {\n            \"taskSpec\": {\n                \"runnables\": [\n                    {\n                       \"script\": {\n\t\t\t      \"text\": \"#!/bin.bash\\n bash ../run.sh\" \n\t\t\t}\n                    }\n                ],\n                \"computeResource\": {\n                    \"cpuMilli\": 2000,\n                    \"memoryMib\": 16\n                },\n                \"maxRetryCount\": 2,\n                \"maxRunDuration\": \"3600s\"\n            },\n            \"taskCount\": 4,\n            \"parallelism\": 2\n        }\n    ],\n    \"allocationPolicy\": {\n        \"instances\": [\n            {\n                \"policy\": { \"machineType\": \"g2-standard-8\" }\n            }\n        ]\n    },\n    \"labels\": {\n        \"department\": \"ai\",\n        \"env\": \"web-scrapper\"\n    },\n    \"logsPolicy\": {\n        \"destination\": \"CLOUD_LOGGING\"\n    }\n}\n\n\u00a0\n\n\u00a0\n\nBoth run.sh and config.json is in the same directory. I ran:\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json\n\n\u00a0\n\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json"
			}
		]
	},
	{
		"Question_title": "Got Welcome email, but doesn't have permission Makersuite.google.com!",
		"Question_created_time": "2023-05-17T08:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Got-Welcome-email-but-doesn-t-have-permission-Makersuite-google/td-p/554306/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 62,
		"Question_body": "I got an email with the following content. But when I click on the \"Visit MakerSuite\", I get a message saying that my gmail account doesn't have permission to see this page.\n\nTried adding my email back into the waiting list, but got \"You're all set!\" page with the \"Get Started\" button. Clicking on \"Get Started\" results in the same error messageAny idea how to resolve this issue?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-17T14:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Same problem."
			},
			{
				"Answer_created_time": "2023-05-18T19:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Same issue, despite trying many times over the course of many hours. Also, I can find *zero* places to contact Google support for help with this."
			},
			{
				"Answer_created_time": "2023-05-19T06:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Same here"
			}
		]
	},
	{
		"Question_title": "chat-bison model not able to answer in closed context.",
		"Question_created_time": "2023-05-17T02:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/chat-bison-model-not-able-to-answer-in-closed-context/td-p/554175/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "I am using `chat-bison` model for chat-bot to answer question based on retrieved documents.  However, it answers questions from its own knowledge without caring about the context.Here is a small experiment, I tried.Example 2\nd=[{'context': 'Please answer the user question based on only the CONTEXT provided. Do not answer of your own. CONTEXT: Sachin Tendulkar is a cricketer. He was born is 1980.', 'messages': [{'author': 'user', 'content': 'When was Sachin Tendulkar born?'}]}]Answer by chat-bison model\nSachin Ramesh Tendulkar was born on April 24, 1973 in Mumbai, India. He is a former international cricketer who served as captain of the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket.Also, what are the values of `author` which depicts if its `user`, `bot`, `system`?@mchrestkha1  ",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\u00a0\n\nthe chat bison does not always give you accurate answers to understanding the context, so you can try to prepend the conversation history, including the user question and previous model responses, to the context before making an inference. This helped me a lot, and I was advised to do so by one of the reps at Triotech Systems.\u00a0 And for the values of the author field, they typically indicate the source of a particular message in the conversation. I hope it is clear or you can take help from some expert!"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I guess, the limitation ( Closed context) related to chat-bison is a known issue with this model."
			}
		]
	},
	{
		"Question_title": "Exceeded limit 'QUOTA_FOR_INSTANCES' on resource 'dataflow-tabular-stats-and-e .... Limit: 24",
		"Question_created_time": "2023-05-16T11:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Exceeded-limit-QUOTA-FOR-INSTANCES-on-resource-dataflow-tabular/td-p/553942/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 28,
		"Question_body": "I'm trying to run Vertex AI using the Multiclass3_mod.csv example and I get the error bellow:The same happens with bank-marketing.csv exampleAny help?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-18T12:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@ruimiguel,\n\nWelcome to Google Cloud Community!\n\nYou are encountering this error since you've reached the quotas and limits of Vertex AI in your project, quotas will restrict how much GCP resource your project can use. You can check this link to learn more about the other quotas and limits of Vertex AI:\u00a0https://cloud.google.com/vertex-ai/docs/quotas#text\n\nTo resolve this issue you can request for a quota increase using this guide but please note that this is subject to approval:\u00a0https://cloud.google.com/docs/quota_detail/view_manage#requesting_higher_quota\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "Receiving quota error when trying to use bison chat model in Vertex AI",
		"Question_created_time": "2023-05-11T07:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Receiving-quota-error-when-trying-to-use-bison-chat-model-in/td-p/552421/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 35,
		"Question_score_count": 6,
		"Question_view_count": 0,
		"Question_body": "Hi, I want to try out the new bison chat model. However, when I'm asking anything I'm receiving this error: Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: chat-bison. Please submit a quota increase request.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-11T10:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Me too.\n\nSubmitting a prompt on\u00a0https://console.cloud.google.com/vertex-ai/generative/language/create/text results in the following error\n\nFailed to submit prompt\n\nError message: \"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request.\"\n\nStatus: 429 Error code: 429\n\nTracking number: xxxxxxxxxxxxx\n\nI've tried to follow the docs on\u00a0Quotas and limits\u00a0but there seem to be no quotas for Generative AI.\n\nI'm on the GCP Free Trial if that is relevant. Unfortunately, this means I can't contact (paid) support."
			},
			{
				"Answer_created_time": "2023-05-11T22:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@oiwejdsd\u00a0can you try again as well and confirm that you can now access 30 queries per minute?\u00a0 Make sure you're using us-central1 region."
			},
			{
				"Answer_created_time": "2023-05-11T23:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@mchrestkha1\u00a0Thanks for your replies to this thread. My quota screen looks like @dashy's and is still at zero."
			},
			{
				"Answer_created_time": "2023-05-11T14:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Here are the quotas\u00a0https://cloud.google.com/vertex-ai/docs/quotas.\u00a0 When requesting a quota increase look for this metric in the filter 'aiplatform.googleapis.com/online_prediction_requests_per_base_model' .\u00a0 You should see quotas by region and model with default values."
			},
			{
				"Answer_created_time": "2023-05-11T15:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes, but all quotas a set to 0 and I can't request increase quota either. It mentions that I need to contact sales."
			},
			{
				"Answer_created_time": "2023-05-11T16:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "could you share a screenshot?"
			},
			{
				"Answer_created_time": "2023-05-11T19:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hey! I am seeing the same issue\u00a0\ud83d\ude14"
			},
			{
				"Answer_created_time": "2023-05-11T22:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@j_molina\u00a0can you try again and let us know if you can now see 30 queries per minute for us-central1 for the bison models.\u00a0 Your screenshot is showing AutoML services which are different."
			},
			{
				"Answer_created_time": "2023-05-11T21:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Here's a screenshot"
			},
			{
				"Answer_created_time": "2023-05-11T21:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Here is what my quotas look like in my paid account\n\n\n\nSince this is a preview service and Google is currently not charging to leverage this service, it may possibly be that Google has disabled this service for free tier accounts."
			},
			{
				"Answer_created_time": "2023-05-11T21:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Kolban, I'm using an enterprise account (no free trial). However, I just tried with my personal account and that have quotas and I can use the models. Not sure. Thanks for checking it out!"
			},
			{
				"Answer_created_time": "2023-05-11T21:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dashy,\n\nMaybe the account that shows 0 quota doesn't have IAM permissions to use Vertex AI but the other account does?"
			},
			{
				"Answer_created_time": "2023-05-11T21:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I was able to enable all Vertex AIs with no issues. Can you help me with the steps on how I can check whether Vertex AI is enabled on the IAM level?"
			},
			{
				"Answer_created_time": "2023-05-11T21:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "thanks for sharing! looking into it.\u00a0 should have a response tomorrow."
			},
			{
				"Answer_created_time": "2023-05-11T22:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@dashy\u00a0can you try again? I believe we increased to 30 queries per minute."
			},
			{
				"Answer_created_time": "2023-05-11T22:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's still at 0 for the enterprise account. For the individual account it's 30."
			},
			{
				"Answer_created_time": "2023-05-13T17:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's the same for my enterprise account. All the quotas are 0 and I can't request more."
			},
			{
				"Answer_created_time": "2023-05-12T00:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm having the same issue; could you increase my limit?"
			},
			{
				"Answer_created_time": "2023-05-13T18:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello all:\n\nI see the same 0 quota with the inability to change it. Says contact sales. My other google account at work does have the quota set to 30."
			},
			{
				"Answer_created_time": "2023-05-12T08:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have the same problem.\u00a0 I was not able even to try it.\nERROR.\u00a0Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: chat-bison\nI even don't have Vertex Service on the Quotas page.\u00a0\n\nCorrection. Vertex has appeared. Now just the quota is zero."
			},
			{
				"Answer_created_time": "2023-05-12T14:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm having similar issue.\u00a0 I created a free account to evaluate the process of finetuning a language model using Generative AI Studio.\u00a0 I have a valid dataset JSONL file and I'm set to use us-central 1, but no matter what I do, every attempt always fails in the pipeline at the 'large-language-model-tuning' step with error AiPlatformException: code=RESOURCE_EXHAUSTED messsage=The following quota metrics exceed quota limits.\u00a0 Oddly enough further down in the large complex error stack I notice in the path in the Execution name: that.../locations/europe-west4 is always mentioned even though all my settings are set to us-central1, even shows location as us-central1 in the Input Parameters.\n\nI guess my ask is;\u00a0 Is there some kind of error going on, or is using Generative AI Studio's Tuning feature just not something you are permitted evaluate in Free mode?"
			},
			{
				"Answer_created_time": "2023-05-14T22:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm having exactly the same issue with a number of my Cloud projects. No matter what I do I always end up with the error:\n\n\"Error message: \"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request.\"\""
			},
			{
				"Answer_created_time": "2023-05-14T23:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Same for us - in a project with billing attached where we're successfully using Vision AI and others"
			},
			{
				"Answer_created_time": "2023-05-15T00:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am having the same problem. I could not even try out the models. I just get:\u00a0\n\nAnd my quotas look like:"
			},
			{
				"Answer_created_time": "2023-05-15T08:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This same problem."
			},
			{
				"Answer_created_time": "2023-05-15T09:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I contacted Sales support, but they told me they can't help since this is a newly launched feature and referred me to technical support.\n\nI encourage everyone to raise this issue with technical support and also submit\u00a0Feedback on Generative AI Studio\u00a0such that we get heard."
			},
			{
				"Answer_created_time": "2023-05-15T09:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "thanks everyone for flagging.\u00a0 We're currently looking into it and will report back."
			},
			{
				"Answer_created_time": "2023-05-15T09:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Appreciate it\u00a0@mchrestkha1\u00a0\ud83d\ude4f"
			},
			{
				"Answer_created_time": "2023-05-15T19:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Me too. Is that mean free quota finished,"
			},
			{
				"Answer_created_time": "2023-05-16T11:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm having same issue/error. I have a paid account with billing account attached. And I looked at my quotas and I have \"Online prediction requests per base model per minute per region per base_model\" for \"text-bison\" set at\u00a030 in us-cental1. So what do I look at next to fix the error that's preventing me from exploring Gen AI Studio?"
			},
			{
				"Answer_created_time": "2023-05-16T11:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I submitted ~5 successful prompts, then started getting this error. Persisted through refresh, page changes, etc. However, it went away when I logged out and back in. Worth a try for those still having the issue.\n\nUpdate: the issue came back after only a couple more attempts, and now does not go with a logout cycle."
			},
			{
				"Answer_created_time": "2023-05-16T18:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "are you guys from europe?\u00a0\n\ni saw that google havent allowed people from some countrys to use palm2, maybe this is due to a restriction based on your account location... Im from Brazil and its also not in the Allowed countrys, and i am getting the same error.\n\nUp"
			},
			{
				"Answer_created_time": "2023-05-16T23:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes, I am from Europe, but I mean, I am not trying to deploy PALM in Europe. I am fine with it being deployed in the US, as I am serving users from the US. If I couldn't test these products, simply because I personally am sitting in Europe, that would be a bad decision by Google in my opinion then."
			},
			{
				"Answer_created_time": "2023-05-16T18:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Today (or yesterday), Google Cloud released several Google Cloud Skills Boost labs (Qwiklabs) that used the same notebooks and console instructions\u00a0 I was having issues with previously (so I don't think it was a quota issue). I got a similar error message when running notebook cells via GCSB, with a little phrase added \"\"Please try again later with backoff\".\u00a0 After consulting Bard on what that meant, Bard indicated that perhaps\u00a0Vertex AI service is experiencing temporary high demand. I eventually was able to run all the code cells and/or console activities, but had to wait and resubmit some. I suggest checking out Google Cloud Skills Boost for labs/quests on \"Generative AI\", to explore using Vertex AI's Generative AI Studio, I found it helpful."
			},
			{
				"Answer_created_time": "2023-05-18T11:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm having the same issue"
			}
		]
	},
	{
		"Question_title": "How to implement custom evaluation jobs (with custom metrics for custom learning tasks) in Vertex AI",
		"Question_created_time": "2023-05-18T08:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-implement-custom-evaluation-jobs-with-custom-metrics-for/td-p/554593/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 28,
		"Question_body": "From what I can see in GCP's official docs: https://cloud.google.com/vertex-ai/docs/evaluation/introductionEvaluation job comes with predefined metrics for predefined tasks. If I have a training task that's slightly more bespoke with metrics/ objective outside the scope of the available tasks/ metrics, how would I implement them?Examples of custom metrics are MMD or KL Divergence loss etc.My current thinking is:Many thanks in advance!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-18T08:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "From what I can see in GCP's official docs:\u00a0https://cloud.google.com/vertex-ai/docs/evaluation/introduction\n\nEvaluation job comes with predefined metrics for predefined tasks. If I have a training task that's slightly more bespoke with metrics/ objective outside the scope of the available tasks/ metrics, how would I implement them?\n\nExamples of custom metrics are MMD or KL Divergence loss etc.\n\nMy current thinking is:\n\nCreate a custom evaluation app in a docker container\nCreate a CustomJob that runs my evaluation app\nOutput the evaluation artefacts (e.g. results, plots) to GCS etc?\n\nMany thanks in advance!"
			}
		]
	},
	{
		"Question_title": "Cloud Retail User events from Google Tag Manager",
		"Question_created_time": "2023-05-18T07:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Retail-User-events-from-Google-Tag-Manager/td-p/554585/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 44,
		"Question_body": "I am reaching out to seek your assistance and guidance regarding an issue I have been experiencing while trying to send user events to Cloud Retail using Google Tag Manager.I have invested a significant amount of time searching for information in the community and other online forums, as well as exploring the provided documentation and videos. However, despite my efforts and multiple attempts, I have been unable to make this functionality work properly. I have come across responses that do not provide exact details about the underlying cause of the problem, making it even more challenging to find a solution.Furthermore, I have noticed that the documentation and videos do not adequately explain or provide a clear visualization of how the sent events are ultimately reflected in Cloud Retail. This lack of precise information hinders my ability to identify potential errors and follow the appropriate recommendations.I have conducted numerous thorough tests and tried every possible configuration, but the events simply do not get registered in Cloud Retail. Without being able to access this information, it is extremely difficult for me to gather data and obtain valuable recommendations to proceed with my implementation.Therefore, I would like to request your valuable assistance and guidance in this matter. Could you please provide additional support to address this specific issue? I would greatly appreciate any assistance you can offer to help me overcome these challenges and achieve a successful implementation of user events in Cloud Retail.Thank you in advance for your kind consideration and help.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-18T07:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am reaching out to seek your assistance and guidance regarding an issue I have been experiencing while trying to send user events to Cloud Retail using Google Tag Manager.\n\nI have invested a significant amount of time searching for information in the community and other online forums, as well as exploring the provided documentation and videos. However, despite my efforts and multiple attempts, I have been unable to make this functionality work properly. I have come across responses that do not provide exact details about the underlying cause of the problem, making it even more challenging to find a solution.\n\nFurthermore, I have noticed that the documentation and videos do not adequately explain or provide a clear visualization of how the sent events are ultimately reflected in Cloud Retail. This lack of precise information hinders my ability to identify potential errors and follow the appropriate recommendations.\n\nI have conducted numerous thorough tests and tried every possible configuration, but the events simply do not get registered in Cloud Retail. Without being able to access this information, it is extremely difficult for me to gather data and obtain valuable recommendations to proceed with my implementation.\n\nTherefore, I would like to request your valuable assistance and guidance in this matter. Could you please provide additional support to address this specific issue? I would greatly appreciate any assistance you can offer to help me overcome these challenges and achieve a successful implementation of user events in Cloud Retail.\n\nThank you in advance for your kind consideration and help."
			}
		]
	},
	{
		"Question_title": "Dialogflow ES API returns payload not found in Intents",
		"Question_created_time": "2023-05-18T03:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-ES-API-returns-payload-not-found-in-Intents/td-p/554537/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 16,
		"Question_body": "Hello,When calling the DialogFlowES agent via API for a query, it returns a payload that is not found in any of the intents. Any ideas on why this is happening? Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-18T03:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nWhen calling the DialogFlowES agent via API for a query, it returns a payload that is not found in any of the intents. Any ideas on why this is happening?\u00a0\n\nThanks!"
			}
		]
	},
	{
		"Question_title": "Moving (or copying) doc processors under a single project",
		"Question_created_time": "2023-05-16T07:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Moving-or-copying-doc-processors-under-a-single-project/td-p/553866/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 32,
		"Question_body": "Hello, we started exploring Doc AI by trying multiple processors under several projects. We are now consolidating them into dev, test and production projects, each with a version of the processors.Our goal is to first train the processors in the test project and then copy/move them into the production project.How can we copy over processors from one project to another? We are unable to find any documentation that would help us accomplish this.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-17T16:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@sivramk,\n\nWelcome to Google Cloud Community!\n\nYou can try importing a processor version using Document AI Workbench to a different project. The destination project\u00a0is where you will import the processor version while the\u00a0source project\u00a0is where the processor version resides. There are multiple requirements for this, you can check the following:\u00a0\n\n1. Processor types and schemas must match.\n2. Destination processer must be enabled and it can have existing versions/datasets.\n3. The version of the source processor must be in the following states: Deployed, Deploying, Undeployed and Undeploying.\u00a0\n\nIn order to avoid permission errors, you must add a\u00a0Document AI Editor permission to DocumentAI Service Agent.\u00a0\n\nYou can check this documentation to learn more:\u00a0https://cloud.google.com/document-ai/docs/manage-processor-versions#import\n\nHope this helps!"
			},
			{
				"Answer_created_time": "2023-05-18T01:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nYou can copy or move processors from one project to another in Doc AI, you can use the Google Cloud Console or the Cloud SDK command-line tools. To copy processors from one project to another in Doc AI, export the processor from the source project using gcloud doc-ai processors export, and then import it into the target project using gcloud doc-ai processors import. This is how we work at Triotech Systems."
			}
		]
	},
	{
		"Question_title": "Document AI OCR problem and IssueTracker no reply",
		"Question_created_time": "2023-05-03T20:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-OCR-problem-and-IssueTracker-no-reply/td-p/550130/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 8,
		"Question_score_count": 0,
		"Question_view_count": 198,
		"Question_body": "Hi,I have submitted reports on IssueTracker about Document AI OCR recognition errors, but the report has not been assigned yet.All the reports I've submitted are about OCR recognition errors, which are affecting my use. In addition, I've also submitted a suggestion report that can improve the accuracy of OCR.Please assigned my report and check my update error info.https://issuetracker.google.com/issues/280659979https://issuetracker.google.com/issues/277497068https://issuetracker.google.com/issues/280231466https://issuetracker.google.com/issues/280480137",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have identified dozens of OCR recognition errors, and all error samples are listed in these few issues.\n\nIf any staff members on the forum require error samples, please reply to me."
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi John,\u00a0\n\nI've responded to your bug. Our new OCR configurations should help with your language errors, please see the release notes for additional details:\u00a0\n\n\u00a0\n\nhttps://cloud.google.com/document-ai/docs/release-notes#March_27_2023"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @daleyj,\nThanks for your reply.\nI saw this, but since I usually only use it in \"console.cloud.google.com\", I hope there to be a language option added to the \"UPLOAD TEST DOCUMENT\" page as well.\nSo that users can choose one or more languages (while retaining the current automatic recognition language) included in the document to improve the accuracy of OCR.\n\nOh right, could you help me look at my error feedback? You can find the ZIP file with the error screenshots and the original image files in the two links below. I need to make sure that they are known by the OCR engineering team.\nhttps://issuetracker.google.com/issues/280902002\nhttps://issuetracker.google.com/issues/280659979"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@john-chen,\n\nThese issues were already assigned to the staff members, but I highly suggest that you reach out to google cloud support, especially if it is a blocker to your project. You can use this link to reach out to google cloud support:\u00a0https://cloud.google.com/support\n\nThank you for being part of Google Cloud Community"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@kvandres\u00a0\n\nThank you for your reply.\n\nI have tried to register for Standard Customer Service. But after I created an account, I could not log in. When I clicked on \"Contact Support Team\", it showed I could log in by verifying my mobile phone, but I couldn't get to that step. So it was a loop and I couldn't log in to my account.\n\nAlso, my IssueTracker posts have been assigned but the assigner has not replied that it has been forwarded to the Engineering Team. Do you know what's going on there?\n\nI observed some posts from the IssueTracker that were never replied to by Google personnel, so I'm worried I might face the same situation. I need to make sure my feedback will be replied to, even if it takes a few business days. Please help me."
			},
			{
				"Answer_created_time": "2023-05-17T08:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@john-chen\u00a0- I want to get you some support! We've referred your requests to a team to triaged. Have you been contacted by support yet?\n\n\u00a0\n\nRoderick G\nCommunity Manager\nGoogle Cloud Community"
			},
			{
				"Answer_created_time": "2023-05-17T18:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi sir,\n\nThe status of my four issues has consistently been either \"Assigned\" or \"Reassigned\", but no one has responded to me. Could you please help me check on the specifics?\n\nhttps://issuetracker.google.com/issues/281081916\nhttps://issuetracker.google.com/issues/281081915\nhttps://issuetracker.google.com/issues/281935756\nhttps://issuetracker.google.com/issues/280902002"
			},
			{
				"Answer_created_time": "2023-05-17T18:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThe status of my four issues has consistently been either \"Assigned\" or \"Reassigned\", but no one has responded to me. Could anyone please help me check on the specifics?\n\nhttps://issuetracker.google.com/issues/281081916\nhttps://issuetracker.google.com/issues/281081915\nhttps://issuetracker.google.com/issues/281935756\nhttps://issuetracker.google.com/issues/280902002"
			}
		]
	},
	{
		"Question_title": "Console Speech to text - transcription",
		"Question_created_time": "2023-05-15T12:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Console-Speech-to-text-transcription/td-p/553603/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 30,
		"Question_body": "When I give a 45 second mp3 file to the transcription program, this program generates only 15 seconds of transcription.  Why does it not transcribe the whole file ????",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-16T15:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@MASPEECHTOTEXT\u00a0,\n\nWelcome to Google Cloud Community!\n\nIf you are requesting thru API, verify if your file is less than 10 MB and less than 60 seconds. Also make sure that you are requesting in this endpoint since it is used for short audio files.\u00a0\n\nhttps://speech.googleapis.com/v1/speech:recognize\n\n\u00a0while for long audio files that are longer than 60 seconds, you can use this endpoint:\u00a0\n\nhttps://speech.googleapis.com/v1/speech:longrunningrecognize\n\nYou can check this link for more information:\u00a0https://cloud.google.com/speech-to-text/docs/sync-recognize\n\nAlso make sure that you are following the best practice when providing data. You can use this link for more information:\u00a0https://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "Google Bard API",
		"Question_created_time": "2023-03-29T12:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Bard-API/td-p/538517/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 20,
		"Question_score_count": 10,
		"Question_view_count": 0,
		"Question_body": "My request to join the Bard preview was recently accepted and I've begun evaluating it. So far it looks amazing and I'm anxious to begin working with it as a developer. I asked Bard if there was an API that I can start to use and it responded with the information below. I cannot seem to find the API waitlist sign-up page in the Bard website, nor can I enable the API from my Cloud Account (https://cloud.google.com/),\n\nIs there any additional action I should take to request to join the wait list for access to the Bard API? Perhaps I am too anxious and just need to wait. \ud83d\ude00\n\nThanks in advance for any advice.Response from Bard itself:\n\nYes, there is an API for Google Bard. It is currently in beta and is available to a limited number of users. You can sign up for the waitlist on the Google AI website.The Google Bard API allows you to access the power of Google's large language models to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.You can use the Google Bard API to create a variety of applications, including:To use the Google Bard API, you will need to create a Google Cloud Platform project and enable the Google Bard API. You will then need to create a service account and download the JSON key file.Once you have done this, you can start using the Google Bard API in your applications. You can find more information about the Google Bard API on the Google AI website.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-19T13:11:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@DeanS - thank you for your patience! We are excited to hear that you are ready to begin working with Google LLM API's as a developer.\u00a0\ud83d\ude0e Check out the information below:\u00a0\n\nEnterprise customers can sign up to Vertex AI LLM Trusted Testers program here:\u00a0https://cloud.google.com/ai/earlyaccess/join?hl=en\nIndependent developers can sign up the MakerSuite and PalmAPI waitlist here:\u00a0https://makersuite.google.com/waitlist.\n\u00a0\n\ud83d\ude80Register for the Google Cloud Innovators Program to stay informed on the latest updates and roadmap sessions.\n\u00a0\nI hope this helps. Thanks!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-02T10:55:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I followed the instructions, and I do not see Google Bard API being listed in the available services to enable for my GCP project."
			},
			{
				"Answer_created_time": "2023-04-03T09:13:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "What is the URL for the Google Bard API waiting list?"
			},
			{
				"Answer_created_time": "2023-04-03T09:22:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Same here. I've been waiting. I got the beta access when Bard first launched and been using since the first day. I wish we find a way to get access to API"
			},
			{
				"Answer_created_time": "2023-04-04T02:30:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Is it possible that the Bard API is accessed through the Vertex AI API?"
			},
			{
				"Answer_created_time": "2023-04-19T13:26:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "looks like for enterprise customers it is through Vertex AI LLM."
			},
			{
				"Answer_created_time": "2023-04-04T03:42:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "+1"
			},
			{
				"Answer_created_time": "2023-04-04T05:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "When and how do we access the Google Bard API?"
			},
			{
				"Answer_created_time": "2023-04-19T05:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I would also like access to the Bard API !"
			},
			{
				"Answer_created_time": "2023-04-19T13:11:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@DeanS - thank you for your patience! We are excited to hear that you are ready to begin working with Google LLM API's as a developer.\u00a0\ud83d\ude0e Check out the information below:\u00a0\n\nEnterprise customers can sign up to Vertex AI LLM Trusted Testers program here:\u00a0https://cloud.google.com/ai/earlyaccess/join?hl=en\nIndependent developers can sign up the MakerSuite and PalmAPI waitlist here:\u00a0https://makersuite.google.com/waitlist.\n\u00a0\n\ud83d\ude80Register for the Google Cloud Innovators Program to stay informed on the latest updates and roadmap sessions.\n\u00a0\nI hope this helps. Thanks!"
			},
			{
				"Answer_created_time": "2023-04-19T13:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for sharing this info, I have joined the waitlist and also signed up for Google I/O. I live close so I can attend in person.\n\nI have a simple question.\n\nSo it is under PalmAPI and not called as Bard API?\u00a0\n\n\u00a0\n\nThanks,\n\nVivek"
			},
			{
				"Answer_created_time": "2023-04-19T13:37:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@justvivek\u00a0-\u00a0\n\nCorrect - it is not called Bard API. Thanks!"
			},
			{
				"Answer_created_time": "2023-04-19T13:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks, very eager to wait to get access and try it out."
			},
			{
				"Answer_created_time": "2023-05-16T11:21:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "i am using Bard as beta\u00a0\n\nand, it says that Bard's API is limited to some beta users\u00a0 only\u00a0\n\nis it true?"
			},
			{
				"Answer_created_time": "2023-04-19T13:21:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "This is amazing news! Thanks so much, Willie. YES!!!"
			},
			{
				"Answer_created_time": "2023-04-19T13:37:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Happy to help!"
			},
			{
				"Answer_created_time": "2023-04-24T12:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for sharing above info ,\nHow to Join\u00a0\u00a0Bard preview version"
			},
			{
				"Answer_created_time": "2023-04-24T12:55:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Now it is public so you can access using\u00a0https://bard.google.com/\n\nBefore to this, I received an email to join the beta or preview of bard. I believe that is because I am also part of Pixel Superfan community.\u00a0\n\n\ud83d\ude42"
			},
			{
				"Answer_created_time": "2023-04-25T02:16:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the info"
			},
			{
				"Answer_created_time": "2023-05-07T10:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I asked the same question and I got an answer but looks like BARD is having hallucination, where LLMs generate very plausible but fanciful response."
			},
			{
				"Answer_created_time": "2023-05-16T11:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "How much time it take to get access to API? I am already in waiting-list for 4 days. Is it paid or free?"
			}
		]
	},
	{
		"Question_title": "Dialogflow ES: Keeping session open while user is visiting multiple pages on a site",
		"Question_created_time": "2023-04-28T15:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-ES-Keeping-session-open-while-user-is-visiting/td-p/548662/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 103,
		"Question_body": "I'm working on a project using Dialogflow ES.  When testing on the site, I see that a Dialogflow session lasts as long as a user is on the same page.  If a user navigates to a different page on the site, the Dialogflow session resets and the user can't see the history of the conversation.  Is there a way to keep a Dialogflow ES session open as long as the user is on the site domain so he/she can navigate the site while interacting with Dialogflow?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-02T08:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@fogpuddle\u00a0,\n\nWelcome back to Google Cloud Community!\n\nYou can try using the Dialogflow Messenger to maintain the users session, you\u00a0can embed a code to your page and it will create a session ID that can be used to keep the users progress, so while the user are navigating the site, their progress are saved and the user can continue the conversation.\u00a0You can learn more here:\u00a0https://cloud.google.com/dialogflow/es/docs/integrations/dialogflow-messenger\n\nHope it helps!"
			},
			{
				"Answer_created_time": "2023-05-08T15:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for your feedback\u00a0@kvandres.\n\nWe do use the embed code of Dialogflow Messenger on the site and I notice that if we don't have a session-id defined in the code, Dialogflow creates a unique one.\u00a0 However, it creates a unique one for every page, thus restarting the conversation on every page visit.\n\nI'm reading in the documentation that if we want a session id throughout the site domain visit, a session id would have to be created by our system and then supplied to Dialogflow via a \"detect intent request.\"\u00a0 However, we're not connected to any APIs that involves a detect intent request for our build.\u00a0 Is this a requirement or can the session ID just be supplied in the Dialogflow Messenger embed code to keep the session open throughout a user's site visit?"
			},
			{
				"Answer_created_time": "2023-05-16T11:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@kvandres\u00a0Let me know if you have any questions about my above response.\u00a0 I'm trying to find a solution to keep a chatbot session open while a user navigates various pages on our site.\u00a0 Right now, the chatbot restarts on every page visit while on our site domain.\u00a0 We have the Dialogflow Messenger embed code in the footer of the site.\u00a0\u00a0Is there a way to keep the session open for an anonymous user while the user is clicking from page to page on our site?"
			}
		]
	},
	{
		"Question_title": "Unable to increase quota for base_mode: chat-bison and text-bison",
		"Question_created_time": "2023-05-16T10:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-increase-quota-for-base-mode-chat-bison-and-text-bison/td-p/553927/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 49,
		"Question_body": "Hi,I have paid GCP account. I am unable to use any Vertex AI services where chat-bison and text-bison are required. The quota is set at 0 and I am not able to increase it. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-16T11:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "See the following similar thread ....\n\nhttps://www.googlecloudcommunity.com/gc/AI-ML/Receiving-quota-error-when-trying-to-use-bison-chat-mo..."
			}
		]
	},
	{
		"Question_title": "Google TTS can't convert text longer than ~500-600 characters when using Neural2 voices",
		"Question_created_time": "2023-02-03T03:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-TTS-can-t-convert-text-longer-than-500-600-characters/td-p/517555/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 7,
		"Question_score_count": 1,
		"Question_view_count": 886,
		"Question_body": "According to Google TTS documentation, the Speech Synthesis Limit is 5000 bytes per request - https://cloud.google.com/text-to-speech/quotas.However, when I use Neural2 voice (for example \"es-US-Neural2-A\"), I can't even convert 600 bytes per request. Instead, I receive an error message: google.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument. Converting text < 500 bytes works fine, so that's definitely not an \"invalid argument\" issue.It looks like an issue on Google's side, but please let me know if you have any ideas on how to fix it.Thank you",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-03T09:06:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "There is already an ongoing internal bug for this. For the meantime, possible workarounds would be, converting the text into <500 bytes as you mentioned or send the request into smaller pieces.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-03T09:06:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "There is already an ongoing internal bug for this. For the meantime, possible workarounds would be, converting the text into <500 bytes as you mentioned or send the request into smaller pieces."
			},
			{
				"Answer_created_time": "2023-02-03T11:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for the quick response! Do you know by any chance how long it will take to solve this bug?\u00a0\n\nThanks again"
			},
			{
				"Answer_created_time": "2023-04-28T09:52:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi - I'm having these errors too:\n\n\u00a0\n\nThis request contains sentences that are too long. To fix, split up long sentences with sentence ending punctuation e.g. periods.\n\n\u00a0\n\n\u00a0\nIt would be good to get this fixed. It appears to be a problem for Neural, WaveNet, and Standard voices right now.\n\u00a0\nThe problem persists even if I lower the TTS characters to under 250 per request."
			},
			{
				"Answer_created_time": "2023-04-28T19:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "same here"
			},
			{
				"Answer_created_time": "2023-05-06T14:54:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, did you find a solution? I have the same problem."
			},
			{
				"Answer_created_time": "2023-05-16T06:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, did you find a solution? I have the same problem.\nMy quick fix was to change the voice, but I may need to support different voices. I can't try them all over and over again.\n\ntks"
			},
			{
				"Answer_created_time": "2023-04-13T08:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Below sentence has only 360 characters, and the api returns error 'This request contains sentences that are too long.'\u00a0\n\n\u2193\n\nThe Post also reviewed approximately 300 photos of classified documents, most of which have not been made public; some of the text documents OG is said to have written out; an audio recording of a man the two group members identified as OG speaking to his companions; and chat records and photographs that show OG communicating with them on the Discord server."
			}
		]
	},
	{
		"Question_title": "nearest neighbors predict pipeline",
		"Question_created_time": "2023-05-16T00:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/nearest-neighbors-predict-pipeline/td-p/553729/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 24,
		"Question_body": "Hi All,I am trying to build pipeline for nearestneighbors from scikit learn. As there is no \"predict\" method in this, i need to write a custom predictor class. can any1 help me on how to call this predict class on model is deployed on endpoint? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-16T00:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi All,\n\nI am trying to build pipeline for nearestneighbors from scikit learn. As there is no \"predict\" method in this, i need to write a custom predictor class.\u00a0\n\ncan any1 help me on how to call this predict class on model is deployed on endpoint?"
			}
		]
	},
	{
		"Question_title": "GenerativeAI Studio is flagging this as inappropriate due to policy...",
		"Question_created_time": "2023-05-15T23:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GenerativeAI-Studio-is-flagging-this-as-inappropriate-due-to/td-p/553719/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 23,
		"Question_body": "\"View this repo at https://github.com/aaronn/slack-gptWhat is it about?\"This points to a Github repo that has code for a Slackbot.On bard.google.com, it runs fine.What's causing this though?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-15T23:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "\"View this repo at https://github.com/aaronn/slack-gpt\n\nWhat is it about?\"\n\nThis points to a Github repo that has code for a Slackbot.\n\nOn bard.google.com, it runs fine.\n\nWhat's causing this though?"
			}
		]
	},
	{
		"Question_title": "Can't enable generativelanguage API even on Admin account for GCP",
		"Question_created_time": "2023-05-15T23:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-t-enable-generativelanguage-API-even-on-Admin-account-for/td-p/553710/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 13,
		"Question_body": "Is there a way to resolve this? As I need it enable for use with Langchain.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-15T23:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Is there a way to resolve this? As I need it enable for use with Langchain."
			}
		]
	},
	{
		"Question_title": "TPU initialization error while running training script",
		"Question_created_time": "2023-05-13T09:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/TPU-initialization-error-while-running-training-script/td-p/553165/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 39,
		"Question_body": "I am trying to train my pytorch model on a TPU pod v3-32 but it shows me the following error while running my training script:2023-05-13 16:35:49.648194: F tensorflow/tsl/platform/statusor.cc:33] Attempting to fetch value instead of handling error UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; Unable to create Node RegisterInterface for node 0, config: device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance\nhttps://symbolize.stripped_domain/r/?trace=7f7acf09500b,7f7acf09508f,7f79a4342bff,7f79a4646a26,7f79a...\n*** SIGABRT received by PID 317591 (TID 317591) on cpu 25 from PID 317591; stack trace: ***\nPC: @ 0x7f7acf09500b (unknown) raise\n@ 0x7f79a07c2a1a 1152 (unknown)\n@ 0x7f7acf095090 468830624 (unknown)\n@ 0x7f79a4342c00 400 tsl::internal_statusor::Helper::Crash()\n@ 0x7f79a4646a27 768 xla::PjRtComputationClient::PjRtComputationClient()\n@ 0x7f79a4629a72 1440 xla::ComputationClient::Create()\n@ 0x7f79a462bfe3 32 std::call_once<>()::{lambda()#2}::_FUN()\n@ 0x7f7acf0404df (unknown) __pthread_once_slow\nhttps://symbolize.stripped_domain/r/?trace=7f7acf09500b,7f79a07c2a19,7f7acf09508f,7f79a4342bff,7f79a...\nE0513 16:35:49.741066 317591 coredump_hook.cc:414] RAW: Remote crash data gathering hook invoked.\nE0513 16:35:49.741084 317591 coredump_hook.cc:453] RAW: Skipping coredump since rlimit was 0 at process start.\nE0513 16:35:49.741108 317591 client.cc:278] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\nE0513 16:35:49.741111 317591 coredump_hook.cc:512] RAW: Sending fingerprint to remote end.\nE0513 16:35:49.741116 317591 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket\nE0513 16:35:49.741129 317591 coredump_hook.cc:518] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\nE0513 16:35:49.741133 317591 coredump_hook.cc:580] RAW: Dumping core locally.\nE0513 16:35:49.817174 317591 process_state.cc:784] RAW: Raising signal 6 with default behavior\nAborted (core dumped) ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-15T22:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can try this using gcloud command:\n\ngcloud compute tpus create my-tpu --zone=zone --project=project --accelerator-type='v3-32' --version='version'"
			}
		]
	},
	{
		"Question_title": "Is it possible to use Vertex AI experiments without Tensorboard?",
		"Question_created_time": "2023-05-12T01:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Is-it-possible-to-use-Vertex-AI-experiments-without-Tensorboard/td-p/552738/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 83,
		"Question_body": "Is it possible to use Vertex AI experiments without Tensorboard?When I programmatically init the SDK and start experiment run as described in docs, Tensorboard is automatically provisioned.I would like to use only summary metrics and parameters tracking (i.e. not time-series metrics), and AFAIK it doesn't require Tensorboard.Here's the code snippet:  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-15T01:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@maxromanovsky\u00a0,\n\nWelcome to Google Cloud Community!\n\nIf you have assigned a backing tensorboard, you can delete the backing\u00a0Vertex AI TensorBoard using this parameter delete_backing_tensorboard_run, I assume that it is not related to your provided snippet since you did not specify the tensorboard or experiment_tensorboard in your run or init. You can check this link to learn more:\u00a0https://cloud.google.com/python/docs/reference/aiplatform/1.15.0/google.cloud.aiplatform.ExperimentR...\n\nHope this will help!"
			},
			{
				"Answer_created_time": "2023-05-15T03:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As kvandre said\u00a0\nYes you can use Vertex AI Experiments without TensorBoard instance (as long as you don't need\u00a0Time Series Metrics).\n\nThe TensorBoard Instance is not automatically created.\u00a0\nTo create it, you have to call explicitly:\n\nexperiment_tensorboard = vertex_ai.Tensorboard.create()\n\nI wrote a deep dive article on Vertex AI Experiments a few weeks ago:\nhttps://medium.com/google-cloud/machine-learning-experiment-tracking-with-vertex-ai-8406f8d44376\n\nBest regards"
			},
			{
				"Answer_created_time": "2023-05-15T22:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@sascha_heyer\u00a0I definitely didn't call\u00a0vertex_ai.Tensorboard.create() explicitly.\n\nAll I did was:\n\nCreated an experiment using console UI. By that time there was no link to Tensorboard in the experiment, and no sign of tensorboard being created under the hood (i.e. no spinning wheel), even after page refresh\nRan locally the script with the snipped above. Once script started running, tensorboard started provisioning.\n\nSDK version was\u00a0google-cloud-aiplatform 1.25.0.\n\nThis was counterintuitive, unexpected and costly \ud83d\ude42"
			}
		]
	},
	{
		"Question_title": "Palm2 not able to read urls?",
		"Question_created_time": "2023-05-15T10:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Palm2-not-able-to-read-urls/td-p/553579/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 32,
		"Question_body": "I've tried this in GenerativeAI studio and via API, but it's either giving me a blank message, or not showing me via policy (in the studio).\"View this repo at <https://github.com/aaronn/slack-gpt>, what is it?\"However, this works perfectly on bard.google.comAny way to fix this?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-15T10:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I've tried this in GenerativeAI studio and via API, but it's either giving me a blank message, or not showing me via policy (in the studio).\n\n\"View this repo at <https://github.com/aaronn/slack-gpt>, what is it?\"\n\nHowever, this works perfectly on bard.google.com\n\nAny way to fix this?"
			}
		]
	},
	{
		"Question_title": "Dialogflow Retrieving Start Page, End Session Page via API call",
		"Question_created_time": "2023-05-12T07:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-Retrieving-Start-Page-End-Session-Page-via-API-call/td-p/552820/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 70,
		"Question_body": "Hi I tried to get Start Page/End Session pages provided from Dialogflow CX as default pages.However I was not able to get those pages in API level which leads me not being able to implement test cases along with those pages.The image above shows one of my flowswhen I trigger list_page API, it just returns the custom pages in blue box but it does not return default pages such as End Session or Start Page in API level.  Could you please help?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-14T18:22:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I\u00a0 resolved this issue by specifying End Session (default) page on current_page of test case\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-05-12T07:40:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "the Start Page ID is\u00a0START_PAGE for the end session one is\u00a0END_SESSION\n\nAnd the default start flow-id is\u00a000000000-0000-0000-0000-000000000000"
			},
			{
				"Answer_created_time": "2023-05-12T07:47:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for your answer\u00a0@xavidop\u00a0\n\nHowever when I trigger getPageRequest, it can not retrieve those pages."
			},
			{
				"Answer_created_time": "2023-05-12T10:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "yup that is correct, these are special ones"
			},
			{
				"Answer_created_time": "2023-05-14T18:22:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I\u00a0 resolved this issue by specifying End Session (default) page on current_page of test case"
			}
		]
	},
	{
		"Question_title": "Building what's next with Generative AI",
		"Question_created_time": "2023-05-03T15:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Building-what-s-next-with-Generative-AI/td-p/550086/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 3,
		"Question_view_count": 240,
		"Question_body": "Join our Innovator Champions that are included in our Gen AI Trusted Tester program to discuss their excitement of Gen AI, predictions, and what they\u2019ve been building in pre-release Gen AI in Vertex AI.Event information:",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-03T15:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can't wait! Thanks\u00a0@taviam"
			},
			{
				"Answer_created_time": "2023-05-05T02:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI am a Google Cloud user.\u00a0 After using it for a long time, I have found many errors that are worth giving feedback on.\n\nI have submitted several error reports for Cloud OCR on Cloud Forums and Issue Tracker, but have not received any response or\u00a0assigned.\n\nI need your help to make the OCR engineering team aware of these errors.\n\nI hope that Google can improve these issues in the future.\n\nYou can see all my reports in here:<https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-OCR-problem-and-IssueTracker-no-reply/td-p/550130>"
			},
			{
				"Answer_created_time": "2023-05-08T02:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "GREAT VIDEO!"
			},
			{
				"Answer_created_time": "2023-05-11T11:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Please fix the \"quota exceeded\" error\u00a0https://www.googlecloudcommunity.com/gc/AI-ML/Receiving-quota-error-when-trying-to-use-bison-chat-mo..."
			},
			{
				"Answer_created_time": "2023-05-14T09:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Nice job, but improvements needed."
			}
		]
	},
	{
		"Question_title": "Issues uploading entities via a CSV file in Dialogflow CX",
		"Question_created_time": "2023-05-11T08:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Issues-uploading-entities-via-a-CSV-file-in-Dialogflow-CX/td-p/552438/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 6,
		"Question_score_count": 0,
		"Question_view_count": 45,
		"Question_body": "Tengo dificultades para cargar un archivo CSV en Dialogflow CX para definir entidades y sus sin\u00f3nimos. A pesar de seguir las recomendaciones sobre el formato de los archivos CSV, encuentro un mensaje de error que indica \"Formato CSV incorrecto: no requiere l\u00ednea de valor 1\" cada vez que intenta cargar el archivo.Tengo dificultades para cargar un archivo CSV en Dialogflow CX para definir entidades y sus sin\u00f3nimos. A pesar de seguir las recomendaciones sobre el formato de los archivos CSV, encuentro un mensaje de error que dice \"Formato CSV incorrecto: no se requiere l\u00ednea de valor 1\" cada vez que intente cargar el archivo.He intentado varias cosas para solucionar este problema. Primero, verifiqu\u00e9 el formato de mi archivo CSV para asegurarme de que se ajusta a las especificaciones de Dialogflow. Tambi\u00e9n me asegur\u00e9 de que mi archivo CSV est\u00e9 codificado en UTF-8 y que los delimitadores de campo y cadena est\u00e9n configurados correctamente. Adem\u00e1s, intent\u00e9 cargar el archivo CSV sin realizar ninguna modificaci\u00f3n despu\u00e9s de descargarlo de Dialogflow. En todos estos casos, esperaba que el archivo CSV se cargara sin problemas. Sin embargo, a pesar de recibir un mensaje de que la carga se realiz\u00f3 correctamente (OK verde), las entidades no se reflejaron en mi agente de Dialogflow.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-11T10:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hola,\n\nHas verificado todos estos pasos?\n\n\n\nThe CSV content should have the following format:\n\nEach entity entry is on one line and ends with a newline.\nEach value should be enclosed in double quotes.\nEach value should be separated by commas.\nFor a\u00a0map entity:\nThe reference value should be at the beginning of the line, followed by synonyms.\nInclude the reference value twice if you want it to be matched by the entity.\nFor a\u00a0list entity:\nEach line contains a single value.\n\nComo alternative puedes la cxcli que yo he desarrollado\u00a0https://cxcli.xavidop.me/cmd/cxcli_entity-type_create/\n\n\u00a0\n\nYa me dices!"
			},
			{
				"Answer_created_time": "2023-05-12T00:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Estimado Xaviop,\n\nAprecio mucho tu ayuda y las instrucciones que me proporcionaste para solucionar el problema con la subida de entidades a trav\u00e9s de archivos CSV en Dialogflow CX.\n\nSiguiendo tus indicaciones, he verificado el formato de mi archivo CSV, asegur\u00e1ndome de que cada entrada de entidad est\u00e9 en una l\u00ednea y termine con una nueva l\u00ednea, que cada valor est\u00e9 entre comillas dobles y que cada valor est\u00e9 separado por comas. Adem\u00e1s, he seguido las especificaciones para entidades de mapeo, colocando el valor de referencia al principio de la l\u00ednea, seguido de los sin\u00f3nimos.\n\nDespu\u00e9s de esto, he intentado subir el archivo CSV a Dialogflow CX. A pesar de que la plataforma indica que la subida fue exitosa (verde OK), las entidades no se reflejan en mi agente Dialogflow. He intentado realizar este procedimiento en diferentes navegadores e incluso en una ventana de inc\u00f3gnito, pero el resultado sigue siendo el mismo.\n\nEstoy adjuntando el archivo CSV que intent\u00e9 subir para tu referencia.\n\nEstoy seguro de que estoy siguiendo todas las instrucciones correctamente, pero a\u00fan as\u00ed el problema persiste. Agradecer\u00eda enormemente cualquier otra sugerencia o ayuda que puedas proporcionarme para resolver este problema.\n\nAtentamente,\nDomingo"
			},
			{
				"Answer_created_time": "2023-05-12T00:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Te\u00a0\u00a0dejo un enlace a un drive donde esta puesto el ultimo intento que he\u00a0realizado no aparece nada en el entities aunque da la se\u00f1al de OK todo perfecto https://docs.google.com/spreadsheets/d/11YWuF1wywubv0OFz05mWabC2mxdSOPcOUAdXPJ1AvDc/edit#gid=0 Estas opciones de configuraci\u00f3n es la forma en que se debe guardar un archivo en formato CSV (valores separados por comas). asi lo tengo configurado Opciones de Campo Configuraci\u00f3n Conjunto de caracteres Selecciona \"Unicode (UTF-8)\" Delimitador de campos Selecciona \",\" (coma) Delimitador de cadenas Selecciona \"\"\" (comillas dobles ) Guardar contenido de celdas como se muestra Marca esta opci\u00f3n Guardar las f\u00f3rmulas y no los valores calculados opci\u00f3n Desmarcar esta Entrecomillar todas las celdas de texto Marcar esta opci\u00f3n Ancho de columna fija Desmarcar esta opci\u00f3n\n\nGracias de nuevo"
			},
			{
				"Answer_created_time": "2023-05-12T00:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "https://docs.google.com/spreadsheets/d/11YWuF1wywubv0OFz05mWabC2mxdSOPcOUAdXPJ1AvDc/edit#gid=0"
			},
			{
				"Answer_created_time": "2023-05-12T00:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hola,\n\nViendo el CSV, se parecen mas a intents que a entidades. Estas seguro que quieres subir entidades?"
			},
			{
				"Answer_created_time": "2023-05-14T03:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Si en primer lugar entidades"
			}
		]
	},
	{
		"Question_title": "Video Intellence API - Calculation Error - Request task has error from parents.",
		"Question_created_time": "2023-05-04T04:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Video-Intellence-API-Calculation-Error-Request-task-has-error/td-p/550210/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 6,
		"Question_score_count": 0,
		"Question_view_count": 159,
		"Question_body": "Hello All,In our team we have been using video intelligence API for speech transcription. The API was running fine for about a year now. However lately we have been getting below error - The error is intermittent that means for about 20 tries to video intelligence API about 15 fails and another 5 are successful in giving transcripts. The videos that we process are of around 1.4 Gb. \n\nDoes anyone know where might be the problem ? This are the things that I have already tried without success  - ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-04T14:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@mota,\u00a0\n\nWelcome to Google Cloud Community!\n\nOne of the possible reasons for this issue might be due to the increasing number of requests for Speech Transcription. If this is a blocker to your project, I highly suggest that you reach out to google cloud support.\nGoogle Cloud Support:\u00a0https://cloud.google.com/support Please note that you will only get charged for the successes based on the service's SLO, so if it is not a blocker, you will still get the successes but it will still fail a few times. Additionally, you can file the issue here:\u00a0https://cloud.google.com/support/docs/issue-trackers#trackers-list\n\nYou can also learn more about the Video Intelligence API SLA using this link: https://cloud.google.com/video-intelligence/sla\n\nHope it helps!"
			},
			{
				"Answer_created_time": "2023-05-05T06:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Mota,\n\nThis is a known issue, and Google will release a fix starting next week.\u00a0\n\nBest regards"
			},
			{
				"Answer_created_time": "2023-05-08T03:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for info."
			},
			{
				"Answer_created_time": "2023-05-11T01:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@sascha_heyer\u00a0 is there any update on fix release date ? Our system are getting conjusted\u00a0\ud83e\udd10"
			},
			{
				"Answer_created_time": "2023-05-13T17:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@sascha_heyer\n\nSame issue here - didn't realize we could just hammer the API til it gave a proper response back, though. Please update if there's an ETA on this fix."
			},
			{
				"Answer_created_time": "2023-05-13T19:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "actually I've run it about 40x on the same video with the error 13 each time now.. I guess spamming the API isn't a fix"
			}
		]
	},
	{
		"Question_title": "Error with \"import vertexai\"",
		"Question_created_time": "2023-05-11T22:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-with-quot-import-vertexai-quot/td-p/552708/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 99,
		"Question_body": "Hi,I'm trying to use GCP: Vertex Ai. I already installed the Vertex AI SDK for Python package successfully.I can \"from google.cloud import aiplatform\" successfully. However, \"import vertexai\" caused error as follows: No module named 'vertexai'I also already installed the module vertexai in the linux machine successfully.Would you help to show me what caused the error? Thanks so much in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-12T16:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@tln888111\u00a0,\n\nWelcome to Google Cloud Community!\n\nAfter you have installed the Vertex AI SDK, the Vertex AI client library is already installed. For Python, the Vertex AI SDK uses google.cloud.aiplatform. You just need to use the code below to get access to the vertex AI SDK.\u00a0\n\nfrom google.cloud import aiplatform\n\nYou can already initialize your vertex AI. You can check this sample code on how to initialize:\u00a0\u00a0https://cloud.google.com/vertex-ai/docs/start/install-sdk#initialize-python-sdk\n\nYou can also check this sample code on how to cancel a data labeling jobs.\u00a0\nhttps://cloud.google.com/vertex-ai/docs/samples/aiplatform-cancel-data-labeling-job-sample\n\nHope this helps!"
			},
			{
				"Answer_created_time": "2023-05-13T12:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Kvandres\n\nThanks so much for your replying.\n\nI would like to share with you that I already resolved the issue by upgrading the google-cloud-aiplatform already installed in the GCP deep learning image (GCP) that I used to create the deep learning server.\n\nThe current version is 1.24.0. I need to upgrade it to 1.25.0 that includes the module vertexai, using the command line: pip install --upgrade googlke-cloud-aiplatform.\n\nEverything is good! I can import vertexai successfully.\n\nThanks so much for all your help.\n\nThuan Nguyen (ainewtechs)"
			}
		]
	},
	{
		"Question_title": "Scheduling Vertex AI Pipeline - Error 503",
		"Question_created_time": "2023-05-10T11:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Scheduling-Vertex-AI-Pipeline-Error-503/td-p/552124/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 55,
		"Question_body": "Hi,I successfully trained and deployed a pipeline in Vertex AI using Kubeflow for a retrieval model, Two Towers.Now I want to schedule this pipeline run every 8 minutes. Here's my code:  The JSON is successfuly created, but the Scheduler Job fails immediately.Logging tells me the httpRequest has an error 503 plus:  Any ideas on how to solve this issue ?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-11T06:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks so much for sharing! I am not a Vertex expert, but learning! Hoping others will weigh in, but this sounds like a permissions issue based on some other scenarios I've seen.\u00a0\n\nHere are some ideas on how to troubleshoot the issue of the Vertex AI Scheduler Job failing immediately with a 503 error:\n\nCheck the Cloud Logging logs.\u00a0The Cloud Logging logs will provide more information about the error, such as the specific API that is failing and the error message. This information can be used to troubleshoot the issue.\nMake sure that the Vertex AI API is enabled.\u00a0The Vertex AI API must be enabled in the Google Cloud Platform Console before it can be used. To enable the API, go to the\u00a0APIs & Services\u00a0page in the Console and search for\u00a0Vertex AI. Click the\u00a0Enable\u00a0button to enable the API.\nMake sure that the Vertex AI service account has the correct permissions.\u00a0The Vertex AI service account must have the correct permissions to create and run Scheduler jobs. To grant the service account the necessary permissions, go to the\u00a0IAM & Admin\u00a0page in the Console and select the\u00a0Service accounts\u00a0tab. Click the name of the Vertex AI service account and then click the\u00a0Edit\u00a0button. In the\u00a0Roles\u00a0section, select the\u00a0Cloud Scheduler Editor\u00a0role.\nCheck the Vertex AI quota.\u00a0The Vertex AI quota may be exceeded. To check the quota, go to the\u00a0Quotas\u00a0page in the Console and search for\u00a0Vertex AI. The\u00a0Quotas\u00a0page will show the current usage and the available quota for each Vertex AI resource.\nIf you continue to experience issues, Contact Google Cloud support.\u00a0If you have tried all of the above and you are still having trouble, you can contact Google Cloud support for help.\n\nI hope this helps!\n\n\u00a0\n\nRoderick G\nCommunity Manager\nGoogle Cloud Community"
			},
			{
				"Answer_created_time": "2023-05-13T09:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I solved with Compute Engine and cron jobs."
			}
		]
	},
	{
		"Question_title": "How do I enable generativelanguage.googleapis.com?",
		"Question_created_time": "2023-05-13T09:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-do-I-enable-generativelanguage-googleapis-com/td-p/553160/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 40,
		"Question_body": "I already have API Access to Vertex AI, and the Python code works with the google AI SDK.However, I'm using this with Langchain, and it requires ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-13T09:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I already have API Access to Vertex AI, and the Python code works with the google AI SDK.\n\nHowever, I'm using this with Langchain, and it requires\u00a0\n\ngoogle.generativeai\u00a0module along with a Google API Key. However I'm unable to enable\n\u00a0\ngenerativelanguage.googleapis.com, due to a permission denied issue. Is there any way to resolve this? Since I already have Enterprise access to Vertex AI."
			}
		]
	},
	{
		"Question_title": "Speech-To-Text High Latency for audio files",
		"Question_created_time": "2023-05-11T02:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-To-Text-High-Latency-for-audio-files/td-p/552344/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 24,
		"Question_body": "I am testing the Speech-To-Text API with audio files and have observed high latency (>25s) from time to time (it happens rarely). The high latency leads to a timeout on my application.I have already followed the best practices as described in here (https://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data). My audio files are around 3 - 6 seconds long each. What other Best Practices are advisable to handle the long latency cases? Retry with back-off?Any hints are greatly appreciated!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-12T14:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@devbox\u00a0,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, assuming that all of the best practices were followed.\u00a0\n\n1. Try checking the endpoint that you are trying to use, if you are using REST, please verify if you are using\u00a0https://speech.googleapis.com/v1/speech:recognize\u00a0endpoint, since you've mentioned that your audio files are 3 - 6 seconds long each. This endpoint is used for short audio that is less than 60 seconds.\n\nYou can check this link to learn more:\u00a0https://cloud.google.com/speech-to-text/docs/sync-recognize\n\n2. You can also check this documentation to optimize your audio files.\u00a0https://cloud.google.com/speech-to-text/docs/optimizing-audio-files-for-speech-to-text\n\n3. You can check if the requests are sent multiple times in a short time, this might have increased the latency.\n\nYou can also reach out Google Cloud Support:\u00a0https://cloud.google.com/support\n\nHope this will help!"
			}
		]
	},
	{
		"Question_title": "PipelineJob using a dedicated service account",
		"Question_created_time": "2023-03-10T00:52:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/PipelineJob-using-a-dedicated-service-account/td-p/530893/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 155,
		"Question_body": "When I submit my pipelinejob (submit)I get the following error message:I am the owner of the project and when I go through the console to create a pipeline job, there is no issue in specifying another service account to run as. It all works if I remove the service_account parameter, so the default account is used. Any idea what the issue can be?Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-12T21:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Make sure the service account has the appropriate roles and permissions to run your PipelineJob. You can check this by going to IAM & Admin > IAM in Google Cloud Console and verifying that the service account has at least one of these roles: Vertex AI Admin, Vertex AI User, or Vertex AI Custom Code Service Agent"
			},
			{
				"Answer_created_time": "2023-05-11T11:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Joevanie\u00a0We are also facing similar issue. The service account does have\u00a0Vertex AI User permission.\n\n@mans123\u00a0was this issue resolved?"
			}
		]
	},
	{
		"Question_title": "Dialogflow CX incorrect parsing of currency in millions e.g. 1million",
		"Question_created_time": "2023-05-02T08:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-incorrect-parsing-of-currency-in-millions-e-g/td-p/549494/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 48,
		"Question_body": "Hi allAfter some extensive testing and trying workarounds, thought I'd post here to mention this issue and see if there's a solution.When capturing a currency amount in the millions, the wrong values are picked up when the user says something like \"1million\" or \"5million\"\n\ne.g.\n\nI want to pay \"2million\" results in this:\n\n\"original\": \"million\",\n\"currency\": \"USD\",\n\"amount\": 1000000\n\nor when I have the currency set to list, it ends up with this:\n\n{\n\"currency\": \"USD\",\n\"amount\": 2,\n\"original\": \"2\"\n},\n{\n\"currency\": \"USD\",\n\"amount\": 1000000,\n\"original\": \"million\"\n}However, adding a space e.g. \"1 million\" gets the right value captured.\n\nI've tried  a few ways to workaround with composite entities, and annotating training phrases but couldn't get it work without custom code. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-03T14:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@FlashMaddison\u00a0,\n\nWelcome back to Google Cloud Community!\n\nThere are several reasons why your training phrases and composite entities are not working, you can validate the solutions below if it will solve your problem.\n\n1. In case you haven't, you need to define 20 training phrases (bare minimum) for every intent in order for the Natural Language Understanding (NLU) Model to match the intent. The reason for this is the NLU model might be leaning more towards other intents with more training phrases which means if I only have only 5 training phrases for intent A while intent B has 50 training phrases, NLU will be bias for intent B. It is also desirable to have approximately 50 training phrases specially for head intents. You can learn more using this link:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/agent-design#phrase-minimum\nhttps://cloud.google.com/dialogflow/cx/docs/concept/agent-design#intent-bias\n\n2. For best practice of annotation, you can use this link:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/agent-design#annotation-consistency\nhttps://cloud.google.com/dialogflow/cx/docs/concept/agent-design#entity-value-variety\n\n3.\u00a0 You can also try adjusting the ML settings on how the user input will be processed to improve the performance of your agent.\u00a0 You can learn more using this link:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/agent#settings-ml\n\n4. As a best practice, you should also avoid nesting composite entities and similar intents. You can check, this link for more information:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/agent-design#entity-composite-nest\n\nHope it helps!"
			},
			{
				"Answer_created_time": "2023-05-11T00:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thank you for your response however none of that helps. I believe this is a bug with the sys.unit-currency entity itself.\n\nAnnotation solution doesn't help when using parameter filling.\n\nsys.number correctly parses input like \"3.3m\" or \"3.3million\" but sys.unit-currency does not.\n\nI cannot find anywhere to raise bugs to the DialogFlow team, how do I report this?"
			}
		]
	},
	{
		"Question_title": "google.cloud.vision.v1.ImageAnnotator exceeded 600000 milliseconds",
		"Question_created_time": "2023-05-09T01:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/google-cloud-vision-v1-ImageAnnotator-exceeded-600000/td-p/551551/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "It was working well for a year and we have not changed anything. I think something has changed recently in cloud vision. I am getting this error on PRODUCTION server no way to know what is wrong. On Production Server I am able to do telnet on visiontelnet vision.googleapis.com 443 gives the expected output.\n\nVersion:\n\"@google-cloud/vision\": \"^2.1.1\",```\nMay 09 13:46:43 fi-services npm[20346]: GoogleError: Total timeout of API google.cloud.vision.v1.ImageAnnotator exceeded 600000 milliseconds before any response was received.\nMay 09 13:46:43 fi-services npm[20346]: at repeat (/opt/******/node_modules/google-gax/build/src/normalCalls/retries.js:66:31)\nMay 09 13:46:43 fi-services npm[20346]: at Timeout._onTimeout (/opt/*****/node_modules/google-gax/build/src/normalCalls/retries.js:101:25)\nMay 09 13:46:43 fi-services npm[20346]: at listOnTimeout (internal/timers.js:549:17)\nMay 09 13:46:43 fi-services npm[20346]: at processTimers (internal/timers.js:492:7) {\nMay 09 13:46:43 fi-services npm[20346]: code: 4\nMay 09 13:46:43 fi-services npm[20346]: } error```\nThank you",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-10T15:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@rikiz,\n\nWelcome to Google Cloud Community!\n\nIf you are using node.js libraries, you can try checking your client library version if it is up to date as well as your dependencies, but I highly recommend that you contact google cloud support or create a support ticket for this inquiry, as engineers have better visibility and access to your logs and API usage. You can use this link:\u00a0cloud.google.com/contact\n\nHope this will help!"
			}
		]
	},
	{
		"Question_title": "Google Cloud Translate API Accuracy",
		"Question_created_time": "2023-05-04T08:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Translate-API-Accuracy/td-p/550287/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 29,
		"Question_body": "Hello,We use Google Cloud Translate API to translate law and policy documents for our web app. As these will be used for research, we need to give users a disclaimer about accuracy in our Terms of Use. I can't see any information about how accurate Google Cloud Translate is, only Google Translate (which I understand have different accuracies). Can someone please help?Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-10T13:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you display the results of the Translation API to your end users without modification, you must make the following disclaimer available on your web site or within your app:\u00a0\n\nTHIS SERVICE MAY CONTAIN TRANSLATIONS POWERED BY GOOGLE. GOOGLE DISCLAIMS ALL WARRANTIES RELATED TO THE TRANSLATIONS, EXPRESS OR IMPLIED, INCLUDING ANY WARRANTIES OF ACCURACY, RELIABILITY, AND ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n\n\u00a0This is from the Translate attribution guide."
			}
		]
	},
	{
		"Question_title": "VertexAI autoML pipeline template error (tabular-regression)",
		"Question_created_time": "2023-05-08T08:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/VertexAI-autoML-pipeline-template-error-tabular-regression/td-p/551367/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 66,
		"Question_body": "I have been using the the autoML regression pipeline templates for a while successfully.  Three days ago overnight they broke.  The same job that worked the night before hangs with the error message: \"Unable to create pipeline run due to the following error: Input parameter type mismatch. PipelineSpec.root.input_definitions.parameters['dataflow_use_public_ips'] is defined as BOOLEAN that parses BOOL_VALUE type, but the default value is provided as NUMBER_VALUE type.\"If I want to re-run a clone of a previously successful training the same happens.  I have tried to set public IP setting to different values -- no success.  I have downloaded and edited the yaml according to the error-- no luck either!Anybody encountered the same?  Is there a workaround?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-10T05:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "OK!\u00a0 The original Google provided template is now fixed and running!\n\nThanks for the help!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-05-09T03:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nIt seems like a recent update may have caused a compatibility issue with your pipeline. Try rolling back to a previous version or reaching out to the autoML support team for assistance with resolving the issue.\n\nMyMorri App\n\nBest regard,\nLloyd225"
			},
			{
				"Answer_created_time": "2023-05-09T09:37:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi Lloyd225\n\nThanks for reaching out!\u00a0 The issue is that the original pipeline template on GCP is broken!\nIt does not work \"out of the box\"!\n\nI don't know how to reach the autoML support team, I searched for it but came up empty.\nCould you point me to a channel to reach out to?\n\nThanks"
			},
			{
				"Answer_created_time": "2023-05-10T05:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "OK!\u00a0 The original Google provided template is now fixed and running!\n\nThanks for the help!"
			}
		]
	},
	{
		"Question_title": "Running Yolov8 on Google Colab leveraging GCP GPUs",
		"Question_created_time": "2023-05-09T13:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Running-Yolov8-on-Google-Colab-leveraging-GCP-GPUs/td-p/551745/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 36,
		"Question_body": "Hello!I'm sure that many of you might find my question/request a trivial problem, and that I am just inexperienced, which I am. This will be my first time using GCP so I would like to be careful so that I don't get billed hundreds of dollars. With that out of the way, I could use some help. I have this colab file (https://colab.research.google.com/drive/13tLrfRPPc7m3V7X7Pdwjrv1w7RXC8CFR#scrollTo=JjooafMMr6zo ) and I would like to run the training portion of it. The problem is, the 100k dataset is far to big for colab's gpus. This led me to find GCP which provides essentially free GPU's with its $300 credit free trial. This leads me to the true problem, I have no idea how I would go about running this colab file (or the code in general) with GCP's GPU's. I've looked at a couple tutorials already, and many of them talk about port-forwarding jupyter things and whatnot which I have no idea how to use. Does anyone have/know about good tutorials which go really in depth (like for complete and total dummies (also preferably a video)) of how to essentially run my colab file, or is anyone willing to step by step work with me to get this done?Thanks,Frank ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-09T14:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have found this repository.\u00a0https://github.com/adhil0/colab_ssh\n\nThis shows me mostly everything that I needed as for one of the portions. The new \"problem\" is I don't know how to set up an instance (I will google this later). Also, will the $300 be enough (most likely, I know exact is difficult) to run my code?\n\nThanks again"
			}
		]
	},
	{
		"Question_title": "Google TTS keeps having a high-pitched, robotic voice crack intermittently",
		"Question_created_time": "2023-05-08T00:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-TTS-keeps-having-a-high-pitched-robotic-voice-crack/td-p/551205/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 28,
		"Question_body": "Google TTS keeps having a high-pitched, robotic voice crack intermittently, regardless of the voice model I use. Why is that happening?Is there no way to resolve this issue?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-09T14:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@geunueom,\n\nWelcome to Google Cloud Community!\n\nI highly recommend that you reach out to Google Cloud Support\u00a0for this inquiry, as engineers has better visibility and access to your logs and API usage. You can use this link to reach out to Google Cloud Support:\u00a0https://cloud.google.com/support\n\nThank you"
			}
		]
	},
	{
		"Question_title": "Glossary application in Google Translate custom AutoML models",
		"Question_created_time": "2023-05-05T14:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Glossary-application-in-Google-Translate-custom-AutoML-models/td-p/550784/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 71,
		"Question_body": "I am using a TMS to perform translations by connecting to my custom AutoML models.  When I don't use glossaries in the TMS setup, the engine seems to behave normally (with the usual mistakes an MT engine can make), but when I use a complex glossary made out of several thousand entries of product names my company makes, I start seeing weird behaviors such as duplication of terms and dropping of important part of text.Is there a document that explains what mechanism Google Translate V3 is going through when applying a glossary to a sentence?  I don't  think it's just a straight search and replace.  Can you shed some light?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-09T10:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@Michel_FC\u00a0,\n\nWelcome to Google Cloud Community!\n\nOne of the reasons why this issue is happening is if you have not re-trained the models in several months and the model was created from previous versions of AutoML (for example, v1beta1), models must be re-trained to ensure the stability and consistency of translations and to get the necessary upgrades since original training.\u00a0\n\nHere is a documentation on how to create and use a glossary:\u00a0https://cloud.google.com/translate/docs/advanced/glossary#create_a_glossary\n\nAlso take note of the stopwords, stopwords will ignore some words that are included in the glossary.\u00a0https://cloud.google.com/translate/docs/advanced/glossary#stopwords\n\nYou can also reach out to Google Cloud Support:\u00a0https://cloud.google.com/support"
			},
			{
				"Answer_created_time": "2023-05-09T10:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We are using a 3rd party to train the models, and I do indeed know that the French model where I have seen this behavior was trained around over a year ago (March 2022).\u00a0 \u00a0Do you have a precise cut-off date, or is there a way for me to check what version of AutoML was used for the model?\n\nThank you."
			},
			{
				"Answer_created_time": "2023-05-09T10:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I see the last release of AutoML seems to be on 10/28/2022.\u00a0 Are you saying that all models created before 10/28/2022 may misbehave with glossaries?\u00a0 Thanks for the clarification.\n\nhttps://cloud.google.com/translate/automl/docs/release-notes"
			}
		]
	},
	{
		"Question_title": "VertexAI notebook does not obtain Shared VPC IP",
		"Question_created_time": "2023-05-05T07:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/VertexAI-notebook-does-not-obtain-Shared-VPC-IP/td-p/550677/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 73,
		"Question_body": "Hello, when I create a new VertexAI notebook I can select:networking:  Shared network (from host project: XXXC) GCP then asks me the network and the subnet.\nIn the subnet dropdown the tooltip reads:Assigns the notebook an IPv4 address from the subnetwork's range. Notebooks in different subnetworks can communicate with each other using their internal IPs as long as they belong to the same network.But this does not seem to work at all ?Once I create the notebook I can run ifconfig in the terminal and there it's clear that the notebook never got an IP from the subnet I selected above.\n\nAnyone knows how I can use this feature?\nThanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-06T21:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hey\u00a0@dgnemo\u00a0\n\nCan you confirm whether you are using a Managed or User-Managed Notebook?\u00a0\n\nIf you are using a Managed notebook, one of the IPs you see in the terminal should be from the\u00a0\nservice producer network. This is a Google-managed project where your notebook/GCE is provisioned.\u00a0\n\nThis diagram is a good reference.\n\nFrom that article: \"...\u00a0A resource in the service producer network is similar to other resources in your VPC network. For example, it's reachable through internal IP addresses by other resources in your VPC network. You can also create firewall rules in your VPC network to control access to the service producer's network...\"\n\nYour SharedVPC should have a VPC network peering connection to the producer project. This connection includes a destination CIDR range with routes for the resources to communicate.\u00a0\n\nCan you share more colour about the blocker(s) you are trying to solve other than the specific IP assigned to the notebook? If you are using a Google-managed notebook, maybe the way to go for your case is a user-managed notebook considering all trade-offs.\n\nI hope this helps.\n\nJC"
			},
			{
				"Answer_created_time": "2023-05-08T07:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sorry for the double posting. we can continue the conversation in the other thread if it's ok.\n\nAnyway, yes, I am using Managed Notebooks.\n\nThe issue I am trying to solve is to allow the notebook to access internet, without exposing it publicly with an external IP address.\n\nTo achieve that, I was hoping that the netbook could use the Cloud NAT available in the Shared VPC.\n\nUnfortunately I could not find a way to make it work.\n\n1. Should the notebook also receive an IP address in the VPC subnet range (like mentioned by the GCP UI ?)\n2. If not, and the only IP assigned to the notebook is the IP in the Google Managed Network, is it possible to configure the network such that the notebook can reach internet through the VPC Cloud Nat ?\n\nThanks a lot!"
			},
			{
				"Answer_created_time": "2023-05-09T09:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Oh, I see. In case anyone else is interested in this thread please follow the conversation here\n\nhttps://www.googlecloudcommunity.com/gc/Infrastructure-Compute-Storage/Help-understanding-Shared-VPC..."
			}
		]
	},
	{
		"Question_title": "I want to bulk the CRD part of the CRUD function of the intent.",
		"Question_created_time": "2023-05-07T20:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/I-want-to-bulk-the-CRD-part-of-the-CRUD-function-of-the-intent/td-p/551160/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 52,
		"Question_body": "For the ES version, we are using the batchUpdate method in the googleapis module to perform bulk updates. However, for the CX version, we are using the @Google-cloud/dialogflow-cx module and there doesn't seem to be a method for bulk adding, modifying, or deleting intents.1. Is it not possible to perform bulk operations in the CX version?\n2. And if not, does CX not have the ability to import and export intents as files like the ES version does? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-09T08:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@zgzgzg\u00a0,\n\nWelcome to Google Cloud Community!\n\nHere are the answers to your questions:\n\n1. Unfortunately this feature is not yet available in the Dialogflow CX, however if you want this feature, you can file a request using this link:\u00a0https://cloud.google.com/support/docs/issue-trackers\n\n2. Yes, you can import and export intents in Dialogflow CX. You can check this link to learn more: https://cloud.google.com/dialogflow/cx/docs/concept/intent#export\n\nHope this will help!"
			}
		]
	},
	{
		"Question_title": "Missing `us-central2-b` when creating TPU VM",
		"Question_created_time": "2023-05-06T11:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Missing-us-central2-b-when-creating-TPU-VM/td-p/550953/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Based on this excerpt:Note: Cloud TPU v4 capacity is located in us-central2 region. Currently, v4 is only available in zone us-central2-b. See Types and Topologies for information about supported v4 TPU types and topologies.from https://cloud.google.com/tpu/docs/regions-zones I'm expecting to find the region however it's not present in the dropdown. CLI is also saying it's either forbidden or mistyped.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-09T07:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@lukas0,\n\nWelcome to Google Cloud Community!\n\nAs of now, you need to reach out to Google Cloud Support in order to enable this feature in your Project. You can check the key note in this documentation:\u00a0https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-v4-config\n\nYou can use this link to reach out to Google Cloud Support:\u00a0https://cloud.google.com/support\n\nHope this will help!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-05-09T07:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@lukas0,\n\nWelcome to Google Cloud Community!\n\nAs of now, you need to reach out to Google Cloud Support in order to enable this feature in your Project. You can check the key note in this documentation:\u00a0https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-v4-config\n\nYou can use this link to reach out to Google Cloud Support:\u00a0https://cloud.google.com/support\n\nHope this will help!"
			}
		]
	},
	{
		"Question_title": "How do I deploy my custom model I have trained on workbench GCP?",
		"Question_created_time": "2023-05-09T01:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-do-I-deploy-my-custom-model-I-have-trained-on-workbench-GCP/td-p/551556/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 21,
		"Question_body": "I have trained a detectron2 model on vertex ai workbench. i have NOT used tensorflow, xgboost or scikit-learn. i have a model.pth file and a metrics.json file stored in my bucket when i run the model. How do i deploy this model on GCP and further evaluate it? Is it possible for me to create an endpoint and directly deploy my model on this or will i have to use something like cloud build?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-09T05:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Nitya\n\nTo deploy your model you have to pack it into a serving container this can be either done with a custom prediction container or with custom prediction routine.\u00a0\n\nI have written a deep dive article and video\nhttps://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3\n\nhttps://www.youtube.com/watch?v=brNMT7Snlh0\n\nBest regards"
			}
		]
	},
	{
		"Question_title": "Experiment not collecting data",
		"Question_created_time": "2023-04-10T09:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Experiment-not-collecting-data/td-p/542251/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 75,
		"Question_body": "Hi, we've been trying to use the experiments feature for a long time now but to no avail. We've set up everything according to documentation: Environments, versions, traffic allocation... Everything seems to work except for the data collection, we can see the agent diverting traffic from the  control version to the non-control one but the experiment remains in the collecting data stage and doesn't show any kind of results no matter for how long we keep it going or how many interactions we have with it (both from the test agent, or through the API). We haven't been able to find any other information regarding the experiments feature besides the documentation, so any help would be greatly appreciated!   ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-11T16:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@daniel_villegas,\n\nWelcome back to Google Cloud Support.\n\nHere are some possible resolutions you may perform to check the issue:\n\nCheck that the data is being sent to the right endpoint (SDK or API code matches the endpoint for the experiment you want to track.)\n\nCheck if your project ID and experiment ID are correctly set up in your SDK or API code.\u00a0\n\nCheck if your app is properly authenticated to send data to Dialogflow.\u00a0\n\nVerify that you have given an allotted enough time to experiment to collect data.\n\nVerify that your training phrases and intents are set up correctly.\n- If your agent is not correctly identifying user input, it may not be able to properly attribute interactions to the correct experiment variant.\n\nHere are some references that may help you.\nhttps://cloud.google.com/dialogflow/docs?_ga=2.175127010.-1392753435.1676655686\n\nhttps://cloud.google.com/document-ai/docs/release-notes?hl=en&_ga=2.212745684.-1392753435.1676655686"
			},
			{
				"Answer_created_time": "2023-04-19T08:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for your answer @Aris_O\u00a0Aris. We're currently checking the points you raised and have some further questions specifically about the endpoints used for experiments. What we've been doing so far is use the dialogflowcx .net nugget and the SessionClient to send messages to the bot\u00a0 using this structure: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/environments/<Environment ID>/sessions/<Session ID>\n\nDo we need to target a different endpoint to trigger the data collection on the experiment? as far as we can see in the CX documentation the experiment mentions only how to set them up on the console but we haven't been able to find how to properly configure a connection to a specific experiment.\n\nThanks in advance for all your help!"
			},
			{
				"Answer_created_time": "2023-05-08T12:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Aris_O\u00a0, any chance you could clarify the part of the experiment endpoint?\n\nThank you!"
			}
		]
	},
	{
		"Question_title": "CX - Can only select 1 language and 1 voice?",
		"Question_created_time": "2023-05-08T00:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/CX-Can-only-select-1-language-and-1-voice/td-p/551213/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 50,
		"Question_body": "In the DialogFlow CX agent settings, I can only select 1 voice, the Default 1. At the same time, the GCP supports many Dutch voices. Have I configured something wrong here?CX \u2003settings:GCP Dutch voices:",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-08T02:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Not natively, but you can do that by integrating the TTS on your webhook and returning an audio file at anytime:\u00a0https://stackoverflow.com/questions/65186184/how-to-send-audio-back-from-dialogflow-cx-webhook\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-05-08T02:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Not natively, but you can do that by integrating the TTS on your webhook and returning an audio file at anytime:\u00a0https://stackoverflow.com/questions/65186184/how-to-send-audio-back-from-dialogflow-cx-webhook"
			},
			{
				"Answer_created_time": "2023-05-08T04:55:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I don't really understand\u00a0@xavidop\u00a0. Why is there a screen to configure the language and voice, if I can never change them?"
			},
			{
				"Answer_created_time": "2023-05-08T05:06:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "so to use multiple voices what you can do is disable the CX voice integration and at any time you want to say anything to the user, call a webhook that does the TTS with the voices you want depending on the use case/scenario using TTS APIs"
			},
			{
				"Answer_created_time": "2023-05-08T06:26:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Ok\u00e9, that makes sense and is helpful. Thanks. I was of course hoping for more configurability on the part of IVR.\n\nI am still confused why CX has a screen to choose the voice while it only offers 1 option. Is there a reason for this screen?"
			}
		]
	},
	{
		"Question_title": "AutoML Translation: again one of 22 requests finishes with timeout",
		"Question_created_time": "2023-04-06T04:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Translation-again-one-of-22-requests-finishes-with/td-p/541243/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 72,
		"Question_body": "Again we meet some problems with AutoML Translation API  \ud83d\ude1eTrained 22 custom models for translation from EN to target language. Try to translate with Google Cloud API and every minute minimum one of 22 custoim models can't execute translation with 504 deadline error.We can not manage or configure our models on your infrustructure. How can we fix 504 deadline error while translating with custom model with API call? Can we increase any quotes? Or pay additional for something? Please, help!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-07T23:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I had a similar message but with training...\u00a0\n\nError: Task [889-----------] missed its deadline: 168-----------\n\nI guess it's a timeout then?"
			},
			{
				"Answer_created_time": "2023-05-08T01:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My question was about deadline while sending request to API to translate with custom model.\n\nAnd you are right - now we are now meeting \u00a0deadline while training the data. Worse and worse \ud83d\ude1e"
			}
		]
	},
	{
		"Question_title": "DocumentAI \u2013 Internal Server Error when training on pre-labeled documents",
		"Question_created_time": "2023-05-04T15:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DocumentAI-Internal-Server-Error-when-training-on-pre-labeled/td-p/550466/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 44,
		"Question_body": "We want to automate training of our custom extraction processor by sending PDFs to the OCR processor, adding entities to the document for the values that we know of, uploading it to GCS as .json, and training the processor on our document. On our initial tries we get the following error:  There is no log in Cloud Logging related to Document AI. The operation is ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-05T15:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@ldiqual\u00a0,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this error, but here are some few solutions that you can validate if it will solve the problem.\u00a0\n\n1. Please check if the bounding boxes are overlapping or intersecting. You can also try relabeling and see if it solves the problem. Please check this documentation for more information regarding labeling and json file:\u00a0\nhttps://cloud.google.com/document-ai/docs/workbench/label-documents#manual-label\n\n2. Please note about the best practices of labeling and the minimum requirements for custom processors, it is required to have at least 10 documents for training and test sets with 10\u00a0\u00a0instances of each label per set. But the recommendation is to have 50 documents for training and test sets with 50 instances of each label per set. You can learn more here: https://cloud.google.com/document-ai/docs/workbench/build-custom-processor#import_pre-labeled_data_t...\n\n2. Kindly verify if the service account has the right permissions to perform certain actions, this issue might be related to the authentication, GCS permissions or bucket permissions. You can learn more here:\nhttps://cloud.google.com/document-ai/docs/workbench/create-dataset\nhttps://cloud.google.com/document-ai/docs/access-control/iam-permissions\nhttps://cloud.google.com/storage/docs/access-control\nhttps://cloud.google.com/storage/docs/access-control/iam-roles\n\n3. Make sure that the Document API and Cloud Storage API are enabled.\u00a0\n\n4. Kindly check if it is a supported language and in a supported region using this link:\u00a0https://cloud.google.com/document-ai/docs/processors-list#processor_cde\n\nYou can also reach out to Google Cloud Support. Here is the link:\u00a0https://cloud.google.com/support"
			}
		]
	},
	{
		"Question_title": "Document AI Warehouse UI administration unreachable",
		"Question_created_time": "2023-03-29T10:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-Warehouse-UI-administration-unreachable/td-p/538454/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 83,
		"Question_body": "Hi,I have completed setup of Document AI Warehouse and I now want to go to the Admin page.In console I have a link like https://documentwarehouse.cloud.google.com/projects/xxxxxx but when I click it, it either opens a new browser tab that is logged into GCP with another account or it opens the docs page.In any event, I am unable to login.Please help",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-30T12:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "HI @pmosconi\u00a0,\n\nWelcome back to Google Cloud Community,\n\nThe name of the account that is currently opened (for example,\u00a0SAMPLE ACCOUNT) appears at the upper left of the page. To change accounts, click the account name and make a selection from the drop-down list. Note that this option is only applicable if you have access to multiple accounts.\n\nThis article might help you:\nhttps://support.google.com/partnerdash#6010264\nhttps://cloud.google.com/iam/docs/custom-roles-permissions-support"
			},
			{
				"Answer_created_time": "2023-05-05T02:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI am a Google Cloud user.\u00a0 After using it for a long time, I have found many errors that are worth giving feedback on.\n\nI have submitted several error reports for Cloud OCR on Cloud Forums and Issue Tracker, but have not received any response or\u00a0assigned.\n\nI need your help to make the OCR engineering team aware of these errors.\n\nI hope that Google can improve these issues in the future.\n\nYou can see all my reports in here:<https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-OCR-problem-and-IssueTracker-no-reply/td-p/550130>"
			},
			{
				"Answer_created_time": "2023-05-04T18:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "i got the same problem, the http status code is 302"
			}
		]
	},
	{
		"Question_title": "Document AI OCR error",
		"Question_created_time": "2023-05-04T20:18:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-OCR-error/td-p/550519/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 18,
		"Question_body": "Hi,My Document AI OCR error, like image show.When \"UPLOAD TEST DOCUMENT\" finish, will recevie error report: \"Failed to preview the document\".No matter whether I upload a PDF or a JPG/BMP file, it always shows this error.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-04T20:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nMy Document AI OCR error, like image show.\n\nWhen \"UPLOAD TEST DOCUMENT\" finish, will recevie error report: \"Failed to preview the document\".\n\nNo matter whether I upload a PDF or a JPG/BMP file, it always shows this error."
			}
		]
	},
	{
		"Question_title": "Vertex AI Resource Exhaustion Error but resources are not even close to exhausted?...",
		"Question_created_time": "2023-05-04T02:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Resource-Exhaustion-Error-but-resources-are-not-even/td-p/550175/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 48,
		"Question_body": "I'm attempting to run a basic pipeline using Kubeflow in Vertex AI. However, when I run it I receive a RESOURCE_EXHAUSTED error in the logs relating to aiplatform.googleapis.com/custom_model_training_cpus.Checking my quotas, I can see that I am not anywhere close to exhausting any of the quotas under  aiplatform.googleapis.com/custom_model_training_cpus (including the region I'm using - us-central1).Has anyone had a similar issue and know what is going on here?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-04T02:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "And here's the exact error:"
			},
			{
				"Answer_created_time": "2023-05-04T16:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Resource exhausted usually refers to the amount of memory used while running the code, not exactly the region quotas. Did you define the machine type in Kubeflow training and deployment, like?\n\n\u00a0\ntraining_op = train_model(epochs,).set_cpu_limit('16').set_memory_limit('32G').set_caching_options(False)\n\u00a0\ndeploy_op = deploy_model(training_op.outputs[\"xx\"] ,\"project\",\"uscentral1\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)"
			}
		]
	},
	{
		"Question_title": "I can't add a follow up Intent using Swahili Language (Dialogflow ES)",
		"Question_created_time": "2023-05-03T06:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/I-can-t-add-a-follow-up-Intent-using-Swahili-Language-Dialogflow/td-p/549845/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "Hello Guys, Am kind stuck because Dialogflow will not allow me to add follow-up intent. Am developing my project using swahili language. However if  I switch the language to English then Dialogflow allows me to add follow-up intents.  What could be the solution besides switching the language?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-04T15:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@Nyanda_Jr\u00a0,\n\nWelcome to Google Cloud Community!\n\nOne of the possible reasons of this issue is due to the language limitations, as of now, the only feature available to Swahili language and other languages is text-only chat, which means speech-to-text, text-to-speech, phone, etc. are not yet available to Swahili language. I highly suggest that you switch languages since intents in the English language will still be copied to other languages including follow-up intents. You can check this documentation for more information regarding language limitations:\u00a0https://cloud.google.com/dialogflow/es/docs/reference/language\n\nHope this will help!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-05-04T15:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@Nyanda_Jr\u00a0,\n\nWelcome to Google Cloud Community!\n\nOne of the possible reasons of this issue is due to the language limitations, as of now, the only feature available to Swahili language and other languages is text-only chat, which means speech-to-text, text-to-speech, phone, etc. are not yet available to Swahili language. I highly suggest that you switch languages since intents in the English language will still be copied to other languages including follow-up intents. You can check this documentation for more information regarding language limitations:\u00a0https://cloud.google.com/dialogflow/es/docs/reference/language\n\nHope this will help!"
			}
		]
	},
	{
		"Question_title": "Empty pages array in Google Document AI API OCR response",
		"Question_created_time": "2023-04-28T05:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Empty-pages-array-in-Google-Document-AI-API-OCR-response/td-p/548320/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 76,
		"Question_body": "I'm currently using the Google Document AI API to extract text from PDFs using OCR. However, I've noticed that the pages array in the OCR response is always empty, even though the OCR operation completes successfully and I'm able to retrieve text from the document.Here's a simplified version of the code I'm using: The document.text attribute contains the text of the entire document, but the pages array is always empty. This is preventing me from extracting text on a per-page basis, which is something I need for my application.I've double-checked the input PDF files to ensure that they have multiple pages, so I'm confident that the issue is not with the input data.I'm using documentai_v1beta3, I've also tried documentai_v1 but still it didn't work.Has anyone else experienced this issue with the Google Document AI API? Any suggestions for how I can retrieve text on a per-page basis?Thanks in advance for your help. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-04T00:19:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks. I noticed that my field mask was \"text\". I changed it to \"text,pages.layout\" and it worked.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-28T17:44:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@rajarehan\u00a0,\n\nWelcome to Google Cloud Community!\n\nOne of the possible reasons for your encountered issue is because the Document AI API was unable to identify the page boundaries in your PDF files, so it's possible that the pages array is empty. This may occur if the page borders in the PDF files are not clearly defined or if the OCR engine fails to recognize the page limits.\n\nUtilizing the layout data provided in the OCR response is one method for extracting text on a per-page basis. Each identified text block's bounding boxes and corresponding page number are included in the layout information.\n\nHere is a possible code that may extract text per page based on the layout details:\n\n\u00a0\n\n\n    # Extract text from the output documents\n    book_text = []\n    for document in output_documents:\n        for page in document.pages:\n            page_text = \"\"\n            for block in page.blocks:\n                # Check if the block is a text block\n                if block.block_type == documentai.Block.Type.TEXT:\n                    # Check if the block is within the current page\n                    if block.layout.page_number == page.page_number:\n                        # Extract the text from the block\n                        block_text = block.text_anchor.text\n                        # Append the text to the page text\n                        page_text += block_text\n            book_text.append(page_text)\n\n    return book_text\n\n\n\u00a0\n\nThis version of the code loops through each block on each page to determine whether it is a text block and whether the block belongs on the current page. We take the text from the block and add it to the page text if both conditions are satisfied. The `book_text` list is then updated with the page text.\n\nHope it helps!"
			},
			{
				"Answer_created_time": "2023-05-04T00:19:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks. I noticed that my field mask was \"text\". I changed it to \"text,pages.layout\" and it worked."
			}
		]
	},
	{
		"Question_title": "Google Retail Facet Search: How to get all facets where result count is not empty in search response",
		"Question_created_time": "2023-04-03T19:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Retail-Facet-Search-How-to-get-all-facets-where-result/td-p/540416/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 105,
		"Question_body": "Dear Developers,I am currently working on implementing Google Retail Search in my application. The challenge I am facing is that I have over 200 custom product attributes, making it impractical to add all of them in the FacetSpecs during searching requests. However, I need to retrieve facets value and their count to build a user-friendly filter.I attempted to use dynamic Facets, but unfortunately, I could not find any specific documentation on how to retrieve dynamic facet data when using Search API. Hence, I am requesting suggestions from experienced developers within the community. Any help provided will be greatly appreciated.Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-04T17:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for summarizing"
			},
			{
				"Answer_created_time": "2023-05-02T07:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello\u00a0@RayOvO\u00a0did you find a solution to this. I too am in a same situation."
			}
		]
	},
	{
		"Question_title": "Dialogflow cx v3 detect intent returning event handler not defined",
		"Question_created_time": "2023-01-17T07:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-cx-v3-detect-intent-returning-event-handler-not/td-p/510845/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 172,
		"Question_body": "Hi, I am getting this error in some detectIntent requests:  No handler is defined for the event.Example:\nIn this page, trying to call the register_finished event:In the conversation history, we can see that the user was in this page when the request was made:Conversation Id: \"070077b0-9673-11ed-990e-bf1f225bb3df\"ps: in other pages this error also happens sometimes",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-18T12:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, It appears that there are various factors for triggering this error. (Naming mismatch, Having another session open while using the same flow... etc) I would suggest to contact Google Support [1] to further investigate this concern as it is not recommended to share some of the project details in public for investigation.\n\n[1]https://cloud.google.com/contact\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-18T12:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, It appears that there are various factors for triggering this error. (Naming mismatch, Having another session open while using the same flow... etc) I would suggest to contact Google Support [1] to further investigate this concern as it is not recommended to share some of the project details in public for investigation.\n\n[1]https://cloud.google.com/contact"
			},
			{
				"Answer_created_time": "2023-05-02T00:45:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThis error usually happens because you are trying to call an event using detectintent inside a context where it is not defined.\nProbably your session moved to a flow where you did not define the event \"register_finished\".\n\nHope this helps!"
			}
		]
	},
	{
		"Question_title": "empty train model (Train) of machine translation GOOGLE ADVANCED",
		"Question_created_time": "2023-04-28T07:08:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/empty-train-model-Train-of-machine-translation-GOOGLE-ADVANCED/td-p/548351/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 65,
		"Question_body": "Hello community, Since the migration from Google AutoML to Google Advanced I can upload a dataset like before but then the \"Train\" button does not respond: there seems there is nothing to be trained. Has anyone faced this problem with Google Advanced when training machine translation models? I leave a screenshot below: Thanks in advance and kind regards, Clara ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-28T18:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@claraginovart,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, please note that in order to access your upgraded resources you will be using the Cloud Translation Advanced API not the AutoML API. Listed below are some possible solutions to the problem:\u00a0\n\n1. If you are using an API to import data into datasets unlike in legacy (AutoML) where you are using CSV file to specify the location of the source file in CGS, in native (Cloud Translation) you need to specify the locations of TSV and TMX files in CGS. You can learn more about the difference of legacy and native resources using this link:\nhttps://cloud.google.com/translate/docs/advanced/automl-upgrade#differences_between_legacy_and_nativ...\n\n2. You need to update your code to use the Cloud Translation API and call the native resource IDs not the legacy resource IDs, although legacy and native are identical they don't have the same resource IDs, same goes for newly created resources and translation predictions where you must use Cloud Translation API rather than\u00a0AutoML API. You can check these documentations to learn more:\n\u00a0https://cloud.google.com/translate/docs/advanced/automl-upgrade#cloud-translation-api\nhttps://cloud.google.com/translate/docs/advanced/translating-text-v3\nhttps://cloud.google.com/translate/docs/advanced/automl-models\n\n\n3. You need to attach the necessary roles in order for the service account to work correctly. To learn more about the necessary roles, you can check this link.\u00a0https://cloud.google.com/translate/docs/intro-to-v3#iam\nIf you are also using the features of GCS or AutoML models, you need to include the necessary roles to the service account.\u00a0\n\n4. Verify your dataset to ensure it is in the proper format and has sufficient data to train the model. To see if the training procedure begins, you may also try uploading a smaller dataset.\n\nHope it helps!\n\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-28T18:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@claraginovart,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, please note that in order to access your upgraded resources you will be using the Cloud Translation Advanced API not the AutoML API. Listed below are some possible solutions to the problem:\u00a0\n\n1. If you are using an API to import data into datasets unlike in legacy (AutoML) where you are using CSV file to specify the location of the source file in CGS, in native (Cloud Translation) you need to specify the locations of TSV and TMX files in CGS. You can learn more about the difference of legacy and native resources using this link:\nhttps://cloud.google.com/translate/docs/advanced/automl-upgrade#differences_between_legacy_and_nativ...\n\n2. You need to update your code to use the Cloud Translation API and call the native resource IDs not the legacy resource IDs, although legacy and native are identical they don't have the same resource IDs, same goes for newly created resources and translation predictions where you must use Cloud Translation API rather than\u00a0AutoML API. You can check these documentations to learn more:\n\u00a0https://cloud.google.com/translate/docs/advanced/automl-upgrade#cloud-translation-api\nhttps://cloud.google.com/translate/docs/advanced/translating-text-v3\nhttps://cloud.google.com/translate/docs/advanced/automl-models\n\n\n3. You need to attach the necessary roles in order for the service account to work correctly. To learn more about the necessary roles, you can check this link.\u00a0https://cloud.google.com/translate/docs/intro-to-v3#iam\nIf you are also using the features of GCS or AutoML models, you need to include the necessary roles to the service account.\u00a0\n\n4. Verify your dataset to ensure it is in the proper format and has sufficient data to train the model. To see if the training procedure begins, you may also try uploading a smaller dataset.\n\nHope it helps!"
			},
			{
				"Answer_created_time": "2023-05-02T00:45:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi Kvandres,\u00a0\n\nI think it's solved. I was missing the role of viewer! Thank you so much!!\u00a0\n\nBest regards,\u00a0\n\nClara"
			}
		]
	},
	{
		"Question_title": "Google Translation API",
		"Question_created_time": "2023-04-30T14:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translation-API/td-p/548941/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Hello,\n\nI am using google translation API with a defined glossary config.\n\nWhen using a glossary with languages like es, pt etc... everything works fine.\n\nbut when creating a glossary that support hebrew language I always get an empty string back\n\nI tried to check the file encoding and its UTF-8, and from what I understand this is the right format. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-05-01T16:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@wisam112\u00a0,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this error, you can validate this list if it solves the problem. This might be an issue with the API request that you are trying to make since the results are null.\u00a0\n\n1. If you have created a new glossary, please make sure you have a specified language pair. These are\u00a0source_language_code which will be the source of your language and the\u00a0target_language_code in this case (iw) which is basically the language that you are translating it to as well as the right file path of your GCS inside the request.json file. If you have recently created it, it will take some time to complete, make sure that the state is running. you can learn more here:\u00a0https://cloud.google.com/translate/docs/advanced/glossary#operation_status\n\n2.\u00a0 Please make sure also that you have validated the right path for your glossary in your glossaryConfig before submitting the API request. You check this documentation for further details and sample code:\nhttps://cloud.google.com/translate/docs/advanced/glossary#v3\n\n\n3. Try checking if your glossary exists and has the right configurations, using this code:\u00a0\n\n\u00a0\n\nGET https://translation.googleapis.com/v3/projects/PROJECT_NUMBER_OR_ID/locations/us-central1/glossaries/glossary-id\n\n\u00a0\n\nYou can check this link for more information:\u00a0https://cloud.google.com/translate/docs/advanced/glossary#get_information_about_a_glossary\n\n4. Verify if words are not included in the stopword. A stopword will make the Cloud Translation ignore what is in the glossary, this means even if it is included in the glossary but it is a stopword, it will result to empty. You can check this link for more information:\u00a0\u00a0https://cloud.google.com/translate/docs/advanced/glossary#stopwords\n\nYou can also reach out to cloud support as this might need further assistance. You can use this link:\u00a0https://cloud.google.com/support\n\nHope it helps!"
			}
		]
	},
	{
		"Question_title": "Out of the box solution for regression problem to get daily prediction",
		"Question_created_time": "2023-04-29T19:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Out-of-the-box-solution-for-regression-problem-to-get-daily/td-p/548817/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 18,
		"Question_body": "hi team , im new to GCP.want to know is there readily available service /product to work on regression problem to get prediction at day level. Or any combination that can be used situation is Historical data  do not have consistency at a day level , but it does have consistent pattern at month level. how to leverage such data to ensure day level prediction accuracy above 80",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-29T21:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What kind of data do you have?\u00a0 Do you have (for example), sales by day and you want to predict future sales?\u00a0 What models have you tried?\u00a0 What do you mean by \"have a consistent pattern at month level\"?\u00a0 \u00a0My first take based on your post would be to try an apply an ARIMA model using BigQuery ML ... see for example here.\u00a0 Using this modeling technique, if you have historical data, you can train a new model and make a forecast for the future."
			}
		]
	},
	{
		"Question_title": "Which service is suitable for speaker verification?",
		"Question_created_time": "2023-04-27T06:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Which-service-is-suitable-for-speaker-verification/td-p/547855/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 81,
		"Question_body": "Hello I am new in google cloud. Which service I can use to build a speaker verification api and use it in the app can you guide me I found speaker-id but I think I can't use it with python.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-28T16:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Good day\u00a0@sparkingdark,\n\nWelcome to Google Cloud Community!\u00a0\n\nSpeaker ID is integrated with Dialogflow CX which is a platform in Google Cloud. Here is a short description of what Dialogflow CX is based on the documentation.\n\nDialogflow CX provides a new way of designing agents, taking a state machine approach to agent design. This gives you clear and explicit control over a conversation, a better end-user experience, and a better development workflow. You can learn more using this link:\u00a0https://cloud.google.com/dialogflow/cx/docs\n\nThis will basically help you simplify the flow and complexity of the conversations and will give you full control of the conversations thru virtual agents. Here are some useful links that will help you understand the fundamentals of the Dialogflow CX.\u00a0\nhttps://cloud.google.com/dialogflow/cx/docs/basics\nhttps://cloud.google.com/dialogflow/cx/docs/video\nhttps://cloud.google.com/dialogflow/cx/docs/quick/setup\n\nHere is a documentation on how to create a virtual agent:\u00a0https://cloud.google.com/dialogflow/cx/docs/quick/build-agent\u00a0\n\nYou can create, delete and manage the agents thru API with python, here is an example:\u00a0https://cloud.google.com/dialogflow/cx/docs/how/agent-create-api\n\nYou can specify how to handle the interactions thru API or Integration. Interactions are the responses from Dialogflow after receiving the input from the user. You can learn more here:\nhttps://cloud.google.com/dialogflow/cx/docs/quick/api\nhttps://cloud.google.com/dialogflow/cx/docs/quick/integration\n\nThank you for being part of the Google Cloud Community, Hope this will help!"
			},
			{
				"Answer_created_time": "2023-04-28T16:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Now my question is can I use it as a api endpoint where I can just send a input speech as base64 and enroll user. After that I can verify the user by calling another endpoint. I don't need that ivr calling part. I want to integrate directly in the app.\n\n\u00a0\n\nIt's more like App--> /enroll --> dialogflow agent next App--> /verify--> dialogflow agent . So like no extra conversation I need.Just I want to make a agent with speakerid. Is it possible?"
			}
		]
	},
	{
		"Question_title": "Dialogflow CX - Voice bot takes too long to response",
		"Question_created_time": "2023-04-28T06:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-Voice-bot-takes-too-long-to-response/td-p/548348/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 63,
		"Question_body": "I have a voice Bot integrated with the Cisco Webex contact center. This BOT is used for collecting customers' names when they call in. But after collecting the name, the BOT is taking almost 15 seconds of silence before moving to the next prompt. Is there any setting to reduce the speech timeout option so that immediately after the agent enters a full name, the BOT will go to the next option?Thanks in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-28T11:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@manojv\u00a0\n\nWelcome to Google Cloud Community.\n\nThe speech timeout parameter in your voice bot can be changed to ensure that it advances to the following question as soon as the agent inputs a full name. The platform or application you use to build and administer your bot may determine the precise processes you follow, however the following are some broad guidelines:\n\nAccess your voice bot's settings. To do this, you might need to enter into a dashboard or control panel or directly access the bot's configuration files.\nAdjust the setting to a lower value. Depending on the specific platform or software you are using, you may be able to set this value in seconds or milliseconds. Start with a low value (e.g., 1 second) and test to see if this is enough time for your agents to enter a full name. If necessary, you can increase the timeout setting gradually until you find the optimal value.\nSave your changes and test the bot to make sure the new timeout setting is working as expected.\n\nPlease take note also that reducing the speech timeout setting too much could result in the bot moving on to the next prompt before the agent has finished speaking, so it's important to find the right balance between responsiveness and accuracy. A setting for speech or voice recognition timeouts should be found. The term \"speech timeout,\" \"recognition timeout,\" or a phrase to that effect may be used to describe this.\n\nHere are some documentation that you may use as a reference:\nhttps://cloud.google.com/blog/topics/partners/updates-on-google-collaborations-with-cisco-featured-a...\nhttps://cloud.google.com/cisco?_ga=2.146133492.-1392753435.1676655686\nhttps://cloud.google.com/dialogflow/es/docs/integrations?_ga=2.146133492.-1392753435.1676655686"
			},
			{
				"Answer_created_time": "2023-04-28T12:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you Aris for the response.\n\nChanging speech timeout is tricky because this setting is valid before the customer speaks any words also. Is there any setting to immediately timeout the speech right after the customer said a valid value? For example, if the customer is asked for his/her name and got a speech response with a first name or first name and last name, then immediately move to the next prompt. Another one is, the customer is asked an 8-digit case number, and immediately after the customer said the 8th digit(matching the entity type with regex) moved to the next prompt. If I use speech timeout with a lower value on the above scenarios, the value will impact if there is delay/silence from the customer before they start talking."
			}
		]
	},
	{
		"Question_title": "Looking for a Tool to Refine Object Recognition AI Model on Vertex AI",
		"Question_created_time": "2023-04-23T05:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Looking-for-a-Tool-to-Refine-Object-Recognition-AI-Model-on/td-p/546397/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "I would like to know if there is a tool that would allow me to refine the image object recognition artificial intelligence model developed on vertex AI that I made. I recognize several objects in an image.\nI would like to send thousands of images to this model and depending on the model's response, I can validate the response, or modify it by adding or removing the label frames.\nI would also like to be able to display the answers according to their scores.\nI don't know if Google offers such a tool or if I have to go through a specialized company.Thank you for your help.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-24T09:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@CJG64,\n\nWelcome to Google Cloud Community.\n\nVertex AI Data classifying Service enables you to improve an image object recognition AI model by manually classifying thousands of images, which you can then use to retrain your model with the new labeled data.\n\nYou can set up instructions for labelers to follow and create custom labeling jobs using the Data Labeling Service. By adding or removing label frames, labelers can validate or alter the model's response. You can also see the outcomes of the labeling task as well as the rankings of each label for a specific image.\n\nAfter you've finished tagging, you can retrain your model to increase accuracy using the labeled data. Using the AutoML Image Training, you can accomplish this.\n\nHere are some documentation you may use as a reference:\nhttps://cloud.google.com/vertex-ai/docs/samples/aiplatform-create-data-labeling-job-images-sample?_g...\nhttps://cloud.google.com/vertex-ai/docs/samples/aiplatform-create-data-labeling-job-specialist-pool-...\nhttps://cloud.google.com/vertex-ai/docs/datasets/data-labeling-job?_ga=2.175503714.-1392753435.16766...\nhttps://cloud.google.com/vertex-ai/docs/datasets/label-using-console?_ga=2.175503714.-1392753435.167...\nhttps://cloud.google.com/ai-platform/data-labeling/pricing?_ga=2.175503714.-1392753435.1676655686"
			},
			{
				"Answer_created_time": "2023-04-28T02:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If i have been clear, and it s good, U made my YEARS. I see that and i come back. Thank you\u00a0@Aris_O"
			},
			{
				"Answer_created_time": "2023-04-28T07:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@CJG64,\n\nI'm glad to help."
			}
		]
	},
	{
		"Question_title": "gcloud ml speech recognize, option for setting phrase/hint boost",
		"Question_created_time": "2023-04-21T03:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/gcloud-ml-speech-recognize-option-for-setting-phrase-hint-boost/td-p/545966/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 53,
		"Question_body": "Hi,I am using the the gcloud ml speech recognize command line tool to test speech to text transcriptions. I am looking for a way to set the boost level for phrases/hints that are supplied in the --hints argument - similar to what can be set in RecognitionConfig of the REST API. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-24T09:23:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi @eoghanoh,\n\nWelcome to Google Cloud Community.\n\nThe `gcloud ml speech recognize` command-line tool may not provide an option to set the boost level for hints or phrases. This is because the `gcloud ml speech recognize` tool uses the older `v1` version of the Speech-to-Text API, which doesn't support setting boost levels for hints or phrases.\n\nHowever, you can use the REST API directly to set boost levels for hints and phrases. You can do this by creating a RecognitionConfig JSON object with the appropriate boost levels, and passing it to the Speech-to-Text API via a POST request. You can use a program like `curl` or `httpie` to send a `POST` request to the Speech-to-Text API with this `RecognitionConfig`JSON object.To use the boost capability, you must activate the Speech-to-Text API's `v1p1beta1` version.\n\nHere are some documentations you may use as a reference:\nhttps://cloud.google.com/speech-to-text/docs/quickstart-client-libraries#before-you-begin\nhttps://cloud.google.com/speech-to-text/docs/adaptation-model?_ga=2.149903862.-1392753435.1676655686\nhttps://cloud.google.com/sdk/gcloud/reference/alpha/ml/speech/recognize?_ga=2.149903862.-1392753435....\nhttps://cloud.google.com/speech-to-text/docs/reference/rest/v1p1beta1/projects.locations.phraseSets?...\nhttps://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data?_ga=2.149903862.-139...\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-24T09:23:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi @eoghanoh,\n\nWelcome to Google Cloud Community.\n\nThe `gcloud ml speech recognize` command-line tool may not provide an option to set the boost level for hints or phrases. This is because the `gcloud ml speech recognize` tool uses the older `v1` version of the Speech-to-Text API, which doesn't support setting boost levels for hints or phrases.\n\nHowever, you can use the REST API directly to set boost levels for hints and phrases. You can do this by creating a RecognitionConfig JSON object with the appropriate boost levels, and passing it to the Speech-to-Text API via a POST request. You can use a program like `curl` or `httpie` to send a `POST` request to the Speech-to-Text API with this `RecognitionConfig`JSON object.To use the boost capability, you must activate the Speech-to-Text API's `v1p1beta1` version.\n\nHere are some documentations you may use as a reference:\nhttps://cloud.google.com/speech-to-text/docs/quickstart-client-libraries#before-you-begin\nhttps://cloud.google.com/speech-to-text/docs/adaptation-model?_ga=2.149903862.-1392753435.1676655686\nhttps://cloud.google.com/sdk/gcloud/reference/alpha/ml/speech/recognize?_ga=2.149903862.-1392753435....\nhttps://cloud.google.com/speech-to-text/docs/reference/rest/v1p1beta1/projects.locations.phraseSets?...\nhttps://cloud.google.com/speech-to-text/docs/best-practices-provide-speech-data?_ga=2.149903862.-139..."
			},
			{
				"Answer_created_time": "2023-04-26T09:45:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the reply and information. I moved to use the Google speech python library with the v1p1beta1 API and was able to set the boost level.\n\nThanks again .\n\nEoghan"
			}
		]
	},
	{
		"Question_title": "Model training never finished",
		"Question_created_time": "2023-04-23T23:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Model-training-never-finished/td-p/546578/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 76,
		"Question_body": "Hi,",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-25T13:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@trofiv,\n\nThere could be several reasons why your model training is taking longer than the estimated 2-5 days mentioned in the documentation. Here are a few reason:\n\nDataset size: The size of your training dataset can significantly affect the training time. If you have a large dataset, it may take longer to train your models.\n\nModel complexity: The complexity of your models can also impact the training time. If you have many features or layers, it can take longer to train the model.\n\nComputing resources: The amount of computing resources allocated to your training job can also affect the training time. If you are using a smaller machine type or fewer workers, it may take longer to complete the training.\n\nJob queue: If there are many other training jobs in the queue, your job may need to wait for available resources, which can increase the training time.\n\nIssues with your dataset: If there are issues with your dataset, such as missing data or data formatting issues, it can impact the training time and potentially cause errors.\n\nIf you continue to experience long training times, you may want to investigate these possible issues and consider optimizing your dataset and models to reduce training time. You can also consider increasing the computing resources allocated to your training job to speed up the training process"
			},
			{
				"Answer_created_time": "2023-04-26T06:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for your reply.\n\nThe training ended magically yesterday.\nBut some of the models could not learn, although I fulfill the data requirements specified for the model \"Others You May Like\"\u00a0\nhere is an error from the log:\n\n{\n\"insertId\": \"17kg65hc1vf\",\n\"jsonPayload\": {\n\"context\": {\n\"reportLocation\": {\n\"functionName\": \"google.cloud.retail.v2main.ModelService.CreateModel\"\n}\n},\n\"@type\": \"type.googleapis.com/google.cloud.retail.logging.ErrorLog\",\n\"message\": \"INSUFFICIENT_TRAINING_DATA Recommendation model others-you_others-you_1682500691242 cannot be trained because of insufficient data.\",\n\"serviceContext\": {\n\"service\": \"recommendationengine.googleapis.com\"\n},\n\"status\": {\n\"@type\": \"type.googleapis.com/google.rpc.Status\",\n\"message\": \"Recommendation model others-you_others-you_1682500691242 cannot be trained because of insufficient data.\",\n\"code\": 9\n}\n},\n\"resource\": {\n\"type\": \"consumed_api\",\n\"labels\": {\n\"credential_id\": \"\",\n\"version\": \"\",\n\"project_id\": \"forward-subject-383106\",\n\"location\": \"global\",\n\"service\": \"recommendationengine.googleapis.com\",\n\"method\": \"google.cloud.retail.v2main.ModelService.CreateModel\"\n}\n},\n\"timestamp\": \"2023-04-26T09:53:17.475952289Z\",\n\"severity\": \"ERROR\",\n\"logName\": \"projects/forward-subject-383106/logs/recommendations-api-log\",\n\"receiveTimestamp\": \"2023-04-26T09:53:18.602174918Z\"\n}"
			}
		]
	},
	{
		"Question_title": "Can we recover a deleted dialogflow cx agent",
		"Question_created_time": "2023-04-26T00:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-we-recover-a-deleted-dialogflow-cx-agent/td-p/547328/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 25,
		"Question_body": "Hi need help with recovering an agent that was accidentally deleted from the Dialogflow CX Console. Can someone guide with potential steps that can be followed to recover this.\n\nFYI1. We donot have any backup blob files to restore from2. We had audit logging enabled incase if that can help",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-26T02:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I dont think so, but in any case, open a support ticket just in case.\n\nMy recommendation would be to create a daily backup by exporting the agent using the cxcli:\u00a0https://cxcli.xavidop.me/"
			}
		]
	},
	{
		"Question_title": "Triton on Vertex AI does not support multiple models?",
		"Question_created_time": "2022-08-25T07:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Triton-on-Vertex-AI-does-not-support-multiple-models/td-p/459822/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 305,
		"Question_body": "Currently, I want to deploy a Triton server to Vertex AI endpoint. However I received this error message.\"failed to start Vertex AI service: Invalid argument - Expect the model repository contains only a single model if default model is not specified\"Is this mean that the Triton server deploy only support one model? It is different from what I have read in this document about concurrent model executionhttps://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-07T13:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The error message suggest that you haven't selected a default model."
			},
			{
				"Answer_created_time": "2022-10-17T07:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I have the same issue and I couldn't find how to set a default model. Could you please link a guide about it or explain how to do that? Thanks"
			},
			{
				"Answer_created_time": "2023-04-25T23:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As specified in the documentation, ensure that you provide the flag\u00a0\n\n--container-args='--strict-model-config=false'\n\nWhile importing it into model registry as follows:\n\ngcloud ai models upload \\\n\u00a0 --region=LOCATION \\\n\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\n\u00a0 --container-image-uri=LOCATION-docker.pkg.dev/PROJECT_ID/getting-started-nvidia-triton/vertex-triton-inference \\\n\u00a0 --artifact-uri=MODEL_ARTIFACTS_REPOSITORY \\\n\u00a0 --container-args='--strict-model-config=false'"
			}
		]
	},
	{
		"Question_title": "How to insert JSON along with PDF into Document AI Warehouse using API",
		"Question_created_time": "2023-04-19T07:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-insert-JSON-along-with-PDF-into-Document-AI-Warehouse/td-p/545296/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 182,
		"Question_body": "Hi, \n\nOur usecase is to process the PDF documents from Document AI process and pass the JSON file along with the PDF to the document warehouse. I am using contentwarehouse.CreateDocumentRequest function, the function works well if I only supply the PDF document, but if I process the file from the document and push the JSON along with the PDF the it gives an error saying the following:File \"<ipython-input-63-d034c5106990>\", line 1, in <module>\nrunfile('C:/Users/HP/Downloads/test simple.py', wdir='C:/Users/HP/Downloads')File \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\nexecfile(filename, namespace)File \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\nexec(compile(f.read(), filename, 'exec'), namespace)File \"C:/Users/HP/Downloads/test simple.py\", line 110, in <module>\nprocess_document_sample(config['project_id'],config['location'],config['Custom_processor_id'],file_path,mime_type)File \"C:/Users/HP/Downloads/test simple.py\", line 88, in process_document_sample\ndoc=documentai.types.Document(docDictionary)File \"D:\\Anaconda3\\lib\\site-packages\\proto\\message.py\", line 566, in __init__\n\"Unknown field for {}: {}\".format(self.__class__.__name__, key)ValueError: Unknown field for Document: _pb Following is the code Snippet:def process_document_sample(\nproject_id: str,\nlocation: str,\nprocessor_id: str,\nfile_path: str,\nmime_type: str,\nfield_mask: str = None,\n\ud83d\ude1e\n# You must set the api_endpoint if you use a location other than 'us'.\nopts = storage.Client(project_id)\n\nclient = documentai.DocumentProcessorServiceClient()# The full resource name of the processor, e.g.:\n# projects/{project_id}/locations/{location}/processors/{processor_id}\nname = client.processor_path(project_id, location, processor_id)# Read the file into memory\nwith open(file_path, \"rb\") as image:\nimage_content = image.read()# Load Binary Data into Document AI RawDocument Object\nraw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)# Configure the process request\nrequest = documentai.ProcessRequest(\nname=name, raw_document=raw_document, field_mask=field_mask\n)result = client.process_document(request=request)\n\n# return result\n# TODO(developer): Uncomment these variables before running the sample.\n# project_number = 'YOUR_PROJECT_NUMBER'\n# location = 'YOUR_PROJECT_LOCATION' # Format is 'us' or 'eu'\n#print(result.document.entities)# Create a Schema Service client\nimport json\nwith open('test.json','w') as f:\njson.dump(documentai.Document.to_dict(result.document),f)\n# documentai.Document.to_dict(result)\ndocument_schema_client = contentwarehouse.DocumentSchemaServiceClient()# The full resource name of the location, e.g.:\n# projects/{project_number}/locations/{location}\nparent = document_schema_client.common_location_path(\nproject=config['project_number'], location=config['location']\n) # Create a Document Service client\ndocument_client = contentwarehouse.DocumentServiceClient()# The full resource name of the location, e.g.:\n# projects/{project_number}/locations/{location}\nparent = document_client.common_location_path(\nproject=config['project_number'], location=config['location']\n)\n#print(result.document._pb)\ndocDictionary = result.document.__dict__\ndoc=documentai.types.Document(docDictionary)\n# Define Document\ndocument = contentwarehouse.Document(\n# raw_document_file_type=1,\ndisplay_name=\"60.pdf\",\ndocument_schema_name=schema_URI,\ninline_raw_document=open('60.pdf','rb').read(),\n#plain_text=str(result.document)\ncloud_ai_document=doc\n)# Define Request\ncreate_document_request = contentwarehouse.CreateDocumentRequest(\nparent=parent, document=document\n)# Create a Document for the given schema\nresponse = document_client.create_document(request=create_document_request)\n# print(response)process_document_sample(config['project_id'],config['location'],config['Custom_processor_id'],file_path,mime_type)I have read all the documentation, but couldn't find why the dictionary is not being picked by the object. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T15:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@malimasood,\n\nWelcome to Google Cloud Community.\n\n\n\nIt looks like that the following line is to look for the error:\n\n\n\n\u00a0\n\ndoc = documentai.types.Document(docDictionary)\n\n\u00a0\n\nAlthough it appears that the Document class constructor does not accept a dictionary as a parameter, the docDictionary variable represents the Document object returned by the Document AI API as a dictionary.\n\nInstead, you should generate a new Document object from the dictionary representation using the from_dict class function of the Document class. Here's how to change your code such that it uses from_dict:\n\n\u00a0\n\ndoc = documentai.types.Document.from_dict(docDictionary)\n\n\u00a0\n\nAfter making this modification, the doc variable ought to have a legitimate Document object that you can give to the constructor of the CreateDocumentRequest method.\n\nHere are some documentation that might help you.\nhttps://cloud.google.com/document-ai/docs/reference/rest/v1/Document?_ga=2.116734662.-1392753435.167...\nhttps://cloud.google.com/document-ai/docs/handle-response?_ga=2.116734662.-1392753435.1676655686\nhttps://cloud.google.com/discovery-engine/media/docs/documents?_ga=2.138870387.-1392753435.167665568..."
			},
			{
				"Answer_created_time": "2023-04-25T22:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the prompt response, but I tried your solution it still throws an exception\n\n\n\n\n\nTraceback (most recent call last):\n\nFile \"<ipython-input-1-d034c5106990>\", line 1, in <module>\nrunfile('C:/Users/HP/Downloads/test simple.py', wdir='C:/Users/HP/Downloads')\n\nFile \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\nexecfile(filename, namespace)\n\nFile \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\nexec(compile(f.read(), filename, 'exec'), namespace)\n\nFile \"C:/Users/HP/Downloads/test simple.py\", line 110, in <module>\nprocess_document_sample(config['project_id'],config['location'],config['Custom_processor_id'],file_path,mime_type)\n\nFile \"C:/Users/HP/Downloads/test simple.py\", line 88, in process_document_sample\ndoc=documentai.types.Document.from_dict(docDictionary)\n\nAttributeError: type object 'Document' has no attribute 'from_dict'\n\n\nCode Changes:\ndocDictionary = result.document.__dict__\ndoc=documentai.types.Document.from_dict(docDictionary)"
			}
		]
	},
	{
		"Question_title": "How to know the total words count of the character per month that I have translated?",
		"Question_created_time": "2023-04-24T00:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-know-the-total-words-count-of-the-character-per-month/td-p/546588/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 154,
		"Question_body": "I use the Cloud Translation - Basic API and I know the pricing for this basic API. But I do not know how many words have been translated by this API this month. Where to check this information? I searched Google Cloud > API/Service Details > Could Translation API > METRICS and QUOTAS and still did not find the information.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-25T13:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Cindyyi\u00a0,\n\nWelcome to Google Cloud Community.\n\nYou can visit the Google Cloud Console and look under the \"APIs & Services\" area to see how the Cloud Translation - Basic API is being used. Find the \"Cloud Translation API\" in the list of APIs by selecting \"Dashboard\" from there.\n\nTo view the API specifics, including usage data, click on the \"Cloud Translation API\". You ought to see a graph that displays your API usage throughout the chosen time frame. With the help of the dropdown menu above the chart, you can change the time period.\n\nClick on the \"Quotas\" tab to get the Cloud Translation - Basic API's specific use information. You may view the current consumption for the chosen time period as well as the API quota limits there.\n\nIt should be noted that the Cloud Translation - Basic API counts the number of translated characters rather than words. The basic API has a monthly character cap of 500,000."
			}
		]
	},
	{
		"Question_title": "API for HITL UI and Operations",
		"Question_created_time": "2023-04-24T15:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/API-for-HITL-UI-and-Operations/td-p/546853/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 243,
		"Question_body": "Hello, is there a set of APIs for HITL operations - labeling, correcting values, adding new entities and training using a custom UI we are building? It appears that all HITL operations have to be performed using the Google Labelers workbench. Any alternative path you can recommend?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-25T13:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@sivramk,\n\nWelcome to Google Cloud Community.\n\nGoogle does provide a set of APIs for HITL tasks including labeling, updating values, adding new entities, and training with a unique user interface. You may build unique annotation tools and workflows, interface with well-known machine learning frameworks, and scale labeling projects using the Cloud Annotations API along with the Cloud Annotations API, Google also offers a number of other machine learning APIs that may be used for tasks like image categorization and text analysis, such as the Cloud Vision API and the Cloud Natural Language API.\n\nThe Google Labelers workbench is necessary for some HITL operations in Google's machine learning platform, such as data labeling for AutoML models. Nevertheless, there are further choices available for unique HITL operations, such as the aforementioned Cloud Annotations API.\u00a0\n\nHere are some documentation you may use as a reference:\nhttps://cloud.google.com/document-ai/docs/hitl/google-workforce\nhttps://cloud.google.com/healthcare-api/docs/concepts/annotations?_ga=2.150394614.-1392753435.167665..."
			}
		]
	},
	{
		"Question_title": "impact on dialog.flow es service due to the termination of the google action",
		"Question_created_time": "2023-04-24T22:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/impact-on-dialog-flow-es-service-due-to-the-termination-of-the/td-p/546887/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 17,
		"Question_body": "I saw a notice like this on google action. \"This Conversational Action will be removed on June 13, 2023.\"\nI use response designated as google assistant in dialog.flow ES, will it affect the dialog.flow ES service?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-25T00:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I dont think so, but it is fact that there is a modern version of ES which is Dialogflow CX"
			}
		]
	},
	{
		"Question_title": "Error from Vertex AI Getting predictions from custom trained models",
		"Question_created_time": "2023-04-12T19:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-from-Vertex-AI-Getting-predictions-from-custom-trained/td-p/543292/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 148,
		"Question_body": "I am trying to call an API to inference from a model I have uploaded to vertex AI.I have tried three methods, and none worked so far.At first, I was following a youtube from standford university, https://www.youtube.com/watch?v=fw6NMQrYc6w&t=3876s which uses ai platform.1. I also tried that, but I think google is trying to get rid of AI platform, and although I succesfully uploaded the model, it doesn't allow me to make a new version, basically allows me nothing.2. I tried to work this tutorial, https://codelabs.developers.google.com/vertex-p2p-predictions#5 and it keeps complaining that my payload is above 1.5MB limit, but my image is only 49KB, so it's ridiculous. maybe something happened in this code, but it's from the tutorial, so the tutorial must be wrong then.  3. Last, I've been trying to call the API from the sample code,https://github.com/googleapis/python-aiplatform/blob/main/samples/snippets/prediction_service/predic...but it gives me a json format error.I have referenced from this website to get the json format.https://github.com/googleapis/python-aiplatform/blob/main/samples/snippets/prediction_service/predic... The error I am getting is as is :   and the 400 error comes from API and the log from vertex AI is not very useful when debugging.Honestly I have been struggling with this issue for days and in my opinion, this should not be this difficult. My experience with GCP and vertex AI is very disappointing and I'm considering to explore other options. Please let me know if any of you have any advices. Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T23:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I actually solved this error by applying this document.\n\nhttps://cloud.google.com/vertex-ai/docs/samples/aiplatform-predict-image-classification-sample.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-13T15:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@jinmc,\n\nWelcome back to Google Cloud Community.\n\nBased on\u00a0analyzing the issues you've encountered here are some possible answer:\n\n1.\u00a0You may try referring to a more updated Vertex AI platform if you were utilizing an AI Platform-based lesson. For the Vertex AI documentation on GCP you may refer here: https://cloud.google.com/vertex-ai/docs.\n\n2.\u00a0Check the data type of your \"x_test\" variable if you are having problems with the payload size in the second technique you tried. The code you gave first turns the image into a NumPy array and then into a list of floats, but it doesn't display the shape of the list that results. The list might be significantly bigger than anticipated, which would push the payload above the cap. Before delivering the list to the endpoint, you can try printing the list's shape to check its size.\n\n3.\u00a0The third method's error indicates that the JSON payload you are submitting to the API is not in the proper format. Each instance of the JSON object in the payload should have a dictionary of input features. The code you supplied generates a nested dictionary with an \"image\" key and a dictionary with a single \"instances\" key. Consider making the payload a list of instances, where each instance is a dictionary with just one \"image\" entry.\n\nHere is an example on\u00a0how could you create the payload using a list of instances:\n\nencoded_content = base64.b64encode(image).decode(\"utf-8\")\ninstances = [{\"image\": {\"b64\": encoded_content}}]\n\nThis list of instances can then be added as the \"instances\" option in your API request.\n\nHere are some references that might help you:\nhttps://cloud.google.com/vertex-ai/docs/training-overview\nhttps://cloud.google.com/vertex-ai/docs/tutorials"
			},
			{
				"Answer_created_time": "2023-04-13T21:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I\u00a0\n\nHi\u00a0@Aris_O\u00a0 .\n\nThank you for quick reply.\u00a0\n\nI have applied the change you have suggested and gives this error.\n\n400 { \"error\": \"Failed to process element: 0 key: image of 'instances' list. Error: Invalid argument: JSON object: does not have named input: image\" }\n\nAlso, I am not training on vertex AI, I am just using vertex AI as a serving tool,\u00a0\n\nso it might be a model issue.\u00a0\n\nI might try another model.\u00a0\n\nAgain, thanks for your reply."
			},
			{
				"Answer_created_time": "2023-04-20T23:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I actually solved this error by applying this document.\n\nhttps://cloud.google.com/vertex-ai/docs/samples/aiplatform-predict-image-classification-sample."
			}
		]
	},
	{
		"Question_title": "What is the best way to use credentials for API calls from databricks notebook?",
		"Question_created_time": "2023-04-19T07:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-is-the-best-way-to-use-credentials-for-API-calls-from/td-p/545303/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 137,
		"Question_body": "Hello, I have an Databricks account on Azure, and the goal is to compare different image tagging services from GCP and other providers via corresponding API calls, with Python notebook. I have problems with GCP vision API calls, specifically with credentials: as far as I understand, the one necessary step is to set 'GOOGLE_APPLICATION_CREDENTIALS' environment variable in my databricks notebook with something like os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='/folder1/credentials.json' where '/folder1/credentials.json' is the place my notebook looks for json file with credentials (notebook is in the same folder, /folder1/notebook_api_test).I am getting this path by looking into Workspace-> Copy file path in the Databricks web page.But this approach doesn't work, when cell is executed, I am getting this error: DefaultCredentialsError: File /folder1/credentials.json was not found. What is the right way to deal with credentials to access google vision API from Databricks notebook?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T16:17:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Ok, here is a trick: in my case, the file with GCP credentials is stored in notebook workspace storage, which is not visible to os.environ() command. So solution is to read a content of this file, and save it to the cluster storage attached to the notebook, which is created with the cluster and is erased when cluster is gone (so we need to repeat this procedure every time the cluster is re-created). According to this doc, we can read the content of the credentials json file stored in notebook workspace with\n\n\u00a0 \u00a0 \u00a0 \u00a0 with open('/Workspace/folder1/cred.json'): #note that I need a full path here, for some reason\n\u00a0 \u00a0 \u00a0 \u00a0 content = f.read()\n\nand then according to this doc, we need to save it on another place in a new file (with the same name in my case, cred.json), namely on cluster storage attached to the notebook (which is visible to os-related functions, like os.environ()), with\n\n\u00a0 \u00a0 \u00a0 \u00a0 fd = os.open(\"cred.json\", os.O_RDWR|os.O_CREAT)\n\u00a0 \u00a0 \u00a0 \u00a0 ret = os.write(fd,content.encode())\n\u00a0 \u00a0 \u00a0 \u00a0 #need to add .encode(), or will get TypeError: a bytes-like object is required, not 'str'\n\u00a0 \u00a0 \u00a0 \u00a0 os.close(fd)\n\nOnly after that we can continue with setting an environment variable, required for GCP authentication:\n\n\u00a0 \u00a0 \u00a0 \u00a0 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='./cred.json'\n\nand then API calls should work fine, without DefaultCredentialsError.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-20T15:16:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@lugger1,\n\nWelcome to Google Cloud Community.\n\nYou must enter your authentication credentials in order to use Google Cloud services, such the Vision API in Databricks.\n\nTo properly set up your Google Cloud credentials in your Databricks notebook, follow these steps:\n\nIn your Google Cloud project, create a service account with the rights required to utilize the Vision API.\nDownload the service account's JSON key file.\nUpload the JSON key file to your Databricks workspace in your Databricks notebook or to a cloud storage platform like Azure Blob Storage.\nSet the path to the JSON key file's location in the GOOGLE_APPLICATION_CREDENTIALS environment variable in your notebook.\n\nAfter setting the environment variable GOOGLE_APPLICATION_CREDENTIALS, you ought to be able to utilize the Google Cloud Vision API in your notebook without encountering the DefaultCredentialsError.\n\nHere are some documentations that might help you:\nhttps://cloud.google.com/databricks?_ga=2.82645974.-1392753435.1676655686\nhttps://cloud.google.com/blog/products/data-analytics/databricks-on-google-cloud?_ga=2.82645974.-139..."
			},
			{
				"Answer_created_time": "2023-04-20T16:17:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Ok, here is a trick: in my case, the file with GCP credentials is stored in notebook workspace storage, which is not visible to os.environ() command. So solution is to read a content of this file, and save it to the cluster storage attached to the notebook, which is created with the cluster and is erased when cluster is gone (so we need to repeat this procedure every time the cluster is re-created). According to this doc, we can read the content of the credentials json file stored in notebook workspace with\n\n\u00a0 \u00a0 \u00a0 \u00a0 with open('/Workspace/folder1/cred.json'): #note that I need a full path here, for some reason\n\u00a0 \u00a0 \u00a0 \u00a0 content = f.read()\n\nand then according to this doc, we need to save it on another place in a new file (with the same name in my case, cred.json), namely on cluster storage attached to the notebook (which is visible to os-related functions, like os.environ()), with\n\n\u00a0 \u00a0 \u00a0 \u00a0 fd = os.open(\"cred.json\", os.O_RDWR|os.O_CREAT)\n\u00a0 \u00a0 \u00a0 \u00a0 ret = os.write(fd,content.encode())\n\u00a0 \u00a0 \u00a0 \u00a0 #need to add .encode(), or will get TypeError: a bytes-like object is required, not 'str'\n\u00a0 \u00a0 \u00a0 \u00a0 os.close(fd)\n\nOnly after that we can continue with setting an environment variable, required for GCP authentication:\n\n\u00a0 \u00a0 \u00a0 \u00a0 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='./cred.json'\n\nand then API calls should work fine, without DefaultCredentialsError."
			}
		]
	},
	{
		"Question_title": "Vertex AI Pipeline Template Schema Issue/Question",
		"Question_created_time": "2023-04-19T09:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Pipeline-Template-Schema-Issue-Question/td-p/545368/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "Hi all,When compiling our artifacts to yaml file, we are expecting the following schema format in our runtime parameter value : https://github.com/kubeflow/pipelines/blob/master/api/v2alpha1/pipeline_spec.proto#L677 When I first tried to upload my yaml file with the following schema above and try to run the pipeline template, the runtime value treats everything as a string instead of converting it to it's appropriate data type. Here is the yaml file I tried to upload and run:\n```",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T15:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @hobb,\n\nWelcome back to Google Cloud Community.\n\nThe issue you are facing is that the runtime parameter value treats everything as a string instead of converting it to its appropriate data type. One possible solution to this problem is to modify your YAML file to use YAML tags to specify the data types of your runtime values.\n\nYou may use YAML tags to specify the data type of each runtime value.\nFor example, to specify an integer value, you can use the !!int tag, and to specify a string value, you can use the !!str tag.\n\nHere is an example of how to modify your YAML file to use YAML tags:\n\nroot:\ndag:\ntasks:\ninput_data:\ninputs:\nparameters:\nallow_large_results_flag:\nruntimeValue:\nconstantValue: !!int 1\nallow_pre_computation_flag:\nruntimeValue:\nconstantValue: !!int 0\ncreate_disposition:\nruntimeValue:\nconstantValue: !!str CREATE_IF_NEEDED\ncustom_config:\ncomponentInputParameter: custom-config\nlabels:\nruntimeValue:\nconstantValue: !!str 'null'\nnon_artifact_input_table:\ncomponentInputParameter: input-table\nunion_bq_shards_flag:\nruntimeValue:\nconstantValue: !!int 0\nwrite_disposition:\nruntimeValue:\nconstantValue: !!str WRITE_EMPTY\n\nIn this modified YAML file, you have to use YAML tags to specify the data types of each runtime value. For example, we have used the !!int tag to specify that the allow_large_results_flag and allow_pre_computation_flag runtime values are integers, and we have used the !!str tag to specify that the create_disposition, labels, and write_disposition runtime values are strings.\n\nBy using YAML tags to specify the data types of your runtime values, the Kubeflow Pipelines runtime should\nbe able to correctly interpret the data types of your runtime values, instead of treating everything as a string.\n\nHere are some documentation that might help you:\n\n\n\nhttps://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template?_ga=2.218108633.-13927534...\n\nhttps://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline?_ga=2.218108633.-1392753435.1676655..."
			}
		]
	},
	{
		"Question_title": "How to replace whitespace with an empty string in Dialog flow ES",
		"Question_created_time": "2023-04-19T05:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-replace-whitespace-with-an-empty-string-in-Dialog-flow-ES/td-p/545247/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 44,
		"Question_body": "Hi, I would like to replace the whitespace with an empty string in Dialog flow ES.I tried with SUBSTIUTE and REPLACE method but no luck. Looks like these 2 options are for Dialog flow CX and not for ES. Lets say, If caller says, ab1234 then getting an output 'a b 1 2 3 4' instead of ab1234.I have configured the regular expression like this ^[a-zA-Z]{2}[0-9]*$ Looking out for an assistance. Regards,Sathish",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T14:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@sathish11k,\n\nWelcome to Google Cloud Support.\n\nFor all whitespace characters, Dialogflow ES's REGEX_REPLACE function can be used to replace them with an empty string.\n\nHere is an sample of how you can apply it:\n\nAdd a parameter for the input text in the Intent editor. Call it \"input_text\" for now.\nAdd a custom payload with the JSON formatted as follows to the response section:\n{\n\"fulfillment_text\": \"Here is your processed input: $output_text\",\n\"output_contexts\": [\n{\n\"name\": \"projects/<project_id>/agent/sessions/<session_id>/contexts/<context_name>\",\n\"lifespanCount\": 5,\n\"parameters\": {\n\"output_text\": \"@sys.text\"\n}\n}\n]\n}\n\nUse the following code in the fulfillment section to replace any whitespace with an empty string:\n\nfunction processInputText(agent) {\nconst inputText = agent.parameters.input_text;\nconst outputText = inputText.replace(/\\s/g, '');\nagent.add(`Here is your processed input: ${outputText}`);\n}\n\n// This is to set the Dialogflow Fulfillment handler function\nexports.dialogflowFirebaseFulfillment = functions.https.onRequest(app);\n\nPlease take note that the code above is presuming that you are utilizing Firebase Functions and the Dialogflow fulfillment webhook. For your particular use case, you might need to alter it. The regular expression you supplied also only recognizes input sequences that begin with two alphabetic letters, followed by zero or more digits. You may also use the regular expression [a-zA-Z0-9]*$ to match strings that contain any alphabetic and numeric combination.\n\nHere are some documentation that might help you:\nhttps://cloud.google.com/dialogflow/es/docs/reference/rest/v2/projects.agent.intents?_ga=2.35954656....\nhttps://cloud.google.com/dialogflow/es/docs/speech-adaptation?_ga=2.35954656.-1150969476.1678208220\nhttps://cloud.google.com/dialogflow/es/docs/reference/rest/v2/projects.agent.intents?_ga=2.110976069..."
			}
		]
	},
	{
		"Question_title": "Language was support with machine translation engine and not supported any more or got error 400",
		"Question_created_time": "2023-04-18T23:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Language-was-support-with-machine-translation-engine-and-not/td-p/545148/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 60,
		"Question_body": "- Last time we work well when choose Arabic Iraq to deal with Kurdish Sorani on google machine engine, but now this is not run well and give us Arabic or kurdish Kurmanji translation instead.\nis this a reason for that? how to fix? how we supposed to deal with Kurdish Sorani now?\n- also for Dari language I got the following error while I read that it is supported from google machine translation engine:\nPretranslation from the machine translation did not complete. Detail: Google Translate API returned error 400: Bad language pair: en|prs (additional info: language pair en-prs is not supported) ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T14:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Doaafoudah,\n\nWelcome back to Google Cloud Community.\n\nHere are some might possible answer to your encountered issues:\n\nIt may be possible that revisions to the machine translation model have affected the translation quality for the first query regarding Kurdish Sorani. It's also possible that the difficulty stems from the particular text being translated or from the manner in which it is being entered into the translation engine. You might experiment with various text inputs or change the translation engine's settings to resolve this. To ensure proper translations, you can also try a different machine translation engine or human translator.\nIt may be possible that the Dari language error message you saw was brought on by the Google Translate API not currently supporting the language combination. To make sure that proper translations in this situation, you might try utilizing a different machine translation engine that supports the language pair or you could think about hiring a human translator.\nYou may also ask a question about Cloud Translation on\u00a0Stack Overflow. Use the tag\u00a0google-translate\u00a0for questions about the Cloud Translation API. This tag not only receives responses from the Stack Overflow community, but also from Google engineers, who monitor the tag and offer unofficial support.\n\n- Discuss Cloud Translation\u00a0the\u00a0google-translate-api\u00a0Google group to discuss the Cloud Translation API and receive Cloud Translation announcement and updates.\n\nYou can also visit the Google Cloud\u00a0Slack community\u00a0to discuss the Cloud Translation API and other Google Cloud products. If you haven't already joined,\u00a0use this form to sign up.\n\n- From the Cloud Translation documentation, click the \"Send feedback\" link near the top right of the page. This opens a feedback form. The comments you submit will be reviewed by the Cloud Translation team.\n\nHere are some references that might help you:\n\nhttps://cloud.google.com/translate/docs/intro-to-v3?_ga=2.152466553.-1150969476.1678208220\nhttps://cloud.google.com/translate/docs/languages?_ga=2.152466553.-1150969476.1678208220\nhttps://cloud.google.com/translate/docs/getting-support?_ga=2.152466553.-1150969476.1678208220"
			}
		]
	},
	{
		"Question_title": "AUTHORIZATION RELATED ERROR using TTS API",
		"Question_created_time": "2023-04-20T05:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AUTHORIZATION-RELATED-ERROR-using-TTS-API/td-p/545629/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 35,
		"Question_body": "Hi,I am using the following code to see if i can use the text to speech api. I am getting a consistent 401 authorization error. I plan to use google cloud text to speech in my existing web app, which was originally using web speech api. I am unable to get the google api to work. Please assist. Thanks. Here is the code I am using:",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-20T06:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In your code, it isn't clear what you are actually passing in for authentication.\u00a0 My guess is that you have created a service account and are passing in a credentials key.\u00a0 What you need to pass is an OAuth2.0 access token.\u00a0 See this link.\u00a0 The notion is that you are invoking a Google Cloud Service directly from the Internet.\u00a0 You need to thus prove to Google Cloud who you are.\u00a0 To do that, you go through an authentication step in the browser (think a login).\u00a0 Google will then create an access token and then you will pass that access token back in the API requests."
			}
		]
	},
	{
		"Question_title": "Parsing single digit in table with Form parser",
		"Question_created_time": "2023-04-18T20:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Parsing-single-digit-in-table-with-Form-parser/td-p/545112/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 62,
		"Question_body": "I am trying to scan the following image with the form parser (pretrained-form-parser-v2.0-2022-11-10) but it does not parse the single-digit number correctly. Any good idea to work around this issue? Can I expect this will be solved in the next coming version?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-19T15:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@anonaka,\u00a0\n\nWelcome to Google Cloud Community.\n\nThe Form Parser employs OCR (Optical Character Recognition) to find text in images, therefore you might want to fiddle with the image pretreatment parameters to improve the accuracy of the OCR. To increase OCR accuracy, experiment with changing the picture preprocessing settings for brightness, contrast, or sharpness.\n\nTry training a custom model using your own labeled data if the Form Parser is having trouble correctly interpreting the single-digit number in your image.\nYou may fine-tune the OCR to better recognize particular types of text in your photographs by training a custom model.\nCreate your own labeled data and train a custom model if the Form Parser is having trouble correctly interpreting the single-digit number in your image.\nYou can improve OCR's ability to identify particular forms of text in your photographs by training a custom model.\n\n\nHere are some documentation that might help you.\nhttps://cloud.google.com/vision/docs?_gl=1*p539ow*_ga*MTM5Mjc1MzQzNS4xNjc2NjU1Njg2*_ga_WH2QY8WWF5*MT...\n\nhttps://cloud.google.com/functions/docs/tutorials/ocr?_gl=1*huz756*_ga*MTM5Mjc1MzQzNS4xNjc2NjU1Njg2*...\n\n0\u00a0Likes"
			},
			{
				"Answer_created_time": "2023-04-20T05:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Unfortunately, the current custom parser does not recognize Japanese text which is mandatory for my project. I am waiting for the next update release."
			}
		]
	},
	{
		"Question_title": "PHP, google-api-vision and CURL",
		"Question_created_time": "2023-04-14T08:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/PHP-google-api-vision-and-CURL/td-p/543895/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 95,
		"Question_body": "HI,My script for API VISION no longer works for me. Everything worked until last week.\nI use PHP and CURL, and I get error Status Code 400.This is my code:      Is the API url still valid? What has changed?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-14T08:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Beyond the status code, an error response usually has some additional data/text associated with it describing the nature of the error.\u00a0 I seem to sense that your request is sending a REST payload to the vision API.\u00a0 Can you run that from the command line and validate it is working?\u00a0 Can we capture the full response payload from the REST request (curl output) and see in full what is returned?"
			},
			{
				"Answer_created_time": "2023-04-14T09:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The error message is:\n\n{ \"error\": { \"code\": 400, \"message\": \"Request must specify image and features.\", \"status\": \"INVALID_ARGUMENT\" } }\n\nI can't understand what has changed."
			},
			{
				"Answer_created_time": "2023-04-14T10:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My recommended next step is to capture what was \"exactly\" send from the PHP app and then send the same thing through CURL on the command line ... and see if there is a distinction.\u00a0 \u00a0Let's assume Google Cloud Vision API has a documented \"contract interface\" .... when it responds with \"invalid arguments\" ... we should take that at face value until we see what precisely we are sending.\u00a0 If we can see what we are sending, my\u00a0guess is that we will look at something and have an \"aha\" moment where see a mismatch between what the Vision API requires/expects and what we are actually sending."
			},
			{
				"Answer_created_time": "2023-04-20T01:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Kolban,\nafter a series of tests I discovered that my server, where the images are stored, had security policies that blocked file_get_contents and CURL.\n\nAfter several inconclusive chats with the hosting provider I decided to change servers and solved the problem.\n\nThe API had no problem. The problem was with my platform."
			}
		]
	},
	{
		"Question_title": "OCR Detect Text .Net does not return score",
		"Question_created_time": "2023-04-18T08:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/OCR-Detect-Text-Net-does-not-return-score/td-p/544923/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 56,
		"Question_body": "Hello All,I am trying to use the c# .Net nuget package to run OCR on images. I can get it to work fine and process images and returns data by following the examples in here: https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest#detect-text-in-a-single... but I don't seem to be able to retrieve information on the \"Score\" of the text read. Both confidence and score are always 0 for all text returned. I am using the \"DetectText\" method specifically and also tried to set the parameter \"EnableTextDetectionConfidenceScore\" to true and pass it via ImageContext, but still no success. Would anybody be able to help or advice what can be investigated further please?This is my first experience with Google cloud in general so apologies if I am posting in the wrong channel, please advice otherwise if this is the case.Kind Regards,Silvio  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-19T14:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@silviop\u00a0\n\nWelcome to Google Cloud Community.\n\nIt looks like that you are attempting to use the Google Cloud Vision API and the C#.NET NuGet package to acquire the confidence score of the text recognized in your photos. The likelihood that the OCR engine correctly identified the text in the image is represented by the confidence score.\n\nYou can change the TextAnnotation.DetectedText property in the example you provided to extract the confidence score of the detected text.\nYou've attempted to enable the EnableTextDetectionConfidenceScore argument in the ImageContext object, as was previously described, however this parameter only affects the answer for DocumentTextDetection and TextDetection requests, not TextAnnotation.\n\nAfter making the mentioned changes to the code, if you are still unable to access the confidence score, you might want to check the OCR outcomes using the Cloud Vision API."
			}
		]
	},
	{
		"Question_title": "Checkbox in custom Form parser not working",
		"Question_created_time": "2023-04-10T00:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Checkbox-in-custom-Form-parser-not-working/td-p/542100/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 1,
		"Question_view_count": 144,
		"Question_body": "hi, I am using custom Document Ai Processor to label and train on my document, but facing issue that every time it missed checkboxes. I have labelled 40 documents for train and 10 for test. Even after training the  f1 score of check boxes are 0. Can anyone please help e out in this. Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-10T16:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @Hami1437\u00a0,\n\nWelcome back to Google Cloud Community,\n\nTraining a machine learning model can be a complex and iterative process. It may take several configurations to get a good performance on checkbox detection.\n\nThere are some possible reasons why your model is not detecting the checkbox.\n\nYou may try to perform this check to see the issue:\n\n1. Check your labeling: Double-check your labeling to make sure that you have correctly labeled all of the checkbox in your training set.\n\n2. Fine-tune on a smaller set of examples that are similar to your target domain. This can help the model learn to recognize the checkbox more accurately.\n\n3. Try adding more number of labeled examples in your training set.\n\n4. Check that your training set includes a variety of different types of documents and checkboxes.\n\n5. Try experimenting with different model architectures to see if you can improve the calibration on checkbox detection.\n\n6. Increase configuring the complexity of your model: If none of the above steps works, you may try on increasing the complexity of your model.\n\nHere are some reference that might help you:\nhttps://cloud.google.com/recaptcha-enterprise/docs/instrument-web-pages-with-checkbox?_ga=2.21438216...\n\nhttps://cloud.google.com/vision-ai/docs/object-detector-model?_ga=2.214382167.-1392753435.1676655686\n\nhttps://cloud.google.com/document-ai/docs/overview?_ga=2.214382167.-1392753435.1676655686"
			},
			{
				"Answer_created_time": "2023-04-10T23:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for reply. Actually, I am using Document AI custom processor and have labeled 50 the documents. and the overall Score of trained model is almost 0.94 but in this there are 3 checkboxes all them have 0 score and never extracted. And I have labelled them same as in the documentation. And one more thing is that i have same template documents. Attaching the Image\n of document. Thanks"
			},
			{
				"Answer_created_time": "2023-04-11T22:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Just to give\u00a0additional information\u00a0@Aris_O.\u00a0We have labelled about 50 documents with checkbox, although the overall f-score is good but for checkbox the f-score is 0.\u00a0\n\nThe above image is of labelling, the data is dummy right now.\u00a0@Aris_O\u00a0can you please help us out what exactly are we doing wrong here."
			},
			{
				"Answer_created_time": "2023-04-19T00:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Facing the same issue. Despite having a very high f1 score, the model is unable to detect checkboxes.\u00a0\nEven if it does detect a checkbox successfully(detected 2 out of 30 in my case), while calling the API, it does not return the detected value( i.e checked or unchecked). Just returns the label name and confidence, which is always lesser than 5%."
			}
		]
	},
	{
		"Question_title": "Google TTS problem with large numbers",
		"Question_created_time": "2023-04-18T17:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-TTS-problem-with-large-numbers/td-p/545075/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 86,
		"Question_body": "I am using Google TTS and running into this error:  with this text which is only 155 bytes:The Assessment Review Board of Canada has determined that the assessment of 185 Corkstown Road in Ottawa should be reduced from 35447892 to 31537798 in the Large Industrial Class and 66985690 total assessed value.Why am I receiving this error?  Could it have something to do with the large numbers?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-18T20:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Sadly, I am not familiar with the \"make.com\" system.\u00a0 Google Cloud provides a rich array of services that can be invoked over the Internet.\u00a0 The text to speech is an example of one such service.\u00a0 Google makes APIs available that can be called to request the service function and, when a request is received, Google satisfies that request.\u00a0 The API call for a text to speech request\u00a0 can be found here.\u00a0 As we look at the API request, we see that it has expected parameters which includes the text to verbalize.\u00a0 In the demo/web site, we pasted in what we think is the text we want verbalized and it comes back correctly.\u00a0 However, all this does is exercise the Google Cloud API ... it doesn't tell us anything about how the \"Make.com\" module either works or the parameters that are\u00a0actually being passed when you invoke your \"make.com\" application.\u00a0 \u00a0About the best I can suggest is to examine very carefully your configuration of the make.com app (scenario?).\u00a0 Be as sure as possible that what you are sending to Google Cloud to verbalize is the actual text you think you are sending.\u00a0 Look also at other parameters and see if you can map those to what are expected by the API.\u00a0 You might also want to reach out to help to the good folks at make.com.\u00a0 Maybe they have a recipe to add debugging to your application to see what is actually being passed in to the Google Text To Speech call.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-18T17:44:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I visited this page ...\u00a0https://cloud.google.com/text-to-speech\n\nand copied/pasted your text ... and it seemed to respond correctly.\n\n\u00a0\n\nCan you have a try and see what happens at your end? Can we also look at the parameters of your TTS request?"
			},
			{
				"Answer_created_time": "2023-04-18T18:23:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The demo worked absolutely fine with my sample text.:\n\nThe problem arises when I try to use the Google Cloud Text to Speech module inside a Make.com scenario. Here is the error from that environment:"
			},
			{
				"Answer_created_time": "2023-04-18T20:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Sadly, I am not familiar with the \"make.com\" system.\u00a0 Google Cloud provides a rich array of services that can be invoked over the Internet.\u00a0 The text to speech is an example of one such service.\u00a0 Google makes APIs available that can be called to request the service function and, when a request is received, Google satisfies that request.\u00a0 The API call for a text to speech request\u00a0 can be found here.\u00a0 As we look at the API request, we see that it has expected parameters which includes the text to verbalize.\u00a0 In the demo/web site, we pasted in what we think is the text we want verbalized and it comes back correctly.\u00a0 However, all this does is exercise the Google Cloud API ... it doesn't tell us anything about how the \"Make.com\" module either works or the parameters that are\u00a0actually being passed when you invoke your \"make.com\" application.\u00a0 \u00a0About the best I can suggest is to examine very carefully your configuration of the make.com app (scenario?).\u00a0 Be as sure as possible that what you are sending to Google Cloud to verbalize is the actual text you think you are sending.\u00a0 Look also at other parameters and see if you can map those to what are expected by the API.\u00a0 You might also want to reach out to help to the good folks at make.com.\u00a0 Maybe they have a recipe to add debugging to your application to see what is actually being passed in to the Google Text To Speech call."
			},
			{
				"Answer_created_time": "2023-04-18T20:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks Kolban. You were certainly on the right track to mention the different parameters that were being passed to the API from the make.com module.\u00a0\n\nWhat I noticed was that the error only occurred when I used one of the Neural2 voices such as the en-US-Neural2-A [MALE] voice.\n\nWhen I switched to one of the other voices such as the en-US-Studio-O [FEMALE] voice everything worked fine with no error:\n\nI found mention of an ongoing internal bug for this issue.\u00a0 My issue is resolved since I don't need the Neural2 voice to work, but it still seems odd that this bug exists."
			}
		]
	},
	{
		"Question_title": "Looking for (paid) help to help me extract text from Instagram images",
		"Question_created_time": "2023-03-30T05:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Looking-for-paid-help-to-help-me-extract-text-from-Instagram/td-p/538848/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 44,
		"Question_body": "Hello all,My skills in data science are too low to allow me to apply Google's tutorials about Google Cloud Vision so I am looking for some (paid) help to show me step by step what I have to do to extract text from (Instagram) images. I can do it for one image but I need to be able to do it for bunches of images.Many many many thanks! \ud83d\ude42Catherine  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-18T19:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Did you get an answer on this yet? I\u2019ve not done it but I\u2019ve done similar with Azure tools. It can\u2019t be that much different on GCP."
			}
		]
	},
	{
		"Question_title": "Max results in batched vision.ImageAnnotatorClient",
		"Question_created_time": "2023-04-18T07:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Max-results-in-batched-vision-ImageAnnotatorClient/td-p/544884/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 36,
		"Question_body": "Hi, I am trying to set maxResults in batched vision.ImageAnnotatorClient as follows but such a parameter is not recognized.    What am I doing wrong?  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-18T07:15:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "try `max_results` instead\n\n@magenti\u00a0wrote:\n\n\nmaxResults\n\n\u00a0\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-18T07:15:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "try `max_results` instead\n\n@magenti\u00a0wrote:\n\n\nmaxResults"
			},
			{
				"Answer_created_time": "2023-04-18T07:41:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yep, it worked fine. Thank you!"
			}
		]
	},
	{
		"Question_title": "ssml text to speech audio tag python",
		"Question_created_time": "2023-04-16T22:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/ssml-text-to-speech-audio-tag-python/td-p/544312/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 54,
		"Question_body": "Cannot get the audio tag to work. Have tried it with both a local and remote mp3 files.  Here is the text:<speak version=\"1.1\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.w3.org/2001/10/synthesis http://www.w3.org/TR/speech-synthesis11/synthesis.xsd\" xml:lang=\"en-GB\">The rain in Spain stays mainly in the plain.\n<audio src=\"uhm_male.mp3\" /> {If I take this out everything works)\nHow kind of you to let me come.</speak>Anybody got any suggestions? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-17T16:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@StevenLMeyer,\n\nWelcome back to Google Cloud Community.\n\nIt appears that you are attempting to use an SSML document with the HTML5 audio element. However, text-to-speech systems cannot use the audio element since it is not supported by SSML. You try to utilize a text-to-speech service or program to produce the audio file separately if you're attempting to create an audio file that contains the spoken text, and then incorporate it into your web page using the HTML5 audio element. JavaScript can also be used to play the audio file whenever a user clicks a button or otherwise engages with your website.\n\nAs an alternative, you can send the audio file to a speech-to-text service and get the transcription in text format if you're utilizing one of such services to transcribe audio.\u00a0The text can then be utilized as necessary on your website.\n\nHere are some documentation that might help you:\nhttps://cloud.google.com/text-to-speech/docs/ssml?_ga=2.112080452.-1392753435.1676655686\nhttps://cloud.google.com/text-to-speech/docs/ssml-tutorial?_ga=2.112080452.-1392753435.1676655686"
			}
		]
	},
	{
		"Question_title": "Dialogflow Messenger and richcontent from a webhook",
		"Question_created_time": "2023-04-14T16:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-Messenger-and-richcontent-from-a-webhook/td-p/544027/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 99,
		"Question_body": "Hi,I'm trying to add a fulfilment response using Webhook.I built a kind of simple Python API with Flask to provide a rich content answer to Dialogflow.The problem is that Dialogflow accepts only messages and not richcontents from my API. ( not rendering, for example, chips).Here is my code :  I tried different ways to write or construct the JSON, but nothing works Could you please advise, knowing that I'm testing on an embedded Dialogflow Messenger UIThanksYassine",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-17T16:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@yassinekoubaa\u00a0\n\nWelcome to Google Cloud Community.\n\nThe format of the response you are providing back to Dialogflow appears to be the potential source of the problem. As a message object rather than a separate element in the fulfillment_response, the richContent field should be included.\u00a0The richContent can be added to your code as a message object. You may create a code wherein\u00a0the richContent\u00a0is included as a payload in a separate message object, along with the regular text message object.\n\nVerify that the Dialogflow API version you're using supports rich content. Dialogflow API versions 2 and higher are the only ones that enable rich content."
			}
		]
	},
	{
		"Question_title": "OAuth Rejection Notice",
		"Question_created_time": "2023-04-11T12:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/OAuth-Rejection-Notice/td-p/542724/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 59,
		"Question_body": "After getting back an error message on running my javascript program in node.js with my VM instance, I was informed by a member of the Google Cloud Community: \"If you need to set up your instance with Identity-Aware Proxy (IAP), you may refer to this document Setting up IAP for Compute Engine. But if you already set up your instance, you just need to enable IAP Enabling IAP for Compute Engine.\" Given that my VM instance had already been set up, I jumped (at the suggestion of the person advising me) to the part of the document dealing with \"Enabling IAP for Compute Engine\" without carrying out the other steps for persons who did not already have an up and running VM instance (i.e., load balancing, etc.) While following the steps listed in the documentation, I discovered I must activate the \"Configure consent screen\" button and create an OAuth consent screen. None of the things I saw after this point seemed to have anything to with my sort of project, that involves no one but myself to give consent to.  But I decided this time not to \"think too hard\" and to simply follow the instructions. After several long, puzzling minutes trying to do what I thought I was supposed to do, I submitted my OAuth request. And after 48 hours or so of waiting for an answer from the Trust & Safety Security & Privacy Team, I just received a Request Denied from them, with no explanation of what was wrong with my submission (though I'm sure several things were), along with a warning that I might need to submit a video. But of what? Perhaps it's worth mentioning at this point that what I wish to do is \"simply\" use Google Cloud Translation (Advanced) with a glossary in .tmx format to translate a .docx file from one Google-supported human language to another Google-supported human language, outputting the result also in .docx format and storing both in different subdirectories of my bucket. (One directory for the .docx input file and another for the .docx output file, the result of the translation.) My main question is this: is it even necessary to create an OAuth consent screen in such a scenario,  with only person (i.e. me) accessing my VM via the GC console? And if a consent screen turns out to be a necessity, can someone please suggest what I might do to get things right this time and not receive a second Request Denied from the members of the Trust & Safety Security & Privacy Team? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-11T14:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello @legrandtimonier\u00a0\n\nWelcome to GCC! If all you are trying to do is to be able to ssh into your VM, This is the link\u00a0https://cloud.google.com/iap/docs/using-tcp-forwarding\u00a0you want to follow. The one you referred to is to setup IAP for Web links. Hope this helps"
			},
			{
				"Answer_created_time": "2023-04-14T18:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi again,\n\nNot liking mysteries, I decided to get to the bottom of this SSH button not working thing (even though Troubleshooting reported all systems Go.) So I took a snapshot of my VM, created a new one in the same region, used my snapshot to build the new one, adapted firewall rules slightly and fixed a few missing things (like a suddenly absent node.js environment) and lo and behold things working perfectly now, including SSHing from within the console. Which just goes to show: sometimes to fix a problem ya just gotta kick the TV set, as it were. \ud83d\ude42"
			},
			{
				"Answer_created_time": "2023-04-17T00:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Just a word to say that after getting your reply I simply recreated a new VM instance in the same region/zone, created a snapshot of the old instance, slightly adjusted firewall rules and such and everything is now working perfectly with respect to SSHing. The only problem I had was that lots of stuff didn't get copied over to the new instance (especially the node.js environment, which I had to download and recreate on the new VM). But that was a detail really, and I am very happy to be able to SSH to my new VM with no more problems. Regards"
			}
		]
	},
	{
		"Question_title": "Adding price as a search facet in Google retail API",
		"Question_created_time": "2023-04-14T07:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Adding-price-as-a-search-facet-in-Google-retail-API/td-p/543862/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 61,
		"Question_body": "Hi Community,I was doing a POC for implementing google retail api for search in my project. I used the default produts json that they provide and i tried to search the products, but it seems like i cannot add price as a facet or provide in the search query. Does anyone has any suggestions how to add the price of the product as a search facet?Any help is appreciated.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-14T16:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@HitendraRathor,\n\nWelcome to Google Cloud Support.\n\nYou must set up the price attribute as a facet in your product feed in order to add price as a facet to your Google Retail API search. You can take the following actions:\n\n1. Include a `price` attribute as a new field in your product feed.\n\n2. Add the `price` attribute to the facetable_attributes field in your product type declaration to configure it as a facet.\n\n3. Send Google Merchant Center the new product feed.\n\nThe price property will be a facet in your Google Retail API search after you have finished these steps. Then, by including it in the `filter` parameter, you can use it in your search query."
			},
			{
				"Answer_created_time": "2023-04-16T22:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the help, in place of facetable attribute, used it as retrivable attributes and it worked."
			}
		]
	},
	{
		"Question_title": "Cloud translation doesn't use glossary in glossary config",
		"Question_created_time": "2023-04-13T22:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-translation-doesn-t-use-glossary-in-glossary-config/td-p/543726/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 64,
		"Question_body": " Hello everyone,Our team has been using the Cloud Translation service to translate strings from Chinese into other languages, with the help of glossaries. However, we have recently encountered an issue where we are unable to use newly uploaded glossaries to perform translations.Despite searching through the documentation, we have not been able to find any helpful information to resolve this issue. We are seeking advice and guidance from the community to determine what could be causing this problem and how we can address it.Thank you for your help in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-14T15:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Will_Chang,\n\nWelcome to Google Cloud Community.\n\nIt's likely that the Cloud Translation API service may be having problems. I advise monitoring the Cloud Translation API status page to see if any service interruptions or outages have been recorded that might be affecting the API's capacity to handle recently submitted glossaries. You may want to check the Cloud Translation API documentation to see if the glossary format you're using is supported by the API.\u00a0\n\nHere are some documentation that might help you:\nhttps://cloud.google.com/translate/docs/reference/rpc?_ga=2.179633764.-1392753435.1676655686\nhttps://cloud.google.com/translate/docs/advanced/glossary?_ga=2.179633764.-1392753435.1676655686\nhttps://cloud.google.com/translation-hub/docs/admin-glossary?_ga=2.179633764.-1392753435.1676655686"
			},
			{
				"Answer_created_time": "2023-04-16T20:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Aris_O,\n\nThanks for your information.\n\nI have gone through all the documentation you provided and tried some additional steps.\n\n\u00a0\n\nList all the glossaries under our project to ensure that the target glossary exists.\nGo to the Translation Hub > Resource > Glossaries and verified that the target glossary was present.\nList all the entries under target glossary\u00a0and everything seemed to be in order.\n\nThen I trying to call translate API with the payload below\n\n{\n   \"targetLanguageCode\":\"en\",\n   \"sourceLanguageCode\":\"zh_TW\",\n   \"mimeType\":\"text/plain\",\n   \"glossaryConfig\":{\n      \"glossary\":\"target_glossary\",\n      \"ignoreCase\":true\n   },\n   \"contents\":[\n      \"\u6e2c\u8a66\"\n   ]\n}\n\nThe response be like:\n\n{\n    \"translations\": [\n        {\n            \"translatedText\": \"test\"\n        }\n    ],\n    \"glossaryTranslations\": [\n        {}\n    ]\n}\n\nTranslation API is not using target glossary to translate. Do you have any idea about this problem?"
			}
		]
	},
	{
		"Question_title": "Google Translate API and Serbian Latin script",
		"Question_created_time": "2022-10-11T02:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translate-API-and-Serbian-Latin-script/td-p/476723/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 0,
		"Question_body": "Hi there,In Serbia we are using 2 scripts side by side - Cyrillic and Latin script.I am heaving an issue with translation to Serbian Latin.\nBy default Google translate offer translation to Serbian Cyrillic , but bellow that default translation there is a translation to Serbian Latin.\nTake a look at this example:\nhttps://translate.google.com/?sl=en&tl=sr&text=Hello%20world!&op=translateI have found this post from early 2019.\nhttps://support.google.com/translate/thread/1836538?hl=enLike in that post my question is the same:\nI need it to support Serbian Latin, for some projects I don`t use the Cyrillic script. Also there is a problem with translating pages or similar plugins, e.g.: Google Language Translator for WordPress and some others CMS system like Kopage you can translate only to Serbian Cyrillic script.As I found this post on stackoverflow:\nhttps://stackoverflow.com/questions/73699065/google-cloud-translate-serbian-latin-not-workingIt seems, according to the poster of that article, that there was a workaround.\nInstead of \"sr\" ISO-639 code you can put \"sr_Latn\" - and you will get translation into Serbian Latin script.\nBut that workaround stop working several weeks ago - according to the poster.Is there a workaround to translate into Serbian Latin characters rather into Serbian Cyrillic characters?Regards,\nBranislav",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-12T08:12:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "It appears that translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API, as discussed in this recent issue. Therefore it\u2019s not assured that any possible workaround will be functional or reliable. You can see the list of supported language codes for translation here.\n\nYou can, however, submit a Feature Request to the public Google issue tracker for Cloud Translation API. The higher the number of users who bring attention to this request, the more likely it is for it to be eventually built into the API.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-10-19T12:17:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "You can just used a rules-based local library to reliably transliterate the sr-Cyrl that Google produces to sr-Latn.\n\nGoogle Translate never supported\u00a0translation\u00a0to sr-Latn.\u00a0 The feature you see at translate.google.com and in the mobile app is rules-based transliteration.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-10-12T08:12:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "It appears that translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API, as discussed in this recent issue. Therefore it\u2019s not assured that any possible workaround will be functional or reliable. You can see the list of supported language codes for translation here.\n\nYou can, however, submit a Feature Request to the public Google issue tracker for Cloud Translation API. The higher the number of users who bring attention to this request, the more likely it is for it to be eventually built into the API."
			},
			{
				"Answer_created_time": "2022-10-19T12:17:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "You can just used a rules-based local library to reliably transliterate the sr-Cyrl that Google produces to sr-Latn.\n\nGoogle Translate never supported\u00a0translation\u00a0to sr-Latn.\u00a0 The feature you see at translate.google.com and in the mobile app is rules-based transliteration."
			},
			{
				"Answer_created_time": "2023-04-16T13:30:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "also on another note, google translate app offers cyrillic keyboard only. i always have to manually add, and switch to serbian latin keyboard upon translating into serbian."
			}
		]
	},
	{
		"Question_title": "location variable setting for the Google Cloud Translation API (Advanced)",
		"Question_created_time": "2023-04-12T22:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/location-variable-setting-for-the-Google-Cloud-Translation-API/td-p/543332/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 72,
		"Question_body": "Does anyone know which locations can be used to set the variable \"location\" in the following code snippet from a .js program designed to use Google Cloud Translation (Advanced) to translate asynchronously a .docx file stored in the subdirectory of a bucket into another supported language? My VM instance is located in \"europe-north1-a\", but is it possible to also use this as a valid region/zone to set the \"const location\" variable to? I don't  see anything about this in the documentation.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-13T14:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@legrandtimonier,\u00a0\n\nWelcome back to Google Cloud Support,\n\nThe area \"europe-north1-a\" is a valid location, You may able to generate resources like buckets and VM instances there, you may use it as the location value for the location variable in the code snippet you gave.\n\nThe location parameter of the Google Cloud Translation API indicates the region in which the Translation API service is hosted. The following locations are listed as being accessible for the API service in the official documentation:\n\nasia-east1\neurope-west2\nus-central1\n\nTo reduce latency and increase speed while using services, You may pick the area that is either closest to your users or where your resources are located.\n\nHere are some references that might help you:\nhttps://cloud.google.com/compute/docs/regions-zones?_ga=2.183424358.-1392753435.1676655686\n\nhttps://cloud.google.com/translate\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-13T14:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@legrandtimonier,\u00a0\n\nWelcome back to Google Cloud Support,\n\nThe area \"europe-north1-a\" is a valid location, You may able to generate resources like buckets and VM instances there, you may use it as the location value for the location variable in the code snippet you gave.\n\nThe location parameter of the Google Cloud Translation API indicates the region in which the Translation API service is hosted. The following locations are listed as being accessible for the API service in the official documentation:\n\nasia-east1\neurope-west2\nus-central1\n\nTo reduce latency and increase speed while using services, You may pick the area that is either closest to your users or where your resources are located.\n\nHere are some references that might help you:\nhttps://cloud.google.com/compute/docs/regions-zones?_ga=2.183424358.-1392753435.1676655686\n\nhttps://cloud.google.com/translate"
			},
			{
				"Answer_created_time": "2023-04-13T15:18:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Many thanks. I suspected that \"location=\" in this line of code wasn't the same as the location of where my bucket, for example, is located (in my case Council Bluffs, Iowa), but I wasn't sure what the right one was for where\u00a0Translation API service is hosted. I thought it might be the same as where my VM instance is hosted (europe-north1-a). Now I know that isn't it either. Thanks again for this important information."
			},
			{
				"Answer_created_time": "2023-04-14T07:44:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hmm. I tried setting \"location='europe-west2'\" as the location of my\nCloud Translation API, but when I run my program I am told, among other\nfrightening things:\n\n(node:4733) UnhandledPromiseRejectionWarning: Error: 3 INVALID_ARGUMENT:\nInvalid location name. Unsupported location 'europe-north1-a'.\nMust be 'us-central1' or 'global'.\n\nI will try setting location to \"global\", hoping that does it, but having no\nidea what should really be inserted as location it's just a shot in the\ndark. My storage bucket is located in \"us-central1\", but theoretically that\nshould have nothing to do with the Cloud Translation API location, right?\nMystery, mystery......"
			}
		]
	},
	{
		"Question_title": "Form parameter 'parameterName' is defined with an unknown entity type '@sys.unit-currency'.",
		"Question_created_time": "2023-04-11T04:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Form-parameter-parameterName-is-defined-with-an-unknown-entity/td-p/542504/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 57,
		"Question_body": "HiFor a while I've been getting strange validation errors in Dialogflow CX about parameters. Now I have this error in the Validations page:\n\nForm parameter 'parameterName' is defined with an unknown entity type '@sys.unit-currency'.\n\nThe parameter is captured and set just fine when testing, and also works fine in the Intent training phrases. Any idea what the cause of this is anyone?Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-13T16:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@FlashMaddison,\n\nWelcome back to Google Cloud Community.\n\nThe entity type you're using in your form parameter might be causing the issue. Even if it is utilized in your description of the form parameter, the error notice implicates that '@sys.unit-currency' is an unknown entity type.\n\nHere are some possible solutions that might involve your issue:\n\nVerify '@sys.unit-currency's spelling and formatting once again. Ensure that it is a perfect match to the entity type utilized by the system entities.\nVerify that your Dialogflow CX agent has the '@sys.unit-currency' entity type enabled. You may accomplish this by visiting the Entities page and making sure the switch is turned on next to \"@sys.unit-currency.\"\n\n\nTry redeploying your agent if you recently added or modified the '@sys.unit-currency' entity type, by doing this, you may help verify that your entity type is being used as of late.\n\n\nTry generating the form parameter again using a different entity type and checking to see if the issue still occurs.\n\nHere are some documentation that might help you:\nhttps://cloud.google.com/dialogflow/es/docs/reference/system-entities?_ga=2.213766740.-1392753435.16...\nhttps://cloud.google.com/dialogflow/es/docs/entities-options?_ga=2.213766740.-1392753435.1676655686\nhttps://cloud.google.com/dialogflow/cx/docs/reference/system-functions?_ga=2.104734656.-1392753435.1...\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-13T16:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@FlashMaddison,\n\nWelcome back to Google Cloud Community.\n\nThe entity type you're using in your form parameter might be causing the issue. Even if it is utilized in your description of the form parameter, the error notice implicates that '@sys.unit-currency' is an unknown entity type.\n\nHere are some possible solutions that might involve your issue:\n\nVerify '@sys.unit-currency's spelling and formatting once again. Ensure that it is a perfect match to the entity type utilized by the system entities.\nVerify that your Dialogflow CX agent has the '@sys.unit-currency' entity type enabled. You may accomplish this by visiting the Entities page and making sure the switch is turned on next to \"@sys.unit-currency.\"\n\n\nTry redeploying your agent if you recently added or modified the '@sys.unit-currency' entity type, by doing this, you may help verify that your entity type is being used as of late.\n\n\nTry generating the form parameter again using a different entity type and checking to see if the issue still occurs.\n\nHere are some documentation that might help you:\nhttps://cloud.google.com/dialogflow/es/docs/reference/system-entities?_ga=2.213766740.-1392753435.16...\nhttps://cloud.google.com/dialogflow/es/docs/entities-options?_ga=2.213766740.-1392753435.1676655686\nhttps://cloud.google.com/dialogflow/cx/docs/reference/system-functions?_ga=2.104734656.-1392753435.1..."
			},
			{
				"Answer_created_time": "2023-04-14T02:36:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for getting back to me!\n\n@sys.unit-currency is in the entity list, and I didn't manually type it when creating the form, I selected it from a drop-down.\u00a0\n\nI've made no changes to the system entity.\n\nI changed the form parameter to @sys.number, then back to @sys.unit-currency and that has removed the validation error.\n\nThanks again \ud83d\ude42"
			}
		]
	},
	{
		"Question_title": "Chatting with Dialogflow ES right after deployment fails",
		"Question_created_time": "2023-04-12T08:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Chatting-with-Dialogflow-ES-right-after-deployment-fails/td-p/543132/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 80,
		"Question_body": "Hi,I have been having issues lately with Dialogflow ES and talking to the bot/agent after I deploy and train the agent on any new content. This is all done programmatically through the Dialogflow APIs, which I am on v2 of.My deployment process first is deploying which uses the: \"agentClient.restoreAgent();\" function within the api. (This is a node app)Then I after successfully uploading the new content, I train the agent: \"agentClient.trainAgent();\"This process has not changed for quite a long time and this issue has only recently started and has gradually gotten worse.To describe the issues further, when I deploy all steps come through as successful and done, leading me to believe the agent has been properly uploaded and trained with the training successful and complete. I then go the chat with the bot/agent and get the below error in my console. This persists for a fair few minutes now (around 5 minutes) before the messages are successfully handled and the intent is recognised correctly. As mentioned, nothing has changed for a long time in my code for how we deploy, train and chat to a Dialogflow agent.This is quite a serious issue since it also effects client production apps as well as my own development environment, and its not great for clients to deploy then be told it is successful from Dialogflow and then still have to wait another 5 minutes because of this error Dialogflow gives for the bot to actually be usable.One other thing worth noting, is that this error also occurs on the Dialogflow test console on the Dialogflow site (https://dialogflow.cloud.google.com/) so it is not something unique to my app. I.e. after I deploy, I try and test here and the same issue I get in my app occurs on the official Dialogflow console.Another extra point, this is not on every deployment, but it is on most of them now.Thanks!Image of error on Dialogflow itself: Error:   ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-13T17:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@apps_k_ebm,\n\nWelcome back to Google Cloud Community.\n\nIt\u00a0appears\u00a0that\u00a0after\u00a0deploying\u00a0and\u00a0training\u00a0your\u00a0Dialogflow\u00a0agent,\u00a0you're\u00a0still\u00a0having\u00a0trouble\u00a0identifying\u00a0intents.\u00a0This\u00a0lag\u00a0is\u00a0present\u00a0on\u00a0the\u00a0Dialogflow\u00a0test\u00a0console,\u00a0in\u00a0your\u00a0development\u00a0environment,\u00a0and\u00a0in\u00a0the\u00a0live\u00a0environment\u00a0of\u00a0your\u00a0client.\n\nHere are some possible answers to resolve the issues you've encountered:\n\nThe\u00a0newly\u00a0installed\u00a0and\u00a0trained\u00a0agent\u00a0may\u00a0still\u00a0be\u00a0propagating\u00a0across\u00a0Dialogflow's\u00a0servers\u00a0and\u00a0may\u00a0not\u00a0yet\u00a0have\u00a0reached\u00a0all\u00a0regions,\u00a0which\u00a0is\u00a0one\u00a0explanation\u00a0for\u00a0this\u00a0behavior\u00a0depending\u00a0on\u00a0the\u00a0size\u00a0of\u00a0the\u00a0agent\u00a0and\u00a0how\u00a0many\u00a0places\u00a0it\u00a0needs\u00a0to\u00a0spread\u00a0to,\u00a0this\u00a0may\u00a0take\u00a0some\u00a0time.\n\nAnother option is that the Dialogflow API is having latency or other problems, which is delaying the recognition of intent. You might wish to deploy and train the agent once more, and then measure the response time for intent recognition to attempt and solve this problem. To check for latency or other performance concerns, you may also try keeping an eye on the API response times.\n\nHere are some documentations that might help you:\n\nhttps://cloud.google.com/dialogflow/es/docs/tutorials/deploy?_ga=2.187494760.-1392753435.1676655686\nhttps://cloud.google.com/dialogflow/cx/docs/concept/continuous-tests?_ga=2.187494760.-1392753435.167...\nhttps://cloud.google.com/dialogflow?_ga=2.187494760.-1392753435.1676655686\nhttps://cloud.google.com/dialogflow/es/docs/quick/setup?_ga=2.187494760.-1392753435.1676655686\nhttps://cloud.google.com/dialogflow/es/docs/agents-versions?_ga=2.187494760.-1392753435.1676655686"
			}
		]
	},
	{
		"Question_title": "cloud retail: real time user events are not appear on retail data page",
		"Question_created_time": "2023-04-11T07:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/cloud-retail-real-time-user-events-are-not-appear-on-retail-data/td-p/542590/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 131,
		"Question_body": "hi, We tried many things, but somehow we could not transfer the events to the retail. I followed these steps:1: Get an API key from APIs & Services > Credentials > Create Credentials > API Key2: Get project id: There are Project number (like 111112222333) and Project ID (like \"kite-first-11111\"). I don't know which one to use. But i tried both.Screenshots of tag manager sections are below. Also we create a trigger. And we use ga4 datalayer without any problem on site side like this: ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-13T16:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@world_of_data,\n\nWelcome back to Google Cloud Community.\n\nYou may try to perform this check to see the issue:\n\nVerify that the API key you obtained has the required permissions to access the Google Analytics 4 API.\nWhen defining the Google Analytics 4 settings in Google Tag Manager, use the Project ID (such as \"kite-first-11111\") than the Project number (such as 111112222333).\n\n\nThe following are used to identify your project:\n\nProject name: A human-readable name for your project.\n\nThe project name isn't used by any Google APIs. You can edit the project name at any time during or after project creation. Project names do not need to be unique.\n\nProject ID: A globally unique identifier for your project.\n\nA project ID is a unique string used to differentiate your project from all others in Google Cloud. You can use the Google Cloud console to generate a project ID, or you can choose your own. You can only modify the project ID when you're creating the project.\n\nProject ID requirements:\n\nMust be 6 to 30 characters in length.\nCan only contain lowercase letters, numbers, and hyphens.\nMust start with a letter.\nCannot end with a hyphen.\nCannot be in use or previously used; this includes deleted projects.\nCannot contain restricted strings, such as\u00a0google\u00a0and\u00a0ssl.\n\nProject number: An automatically generated unique identifier for your project.\n\nNote: Don't include sensitive information in your project name, project ID, or other resource names. The project ID is used in the name of many other Google Cloud resources, and any reference to the project or related resources exposes the project ID and resource name.\n\nVerify that the trigger you created is firing appropriately when the website event that you want to track occurs.\n\n\nVerify if you're utilizing dataLayer variables to send information to Google Tag Manager, double check that they are formatted in accordingly with the Google Analytics 4 measurement standard.\n\n\nVerify if you can view real-time events report in the Google Analytics 4 dashboard to confirm that the events are being transmitted to Google Analytics 4.\n\nHere are some documentation that might help you:\n\nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects?\nhttps://support.google.com..."
			}
		]
	},
	{
		"Question_title": "DocumentAI - How to highlight bounding box after ProcessDocument(Request)",
		"Question_created_time": "2023-04-09T18:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DocumentAI-How-to-highlight-bounding-box-after-ProcessDocument/td-p/542055/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "Hello,After following this example with success: https://cloud.google.com/document-ai/docs/libraries#client-libraries-usage-csharpQuestion is: How to highlight each detected returned value on source PDF as Google does in their example? If I press F12 on their example it shows tags like:ai-document-view\nai-labeling-layout\nai-annotated-document-view\nai-document-tree-row-annotation-panel\nai-document-row-annotation\nai-connecting-lineI'd like to perform something similar so my users will be able to fix values with low confidence or just let them improving the results viewing the highlighted results on PDF.Any clue or code snipped will be appreciated.kind regards,Francisco",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-09T20:07:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "If we look at the genetic response from a Document AI processing output we see that it is a structured document (JSON).\u00a0 Within the JSON output we get indications of what was detected but also\u00a0where\u00a0it was detected.\u00a0 This makes me think that the PDF is converted into an image representation and the image processed using OCR.\u00a0 The results would then be the bounding polygons of an image representation of where each items was found.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-04-09T20:07:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "If we look at the genetic response from a Document AI processing output we see that it is a structured document (JSON).\u00a0 Within the JSON output we get indications of what was detected but also\u00a0where\u00a0it was detected.\u00a0 This makes me think that the PDF is converted into an image representation and the image processed using OCR.\u00a0 The results would then be the bounding polygons of an image representation of where each items was found."
			},
			{
				"Answer_created_time": "2023-04-12T19:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you about it!\nWhy I was thinking just doing it on PDF :S"
			}
		]
	},
	{
		"Question_title": "GPU Machines in Vertex AI Pipelines failing with RuntimeError: NCCL Error 2: unhandled system error",
		"Question_created_time": "2023-04-12T12:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GPU-Machines-in-Vertex-AI-Pipelines-failing-with-RuntimeError/td-p/543230/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Hi AI platform, I am using Vertex AI Pipelines to train detection models, and a since today my pipelines are failing due to a NCCL error on the GPU machine, I am creating the task as follows:Then after a few minutes my training step fails due to:\n\nNCCL Error 2: unhandled system error\n\nWhich seems to be related to shm size on the container. I have tried setting and unsetting the variable `NCCL_SHM_DISABLE` but to no avail, any idea on how to get support for this issue? \n\nThanks\n\nI am using vertex and kubeflow",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-12T12:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My logs are normal until they go to the training loop:"
			}
		]
	},
	{
		"Question_title": "Error: Input sets do not have the same labels.",
		"Question_created_time": "2022-12-14T15:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-Input-sets-do-not-have-the-same-labels/td-p/499698/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 140,
		"Question_body": "I have tried to train a model with over 50,000 images and 227 labels (with bounding boxes) I loaded the images using a csv file with each image left as UNASSIGNED for ML use. Each time I try to train it, it gets to the 28 minute mark and errors out with Training pipeline failed with error message: Input sets do not have the same labels.I am at my wits end trying to work out what is the issue. I cannot find anyone else with the same issue (or I have found one, but there was no solution) and no matter what I do, it errors out at the same point (I have removed some labels etc) Does anyone know what the issue is (with more detail) so I can try to fix it? I assume it is talking about the ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-15T10:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nWhat specific product under Vertex AI are you using?Are you training for AutoML Vision or Custom training?"
			},
			{
				"Answer_created_time": "2022-12-15T15:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "AutoML training. I thought it would be a spaces issue(in the label names) , and changed them all to underscores, but that had the same issue. Can a label be too long?"
			},
			{
				"Answer_created_time": "2022-12-15T16:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is really difficult to pin point the exact problem if you won't have help with GCP support. Best I could suggest is to make sure you are following the best practices [1] [2] for preparing your training data.\n\n[1]\u00a0https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data\n\n[2]\u00a0https://cloud.google.com/vertex-ai/docs/image-data/object-detection/prepare-data"
			},
			{
				"Answer_created_time": "2022-12-15T16:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I may have to get support.\u00a0 The best I can think is that of the 270 odd labels,, some have 20 images, some have 2000. That could be what it causing it to fail .\n\nThe issue is, I just want support on this one matter, and am happy to pay for it, but have no desire to sign my domain over to Google workspace just to get support on a product that I am trialing to see if it will work for my first project."
			},
			{
				"Answer_created_time": "2023-04-11T17:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Continuum200\u00a0did you ever manage to resolve this? I'm seeing the same thing also at the 28 minute mark"
			}
		]
	},
	{
		"Question_title": "Glossary file format(s) for use with Google Cloud Translation",
		"Question_created_time": "2023-04-10T21:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Glossary-file-format-s-for-use-with-Google-Cloud-Translation/td-p/542421/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Hello,I'm interested to know whether it is possible to use a .tmx file as a  source/target-language bilingual glossary with Google Cloud Translation (Advanced) instead of a .csv file. The reason I ask is that when running my program in its early stage of development I got back an error message stating that only a .csv file is the accepted. However,  in a video I happened upon recently, produced by the Google Cloud team, I definitely heard at one point that .tmx was also an accepted format. Does anyone know the definitive answer to this question? Thanks in advance. Regards",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-10T22:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nMany thanks for this response. The problem is that in an official Google\nCloud video posted around two years ago, the Google presenter states\nclearly at around 3 min 10 seconds into the video that it's possible to use\nnot only .CSV files, but .tmx as well. I've seen similar mentions of this\nelsewhere.\n\nThe official Google video is available, by the way, at:\n\nhttps://youtbe.be/A3knNbGfXh4\n\nRegards,\n\nGary"
			},
			{
				"Answer_created_time": "2023-04-11T09:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello! Thanks for the question. The file format depends on the glossary type:\u00a0https://cloud.google.com/translate/docs/advanced/glossary#format-glossary. For unidirectional glossaries, you can use TMX or CSV. Equivalent term sets require CSV."
			},
			{
				"Answer_created_time": "2023-04-11T11:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThanks for getting back to me with this good news. \ud83d\ude42 I will be\ntranslating texts in a single direction (for now), and so I will start with\nthe .tmx files, which are much easier to deal with in my scenario. Within\nthe javascript itself is there anything besides the .tmx that must be added\nto the code to tell it to use .tmx? or do I simply provide a .tmx file in\nthe same subdirectory of my bucket and change the extension from .csv to\n.tmx?\n\nRegards"
			},
			{
				"Answer_created_time": "2023-04-11T11:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I believe you'll need to specify\u00a0LanguageCodePair instead of\u00a0LanguageCodesSet. After that, pointing to your TMX file should work."
			},
			{
				"Answer_created_time": "2023-04-11T12:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Many thanks. That was what I needed to know.\n\nRegards"
			}
		]
	},
	{
		"Question_title": "How to integrate ML model with excel sheet containing 400k image links",
		"Question_created_time": "2023-04-11T06:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-integrate-ML-model-with-excel-sheet-containing-400k-image/td-p/542553/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 34,
		"Question_body": "I created a model to identify good or bad profile images for a company, Data is in excel sheet with 400k+ links, I don't want to download all the images. I'm looking for another method in which one can run ML model on excel sheet. Is there anyone who has done this before or know the method? I created the model on Jupyter notebook, I don't know where to deploy the model to get the API key. Any reference will be apricated.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-11T06:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Howdy ... I'm imagining a sheet where one of the columns is a link to an image file.\u00a0 I'm now imagining you want to train your model to detect good and bad images.\u00a0 Where did you get the model from?\u00a0 What does the model consider a good or bad image?\u00a0 Does the model already exist and you just want to determine (for each image) whether it is good or bad or does the model need to be trained on a set of images (the training set and the validation set)?"
			}
		]
	},
	{
		"Question_title": "Vertex AI Pipeline: The replica workerpool0-0 exited with a non-zero status of 13",
		"Question_created_time": "2023-03-08T03:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Pipeline-The-replica-workerpool0-0-exited-with-a-non/td-p/530067/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 219,
		"Question_body": "I'm trying to run a vertex AI pipeline (AutoML) using default setting and required service account permission but every time i run that pipeline, i will get failed with above error.  same i had run with custom will success but when running with AutoML for Tabular Classification / Regression it will get failed after long process time. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-08T03:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@lambdalove\u00a0kindly have a look"
			},
			{
				"Answer_created_time": "2023-03-11T15:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sorry\u00a0@norahul1020\u00a0but I'm not familiar with\u00a0Vertex AI Pipelines or with the error message."
			},
			{
				"Answer_created_time": "2023-04-09T05:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Not a problem still looking for the solutions."
			},
			{
				"Answer_created_time": "2023-04-10T10:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am running into the same problem and have not been able to figure out why the error is happening. I also saw a similar post on reddit\u00a0https://www.reddit.com/r/googlecloud/comments/1114k19/help_for_gcpvertexai_error_code_the_replica/\n\nPlease post an update here if you resolve the issue. I will do the same. On the surface it appears to be a permission issue. I keep adding more permissions but I have not been able to move forward. It might be something else."
			}
		]
	},
	{
		"Question_title": "Google cloud transcription Spanish not useful at all",
		"Question_created_time": "2023-04-09T02:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-cloud-transcription-Spanish-not-useful-at-all/td-p/541965/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 32,
		"Question_body": "The transcription I'm receiving with the Google cloud transcription service in Spanish (MX and CO) languages are not useful at all. More than 40% of the text transcribed is wrong. I have used the https://console.cloud.google.com/speech/transcriptions url with the UI of Speech-to-Text. Not useful at all, tbh. Is there any way of improve the result of this languages?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-09T20:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Looking at this page ...\u00a0https://cloud.google.com/speech-to-text\u00a0... there seems to be a pre-provided sample where you can supply your own audio and see what the transcription return will be.\u00a0 What would be useful is if you examined the source of your audio and see what quality it contains.\u00a0 Perhaps find some pre-recorded Spanish recordings and pass those into the sample and see what the resulting quality of output looks like and see if it differs from what you are finding.\u00a0 Is there something distinct about your audio input ... for example is it highly domain specific and contains rich sets of domain specific jargon or phrases?\u00a0 Obviously, don't post anything in the least bit sensitive ... but if you can, examples of what you think was said and what Google transcribed it as would be useful."
			},
			{
				"Answer_created_time": "2023-04-09T20:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Kolban,\n\n\u00a0\n\nThank you for your answer. My concern when I published this post was the tool I was using was not really focus on the exact transcription I needed. So I create an instance at\u00a0https://console.cloud.google.com/speech/transcriptions/list?project=******\"\u00a0and then configured with the same language (Spanish Mexican) used in the audio. After this, followed all the guidelines to the exact detail, but the result was the same, most of the transcription is not useful for me, full of wrong words, so I have to listen myself and write by myself word by word. Now I don't know if I will be charged for a wrong transcription and, moreover, will the service be useful in the future for Mexican audio? In my example, the audio is perfect, is a chat conversation recorded in a studio between 2 mexican guys, so no way the audio would be wrong.\n\n\u00a0\n\nThank you for your support."
			}
		]
	},
	{
		"Question_title": "Vertex AI endpoint updating model version",
		"Question_created_time": "2023-04-09T19:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-endpoint-updating-model-version/td-p/542061/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 63,
		"Question_body": "Hi, currently I'm working on vertex ai pipelines using kfp. I'm confused on how to update my model version on the same endpoint(not updating model version on model registry) using python script, is there a way to do it? Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-09T19:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, currently I'm working on vertex ai pipelines using kfp. I'm confused on how to update my model version on the same endpoint(not updating model version on model registry) using python script, is there a way to do it? Thanks"
			}
		]
	},
	{
		"Question_title": "Turn off text normalization in Speech to Text API",
		"Question_created_time": "2023-04-06T02:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Turn-off-text-normalization-in-Speech-to-Text-API/td-p/541225/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 56,
		"Question_body": "I am using Speech to Text API to transcribe audio files. I see that the output contains a lot of characters which might be possibly occurring due to inverse text normalization somewhere. Symbols like $ for dollars and other currency symbols and also numbers written in numeric format rather than words. Is there some option in RecognitionConfig which gives me verbatim output in words instead of numbers and symbols? I see there's a \"transcriptNormalization\" option in the config but then I have to provide my own rules. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-07T17:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@agupta54,\n\nWelcome back to Google Cloud Community.\n\nA Speech-to-Text API synchronous recognition request is the simplest method for performing recognition on speech audio data. Speech-to-Text can process up to 1 minute of speech audio data sent in a synchronous request. After Speech-to-Text processes and recognizes all of the audio, it returns a response.\n\nA synchronous request is blocking, meaning that Speech-to-Text must return a response before processing the next request. Speech-to-Text typically processes audio faster than realtime, processing 30 seconds of audio in 15 seconds on average. In cases of poor audio quality, your recognition request can take significantly longer.\n\nHere are some references that might help you.\nhttps://cloud.google.com/speech-to-text/docs/speech-to-text-requests?_ga=2.109059397.-1392753435.167...\n\nhttps://cloud.google.com/speech-to-text/docs/reference/rest/v1p1beta1/RecognitionConfig?_ga=2.221781..."
			}
		]
	},
	{
		"Question_title": "Static HTML site translation",
		"Question_created_time": "2023-04-05T13:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Static-HTML-site-translation/td-p/541034/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 51,
		"Question_body": "My site has no php (yet), only HTML and js. The following 3 extra lines on a page insert the functionality for the visitor (with a flag) to convert the page to other languages.  This free service works quite well except with some names. I have collected a table of proper translations and would like to use that as a glossary. Please help me to modify the code. A GCloud project and the Translate API is already created, but I did not find how to use them. TIA ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-06T13:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@sbarany,\n\nWelcome back to Google Cloud Community\n\nCloud Translation enables your websites and applications to dynamically translate text\u00a0programmatically\u00a0through an API. Translation uses a Google pre-trained or a custom machine learning model to translate text. By default, Translation uses a Google pre-trained Neural Machine Translation (NMT) model, which Google updates on semi-regular cadence when more training data or better techniques become available.\n\nHere are some references that might help you.\n\nCloud Translation\nhttps://cloud.google.com/translate/docs/overview\n\nGlossary stopwords\nhttps://cloud.google.com/translation-hub/docs/admin-glossary-stopwords"
			}
		]
	},
	{
		"Question_title": "Text to Speech - Custom Voice",
		"Question_created_time": "2023-04-05T01:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Text-to-Speech-Custom-Voice/td-p/540806/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 62,
		"Question_body": "Hello,we are interesting in a custom voice model which should be used for Dialogflow voicebot. As per this link https://cloud.google.com/text-to-speech/custom-voice/docs   I am trying to submit a new model / request but was not able to finish it. Everything is prepared but when apply the curl command it ends with two different errors. {\n\"input\":{\n\"text\":\"Android is a mobile operating system developed by Google, based on the Linux kernel and designed primarily for touchscreen mobile devices such as smartphones and tablets.\"\n},\n\"voice\":{\n\"languageCode\":\"en-US\",\n\"custom_voice\":{\n\"reportedUsage\":\"REALTIME\",\n\"model\":\"projects/<xxx>/locations/us-central1/models/cctr\",\n}\n},\n\"audioConfig\":{\n\"audioEncoding\":\"LINEAR16\"\n}\n}First is missing language code where I add \"languageCode\":\"en-US\",  based on the documentation but then it ends with missing model error. I think I missed some step but was not able to find which one, probably some step with model creation?Any help will be appreciated.Thank you,Roman",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-06T11:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Roman5\u00a0\n\nWelcome back to Google Cloud Community\n\nAfter you set up your text-to-speech custom voice you need to implement your model.\nTo implement Custom Voice, please\u00a0contact a member of the sales team\n\nModel training\nIt takes Google several weeks to train and evaluate your custom voice model. There is no SLA support for critical bugs for Beta features.\n\nEvaluation and user acceptance tests\nGoogle conducts an initial round of evaluation of the trained model. Once it passes our internal quality criteria, we will send you some offline audio samples recorded using your custom model. You will then follow a user acceptance testing process to evaluate the audio results and officially sign off on the model.\n\n\nHere is a reference that might help you.\n\nText-to-Speech documentation\nhttps://cloud.google.com/text-to-speech/custom-voice/docs\n\nCustom Voice quickstart\nhttps://cloud.google.com/text-to-speech/custom-voice/docs/quickstart\n\u00a0\nContact sales team\nhttps://cloud.google.com/contact"
			}
		]
	},
	{
		"Question_title": "Google Cloud Translate API - Your client does not have permission to get URL /language/translate/v2",
		"Question_created_time": "2023-03-17T03:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Translate-API-Your-client-does-not-have-permission/td-p/533662/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 127,
		"Question_body": "I'm trying to make a request to google cloud translate:  However, I'm getting a response that:  The same code on a different server works without problems. I didn't setup any restrictions for api key:What could be the possible causes for this ? Can the request be blocked by the server and how should I check it ? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-21T08:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The error message you are seeing might be a 403 Forbidden error which means that the server is refusing to fulfill your request. Can you try accessing the server from a different ip address?"
			},
			{
				"Answer_created_time": "2023-03-21T09:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I've tried that, but still getting the same results. Is it possible that the restriction is from the google server side ? what could be causing this ?"
			},
			{
				"Answer_created_time": "2023-03-23T09:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I suggest you contact Google Cloud support for further assistance.\u00a0Engineers will have a better visibility of your project"
			},
			{
				"Answer_created_time": "2023-04-06T06:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "just for reference if someone gets the same error in the future - we needed to update the curl version in the server after that it worked fine."
			}
		]
	},
	{
		"Question_title": "How to compare and evaluate all trial models in AutoML?",
		"Question_created_time": "2023-04-05T17:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-compare-and-evaluate-all-trial-models-in-AutoML/td-p/541098/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 60,
		"Question_body": "I have ran an AutoML forecasting job to predict sales. The training job modeled 69 models (from trial models) and 9 of them were selected to create an ensemble model as final model.Q1: I want to evaluate and compare all trial models that autoML performed in the training, is it possible?\nQ2: From the logs, I have all 69 models with their hyperparameters, how do I recreate all these model and get another predictions over it?Thank you for your interest and please let me know if you have more questions. logs from the trial training job with total 69 trial models and hyperparameters  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-05T17:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have ran an AutoML forecasting job to predict sales. The training job modeled 69 models (from trial models) and 9 of them were selected to create an ensemble model as final model.\n\nQ1: I want to evaluate and compare all trial models that autoML performed in the training, is it possible?\nQ2: From the logs, I have all 69 models with their hyperparameters, how do I recreate all these model and get another predictions over it?\n\nThank you for your interest and please let me know if you have more questions.\n\n\u00a0\n\nlogs from the trial training job with total 69 trial models and hyperparameters"
			}
		]
	},
	{
		"Question_title": "Cloud Retail - Tag Manager Setup",
		"Question_created_time": "2023-04-04T15:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Retail-Tag-Manager-Setup/td-p/540701/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "We've been trying to setup tag manager to get events flowing into our cloud retail project. It feels like I'm stuck on step 0, which is setting up the Tag correctly. There's a step that asks for the project number but when I hover over it, it actually says it's the project ID. I'm not sure what to put in here, the Project ID of our cloud project or the Project Number of the cloud project which is available on the project dashboard.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-05T10:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@beer_chops,\n\nWelcome back to Google Cloud Community.\n\n\n\nProject ID: A globally unique identifier for your project.\n\nA project ID is a unique string used to differentiate your project from all others in Google Cloud. You can use the Google Cloud console to generate a project ID, or you can choose your own. You can only modify the project ID when you're creating the project.\n\nProject ID requirements:\n\nMust be 6 to 30 characters in length.\nCan only contain lowercase letters, numbers, and hyphens.\nMust start with a letter.\nCannot end with a hyphen.\nCannot be in use or previously used; this includes deleted projects.\nCannot contain restricted strings, such as\u00a0google\u00a0and\u00a0ssl.\n\n\nDon't include sensitive information in your project name, project ID, or other resource names. The project ID is used in the name of many other Google Cloud resources, and any reference to the project or related resources exposes the project ID and resource name.\n\nHere are some references that might help you:\nhttps://cloud.google.com/resource-manager/docs/creating-managing-projects\n\nhttps://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy#projects"
			},
			{
				"Answer_created_time": "2023-04-05T11:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks @Aris_O\u00a0- I know what my Project ID is, however, when setting up cloud retail as a tag in tag manager the instructions are ambiguous as they say to use the \"Project Number\" in some places and \"Project ID\" in other places."
			}
		]
	},
	{
		"Question_title": "best practice for document AI",
		"Question_created_time": "2023-04-02T04:08:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/best-practice-for-document-AI/td-p/539911/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "while defining label locations, is it better to strictly select the desired data, or is it more beneficial to the AI if you select the desired data, as well as its surrounding data, and then delete the surrounding data in the selection text box?to be clear i only want the name field,but wondering whether selecting the entire address and prior headline text and then removing it from in the left text box field is better for learning?see pics for clarification   ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-03T16:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Welcome back to Google Cloud Community.\n\nI understand that you are getting lot of error for your application.\u00a0\n\nI would like to have some more information so that I can investigate further for your issue.\n\n1.) Can you provide me the configuration files (app.yaml, queue.yaml and snippet of the code which causing the error)\n2.) What is your application doing ?\n3.) Did the same code was working fine before or not ? If yes then did you made any changes in your code ?\n\n\n\nOnce I have above information in hand. I will be in a better position to help you further. Let me know if you have any other question or concern."
			},
			{
				"Answer_created_time": "2023-04-05T10:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@binary420,\n\nWelcome back to Google Cloud Community.\n\nIf your documents all have a similar format (for example, a fixed form with very low variation), then fewer documents are required to achieve accuracy. The higher the variation, the more documents are required.\n\nHere are some reference that might help you.\n\nhttps://cloud.google.com/document-ai/docs/workbench/training-overview\n\nhttps://cloud.google.com/document-ai/docs/workbench/train-processor?_ga=2.208094802.-1362987118.1680...\n\nhttps://cloud.google.com/document-ai/docs/workbench/label-documents?_ga=2.212100308.-1362987118.1680..."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX - Voximplant liveAgentHandoff issues",
		"Question_created_time": "2023-03-13T08:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-Voximplant-liveAgentHandoff-issues/td-p/531855/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 15,
		"Question_score_count": 0,
		"Question_view_count": 312,
		"Question_body": "Hi,I am currently experimenting with the liveAgentHandoff Feature and Voximplant and noticed that when the liveAgentHandoff gets triggered, my inputs still get recognized by Voximplant but Dialogflow gets stuck and wont react or continue the flow. Also sending an Event to Dialogflow doesn't have any impact.  How can I resolve that issue, does Dialogflow just stop by default when the liveAgentHandoff gets triggered or is the Voximplant connection the problem?Thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-14T05:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "// You can try this code \nconversationParticipant.addEventListener(CCAI.Events.Participant.MarkerReached, (e) => { \n    Logger.write(\"MARKER REACHED IN USER VE SCRIPT\") \n    // some flag that set for live agent handoff \n    if (transfer) { call.stopMediaTo(conversationParticipant); \n        // Send the Event to Dialogflow after media has stopped flowing \n        conversationParticipant.analyzeContent({ eventInput: { name: \"WELCOME\", languageCode: languageCode } }); \n    } \n})"
			},
			{
				"Answer_created_time": "2023-03-14T06:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks, but I already use this code. It seems, the Dialogflow Event handler does not recognize the eventInput of Voximplant, because I can see in the log that the event gets executed and als the WELCOME event works to kick of the conversation."
			},
			{
				"Answer_created_time": "2023-03-14T06:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It will recognize it if you use this exact code, not just analyzeContent function. You need to stop media to DF in MarkerReached event, since when the media is flowing via GRPC DF doesn't recognize commands."
			},
			{
				"Answer_created_time": "2023-03-15T04:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "conversationParticipant.addEventListener(CCAI.Events.Participant.PlaybackFinished, (e) => {\n  if (transfer) {\n\n    transfer = false;\n    /* callPSTN calls the first phone number and gets the phone number dynamic from the incoming call from call.callerid()\n    The first phone number has to be verified by Voximplant/Settings/CallerID */\n    outboundCall = VoxEngine.callPSTN('+exampleNumber', call.callerid() );\n      Logger.write(\"executed calllPSTN\")\n    outboundCall.addEventListener(CallEvents.Connected, () => {\n      Logger.write(\"call connected\")\n        call.stopMediaTo(conversationParticipant);\n        conversationParticipant.analyzeContent({\n          eventInput: { name: \"TRANSFER_SUCCESS\", languageCode: languageCode },\n        });\n        endConversation();\n    });\n    outboundCall.addEventListener(CallEvents.Failed, (e) => {\n      Logger.write(`Transfer failed: ${JSON.stringify(e)}`)\n      call.stopMediaTo(conversationParticipant);\n      conversationParticipant.analyzeContent({\n        eventInput: { name: \"TRANSFER_FAIL\", languageCode: languageCode },\n      });\n      call.sendMediaTo(conversationParticipant);\n    });\n  }\n\n  else if (hangup) {\n    endConversation();\n  }\n  });\n\nTried it like this now, but it does not work, the event gets sent according to the log but the call remains silent but connected and the dialogflow fulfillment dialogue is not triggered.\n\nAlso I've checked the names of the Event sent and the Events in DF are the same.\n\nAny other idea where the problem may is?"
			},
			{
				"Answer_created_time": "2023-03-15T04:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, the problem is that you have\u00a0\n\ncall.stopMediaTo(conversationParticipant);\n\nin the wrong event, this is being overriden in\u00a0\n\nCCAI.Events.Participant.MarkerReached\n\nSo you need to add this handle and call stopMediaTo there"
			},
			{
				"Answer_created_time": "2023-03-21T08:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Did you get it working? I have exactly the same problem. I've listened for the MarkerReached event, and called stopMediaTo, but it makes no difference.\n\n\n\nconversationParticipant.addEventListener(CCAI.Events.Participant.MarkerReached, (e) => { \n        Logger.write(\"MARKER REACHED IN USER VE SCRIPT\") \n        if (transfer) { \n            call.stopMediaTo(conversationParticipant); \n        } \n    });\nconversationParticipant.addEventListener(CCAI.Events.Participant.PlaybackFinished, (e) => {\n        if (transfer) {\n            Logger.write(\"Do transfer\");\n            transfer = false;\n            // Do an outbound call and connect it with the inbound one\n\n            outboundCall = VoxEngine.callSIP(SIP_CONNECTION, {\n                authUser: USERNAME,\n                password: \"wrong_password\",\n                extraHeaders: {},\n                video: false,\n                outProxy: null\n            });\n\n\n            outboundCall.addEventListener(CallEvents.Connected, () => {\n                Logger.write('connected');\n                VoxEngine.easyProcess(call, outboundCall, () => {\n                    conversationParticipant.analyzeContent({\n                        eventInput: { name: \"TRANSFER_SUCCESS\", languageCode: languageCode },\n                    });\n                    endConversation();\n                    recorder.stop();\n                });\n            });\n            outboundCall.addEventListener(CallEvents.Failed, (e) => {\n                Logger.write(`Transfer failed: ${JSON.stringify(e)}`)\n                            \n                conversationParticipant.analyzeContent({\n                    eventInput: { name: \"TRANSFER_FAIL\", languageCode: languageCode },\n                });\n            });\n        }else \n        if (hangup) {\n            endConversation();\n        }\n    })"
			},
			{
				"Answer_created_time": "2023-03-21T08:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can use Logger.write and check in the log when stopMediaTo and startMediaTo are being called during the scenario execution. When media is going to the DF agent it doesn't detect events, although GRPC allows this, but DF doesn't support this. In your scenario it's likely that the order of calls to start/stop media function is wrong"
			},
			{
				"Answer_created_time": "2023-03-21T08:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\nI tried to put the stopmedia in different places but the logging looks right to me:\nthis is the part before the sip call:\n\n2023-03-21 15:26:39 Agent handoff\n2023-03-21 15:26:39 Executing JS command: StopMedia with params [{from = A111E0CD26143ED7.1679412381.1949363 ; to = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; } ;  ]\n2023-03-21 15:26:39 Sent event to JS onPhoneEvent with params [{id = Le3FtBGxRIeeijnq6W0EVYg5dRYGREIql9abd_2WRbM ; name = AI.Events.CcaiParticipantPlaybackStarted ; participantId = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; playbackId = d9b39eb8-3b77-4eae-a8e7-207bde2ca262 ; } ;  ]\n2023-03-21 15:26:40 Sent event to JS onPhoneEvent with params [{id = Le3FtBGxRIeeijnq6W0EVYg5dRYGREIql9abd_2WRbM ; name = AI.Events.CcaiParticipantMarkerReached ; offset = -500 ; participantId = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; playbackId = d9b39eb8-3b77-4eae-a8e7-207bde2ca262 ; } ;  ]\n2023-03-21 15:26:40 Executing JS command: SendMedia with params [{from = A111E0CD26143ED7.1679412381.1949363 ; to = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; } ;  ]\n2023-03-21 15:26:40 MARKER REACHED IN USER VE SCRIPT\n2023-03-21 15:26:40 Executing JS command: StopMedia with params [{from = A111E0CD26143ED7.1679412381.1949363 ; to = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; } ;  ]\n2023-03-21 15:26:41 Sent event to JS onPhoneEvent with params [{id = Le3FtBGxRIeeijnq6W0EVYg5dRYGREIql9abd_2WRbM ; name = AI.Events.CcaiParticipantPlaybackFinished ; participantId = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; playbackId = d9b39eb8-3b77-4eae-a8e7-207bde2ca262 ; } ;  ]\n2023-03-21 15:26:41 Sent event to JS onPhoneEvent with params [{id = Le3FtBGxRIeeijnq6W0EVYg5dRYGREIql9abd_2WRbM ; name = AI.Events.CcaiParticipantPlaybackStopped ; participantId = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; playbackId = d9b39eb8-3b77-4eae-a8e7-207bde2ca262 ; } ;  ]\n2023-03-21 15:26:41 Do transfer\n2023-03-21 15:26:41 Executing JS command: CallSIP with params [{id = EnXfkzRvSheoIzOY3YMOjIUBS-FzdUnAt4ZlHcW7C-E ; } ; \n\nThen there is some sip logging.\nThen it fails because of the wrong password:\n\n2023-03-21 15:26:41 Sent event to JS onPhoneEvent with params [{code = 407 ; headers = {Reason =  ; } ; id = EnXfkzRvSheoIzOY3YMOjIUBS-FzdUnAt4ZlHcW7C-E ; name = Call.Failed ; reason = Proxy Authentication Required ; } ;  ]\n2023-03-21 15:26:41 Transfer failed: {\"code\":407,\"headers\":{\"Reason\":\"\"},\"id\":\"EnXfkzRvSheoIzOY3YMOjIUBS-FzdUnAt4ZlHcW7C-E\",\"name\":\"Call.Failed\",\"reason\":\"Proxy Authentication Required\",\"eventSourceField\":\"call\",\"call\":{\"rism\":{},\"is_conf\":false}}\n2023-03-21 15:26:41 Executing JS command: CcaiParticipantAnalyzeContent with params [{conversationId = Le3FtBGxRIeeijnq6W0EVYg5dRYGREIql9abd_2WRbM ; id = L6rtn7f8QUqRtzHQuWVy2i4OUdglnkaRq7sollbMVew ; } ;  {eventInput = {languageCode = nl ; name = TRANSFER_FAIL ; } ; } ;  ]\n2023-03-21 15:26:44 Sent event to JS onPhoneEvent with params [{cost = 0.00469 ; direction = All numbers ; duration = 24 ; headers = {} ; id = A111E0CD26143ED7.1679412381.1949363 ; internalCode = 200 ; name = Call.Disconnected ; reason = Normal call clearing ; } ;  ]\n\nIt looks like StopMedia is called last before the TRANSFER_FAIL event is triggert.\n\nThanks for the help."
			},
			{
				"Answer_created_time": "2023-03-22T01:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Nahh ive tried some more\u00a0\n\nconversationParticipant.addEventListener(CCAI.Events.Participant.PlaybackFinished, (e) => {\n  if (transfer) {\n    transfer = false;\n    /* callPSTN calls the first phone number and gets the phone number dynamic from the incoming call from call.callerid()\n    The first phone number has to be verified by Voximplant/Settings/CallerID */\n    outboundCall = VoxEngine.callPSTN('+0000000000', call.callerid());\n      Logger.write(\"executed calllPSTN\")\n    outboundCall.addEventListener(CallEvents.Connected, () => {\n        Logger.write(\"call connected\")\n        success = true;\n        conversationParticipant.addPlaybackMarker(0);        \n        endConversation();\n    });\n\n    outboundCall.addEventListener(CallEvents.Failed, (e) => {\n      Logger.write(`Transfer failed: ${JSON.stringify(e)}`)\n      failure = true;\n      conversationParticipant.addPlaybackMarker(0);\n    });\n  }\n  else if (hangup) {\n      endConversation();\n    }\n  });\n\n  conversationParticipant.addEventListener(CCAI.Events.Participant.MarkerReached, (e) => {\n        Logger.write(\"MARKER  REACHED IN USER VE SCRIPT\") \n        if(success){\n          call.stopMediaTo(conversationParticipant);\n          conversationParticipant.analyzeContent({\n            eventInput: { name: \"TRANSFER_SUCCESS\", languageCode: languageCode },\n          });\n          endConversation();\n        }else if(failure){\n          call.stopMediaTo(conversationParticipant);\n          conversationParticipant.analyzeContent({\n            eventInput: { name: \"TRANSFER_FAIL\", languageCode: languageCode },\n          });\n        }\n    });\n\nafter the transfer fails the addPlaybackmarker gets executed but the MarkerReached event does not trigger and the Event is not sent.\u00a0\n\n023-03-22 07:57:36 Executing JS command: CallPSTN with params [{id = cdspU_FMShK38PqhGRtioYFobPsLzkxKlKyQ4UcADiM ; } ;  {callerid = 41798524132 ; carrier = 0 ; dump = NULL ; number = 49151100064540 ; retryOn480 = NULL ; } ;  ]\n2023-03-22 07:57:36 Executing JS command: EnableMediaStatistics with params [{id = cdspU_FMShK38PqhGRtioYFobPsLzkxKlKyQ4UcADiM ; } ;  ]\n2023-03-22 07:57:36 executed calllPSTN\n2023-03-22 07:57:37 Sent event to JS onPhoneEvent with params [{code = 404 ; headers = {Reason = Q.850;cause=1;text=\"Unallocated\";iintcode=12013;isubsystem=9 ; } ; id = cdspU_FMShK38PqhGRtioYFobPsLzkxKlKyQ4UcADiM ; name = Call.Failed ; reason = Not Found ; } ;  ]\n2023-03-22 07:57:37 Transfer failed: {\"code\":404,\"headers\":{\"Reason\":\"Q.850;cause=1;text=\\\"Unallocated\\\";iintcode=12013;isubsystem=9\"},\"id\":\"cdspU_FMShK38PqhGRtioYFobPsLzkxKlKyQ4UcADiM\",\"name\":\"Call.Failed\",\"reason\":\"Not Found\",\"eventSourceField\":\"call\",\"call\":{\"rism\":{},\"is_conf\":false}}\n2023-03-22 07:57:37 Executing JS command: CcaiParticipantAddPlaybackMarker with params [{conversationId = lGadmzhISE28nfkFgWkpPUcCIIP7GEZGuQssG6IVz50 ; id = O17kqKwIQnuiTy7Q1A9-gz6RfVEo3knSlcgJCedMb6Q ; } ;  {offset = 0 ; } ;  ]\n2023-03-22 07:57:48 Sent event to JS onPhoneEvent with params [{cost = 0.004655 ; direction = All numbers ; duration = 49 ; headers = {} ; id = 0361B6B046342332.1679471819.2340406 ; internalCode = 200 ; name = Call.Disconnected ; reason = Normal call clearing ; } ;  ]\n2023-03-22 07:57:48 Executing JS command: CcaiCompleteConversation with params [{id = lGadmzhISE28nfkFgWkpPUcCIIP7GEZGuQssG6IVz50 ; } ;  ]"
			},
			{
				"Answer_created_time": "2023-03-27T05:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "all you need to do is to have this code in your scenario:\n\nconversationParticipant.addEventListener(CCAI.Events.Participant.MarkerReached, (e) => {\n    Logger.write(\u201cMARKER REACHED IN USER VE SCRIPT\u201d)\n    call.stopMediaTo(conversationParticipant);\n})\n\nIt will stop sending media to Dialogflow from the call to DF, after that you can send either event to the DF using analyzeContent, or media using startMediaTo."
			},
			{
				"Answer_created_time": "2023-03-27T07:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thatis exactly what I have in my code, but it looks like the analizeContent doesn't work.\u00a0\n\nconversationParticipant.analyzeContent({\n             eventInput: { name: \"TRANSFER_FAIL\", languageCode: languageCode },\n           });\n\n\u00a0I've tried everything but unfortunately it just doesn't work."
			},
			{
				"Answer_created_time": "2023-03-27T07:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It does work for me, so should work for you as well. Hard to tell without seeing the whole scenario code where the issue is, you can remove all private info / credentials from your scenario and post it here, then we will check and let you know"
			},
			{
				"Answer_created_time": "2023-03-27T07:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The human handover itself does work, but the events \"TRANSFER_FAIL\" and \"TRANSFER_SUCCESS\" do never arrive at Dialogflow. I try to hit the TRANSFER_FAIL by using a wrong password.\n\nThis is the complete code:\n\nrequire(Modules.AI);\nconst languageCode = \"\";\nconst agentId = 0000;\nconst profile = \"\";\nconst appName = \"\";\nconst region = \"\";\n\nlet agent,\n    call,\n    conversation,\n    conversationParticipant,\n    isConversationCreated = false,\n    isCallCreated = false,\n    isCallConnected = false,\n    isParticipantCreated = false,\n    hangup = false,\n    transfer = false,\n    outboundCall,\n    phonenumberCaller,\n    phonenumberCalled,\n    singleResponse = false,\n    sipUsername,\n    sipPassword,\n    sipConnectionstring,\n    shortAnswers = []; //[\"ja\",\"nee\"];\n\nVoxEngine.addEventListener(AppEvents.Started, function(ev) {\n    agent = new CCAI.Agent(agentId, region);\n    agent.addEventListener(CCAI.Events.Agent.Started, () => {\n        conversation = new CCAI.Conversation({\n            agent,\n            profile: {\n                name: profile\n            },\n            project: appName\n        });\n        conversation.addEventListener(CCAI.Events.Conversation.Created, () => {\n            isConversationCreated = true;\n            createParticipant();\n        });\n    });\n});\nVoxEngine.addEventListener(AppEvents.CallAlerting, function(ev) {\n    isCallCreated = true;\n    createParticipant();\n    call = ev.call;\n    call.record();\n    \n    phonenumberCaller = ev.callerid;\n    displayName = ev.displayName; \n\n    if(ev.headers.Diversion != null){\n        phonenumberCalled = ev.headers.Diversion.substring(ev.headers.Diversion.indexOf(':')+1,ev.headers.Diversion.indexOf('@'));\n    }else {\n        phonenumberCalled = ev.destination;\n    }\n\n    call.answer();\n    call.addEventListener(CallEvents.Connected, function() {\n        isCallConnected = true;\n    });\n    call.addEventListener(CallEvents.Disconnected, function() {\n        conversationParticipant.analyzeContent({\n            eventInput: { name: \"EARLY_HANGUP\", languageCode: languageCode },\n        });\n\n        conversation.stop();\n        VoxEngine.terminate();\n    });\n});\n\nfunction endConversation() {\n    conversation.stop();\n    call.hangup();\n    VoxEngine.terminate();\n}\n\nfunction createParticipant() {\n    if (!isConversationCreated || !isCallCreated) return;\n    conversationParticipant = conversation.addParticipant({\n        call: call,\n        options: {\n            role: \"END_USER\"\n        },\n        dialogflowSettings: {\n            enableMixingAudio: true,\n            lang: languageCode,\n            singleUtterance: true,\n            replyAudioConfig: {\n                audioEncoding: \"OUTPUT_AUDIO_ENCODING_OGG_OPUS\",\n                // Synthesized voice configuration\n                synthesizeSpeechConfig: {\n                    voice: {\n                        name: \"nl-NL-Wavenet-E\"\n                    }\n                },\n            }\n        }\n    });\n\n    conversationParticipant.addEventListener(CCAI.Events.Participant.Created, () => {\n        isParticipantCreated = true;\n        setupMedia();\n    });\n\n    conversationParticipant.addEventListener(CCAI.Events.Participant.Response, (e) => {\n  if (e.response.automatedAgentReply?.responseMessages) {\n    e.response.automatedAgentReply.responseMessages.forEach((response) => {\n      if (response.liveAgentHandoff) transfer = true;\n      if (response.endInteraction && e.response.replyText) hangup = true;\n      else if (response.endInteraction) endConversation();\n    })\n  }\n});\n\n\n    conversationParticipant.addEventListener(CCAI.Events.Participant.Response, (e) => {\n\n        if (e.response.automatedAgentReply) {\n                let webhookPayload = e.response.automatedAgentReply.parameters\n                if (\n                    webhookPayload &&\n                    webhookPayload.expectShortAnswer &&\n                    webhookPayload.shortAnswers &&\n                    webhookPayload.shortAnswers.length > 0\n                ) {\n                    Logger.write(\"Expecting a short answer, so starting check for intermediary speech recognition results\");\n                    singleResponse = true;\n                    shortAnswers = webhookPayload.shortAnswers;\n                    Logger.write(shortAnswers);\n                } else {\n                    singleResponse = false;\n                    shortAnswers = [];\n                }\n        }\n\n        if (!singleResponse && e.response.automatedAgentReply?.responseMessages) {\n            e.response.automatedAgentReply.responseMessages.forEach((response) => {\n                if (response.liveAgentHandoff) {\n                    Logger.write(\"Agent handoff\");\n                    let webhookPayload = e.response.automatedAgentReply.parameters;\n    \n                    sipUsername = webhookPayload.SIP_USERNAME;\n                    sipPassword = webhookPayload.SIP_PASSWORD;\n                    sipConnectionstring = webhookPayload.SIP_CONNECTIONSTRING;\n\n                    transfer = true;\n                    }\n                if (response.endInteraction && e.response.replyText) {\n                    Logger.write(\"Hangup\");\n                    hangup = true;\n                    }\n                else if (response.endInteraction) {\n                    Logger.write(\"End conversation\");\n                    endConversation();\n                    }\n            })\n        }else{\n\n            if (singleResponse && e.response.recognitionResult && !e.response.automatedAgentReply) {\n                const transcript = e.response.recognitionResult.transcript;\n                if (transcript) {\n                    Logger.write(\"Found transcript\");\n                    \n                    if (e.response.recognitionResult.isFinal) {\n                        Logger.write(\"Got a final response from Dialogflow so sending query\")\n                        call.stopMediaTo(conversationParticipant) // Stop sending media to Dialogflow\n                    } else if (singleResponse) {\n                        if (shortAnswers.includes(transcript.toLowerCase())) {\n                            Logger.write(\"Short answer found\");\n                            singleResponse = false;\n                           call.stopMediaTo(conversationParticipant); \n                        }\n                        else {\n                            Logger.write(\"No short answer match\");\n                            call.stopMediaTo(conversationParticipant); \n                        }\n                    }\n                } else {\n                    Logger.write(\"No matches\");\n                    call.stopMediaTo(conversationParticipant);\n                }\n            }\n        }\n    });\n\n\n    conversationParticipant.addEventListener(CCAI.Events.Participant.PlaybackFinished, (e) => {\n        if (transfer) {\n            Logger.write(\"Do transfer\");\n            transfer = false;     \n\n            outboundCall = VoxEngine.callSIP(sipConnectionstring, {\n                authUser: sipUsername,\n                password: \"wrong_password\",\n                extraHeaders: {},\n                video: false,\n                outProxy: null\n            });\n            \n\n            outboundCall.addEventListener(CallEvents.Connected, () => {\n                Logger.write('Transfer connected');\n                VoxEngine.easyProcess(call, outboundCall, () => {\n                    conversationParticipant.analyzeContent({\n                        eventInput: { name: \"TRANSFER_SUCCESS\", languageCode: languageCode, parameters:{} },\n                    });\n                    endConversation();\n                });\n            });\n            outboundCall.addEventListener(CallEvents.Failed, (a) => {\n              Logger.write('Transfer failed')\n              conversationParticipant.analyzeContent({\n                eventInput: { name: \"TRANSFER_FAIL\", languageCode: languageCode, parameters:{} },\n              });\n            });\n        }else \n        if (hangup) {\n            endConversation();\n        }\n    })\n\n    \n    \n    conversationParticipant.addEventListener(CCAI.Events.Participant.MarkerReached, (e) => {\n        // without transfer check, the conversation stops immediately\n        if (transfer) {\n            Logger.write(\"MARKER REACHED IN USER VE SCRIPT\") \n            call.stopMediaTo(conversationParticipant); \n        }\n    });\n}\n\nfunction setupMedia() {\n    if (!isParticipantCreated || !isCallConnected) return;\n    conversationParticipant.analyzeContent({\n        eventInput: {\n            name: \"WELCOME\",\n            languageCode: languageCode,\n            parameters: {\n                phonenumber: phonenumberCaller,\n                phonenumberCalled: phonenumberCalled\n            }\n        },\n    });\n\n    conversationParticipant.sendMediaTo(call);\n}\n\nThanks for the help.\nGreetings"
			},
			{
				"Answer_created_time": "2023-03-27T08:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Do you have a message for playback in case of the live agent handoff response that is played back to the user? Please send me the call log using Direct Message functionality of the forum"
			},
			{
				"Answer_created_time": "2023-04-05T08:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I checked your code and enabled Cloud logging for the DF agent, I see that events are actually reaching DF backend in Logs section of Google Cloud console. Try to enable cloud logging and give it a try."
			}
		]
	},
	{
		"Question_title": "Create User Managed Workbook with A100 40gb",
		"Question_created_time": "2023-04-03T06:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Create-User-Managed-Workbook-with-A100-40gb/td-p/540188/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 49,
		"Question_body": "I'm trying to create a user managed workbook with an a100 40gb GPU, but I am always running into this same error:Project-Name: The zone 'projects/project-name/zones/us-east1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later.: Something went wrong. Sorry about that.I am pretty sure I have the appropriate quotas, and I don't have any other workbooks so my global gpu quota shouldn't be the chokepoint. (Do I need the Preemptible Nvidia A100 GPUs quota? All of those are set to 0.) I have the \"Managed Notebooks NVIDIA A100 GPUs per region\" set to 1, but I guess those only apply to, well, managed notebooks. Any tips? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-05T06:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What region are you in and did you try in other regions? It is possible that you may need to request additional quotas for preemptible nvidia a100 GPUS if you don't have any available. You can do so by referring to this documentation."
			},
			{
				"Answer_created_time": "2023-04-05T07:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I tried creating workbooks in every US region where the A100 is available, but I did manage to create a (non-user) managed workbook with an A100 GPU, so I guess my quotas allow at least for that. I guess I shouldn't be surprised it's so unavailable, everyone needs lots of compute nowadays."
			}
		]
	},
	{
		"Question_title": "Experiment tracking - Metadata store",
		"Question_created_time": "2023-04-03T02:04:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Experiment-tracking-Metadata-store/td-p/540132/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Hey,i got a question regarding the MLOps-Principles. Is the metadatastore one component of experiment tracking or are those different aspects of MLOps. What i thought, experiment tracking tracks the metadata for the experiments in creating a training pipeline and the metadata store tracks the information about the pipeline runs in production. Or is everything of that included in experiment tracking?Can somebody help me with that? Thanks in advance",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-05T06:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In MLOps, experiment tracking and metadata management are closely related but distinct concepts. Experiment tracking is about keeping track of the details of each experiment you run, such as the hyperparameters, training data, and evaluation metrics, so that you can reproduce and iterate on your results. Metadata management, on the other hand, is about keeping track of the context and dependencies of your models and pipelines, such as the data sources, code versions, and infrastructure configurations, so that you can understand and manage the entire lifecycle of your models."
			}
		]
	},
	{
		"Question_title": "DocAI Warehouse UI provision problem",
		"Question_created_time": "2023-02-04T14:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DocAI-Warehouse-UI-provision-problem/td-p/518134/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 128,
		"Question_body": "hi. have the problem with Provision DocAI Warehouse. I can complete all 4 steps but on the last step when all is don and I seeDocAI Warehouse UI\nVisit the link below to the DocAI Warehouse UI (Preview) to finalize setup of your schema, ACLs, AI processor mappings, and notifications.https://documentwarehouse.cloud.google.com/provision/***********When I enter to this link to complete my provision. On the first step I see the project number and the following message \"Request had insufficient authentication scopes. Logout and re-login will fix this problem.\" - it somehow log out from my Google account and provision page somehow on the unlogged page and I am stuck. On the provision page all other steps 2-4 are grey. according the docs all have to be easy - https://cloud.google.com/document-warehouse/docs/administer-warehouse on the first step the author on the top right corner of the screen have Search, project name, admin, and his Google account. In my case I have only Search and project name. Admin and my Google account from no reason does not exist. Can you please help me? I have the project owner right. \"\"",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-04T01:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Vlad_ML ,\n\nDid you manage to solve this issue?"
			},
			{
				"Answer_created_time": "2023-04-04T20:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'd suggest looking here ...\n\nhttps://cloud.google.com/document-warehouse/docs/administer-warehouse#troubleshooting\n\nSince the UI is still beta code, it seems that one must join a Google group to get access.\u00a0 I have seen strange things happen if one doesn't join that group.\u00a0 Things get confused.\u00a0 I have also seen examples of, after joining the group, one needs to sign-out of the Google Chrome browser and sign back in again before the change is detected."
			}
		]
	},
	{
		"Question_title": "AutoML : Unable to get the Details of Native Models",
		"Question_created_time": "2023-04-04T02:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Unable-to-get-the-Details-of-Native-Models/td-p/540509/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 116,
		"Question_body": "We are using Auto ML Client libraries(C# and PHP) for listing the AutoML custom Translation models.\nBut with the API'swe are only able to list the Legacy models, Native models details are missing from the response. Is there any other API's that can be used to list Native models?\n\nIn console GCP suggests to use Cloud Translation API for managing the native models, but in that library, there are no API's to list the models  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-04T02:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We are using Auto ML Client libraries(C# and PHP) for listing the AutoML custom Translation models.\nBut with the API's\n\nAutoMlClient.ListModels(parent);\n$autoMlClient->listModels($formattedParent);\n\nwe are only able to list the Legacy models, Native models details are missing from the response. Is there any other API's that can be used to list Native models?\n\nIn console GCP suggests to use Cloud Translation API for managing the native models, but in that library, there are no API's to list the models"
			}
		]
	},
	{
		"Question_title": "whats the proper syntax to print out a specific entity in python",
		"Question_created_time": "2023-04-02T17:28:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/whats-the-proper-syntax-to-print-out-a-specific-entity-in-python/td-p/540001/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 43,
		"Question_body": "Ive gotten all the way to getting this working document_object = result.document\nprint(\"Document processing complete.\")\nprint(f\"Text: {document_object.text}\"unfortunately that just prints out the entire page.whats the syntax to print out just a specific field \"entity\"?cant figure out the proper syntax",
		"Answer_list": [
			{
				"Answer_created_time": "2023-04-03T16:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Welcome back to Google Cloud Community.\n\nI understand that you are getting lot of error for your application.\u00a0\n\nI would like to have some more information so that I can investigate further for your issue.\n\n1.) Can you provide me the configuration files (app.yaml, queue.yaml and snippet of the code which causing the error)\n2.) What is your application doing ?\n3.) Did the same code was working fine before or not ? If yes then did you made any changes in your code ?\n\n\n\nOnce I have above information in hand. I will be in a better position to help you further. Let me know if you have any other question or concern."
			}
		]
	},
	{
		"Question_title": "How do I run my custom model training from vscode",
		"Question_created_time": "2023-03-30T02:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-do-I-run-my-custom-model-training-from-vscode/td-p/538798/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 126,
		"Question_body": "I have run a custom model training on workbench. I want to execute this training from VSCode. Is that possible and if yes, how do i do it?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-31T12:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@nitya_r\u00a0\n\nWelcome back to Google Cloud Community.\n\nIt is possible to execute training to VSCode. Connect to a Custom User Managed Notebook\n\nTo use VSCode with a Vertex AI Workbench obtained starting from a custom container (e.g. our custom-image machine) a few additional steps are required:\n\nConnect to the instance following the same procedure described in the previous section.If you check /home/jupyter directory you will find the cloned content, but you will not find libraries, environments or code configured on your container.\nIn fact, your custom container is executed on a docker instance on the virtual machine.\nTo be able to connect to the container with VSCode, your user must belong to the docker group of the remote machine. Add your user as follows.\n\nThis reference might help you:\nhttps://medium.com/google-cloud/choose-the-ide-you-want-and-develop-on-vertex-ai-workbench-part-i-94..."
			},
			{
				"Answer_created_time": "2023-04-02T11:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@nitya_r\u00a0did you use any pipelines like tfx etc., or was your ingestion, preprocessing, training loop all in tf/keras?"
			}
		]
	},
	{
		"Question_title": "AutoML Translation: 30% of all our requests finishes with timeout",
		"Question_created_time": "2023-03-29T07:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Translation-30-of-all-our-requests-finishes-with-timeout/td-p/538365/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 86,
		"Question_body": "In our project we use 21 custom trained models to translate from EN to target_language.Last week 30% of all our requests finishes with timeout! What is the problem?!How can you/we fix it? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-30T12:40:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@ochkarik\u00a0\n\nWelcome back to Google Cloud Community.\n\nSetting request timeout (services)\nFor Cloud Run services, the request timeout setting specifies the time within which a response must be returned by services deployed to Cloud Run. If a response isn't returned within the time specified, the request ends and error 504 is returned.\n\nThe timeout is set by default to 5 minutes and can be extended up to 60 minutes.\n\nHere are some articles that might help you:\nhttps://cloud.google.com/run/docs/configuring/request-timeout\n\nhttps://cloud.google.com/python/docs/reference/storage/1.39.0/retry_timeout?_ga=2.21056062.-48059091...\n\nhttps://cloud.google.com/translate/docs/reference/rpc/google.longrunning?_ga=2.25258296.-480590913.1...\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-30T12:40:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@ochkarik\u00a0\n\nWelcome back to Google Cloud Community.\n\nSetting request timeout (services)\nFor Cloud Run services, the request timeout setting specifies the time within which a response must be returned by services deployed to Cloud Run. If a response isn't returned within the time specified, the request ends and error 504 is returned.\n\nThe timeout is set by default to 5 minutes and can be extended up to 60 minutes.\n\nHere are some articles that might help you:\nhttps://cloud.google.com/run/docs/configuring/request-timeout\n\nhttps://cloud.google.com/python/docs/reference/storage/1.39.0/retry_timeout?_ga=2.21056062.-48059091...\n\nhttps://cloud.google.com/translate/docs/reference/rpc/google.longrunning?_ga=2.25258296.-480590913.1..."
			},
			{
				"Answer_created_time": "2023-04-02T02:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello, friends! Now everything is working very well - 0 timeout error! Thank you!\n\nBut we noticed another minor problem: after uploading new dataset for training language model and after training the model we do not receive any email notifications about operation finishing."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX - ES-US Spanish Currency Collection",
		"Question_created_time": "2023-03-26T12:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-ES-US-Spanish-Currency-Collection/td-p/537201/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 66,
		"Question_body": "I'm running into issues where the unit-currency Entity for Spanish is not collecting cents (centavos) when text is fed into it.  Ex: $5 y 5 centavos is only being recognized as $5 instead of $5.05 by the entity.Has anyone else run into this?  Also due the the size of the agent we are using Standard NLU.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-28T16:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for contacting Google Cloud Platform. I understand that you are getting lot of error for your application.\u00a0\n\nI would like to have some more information so that I can investigate further for your issue.\n\n1.) Can you provide me the configuration files (app.yaml, queue.yaml and snippet of the code which causing the error)\n2.) What is your application doing ?\n3.) Did the same code was working fine before or not ? If yes then did you made any changes in your code ?\n4.) Are you using deferred library for your taskqueue ?\n\nOnce I have above information in hand. I will be in a better position to help you further. Let me know if you have any other question or concern. I will be more than happy to help you."
			},
			{
				"Answer_created_time": "2023-03-30T09:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @kburnam,\n\nWelcome back to Google Cloud Community!\n\nThis article might help you:\nhttps://cloud.google.com/dialogflow/es/docs/reference/system-entities"
			}
		]
	},
	{
		"Question_title": "Documentai batch failure",
		"Question_created_time": "2023-03-26T22:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Documentai-batch-failure/td-p/537268/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "Hi,I am new to Google cloud platform, but I am testing out the Documentai API in python. I created an invoice parser and had successfully processed a few dozen invoices sequentially, before trying the batch API following the documentation. However, the code sample provided doen't work as a batch. Here is the sample code provided.https://cloud.google.com/document-ai/docs/samples/documentai-batch-process-documents-processor-versi...I used the alternative approach, to specify entire directory prefix. The batch started successfully, but terminated after 3 invoices we processed. It then threw below error. Has anyone encountered this error while using batch processing with Documentai? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-28T16:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for contacting Google Cloud Platform. I understand that you are getting lot of error for your application.\u00a0\n\nI would like to have some more information so that I can investigate further for your issue.\n\n1.) Can you provide me the configuration files (app.yaml, queue.yaml and snippet of the code which causing the error)\n2.) What is your application doing ?\n3.) Did the same code was working fine before or not ? If yes then did you made any changes in your code ?\n4.) Are you using deferred library for your taskqueue ?\n\nOnce I have above information in hand. I will be in a better position to help you further. Let me know if you have any other question or concern. I will be more than happy to help you."
			},
			{
				"Answer_created_time": "2023-03-30T09:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@ddonkuro,\n\nWelcome back to Google Cloud Community!\n\nThis article might help you:\nhttps://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.table.RowIterat...\nhttps://cloud.google.com/batch/docs/get-started"
			}
		]
	},
	{
		"Question_title": "Dialogflow CX can't connect to integrations",
		"Question_created_time": "2023-03-28T20:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-can-t-connect-to-integrations/td-p/538170/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "my  text messenger integrations (LINE, Facebook) does not receive a response message from Dialogflow CX but has no problem on Dialogflow CX test agent simulator and Dialogflow ES",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-29T15:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@jbhop,\n\nWelcome to Google Cloud Community.\n\nYou can check your inegration setup from Dialogflow CX\n\nThis articles might help you:\nhttps://cloud.google.com/dialogflow/cx/docs/concept/integration/facebook-messenger\nhttps://cloud.google.com/dialogflow/cx/docs/concept/console"
			},
			{
				"Answer_created_time": "2023-03-30T01:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "thank you,\u00a0@Aris_O\u00a0\n\nI try to set up a Facebook integration on Dialogflow CX following the docs\u00a0 but I still have a problem\n\nIt works on Dialogflow ES in the same way"
			}
		]
	},
	{
		"Question_title": "BigQuery ML Evaluation Metrics",
		"Question_created_time": "2023-03-27T09:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/BigQuery-ML-Evaluation-Metrics/td-p/537516/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 68,
		"Question_body": "Hi,Wondering for BigQuery ML model evaluation, is it possible to pass custom metrics for model evaluation or model comparison purpose while using BigQuery. I am planning to do a binary classification and have some self defined metrics that I want to use to compare and pick the best model.Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-28T16:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for contacting Google Cloud Platform. I understand that you are getting lot of error for your application. My name is Shobhit and I am glad to assist you with this case.\n\nI would like to have some more information so that I can investigate further for your issue.\n\n1.) Can you provide me the configuration files (app.yaml, queue.yaml and snippet of the code which causing the error)\n2.) What is your application doing ?\n3.) Did the same code was working fine before or not ? If yes then did you made any changes in your code ?\n4.) Are you using deferred library for your taskqueue ?\n\nOnce I have above information in hand. I will be in a better position to help you further. Let me know if you have any other question or concern. I will be more than happy to help you."
			},
			{
				"Answer_created_time": "2023-03-29T12:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Amyhooo\u00a0\n\nWelcome to Google Cloud Community,\n\nYou can use the\u00a0ML.EVALUATE\u00a0function to evaluate model metrics.\n\nFor information about model evaluation in BigQuery ML, see\u00a0Model evaluation overview.\n\nFor information about supported model types of each SQL statement and function, and all supported SQL statements and functions for each model type, read\u00a0End-to-end user journey for each model.\n\nThanks!"
			},
			{
				"Answer_created_time": "2023-03-29T12:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Aris, Thanks for your reply! I was reading through the documents and saw that in the ML.EVALUATE, for classification it outputs: \"Reports the precision, the recall, the accuracy, the F1 score, the log los and the roc auc\"\n\nJust wondering for example we have some self defined metrics (formula) to evaluate classification, are we able to set that somewhere and ML.EVALUATE can utilize that for evaluating models?"
			}
		]
	},
	{
		"Question_title": "Hello NON-engineer here...... How to view previous VM instance history",
		"Question_created_time": "2023-03-29T06:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Hello-NON-engineer-here-How-to-view-previous-VM-instance-history/td-p/538307/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 22,
		"Question_body": "Hello as the title suggest I am not an engineer. But I hired a freelancer a while ago to create a computer vision model for me and then after a couple of weeks he just stopped responding to me. The last thing he told me was the computer vision model was working great as he was training it in google cloud. But as I mentioned he then just abruptly stopped answering all my forms of communication. So I see that he created a VM instance in google cloud is there any way to retrieve the model/dataset or anything that he was working on in my google cloud account?FYI: I granted him administrator access so he logged into my google cloud so that I would pay for the usage fees but like I said I listed his email under administrator so that he can log in and out freely.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-29T08:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "First thing revoke his access.\n\nNext in billing you can view all resources being used if you take the report under billing and filter based on SKU. Check if the ones expected are running and go to respective page by navigating from the search bar on top."
			}
		]
	},
	{
		"Question_title": "App building assistance",
		"Question_created_time": "2023-03-27T09:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/App-building-assistance/td-p/537508/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 48,
		"Question_body": "Hi there,I am completely new to this and have no programming language or coding experience. I am looking to desging an AI assistant app using Dialog flow. I am trying to navigate my way and find out where to start. Anyone that can help?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-29T06:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Here are some steps you can follow to design an AI assistant app using Dialogflow for beginners:\n1. Sign up for a free account on Dialogflow.\n2. Create a new agent and give it a name.\n3. Add intents to your agent. Intents are what your users will say to your agent, and how your agent will respond.\n4. Train your agent by providing it with examples of what your users might say.\n5. Test your agent by typing in some sample phrases and seeing how it responds.\n\nHere are great videos that might be useful for you."
			}
		]
	},
	{
		"Question_title": "Google TTS Spring Boot Program - Not getting any response",
		"Question_created_time": "2023-03-16T23:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-TTS-Spring-Boot-Program-Not-getting-any-response/td-p/533592/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 7,
		"Question_score_count": 0,
		"Question_view_count": 181,
		"Question_body": "      The program stops at the line  Does not get any response from there. When I am testing with Postman, I get a Gateway Timeout Error and if I check for logs, anything after this line is not coming. No exceptions. Nothing. Like the program just stops. Please help. Thanks in advance",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-21T07:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "can you enable debug logging to get more information on the issue?"
			},
			{
				"Answer_created_time": "2023-03-28T22:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Joevanie,\n\nThe WAR is deployed to an AWS server and the output is meant to be stored in the same.\n\nHas this got to do anything with the persistent error."
			},
			{
				"Answer_created_time": "2023-03-21T21:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have pasted the logs here twice. It is getting removed. What should I do ?"
			},
			{
				"Answer_created_time": "2023-03-21T21:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have enabled the following two logging \n\nlogging.level.io.grpc=DEBUG\nlogging.level.com.google.cloud.texttospeech.v1=DEBUG\n\n \n\n------------------------------------\n\n2023-03-17 04:56:32,868 INFO c.m.c.TextToSpeechController [ajp-nio-8009-exec-605] Controller V2 - Received Text : {\n\"text\":\"Sample Google Text-To-Speech\"\n}\n2023-03-17 04:56:32,869 INFO c.m.s.TextToSpeechServiceV2 [ajp-nio-8009-exec-605] Received Text : {\n\"text\":\"Sample Google Text-To-Speech\"\n}\n2023-03-17 04:56:32,949 INFO c.m.s.TextToSpeechServiceV2 [ajp-nio-8009-exec-605] Initialised Google Credentials\n2023-03-17 04:56:33,005 INFO c.m.s.TextToSpeechServiceV2 [ajp-nio-8009-exec-605] Build TTS Settings\n2023-03-17 04:56:33,127 DEBUG i.g.n.s.i.n.u.i.l.InternalLoggerFactory [ajp-nio-8009-exec-605] Using SLF4J as the default logging framework\n2023-03-17 04:56:33,131 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] -Dio.netty.noUnsafe: false\n2023-03-17 04:56:33,132 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] Java version: 8\n2023-03-17 04:56:33,134 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] sun.misc.Unsafe.theUnsafe: available\n2023-03-17 04:56:33,135 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] sun.misc.Unsafe.copyMemory: available\n2023-03-17 04:56:33,136 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] sun.misc.Unsafe.storeFence: available\n2023-03-17 04:56:33,137 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] java.nio.Buffer.address: available\n2023-03-17 04:56:33,138 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] direct buffer constructor: available\n2023-03-17 04:56:33,139 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] java.nio.Bits.unaligned: available, true\n2023-03-17 04:56:33,140 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9\n2023-03-17 04:56:33,140 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-605] java.nio.DirectByteBuffer.<init>(long, int): available\n2023-03-17 04:56:33,140 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] sun.misc.Unsafe: available\n2023-03-17 04:56:33,140 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] -Dio.netty.tmpdir: /opt/tomcat/apache-tomcat-8.5.32/temp (java.io.tmpdir)\n2023-03-17 04:56:33,141 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] -Dio.netty.bitMode: 64 (sun.arch.data.model)\n2023-03-17 04:56:33,142 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] -Dio.netty.maxDirectMemory: 3633315840 bytes\n2023-03-17 04:56:33,142 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] -Dio.netty.uninitializedArrayAllocationThreshold: -1\n2023-03-17 04:56:33,144 DEBUG i.g.n.s.i.n.u.i.CleanerJava6 [ajp-nio-8009-exec-605] java.nio.ByteBuffer.cleaner(): available\n2023-03-17 04:56:33,144 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] -Dio.netty.noPreferDirect: false\n2023-03-17 04:56:33,187 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] -Dio.netty.native.workdir: /opt/tomcat/apache-tomcat-8.5.32/temp (io.netty.tmpdir)\n2023-03-17 04:56:33,188 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] -Dio.netty.native.deleteLibAfterLoading: true\n2023-03-17 04:56:33,188 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] -Dio.netty.native.tryPatchShadedId: true\n2023-03-17 04:56:33,188 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] -Dio.netty.native.detectNativeLibraryDuplicates: true\n2023-03-17 04:56:33,203 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] Successfully loaded the library /opt/tomcat/apache-tomcat-8.5.32/temp/libio_grpc_netty_shaded_netty_transport_native_epoll_x86_642061884417420384413.so\n2023-03-17 04:56:33,209 DEBUG i.g.n.s.i.n.u.NetUtil [ajp-nio-8009-exec-605] -Djava.net.preferIPv4Stack: false\n2023-03-17 04:56:33,209 DEBUG i.g.n.s.i.n.u.NetUtil [ajp-nio-8009-exec-605] -Djava.net.preferIPv6Addresses: false\n2023-03-17 04:56:33,212 DEBUG i.g.n.s.i.n.u.NetUtilInitializations [ajp-nio-8009-exec-605] Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)\n2023-03-17 04:56:33,213 DEBUG i.g.n.s.i.n.u.NetUtil [ajp-nio-8009-exec-605] /proc/sys/net/core/somaxconn: 128\n2023-03-17 04:56:33,251 DEBUG i.g.n.s.i.n.c.MultithreadEventLoopGroup [ajp-nio-8009-exec-605] -Dio.netty.eventLoopThreads: 8\n2023-03-17 04:56:33,329 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] Successfully loaded the library /opt/tomcat/apache-tomcat-8.5.32/temp/libio_grpc_netty_shaded_netty_tcnative_linux_x86_644871492624941475193.so\n2023-03-17 04:56:33,329 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-605] Loaded library with name 'netty_tcnative_linux_x86_64'\n2023-03-17 04:56:33,329 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-605] Initialize netty-tcnative using engine: 'default'\n2023-03-17 04:56:33,329 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-605] netty-tcnative using native library: BoringSSL\n2023-03-17 04:56:33,359 DEBUG i.g.n.s.i.n.u.ResourceLeakDetector [ajp-nio-8009-exec-605] -Dio.grpc.netty.shaded.io.netty.leakDetection.level: simple\n2023-03-17 04:56:33,359 DEBUG i.g.n.s.i.n.u.ResourceLeakDetector [ajp-nio-8009-exec-605] -Dio.grpc.netty.shaded.io.netty.leakDetection.targetRecords: 4\n2023-03-17 04:56:33,367 DEBUG i.g.n.s.i.n.b.AbstractByteBuf [ajp-nio-8009-exec-605] -Dio.grpc.netty.shaded.io.netty.buffer.checkAccessible: true\n2023-03-17 04:56:33,367 DEBUG i.g.n.s.i.n.b.AbstractByteBuf [ajp-nio-8009-exec-605] -Dio.grpc.netty.shaded.io.netty.buffer.checkBounds: true\n2023-03-17 04:56:33,369 DEBUG i.g.n.s.i.n.u.ResourceLeakDetectorFactory [ajp-nio-8009-exec-605] Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@5fa731bb\n2023-03-17 04:56:33,385 DEBUG i.g.n.s.i.n.u.i.InternalThreadLocalMap [ajp-nio-8009-exec-605] -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024\n2023-03-17 04:56:33,385 DEBUG i.g.n.s.i.n.u.i.InternalThreadLocalMap [ajp-nio-8009-exec-605] -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096\n2023-03-17 04:56:33,388 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.numHeapArenas: 8\n2023-03-17 04:56:33,388 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.numDirectArenas: 8\n2023-03-17 04:56:33,388 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.pageSize: 8192\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.maxOrder: 9\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.chunkSize: 4194304\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.smallCacheSize: 256\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.normalCacheSize: 64\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.maxCachedBufferCapacity: 32768\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.cacheTrimInterval: 8192\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.cacheTrimIntervalMillis: 0\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.useCacheForAllThreads: false\n2023-03-17 04:56:33,389 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-605] -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023\n2023-03-17 04:56:33,401 DEBUG i.g.n.s.i.n.b.ByteBufUtil [ajp-nio-8009-exec-605] -Dio.netty.allocator.type: pooled\n2023-03-17 04:56:33,401 DEBUG i.g.n.s.i.n.b.ByteBufUtil [ajp-nio-8009-exec-605] -Dio.netty.threadLocalDirectBufferSize: 0\n2023-03-17 04:56:33,401 DEBUG i.g.n.s.i.n.b.ByteBufUtil [ajp-nio-8009-exec-605] -Dio.netty.maxThreadLocalCharBufferSize: 16384\n2023-03-17 04:56:33,407 DEBUG i.g.n.s.i.n.u.ResourceLeakDetectorFactory [ajp-nio-8009-exec-605] Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@286fb77c\n2023-03-17 04:56:33,419 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-605] -Dio.netty.recycler.maxCapacityPerThread: 4096\n2023-03-17 04:56:33,419 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-605] -Dio.netty.recycler.ratio: 8\n2023-03-17 04:56:33,419 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-605] -Dio.netty.recycler.chunkSize: 32\n2023-03-17 04:56:33,419 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-605] -Dio.netty.recycler.blocking: false\n2023-03-17 04:56:33,433 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-605] org.jctools-core.MpscChunkedArrayQueue: available\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 => ECDHE-ECDSA-AES128-GCM-SHA256\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 => ECDHE-ECDSA-AES128-GCM-SHA256\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 => ECDHE-RSA-AES128-GCM-SHA256\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_128_GCM_SHA256 => ECDHE-RSA-AES128-GCM-SHA256\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 => ECDHE-ECDSA-AES256-GCM-SHA384\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 => ECDHE-ECDSA-AES256-GCM-SHA384\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 => ECDHE-RSA-AES256-GCM-SHA384\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_256_GCM_SHA384 => ECDHE-RSA-AES256-GCM-SHA384\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-ECDSA-CHACHA20-POLY1305\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-ECDSA-CHACHA20-POLY1305\n2023-03-17 04:56:33,451 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-RSA-CHACHA20-POLY1305\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-RSA-CHACHA20-POLY1305\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-PSK-CHACHA20-POLY1305\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-PSK-CHACHA20-POLY1305\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA => ECDHE-ECDSA-AES128-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_128_CBC_SHA => ECDHE-ECDSA-AES128-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA => ECDHE-RSA-AES128-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_128_CBC_SHA => ECDHE-RSA-AES128-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA => ECDHE-PSK-AES128-CBC-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_PSK_WITH_AES_128_CBC_SHA => ECDHE-PSK-AES128-CBC-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA => ECDHE-ECDSA-AES256-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_256_CBC_SHA => ECDHE-ECDSA-AES256-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA => ECDHE-RSA-AES256-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_256_CBC_SHA => ECDHE-RSA-AES256-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA => ECDHE-PSK-AES256-CBC-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_ECDHE_PSK_WITH_AES_256_CBC_SHA => ECDHE-PSK-AES256-CBC-SHA\n2023-03-17 04:56:33,452 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_RSA_WITH_AES_128_GCM_SHA256 => AES128-GCM-SHA256\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_RSA_WITH_AES_128_GCM_SHA256 => AES128-GCM-SHA256\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_RSA_WITH_AES_256_GCM_SHA384 => AES256-GCM-SHA384\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_RSA_WITH_AES_256_GCM_SHA384 => AES256-GCM-SHA384\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_RSA_WITH_AES_128_CBC_SHA => AES128-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_RSA_WITH_AES_128_CBC_SHA => AES128-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_PSK_WITH_AES_128_CBC_SHA => PSK-AES128-CBC-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_PSK_WITH_AES_128_CBC_SHA => PSK-AES128-CBC-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_RSA_WITH_AES_256_CBC_SHA => AES256-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_RSA_WITH_AES_256_CBC_SHA => AES256-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_PSK_WITH_AES_256_CBC_SHA => PSK-AES256-CBC-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_PSK_WITH_AES_256_CBC_SHA => PSK-AES256-CBC-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: TLS_RSA_WITH_3DES_EDE_CBC_SHA => DES-CBC3-SHA\n2023-03-17 04:56:33,453 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-605] Cipher suite mapping: SSL_RSA_WITH_3DES_EDE_CBC_SHA => DES-CBC3-SHA\n2023-03-17 04:56:33,454 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-605] Supported protocols (OpenSSL): [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2]\n2023-03-17 04:56:33,454 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-605] Default cipher suites (OpenSSL): [TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256]\n2023-03-17 04:56:33,662 INFO c.m.s.TextToSpeechServiceV2 [ajp-nio-8009-exec-605] In Try - Create TTS client\n2023-03-17 04:56:33,674 INFO c.m.s.TextToSpeechServiceV2 [ajp-nio-8009-exec-605] Set Voice Params\n2023-03-17 04:56:33,682 INFO c.m.s.TextToSpeechServiceV2 [ajp-nio-8009-exec-605] Audio Configured\n2023-03-17 04:56:33,889 DEBUG i.g.n.s.i.n.c.DefaultChannelId [grpc-default-executor-0] -Dio.netty.processId: 3588 (auto-detected)\n2023-03-17 04:56:33,891 DEBUG i.g.n.s.i.n.c.DefaultChannelId [grpc-default-executor-0] -Dio.netty.machineId: 02:24:3d:ff:fe:ae:9c:6a (auto-detected)\n2023-03-17 04:56:33,945 DEBUG i.g.n.s.i.n.u.ResourceLeakDetectorFactory [grpc-default-worker-ELG-1-4] Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@704160a5\n2023-03-17 04:56:34,007 DEBUG i.g.n.s.i.n.h.s.SslHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] HANDSHAKEN: protocol:TLSv1.2 cipher suite:TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n2023-03-17 04:56:34,014 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND SETTINGS: ack=false settings={ENABLE_PUSH=0, MAX_CONCURRENT_STREAMS=0, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}\n2023-03-17 04:56:34,017 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041\n2023-03-17 04:56:34,022 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND SETTINGS: ack=false settings={MAX_CONCURRENT_STREAMS=100, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=65536}\n2023-03-17 04:56:34,024 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND SETTINGS: ack=true\n2023-03-17 04:56:34,082 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041\n2023-03-17 04:56:34,083 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x26b02053, L:/10.2.0.106:36826 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND SETTINGS: ack=true"
			},
			{
				"Answer_created_time": "2023-03-23T09:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I suggest try simplifying the program or removing any unnecessary dependencies to isolate the issue. You can start with a basic program that only calls the textToSpeechClient.synthesizeSpeech() method with minimal inputs and see if that works. Then gradually add more complexity to the program until you encounter the issue again."
			},
			{
				"Answer_created_time": "2023-03-26T23:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Have reduced the code to below :\n\npublic void generateAudioFile() throws Exception\n\t{\n\t\tlogger.info(\"----------------Test Service----------------\");\n\n\t\t// Load the service account key file as a GoogleCredentials object\n\t\tGoogleCredentials credentials = GoogleCredentials.fromStream(googleJsonResource.getInputStream());\n\n\t\tlogger.info(\"Initialised Google Credentials\");\n\n\t\t// Build the TextToSpeechSettings using the builder pattern and set the service\n\t\tTextToSpeechSettings settings = TextToSpeechSettings.newBuilder().setCredentialsProvider(() -> credentials)\n\t\t\t\t.build();\n\t\t\n\t\tlogger.info(\"Settings Built\");\n\t\t\n\t\ttry (TextToSpeechClient textToSpeechClient = TextToSpeechClient.create(settings))\n\t\t{\n\t\t\tSynthesisInput synthesisInput = SynthesisInput.newBuilder().setText(\"Test Service Audio Output\").build();\n\t\t\tVoiceSelectionParams voiceSelectionParams = VoiceSelectionParams.newBuilder().setLanguageCode(\"en-US\").setSsmlGender(SsmlVoiceGender.NEUTRAL).build();\n            AudioConfig audioConfig = AudioConfig.newBuilder().setAudioEncoding(AudioEncoding.MP3).build();\n\t\t\t\n\t\t\tSynthesizeSpeechRequest request = SynthesizeSpeechRequest.newBuilder()\n                    .setInput(synthesisInput)\n                    .setVoice(voiceSelectionParams)\n                    .setAudioConfig(audioConfig)\n                    .build();\n\t\t\t\n\t\t\tlogger.info(\"Build Synthesize Speech Request\");\n\t\t\t// Perform the text-to-speech request\n\t\t\tSynthesizeSpeechResponse response = textToSpeechClient.synthesizeSpeech(request);\n\t\t\tlogger.info(\"Audio Synthesised : \"+response);\n\t\t\t\n\t\t}catch (Exception e) {\n\t\t\tlogger.error(\"Exception : \" + e.getMessage());\n\t\t\tlogger.error(e.getMessage(), e);\n\t\t\tthrow e;\n\t\t}\n\t\tlogger.info(\"Exited Try\");\n\t}"
			},
			{
				"Answer_created_time": "2023-03-26T23:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Logs :\n\n\u00a0\n\n2023-03-27 06:16:17,578 DEBUG i.g.n.s.i.n.u.i.l.InternalLoggerFactory [ajp-nio-8009-exec-969] Using SLF4J as the default logging framework\n2023-03-27 06:16:17,583 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] -Dio.netty.noUnsafe: false\n2023-03-27 06:16:17,583 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] Java version: 8\n2023-03-27 06:16:17,585 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] sun.misc.Unsafe.theUnsafe: available\n2023-03-27 06:16:17,586 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] sun.misc.Unsafe.copyMemory: available\n2023-03-27 06:16:17,587 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] sun.misc.Unsafe.storeFence: available\n2023-03-27 06:16:17,588 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] java.nio.Buffer.address: available\n2023-03-27 06:16:17,589 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] direct buffer constructor: available\n2023-03-27 06:16:17,591 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] java.nio.Bits.unaligned: available, true\n2023-03-27 06:16:17,591 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9\n2023-03-27 06:16:17,591 DEBUG i.g.n.s.i.n.u.i.PlatformDependent0 [ajp-nio-8009-exec-969] java.nio.DirectByteBuffer.<init>(long, int): available\n2023-03-27 06:16:17,591 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] sun.misc.Unsafe: available\n2023-03-27 06:16:17,592 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] -Dio.netty.tmpdir: /opt/tomcat/apache-tomcat-8.5.32/temp (java.io.tmpdir)\n2023-03-27 06:16:17,592 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] -Dio.netty.bitMode: 64 (sun.arch.data.model)\n2023-03-27 06:16:17,594 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] -Dio.netty.maxDirectMemory: 3633315840 bytes\n2023-03-27 06:16:17,594 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] -Dio.netty.uninitializedArrayAllocationThreshold: -1\n2023-03-27 06:16:17,596 DEBUG i.g.n.s.i.n.u.i.CleanerJava6 [ajp-nio-8009-exec-969] java.nio.ByteBuffer.cleaner(): available\n2023-03-27 06:16:17,596 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] -Dio.netty.noPreferDirect: false\n2023-03-27 06:16:17,648 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] -Dio.netty.native.workdir: /opt/tomcat/apache-tomcat-8.5.32/temp (io.netty.tmpdir)\n2023-03-27 06:16:17,649 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] -Dio.netty.native.deleteLibAfterLoading: true\n2023-03-27 06:16:17,649 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] -Dio.netty.native.tryPatchShadedId: true\n2023-03-27 06:16:17,649 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] -Dio.netty.native.detectNativeLibraryDuplicates: true\n2023-03-27 06:16:17,660 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] Successfully loaded the library /opt/tomcat/apache-tomcat-8.5.32/temp/libio_grpc_netty_shaded_netty_transport_native_epoll_x86_642467445789205395363.so\n2023-03-27 06:16:17,666 DEBUG i.g.n.s.i.n.u.NetUtil [ajp-nio-8009-exec-969] -Djava.net.preferIPv4Stack: false\n2023-03-27 06:16:17,666 DEBUG i.g.n.s.i.n.u.NetUtil [ajp-nio-8009-exec-969] -Djava.net.preferIPv6Addresses: false\n2023-03-27 06:16:17,670 DEBUG i.g.n.s.i.n.u.NetUtilInitializations [ajp-nio-8009-exec-969] Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)\n2023-03-27 06:16:17,672 DEBUG i.g.n.s.i.n.u.NetUtil [ajp-nio-8009-exec-969] /proc/sys/net/core/somaxconn: 128\n2023-03-27 06:16:17,717 DEBUG i.g.n.s.i.n.c.MultithreadEventLoopGroup [ajp-nio-8009-exec-969] -Dio.netty.eventLoopThreads: 8\n2023-03-27 06:16:17,809 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] Successfully loaded the library /opt/tomcat/apache-tomcat-8.5.32/temp/libio_grpc_netty_shaded_netty_tcnative_linux_x86_642968362480758772009.so\n2023-03-27 06:16:17,809 DEBUG i.g.n.s.i.n.u.i.NativeLibraryLoader [ajp-nio-8009-exec-969] Loaded library with name 'netty_tcnative_linux_x86_64'\n2023-03-27 06:16:17,809 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-969] Initialize netty-tcnative using engine: 'default'\n2023-03-27 06:16:17,809 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-969] netty-tcnative using native library: BoringSSL\n2023-03-27 06:16:17,844 DEBUG i.g.n.s.i.n.u.ResourceLeakDetector [ajp-nio-8009-exec-969] -Dio.grpc.netty.shaded.io.netty.leakDetection.level: simple\n2023-03-27 06:16:17,844 DEBUG i.g.n.s.i.n.u.ResourceLeakDetector [ajp-nio-8009-exec-969] -Dio.grpc.netty.shaded.io.netty.leakDetection.targetRecords: 4\n2023-03-27 06:16:17,853 DEBUG i.g.n.s.i.n.b.AbstractByteBuf [ajp-nio-8009-exec-969] -Dio.grpc.netty.shaded.io.netty.buffer.checkAccessible: true\n2023-03-27 06:16:17,853 DEBUG i.g.n.s.i.n.b.AbstractByteBuf [ajp-nio-8009-exec-969] -Dio.grpc.netty.shaded.io.netty.buffer.checkBounds: true\n2023-03-27 06:16:17,855 DEBUG i.g.n.s.i.n.u.ResourceLeakDetectorFactory [ajp-nio-8009-exec-969] Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@55afb3b3\n2023-03-27 06:16:17,873 DEBUG i.g.n.s.i.n.u.i.InternalThreadLocalMap [ajp-nio-8009-exec-969] -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024\n2023-03-27 06:16:17,873 DEBUG i.g.n.s.i.n.u.i.InternalThreadLocalMap [ajp-nio-8009-exec-969] -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096\n2023-03-27 06:16:17,877 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.numHeapArenas: 8\n2023-03-27 06:16:17,877 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.numDirectArenas: 8\n2023-03-27 06:16:17,877 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.pageSize: 8192\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.maxOrder: 9\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.chunkSize: 4194304\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.smallCacheSize: 256\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.normalCacheSize: 64\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.maxCachedBufferCapacity: 32768\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.cacheTrimInterval: 8192\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.cacheTrimIntervalMillis: 0\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.useCacheForAllThreads: false\n2023-03-27 06:16:17,878 DEBUG i.g.n.s.i.n.b.PooledByteBufAllocator [ajp-nio-8009-exec-969] -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023\n2023-03-27 06:16:17,892 DEBUG i.g.n.s.i.n.b.ByteBufUtil [ajp-nio-8009-exec-969] -Dio.netty.allocator.type: pooled\n2023-03-27 06:16:17,892 DEBUG i.g.n.s.i.n.b.ByteBufUtil [ajp-nio-8009-exec-969] -Dio.netty.threadLocalDirectBufferSize: 0\n2023-03-27 06:16:17,893 DEBUG i.g.n.s.i.n.b.ByteBufUtil [ajp-nio-8009-exec-969] -Dio.netty.maxThreadLocalCharBufferSize: 16384\n2023-03-27 06:16:17,899 DEBUG i.g.n.s.i.n.u.ResourceLeakDetectorFactory [ajp-nio-8009-exec-969] Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@5b4be03a\n2023-03-27 06:16:17,913 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-969] -Dio.netty.recycler.maxCapacityPerThread: 4096\n2023-03-27 06:16:17,913 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-969] -Dio.netty.recycler.ratio: 8\n2023-03-27 06:16:17,913 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-969] -Dio.netty.recycler.chunkSize: 32\n2023-03-27 06:16:17,913 DEBUG i.g.n.s.i.n.u.Recycler [ajp-nio-8009-exec-969] -Dio.netty.recycler.blocking: false\n2023-03-27 06:16:17,928 DEBUG i.g.n.s.i.n.u.i.PlatformDependent [ajp-nio-8009-exec-969] org.jctools-core.MpscChunkedArrayQueue: available\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 => ECDHE-ECDSA-AES128-GCM-SHA256\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 => ECDHE-ECDSA-AES128-GCM-SHA256\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 => ECDHE-RSA-AES128-GCM-SHA256\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_128_GCM_SHA256 => ECDHE-RSA-AES128-GCM-SHA256\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 => ECDHE-ECDSA-AES256-GCM-SHA384\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 => ECDHE-ECDSA-AES256-GCM-SHA384\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 => ECDHE-RSA-AES256-GCM-SHA384\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_256_GCM_SHA384 => ECDHE-RSA-AES256-GCM-SHA384\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-ECDSA-CHACHA20-POLY1305\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-ECDSA-CHACHA20-POLY1305\n2023-03-27 06:16:17,947 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-RSA-CHACHA20-POLY1305\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-RSA-CHACHA20-POLY1305\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-PSK-CHACHA20-POLY1305\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-PSK-CHACHA20-POLY1305\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA => ECDHE-ECDSA-AES128-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_128_CBC_SHA => ECDHE-ECDSA-AES128-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA => ECDHE-RSA-AES128-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_128_CBC_SHA => ECDHE-RSA-AES128-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA => ECDHE-PSK-AES128-CBC-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_PSK_WITH_AES_128_CBC_SHA => ECDHE-PSK-AES128-CBC-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA => ECDHE-ECDSA-AES256-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_256_CBC_SHA => ECDHE-ECDSA-AES256-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA => ECDHE-RSA-AES256-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_256_CBC_SHA => ECDHE-RSA-AES256-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA => ECDHE-PSK-AES256-CBC-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_ECDHE_PSK_WITH_AES_256_CBC_SHA => ECDHE-PSK-AES256-CBC-SHA\n2023-03-27 06:16:17,948 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_RSA_WITH_AES_128_GCM_SHA256 => AES128-GCM-SHA256\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_RSA_WITH_AES_128_GCM_SHA256 => AES128-GCM-SHA256\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_RSA_WITH_AES_256_GCM_SHA384 => AES256-GCM-SHA384\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_RSA_WITH_AES_256_GCM_SHA384 => AES256-GCM-SHA384\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_RSA_WITH_AES_128_CBC_SHA => AES128-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_RSA_WITH_AES_128_CBC_SHA => AES128-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_PSK_WITH_AES_128_CBC_SHA => PSK-AES128-CBC-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_PSK_WITH_AES_128_CBC_SHA => PSK-AES128-CBC-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_RSA_WITH_AES_256_CBC_SHA => AES256-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_RSA_WITH_AES_256_CBC_SHA => AES256-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_PSK_WITH_AES_256_CBC_SHA => PSK-AES256-CBC-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_PSK_WITH_AES_256_CBC_SHA => PSK-AES256-CBC-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: TLS_RSA_WITH_3DES_EDE_CBC_SHA => DES-CBC3-SHA\n2023-03-27 06:16:17,949 DEBUG i.g.n.s.i.n.h.s.CipherSuiteConverter [ajp-nio-8009-exec-969] Cipher suite mapping: SSL_RSA_WITH_3DES_EDE_CBC_SHA => DES-CBC3-SHA\n2023-03-27 06:16:17,950 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-969] Supported protocols (OpenSSL): [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2]\n2023-03-27 06:16:17,950 DEBUG i.g.n.s.i.n.h.s.OpenSsl [ajp-nio-8009-exec-969] Default cipher suites (OpenSSL): [TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256]\n2023-03-27 06:16:18,579 DEBUG i.g.n.s.i.n.c.DefaultChannelId [grpc-default-executor-0] -Dio.netty.processId: 3588 (auto-detected)\n2023-03-27 06:16:18,582 DEBUG i.g.n.s.i.n.c.DefaultChannelId [grpc-default-executor-0] -Dio.netty.machineId: 02:24:3d:ff:fe:ae:9c:6a (auto-detected)\n2023-03-27 06:16:18,643 DEBUG i.g.n.s.i.n.u.ResourceLeakDetectorFactory [grpc-default-worker-ELG-1-4] Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@2960b79d\n2023-03-27 06:16:18,716 DEBUG i.g.n.s.i.n.h.s.SslHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] HANDSHAKEN: protocol:TLSv1.2 cipher suite:TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n2023-03-27 06:16:18,724 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND SETTINGS: ack=false settings={ENABLE_PUSH=0, MAX_CONCURRENT_STREAMS=0, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}\n2023-03-27 06:16:18,727 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041\n2023-03-27 06:16:18,734 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND SETTINGS: ack=false settings={MAX_CONCURRENT_STREAMS=100, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=65536}\n2023-03-27 06:16:18,736 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND SETTINGS: ack=true\n2023-03-27 06:16:18,794 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041\n2023-03-27 06:16:18,795 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND SETTINGS: ack=true\n2023-03-27 06:20:18,705 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND GO_AWAY: lastStreamId=2147483647 errorCode=0 length=17 bytes=73657373696f6e5f74696d65645f6f7574\n2023-03-27 06:20:18,719 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND HEADERS: streamId=3 headers=GrpcHttp2OutboundHeaders[:authority: texttospeech.googleapis.com:443, :path: /google.cloud.texttospeech.v1.TextToSpeech/SynthesizeSpeech, :method: POST, :scheme: https, content-type: application/grpc, te: trailers, user-agent: grpc-java-netty/1.53.0, x-goog-api-client: gl-java/1.8.0_191 gapic/2.6.0 gax/2.23.3 grpc/1.50.1, grpc-accept-encoding: gzip, authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImNkZDc5OGViYTVkZDk1NTYwYjc4N2Q2ZmM2Y2ViNWU3ZDZiMjBhYTQiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJodHRwczovL3RleHR0b3NwZWVjaC5nb29nbGVhcGlzLmNvbS9nb29nbGUuY2xvdWQudGV4dHRvc3BlZWNoLnYxLlRleHRUb1NwZWVjaCIsImV4cCI6MTY3OTkwMTM3OCwiaWF0IjoxNjc5ODk3Nzc4LCJpc3MiOiJ0ZXh0LXRvLXNwZWVjaEBjYXJlZnVsLWVuc2lnbi0zODA3MDQuaWFtLmdzZXJ2aWNlYWNjb3VudC5jb20iLCJzdWIiOiJ0ZXh0LXRvLXNwZWVjaEBjYXJlZnVsLWVuc2lnbi0zODA3MDQuaWFtLmdzZXJ2aWNlYWNjb3VudC5jb20ifQ.khEPq-YHHHo9xvU3rYvv4lib18ofR5x1xs5g8Ok94TaVq_XjyuOnC8L4XgBGdh0EpQW48tpv-_x-ilH5L8m4rDomXWjyBFlUkln1TU8etQewxrBn2Mlz4uHibxRirVnl0RsdMiHAa6yHgX39SrOn0PQjNLq3lFE49gwdF2Fplk_ZyJVGCfRDesLa8kynG-oNSl8xJlVeAKt4lAigGdVccVOm3ZPDLPb8KRU7Po25cr0mHMNCrTJZ9U7kKtKIrFJ0lVJitV7OdCjfg6OdZf5Ay9wAPCs_YAaYCe5WBIzVGZh4-EQM-hPYXEBJBrroTOeZR0jaSMt_qQyQFQ2YEehjHg, grpc-timeout: 299455m] streamDependency=0 weight=16 exclusive=false padding=0 endStream=false\n2023-03-27 06:20:18,730 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND PING: ack=false bytes=0\n2023-03-27 06:20:18,730 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND PING: ack=true bytes=0\n2023-03-27 06:20:18,732 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND GO_AWAY: lastStreamId=3 errorCode=0 length=17 bytes=73657373696f6e5f74696d65645f6f7574\n2023-03-27 06:24:18,743 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND HEADERS: streamId=3 headers=GrpcHttp2ResponseHeaders[:status: 200, content-type: application/grpc, grpc-status: 14, grpc-message: 408:Request Timeout, content-length: 0, date: Mon, 27 Mar 2023 06:24:18 GMT, alt-svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000] padding=0 endStream=true\n2023-03-27 06:24:18,753 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND RST_STREAM: streamId=3 errorCode=0\n2023-03-27 06:24:18,754 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] INBOUND PING: ack=false bytes=2\n2023-03-27 06:24:18,754 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 - R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND PING: ack=true bytes=2\n2023-03-27 06:24:18,756 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 ! R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND RST_STREAM: streamId=3 errorCode=8\n2023-03-27 06:24:18,757 DEBUG i.g.n.s.i.g.n.NettyClientHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 ! R:texttospeech.googleapis.com/74.125.193.95:443] OUTBOUND GO_AWAY: lastStreamId=2147483647 errorCode=2 length=0 bytes=\n2023-03-27 06:24:18,759 DEBUG i.g.n.s.i.n.h.c.h.Http2ConnectionHandler [grpc-default-worker-ELG-1-4] [id: 0x13ed7dbe, L:/10.2.0.106:56978 ! R:texttospeech.googleapis.com/74.125.193.95:443] Sending GOAWAY failed: lastStreamId '2147483647', errorCode '2', debugData ''. Forcing shutdown of the connection.\nio.grpc.netty.shaded.io.netty.channel.StacklessClosedChannelException: null\nat io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[grpc-netty-shaded-1.53.0.jar:1.53.0]"
			}
		]
	},
	{
		"Question_title": "Google Cloud Translate API - Does en_IN not support en yet?",
		"Question_created_time": "2023-03-26T23:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Translate-API-Does-en-IN-not-support-en-yet/td-p/537271/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 30,
		"Question_body": "https://translation.googleapis.com/language/translate/v2\n-d '{\"target\":\"en_IN\",\"source\":\"en\",\"format\":\"html\",\"model\":\"base\",\"q\":\"Free Tournament Ticket!\",\"q\":\"Congratulations!\"}'\n\nUpper request return error like below.\n{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Bad language pair: {0}\",\n    \"errors\": [\n      {\n        \"message\": \"Bad language pair: {0}\",\n        \"domain\": \"global\",\n        \"reason\": \"badRequest\"\n      }\n    ]\n  }\n}\n\nBut if source is \"ko\" or \"zh_CN\", it doesn't return error.\nex) https://translation.googleapis.com/language/translate/v2\n-d '{\"target\":\"en_IN\",\"source\":\"ko\",\"format\":\"html\",\"model\":\"base\",\"q\":\"\uc548\ub155!\",\"q\":\"\ucd95\ud558!\"}'\n\nFor now, en_IN is not on the support language list, (https://cloud.google.com/translate/docs/languages?hl=ko).\nShould we wait until en_IN is on the supported language list,\nto be used in \"target\" with other supported language?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-28T07:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "en is supported but en_IN is not currently listed as a supported language for the Google Cloud Translate API. You may need to consider alternative translation services or work with Google Cloud support to request the addition of \"en_IN\" to the list of supported languages."
			}
		]
	},
	{
		"Question_title": "StreamingRecognize api very slow this morning (3/24)?",
		"Question_created_time": "2023-03-24T09:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/StreamingRecognize-api-very-slow-this-morning-3-24/td-p/536621/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 36,
		"Question_body": "Today March 24 between 9-10 AM EST I found that for some reason the streamingrecognize api became very slow.  For instance, after starting a streaming transcribe and saying \"no\" I got no transcripts back for 14 seconds.  I then called WritesDone() on my  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-28T06:54:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Upon checking the service health dashboard, there has been no recent issue. If you're experiencing the same, I would recommend reaching out to Google Cloud support for further assistance.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-28T06:54:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Upon checking the service health dashboard, there has been no recent issue. If you're experiencing the same, I would recommend reaching out to Google Cloud support for further assistance."
			}
		]
	},
	{
		"Question_title": "Observation related to Reponses",
		"Question_created_time": "2023-03-24T00:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Observation-related-to-Reponses/td-p/536378/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 60,
		"Question_body": "We have observed that there is a mismatch in request and response.In Detect language, Unable to detect multiple language still its response type Location is of arrayThese were some observation we found.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-26T15:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The reason why the languages field in the Detect Language API response is an array is because it can detect multiple languages in a given text. The API returns an array of detected languages sorted by their confidence scores. Similarly, the reason why the byteStreamOutputs field in the Translate Document API response is an array is because it can translate multiple documents in a single request. The API returns an array of translated documents in the same order as the input documents."
			},
			{
				"Answer_created_time": "2023-03-26T23:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for reply Joevanie,\n\nSorry but It Doesn't behave as you explain, At this moment detect language API can detect only one language.\n\nSuppose, In Input we have pass two languages but in response it detect one language only as shown in below image.\u00a0\n\nIn same way, In translate document in Request we can pass only once Document.\n\nCould you pls guide on same?"
			}
		]
	},
	{
		"Question_title": "Can we create chatrooms with Dialogflow cx",
		"Question_created_time": "2023-03-22T22:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-we-create-chatrooms-with-Dialogflow-cx/td-p/535972/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "can we create chatrooms with Dialogflow?Different chat window for different user (we want this feature to address issues) user is just example here, we have specific id to the issues(situations) and for that individual situation, client want specific chat window.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-26T15:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can use Dialogflow CX Web Demo to create different chat windows for different users by using the session ID parameter in the URL. You can also integrate Dialogflow with Google Chat to create chat rooms."
			}
		]
	},
	{
		"Question_title": "dtml dialogflow cx twilio",
		"Question_created_time": "2023-03-21T08:28:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/dtml-dialogflow-cx-twilio/td-p/535148/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 63,
		"Question_body": "  https://groups.google.com/g/dialogflow-cx-edition-users/c/y3EWGi9DQJ0Good morning,I'm trying to set up a scenario with a dtml on a phone number provided by Twilio.\nThe chat bot works but when I try on the phone it does not take into account the keys on the other hand if I say verbally it works.\nI followed the procedure as the link above but it still does not work\nTwilio tells me it's from Diallogflow cx but I can't see what's wrong.\nPlease tell me if you have some time to tell me what's wrongBest.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-26T15:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you are using Dialogflow CX with Twilio, you can provide the configuration information requested by Twilio after clicking the Connect button for the Twilio integration. This documentation might be helpful."
			}
		]
	},
	{
		"Question_title": "Vertex AI job run failure",
		"Question_created_time": "2023-03-21T06:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-job-run-failure/td-p/535079/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "We are streamlining our models workflow and as part of that I am building a pipeline for the ease of usability, I am using a custom container image for my component but when the job runs the component fails and returns an error thats its worked has exited, im unable to find the error since the logs dont specify what error is there exactly, I have verified running the component running it on a virtual machine however vertex ai does not work. Can ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-26T15:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "what did the logs explorer at least say?"
			}
		]
	},
	{
		"Question_title": "GCP Speech to Text Nodejs with GCP Cloud Storage Example",
		"Question_created_time": "2023-03-25T07:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GCP-Speech-to-Text-Nodejs-with-GCP-Cloud-Storage-Example/td-p/536941/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 1,
		"Question_view_count": 45,
		"Question_body": "Just wanted to share this simple code to convert an audio file to text using just GCP Speech to text API and Google Cloud Storage.  It takes in a .flac file and outputs a text file.  You need to upload the .flac file to the Google Cloud first.  Here is the GitHub Repo.  Feel free to try it.  You will need to create the service account key with access to the Speech-to-Text API and Google Cloud Storage (in this case the files on GCP are set to public\ud83d\ude1ehttps://github.com/jameslangdon1/GCP-Speech-to-Text-with-Storage-Bucket-public ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-25T07:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Just wanted to share this simple code to convert an audio file to text using just GCP Speech to text API and Google Cloud Storage.\u00a0 It takes in a .flac file and outputs a text file.\u00a0 You need to upload the .flac file to the Google Cloud first.\u00a0 Here is the GitHub Repo.\u00a0 Feel free to try it.\u00a0 You will need to create the\u00a0service account key with access to the Speech-to-Text API and Google Cloud Storage (in this case the files on GCP are set to public\ud83d\ude1e\n\nhttps://github.com/jameslangdon1/GCP-Speech-to-Text-with-Storage-Bucket-public"
			}
		]
	},
	{
		"Question_title": "Issue Link between Vertex AI and Appsheet",
		"Question_created_time": "2023-03-19T20:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Issue-Link-between-Vertex-AI-and-Appsheet/td-p/534403/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 85,
		"Question_body": "Hello,I am working on quantity counting by combining appsheet and vertex AI.\nI have a json file link at GCP, deploy model of vertex AI, but when I go to the appsheet, it can't count the number and can't be identified. Where am I doing wrong. Please help me.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-21T08:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you please provide more details about the problem you are facing? What is the error message you are seeing? What is the expected output?"
			},
			{
				"Answer_created_time": "2023-03-21T18:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dear,\n\nIt doesn't give me an error so I don't know where the problem is. I linked using the json file API but it doesn't work.\n\nI want when I deploy the dataset vertex ai, the appsheet side will receive that dataset and it will recognize the image\n\nThank"
			},
			{
				"Answer_created_time": "2023-03-23T09:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I suggest you consider reaching out to the support teams of AppSheet and Vertex AI for further assistance with troubleshooting the issue."
			}
		]
	},
	{
		"Question_title": "AIML",
		"Question_created_time": "2022-12-20T04:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AIML/td-p/501497/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 82,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-22T09:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@aakash1710,\u00a0welcome to the community! Check out these Machine Learning Crash Courses.\n\nI hope this helps!"
			},
			{
				"Answer_created_time": "2023-03-23T01:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Link not accessible......."
			},
			{
				"Answer_created_time": "2023-03-23T07:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Try this. Google Crash course on Machine Learning.\n\nMachine Learning Crash Course"
			}
		]
	},
	{
		"Question_title": "Can't use medical model in STT version 2?",
		"Question_created_time": "2023-03-16T23:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-t-use-medical-model-in-STT-version-2/td-p/533595/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 66,
		"Question_body": "An error occurs if auto dictation is required.\nAre you still not supported?",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you please send the error details in text? Also, are you referring to automatic punctuation?"
			}
		]
	},
	{
		"Question_title": "Idle shutdown for user-managed notebook (vertex-AI)",
		"Question_created_time": "2022-06-02T07:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Idle-shutdown-for-user-managed-notebook-vertex-AI/td-p/428171/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 720,
		"Question_body": "There are two types of notebooks in Vertex-AI1) managed notebook: https://cloud.google.com/vertex-ai/docs/workbench/managed/introduction2) user-managed notebook: https://cloud.google.com/vertex-ai/docs/workbench/user-managed/introductionI see that the former has a useful function called \"idle shutdown\" that help manage costs: managed notebooks instances shut down after being idle for a specific time period by default.Why we didn't make it available for user-managed notebook as well? Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-02T22:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the feedback here. We are aware of the request and this is further to prioritize this work. Happy to get back when we have a concrete plan for this feature."
			},
			{
				"Answer_created_time": "2023-03-20T23:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Unfortunately, as per the update here [1], this feature won't be added to user-managed notebooks.\n\n[1]\u00a0https://issuetracker.google.com/issues/237305892"
			}
		]
	},
	{
		"Question_title": "Dialogflow ES - Chat Window Content Customization to fix scroll issue",
		"Question_created_time": "2023-03-15T14:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-ES-Chat-Window-Content-Customization-to-fix-scroll/td-p/533030/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 73,
		"Question_body": "Hello,I'm working on a Dialogflow ES agent where the bot has long responses in my use cases to where the beginning of most responses get hidden under the titlebar in the header of the chot window.  The user would have to scroll up to see the start of the message which is not an ideal UX. A thought to fix this issue is show one paragraph at a time in the response by using a delay and maybe showing three dots like someone is typing so a user would have time to read the response and has some kind of signal that there's more to the response vs dialogflow showing all the response at once.  How would I go about setting that up either through configurations and/or scripts? The HTML is something like:<div id=\"messageList\">\n    <div class=\"message\">Paragraph 1</div>\n    <div class=\"message\">Paragraph 2</div>\n   <div class=\"message\">Paragraph 3</div>\n   <div class=\"message\">Paragraph 4</div>\n</div>Thanks! ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-17T04:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can use JavaScript to show one paragraph at a time in the response by using a delay and maybe showing three dots like someone is typing so a user would have time to read the response and has some kind of signal that there\u2019s more to the response. Here\u2019s an example of how you can do this:\n\n<div id=\"messageList\">\n   <div class=\"message\">Paragraph 1</div>\n   <div class=\"message\">Paragraph 2</div>\n   <div class=\"message\">Paragraph 3</div>\n   <div class=\"message\">Paragraph 4</div>\n</div>\nconst messageList = document.getElementById('messageList');\nconst messages = messageList.querySelectorAll('.message');\nlet i = 0;\nconst delay = 2000; // 2 seconds\nconst dots = '...';\nfunction showNextMessage() {\nif (i < messages.length) {\nconst message = messages[i];\nmessage.style.display = 'block';\ni++;\nsetTimeout(showNextMessage, delay);\n} else {\n// All messages have been shown\n}\n}\nshowNextMessage();"
			},
			{
				"Answer_created_time": "2023-03-20T09:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for this possible solution.\u00a0 How I would I be able to execute this script (or similar) on every chatbot response that gets added dynamically through the the lifetime of the chatbot conversation session?\u00a0 Can you point me to documentation on how this can be done?\n\nThanks"
			}
		]
	},
	{
		"Question_title": "Having Issues with authentication using REST API",
		"Question_created_time": "2023-03-16T15:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Having-Issues-with-authentication-using-REST-API/td-p/533517/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 135,
		"Question_body": "Hello,I'm building a rust application that needs to use cloud vision ocr. I use service account key JSON file to authenticate, and I had no problems with it using the Python and Go libraries. I wasn't able to understand how to use this authentication method in the rust library, so I decided to use the POST request method. To do such thing I need to somehow include my key in the request header, and I have not been able to do it. My code and the response I get are below.```  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-19T15:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "is this the documentation you followed?"
			}
		]
	},
	{
		"Question_title": "Multiple Neural2 Speakers with text-to-speech error.",
		"Question_created_time": "2023-03-16T10:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Multiple-Neural2-Speakers-with-text-to-speech-error/td-p/533408/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 110,
		"Question_body": "Hello,I'm trying to use the text-to-speech API to generate a multi speaker audio file. When I use the older wavenet voices it works just fine. But when I replace the speakers with the newer Neural2 models I get a 400 error saying: How can I get this to work for multiple speakers using the newer models?Here is a sample:\n<speak>\n<voice name=\"en-US-Neural2-J\">\n<p>Hello, everyone! Welcome to today's podcast. I'm your host A, and joining me is my co-host, B.</p>\n</voice>\n<voice name=\"en-US-Neural2-I\">\n<p>Hi, A! It's great to be here. Today, we're going to discuss an interesting topic that's been making headlines recently.</p>\n</voice>\n<voice name=\"en-US-Neural2-J\">\n<p>That's right, B. We're talking about the collapse of Silicon Valley Bank, which was triggered by a massive online bank run.</p>\n</voice>\n<voice name=\"en-US-Neural2-I\">\n<p>Indeed, A. This bank run was unlike any other we've seen before, as it was primarily fueled by social media platforms and private chat groups.</p>\n</voice>\n</speak> This sample works when I replace en-US-Neural2 speakers with en-US-Wavenet.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-19T14:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Possible workarounds would be, converting the text into <500 bytes or sending the request into smaller pieces. This might be a related case"
			}
		]
	},
	{
		"Question_title": "A brief introduction to Convolutional Neural Network with Tensorflow",
		"Question_created_time": "2023-03-18T12:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/A-brief-introduction-to-Convolutional-Neural-Network-with/td-p/534073/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 25,
		"Question_body": "Dear Learners' Community,I hope you're having a good day. I've created a notebook on Kaggle to simplify CNN with tf. You can review it here:https://www.kaggle.com/code/eslamfouad/introduction-to-convolutions-with-tensorflow?scriptVersionId=...Regards.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-18T12:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dear Learners' Community,\n\nI hope you're having a good day. I've created a notebook on Kaggle to simplify CNN with tf. You can review it here:\n\nhttps://www.kaggle.com/code/eslamfouad/introduction-to-convolutions-with-tensorflow?scriptVersionId=...\n\nRegards."
			}
		]
	},
	{
		"Question_title": "Enconding $session.params.[parameter] on Dialog Flow CX ERROR: ByteString is not valid UTF8",
		"Question_created_time": "2023-03-15T17:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Enconding-session-params-parameter-on-Dialog-Flow-CX-ERROR/td-p/533056/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 116,
		"Question_body": "I am having a problem to display a message of type $session.params.[parameter] on DialogFlow CXI am reading a json file from Google Bucket and when the Json is with no special characters it works fine example Convencao.But when I put some special character example Conven\u00e7\u00e3o it not working giving me the error below:},This is the .json that workThis is the .json that not workThis is my java codeThis is my return on Google Cloud Function LogWhen it passes Conven\u00e7\u00e3o I receive the error above when it is Convencao it displays the message with success.I tried some enconding UTF-8 ways without suceessI tried some solutions belowMy return must beChoose the event:but in it is Conven\u00e7\u00e3o I receivedChoose the event:but in it is Convencao I receivedChoose the event:No one of them workedI do not know more what I could try to do to solve this errorCould you help me please?Tried to use some enconding methods that I know",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-17T05:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I would suggest that you try to force the encoding of the JSON file to UTF-8 by adding the following header to your JSON file: {'Content-Type' : 'application/json; charset=UTF-8'}\n\nThis SO post might be helpful."
			}
		]
	},
	{
		"Question_title": "Title: Dialogflow ES not matching the correct intent even with the exact training phrase",
		"Question_created_time": "2023-03-15T03:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Title-Dialogflow-ES-not-matching-the-correct-intent-even-with/td-p/532741/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 81,
		"Question_body": "I'm facing an issue with Dialogflow ES where it's not matching the correct intent even when I use the exact training phrase that I have entered as a user expression. Instead, it keeps matching the fallback intent or the __system_counters__ context.I have tried the following troubleshooting steps:Despite trying these steps, the problem persists. When using the \"Try it now\" feature in the Dialogflow console and inputting the exact training phrase (even with the same entity reference), the correct intent is still not being matched.Any help or guidance on how to resolve this issue would be greatly appreciated.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-17T02:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can try disabling the ML (Machine Learning) option in the intent settings. This will cause the intent to follow a rule-based grammar matching algorithm, which means it will only match user expressions with the exact training phrases defined in the intent."
			}
		]
	},
	{
		"Question_title": "created a project on Google Doc AI ,exported its dataset and facing problem.",
		"Question_created_time": "2023-03-13T23:37:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/created-a-project-on-Google-Doc-AI-exported-its-dataset-and/td-p/532118/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 57,
		"Question_body": "i created a doc processing project on Google Doc AI, there i labelled docs and ran pipeline,then i exported the dataset to use it in another processor ,i imported the dataset,i right hand side of GUI its showing all labelled doc counts, number of entities we labelled in all docs but it is not showing anything in right of gui where we get option to run the pipeline ,i will attach image for reference.    ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-16T06:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you please clarify what you mean by \u201cnot showing anything in right of GUI where we get option to run the pipeline\u201d? Are you saying that the pipeline is not running or that you cannot see the option to run the pipeline?"
			}
		]
	},
	{
		"Question_title": "Deployed model does not support explanation.",
		"Question_created_time": "2023-02-24T08:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Deployed-model-does-not-support-explanation/td-p/525932/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 180,
		"Question_body": "I have deployed my model and tried to test it, but I received the error message:\"Deployed model XXX does not support explanation.\" Can anyone help? Thanks very much",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-27T14:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nYour question seems to be vague. Can you elaborate and state exactly what GCP service you are using? Also, if it is not confidential, may I know\u00a0 the model you are trying to deploy?\n\nThank you!"
			},
			{
				"Answer_created_time": "2023-03-16T04:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Paola, thanks for coming back to and sorry for the late reply! I am trying to use Vertex AI and I try to deploy the model I created: \"SprecherZeitPunkt\".\u00a0 Do you need any furhter info? Thank you!"
			}
		]
	},
	{
		"Question_title": "SSML mark timepointing (v1beta1) suddenly only returns the timepoints until first period",
		"Question_created_time": "2023-03-06T11:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/SSML-mark-timepointing-v1beta1-suddenly-only-returns-the/td-p/529358/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 87,
		"Question_body": "I'm loving GCP's Text to Speech API! I have a live product (used by thousands of users every day) that relies on the TTS API v1beta1 (Method: text.synthesize  |  Cloud Text-to-Speech Documentation  |  Google Cloud).  I've gotten a lot of bug reports over the past few days and traced it to a change in behavior in the `text.synthesize` method (v1beta1), when `enableTimePointing: [\"SSML_MARK\"]`.Before: `text.synthesize` with `enableTimePointing: [\"SSML_MARK\"]` would return an object in `timepoints` with a `markName=i` and `timeSeconds` for each `<mark name = i>` in the input SSML.Now: `text.synthesize` with `enableTimePointing: [\"SSML_MARK\"]`'s response `timepoints` object only contains timepoints for a fraction of  all the `<mark name = i>` in the input SSML. For example:1) Use the API explorer at Method: text.synthesize  |  Cloud Text-to-Speech Documentation  |  Google Cloud2) Set the request body to:{\n\"enableTimePointing\": [\n\"SSML_MARK\"\n],\n\"input\": {\n\"ssml\": \"<speak><prosody><mark name=\\\"0\\\"/>I <mark name=\\\"1\\\"/>am <mark name=\\\"2\\\"/>my <mark name=\\\"3\\\"/>aunt's <mark name=\\\"4\\\"/>sister's <mark name=\\\"5\\\"/>daughter. <mark name=\\\"6\\\"/>He <mark name=\\\"7\\\"/>was <mark name=\\\"8\\\"/>sure <mark name=\\\"9\\\"/>the <mark name=\\\"10\\\"/>Devil <mark name=\\\"11\\\"/>created <mark name=\\\"12\\\"/>red <mark name=\\\"13\\\"/>sparkly <mark name=\\\"14\\\"/>glitter.</prosody></speak>\"\n},\n\"voice\": {\n\"name\": \"en-US-Standard-A\",\n\"languageCode\": \"en-US\"\n},\n\"audioConfig\": {\n\"audioEncoding\": \"MP3\"\n}\n}3) See how the `timepoints` object in the response only has timepoints for mark names \"0\" through \"5\". The actual number of mark tags in the input SSML is fifteen (largest mark tag being \"14\").  4. In the input SSML, remove the period at the end of \"<mark name=\\\"5\\\"/>daughter.\", execute, and notice how the `timepoints` object in the response now has timepoints for all the mark names \"0\" through \"14\".Can someone confirm that this is unexpected behavior? And if it's being worked on, roughly when can we expect a fix? Thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-07T10:49:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I used your request body and it was able to get to 14. Are you still getting \"fractions\" of timepoints now?\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-07T10:49:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I used your request body and it was able to get to 14. Are you still getting \"fractions\" of timepoints now?"
			},
			{
				"Answer_created_time": "2023-03-15T08:18:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for responding. I do see the expected behavior now with the response returning all the timepoints marked with <mark>. This seems to have been an intermittent issue with the Cloud Text-to-speech API.\n\nJust curious, is there a timeline for when the v1beta1 API will graduate out of beta, so we can expect a more stable service?\n\nThank you again for your help!"
			}
		]
	},
	{
		"Question_title": "Does custom processor supports indian regional languages",
		"Question_created_time": "2023-03-09T22:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Does-custom-processor-supports-indian-regional-languages/td-p/530869/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 49,
		"Question_body": "We are creating a custom document data extractor in document AI for electricity bills. Some of the electricity bills are in indian regional languages and when we are trying to tag values in regional language it automatically translates them into english (often translated wrong) automatically. We would like to know whether custom processors supports regional indian languages like Malayalam , Kannada, Punjabi, Gujarati, Hindi, Tamil, Telugu, etc or not. If it does not support these language's , then is there is any workaround for this? We are referring to this link for languages it supports: https://cloud.google.com/document-ai/docs/languages",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-12T20:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The link you provided lists the languages that are supported by the custom processor feature as of now. Unfortunately, none of the Indian regional languages that you mentioned are supported by this feature yet. You may have to look for other solutions. Check out this interesting article."
			}
		]
	},
	{
		"Question_title": "Error 403 in Vertex Ai Workbench",
		"Question_created_time": "2023-03-08T02:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-403-in-Vertex-Ai-Workbench/td-p/530061/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 154,
		"Question_body": "I am facing an issue where if I click on the \"OPEN JUPITER LAB\" link on my instance it directs me to a Error 403 page with the text: \"403. That\u2019s an error. That\u2019s all we know.\"What access am I missing?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-12T19:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Make sure you have the necessary IAM roles and permissions to access the instance and open JupyterLab. Check if the Vertex AI administrator role quickly fixes your problem. You may also follow this documentation."
			}
		]
	},
	{
		"Question_title": "Google Transliteration API has stopped working",
		"Question_created_time": "2023-03-11T02:37:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Transliteration-API-has-stopped-working/td-p/531290/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 40,
		"Question_body": "Google Transliteration API was deprecated long time ago. Yet, the api link for the same worked for me till recent time. This API now generates an error response and I'm not able to find any solution to it. Much grateful to the community for shedding some light on this problem. Regards. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-11T02:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Google Transliteration API was deprecated long time ago. Yet, the api link for the same worked for me till recent time. This API now generates an error response and I'm not able to find any solution to it.\u00a0\n\nMuch grateful to the community for shedding some light on this problem.\u00a0\n\nRegards."
			}
		]
	},
	{
		"Question_title": "Cloud Vision issue: Incorrect end time for MP4 videos",
		"Question_created_time": "2023-02-24T00:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Vision-issue-Incorrect-end-time-for-MP4-videos/td-p/525828/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 74,
		"Question_body": "Hi!\nI encountered a weird issue with Cloud Vision.\nWhen processing MP4 videos that were recorded on iPhone (Safari) with 5 fps, the endTimeOffset is incorrectly computed.For example, processing a 5 seconds MP4 with a frame-rate of 5fps would yield the following start/end time:  This is clearly incorrect.\nIs this a known issue?\nIs there a way to resolve this?\n\nThanks.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-28T05:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "May I know the documentation that you are currently following?"
			},
			{
				"Answer_created_time": "2023-03-08T08:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\nThanks for replying and sorry for the late reply. (the notifications for this forum went to my spam folder for some reason)\n\nThough it was some time ago since I implemented this code section, I believe I relied partly on the following:\nhttps://cloud.google.com/vision/docs/detect-labels-image-client-libraries\nhttps://cloud.google.com/vision/docs/labels\nAnd also on the following reference:\u00a0\n\nhttps://googleapis.github.io/googleapis/java/all/latest/apidocs/com/google/cloud/videointelligence/v1beta2/AnnotateVideoRequest.Builder.html#setInputUri-java.lang.String-\n\nTo be honest, it was a bit challenging finding the documentation.\n\nAnyway, I have opened a bug on this issue and it contains some additional info on the exact scenario. Here's the issue:\nhttps://issuetracker.google.com/issues/271896083\n\nIf there is some known solution for this issue then I would be happy to know.\nThanks!"
			},
			{
				"Answer_created_time": "2023-03-09T16:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nWhile we wait for the response of Cloud Vision Engineering Team, please try Video Intelligence API Quickstart. It is more appropriate when detecting labels in videos. This service is free for the first 1000 minutes and\u00a0$0.10 / minute after exceeding.\n\nThanks!"
			}
		]
	},
	{
		"Question_title": "Rate Limiting on Vertex AI",
		"Question_created_time": "2023-03-09T15:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Rate-Limiting-on-Vertex-AI/td-p/530748/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 53,
		"Question_body": "I'm preparing to launch a prod endpoint using vertex ai automl text solution. I want to understand more prelaunch how many requests/second my endpoint will handle.  I see that https://cloud.google.com/vertex-ai/docs/quotas this implies I can run 30,000/minute. Is that the limit or will things break down before then? Looking for some best practices. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-09T15:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm preparing to launch a prod endpoint using vertex ai automl text solution. I want to understand more prelaunch how many requests/second my endpoint will handle.\u00a0\n\n\u00a0\n\nI see that\u00a0https://cloud.google.com/vertex-ai/docs/quotas\u00a0this implies I can run 30,000/minute. Is that the limit or will things break down before then? Looking for some best practices."
			}
		]
	},
	{
		"Question_title": "GPU shortage on europe-west1",
		"Question_created_time": "2023-03-07T09:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GPU-shortage-on-europe-west1/td-p/529808/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 149,
		"Question_body": "Hi everyone,I am trying to launch a Vertex AI CustomJob Training on europe-west1 using a T4 GPU.It's been two days I keep receiving a \"Insufficient Ressources\" denial and after like 50 trials, I wonder if I am the only one experimenting this issue. Does anyone managed to trigger a training job on T4 ?I've checked and my quotas are ok. Thanks for the feedback",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-08T10:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I was able to do training on T4. Also, per the service health dashboard, there was an issue last March 3 but it was specifically for\u00a0us-central1, us-east1, and europe-west3, and it was resolved a day after. Running also on cloudshell the command below shows T4 is available in europe-west1-b/c/d\n\ngcloud compute accelerator-types list"
			},
			{
				"Answer_created_time": "2023-03-08T15:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Joevanie,\n\nThanks for your input.\u00a0\nYou say\u00a0 you managed to 'gcloud ai custom-jobs create --region=europe-west1 ...' with a T4 accelerator recently ?\n\nI've just re(re)tried and it keeps on erroring with 'Resources are insufficient in region: europe-west1. Please try a different region. If you use K80, please consider using P100 or V100 instead.'\n\nAlso, I believe\u00a0\n\ngcloud compute accelerator-types list\n\nlists the theorically available accelerators per region, but it does not realtime check for effective availability. I could not find any place nor tool to have this realtime availability check.\n\nI've read here and there that when a region gets shorts on GPU, the most valuable customers get prioritized by Google. As I am not yet spending a lot on GCP, I guess this could explain why I can't get a GPU. But as long as I can't train my models, I have no way to spend my budget on GCP neither. Hope this prioritize the good customer is only a rumor."
			},
			{
				"Answer_created_time": "2023-03-09T09:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@LawrenceAlgocat\u00a0wrote:\n\n\nYou say\u00a0 you managed to 'gcloud ai custom-jobs create --region=europe-west1 ...' with a T4 accelerator recently ?\n\n\u00a0\n\nI did it using the console. Can I ask what documentation did you follow? You may want to check this out. Also, have you tried choosing a different region/gpu just to test?"
			},
			{
				"Answer_created_time": "2023-03-09T14:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Joevanie,\n\nI followed the documentation you linked as well as the CLI docs.\n\nThe job works great when ran without accelerator.\n\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-8\n  replicaCount: 1\n  containerSpec:\n    [...]\n\nAs soon as I request a GPU with accelerator-type and accelerator-count it fails with \"Insufficient ressources\" error.\n\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-standard-8\n    acceleratorType: NVIDIA_TESLA_T4\n    acceleratorCount: 1\n  replicaCount: 1\n  containerSpec:\n    [...]\n\nAs of now, I only have quotas available for \" Custom model training Nvidia T4 GPUs per region\" on europe-west1. All other regions are 0 (see screenshot). I've asked to raise the same quota on all european regions, but still waiting for an answer from GCP team. So I can't test yet elsewhere."
			}
		]
	},
	{
		"Question_title": "GCP Forecast horizon without literal value",
		"Question_created_time": "2023-02-03T07:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GCP-Forecast-horizon-without-literal-value/td-p/517704/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 103,
		"Question_body": "Is there a way to set horizon value with ML.FORECAST without using literal valueI get the following error: Query error: Invalid table-valued function ML.FORECAST Table Valued Function expects the settings struct to have literal constant",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T19:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What documentation are you currently following?"
			},
			{
				"Answer_created_time": "2023-03-09T13:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Are there any updates on this?\n\nGetting the same error when declaring a INT64 variable and using it in the settings struct of ML.EXPLAIN_FORECAST."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX environment specific webhook timeout is not considered",
		"Question_created_time": "2023-03-07T01:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-environment-specific-webhook-timeout-is-not/td-p/529553/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 172,
		"Question_body": "Hello,I am running into an issue with timeout value for environment specific webhooks in Dialogflow CX.We have extended the timeout value to 15 seconds for the webhook and we have multiple environment specific webhook URLs set as well.  You can see precise setup as part of this issue posted by another user facing the exact same issue 3 months back : https://issuetracker.google.com/issues/261683010It seems like the webhook timeout value is not being considered for environment specific URLs.  I see it does work for the agent level URL which is not what I would like to use as we have for obvious reasons different URL for non-prod and production apps.Is there any workaround or possible solution to this problem?Cheers, jags",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-09T07:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Found a suitable workaround:\n\nUpdate the default webhook URL to production with timeout 15 seconds.\u00a0 Remove the environment specific webhook URL.\nRelease a version of the flow and use it with the production environment.\nUpdate the default webhook URL back to test endpoint with timeout 15 seconds.\nRepeat the steps before releasing the new version for production usage.\n\nWhen you release the version of a flow, Dialogflow CX would create the snapshot of webhook configuration as well along with flow.\u00a0 And when you update the default webhook URL again, it would only update for unpublished/draft version and it does not affect the webhook URL in the released version of the flow.\u00a0 It's verified and working approach!\n\nCheers, jags\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-07T01:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "If this is the case,\n\nMy suggestions is going to be to export your current agent, import this agent into a newer one, change the global webhook URL and set the 15s timeout.\n\nI know it is a little bit tricky but it could help you.\n\nAlso, you can automate all these things with the Dialogflow CX CLI:\u00a0\n\nhttps://cxcli.xavidop.me/cmd/cxcli/"
			},
			{
				"Answer_created_time": "2023-03-08T00:46:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@xavidop\u00a0, I understand the solution.\u00a0 It could be a temporary workaround as the whole purpose of environments is no longer useful with this setup."
			},
			{
				"Answer_created_time": "2023-03-08T00:52:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Exactly, a little bit hacky until a solution is implemented"
			},
			{
				"Answer_created_time": "2023-03-09T07:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Found a suitable workaround:\n\nUpdate the default webhook URL to production with timeout 15 seconds.\u00a0 Remove the environment specific webhook URL.\nRelease a version of the flow and use it with the production environment.\nUpdate the default webhook URL back to test endpoint with timeout 15 seconds.\nRepeat the steps before releasing the new version for production usage.\n\nWhen you release the version of a flow, Dialogflow CX would create the snapshot of webhook configuration as well along with flow.\u00a0 And when you update the default webhook URL again, it would only update for unpublished/draft version and it does not affect the webhook URL in the released version of the flow.\u00a0 It's verified and working approach!\n\nCheers, jags"
			}
		]
	},
	{
		"Question_title": "Does Dialog flow (ES or CX) have the Capability?",
		"Question_created_time": "2023-02-01T21:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Does-Dialog-flow-ES-or-CX-have-the-Capability/td-p/516750/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 105,
		"Question_body": "Dear Team, before proceeding with Dialogflow , we need to confirm if we can achieve below requirements with Google dialogflow1. Passing values to the Bot: When Bot is initialized on webpage, we want to send some values to the Bot directly (like passing values from backend), and the Bot should not need to take inputs from the user. Can we do this here with any workaround?2. Show Values in Tabular Format: Bot should display values received from API in the tabular format (or any other presentable format). Need guidance here.3. Load existing chat history: Whenever user opens the Bot, it should load existing chat history in the chat window. Can we do it with this?4. Send Notifications to Bot: We need to send confirmation notifications to the bot, we are connecting to the other services If we get initial heads up, we can spend time on exploration on this type of customization.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-02T17:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It is not possible to pass values to the chatbot without user inputs since its job is to respond to user inputs, but you can create custom events by invoking events with API or with fulfillment.\n\nIn your second requirement, you can look up this documentation.\n\nRegarding the loading of existing chat history, it is only the conversation history tool where you can view the actual conversation of your agent and the end-user. You will have to add dialogflow.conversations permission to use this tool.\n\nThe fourth requirement was a little unclear in regards to \"other services\" and I want to distinguish if it is about the integrations with conversation platforms. You can go through Google Assistant's Push Notification (Dialogflow) documentation."
			},
			{
				"Answer_created_time": "2023-03-09T04:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Poala,\n\nother services as in for ex. Stackstorm , can we call any dialog flow endpoint, because there are scenarios where few operations takes time to process and then can send the response later, in this case if we want to notify the bot, what can be done?\n\nanother thing is can we create chatrooms with Dialogflow?\n\nDifferent chat window for different chat different user (we want this feature to address issues) user is just example here, we have specific if to the issues(situations) and for that individual situation client want specific chat window."
			}
		]
	},
	{
		"Question_title": "Cannot remove user events in Retail API",
		"Question_created_time": "2023-03-08T22:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cannot-remove-user-events-in-Retail-API/td-p/530421/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 1,
		"Question_view_count": 94,
		"Question_body": "I've imported user events for testing but I want to clear all user events and re-import new user events for another testing. I've followed the instruction to remove user events (Link). After waiting for 24hrs, operation status is completed.This is a POST request to delete user events between 2023-03-01 to 2023-03-07This is a response of operation statusI expect to see 0 events from Retail console in GCP but the number of events ingested still existed (nothing changed)How can I remove user events under same project without deleting the entire project? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-08T22:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I've imported user events for testing but I want to clear all user events and re-import new user events for another testing. I've followed the instruction to remove user events (Link). After waiting for 24hrs, operation status is completed.\n\nThis is a POST request to delete user events between 2023-03-01 to 2023-03-07\n\nRESPONSE=$(curl -X POST \\\n  -H \"Authorization: Bearer \"${ACCESS_TOKEN}\"\" \\\n  -H \"Content-Type: application/json; charset=utf-8\" \\\n  --data '{\n  \t\"filter\":\"eventTime > \\\"2023-03-01T00:00:01.511Z\\\" eventTime < \\\"2023-03-07T00:00:01.511Z\\\"\",\n  }' \\\n  \"https://retail.googleapis.com/v2/projects/${PROJECT_ID}/locations/global/catalogs/default_catalog/userEvents:purge\"\n)\necho $RESPONSE\n\nThis is a response of operation status\n\n{\n  \"name\": \"projects/{PROJECT_ID}/locations/global/catalogs/default_catalog/operations/purge-user-events-XXXXXXXXXX\",\n  \"done\": true,\n  \"response\": {\n    \"@type\": \"type.googleapis.com/google.cloud.retail.v2.PurgeUserEventsResponse\",\n    \"purgedEventsCount\": \"6\"\n  }\n}\n\nI expect to see 0 events from Retail console in GCP but the number of events ingested still existed (nothing changed)\n\nHow can I remove user events under same project without deleting the entire project?"
			}
		]
	},
	{
		"Question_title": "Bad Gateway error when using speech_v1 in a Django application that is running on nginx and waitress",
		"Question_created_time": "2023-03-08T11:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Bad-Gateway-error-when-using-speech-v1-in-a-Django-application/td-p/530239/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 37,
		"Question_body": "Hi everyone,I've been working on a Django project using Google's speech-to-text api. It works fine on my local as well as a standalone application. However, when I deploy it to the production that has nginx working with waitress, it doesn't work and gives Bad Gateway error and the nginx server stops running.This is where it happens.Here is my nginx-waitress configuration:I tried with that $host and $remote_addr lines, still no luck.Here is the credentials json file that is working fine in both local django server and standalone application.Is there any other configurations that I need to do in the nginx server side? Or anything else? I would highly appreciate your help.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-08T11:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi everyone,\n\nI've been working on a Django project using Google's speech-to-text api. It works fine on my local as well as a standalone application. However, when I deploy it to the production that has nginx working with waitress, it doesn't work and gives Bad Gateway error and the nginx server stops running.\n\nThis is where it happens.\n\nfrom google.cloud import speech_v1\n....\n....\nclient = speech_v1.SpeechClient()\n\nHere is my nginx-waitress configuration:\n\n    #Finally, send all non-media requests to the Django server.\n    location / { \n    proxy_pass http://localhost:8080;\n    #proxy_set_header Host $host;\n    #proxy_set_header X-Forwarded-For $remote_addr;\n    }\n\nI tried with that $host and $remote_addr lines, still no luck.\n\nHere is the credentials json file that is working fine in both local django server and standalone application.\n\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"my_project_id\",\n  \"private_key_id\": \"my_private_key_id\",\n  \"private_key\": \"my_private_key\",\n  \"client_email\": \"my_client_email\",\n  \"client_id\": \"my_client_id\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/speechtoblog-service%40yt2blog-377413.iam.gserviceaccount.com\"\n}\n\nIs there any other configurations that I need to do in the nginx server side? Or anything else? I would highly appreciate your help."
			}
		]
	},
	{
		"Question_title": "\"Internal error encountered\" \"code: 13\"",
		"Question_created_time": "2022-12-26T12:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/quot-Internal-error-encountered-quot-quot-code-13-quot/td-p/503519/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 13,
		"Question_score_count": 1,
		"Question_view_count": 802,
		"Question_body": "Hello friends, I would like to ask you a question.\nI am working with DocumentIA, and after uploading the documents and tagging them, when training the model, I got the following error:error\ncode: 13\nmessage: \"Internal error encountered,\ndetails: []As you can see there are no details. I didn't find anything in the documentation either.Can anyone tell me what that means and how to fix it.Thank you very much!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-27T13:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It will be hard on my end to reproduce since I don't have access on your project and there are no additional details in your error. I suggest to have a 1:1 Support. An engineer that has access to your project and will be dedicated on your case can investigate more thoroughly."
			},
			{
				"Answer_created_time": "2023-01-19T05:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@anjelab\u00a0but for that a paid support plan is required. Is it not possible to get free support from GCP to work around a bug for which GCP itself is responsible?"
			},
			{
				"Answer_created_time": "2023-01-19T05:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have the same error. It is very unfortunate to receive such useless error messages.\n\n\u00a0\n\n{\n\"name\": \"projects/570***756/locations/us/operations/3597173263047540318\",\n\"done\": true,\n\"result\": \"error\",\n\"response\": {},\n\"metadata\": {\n\"@type\": \"type.googleapis.com/google.cloud.documentai.uiv1beta3.TrainProcessorVersionMetadata\",\n\"commonMetadata\": {\n\"state\": \"FAILED\",\n\"createTime\": \"2023-01-19T13:40:54.538320Z\",\n\"updateTime\": \"2023-01-19T13:43:25.142621Z\",\n\"resource\": \"projects/570***756/locations/us/processors/4c3efa0d70e12950/processorVersions/31ebba062b7b3bd6\"\n},\n\"trainingDatasetValidation\": {},\n\"testDatasetValidation\": {}\n},\n\"error\": {\n\"code\": 13,\n\"message\": \"Internal error encountered.\",\n\"details\": []\n}\n}"
			},
			{
				"Answer_created_time": "2023-01-19T06:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Howdy ... I'm wondering if there might be additional diagnostics written to Cloud Logging?\u00a0 Maybe have a look in your Cloud Logging through the Google Cloud console and see if any errors are written/logged at the time of the API request error."
			},
			{
				"Answer_created_time": "2023-01-30T11:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Well, a retry a few hours later worked like a charm. There were no changes between the failed and successful executions.\u00a0\ud83d\ude2d"
			},
			{
				"Answer_created_time": "2023-01-20T08:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In our case, we discovered by chance, and not thanks to the information provided by GCP, that the problem was in the 'date' field. When we classified the date field as text, error 13 disappeared."
			},
			{
				"Answer_created_time": "2023-01-24T09:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We are having the same issue, worked fine for training as of a week ago and now failing every training attempt with error 13. No explanation or assistance on the error, what can be done?"
			},
			{
				"Answer_created_time": "2023-01-24T09:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In the logs, I am seeing this:\u00a0Task [4563861179629305857] was already retried over max attempts: 0\""
			},
			{
				"Answer_created_time": "2023-01-25T04:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Remove fields that are 'date'. We did this and it gave no more errors.\n\n\n[image: Zentricx Logo]\n[image: Instagram] <>\n[image: Facebook] <>\n[image: Linkedin] <>\n[image: Web] <>\n[image: Youtube] <>\n\n[image: Grate][image: Cloud]\n*Rodrigo Robert*\n| Data Scientist, Zentricx\n+ (PII Removed by Staff)\n+ (PII Removed by Staff)\n(PII Removed by Staff)\n*Linking People to** CX*\n\n\n\n*CONFIDENCIALIDAD: Este mensaje es privado y los archivos adjuntos al mismo\nson confidenciales y dirigidos exclusivamente a los destinatarios de los\nmismos.Por favor, si Ud. no es uno de dichos destinatarios, s\u00edrvase\nnotificarnos este hecho y no copie o revele su contenido a\nterceros.CONFIDENTIALITY: This message and any attached files are\nconfidential and intended only for the named recipients.If you are not one\nof them, please, notify us this fact and do not disclose its contents to\nanother person or take any copies.*"
			},
			{
				"Answer_created_time": "2023-03-08T07:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dear colleagues\n\nI am getting the same useless error \"13\" when trying to train a new model. Does anyone know how to get more information on the cause of the error?\n\n\u00a0\n\"trainingDatasetValidation\": {},\n\"testDatasetValidation\": {}\n},\n\"error\": {\n\"code\": 13,\n\"message\": \"Internal error encountered.\",\n\"details\": []\n}"
			},
			{
				"Answer_created_time": "2023-03-08T08:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In my case, the problem was with the extraction of the date field, GCP\nsupports only some date formats.\n\n\n[image: Zentricx Logo]\n[image: Instagram] <>\n[image: Facebook] <>\n[image: Linkedin] <>\n[image: Web] <>\n[image: Youtube] <>\n\n[image: Grate][image: Cloud]\n*Rodrigo Robert*\n| Data Scientist, Zentricx\n+(PII Removed by Staff)\n+(PII Removed by Staff)\n(PII removed by staff)\n*Linking People to** CX*\n\n\n\n*CONFIDENCIALIDAD: Este mensaje es privado y los archivos adjuntos al mismo\nson confidenciales y dirigidos exclusivamente a los destinatarios de los\nmismos.Por favor, si Ud. no es uno de dichos destinatarios, s\u00edrvase\nnotificarnos este hecho y no copie o revele su contenido a\nterceros.CONFIDENTIALITY: This message and any attached files are\nconfidential and intended only for the named recipients.If you are not one\nof them, please, notify us this fact and do not disclose its contents to\nanother person or take any copies.*"
			},
			{
				"Answer_created_time": "2023-03-08T10:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dear rodrigorobert\n\nThanks for your reply. Following your suggestion, I have already replaced all \"Data\" fields by \"Text\" field, without success. Most likely, the problem is in the labeling of some specific document. I removed all documents from Training and Test sets, and will re-insert in sets of 10, until the problematic document shows up. This is a stupid way to debug but unfortunately the only available so far. Thats why I would like to know some way of retrieving log errors. Thanks again for your help!"
			},
			{
				"Answer_created_time": "2023-03-08T10:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dear, thank you for your reply as well. Error '13' says nothing, it leaves\nus no other less stupid ways. I hope your problem is solved, good luck with\nthat!\n\n\n[image: Zentricx Logo]\n[image: Instagram] <>\n[image: Facebook] <>\n[image: Linkedin] <>\n[image: Web] <>\n[image: Youtube] <>\n\n[image: Grate][image: Cloud]\n*Rodrigo Robert*\n| Data Scientist, Zentricx\n(PII Removed by Staff)\n(PII Removed by Staff)\n(PII Removed by Staff)\n*Linking People to** CX*\n\n\n\n*CONFIDENCIALIDAD: Este mensaje es privado y los archivos adjuntos al mismo\nson confidenciales y dirigidos exclusivamente a los destinatarios de los\nmismos.Por favor, si Ud. no es uno de dichos destinatarios, s\u00edrvase\nnotificarnos este hecho y no copie o revele su contenido a\nterceros.CONFIDENTIALITY: This message and any attached files are\nconfidential and intended only for the named recipients.If you are not one\nof them, please, notify us this fact and do not disclose its contents to\nanother person or take any copies.*"
			}
		]
	},
	{
		"Question_title": "Problem integrating Python asyncio, sound device, Dialogflow streaming_detect_intent",
		"Question_created_time": "2023-03-06T06:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Problem-integrating-Python-asyncio-sound-device-Dialogflow/td-p/529250/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 145,
		"Question_body": "Hi Folks:I am trying to write a custom Dialogflow integration. I am writing a small program that streams audio to Dialogflow's streaming_detect_intent(). I have modified existing examples. For the audio, I am using sounddevice. I have two Python tasks. One runs the audio, the other Dialogflow. The tasks communicate through a shared queue. I can successfully stream audio into a file. I can successfully stream a file into Dialogflow. My code fails when I stream audio into Dialogflow. The immediate culprit is an asyncio.CancelledError(). The trace is    The code fragment is          Any insights would be appreciated!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-07T16:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Per the official docs, exception\u00a0asyncio.CancelledError happens when the operation has been cancelled. You may want to check the audio stream and make sure it's\u00a0not closing prematurely before the Dialogflow request can complete. Can you share the original sample code that you modified, and/or any guide/documentation you might have followed?"
			},
			{
				"Answer_created_time": "2023-03-08T10:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @Joevanie\u00a0\n\nThanks for the suggestions! The audio stream ought to close with the Ctrl C. However I get the CancelledError() almost immediately. I guess I could check this. I'll post the original code. If you need the project_id, I could send that to you separately. You ought to be able to play with the different examples. Again, thanks for the help!\n\nP.S - my original code did not have the single_utterance = True\n\nAndrew\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n#!/usr/bin/env python3\n\"\"\"Create a recording with arbitrary duration.\nThe soundfile module (https://python-soundfile.readthedocs.io/)\nhas to be installed!\n\"\"\"\n\nimport pdb\nimport uuid\nimport asyncio\nimport argparse\nimport tempfile\nimport queue\nimport sys\nimport contextlib\nimport traceback\nimport aiofiles\n\nimport sounddevice as sd\nimport soundfile as sf\nimport numpy  # Make sure NumPy is loaded before it is used in the callback\n\nassert numpy  # avoid \"imported but unused\" message (W0611)\n\nfrom google.cloud import dialogflow_v2beta1 as dialogflow\n\n\nasync def write_file(fp, audio_queue):\n    while True:\n        chunk = await audio_queue.get()\n        fp.write(chunk)\n\n\nasync def read_file(file_name, input_queue):\n    with open(file_name, \"rb\") as fp:\n        while True:\n            chunk = fp.read(4096)\n            print(f\"writing {len(chunk)}\")\n            input_queue.put_nowait(chunk)\n            if not chunk:\n                break\n\nasync def aio_read_file(file_name, input_queue):\n    async with aiofiles.open(file_name, \"rb\") as fp:\n        while True:\n            chunk = await fp.read(4096)\n            print(f\"writing {len(chunk)}\")\n            input_queue.put_nowait(chunk)\n            if not chunk:\n                break\n\nasync def sample_streaming_detect_intent(\n    loop, audio_queue, project_id, session_id, sample_rate\n):\n    # Create a client\n\n    client = dialogflow.SessionsAsyncClient()\n\n    audio_config = dialogflow.InputAudioConfig(\n        audio_encoding=dialogflow.AudioEncoding.AUDIO_ENCODING_LINEAR_16,\n        language_code=\"en\",\n        sample_rate_hertz=sample_rate,\n        single_utterance=True\n    )\n\n    async def request_generator(loop, project_id, session_id, audio_config, audio_queue):\n\n        query_input = dialogflow.QueryInput(audio_config=audio_config)\n\n        # Initialize request argument(s)\n        yield dialogflow.StreamingDetectIntentRequest(\n            session=client.session_path(project_id, session_id), query_input=query_input\n        )\n\n        while True:\n            chunk = await audio_queue.get()\n            if not chunk:\n                break\n            # The later requests contains audio data.\n            yield dialogflow.StreamingDetectIntentRequest(input_audio=chunk)\n\n    # Make the request\n    client_task = asyncio.create_task(\n        client.streaming_detect_intent(\n            requests=request_generator(\n                loop, project_id, session_id, audio_config, audio_queue\n            )\n        )\n    )\n\n    try:\n        stream = await client_task\n    except Exception as e:\n        print(f\"failed with {e.__cause__}\")\n\n    try:\n        async for response in stream:\n            print(response)\n    except Exception as e:\n        print(f\"failed with {e.__cause__}\")\n\n    query_result = response.query_result\n\n    print(\"=\" * 20)\n    print(\"Query text: {}\".format(query_result.query_text))\n    print(\n        \"Detected intent: {} (confidence: {})\\n\".format(\n            query_result.intent.display_name, query_result.intent_detection_confidence\n        )\n    )\n    print(\"Fulfillment text: {}\\n\".format(query_result.fulfillment_text))\n\n\nasync def record_audio(fp, loop, audio_queue, sample_rate, device, channels):\n\n    q = asyncio.Queue()\n\n    def callback(indata, frames, time, status):\n        loop.call_soon_threadsafe(q.put_nowait, indata.copy())\n        \n    try:\n        with sd.InputStream(\n            samplerate=sample_rate, device=device, channels=channels, callback=callback\n        ):\n            print(\"#\" * 80)\n            print(\"press Ctrl+C to stop the recording\")\n            print(\"#\" * 80)\n            while True:\n                chunk = await q.get()\n                audio_queue.put_nowait(chunk)\n\n    except KeyboardInterrupt:\n        print(\"\\nRecording finished\")\n        fp.close()\n    except asyncio.CancelledError as e:\n        print(\"caught a cancelled Error ****\")\n        traceback.print_exc(limit=None, file=None, chain=True)\n    except Exception as e:\n        print(e.__cause__)\n        print(type(e).__name__ + \": \" + str(e))\n\n\nasync def test_one():\n\n    device = None\n    channels = 1\n    project_id = \"\"\n    session_id = str(uuid.uuid4())\n    mode = \"x\"\n    subtype = None\n\n    audio_queue = asyncio.Queue()\n\n    device_info = sd.query_devices(device, \"input\")\n    sample_rate = int(device_info[\"default_samplerate\"])\n\n    filename = tempfile.mktemp(prefix=\"delme_rec_unlimited_\", suffix=\".wav\", dir=\"\")\n\n    fp = sf.SoundFile(filename, mode, sample_rate, channels, subtype)\n\n    loop = asyncio.get_event_loop()\n    await asyncio.gather(\n        record_audio(fp, loop, audio_queue, sample_rate, device, channels),\n        write_file(fp, audio_queue),\n    )\n\n\nasync def test_two():\n\n    sample_rate = 24000\n    device = None\n    channels = 1\n    project_id = \"\"\n    session_id = str(uuid.uuid4())\n    mode = \"x\"\n    subtype = None\n\n    audio_queue = asyncio.Queue()\n\n    device_info = sd.query_devices(device, \"input\")\n    samplerate = int(device_info[\"default_samplerate\"])\n\n    loop = asyncio.get_event_loop()\n    await asyncio.gather(\n        read_file(\"request.wav\", audio_queue),\n        sample_streaming_detect_intent(\n            loop, audio_queue, project_id, session_id, sample_rate\n        ),\n    )\n\n\nasync def test_three():\n    \"\"\"\n    stream from mic into Dialogflow\n    \"\"\"\n\n    device = None\n    channels = 1\n    project_id = \"\"\n    session_id = str(uuid.uuid4())\n    mode = \"x\"\n    subtype = None\n\n    audio_queue = asyncio.Queue()\n\n    device_info = sd.query_devices(device, \"input\")\n    sample_rate = int(device_info[\"default_samplerate\"])\n\n    # let's do this just for consistency\n    filename = tempfile.mktemp(prefix=\"delme_rec_unlimited_\", suffix=\".wav\", dir=\"\")\n\n    fp = sf.SoundFile(filename, mode, sample_rate, channels, subtype)\n\n    loop = asyncio.get_event_loop()\n    await asyncio.gather(\n        record_audio(fp, loop, audio_queue, sample_rate, device, channels),\n        sample_streaming_detect_intent(\n            loop, audio_queue, project_id, session_id, sample_rate\n        ),\n    )\n\n\n\nasync def test_five():\n\n    sample_rate = 24000\n    device = None\n    channels = 1\n    project_id = \"\"\n    session_id = str(uuid.uuid4())\n    mode = \"x\"\n    subtype = None\n\n    audio_queue = asyncio.Queue()\n\n    device_info = sd.query_devices(device, \"input\")\n    samplerate = int(device_info[\"default_samplerate\"])\n\n    loop = asyncio.get_event_loop()\n\n    await asyncio.gather(\n        aio_read_file(\"request.wav\", audio_queue),\n        sample_streaming_detect_intent(\n            loop, audio_queue, project_id, session_id, sample_rate\n        ),\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(test_three(), debug=True)"
			}
		]
	},
	{
		"Question_title": "Feature Store MLOps Pipeline",
		"Question_created_time": "2023-03-04T02:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Feature-Store-MLOps-Pipeline/td-p/528764/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 97,
		"Question_body": "Hey,i'm trying to understand the MLOps Pipeline with the CI/CD-Automation (Stage 2 Maturity Level) and struggle with the Feature Store as the component feeding the Automated Pipeline with data. What i found out in the internet was, that Feature Stores extract data from different sources, transform them and create training data which can be used to train the model (retraining with new data). But in the pipeline the steps like Data preperation and Data extraction come after the Feature Store.Can somebody explain to me, whats the output of the Feature Store and how it is used to serve the data for the Automated Pipeline and the Prediction Service?Thanks in advance ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-06T14:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The Feature Store is just a centralized repository of features. By that, its output is just a set of features typically used to train an ML model. Depending on your specific needs, you can serve the ingested data in the Feature Store to the model right away (in what is called feature serving) or export feature values and do further preparation of data.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-06T14:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The Feature Store is just a centralized repository of features. By that, its output is just a set of features typically used to train an ML model. Depending on your specific needs, you can serve the ingested data in the Feature Store to the model right away (in what is called feature serving) or export feature values and do further preparation of data."
			}
		]
	},
	{
		"Question_title": "Need \"no code\" help with buckets and datasets.",
		"Question_created_time": "2023-03-05T18:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Need-quot-no-code-quot-help-with-buckets-and-datasets/td-p/529084/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 40,
		"Question_body": "I have uploaded thousands of images (individual jpgs) to a Bucket. I now want to access those and use them in a new dataset. I can see all my uploaded images in the \"cloud storage\" \"bucket\" interface, but I cannot, for the life of me, work out how to attach any of those to a new dataset, except one at a time. Is there a way to make these images available to a new dataset in batches? Also, and I can't stress this enough, if that involves any form of coding, then that's a show-stopper for me.     ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-06T10:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "To upload images to a dataset in batch, you may create a csv file containing all the images that you will upload. Here is the full documentation. In summary, below is what your csv file should contain:\n\n[ML_USE],GCS_FILE_PATH,[LABEL]\n\nNote that GCS_FILE_PATH is the only one required. If you only need to upload from bucket to dataset, you can just list all the filepaths like this:\n\ngs://BUCKET_NAME/FILE_NAME_1.jpg\ngs://BUCKET_NAME/FILE_NAME_2.jpg\ngs://BUCKET_NAME/FILE_NAME_3.jpg\n\nYou can program this to save time but you can also do it manually if that's what you prefer.\n\nAfter creating the csv file:\n\n1. Upload it in the same bucket containing the images\n\n2. Choose \"select an import file from Cloud Storage\" in Datasets\n\n3. Choose the import csv file\n\nIf you need further clarification, here is the full documentation for Creating datasets. Hope this helps"
			}
		]
	},
	{
		"Question_title": "Speech to text API not returning long running operation results - is service impaired?",
		"Question_created_time": "2023-03-02T12:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-text-API-not-returning-long-running-operation-results/td-p/528170/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 85,
		"Question_body": "The Speech to text API stopped returning results today. It was working properly yesterday evening (16 hours ago.\n\nI have tried it from python and node, both of which worked fine yesterday.\n\nI am using the same code and short audio (2 minutes) that I have been using.I am using the python speech_v1p1beta1 lib and have tried speech_v1 also.I get the operation object but the results just timeout no matter what timeout I set\n\nThanks",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-03T12:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It is working again as of late yesterday but timestamps are off."
			},
			{
				"Answer_created_time": "2023-03-03T20:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It looks like there's no recent service impairment of Speech-to-Text.\u00a0If there are any ongoing incidents, you should see them listed on the dashboard. As for the timestamp, you may want to check this documentation."
			}
		]
	},
	{
		"Question_title": "Selecting a region for Cloud Talent Solution",
		"Question_created_time": "2023-03-02T04:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Selecting-a-region-for-Cloud-Talent-Solution/td-p/527976/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 75,
		"Question_body": "Starting using Cloud Talent Solution service but i am not able to see an option to configure a default region for this service. I would like to choose the region us-west-1 (oregon), is there a way i can do that?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-03T20:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What specific Cloud Solutions task are you trying to do? If job search, try following this documentation. Check also if a simple run of this command quickly fixes your concern\n\ngcloud config set run/region us-west1"
			}
		]
	},
	{
		"Question_title": "Translate text with a glossary (unsupported languages)",
		"Question_created_time": "2023-03-03T05:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Translate-text-with-a-glossary-unsupported-languages/td-p/528448/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 88,
		"Question_body": "Previously, in the list of supported languages (https://cloud.google.com/translate/docs/languages), some languages were marked with an asterisk (*) and there was a note at the end of the list stating this: \u201c* You cannot use a glossary when translating to or from these languages. Glossaries aren't supported for these languages.\u201dNow that information has disappeared from the list. Does this mean all listed languages now support the glossary feature?These were the languages listed as unsupported for this feature: \"Akan, Assamese, Aymara, Bhojpuri, Bambara, Kurdish (Sorani), Dogri, Divehi, Ewe, Guarani, Goan Konkani, Iloko, Krio, Ganda, Lingala, Mizo, Maithili, Manipuri (Meitei Mayek), Northern Sotho, Oromo, Quechua, Sanskrit, Tigrinya, Tsonga\"BR,Julian",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-03T12:21:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "According to the latest release note, there are 24 languages added to the glossary. All the languages provided here are all supported languages, that is why there is no asterisk(*) indication anymore. Also, there are still languages that were not added. I suggest to just check the release notes every now and then for any new announcements.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-03T12:21:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "According to the latest release note, there are 24 languages added to the glossary. All the languages provided here are all supported languages, that is why there is no asterisk(*) indication anymore. Also, there are still languages that were not added. I suggest to just check the release notes every now and then for any new announcements."
			}
		]
	},
	{
		"Question_title": "Google cloud transcription API",
		"Question_created_time": "2022-05-26T21:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-cloud-transcription-API/td-p/426546/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 117,
		"Question_body": "I would like to calculate the time duration for every speaker in a two way conversation call with speaker tag, transcription, time stamp of speaker duration and confidence of it.For example: I have mp3 file of a customer care support with 2 speaker count. I would like to know the time duration of the speaker with speaker tag, transcription and confidence of the transcription.I am facing issues with end time and confidence of the transcription. I'm getting confidence as 0 in transcription and end time is not appropriate with actual end time.audio link: https://drive.google.com/file/d/1OhwQ-xI7Rd-iKNj_dKP2unNxQzMIYlNW/view?usp=sharing ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-02T15:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, we noticed that you posted the same question in StackOverflow, and since this seems like an issue in your code, I encourage you to follow this up in that forum."
			},
			{
				"Answer_created_time": "2023-03-03T08:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "And we all know how well StackOverflow answers are reasonable & working code. Wouldn't it be great when all Gcloud users\u00a0 are supported with answers that are moderated and verified? Unbundling and bundling etc."
			}
		]
	},
	{
		"Question_title": "Dialogflow ES Bad update mask fields",
		"Question_created_time": "2023-03-01T05:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-ES-Bad-update-mask-fields/td-p/527470/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 57,
		"Question_body": "Hello everyone,I am encountering an error while trying to update the text_to_speech_settings.enable_text_to_speech parameter using the Dialogflow API. The error message I'm getting is \"Bad update mask fields: [text_to_speech_settings.enable_text_to_speech]\".I'm not sure what is causing this error, and I've already checked my API request and made sure that the resource name and update mask are correct. Can someone please help me troubleshoot this issue?Here are some additional details about my setup: Thank you in advance for your help!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-02T12:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I believe fieldmask is for overwriting data type and not for parameter in a configuration, as mentioned here:\u00a0 https://cloud.google.com/dialogflow/es/docs/how/field-mask\n\nI would suggest to have an environment of your agent and in the console you can enable or disable\u00a0 before publishing a version:\u00a0\n\n\u00a0\n\n*Assuming you are using dialogflow es"
			}
		]
	},
	{
		"Question_title": "Deleting a failed dataset",
		"Question_created_time": "2023-02-28T07:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Deleting-a-failed-dataset/td-p/527077/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 84,
		"Question_body": "Under the Vertex AI - a dataset failed to create due to a constraint applied to the organization. It does not allow for the deletion of the dataset, I attempted using python (Delete a dataset  |  Vertex AI  |  Google Cloud) and the response was - \"...is in failure state and cannot be deleted. It will be deleted automatically after a few days.\"  but it didn't delete. There is not a gcloud command to correct. Short of a support request..how can the dataset be removed as I foresee this occuring as others attempt experiments. I have addressed the issue with the constraint.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-03-01T14:35:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-01T14:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "yeah - I knew the code worked as I have a cloud task that cleaned up datasets in another project - the root issue is that it failed on creation...thanks\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-03-01T14:35:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate."
			},
			{
				"Answer_created_time": "2023-03-01T14:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "yeah - I knew the code worked as I have a cloud task that cleaned up datasets in another project - the root issue is that it failed on creation...thanks"
			}
		]
	},
	{
		"Question_title": "Dialogflow CX bot does not display the fulfilment text when called by api",
		"Question_created_time": "2023-02-24T10:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-bot-does-not-display-the-fulfilment-text-when/td-p/525966/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 90,
		"Question_body": "Hi guys, I'm struggling with a problem related to calling the Dialogflow API. I'm using an API to call a specific page using the following payload:   and to call it, I use:  I'm trying to call this specific page:But I don't get any response. No text is sent back.Now, if I use a parameter, I'll get the response:But here's the thing, I won't use parameters everywhere. I need to get the fulfillment message.Does anyone have any idea what I can do? Thank you all!!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-28T09:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I believe you need intents for the flow to\u00a0 enter the fulfilment page.\u00a0\u00a0\n\nIntent documentation:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/intent\n\nFullfilment documentation and use cases:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/fulfillment"
			}
		]
	},
	{
		"Question_title": "Google Document AI - The Caller Does Not Have Permission Error",
		"Question_created_time": "2023-02-27T13:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Document-AI-The-Caller-Does-Not-Have-Permission-Error/td-p/526760/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 131,
		"Question_body": "Hi , i'm trying to use Google Document API to parse invoices using invoice parser. Followed the doc and imported 10 invoices to train the parser. Used Auto Labeling option. While trying to mark the imported invoices as \"Mark as Labeled\" after few corrections, Im getting an error saying \"The caller does not have permission\". The user trying to mark is the owner of the project. The owner of the project has all permissions to the parser and storage. Can some one please help.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-27T16:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Howdy\u00a0ganeshsarathi,\nWhat an awesome post.\u00a0 THANK YOU for providing all these details.\u00a0 I used the post content as search arguments inside Google to see if other customers have had similar problems.\u00a0 I immediately got lucky and found a report (for my records\u00a0b/266677102).\u00a0 This report had exactly the same symptoms and was reported on 2023-01-25 which means that it is recent.\u00a0 The problem (for that client) was that the GCS bucket that was being used to host the documents had a \"retention policy\" defined on it.\u00a0 As I understand retention policies, they effectively mean that once an object has been written into the bucket, it may not be deleted or updated.\u00a0 Retention policies are used to prevent accidental deletion and mean that the content becomes immutable for the period of time that the documents are being retained.\u00a0 We can read more about buckets with retention policies here.\n\nWhat I'd like you to do is examine your environment as it relates to Document AI.\u00a0 Let's see which Google Cloud Storage buckets may be in play (i.e. where the documents may be living) and whether or not they have retention policies applied.\u00a0 If the answer is yes, then we have an exact match.\u00a0 If no, then we will have to look for other reasons ... but we'll start here.\n\nThe other customer switched to using a bucket that did not have a retention policy.\u00a0 However, the ticket remains open while Google considers what this means in terms of retaining documents while at the same time allowing HITL processing.\u00a0 Looking forward to hearing back.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-27T14:28:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I'm wondering if there are more details about the permission being sought?\u00a0 Is there more message content?\u00a0 Was anything written to Cloud Logging?\u00a0 I'm also wondering if we can capture the exact steps you followed to get to the error.\n\nThis appears to be the article page that lists the IAM roles for Doc AI:\n\nhttps://cloud.google.com/document-ai/docs/access-control/iam-roles"
			},
			{
				"Answer_created_time": "2023-02-27T15:24:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "hi Kolban,\u00a0 \u00a0the project is having\u00a0Document AI Administrator\u00a0(roles/documentai.admin) role.\u00a0\n\n\nReg the exact steps,\u00a0 in the following screenshot,\u00a0 im at third link create and uptrain invoice parser.\u00a0\u00a0\n\n\u00a0\n\n\u00a0\n\nIn this at Setp 5, i have to mark all imported auto labeled docs as \"Mark as Labeled. \"\u00a0 While clicking that button, getting the error given in the screenshot.\u00a0\n\n\u00a0\n\nHere is the error message screenshot."
			},
			{
				"Answer_created_time": "2023-02-27T16:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Howdy\u00a0ganeshsarathi,\nWhat an awesome post.\u00a0 THANK YOU for providing all these details.\u00a0 I used the post content as search arguments inside Google to see if other customers have had similar problems.\u00a0 I immediately got lucky and found a report (for my records\u00a0b/266677102).\u00a0 This report had exactly the same symptoms and was reported on 2023-01-25 which means that it is recent.\u00a0 The problem (for that client) was that the GCS bucket that was being used to host the documents had a \"retention policy\" defined on it.\u00a0 As I understand retention policies, they effectively mean that once an object has been written into the bucket, it may not be deleted or updated.\u00a0 Retention policies are used to prevent accidental deletion and mean that the content becomes immutable for the period of time that the documents are being retained.\u00a0 We can read more about buckets with retention policies here.\n\nWhat I'd like you to do is examine your environment as it relates to Document AI.\u00a0 Let's see which Google Cloud Storage buckets may be in play (i.e. where the documents may be living) and whether or not they have retention policies applied.\u00a0 If the answer is yes, then we have an exact match.\u00a0 If no, then we will have to look for other reasons ... but we'll start here.\n\nThe other customer switched to using a bucket that did not have a retention policy.\u00a0 However, the ticket remains open while Google considers what this means in terms of retaining documents while at the same time allowing HITL processing.\u00a0 Looking forward to hearing back."
			},
			{
				"Answer_created_time": "2023-02-27T17:29:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks a billion Kolban.\u00a0 You should be a rock star.\u00a0 Struggled for whole day.\u00a0 Yes the bucket had a retention policy.\u00a0 Removed it.\u00a0 Now its WORKING !!!!.\u00a0 Thank you again for helping."
			}
		]
	},
	{
		"Question_title": "Talent Solution - Orphaned companies",
		"Question_created_time": "2023-02-24T16:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Talent-Solution-Orphaned-companies/td-p/526050/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 49,
		"Question_body": "I have been experimenting/testing with Talent Solution, but hit a (not so interesting) problem.It seems to be possible to delete a tenant even while it contains active companies. Unfortunately, once you do that, it seems to be impossible to get a handle back to the company. If I try to get it directly (GetCompanyRequest), I get an error that the tenant does not exist. I know the company is not cleaned up after running an export on the \"Jobs and companies\" summary page which is showing a bunch of companies I am not expecting...Is there a way to clean up the companies? \ud83d\ude1eThanks",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-27T13:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It looks like an issue with the api, I would suggest creating a public issue tracker for engineers to look in to\u00a0 your inquiry.\n\n\nIssue Tracker Documentation: https://cloud.google.com/support/docs/issue-trackers\n\nTalent solution issue tracker: https://issuetracker.google.com/issues/new?component=187187&template=0\n\nFill the requested details then submit the tracker. Also they may ask further information for confirmation about your question."
			}
		]
	},
	{
		"Question_title": "Neural2 voices sometimes appear to sound like it's drunk or having a stroke",
		"Question_created_time": "2023-02-23T09:37:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Neural2-voices-sometimes-appear-to-sound-like-it-s-drunk-or/td-p/525604/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 133,
		"Question_body": "For some reasons, Neural2 voices sometimes appear to sound like it's drunk or having a stroke. For instance, synthesize the following text with en-US-Neural2-J using the demo prompt on this page:First came a stout puffy gentleman with a carpet bag; he wanted to go to the Bishopsgate station; then we were called by a party who wished to be taken to the Regent's Park; and next we were wanted in a side street where a timid, anxious old lady was waiting to be taken to the bank; there we had to stop to take her back again, and just as we had set her down a red-faced gentleman, with a handful of papers, came running up out of breath, and before Jerry could get down he had opened the door, popped himself in, and called out, \u201cBow Street Police Station, quick!\u201d so off we went with him, and when after another turn or two we came back, there was no other cab on the stand.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-24T10:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My conclusion is the Text-to-Speech was having trouble with too many commas and semicolons. I just replaced one semicolon with a period since it started pronouncing weirdly at the part of \"And next we were wanted in a side street where a timid\" and it worked! The whole input text was pronounced clearly.\n\nFirst came a stout puffy gentleman with a carpet bag; he wanted to go to the Bishopsgate station; then we were called by a party who wished to be taken to the Regent's Park. And next we were wanted in a side street where a timid, anxious old lady was waiting to be taken to the bank; there we had to stop to take her back again, and just as we had set her down a red-faced gentleman, with a handful of papers, came running up out of breath, and before Jerry could get down he had opened the door, popped himself in, and called out, \u201cBow Street Police Station, quick!\u201d so off we went with him, and when after another turn or two we came back, there was no other cab on the stand."
			},
			{
				"Answer_created_time": "2023-02-27T10:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Poala_Tenorio\u00a0Unfortunately this workaround isn't really viable for me; I created a text-to-speech extension for chrome that relies on the GCloud API so users don't have control over the selected text most of the time (since the text may be a selection from a webpage and they would need to open the dev tools and edit the HTML every-time this issue arrises).\n\nSeems to happen quite frequently as I'm getting many reports for this. Any chances this bug could be flagged to the team working on the Text-to-Speech API?"
			}
		]
	},
	{
		"Question_title": "Speech-to-text in web application giving unexpected results",
		"Question_created_time": "2023-02-27T07:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-text-in-web-application-giving-unexpected-results/td-p/526612/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 112,
		"Question_body": "I am trying to implement Google Cloud's Speech-to-Text API in a web application, so users can speak into a microphone and see what they say in real time. I am using React.js on the frontend and Express.js in the backend. I am using the `microphone-stream` npm package to capture and stream user audio and the `websocket-stream` npm package to stream the audio through a web socket. Here is my source code on the frontend:  And here is how I handle it on the backend:  The data is sent through correctly, but I am getting very unexpected results. I keep getting a transcript that reads \"play\" or \"play radio\", even if I say nothing. This is an example response:   Am I approaching this correctly? Any help or advice would be greatly appreciated.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-27T08:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It looks right to my eyes after a few minutes examination.\u00a0 If I were in your shoes, I'd do some debugging by breaking apart the pieces.\u00a0 In your back-end, you are receiving an audio stream and piping that directly to speech to text.\u00a0 What about writing it to a file and, when your transmission is over, download the file and validate that it contains what you expect.\u00a0 \u00a0Next, write a test that reads the input data from the file and tries to convert it.\u00a0 Now you have separated the two parts of your story ... maybe something interesting will show up.\u00a0 If nothing else, you have halved the problem.\u00a0 Assuming you end up with a good audio file, you can now package up the puzzle (samples audio + code) so that others can try and recreate.\u00a0 Sadly, these forums aren't great for \"Here is my code ... what is wrong\" ... if we can narrow it down to as simple a failing thing ... i.e. \"I am using XXX Google Cloud API and expected AAA but got BBB, what might be wrong?\".\n\nThat said ... one of the best written reports I have seen in a long time.\u00a0 Thank you for that!!!"
			}
		]
	},
	{
		"Question_title": "AutoML forecasting: understanding the rolling forecast window during model evaluation",
		"Question_created_time": "2023-01-31T13:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-forecasting-understanding-the-rolling-forecast-window/td-p/516170/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 6,
		"Question_score_count": 0,
		"Question_view_count": 160,
		"Question_body": "I trained a time series forecasting model with AutoML. During training, I checked the option to \"Export test dataset to BigQuery.\" I have a question about how to understand the data that appears in the exported table.My understanding is that a \"predicted_on\" timestamp is essentially the first date of the forecast horizon of a rolling forecast window. I see that for each \"predicted_on\" timestamp, there are 6 timestamps from the test data split of my training data. This suggests that the forecast horizon is 6 weeks long; i.e., for each \"predicted_on\" date (and starting on that date) it predicts 6 weeks of data.My question is, where does the number 6 come from? (When I trained the model, I specified that the forecast horizon is 26 weeks, not 6...)",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-24T08:22:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "To answer my question:\n\nI'm not sure where the number 6 came from, but I've since discovered the following.\n\nThe timestamp format I was using is not among the timestamp formats supported by Google according to this documentation. I changed the format of my timestamps. I also ensured that every number in my target column has a decimal (it was previously a mix of integers and decimal numbers).\n\nAfter making these changes, I trained a new model and examined the data exported to BigQuery.\n\nNow I see that there are 26 weeks of timestamps from the test data split associated with the first\u00a0`predicted_on` timestamp. This would suggest that the forecast horizon of the rolling forecast window is 26 weeks long. This is what I would expect, given that I set the forecast horizon to 26 when I trained the model.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-01T10:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, Just wanted to ask if you are currently following any documentation for this task? This for replication purposes.\n\nThanks!"
			},
			{
				"Answer_created_time": "2023-02-01T10:11:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\nYes, the documentation I've read which feels most relevant to the question is:\n\nBest practices for tabular forecasting models\n\nData splits for forecasting\n\nI've also read every article that is linked on the Forecasting overview page.\n\nAnd I've followed this Build an AutoML Forecasting Model with Vertex AI lab."
			},
			{
				"Answer_created_time": "2023-02-01T11:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for these information, I will attempt and replicate your scenario. Also I assume this is related to :\u00a0https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-forecast-model-batch-predictions-quot-rows-with...\u00a0question too. Thanks!"
			},
			{
				"Answer_created_time": "2023-02-01T11:40:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks, I really appreciate the help!\n\nYes, my other question you linked is related in the sense that both questions are regarding the same forecasting model. (The question you linked is more pressing, since it's about how we're struggling to get any actual predictions from the model - that's our top priority at the moment.)"
			},
			{
				"Answer_created_time": "2023-02-07T07:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "@nceniza\u00a0Just checking in to see if you've had a chance to look into this, in particular the issue described in my other question which you linked (i.e., the error when we attempt to get batch predictions:\u00a0\"There are rows with non-empty target values after this row.\")\n\nWe are really stuck on this error. Although we've invested time and money in training the model, this error is preventing us from getting predictions. We are so close, yet we can't get our project over the finish line. If there is anything you can do to help, we'd really appreciate it. Thanks!"
			},
			{
				"Answer_created_time": "2023-02-24T08:22:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "To answer my question:\n\nI'm not sure where the number 6 came from, but I've since discovered the following.\n\nThe timestamp format I was using is not among the timestamp formats supported by Google according to this documentation. I changed the format of my timestamps. I also ensured that every number in my target column has a decimal (it was previously a mix of integers and decimal numbers).\n\nAfter making these changes, I trained a new model and examined the data exported to BigQuery.\n\nNow I see that there are 26 weeks of timestamps from the test data split associated with the first\u00a0`predicted_on` timestamp. This would suggest that the forecast horizon of the rolling forecast window is 26 weeks long. This is what I would expect, given that I set the forecast horizon to 26 when I trained the model."
			}
		]
	},
	{
		"Question_title": "AutoML forecast model batch predictions: \"rows with non-empty target values\" error",
		"Question_created_time": "2023-01-31T13:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-forecast-model-batch-predictions-quot-rows-with-non-empty/td-p/516165/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 224,
		"Question_body": "I'm trying to get a batch prediction from a time series forecasting model trained with AutoML. I'm seeing the following error in the BigQuery \"errors_validation\" table:\"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\"\n(There are 7 such error messages, one for each time series. Each error message indicates \"01/01/2023\" as the timestamp.\u2020)I just can't see how what the error message is saying could possibly be true.- The granularity is weekly, and the forecast horizon is 26 weeks.\n- For the batch prediction, I'm using a CSV file which consists of all the data used in training, plus an additional 26 weeks of future timestamps appended to it. (To be precise, each of the 26 future timestamps appears 7 times - once for each time series.)\n- For each row with a future timestamp, the target column is empty (i.e., in the CSV there is nothing in the column whatsoever).\n- I also tried a batch prediction sourced from a BigQuery table. I created the table from the CSV file described above. I examined the table and confirmed that there are null values in the target column wherever there is a future timestamp. I got the same errors.I'm at a loss. Any help would be greatly appreciated.\u2020 I'm not sure if this is significant in any way, but the forecast horizon actually starts on 12/04/2022 (earlier than the timestamp indicated by the errors). In any case, there is no data in the forecast horizon with a non-empty/non-null target value.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-24T07:12:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I was able to get batch predictions after making the following changes to the data and training a new model:\n\nChanged the format of the timestamps from \"mm/dd/yyyy\" to \"yyyy-mm-dd\".\nEnsured that all numbers in the target column have decimals (\"0.0\" instead of \"0\", etc.). (Previously the numbers in this column were a mix of integers and decimal numbers.)\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-02T12:53:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "If it helps, here is an excerpt of the CSV file I'm using to get the batch predictions.\n\n\"submission_week\" is the timestamp column.\n\"weekly_expense_total\" is the target column.\n\"category\" is the time series identifier (there are 7 time series).\nThe granularity is weekly.\n\n\nsubmission_week,weekly_expense_total,category\n...\n11/27/2022,320.49,Food\n11/27/2022,385.88,Health & Fitness\n11/27/2022,0,Learning & Development\n11/27/2022,1318.37,Office\n11/27/2022,450,Other\n11/27/2022,0,Team Activities\n11/27/2022,980.23,Travel & Accommodations <- End of historical data\n12/04/2022,,Food                          <- First week of the forecast horizon\n12/04/2022,,Health & Fitness\n12/04/2022,,Learning & Development\n12/04/2022,,Office\n12/04/2022,,Other\n12/04/2022,,Team Activities\n12/04/2022,,Travel & Accommodations\n... (followed by 25 additional weeks - forecast horizon is 26 weeks long)\n\n\u00a0As you can see, the target column is blank where the forecast horizon starts. In fact, it's blank for every row with a timestamp in the forecast horizon.\n\nThus the \"There are rows with non-empty target values after this row\" error message is really puzzling (\"this row\" being 01/01/2023 - not sure why that's the timestamp it calls out).\n\nI also checked to see if the CSV contains any non-printable characters, but I couldn't find any."
			},
			{
				"Answer_created_time": "2023-02-24T07:12:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I was able to get batch predictions after making the following changes to the data and training a new model:\n\nChanged the format of the timestamps from \"mm/dd/yyyy\" to \"yyyy-mm-dd\".\nEnsured that all numbers in the target column have decimals (\"0.0\" instead of \"0\", etc.). (Previously the numbers in this column were a mix of integers and decimal numbers.)"
			}
		]
	},
	{
		"Question_title": "Make Google Translator Widget Language Labels in English rather than user's browser language",
		"Question_created_time": "2023-02-23T20:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Make-Google-Translator-Widget-Language-Labels-in-English-rather/td-p/525800/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 34,
		"Question_body": "function googleTranslateElementInit() { new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE, autoDisplay: false}, 'google_translate_element'); }",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-23T20:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am using a google translator widget on my website. Script below.\n\nfunction googleTranslateElementInit() { new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE, autoDisplay: false}, 'google_translate_element'); }\n\nThe translation widget comes up and language switching works fine.\nBut the problem is that the text in google widget drop-down is getting loaded in the browser language.\nFor example, if I open the website from Netherlands/Dutch, it will show\u00a0Engels\u00a0instead of\u00a0English\nWhat I need is to display the widget text always in English.\nI did try adding the\u00a0Pagelaguage\u00a0as 'en' but didn't work.\nNeed help to solve this."
			}
		]
	},
	{
		"Question_title": "Can I see the models vertex AI AutoML has used to train my model?",
		"Question_created_time": "2023-02-22T02:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-I-see-the-models-vertex-AI-AutoML-has-used-to-train-my-model/td-p/524954/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 68,
		"Question_body": "I have trained a few object detection models on vertex ai automl. is there a way for me to see what ML model has been used by vertex ai to train my data?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-23T13:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can view model details at your model registry tab in your project, alternatively you can view your models at vertex AI homepage in the recent model windows. For more details please see this documentation."
			}
		]
	},
	{
		"Question_title": "Google Cloud Translate API",
		"Question_created_time": "2022-11-14T02:52:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Translate-API/td-p/489124/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 889,
		"Question_body": "Hi,i am use free trails of Google Cloud Translate send request on this URL  https://translation.googleapis.com/language/translate/v2 with API key with body rowget error response kindly see below\"code\": 403,\n\"message\": \"The request is missing a valid API key.\",\n\"errors\":\n\n\"message\": \"The request is missing a valid API key.\",\n\"domain\": \"global\",\n\"reason\": \"forbidden\"\n\"status\": \"PERMISSION_DENIED\"",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-15T09:16:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, mjunaid,\n\nThe key you're using might not have the permission to use Translate APIs.\n\nTo fix this:\n\nGo to the Google Cloud Platform console\nChoose your project from the drop-down menu in the top bar\nGo to API & Services > Library\nSearch for Cloud Translation API and click on it\nEnable it\n\nGo to API & Services > Credentials\nSelect the key you are using in your Android App\nFrom the menu called Restrict key, choose Cloud Translation API\nSave your edit.\n\nNow the APIs should work properly.\n\nAdditionally, please note that the documentation\u00a0mentions that the structure of the HTTP method should be something like:\n\nhttps://translation.googleapis.com/language/translate/v2?key=[yourAPIkey]&target=language\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-15T09:16:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, mjunaid,\n\nThe key you're using might not have the permission to use Translate APIs.\n\nTo fix this:\n\nGo to the Google Cloud Platform console\nChoose your project from the drop-down menu in the top bar\nGo to API & Services > Library\nSearch for Cloud Translation API and click on it\nEnable it\n\nGo to API & Services > Credentials\nSelect the key you are using in your Android App\nFrom the menu called Restrict key, choose Cloud Translation API\nSave your edit.\n\nNow the APIs should work properly.\n\nAdditionally, please note that the documentation\u00a0mentions that the structure of the HTTP method should be something like:\n\nhttps://translation.googleapis.com/language/translate/v2?key=[yourAPIkey]&target=language"
			},
			{
				"Answer_created_time": "2022-11-15T22:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks"
			},
			{
				"Answer_created_time": "2023-01-22T02:46:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for informative article. This is one of the best resource I have found in quite sometime\nGoogle Cloud Training."
			},
			{
				"Answer_created_time": "2023-02-23T11:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "@mjunaid\u00a0I use Google's translate api service in my Translate.How project, sometimes I get access errors. All permissions were given. I don't know if there is a problem with API overlap. I make at least 40 multiple connections per second."
			}
		]
	},
	{
		"Question_title": "Error- Warning message: Annotation label is deduped",
		"Question_created_time": "2023-02-13T00:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-Warning-message-Annotation-label-is-deduped/td-p/521402/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 168,
		"Question_body": "I have uploaded a csv for image classification. I have checked the csv for any errors or duplicate values. But when I import the csv, i get the error that annotation is deduped and only a few images out of all the images are getting imported.\n\nWhat could be the possible cause for this?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-13T22:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Have you tried it in Vertex AI since it has more features?"
			},
			{
				"Answer_created_time": "2023-02-13T23:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "yes, i am using vertex ai"
			},
			{
				"Answer_created_time": "2023-02-23T08:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The possible reason for having deduped annotation is when the labelling on your CSV file was not read properly, since Vertex AI automatically produces the annotation set based on labels in your CSV.\n\nYou can try to create a new dataset and a new annotation set."
			}
		]
	},
	{
		"Question_title": "DTMF updation through client sdk",
		"Question_created_time": "2023-02-22T23:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DTMF-updation-through-client-sdk/td-p/525382/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 56,
		"Question_body": "can we update DTMF values through client SDK from backend?. if yes how?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-23T01:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "yes you can: check the docs here:\u00a0https://cloud.google.com/dialogflow/cx/docs/concept/parameter#use\n\nYou can set that in the queryParams under your response object like this:\n\nqueryParams: {\n    parameters: params\n  }"
			},
			{
				"Answer_created_time": "2023-02-23T03:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "thanks,\n\nI checked it already , but it shows DTMF API field is Not available when I try to update DTMF values, that advance speech setting options are not available in client SDK . is there any other\u00a0 way to update DTMF values from SDK?"
			},
			{
				"Answer_created_time": "2023-02-23T04:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "That is for enabling/disabling DTMF, if you just wanna set the parameter, you can pass it in the QueryParams object:\u00a0https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/QueryParameters#FIELDS.payload"
			}
		]
	},
	{
		"Question_title": "Translate service error - Unsupported language pair",
		"Question_created_time": "2022-08-25T04:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Translate-service-error-Unsupported-language-pair/td-p/459774/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 14,
		"Question_score_count": 3,
		"Question_view_count": 700,
		"Question_body": "Our application started to have some strange error from 25th of August which was working properly until today. Some very basic translation requests get the \"Status(StatusCode=\"InvalidArgument\", Detail=\"Unsupported language pair.\" error. For example the words \"loan\", \"excellent\", \"wonderful\" get the errors from service. I checked the release notes of the service but found nothing. Could you please help about the issue?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-25T05:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We get the same error for order numbers ABC123 for example will return \"Unsupported language pair.\" But\u00a0ABC1234 works, ABC12 works, ABC123 return error, ABC1233 return error."
			},
			{
				"Answer_created_time": "2022-08-25T23:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Our company also has the same problem unfortunately, one of our internal tools basically cannot be used at all since yesterday."
			},
			{
				"Answer_created_time": "2022-08-26T00:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We have a similar problem\n\ncode = InvalidArgument desc = Unsupported language pair."
			},
			{
				"Answer_created_time": "2022-08-26T06:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We are having similar problems with specific language pairs."
			},
			{
				"Answer_created_time": "2022-08-26T09:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We're having this issue since August, 24 at 01:07 AM.\n\nGoogle.GoogleApiException: Google.Apis.Requests.RequestError\nBad language pair: {0} [400]\nErrors [\n    Message[Bad language pair: {0}] Location[ - ] Reason[badRequest] Domain[global]\n]\nat Google.Apis.Requests.ClientServiceRequest`1.ParseResponse(HttpResponseMessage response)\nat Google.Apis.Requests.ClientServiceRequest`1.ExecuteAsync(CancellationToken cancellationToken)\nat Google.Cloud.Translation.V2.TranslationClientImpl.TranslateHtmlAsync(IEnumerable`1 htmlItems, String targetLanguage, String sourceLanguage, Nullable`1 model, CancellationToken cancellationToken)"
			},
			{
				"Answer_created_time": "2022-08-29T12:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Has this been corrected?"
			},
			{
				"Answer_created_time": "2022-08-29T22:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I believe so, I tested yesterday and had no issues."
			},
			{
				"Answer_created_time": "2022-09-01T03:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The same error!\n\nExample of translated text: \"\u041c\u0430\u0441\u043b\u043e MITASU 5W30 PLATINUM PAO SN Dexos2 1L\"\n\nTranslation to Romanian from autodetect using NeuralMachineTranslation.\n\nif the word \"PAO\" is removed, it translate ok."
			},
			{
				"Answer_created_time": "2022-09-07T06:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It has been fixed. Thanks"
			},
			{
				"Answer_created_time": "2022-09-07T21:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Issue still exist for English to Romanian translation. appreciate any thoughts."
			},
			{
				"Answer_created_time": "2022-09-09T01:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Unfortunately we started to get same exception (Unsupported language pair.)\u00a0 with the following inputs while translating to English (en-US) Is there any new deployment to the service? Could you please check?\n\nSome Sample Problematic Inputs :\u00a0\"Ok\" , \"1000\", \"wonderful\""
			},
			{
				"Answer_created_time": "2022-09-09T12:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Unclear if it's related, but a well known Trados plug-in for Google AutoML machine translation engine is also failing with the same error and this is also new.\u00a0 See\u00a0https://community.rws.com/product-groups/trados-portfolio/rws-appstore/f/rws-appstore/43110/mt-enhan....\u00a0 \u00a0This is with V3 Advanced API, but looks like what is reported her is happening with V2 Basic API."
			},
			{
				"Answer_created_time": "2022-09-13T13:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We found a solution for this issue.\n\nFor OpenSay, when we used Cloud Translation's REST API's analyzeText method with an English content and a target language type 'en-US' it failed with \"Unsupported language pair.\".\u00a0\n\nIt took some tinkering, but eventually we changed the 'en-US' to 'en' and it worked."
			},
			{
				"Answer_created_time": "2023-02-23T04:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My\u00a0Translate.How\u00a0\u00a0app working good. Some times i have getting api errors in v2."
			}
		]
	},
	{
		"Question_title": "Duplication of intents and entities while importing an updated version of a pre-existing flow of sam",
		"Question_created_time": "2023-02-22T04:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Duplication-of-intents-and-entities-while-importing-an-updated/td-p/524979/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 66,
		"Question_body": "With Feb  06 2023  release dialogflow cx introduced the functionality ; in which while importing a flow a person was given a choice if he/she wants to keep the original resources or replace the existing resources ,  \nthe link to release note - ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-23T01:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "you can use the CX CLI to achieve that:\u00a0https://cxcli.xavidop.me/cmd/cxcli/\n\nFor entities:\n\ncxcli entity-type update pokemon --entities \"pikachu@25|pika,charmander@3\" --agent-name test-agent --project-id test-cx-346408 --location-id us-central1\n\nFor intents:\n\ncxcli intent update test_intent --training-phrases \"hello, hi how are you today@sys.date, morning\"  --agent-name test-agent --project-id test-cx-346408 --location-id us-central1"
			},
			{
				"Answer_created_time": "2023-02-23T01:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can I import a specific flow using this CLI tool?"
			},
			{
				"Answer_created_time": "2023-02-23T01:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Not yet!\n\nI can add this to the tool in the upcoming weeks and let you know!"
			},
			{
				"Answer_created_time": "2023-02-23T01:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you so much ,\nBut please look out for the functionality in which we can replace the original resources that exists in dialogflow with the resources of flow that we are importing .\n(introduced in Feb 06 release of df)\n\nThanks again your replies are appreciated \ud83d\ude42"
			},
			{
				"Answer_created_time": "2023-02-23T01:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sure thing!\n\nI will keep you posted!"
			}
		]
	},
	{
		"Question_title": "capturing Parameter advanced speech settings from pages through pagesClient",
		"Question_created_time": "2023-02-22T01:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/capturing-Parameter-advanced-speech-settings-from-pages-through/td-p/524933/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "hello everyone,I was trying to capture parameter DTMF settings in pages from backend through pagesClient , but that client has limitation , it is unable to capture the DTMF settings. Is there any other way to capture DTMF settings from backend?reference : @https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.flows.pages#...",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-22T01:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nSo first you will need to enable Advanced Speech at Agent level, then you can create a parameter on a Page of type\u00a0@sys.number and enable the DTMF:\n\nEnabling DTMF:"
			},
			{
				"Answer_created_time": "2023-02-22T04:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "can we implement this thing through client SDK"
			},
			{
				"Answer_created_time": "2023-02-23T01:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "what do you mean?"
			}
		]
	},
	{
		"Question_title": "Dialogflow",
		"Question_created_time": "2023-02-21T00:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow/td-p/524405/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 81,
		"Question_body": "Hi, is it possible to localise my bot to a specific group of users (Maybe IP) when I integrate on the web?This is for the Dialogflow ES bot",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-21T23:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nThis is not natively possible. You will need to do that on your own! if your chatbot is a webapp, you can add that IP whitelist there."
			},
			{
				"Answer_created_time": "2023-02-22T14:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for your response. Can you share a sample snippet, please?"
			},
			{
				"Answer_created_time": "2023-02-23T01:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I mean, this is not a snippet that can be shared. This is part of your architecture and your netwrking"
			}
		]
	},
	{
		"Question_title": "Time taking to take A Job in a Cluster on GCP DataProc",
		"Question_created_time": "2023-02-21T13:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Time-taking-to-take-A-Job-in-a-Cluster-on-GCP-DataProc/td-p/524763/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 30,
		"Question_body": "Dear Googlians,There was a problem relevant to the time taken to create a cluster on GCP Dataproc. What may cause such a problem?Regards.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-22T10:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you elborate your scenario? What is your method in creating your cluster? What is your region? What typ of connectivity do you use?"
			}
		]
	},
	{
		"Question_title": "REQUEST : Google ML Kit for web",
		"Question_created_time": "2023-02-20T22:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/REQUEST-Google-ML-Kit-for-web/td-p/524375/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 86,
		"Question_body": "HiAm wondering can we have Google ML kit Vision APIs like Barcode Scanning for web. currently its only for mobile phones android and ios platform,  it will be good if we can use it on web like through JS upload image having barcode and we get scan resulthttps://developers.google.com/ml-kit/vision/barcode-scanningthanks ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-22T09:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As of now, this is not yet available and no possible workarounds available. However, you may file a feature request\u00a0https://cloud.google.com/support/docs/issue-trackers#feature_requests\u00a0for this one."
			}
		]
	},
	{
		"Question_title": "English to Chinese glossary is ignored",
		"Question_created_time": "2023-02-20T03:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/English-to-Chinese-glossary-is-ignored/td-p/524065/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 50,
		"Question_body": "I am trying to translate domain-specific English (en-US) text to Chinese simplified (zh). I have created a glossary, however, it looks like CloudTranslate gives precedence to other translations over the glossary.For example, the word \"fabric\" has many translations to Chinese, so the glossary has an entry \"the fabric,\u4ea4\u6362\u7f51\", but the API returns \"\u9762\u6599\", which is the same translation as I get from Google Translate website.Question 1: Since I didn't get any error, is it true to assume that the glossary was found and used?Question 2: I expected that the glossary will get strict precedence over other translations. Is there a way to force it?",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "When you have created your glossary, you should now then call glossary_translation instead of translation so that glossaries will override the results for the terms used in the model.\n\n\u00a0\n\nYou may refer to this documentation\u00a0https://cloud.google.com/translate/docs/advanced/glossary#use_glossaries"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks, this is the same reference that I used. As I understand it is the same function as translating without a glossary, but I need to populate the \"glossary_config\" in the request.\n\nDid I miss anything?\n\nThis is my code:\n\n\u00a0\n\n\u00a0\n\n    def my_translate(self, text):\n        response = self.client.translate_text(\n            request={\n                \"parent\": self.parent,\n                \"contents\": [text],\n                \"mime_type\": \"text/plain\",  # mime types: text/plain, text/html\n                \"source_language_code\": self.source_language,\n                \"target_language_code\": self.target_language,\n                \"glossary_config\": self.glossary_config,\n            }\n        )\n\n        # Display the translation for each input text provided\n        translated_text = \"\"\n        for translation in response.translations:\n            translated_text += translation.translated_text\n\n        return translated_text\n\n\u00a0\n\nI also tried to provide a non-valid glossary and received an exception, which means that the \"glossary_config\" is not ignored."
			}
		]
	},
	{
		"Question_title": "Vertex AI Model Deployment Error",
		"Question_created_time": "2022-12-01T05:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Model-Deployment-Error/td-p/495020/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 260,
		"Question_body": "Hi, I just got started using vertex ai with google cloud console. I am trying to deploy this mode to an endpoint. https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2 I successfully imported it into a google storage bucket and uploaded it to the model registry. However, when I attempt to deploy the model to an endpoint, I receive the following error.I have attempted to change the TensorFlow version and the folder that i import (I attempted to import the containing folder instead of the model folder) however nothing seems to help. Any suggestions would be greatly appreciated. Thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-21T15:48:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \"./efficientnet_lite0_feature-vector_2/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to /tmp/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='./efficientnet_lite0_feature-vector_2/info.meta')\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-12-02T09:36:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nCan you share the steps on how you are attempting to deploy the model to the endpoint? Or are you following any documentation to do this?"
			},
			{
				"Answer_created_time": "2023-02-15T18:36:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Have you solved this problem? I have the same problem, how did you solve it?"
			},
			{
				"Answer_created_time": "2023-02-21T15:48:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \"./efficientnet_lite0_feature-vector_2/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to /tmp/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='./efficientnet_lite0_feature-vector_2/info.meta')"
			}
		]
	},
	{
		"Question_title": "Custom container in vertex workbench",
		"Question_created_time": "2022-06-10T10:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Custom-container-in-vertex-workbench/td-p/430464/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 555,
		"Question_body": "I noticed that you can use a custom container from the container registry when creating user-managed notebook, but I couldn't find any documentation on the required configuration/dockerfile specs for it to work with jupyterlab in a similar fashion to launcing a regular workbench environment (e.g. python 3). Should I open default jupyter lab port? anything else?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-20T15:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "1. Create the initial Dockerfile and run modification commands.\n\u00a0 To start, you create a Deep Learning Containers container using one of the available image types. Then use conda, pip, or Jupyter commands to modify the\u00a0 \u00a0container image for your needs, you can add extra packages when you create your custom container.\n\nFROM gcr.io/deeplearning-platform-release/tf-gpu:latest\nRUN pip install -y tensorflow\u00a0\n\n\u00a0 \u00a02.Build and push the container image.\n\u00a0 \u00a0Build the container image, and then push it to somewhere that is accessible to your Compute Engine service account.\n\nexport PROJECT=$(gcloud config list project --format \"value(core.project)\")\ndocker build . -f Dockerfile.example -t \"gcr.io/${PROJECT}/tf-custom:latest\"\ndocker push \"gcr.io/${PROJECT}/tf-custom:latest\"\n\n\n\nSpecify the container when launching the execution Custom container."
			},
			{
				"Answer_created_time": "2023-02-21T07:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks. It'd be great if there were some clear instructions to create an image for a very common need (python > 3.7), where the current prebuilt containers are all 3.7 (including the image you gave in the example)."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX CLI",
		"Question_created_time": "2023-02-19T11:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-CLI/td-p/523858/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 1,
		"Question_view_count": 82,
		"Question_body": "hello!! I\u2019ve been building an open-source project for several months and I think is ready to go! It is a CLI to interact with Dialogflow CX, its NLU and has quite useful testing capabilities within the conversational AI field. It also has possibilities of Speech-to-text and Text-to-speech  https://cxcli.xavidop.me/\nI have written it in Go (quite a cool experience, really) Tell me what you think of it please, if you can give the repo a star, it would also be great: https://github.com/xavidop/dialogflow-cx-cliToday I just released the biggest version since I created the Dialogflow CX CLI!\nthis version includes:\n1. Create and update an agent\n2. Export and Restore an agent with the `json-package` format\n3. Create, update and delete webhooks\n4. Create, update and delete versions (under the versioning command)\n5. Create, update and delete environments\n6. Update intents. Create and update the description of an entity\n7. Update entity types. Create and update redacted entity types using `--redacted` parameter8. Create, Update, Delete and train flows!Please read the usage here: https://cxcli.xavidop.me/cmd/cxcli/You can support the project by starring or sponsoring the project: https://github.com/xavidop/dialogflow-cx-cli",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-19T11:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "hello!! I\u2019ve been building an open-source project for several months and I think is ready to go! It is a CLI to interact with Dialogflow CX, its NLU and has quite useful testing capabilities within the conversational AI field. It also has possibilities of Speech-to-text and Text-to-speech\u00a0\u00a0https://cxcli.xavidop.me/\nI have written it in Go (quite a cool experience, really) Tell me what you think of it please, if you can give the repo a star, it would also be great:\u00a0https://github.com/xavidop/dialogflow-cx-cli\n\nToday I just released the biggest version since I created the Dialogflow CX CLI!\nthis version includes:\n1. Create and update an agent\n2. Export and Restore an agent with the `json-package` format\n3. Create, update and delete webhooks\n4. Create, update and delete versions (under the versioning command)\n5. Create, update and delete environments\n6. Update intents. Create and update the description of an entity\n7. Update entity types. Create and update redacted entity types using `--redacted` parameter\n\n8. Create, Update, Delete and train flows!\n\nPlease read the usage here: https://cxcli.xavidop.me/cmd/cxcli/\n\nYou can support the project by starring or sponsoring the project: https://github.com/xavidop/dialogflow-cx-cli"
			}
		]
	},
	{
		"Question_title": "Dialog flow cx Integration Twilio",
		"Question_created_time": "2023-02-11T08:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialog-flow-cx-Integration-Twilio/td-p/521061/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 97,
		"Question_body": "Dialog flow CX having problems in integration twilio options is no more visible.\nPreviously I was able to change/update the integration easily till 11-02-2023.Now it just doesn't show any option for TWILIO.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-13T16:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI have investigated your case and found out that Twilio is not also available at my end and in other users/projects that I checked. I consulted the internal team if there are any unreleased notes or bug issues regarding Twilio integration in Dialogflow CX.\n\nI will keep you posted.\n\nThanks!"
			},
			{
				"Answer_created_time": "2023-02-17T15:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThe issue regarding Twilio has been resolved and it is now available in Dialogflow CX again.\n\nThank you!"
			},
			{
				"Answer_created_time": "2023-02-19T11:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Friday Dialogflow CX was very buggy! it is available again!"
			}
		]
	},
	{
		"Question_title": "Return a list in Dialogflow CX",
		"Question_created_time": "2023-02-15T04:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Return-a-list-in-Dialogflow-CX/td-p/522398/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 82,
		"Question_body": "Hi guys how do you return a list in fulfillment in Dialogflow CX",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-16T12:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Looking at the documentation I did not see any fulfilment list but I stumbled upon an HTTP request on retrieving pages on a specific flow.\u00a0 Here:\u00a0https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.flows.pages/....\n\nThe result is not a list but is on json format.\u00a0 Snippet of result:"
			}
		]
	},
	{
		"Question_title": "Agent Assist Summarization Model Fail to Deploy",
		"Question_created_time": "2023-02-13T10:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Agent-Assist-Summarization-Model-Fail-to-Deploy/td-p/521639/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 7,
		"Question_score_count": 0,
		"Question_view_count": 177,
		"Question_body": "Hello, I attempted to create a custom model in agent assist using the public summarization dataset (gs://summarization_integration_test_data/data/*). The model finished training, but it fails to deploy. The only error message I receive is \"operation failed\". What might be causing this issue/is there away to see agent assist logs? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-14T11:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you share the documentation you followed when you create, train and deploy your custom model for agent assist?"
			},
			{
				"Answer_created_time": "2023-02-14T11:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "https://cloud.google.com/agent-assist/docs/summarization-console\n\nAfter the model finishes training, \"undeployed\" status appears. I then press the \"deploy\" button where eventually I receive the failed status."
			},
			{
				"Answer_created_time": "2023-02-14T14:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you try following this\u00a0https://cloud.google.com/agent-assist/docs/summarization\u00a0?"
			},
			{
				"Answer_created_time": "2023-02-15T08:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have attempted this. I'm making it to the same deployment stage and still having issues. I call the deployconversationmodels api with the following command:\n\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ \"https://dialogflow.googleapis.com/v2/projects/*/conversationModels/*:deploy\"\n\nand I receive the response:\n\"curl: (3) URL using bad/illegal format or missing URL\"\n\nI'm not positive that I'm formatting the URL correctly, but here is a link to the API page.\u00a0\nhttps://cloud.google.com/dialogflow/es/docs/reference/rest/v2/projects.conversationModels/deploy"
			},
			{
				"Answer_created_time": "2023-02-15T13:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you try doing below for your updated URL on your POST request:\n\nhttps://dialogflow.googleapis.com/v2/projects/<your-projectID>/conversationModels/<your-conversationModelID>:deploy"
			},
			{
				"Answer_created_time": "2023-02-15T13:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes, same response."
			},
			{
				"Answer_created_time": "2023-02-15T14:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You\u00a0 may reach out to\u00a0https://cloud.google.com/support\u00a0for better troubleshooting steps so that your project can be inspected if there are issues/bugs on your end."
			}
		]
	},
	{
		"Question_title": "Document AI don't recognize parent label area correctly, and does it only on per line basis",
		"Question_created_time": "2023-02-11T10:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-don-t-recognize-parent-label-area-correctly-and-does/td-p/521091/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 85,
		"Question_body": "Good Day, I have an issue with Document AI. When I try to create Parent label, with child labels in it, it does not recognize whole area of parent label correctly, and only recongizes it on per line basis, with separate label for each. Can it be done somehow that it recognize both lines in one parent label insteand of dividing it? \n\nHere is example how AI labeled it.Below I have provided example on how I labeled the \"Training/Testing\" datasets",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-13T15:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I would suggest creating a support case for this specially if the quality of the result is not as expected, Engineers will have better visibility with the model and data in the project to proceed with a investigation.\n\nhttps://cloud.google.com/contact"
			}
		]
	},
	{
		"Question_title": "Unprocessed or partial text in a few images",
		"Question_created_time": "2023-02-12T02:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unprocessed-or-partial-text-in-a-few-images/td-p/521178/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "I attached a few images that are a bit problematic for vision api, note that i'll keep updating this topic with more images and their area of problem if there are.Description for each image is in its caption.the string \"omerat\" not being pickedthe string \"jEWONDER\" is being picked with an upper-case \"J\" ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-12T08:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sadly, I'm not a Vertex AI Image specialist ... but I'll throw in my 2 cents.\u00a0 When we perform text extraction from an image, we get back a structured record describing what was found.\u00a0 I believe that this is the doc.\u00a0 When we look at a record, we see that each entity has an associated \"score\" between 0 and 1 that represents the confidence/score.\u00a0 \u00a0If the score is high, Vertex AI is claiming that it is more confident in the result ... if low, then Vertex AI has less confidence in the result.\u00a0 You started your post with the notion that Vertex AI is correct 99.9% of the time.\u00a0 From my perspective, that feels good.\u00a0 We usually think of software as being correct 100% of the time ... 1 + 1 had always better equal exactly 2 ... however I'm getting the sense that in your design, you are \"grabbing\" screen images, converting them to image format and then passing those image formats through Vertex AI to extract information.\u00a0 \u00a0I liken this to the notion of instead of sending someone a spreadsheet of data, we are printing out the spreadsheet, taking a photocopy, sending it via fax and then asking someone to re-enter the data.\u00a0 Transcription errors will occur.\n\nIn Vertex AI, the score value is typically used to direct extracted data to a secondary process for resolution.\u00a0 For example, if this were an \"invoice\" being processed, we had better be sure that the amounts are as accurate as possible.\u00a0 If a recognition score were low, we might send that to a human to review and correct if necessary while allowing good scored data (say 97% or above) to be passed through without human interaction."
			},
			{
				"Answer_created_time": "2023-02-12T10:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm dealing with 100s of images like these on a daily basis and the Vision api does an excellent job in terms of accuracy. I just wanted to let the team know about these two images in order to train the model to be more accurate so I can prevent such glitch in the future. But it's very rare to get these type of mistakes, that's all."
			},
			{
				"Answer_created_time": "2023-02-12T13:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you so much for the report.\u00a0 I'm sure members from the Google Vision AI teams monitor this community.\u00a0 Getting feedback on images that don't interpret correctly is likely going to be excellent data that can be used to improve the recognition in the future.\u00a0 Again, thank you so much for taking the time to report your findings.\u00a0 Very much appreciated."
			}
		]
	},
	{
		"Question_title": "Adding Dialogflow es library to python virtualenv",
		"Question_created_time": "2023-02-09T14:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Adding-Dialogflow-es-library-to-python-virtualenv/td-p/520568/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 72,
		"Question_body": "First, thank you to those that contribute to this forum.I am having a hard time trying to get the library imported into my python environment. I've tried all different methods and nothing seems to work.I've upgraded my environment. My python version is 3.11. I have isolated the environment in its own virtualenv. I've tried uninstalling/reinstalling. I attempted the different imports.I would greatly appreciate any direction, links, articles, kind words. Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-12T12:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for your reply. I had a few things off. I needed to install the SDK and write the following into my code:\u00a0 I was using the first two (commented out with #) and they didn't work perhaps because they were deprecated. Once I did this, I was able to connect to the agent.\n\n# import dialogflow_v2 as dialogflow\n# import google_cloud_dialogflow as dialogflow\nfrom google.cloud import dialogflow_v2beta1 as dialogflow\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-10T21:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Are you done installing the actual dialogflow package? You may refer to this\u00a0https://pypi.org/project/dialogflow/"
			},
			{
				"Answer_created_time": "2023-02-12T12:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for your reply. I had a few things off. I needed to install the SDK and write the following into my code:\u00a0 I was using the first two (commented out with #) and they didn't work perhaps because they were deprecated. Once I did this, I was able to connect to the agent.\n\n# import dialogflow_v2 as dialogflow\n# import google_cloud_dialogflow as dialogflow\nfrom google.cloud import dialogflow_v2beta1 as dialogflow"
			}
		]
	},
	{
		"Question_title": "Internal error while trying to import csv for object detection AutoML",
		"Question_created_time": "2023-02-08T23:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Internal-error-while-trying-to-import-csv-for-object-detection/td-p/520214/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 51,
		"Question_body": "I have created a csv according to the format required by gcp for object detection. there are about 50,000 images in my folder. when i try to import the csv for object detection, it is giving me an internal error.  i have cross checked everything, and everything seems right. can someone help me with why this is happening?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-10T15:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThe internal error that you are experiencing must be because AutoML Object Detection has been deprecated. You can migrate to Vertex AI and new features for Vertex AI Object Detection are available there.\n\nThanks!"
			}
		]
	},
	{
		"Question_title": "Detailed Vision API usage report",
		"Question_created_time": "2023-02-10T02:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Detailed-Vision-API-usage-report/td-p/520704/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 39,
		"Question_body": "Hello,We would need help to get list of Vision API hits with source IPs as there are no standard reports available for the same GCP consoleIf some one can share details or ways to extract those data, it will be great help  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-10T14:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nYou can use Cloud Audit Logs for fetching your usage report of Cloud Vision API. It can help you access and monitor administrative activities."
			}
		]
	},
	{
		"Question_title": "Unable to get auto speech adaption to work",
		"Question_created_time": "2023-02-07T12:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-get-auto-speech-adaption-to-work/td-p/519495/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 59,
		"Question_body": "(this is actually Dialogflow ES, wouldn't let me label it that way) Trying to get dialogflow auto speech adaption to take a 9 digit number consistently when being spoken.  But no matter what I do it keeps trying to convert what I say to a 10 digit phone number (adding in extra numbers).  I am speaking as slow as I possibly can clearly delineating the numbers, yet it'll add an extra number randomly to make it a phone number.  Any assistance would be helpful.I have a relatively empty dialogflow agent (enabled for ES/speech adaption/all the right settings turned on). with a single intent The single intent is set to highest priority, no input contexts, The NineDigitNumber entity is set up as a parameter that is marked as required, no matter what I do a number like 200733201 is getting turned into 2007-332-201.  I've tried using input contexts, spot filling, tried using synonyms to define (2007) as a prefix entity and 0-9 as \"number\" entities and do \"@prefix @number @number\" etc, nothing seems to work.  I have another entity where it starts R12345678 and that one works fine, i just can't get dialogflow to just stop turning 9 digit numbers into phone numbers.  Any ideas would be appreciated.  Even if there was a way to use CSTT rather than Dialogflow to do the transcription is acceptable, but needing to get the engine to preference a 9 digit response.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-08T13:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You may try using\u00a0@sys.number-sequence\u00a0as shown below on my test intent.\n\nThis is the my output during my testing using speech adaptation:\n\n\u00a0 \u00a0\n\n\u00a0\n\nYou may refer to this\u00a0documentation\u00a0for more information."
			},
			{
				"Answer_created_time": "2023-02-10T07:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My issue is that i'm also doing alpha numeric, so I want the system to be aware that I want a specific length, and want possibly HA200733201.\u00a0 And want it to only recognize exactly 9 digits no more no less......unless it begins with an R then 8 digits...."
			}
		]
	},
	{
		"Question_title": "How do I delete all conversations in the Dialogflow history?",
		"Question_created_time": "2023-02-08T08:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-do-I-delete-all-conversations-in-the-Dialogflow-history/td-p/519907/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 105,
		"Question_body": "Hello, I know that you can delete conversations through the Dialogflow ES user interface, as described here: https://cloud.google.com/dialogflow/es/docs/history. However, it's only possible to delete individual conversations. I would like to delete all conversations. Is there a way to accomplish this via API?Thanks for your help",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-09T23:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "According to the current API[1], bulk delete is not supported. Meanwhile, there is already a feature request filed for this case scenario. But, you can try these alternative steps to delete conversation history in your Dialogflow:\n\n1. Backup your Dialogflow agent by exporting it.\n2. Delete your agent.\n3. Restore your backup agent in the same project.\n4. Disable interaction logs in agent settings.\n\nNote: Disabling interaction logs will prevent logging of new logs and the deletion of agent can only affect the logs on the history page.\n\n[1]\u00a0https://cloud.google.com/dialogflow/priv/docs/insights/reference/rest/v1alpha1/projects.locations.co...\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-09T23:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "According to the current API[1], bulk delete is not supported. Meanwhile, there is already a feature request filed for this case scenario. But, you can try these alternative steps to delete conversation history in your Dialogflow:\n\n1. Backup your Dialogflow agent by exporting it.\n2. Delete your agent.\n3. Restore your backup agent in the same project.\n4. Disable interaction logs in agent settings.\n\nNote: Disabling interaction logs will prevent logging of new logs and the deletion of agent can only affect the logs on the history page.\n\n[1]\u00a0https://cloud.google.com/dialogflow/priv/docs/insights/reference/rest/v1alpha1/projects.locations.co..."
			}
		]
	},
	{
		"Question_title": "How do I list all the image URIs from a bucket to create a csv for classification automl model?",
		"Question_created_time": "2023-02-08T21:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-do-I-list-all-the-image-URIs-from-a-bucket-to-create-a-csv/td-p/520177/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 41,
		"Question_body": "I have a bucket which has multiple folders with images in them. I want to make a csv for image classification with [ML_USE],GCS_FILE_PATH,[LABEL] format. how do i get the gcs file path for it?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-09T10:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There is no built in way or straight forward way to get a GCS filepath/URI of different objects inside different folders.\n\nPossible work around is:\n\nUse this List Objects\u00a0(I suggest to use Client Libraries for more flexible usage). On the returned JSON, you may extract the different \"your-folder/filename\" on the \"name\" key.\nOnce you get all the path of folder+filename, you may now concat is to the bucket name.\n\neg. \"my-bucket/\"\u00a0 to concat with \"your-folder/filename\" will have you \"my-bucket/your-folder/filename\" through your code. Or if you want to transform it into a URI, you may concat \"gs://my-bucket/\" to all of your\u00a0\"your-folder/filename\".\n\nThen"
			}
		]
	},
	{
		"Question_title": "Google Vision, Text detection, bug : ImportError: cannot import name 'cygrpc' from 'grpc._cython'",
		"Question_created_time": "2023-02-08T06:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Vision-Text-detection-bug-ImportError-cannot-import-name/td-p/519815/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 120,
		"Question_body": "Hello,When running recommanded code for text detection on page D\u00e9tecter le texte dans les images  |  API Cloud Vision  |  Google Cloud :  ... I get following error:  I found somewhere that it might be my python version. So, I changed from Python310 to Python377. But nothing changed.Would you have any idea?Kind regards.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-09T10:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I believe it has something to do with your python dependencies, Can you run this command in your library ? ```sudo pip3 install -U grpcio```"
			}
		]
	},
	{
		"Question_title": "Cloud Vision API is super slow today",
		"Question_created_time": "2023-02-08T08:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Vision-API-is-super-slow-today/td-p/519864/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 103,
		"Question_body": "Facing slowness with google vision api. We have only exhausted 50% of the quota, and dont see irregular usage.\nBut we see that api thoughput have reduced from regular 40rps to 9rps and the latency of 99% percentile have increased from 1s to 17s. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-09T10:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Since last week and until today, there was no reported outage on the Cloud Vision API in all regions. You may contact\u00a0https://cloud.google.com/support\u00a0for better troubleshooting so that your project can be inspected."
			}
		]
	},
	{
		"Question_title": "reCAPTCHA IA",
		"Question_created_time": "2023-02-07T12:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/reCAPTCHA-IA/td-p/519498/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Hello, Captcha has an artificial intelligence model that allows it to learn to improve security scores and thus know when it asks for validation or not? my question is related to a user who always accesses from the same pc and the same ip to a site that has reCAPTCHA validation and always asks for validation, is it possible to avoid this?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-08T13:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello leonardom80,\n\nAs per this non-Google documentation, use a VPN.\u00a0VPN locations allow you to legitimately bypass Google\u2019s ReCAPTCHA roadblocks. For the best results, choose a\u00a0well-known VPN service\u00a0instead of a free VPN which would arrive\u00a0with its own set of problems. Good VPNs disguise your traffic, protect your device details and don\u2019t record logs."
			}
		]
	},
	{
		"Question_title": "How to organize intents/pages for a non-service/support application",
		"Question_created_time": "2022-11-21T04:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-organize-intents-pages-for-a-non-service-support/td-p/491306/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 108,
		"Question_body": "I am a new Dialogflow user and I need to create an agent for an application that is not service or support oriented.  The application is educational and has a large collection of questions and answers (1,000s) with no concrete conclusion (for example,  to renew a driver's license).  For the POC I did in Watson I was able to use folders to organize sub-topics.  What is the best way to group intents and responses by topic and sub-topic (for example, President Lincoln's early life President Lincoln's career)?  I expect it would be difficult to manage a list of 1,000s of pages in the left pane.  Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-22T09:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@NoankMary, are you using Dialogflow CX or ES?"
			},
			{
				"Answer_created_time": "2023-02-08T10:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dialogflow ES."
			}
		]
	},
	{
		"Question_title": "Vertex AI Datasets & batch predictions",
		"Question_created_time": "2022-07-24T13:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Datasets-amp-batch-predictions/td-p/446346/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 366,
		"Question_body": "Hi all,I just started to play with Vartex AI. I am working with \"Tabular\" - \"Forecasting\" and currently struggling with few things and i hope you can help me in order i can continue. I tried to organize my questions to three categories:1) Datasets for training:      a) \"series identifier\" define to which time series data are belonging ... lets assume that i have two series identifier - one is called \"A\" and one is called \"B\". Does this means that AI treats them as completely separate and noncorrelated - this means any data that belongs to series A don't have any correlation to B, right? This give me possibility to train different dataset with one shot right? Otherwise i would need to make (in my case) two trainings - one for A and one for B.2) Training new model --> Model detailsa) Is possible to predict more then one target column? b) lets assume that my dataset data granularity is 1 day. Can i use data granularity of \"5min\" for Forecast configuration or can this setup decrease quality of my forecast? Should it be more correct to use already at beginning lets say dataset granularity of 5 minute and afterwards it could be more flexible when setting data granularity for forecast configuration without influencing forecast quality?c) If I set Forecast horizon of 7 and context window 30, does this means that this setting limit my forecast to maximum 7 time steps and requesting always exactly 30 time steps of historical data as input when forecasting on existing trained model?3) Batch predictionsa) Batch Source fileLets assume that i have data with 15 columns from which one is \"serial identifier\" , one is \"time step\" - actually date and one of those columns is target column. Rest of 12 columns are used as influencer and used to train my model. I know that i need to have same structure for batch source file - i read that i can use same file as i used for training, but i just need to add in my case a 2x7 new rows with adding 7 dates and serial identification (in my case 7x A and 7xB) and target column need to be empty. But what should i do with data of rest 12 columns? Do i need manually to enter data for those new rows (2x7) of those 12 columns which values should be for future? But what if i don't have those data? Does this means i cannot do prediction?I tried to make prediction without those future data and i got following message:\"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\"I hope you can help me with above questions. Tnx in advance! Regards, Arny ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-01T09:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "A)\nYes, that's correct you can train different datasets with the same shot (only if there is a specific column that differentiates them). Follow this document\u00a0for best practices.\n\nB)\nNo, for datasets that train AutoML models, one column must be the target, and there must be at least one feature available to train the model.\n\nWhen you train a forecasting model, you specify the data granularity, or the time interval between the training data rows. It can be hourly, daily, weekly, monthly, or yearly. In addition, it can be every 1, 5, 10, 15, or 30 minutes.\u00a0 Vertex AI treats the interim day as missing data, which can degrade model performance.\n\nC)\nFor this error what you could do is to copy the table into another and set the value to NULL to the other columns or set the data values manually."
			},
			{
				"Answer_created_time": "2023-02-05T04:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0josegutierrez,\n\nCan you please elaborate on your \"C\" answer. I am bit stuck at this error \"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\".\u00a0Also can you please include an example too!!\u00a0\n\nThank You,\n\nOm"
			},
			{
				"Answer_created_time": "2023-02-06T07:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm also stuck on the same error. The target column is empty for every row in my forecast horizon, so I'm really at a loss. I've posted my own question about this here:\n\nAutoML forecast model batch predictions: \"rows with non-empty target values\" error\n\n@om_shah\u00a0Not sure if this helps you, but in response to your asking for an example, you might want to check out this guide if you haven't already. It includes sample data for requesting batch predictions.\n(Again, not sure how much this helps, because in my case I'm seeing the error message even though my target column is, in fact, empty where I expect the model to make predictions.)"
			},
			{
				"Answer_created_time": "2023-02-08T08:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0lambdalove,\n\nYes!! I have checked your post and the guide. I have repeated same steps from the guide but no luck. Waiting for the answer."
			},
			{
				"Answer_created_time": "2023-02-08T08:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@josegutierrez, @arny\u00a0,\n\nWaiting for your reply. I am having an urgency to complete this.\u00a0\n\nThank You,\u00a0\n\nOm"
			}
		]
	},
	{
		"Question_title": "Usage of spot machines while training in Vertex AI",
		"Question_created_time": "2023-01-19T11:52:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Usage-of-spot-machines-while-training-in-Vertex-AI/td-p/511862/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 355,
		"Question_body": "Hello GCP community, I have the following question, I am training in Vertex using a custom container, I am porting pipelines that were in Kubeflow to vertex and using this to train:It looks ok, but here is my question is there anyway in which I can do the training but in a SPOT machine to try to reduce my training costs.\n\nThanks! ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-19T12:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi David\n\nNo unfortunately there is no support for spot / preemptible instances with Vertex AI.\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-19T12:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi David\n\nNo unfortunately there is no support for spot / preemptible instances with Vertex AI."
			},
			{
				"Answer_created_time": "2023-02-08T06:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Is there a way to attach persistent disks when submitting a custom training job using Vertex AI?"
			}
		]
	},
	{
		"Question_title": "Introducing Bard: An experimental conversational AI service",
		"Question_created_time": "2023-02-06T13:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Introducing-Bard-An-experimental-conversational-AI-service/td-p/518901/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 2,
		"Question_view_count": 910,
		"Question_body": "Have you heard about Bard? Announced today, Bard is an experimental conversational AI service powered by Google's latest version of LaMDA (Language Model for Dialogue Applications), which draws from the web to provide fresh, high-quality responses. Learn more here and let us know your thoughts in the comments. We'd love to hear from you! ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T22:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Well, where is it? Let us test it.."
			},
			{
				"Answer_created_time": "2023-02-06T23:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For sure"
			},
			{
				"Answer_created_time": "2023-02-06T23:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Would love to test it! How can I?"
			},
			{
				"Answer_created_time": "2023-02-07T21:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Is there an open beta with an API? \u00a0I'm surprised that there isn't a readily apparent page for Bard or even for a beta, even if the page were to say that there's no open beta at this time. \u00a0I can only assume that many developers will spend a great deal of time looking for a portal that may not exist. \u00a0In any case, congratulations on your new venture. \u00a0I'm looking forward to trying your newest chatbot."
			}
		]
	},
	{
		"Question_title": "Document AI Training Errors - JSON and Schema not in agreement",
		"Question_created_time": "2023-02-07T12:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-Training-Errors-JSON-and-Schema-not-in-agreement/td-p/519506/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 100,
		"Question_body": "Hello,I'm a novice user in the Document.AI world and while attempting to train a processor I've encountered the \"Training stopped due to errors\" message. When I investigate this error I observe sections of the JSON similar to:  The field union is set in the schema as \"optional once\" and so the metadata's report that 0 are found and 1 required seems off. I understand there is a UI bug currently being investigated regarding these text_segments errors but it's unclear if I can work around this. I've been at a standstill for a week now, and shy of only identifying a single field per image it's not obvious to me what I'm doing on these particular records that is causing the error to appear.Some of my identified fields overlap, which someone suggested could be the cause, however, the handwriting does overflow the typical fields and sometimes overlaps, so guidance would be appreciated. Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-07T20:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It appears there is a similar report here:\n\nhttps://issuetracker.google.com/267366576\n\nReading that record provided by Google support, they say that the issue has been forwarded to Document AI Engineering for investigation.\u00a0 \u00a0I also note that the Train and evaluate processors feature of Document AI is flagged is Pre-GA.\u00a0 This typically means that there may be issues.\u00a0 If you are an enterprise customer, I'd suggest reaching out to your Google sales rep or your Google customer engineer.\u00a0 Explain the situation to them.\u00a0 If you have an NDA in place with Google, they will likely be able to share with you the roadmap for expected General Availability of this service.\n\nYou might consider using Google Issue Tracker and searching for similar issues and saying \"me too\" or else raise your own ticket.\u00a0 The clearer your description and steps to reproduce the better (should you raise your own ticket).\u00a0 If you have a full recreate story that can be posted which does\u00a0not include sensitive information the better."
			}
		]
	},
	{
		"Question_title": "Vertex AI: Workbench - Failing to Created a schedule based recurrent execution",
		"Question_created_time": "2022-12-12T23:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Workbench-Failing-to-Created-a-schedule-based/td-p/498980/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 8,
		"Question_score_count": 0,
		"Question_view_count": 249,
		"Question_body": "Hello,I am trying to create a schedule-based recurrent execution via the Vertex AI workbench. It fails with error code 2- unknown error. Attaching the error snippet. Any help to understand the root cause is highly appreciated. \nappreciated. Note: I can do standalone execution successfully. It's only when trying to create a schedule based event, this error occurs. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-13T11:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@palak_raina, may I know if this is under managed or user-managed notebook?"
			},
			{
				"Answer_created_time": "2022-12-13T19:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is under managed notebook."
			},
			{
				"Answer_created_time": "2022-12-23T02:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Any further inputs on this?"
			},
			{
				"Answer_created_time": "2022-12-26T09:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Apologies for the late response, as this would be hard to reproduce on my end without access to your project, I suggest you raise a public issue tracker. By raising a PIT, an engineer working on the product can check your issue thoroughly."
			},
			{
				"Answer_created_time": "2023-02-07T01:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Same error here under exactly same conditions. Did u manage to fix it?"
			},
			{
				"Answer_created_time": "2023-02-07T01:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\nNo, I wasn't able to fix it. I also took a different approach to scheduling the pipeline run, so I didn't go ahead and raise PIT as suggested earlier."
			},
			{
				"Answer_created_time": "2023-02-07T01:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hey palak,\n\nWow that was a quick reply!\u00a0\n\nWhich approach are u using now?\u00a0\n\nI raised a PIT, lets hope for a quick fix..."
			},
			{
				"Answer_created_time": "2023-02-07T02:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks! Please post here once you find a solution anyway.\nI am using Airflow to schedule the pipeline."
			}
		]
	},
	{
		"Question_title": "Vertex AI not training custom jobs in batches larger than 20?",
		"Question_created_time": "2023-02-05T11:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-not-training-custom-jobs-in-batches-larger-than-20/td-p/518322/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 240,
		"Question_body": "Whenever I try to train more than 20 jobs (custom containers) in parallel it seems like the jobs enter a queue such that only 20 run at one time, my quotas should allow me to do much more (cpu and vertex ai api way under limit, same with all my compute stuff).Is there any way to see why those jobs aren't starting instantly or to increase the number of custom jobs running at one time? I don't even see them in pending but when I submit them all using sync=False in the workbench they get submitted with no error instantly.I also tried to throttle the start of each training job incase its a per min request limit thats causing the limit of 20 parallel jobs but that didnt solve it.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T19:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There could be a possible misconfiguration during your QUIR (Quota Increase Request). You may contact\u00a0https://cloud.google.com/support\u00a0so that your project may be inspected and your previous QIR as well to check if there are needed adjustments on your project or if there were misconfiguration on the QIR."
			}
		]
	},
	{
		"Question_title": "GOOGLE VISION DETECT PAGE NUMBER PDF",
		"Question_created_time": "2023-02-06T01:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GOOGLE-VISION-DETECT-PAGE-NUMBER-PDF/td-p/518515/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 135,
		"Question_body": "Hi AllThe filename will be output-x-to-y, where x and y represent the PDF/TIFF page numbers included in that output file.\nHow to get page numbers or detect it? I don't know how to recognize the number of pages in a PDF when it is used config in Output Config. I've tried but I can't search it. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T02:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Tuongg\u00a0\n\nNumber of pages grouped for output files is defined in batch_size variable.\n\nnow here\u00a0Since we specified batch_size=2, the first response contains, the first two pages of the input file."
			},
			{
				"Answer_created_time": "2023-02-06T08:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank for your reply. But I don't understand problem.\n\nIf case is continuously detect 2 PDF files in it contain (with 1 file 1 page and 1 file 2 pages) to Storage and then bucket will contain 2 file output:\n\n\u00a0\n\nSo how to recognize which output file is updated later?\n\nBecause\u00a0filename will be output-x-to-y by number page, so I want to detect number page before file is output to\u00a0recognize\u00a0files detect later?\n\nThanks"
			},
			{
				"Answer_created_time": "2023-02-06T13:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What are you wanting to extract/detect from your processing of input documents?\u00a0 I'm curious if it is really Vision API you are wanting to use or text extraction (Document AI)?"
			},
			{
				"Answer_created_time": "2023-02-06T17:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@kolban\u00a0wrote:\n\n\nI'm curious if it is really Vision API you are wanting to use or text extraction\n\nThank you for your attention to this matter.\n\nYes, I've used Vision API but I'm not clearly about it.\n\nMy problem is when I have input as pdf array with different number of pages then make request completed the output has been written to GCS with name (x - to -y).ex:\n\nBut when I list all output files I don't know which file was written to last.\nSo I want to extract/detect that through the page number of the pdf before it was written in the output.\n\nI also have research to get the last modified on GCS but I can't find it. It might be a bad idea, but I've been stuck at this problem for too long. Can you help me come up with a solution?"
			}
		]
	},
	{
		"Question_title": "Exploring General-AI-Like Functionality in Dialogflow and Google Cloud",
		"Question_created_time": "2023-02-03T13:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Exploring-General-AI-Like-Functionality-in-Dialogflow-and-Google/td-p/517876/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 107,
		"Question_body": "Hello everyone,I hope this message finds you well. I'm new to this forum and have recently been exploring Dialogflow as a potential solution for my research on general-AI-like dialogue bots. I have set up an agent and connected it to Telegram via a webhook using repl.it and OpenAI's GPT-3 davinci model.However, I was curious if Google Cloud offers any similar functionality to GPT-3. I would greatly appreciate any advice or guidance on this matter.Thank you and best regards, Tom",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T13:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You might perhaps referring to Google's Lambda:\u00a0https://blog.google/technology/ai/lamda/\u00a0. But at the moment I believe there is no documentation of integrating this to dialogflow and only accessible on mobile devices only."
			},
			{
				"Answer_created_time": "2023-02-06T13:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In a Blog post just posted today (at the time of writing) that is titled:\n\nAn important next step on our AI journey\n\nThis comes directly from the CEO of Google.\u00a0 \u00a0I would suggest that we all tune in to this blog and related announcements.\u00a0 I think your question is a smidge early and what we have here is a \"breaking news\" story."
			},
			{
				"Answer_created_time": "2023-02-06T14:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for both of your responses. I will take a closer look at Lambda, as I was previously unfamiliar with it, and I will definitely check out Sundar's blog post.\n\nI appreciate your assistance.\n\nBest regards,\nTom"
			}
		]
	},
	{
		"Question_title": "Does vertex ai support simultaneous ingestion (batch) from multiple sources?",
		"Question_created_time": "2023-02-04T21:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Does-vertex-ai-support-simultaneous-ingestion-batch-from/td-p/518165/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "I read here that it supports ingestion from BigQuery and GCS, however, I was wondering if it supports simultaneous ingestions from various sources",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T11:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "At the moment, You can select datasources from BigQuery, Cloud Storage or to your local computer. You can see more details here:\u00a0https://cloud.google.com/vertex-ai/docs/featurestore/source-data"
			},
			{
				"Answer_created_time": "2023-02-06T13:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In addition to\u00a0@nceniza\u00a0response, I see in your question you were asking about simultaneous ingestion.\u00a0 I'm not seeing anything that would indicate that we couldn't ingest source data from MULTIPLE sources simultaneously.\u00a0 I'm also seeing that you can ingest data through API streaming.\u00a0 So if your data isn't already available in BQ or CMS, you appear to be able to ingest through sourcing the data yourself\u00a0 in an app and streaming it in through API."
			}
		]
	},
	{
		"Question_title": "Quota not listed in Vertex AI pipelines?",
		"Question_created_time": "2023-02-05T22:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Quota-not-listed-in-Vertex-AI-pipelines/td-p/518466/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 164,
		"Question_body": "Hi everyone I have a step in vertex ai pipelines that looks like this:yet that task is not executed due to:\n\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_nvidia_t4_gpus, cause=null; Failed to create custom job for the task.\n\nBut that quota is not listed anywhere in the quota manager, how can I enable GPU in Vertex AI pipelines? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-06T07:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I solved it, it is quite not easy to find:\n\nSo for anyone with the same problem, go to Quotas and use the following filters:\n\n\n\nHope it can help anyone\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-06T07:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I solved it, it is quite not easy to find:\n\nSo for anyone with the same problem, go to Quotas and use the following filters:\n\n\n\nHope it can help anyone"
			}
		]
	},
	{
		"Question_title": "Service account not having storage account create access",
		"Question_created_time": "2023-01-25T06:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Service-account-not-having-storage-account-create-access/td-p/513839/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 352,
		"Question_body": "Trying to connect glossary in phrase and using service account key from google for our trained MT engines. Receiving the error that the service account 'does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist).\", \"domain\": \"global\", \"reason\": \"forbidden\" } ] } }'But we have granted this account the following access in google console already:Cloud translation API editorStorage object viewerand previously:Storage object creatorlogged out of phrase, recreated the MT profile, uploaded glossary again and getting same error. Please help urgently to resolve.Thanks!Poppy",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-25T06:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Giving the service account \"Storage Object Creator\" role is needed to allow the account to create objects in a bucket.\u00a0 see\u00a0https://cloud.google.com/storage/docs/access-control/iam-roles\n\nHowever, we need to drill down on \"which\" bucket is being accessed.\u00a0 \u00a0For example, imagine we have a user called \"poppy\" and \"poppy\" is working in the project \"poppy-proj\".\u00a0 \u00a0If we give \"poppy\" the above role, then \"poppy\" can create objects in buckets OWNED by the project called \"poppy-proj\".\u00a0 \u00a0However, if the application is trying to create objects in a bucket that is NOT owned by \"poppy-proj\" then \"poppy\" would need the role added to the project which DOES own the bucket.\n\nNote ... I don't recommend adding Storage Object Creator at the project level.\u00a0 Rather, find the bucket that you want to create objects in and give \"poppy\" that role AGAINST the bucket only.\u00a0 Giving yourself the role at the project level means you have that role for ALL buckets owned by the project.\u00a0 Giving yourself that role for JUST the single bucket still allows you to work with the bucket but in the event that there was an accident, your \"blast radius\" is dramatically decreased."
			},
			{
				"Answer_created_time": "2023-02-06T03:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thank you for your reply.\u00a0\n\nWe had already previously granted the service account associated with the MT profiles 'Storage object creator' permissions and seemingly none of the IAM roles we grant the service account, are affecting the success. I still receive this error in Phrase:\u00a0does not have\u00a0storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist).\", \"domain\": \"global\", \"reason\": \"forbidden\" } ] } }'\n\nI don't fully understand the supp\u00f2rt you gave regarding the bucket. How can I check whether I am accessing and giving the correct permissions related to the correct bucket please?\n\nThanks,\n\nPoppy"
			}
		]
	},
	{
		"Question_title": "Im implementing logistic regression ,but somehow this code not seems to be working..please help me.",
		"Question_created_time": "2023-02-03T00:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Im-implementing-logistic-regression-but-somehow-this-code-not/td-p/517491/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 89,
		"Question_body": "This is the python code for logisticRegression using Preceptron Trick but not working.. im having doubt in finding coefficients..please check and correct the code.     ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-03T10:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nWelcome to Google Cloud Community!\n\nTo properly replicate your use case, can you show any error message you are having? And what is your expected output/behavior?"
			},
			{
				"Answer_created_time": "2023-02-03T21:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Look at this r2_score is coming worst negetive."
			}
		]
	},
	{
		"Question_title": "webhook timeouts debugging issue",
		"Question_created_time": "2023-02-02T06:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/webhook-timeouts-debugging-issue/td-p/516923/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 73,
		"Question_body": "Hi there,When users are interacting with our agent it will randomly timeout during the conversation. There seems to be no immediate pattern to the timeouts, and it tends to occur at different points in the conversation flow, making it hard to debug. Could anyone outline to me common reasons for agent/webhook timeouts? And any steps I can take to resolve/pinpoint the issue? Thank you in advance,Vicky",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-03T10:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThe default webhook timeout limit is 5 seconds,\u00a0 but according to your case it randomly occurs with no consistent pattern. You must be experiencing a cold start where you can reduce by setting the minimum number of instances or your Cloud Function is not starting during the timeout. If you have not tried it yet, you can try to increase your webhook timeout by going to\u00a0Manage > Webhook > Select Webhook and from there you can increase your webhook timeout (maximum is 30 seconds)."
			}
		]
	},
	{
		"Question_title": "Kubeflow pipeline fails on GKE cluster",
		"Question_created_time": "2023-02-03T08:08:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Kubeflow-pipeline-fails-on-GKE-cluster/td-p/517717/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 109,
		"Question_body": "Need to create a Kubeflow pipeline for ML use-cases on GKE cluster, currently working on recommendation. Have made the Vertex AI pipeline for the same but vertex being serverless takes time to make the containers up and then run the pipeline. We needed a platform where the kubeflow pipelines can be executed faster - especially during development the results are needed instantly so that we can change inputs or tune the model.After uploading the yaml file, creating a run for the pipeline. After the first component the file read fails to write on the given bucket with below error.FileNotFoundError: [Errno 2] No such file or directory: '/gcs/bucket/reco_v2/637705bf-381e-40fa-8597-91089e700aaf/pipeline/reco_v2/637705bf-381e-40fa-8597-91089e700aaf/get-dataframe/df_path.csv'39F0203 13:57:24.664574      18 main.go:50] Failed to execute component: exit status 140time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact /tmp/outputs/test_df_path/data\" argo=true error=\"stat /tmp/outputs/test_df_path/data: no such file or directory\"41time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact /tmp/outputs/train_df_path/data\" argo=true error=\"stat /tmp/outputs/train_df_path/data: no such file or directory\"42Error: exit status 1Tried running the sample tutorial kubeflow pipelines, they are running fine and does not throw any such file write error.We have created a cluster and integrated Kubeflow pipelines from the GCP marketplace -> https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelinesNeed to know if there is a version issue or code is not proper or anything else. Have tried running a kfp v2 code too(from tutorials online) that worked well too.The kubeflow pipeline should have worked and saved outputs to the bucket given.Please help",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-03T08:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Need to create a Kubeflow pipeline for ML use-cases on GKE cluster, currently working on recommendation.\u00a0Have made the Vertex AI pipeline for the same but vertex being serverless takes time to make the containers up and then run the pipeline.\u00a0We needed a platform where the kubeflow pipelines can be executed faster - especially during development the results are needed instantly so that we can change inputs or tune the model.\n\nAfter uploading the yaml file, creating a run for the pipeline. After the first component the file read fails to write on the given bucket with below error.\n\nFileNotFoundError: [Errno 2] No such file or directory: '/gcs/bucket/reco_v2/637705bf-381e-40fa-8597-91089e700aaf/pipeline/reco_v2/637705bf-381e-40fa-8597-91089e700aaf/get-dataframe/df_path.csv'39F0203 13:57:24.664574\u00a0 \u00a0 \u00a0 18 main.go:50] Failed to execute component: exit status 140time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact /tmp/outputs/test_df_path/data\" argo=true error=\"stat /tmp/outputs/test_df_path/data: no such file or directory\"41time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact /tmp/outputs/train_df_path/data\" argo=true error=\"stat /tmp/outputs/train_df_path/data: no such file or directory\"42Error: exit status 1\n\nTried running the sample tutorial kubeflow pipelines, they are running fine and does not throw any such file write error.\n\nWe have created a cluster and integrated Kubeflow pipelines from the GCP marketplace -> https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelines\n\nNeed to know if there is a version issue or code is not proper or anything else. Have tried running a kfp v2 code too(from tutorials online) that worked well too.\n\nThe kubeflow pipeline should have worked and saved outputs to the bucket given.\n\nPlease help"
			}
		]
	},
	{
		"Question_title": "Dialogflow cx Language Issue in Console",
		"Question_created_time": "2023-02-01T01:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-cx-Language-Issue-in-Console/td-p/516356/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 103,
		"Question_body": "\"Dialog flow CX having problems while changing language in the console. It doesn't matter how many times I change it in the console it just comes back to English(Default Language). Previously I was able to change it pretty easily till01-02-2023.Now it just gives an Error: An error has occurred during knowledge base API calls.\" @",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-02T22:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI have seen other cases regarding this issue in changing language in Dialogflow CX which also occurred on 01-02-2023 and this was categorized as a bug issue. According to the latest update regarding this, the bug has been fixed and rolled out to production.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-02-02T22:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI have seen other cases regarding this issue in changing language in Dialogflow CX which also occurred on 01-02-2023 and this was categorized as a bug issue. According to the latest update regarding this, the bug has been fixed and rolled out to production."
			}
		]
	},
	{
		"Question_title": "Agent Assist - Simulator View - no smart reply suggestion",
		"Question_created_time": "2023-02-01T05:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Agent-Assist-Simulator-View-no-smart-reply-suggestion/td-p/516396/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "I was trying to use \"smart reply\" option in the google agent assist but no suggestion were populated.I have tried \"smart reply\" using sample data with option \"I would like to try it with sample data\" which autopopulated the conversation UI as snapshot given below. After this it took good amount of time approx 10-12 hrs to train with given data. Once conversation profile was set with trained data, I tried smart reply on simulator view but was not getting any suggestion as smart reply. Please suggest where I am going wrong. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-02T13:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello abhirajgandhi,\n\nIn the conversation profile created, you can lower the confidence threshold to 0.0001 to see if the suggestion will be shown."
			},
			{
				"Answer_created_time": "2023-02-02T21:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@dionv\u00a0 - Thanks it helped to get FAQs and Articles populated however I still not see smart reply suggestion popup. Below are the snapshots which I see after recommended changes and my configuaration of conversation profile. Please have a look and let me know if I need a change in configuration."
			}
		]
	},
	{
		"Question_title": "GCP STT (language-pt-BR) getting concatenated utterances",
		"Question_created_time": "2023-02-01T00:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GCP-STT-language-pt-BR-getting-concatenated-utterances/td-p/516347/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 137,
		"Question_body": "For voice channel (telephony), pt-BR language, We are facing an issue where the two utterances are getting concatenated from GCP side (STT)As we can see in the above image of the logs,The first utterance is \"fotos\" and then the second utterance is \"sim\" but we get the concatenated output \"foto sim\" as second utterance. This happens because the \"final\" value remains \"false\",  because of which the intent is not getting recognized. We would like to get support for this scenario for this particular language.Note: This scenario is only for pt-BR language and is not replicable in other languages.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-02T13:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nWelcome to Google Cloud Community!\n\nSince your concern is specific to pt-BR\u00a0and can only happen/replicated on that specific language, you may contact\u00a0https://cloud.google.com/support. This scenario happens since each country's language is continuously evolving and has a lot of differences especially in utterances."
			}
		]
	},
	{
		"Question_title": "Personal information / security assessment for GC translation service",
		"Question_created_time": "2023-01-25T20:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Personal-information-security-assessment-for-GC-translation/td-p/514131/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 86,
		"Question_body": "Hi Team,I was searching for PIA and data retention policy for translation service. Any resources list for the same would be appreciated.Also, what process i can follow to ensure maximum confidentiality and higher security to limit the data exposure limit.Regards",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-26T08:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Not PIA but this documentation [1] might be helpful about your inquiries about Google Cloud's Translation API. There is a section on how Google protect your data when using the services, Compliances etc etc.\u00a0\n\n[1]\u00a0https://cloud.google.com/translate/data-usage"
			},
			{
				"Answer_created_time": "2023-02-02T08:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for your response.\n\nI would like to know, what process i can follow to ensure maximum confidentiality and higher security to limit the data exposure limit.\n\nRegards\n\nNitish"
			}
		]
	},
	{
		"Question_title": "Custom Training Job with custom container failed with error 'Cant find specification for module...'",
		"Question_created_time": "2023-02-01T22:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Custom-Training-Job-with-custom-container-failed-with-error-Cant/td-p/516766/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 71,
		"Question_body": "Hi everyone,For some reason, my custom training Jobs with custom container keep failing on vertex AI but the local run is working fine (I verified by running with local run as indicated in the docs here. I also built the image and run it manually and it works fine).  Error log when running custom job: <code>{\n\"insertId\": \"2s7rqvfjzoq4v\",\n\"jsonPayload\": {\n\"attrs\": {\n\"tag\": \"workerpool0-0\"\n},\n\"message\": \"/opt/conda/bin/python: Error while finding module specification for 'trainer.train' (ModuleNotFoundError: No module named 'trainer')\\n\",\n\"levelname\": \"ERROR\"\n},</code> ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-01T22:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you share the code or gcloud command you're using to submit the job. With a custom container you don't need to define a trainer."
			},
			{
				"Answer_created_time": "2023-02-02T00:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @sascha_heyer\u00a0,\n\nReally appreciate for your kind help.\n\nBut I was a bit hasty for posting this thread. I started the custom job from a workbench with code:\n\ngcloud.aiplatform.CustomContainerJob(...)\n\nThe error logs I received was for the Training Pipeline in Vertex (Not sure why Training Pipelinee was created while I only init the Custom Job).\n\nA while later the logs updates and the training job was executed correctly\u00a0 :))\n\nSo I think I'm fine now. Tks so much for your kind help.\n\np/s: I created trainer module since i was following the docs for custom container with their code structure (they recommend so). Now I learned that I could have just use simple training script instead. Tks again"
			}
		]
	},
	{
		"Question_title": "cloud vision logging",
		"Question_created_time": "2023-01-31T17:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/cloud-vision-logging/td-p/516239/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Hi Everyone,I've been using Google Vision for quite sometime. However, through Logs Explorer, I cannot find any log related to that service. I've tried to query using this filter:",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-01T11:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "To ensure that your Cloud Logging is working properly, you may try to manually use Cloud Vision API. You may use this\u00a0Quickstart\u00a0as reference. Once executed, you may immediately go to Logs Explorer and in the query, use the filter\n\nprotoPayload.serviceName=\"vision.googleapis.com\"\n\nIf there are still no logs appear during the query, you may have some issues that require inpecting your actual project. With this, you may contact\u00a0https://cloud.google.com/support\u00a0so that a support can inspect your actual project and identify/resolve the issue."
			},
			{
				"Answer_created_time": "2023-02-01T22:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@aaronscottb\u00a0, thank you.\n\nI tried your suggestion and indeed I can see the log Logs Explorer.\u00a0\u00a0\n\nHowever, it seems it is an AuditLog. What I am looking for is information like image data being sent, result of the prediction. Is there any information like that?\n\nThank you"
			}
		]
	},
	{
		"Question_title": "Google Agent Assist - Simulator view - feature FAQ, article suggestion CHECKBOX disabled",
		"Question_created_time": "2023-01-26T03:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Agent-Assist-Simulator-view-feature-FAQ-article/td-p/514223/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 91,
		"Question_body": "While working on simulator of Google Agent assist, the checkbox to enable FAQ or Article Suggestion or smart reply option is DISABLED.The chat on simulator is working with virtual agent I have configured via Dialog flow with intents that I have built, but can't use agent assist feature since I am not able to enable it.Please note I have configured smart reply, article suggestion and Knowledge Base option in the agent assist in my google project.Looking for help to fix the issue.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-02-01T04:44:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Smart reply, FAQ and Articlet Suggestion of google agent assist became visible after I disabled the \"Choose to use Dialogflow\" option in my selected \"Conversation Profile\".\n\nHowever there is another issue. The \"smart reply\"/\"FAQ\"/\"Article Suggestion\" though enabled and visible in UI but they are not working. I am not getting any suggestion or smart reply for the customer chat. Any idea what could be wrong ?\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-26T04:37:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The \"Simulator view\" feature in Google Agent Assist allows you to preview how your agent will respond to user inputs, and the \"FAQ\" and \"Article suggestion\" checkboxes are options that can be enabled or disabled within that view. If the checkboxes are disabled, it means that the corresponding functionality (FAQ or article suggestion) is not currently active for your agent. To enable the functionality, you will need to check the corresponding checkbox in the Simulator view."
			},
			{
				"Answer_created_time": "2023-02-01T04:44:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Smart reply, FAQ and Articlet Suggestion of google agent assist became visible after I disabled the \"Choose to use Dialogflow\" option in my selected \"Conversation Profile\".\n\nHowever there is another issue. The \"smart reply\"/\"FAQ\"/\"Article Suggestion\" though enabled and visible in UI but they are not working. I am not getting any suggestion or smart reply for the customer chat. Any idea what could be wrong ?"
			}
		]
	},
	{
		"Question_title": "cannot access documentai api from compute",
		"Question_created_time": "2023-01-30T15:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/cannot-access-documentai-api-from-compute/td-p/515739/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 84,
		"Question_body": "Hi all,I've got a compute instance that cannot access the documentai processor.  The compute engine is in the same project as the processor, and I've given the service account the rolesThe error I receive is\"7 PERMISSION_DENIED: Request had insufficient authentication scopes.\" which feels like an Oauth issue, but my reading leads me to believe that documentAI uses Application Default Credentials, and that my compute instance should use the service account for the request.  thanks in advance for any insight. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-31T18:46:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nthanks, I had tried adding scopes, but the solution was to add a key to the service account\u00a0 add the json config to the file system and add the environment variable\u00a0\n\nGOOGLE_APPLICATION_CREDENTIALS\n\nas per\u00a0\n\nhttps://stackoverflow.com/questions/65703339/fixedcredentialsprovider-gives-unauthorized-exception-w...\n\n\u00a0\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-30T19:52:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I'll make a guess ... in your Compute Engine configuration, there is a section called \"Access Scopes\".\u00a0 My bet is that the set of allowed services you are permitted to call does\u00a0not include Document AI.\u00a0 Take a look at this Google documentation article:\n\nAccess Scopes\n\nand this medium article:\n\nSetting Access Scope of Google Cloud VM instances\n\nYou may either enable Document AI explicitly or switch to \"Allow full access to all Cloud APIs\".\u00a0 \u00a0If I understand the story correctly, this is a legacy technology but maintains backwards compatibility.\u00a0 The recommendation is to switch to \"Allow full access to all Cloud APIs\" and then use IAM for access control."
			},
			{
				"Answer_created_time": "2023-01-31T18:46:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nthanks, I had tried adding scopes, but the solution was to add a key to the service account\u00a0 add the json config to the file system and add the environment variable\u00a0\n\nGOOGLE_APPLICATION_CREDENTIALS\n\nas per\u00a0\n\nhttps://stackoverflow.com/questions/65703339/fixedcredentialsprovider-gives-unauthorized-exception-w..."
			}
		]
	},
	{
		"Question_title": "How can I effectively use Google Cloud for machine learning and AI?",
		"Question_created_time": "2023-01-29T07:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-can-I-effectively-use-Google-Cloud-for-machine-learning-and/td-p/515192/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 43,
		"Question_body": "Using Google Cloud's suite of tools and services for machine learning and artificial intelligence. The person asking the question is looking for steps they can follow to make the most of Google Cloud's capabilities in this area, including how to determine their requirements, preprocess and clean data, train and evaluate models, deploy models, and monitor and maintain them. The goal is to build high-quality, reliable models that deliver real-world impact.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-29T09:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "A good portal page for understanding Google's propositions for building AI/ML solutions can be found here:\n\nAI and machine learning products\n\nAt that site you will find many links and customer references illustrating how Google's story in this area.\n\nAnother good site is\u00a0\n\nData science and machine learning on Cloud AI Platform"
			},
			{
				"Answer_created_time": "2023-01-29T10:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What Kolban shared + I can highly recommend the Google documentation which is also full of best practices.\u00a0\nhttps://cloud.google.com/vertex-ai/docs\n\n\nYou can also check out my YouTube channel\nhttps://www.youtube.com/channel/UC--Sm3D-rqCUeLXmraypdPQ\n\nand I have a bunch of articles\nhttps://medium.com/@saschaheyer\n\nAnd obviously the google cloud community on medium.com\nhttps://medium.com/google-cloud"
			}
		]
	},
	{
		"Question_title": "Issue with using system functions in parameter presets",
		"Question_created_time": "2023-01-25T06:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Issue-with-using-system-functions-in-parameter-presets/td-p/513851/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 136,
		"Question_body": "Hi there,I'm trying to dynamically append values to a list parameter using the 'Parameter Preset' box when certain conditions are met for a transitional route. The list parameter is defined as follows: Then, during the route I use the following system function:  However, when the condition is met during the conversation, the updated value in $session.params.negative_products is not: [\" \", credit score], but prints out the whole text in the parameter preset box, e.g., $sys.func.APPEND($session.params.negative_products, credit score). Why is this the case? I thought system functions were able to be used to dynamically change values in parameter presets and I have no idea why it just keeps on printing out the function as a string! Any help would be much appreciated,Vicky",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-27T02:03:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hey!\n\nWhen referencing the parameter in my fulfilment I use:\u00a0\n\n$session.params.negative_products\n\n\u00a0I did figure a workaround yesterday. I realised when I define $session.params.negative_products within the 'Entry Fulfilment', 'Parameter Presets' section of the page the appending works:\u00a0\n\n1. Define empty list parameter in presets of the Entry Fulfilment, i.e., before I want to append to the list:\u00a0\n\n\u20032. During a conditional route, append to the empty list using system functions in the parameter presets with the following command:\u00a0\n\n$sys.func.APPEND($session.params.negative_products, \"credit score\")\n\n3. This then prints out the correct output when referencing $session.params.negative_products:\u00a0\n\n\" , credit score\"\n\nI think my original issue was that when trying to define $session.params.negative_products as a 'isList' parameter, it wasn't actually generating a list per se. So, when I was trying to call it in $sys.func.APPEND(), the function wasn't reading the $session.params.negative_products as a list, and as a result, just printed out the system command.\u00a0\n\nHope this helps other users who get faced with a similar issue!\n\nVicky\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-27T00:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nWelcome to Google Cloud Community!\n\nTo properly replicate your use case, can you show how you called the parameter on your fulfillment?"
			},
			{
				"Answer_created_time": "2023-01-27T02:03:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hey!\n\nWhen referencing the parameter in my fulfilment I use:\u00a0\n\n$session.params.negative_products\n\n\u00a0I did figure a workaround yesterday. I realised when I define $session.params.negative_products within the 'Entry Fulfilment', 'Parameter Presets' section of the page the appending works:\u00a0\n\n1. Define empty list parameter in presets of the Entry Fulfilment, i.e., before I want to append to the list:\u00a0\n\n\u20032. During a conditional route, append to the empty list using system functions in the parameter presets with the following command:\u00a0\n\n$sys.func.APPEND($session.params.negative_products, \"credit score\")\n\n3. This then prints out the correct output when referencing $session.params.negative_products:\u00a0\n\n\" , credit score\"\n\nI think my original issue was that when trying to define $session.params.negative_products as a 'isList' parameter, it wasn't actually generating a list per se. So, when I was trying to call it in $sys.func.APPEND(), the function wasn't reading the $session.params.negative_products as a list, and as a result, just printed out the system command.\u00a0\n\nHope this helps other users who get faced with a similar issue!\n\nVicky"
			}
		]
	},
	{
		"Question_title": "Incremental ingestion into Feature Store",
		"Question_created_time": "2023-01-24T11:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Incremental-ingestion-into-Feature-Store/td-p/513484/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 43,
		"Question_body": "Hello evryone,How to set up a incremental ingestion for online serving using Feature Store?I would like to use this flow: Query into The BigQuery with delta timeIngestion into FeatureStore  witho only this incremental valuesUsing the online serving with Incremental + Previous data into FeatureStoreHow Can I?",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, can you please elaborate on what you are trying to achieve in this case? Do you also follow any documentation or quickstarts as of the moment?"
			}
		]
	},
	{
		"Question_title": "question about IAM Dialogflow API Client role",
		"Question_created_time": "2022-12-19T03:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/question-about-IAM-Dialogflow-API-Client-role/td-p/501058/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 8,
		"Question_score_count": 1,
		"Question_view_count": 352,
		"Question_body": "Hello, Can anyone help me please? I received the following message: In Dialogflow, the IAM Viewer role currently allows users of the console simulator to test agent conversations and to call the API's detectIntent/streamingDetectIntent methods. Starting January 31, 2023, Google will remove this permission from this role. Therefore these actions will require the \u2018dialogflow.sessions.detectIntent' or \u2018dialogflow.sessions.streamingDetectIntent' permission, which is included in roles such as the IAM Dialogflow API Client role. Who can tell what to do, where, and how? Thanks in advance, Anton",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-20T11:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI assume your project uses\u00a0higher-privileged generic roles (Editor or Owner) also confer Viewer privileges. Users who uses these roles\u00a0may lose access to the simulator UI on Jan 31st.\n\nWhat you need to do is to add the DialogFlow API Client role or \u00a0Dialogflow API Admin role (depending how privileged\u00a0your users are) to access the console."
			},
			{
				"Answer_created_time": "2022-12-21T01:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello! Thank you very much for your answer.\u00a0\n\nI have in\u00a0IAM Dialogflow API Client role \u0430\u00a0\"dialogflow.sessions.detectIntent\".\u00a0But I did not find this role: \"dialogflow.sessions.streamingDetectIntent\". Does this mean that if I do not change or add anything, then by default this role will remain: \"dialogflow.sessions.detectIntent\"?\n\nBest regards, Anton"
			},
			{
				"Answer_created_time": "2022-12-21T07:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nRole\u00a0dialogflow.sessions.streamingDetectIntent should be included in Dialogflow API Client since it has permissions for\u00a0dialogflow.sessions.*"
			},
			{
				"Answer_created_time": "2023-01-23T11:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You said \"Users who uses these roles\u00a0\"\u00a0 you used plural.\nSo does that mean even (Editor or Owner) roles are going away?\n\nAccording to email from Google only\n\u2018dialogflow.sessions.detectIntent' or \u2018dialogflow.sessions.streamingDetectIntent'\npermissions from \"Viewer\" role are removed.\nPlease confirm"
			},
			{
				"Answer_created_time": "2023-01-25T12:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nOur account on Dialogflow also received the\u00a0 same letter recently; After spending many hours on this letter and searching on the web, we also do not know what we should do about our projects on this issue; Our account is using the Essential Version;\n\nHi Varun, did you fix the above issue? If not, we should demand Dialogflow team to provide good answer to our Q here;"
			},
			{
				"Answer_created_time": "2023-01-25T13:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Unslave,\n\nricconoel\nHe did answer the question, but it was adding more confusion. That's why I asked followup question to be clear.\n\nMy understanding is If any of our account has \"Viewer\" access we need to change it to \"Dialogflow API Client\". I am not 100% sure"
			},
			{
				"Answer_created_time": "2023-01-25T15:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello All,\n\n#Now, we got correct understanding of the meaning of this name \"console simulator\" mentioned below, and the following is from the letter sent to us by Dialogflow Team in Dec/2022;\n****************\nIn Dialogflow, the IAM Viewer role currently allows users of the\nconsole simulator to test agent conversations and to call the API's\ndetectIntent/streamingDetectIntent methods.\n******************\n\n# Based on the above knowledge, we believe that the coming change mention in above letter will not disable our web app calling the APIs of the project in our account on Dialogflow platform;"
			},
			{
				"Answer_created_time": "2023-01-25T17:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Dialogflow Team,\n\nThe following is not clear to us; I hope that you can post more details on this point;\n******************\nWhat you need to do is to add the DialogFlow API Client role or Dialogflow API Admin role\nto access the console.\n**********************\n\nThank you!"
			}
		]
	},
	{
		"Question_title": "How to invoke a rich response button in Diagflow CX through API",
		"Question_created_time": "2023-01-24T08:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-invoke-a-rich-response-button-in-Diagflow-CX-through-API/td-p/513390/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 61,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-25T10:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nWelcome to Google Cloud Community!\n\n\u00a0\n\nTo properly address your concern, can you clarify below:\n\nDoes your JAVA program have a UI?\nIf yes, are you trying to have a rich response button on the JAVA UI?\nCan you show a screenshot of the output of your current payload code?"
			}
		]
	},
	{
		"Question_title": "Speech to text hangs (infintie load)",
		"Question_created_time": "2023-01-22T07:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-text-hangs-infintie-load/td-p/512571/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 201,
		"Question_body": "I uploaded a mp3 file (1.07 min) to transcribe using the video model, and the transcription is stuck, no errors, just infinite loadingThis is the file https://drive.google.com/file/d/1QLucfAwJZXxSAOKSIWoPtIbljSM9pvsl/view?usp=sharingUPDATE: It failed with \"Error running recognize request. Too many retries, giving up.\"",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-23T14:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "May I know the documentation you are using in transcribing your audio?"
			},
			{
				"Answer_created_time": "2023-01-24T01:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In the case above I'm sending the file to transcription using the UI in google cloud console\n\nBut I did it after noticing the issue by sending the same file using the Node.js api, following the docs here: https://cloud.google.com/speech-to-text/docs"
			},
			{
				"Answer_created_time": "2023-01-24T12:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I was able to fetch the transcription in JSON format using gcloud CLI.\u00a0I\u00a0used Asynchronous Speech Recognition for transcribing an audio file that is longer than a minute. But I converted your audio file from Mp3 to FLAC since the process can't be completed(based on my replication) when I used Mp3.\n\nThese are the commands that I used:\n\ngcloud ml speech recognize-long-running 'gs://bucket-name/audio.flac' --language-code='en-US' --async --audio-channel-count=2 --separate-channel-recognition\n\ngcloud ml speech operations describe [name]\n\nThis is a snippet of my output:"
			},
			{
				"Answer_created_time": "2023-01-25T00:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "So the issue is the file is an mp3 and not flac? should I also re-encode the file?\n\nif it just a container change, then it is weird that mp3 fails and flac doesn't because it is the same encoded data inside\n\nalso, I can't access the output link you shared, it requires a google employee account"
			}
		]
	},
	{
		"Question_title": "Java API to Document AI - Proxy-Configuration",
		"Question_created_time": "2023-01-20T00:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Java-API-to-Document-AI-Proxy-Configuration/td-p/512059/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 133,
		"Question_body": "Hello,\nmy question is: How can I configure a Proxy for the Document-AI-Client by using Java-Code, not using Java-System-Property.I am using this code for creating the Document-AI-Client.If I use Java-System-Property \"-D http.proxy .... \" Everything works fine.But I have to access other Systems in the LAN by using e.g. Apache httpclient and other clients.\nThe global Java-System-Property interferes with the other kinds of clients, because all of them are using the Java-System-Property. I cannot Mix \"Proxy\" and \"no Proxy\". Of cause the other clients can be separately configured to use a Proxy or not.\nAnd the Document-AI-Client can use the Java-System-Property.\nBut it would be much better if I could configure the Proxy for the Document-AI-Client alike.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-24T03:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I found the solution here\u00a0\nhttps://stackoverflow.com/questions/68300018/how-to-configure-proxy-credentials-for-google-pub-sub-g...\n\nProxying is well documented here\nhttps://github.com/googleapis/google-cloud-java#configuring-a-proxy\nThe above Dokument seem to be the actual basis for google-cloud-clients. But not for all Google-APIs!"
			}
		]
	},
	{
		"Question_title": "Urgent access to private marked Bank Statement Parser & Procurement Doc Splitter Processor",
		"Question_created_time": "2023-01-22T01:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Urgent-access-to-private-marked-Bank-Statement-Parser-amp/td-p/512529/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "I wanted to access Bank Statement Parser & Procurement Doc Splitter Processor in Document AI. It is currently marked as private. I have already Requested Access for it. But I have not heard back from Google. I need it urgently for POC to show it my client. Please help asap. We really want to use this in our project.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-23T12:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you have a pending request for the parser I would suggest to contact your Customer Engineer or contact Google support[1] to follow up the request. With that said Please be informed that there is no documented time when the request will be fullfiled.\n\n[1]https://cloud.google.com/contact"
			}
		]
	},
	{
		"Question_title": "Dialogflow CX: intent detection issue when we delete a page",
		"Question_created_time": "2023-01-19T08:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-intent-detection-issue-when-we-delete-a-page/td-p/511745/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 82,
		"Question_body": "Hi,\nWhen we remove a page, the users who have a session get an error:\n\"Could not find Page with ID...\"\nwhat should we do in this situation? Is there any way to disable all active sessions?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-20T15:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Are the users are in the same page as the page deleted?\u00a0 According to this documentation[1] Session can be\u00a0 active for 30 mins. It may be possible creating a handlers[2] to handle this error and transition this event to a different page.\u00a0\u00a0\n\n\u00a0\n\n[1]https://cloud.google.com/dialogflow/cx/docs/concept/session\n\n[2]https://cloud.google.com/dialogflow/cx/docs/concept/handler"
			},
			{
				"Answer_created_time": "2023-01-23T12:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes some of our users are in the deleted pages.\nAlso I've tried event handlers but when I tried to trigger an event to clear users' sessions, I faced the same error."
			}
		]
	},
	{
		"Question_title": "RESOURCE_EXHAUSTED at custom-training-job.",
		"Question_created_time": "2023-01-21T18:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/RESOURCE-EXHAUSTED-at-custom-training-job/td-p/512495/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 79,
		"Question_body": "I have trouble to execute this below vertex pipeline example from google in vertexAI workbench notebook.https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/goog...I have below error code which mention RESOURCE_EXHAUSTED at custom-training-job.\"com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\"I change kind of cpu or memory . however it is meaningless for me.Could you please give me some idea?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-23T09:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The issue you are experiencing is related to\u00a0the quota of the resources in the Vertex service.\n\nFor you to request\u00a0higher quota and have more available resources, you may request a\u00a0Quota Increase for Vertex AI\u00a0 by following this\u00a0documentation."
			}
		]
	},
	{
		"Question_title": "AI/ML",
		"Question_created_time": "2022-12-29T00:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AI-ML/td-p/504317/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 70,
		"Question_body": "hello. could you please tell me which language is best for AI/ML?  and how could I start learning?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-30T12:14:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "You may start on the lists of AI and Machine Learning Products that Google Cloud has to offer. Languages/Client libraries available for these products are Go, Node.js, Python and many more. Quickstarts are available on the documentation of each product and may serve as a guide as you navigate your way on AI and Machine Learning.\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-12-30T12:14:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "You may start on the lists of AI and Machine Learning Products that Google Cloud has to offer. Languages/Client libraries available for these products are Go, Node.js, Python and many more. Quickstarts are available on the documentation of each product and may serve as a guide as you navigate your way on AI and Machine Learning."
			},
			{
				"Answer_created_time": "2023-01-22T15:59:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "AI/ML Quantum Computing\n\nWhere do I submit the new coolant solution for Quantum Computing?\n\nIt uses a colder coolant without using lasers and is compatible with current dilution refrigerator technology.\n\nIt is also in abundance compared with existing Helium shortages.\n\n\u00a0\n\nTwo birds with one stone.\n\n\u00a0\n\n-Caleb Young"
			}
		]
	},
	{
		"Question_title": "Exported edge model always returns the same values",
		"Question_created_time": "2023-01-19T06:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Exported-edge-model-always-returns-the-same-values/td-p/511682/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 66,
		"Question_body": "Hi,I trained a binary classification model with AutoML, and I have exported it for use in a container.I have followed this article to the letter https://cloud.google.com/automl-tables/docs/model-export and the prediction server is running OK. However, it doesn't matter what values I submit to it as the inputs, the output is always absolutely the same value.I should specify that when deploying the same model to a VertexAI endpoint, it works great. I thought the exported model and container was supposed to work the same as using an endpoint.Any ideas would be great.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-20T15:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI suppose that there is a key feature which is present in VertexAI\u00a0that is why the value you are fetching works accordingly. As it also suggested to migrate your datasets in AutoML to VertexAI since it has many features that are unavailable in AutoML platform.\n\nVertexAI deemed to be a one-stop-shop for all AI in GCP and it leverages the MLO principles. If suitable, I suggest to just use VertexAI instead for it is composed of already available GCP tools."
			},
			{
				"Answer_created_time": "2023-01-21T11:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi there,\n\nThe point is that I am following the documentation step-by-step, and I get different results when the model is exported, as per the article, and when deployed to an endpoint. But there shouldn't be a difference. It's either that the container mentioned in the article is not correct (i.e. outdated), or the model is not being exported correctly. At least that's my suspicion."
			}
		]
	},
	{
		"Question_title": "Wrong OCR text detection.",
		"Question_created_time": "2023-01-19T01:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Wrong-OCR-text-detection/td-p/511605/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 106,
		"Question_body": "Hello All,I use this official playground site (https://cloud.google.com/vision/docs/drag-and-drop) to try out the text detection, but the most important word is wrong.BALLANTINE  to KALLANTINEHere are request details :url : https://vision.googleapis.com/v1/images:annotate",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-20T15:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The models used by\u00a0Cloud Vision API\u00a0service are always being improved in order to provide a better recognition accuracy; however, sometimes they get the characters wrong or even they don't recognize the characters themselves. Keep in mind these services are trained in a daily basis which means the recognition quality will increase accordingly.\n\nFor best results, you may follow the supported\u00a0file format,\u00a0image size\u00a0and\u00a0language.\u00a0 This includes the feature languageHints\u00a0for more accurate result since you can already specify the exact language to be detected."
			},
			{
				"Answer_created_time": "2023-01-21T08:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm wondering if an additional possibility might be domain specific.\u00a0 For example, if the challenge is to extract text from a domain of bottles, I'm thinking that there might be a manageable set of possibilities where the recognition could be improved with retraining.\u00a0 See for example:\u00a0\u00a0https://cloud.google.com/vertex-ai/docs/training-overview\n\nI'm wondering what the \"confidence\" score on the extracted text might be for the example?\u00a0 \u00a0If the confidence is low then this might be a trigger to pass it for human review, explicit labeling and retraining so that for FUTURE recognitions, the accuracy would be better."
			}
		]
	},
	{
		"Question_title": "Vertex Batch prediction with BigQuery table as input. Predict API receives input without column name",
		"Question_created_time": "2022-12-29T05:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-Batch-prediction-with-BigQuery-table-as-input-Predict-API/td-p/504368/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 150,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-20T05:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\n\nIf you use BigQuery as input for your batch prediction job there are no column names included. You need to build your model in a way that it can process the instances without the header."
			}
		]
	},
	{
		"Question_title": "What qualifies as a \"request\" in Google Video Intelligence API?",
		"Question_created_time": "2023-01-19T00:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-qualifies-as-a-quot-request-quot-in-Google-Video/td-p/511593/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 100,
		"Question_body": "I have a Requests per Minute quota limit of 120 and Backend time of 600 seconds per minute.\nEven when I annotate 20 videos together (distributed using multiple cores on Databricks ), it shows the following error. However, since 20 is far less than 120, I can't seem to identify why it exceeds 120 requests per minute. So I'm wondering if any other thing other than \".annotate()\" function could be classified as a request.     ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-19T13:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI would suggest creating a case for this, as we cannot view logs and your actual code from here (also is not advisable since it may include sensitive details about your project) Specially if you think this a quota/billing mis match. You can try to create a case here[1]\n\n\u00a0\n\n[1]https://cloud.google.com/contact"
			},
			{
				"Answer_created_time": "2023-01-19T20:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My tests would likely be to reduce the number of parallel videos to 1 then 2 then 3 and see if new symptoms/patterns show up.\u00a0 \u00a0I also think that there are quota pages in the console that report specifically which quotas have been breached.\u00a0 I'd also look in Cloud Logging to see if there are clues in\u00a0 there.\u00a0 \u00a0I'm also noting that the quota is 120 per minute.\u00a0 Remember that is 120 per minute for the WHOLE of the GCP project.\u00a0 \u00a0Do you perhaps have additional work being run in your project other than the work you yourself are doing?\u00a0 \u00a0Maybe don't run any work yourself for 5 minutes and look at the quota usage for your project during that 5 minute period.\u00a0 Is it zero?"
			}
		]
	},
	{
		"Question_title": "OCR text recognition for changing images.",
		"Question_created_time": "2023-01-17T16:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/OCR-text-recognition-for-changing-images/td-p/511081/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 182,
		"Question_body": "Hi, I'm new to Cloud vision and am looking into how I can Scan an image and detect a specific Job Number E.g. 28379 that has a unique colour. We have images of steel which have lots of text and we figured if we write the Job number in a different colour, we could extract only that information to then rename the images to this job number and if there is multiple of the same job number append the data with a (1),(2),(3) etc on the end of each image. My goal is to automate renaming a full folder of photos based of the Job number detected in each image. We are using Power Automate for most of our automation process, so ideally we would like to access the Cloud via Power Automate (Text detection via API Gateway key to access Google Cloud SDK Shell). Does anyone know how to do this or any advice/tips on how to get this working. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-17T20:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Do you have any control over the textual content contained in the image?\u00a0 I think I am sensing that you may and you are looking to encode information (meta information ... i.e. Job Number) in the visual color of the text.\u00a0 A thought that is occurring to me is not to use color but instead to \"tag\" the information.\u00a0 For example instead of \"28379\" have the text be \"JN:28379\".\u00a0 \u00a0This way OCR could or other text extraction could be applied to the image and the tagged Job Number might lend itself to identification?\u00a0 \u00a0Another thought is position of the text.\u00a0 For example, if the Job Number were always in the top left corner of the image, then we could potentially train a Document AI parser with labeled data as the Document AI Parser has layout awareness.\n\nIf you do want to go with color encoding ... an initial thought occurs to me which would be to \"filter\" the image before applying OCR or text extraction.\u00a0 For example, if you coded the color of the Job Number as Magenta (#FF00FF) then we could perform a filter over the image ... for example capturing pixels where:\n\nR > #FA, G < #04, B > #FA\n\nand then perform OCR/text extraction over that filtered image.\u00a0 Presumably other text would be removed."
			},
			{
				"Answer_created_time": "2023-01-17T21:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks\u00a0@kolban\u00a0for your reply. I don't have much control over the photos and they are different each time, and is why we are thinking up strategies that can allow us to have enough control to reliable OCR extract the information we need. Good idea though for the JN:28379, We have thought of possibly doing something like this. If we made this change, how could I hypothetically code this? Or do you know of a tool in Power automate that could extract this specific information."
			},
			{
				"Answer_created_time": "2023-01-18T06:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Howdy.\u00a0 From the Google cloud perspective, my first examination would be the use of Google Cloud Vision AI.\u00a0 This is an API service that can take an image as input and return you a structured document (JSON) containing the extracted text.\u00a0 Once you have the extracted text, you could then parse that data looking for your data.\u00a0 \u00a0For example, if Job Numbers were tagged, you could use a REGEX to look for the prefixes.\u00a0 \u00a0Another possibility is Google Document AI but I'm not sensing this to be as good a match for you based on the description so far.\n\nTo code the extraction, at the highest level ... we would have to look at where the image originates.\u00a0 Are you processing the image on an on-premises server or in the cloud?\u00a0 Is it available through code / APIs or is it saved as a file on disk or blob storage?\u00a0 Do you want to process the images one at a time or in batch?\n\nSo many permutations.\u00a0 \u00a0However, from the Google Cloud Service perspective, loosely think of Google Cloud as a \"black box\".\u00a0 I could imagine you using Google Cloud Functions to expose a REST based service that could be called.\u00a0 You would then invoke your Cloud Functions service passing in the image data as a parameter.\u00a0 The Cloud Function would then invoke Google Cloud Vision AI passing in the image.\u00a0 The output would be the extracted text from the image.\u00a0 This would then be returned to the caller."
			},
			{
				"Answer_created_time": "2023-01-19T17:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Currently I have set up a Cloud vision AI to extract the data\u00a0I need (Via an API access)\u00a0 which is brought into Power Automate. The process of extraction is from a folder on my desktop for testing purposes but the final 'version' we want to work on our server (which I know how to setup). I did not know you could batch process the images however, I kept on getting errors back, could you possibly explain how this would work?\u00a0 Also I've tried to implement a regex but don't know too much about it and the closest I've got is to 'look' only for 5 numbers, and obviously sometimes it detects multiple and chooses the wrong\u00a0 number, do you know much on and regex pattern that I can implement. The one I have currently is \"\\d{5}\".\u00a0\n\nAlso to loosely explain my process for clarification what I want to do is have images scanned via a Cloud vision ai to detect for text which is brought back to power automate (Completely understand you may not know this program). Then using the text I get, extract only the \"JN:34894\" (Job number), which is then used to rename the image - And if the image name already exist, append the data with a (1),(2), etc. and then obviously repeat this till all images in the folder are renamed."
			},
			{
				"Answer_created_time": "2023-01-19T19:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Howdy ... unfortunately through this community, we can't *code up* a solution for you.\n\nBatch processing:\u00a0 I am seeing two primary APIs for OCR processing:\n1.\u00a0https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate\u00a0 ... takes a set of images and OCR annotates them but BLOCKS with the result\n\n2.\u00a0https://cloud.google.com/vision/docs/reference/rest/v1/images/asyncBatchAnnotate\u00a0.... takes a set of images and OCR annotates them in the background and you can determine when done\n\nAs for RegEx, here are some good links:\n\nhttps://regex101.com/\nhttps://regexr.com/\nhttps://www.regular-expressions.info/\n\nTo match \"JN:34894\", an expression similar to /JN:\\d{5}/\u00a0 feels like it might be right."
			}
		]
	},
	{
		"Question_title": "Cloud translation API not working in \"en\" to \"en\" OR \"he\" to \"he\" OR \"he\" to \"iw\". same language to",
		"Question_created_time": "2023-01-11T03:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-translation-API-not-working-in-quot-en-quot-to-quot-en/td-p/508958/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 143,
		"Question_body": "Hello Google Team,\n\nI am stuck in same language translation\n\nEnvironment details\nOS: Linux\nPHP version: 7.2.34\n\nSteps to reproduce\njust pass the same language code in $source_language and $target_language (\"en\",\"en\")\n\nCode example\n$text = \"I Love google translation\";\n$tl = 'en';\n$sl = 'en';\n$project_id = 'MY_PROJECT_ID';\n$glossary_id = 'MY_GLOSSARY_ID';\n\nfunction v3_translate_text_with_glossary($text,$targetLanguage,$sourceLanguage,$projectId,$glossaryId)\n{\n$translationServiceClient = new TranslationServiceClient();\n\n$request[\"content\"] = $text;\n\ntry {\n    $formattedParent = $translationServiceClient->locationName($projectId, 'global');\n   \n    $response = $translationServiceClient->detectLanguage($formattedParent, $request);\n    //echo \"<pre>\";print_r($response->getLanguageCode());die;\n    foreach ($response->getLanguages() as $language) {\n        printf('getDetectedLanguageCode text: %s' . PHP_EOL, $language->getLanguageCode());\n    }\n   \n}\nfinally {\n    $translationServiceClient->close();\n}\n\n$glossaryPath = $translationServiceClient->glossaryName(\n    $projectId,\n    'us-central1',\n    $glossaryId\n);\n$contents = [$text];\n$formattedParent = $translationServiceClient->locationName(\n    $projectId,\n    'us-central1'\n);\n$glossaryConfig = new TranslateTextGlossaryConfig();\n$glossaryConfig->setGlossary($glossaryPath);\n\n// Optional. Can be \"text/plain\" or \"text/html\".\n$mimeType = 'text/plain';\n\ntry {\n    $response = $translationServiceClient->translateText(\n        $contents,\n        $targetLanguage,\n        $formattedParent,\n        [\n            'sourceLanguageCode' => $sourceLanguage,\n            'glossaryConfig' => $glossaryConfig,\n            'mimeType' => $mimeType\n        ]\n    );\n    // Display the translation for each input text provided\n    foreach ($response->getGlossaryTranslations() as $translation) {\n        printf('Translated text: %s' . PHP_EOL, $translation->getTranslatedText());\n    }\n} finally {\n    $translationServiceClient->close();\n}\n}\n\nv3_translate_text_with_glossary($text, $tl, $sl, $project_id, $glossary_id);\n\n/////////////////////////////////////// ERROR RESPONCE /////////////////////////////\n\nError: Call to a member function getTag() on null: { \"message\": \"Target language can't be equal to source language.\", \"code\": 3, \"status\": \"INVALID_ARGUMENT\", \"details\": [ { \"@type\": \"type.googleapis.com/google.rpc.BadRequest\", \"fieldViolations\": [ { \"field\": \"source_language_code\", \"description\": \"Source language: iw\" }, { \"field\": \"target_language_code\", \"description\": \"Target language: he\" } ] } ] }\n\n/////////////////////////////////// ERROR END ///////////////////////////////////////////////\n\nHow can I translate text in \"en\" to \"en\" OR \"he\" to \"he\" OR \"he\" to \"iw\"\n\nThanks\nAnkit",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-11T05:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "what is the purpose of translating EN to EN?\n\nAs per error response it is not possible to that the\u00a0target language equals the source language."
			},
			{
				"Answer_created_time": "2023-01-11T21:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am running a script for translation text and I have 4 languages installed ar, en, he, ru, and th\nnow I am detecting the language code of the text, suppose it gives me \"iw\"\nand my website has \"he\", in this case, both language codes are the same because both language code point to Hebrew,\nThat's why I want the translation of same language to same language\nThanks\n\nAnkit"
			},
			{
				"Answer_created_time": "2023-01-19T12:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you translate iw (\u05e9\u05dc\u05d5\u05dd) to he it will be still (\u05e9\u05dc\u05d5\u05dd).\u00a0\nSo there is no need to translate.\u00a0 That's why you get the error."
			}
		]
	},
	{
		"Question_title": "Using text recognition while also using object detection",
		"Question_created_time": "2022-11-16T21:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Using-text-recognition-while-also-using-object-detection/td-p/490329/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 67,
		"Question_body": "Can you use visionAI text recognition while using object detection, or will they have to be two separate calls the Vertex AI?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-17T11:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nYou will be needing calling text recognition and object detection separately.\u00a0Endpoints for text recognition and object detection are separate hence you need to call them separately. See text recognition example and object detection example."
			},
			{
				"Answer_created_time": "2023-01-19T12:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can actually to multiple recognitions with one API call.\u00a0\nBoth text recognition and object detection using the same API endpoint.\u00a0\n\nPOST https://vision.googleapis.com/v1/images:annotate\n\nThe API endpoint request body has a parameter called features.\u00a0\nWhich is an array and can hold multiple features.\u00a0\nsee documentation\nhttps://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageRequest\n\n\n(The type of Google Cloud Vision API detection to perform, and the maximum number of results to return for that type. Multiple\u00a0Feature\u00a0objects can be specified in the\u00a0features\u00a0list.)\n\nsee documentation:\nhttps://cloud.google.com/vision/docs/reference/rest/v1/Feature\n\nAs response you get all the features you requested\nhttps://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse\n\nEnjoy\nSascha"
			}
		]
	},
	{
		"Question_title": "Issue with Load to draft in version of dialogflow Cx agent",
		"Question_created_time": "2023-01-16T01:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Issue-with-Load-to-draft-in-version-of-dialogflow-Cx-agent/td-p/510456/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 98,
		"Question_body": "-- Issue description (short summary)  --\nIn dialogflow agent, after creating the version, to restore back the selected version there is an option called LOAD TO DRAFT. After using this everything was not restoring. It only restores pages and flows but not intends, entities, webhooks etc...\n\nThere is also a special checkbox which indicates , this will override the current resources in the flow( Intends, Entities, webhooks..etc).Even after check-in it doesn't worked.\n\n-- Steps to reproduce the issue (detailed description)  --\n\n1.Create dialogflow agent with some pages, flows, intends and webhooks\n\n2. Create a version in manage section\n\n3. Make some changes in agent like adding new pages, intends and deleting some intends.\n\n4. Now navigate to manage > version\n\n5. Select the version previously created, in option click Load to Draft.\n\n6. After loading, refresh the pages\n\n\n-- Expected behavior  --\n7. Agent should completely restore with pages, intends (same as before changes done )\n\n-- Actual behavior  --\n\n8 . But the actual output was, only pages are updated but no changes in intends , webhooks, entities... etc.Reference : correct me if I did anything wrong.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-17T12:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Just to confirm you wanted when you load a version but it does not goes back to the actual version loaded and changes are still intact after rolling back to a previous version?"
			},
			{
				"Answer_created_time": "2023-01-17T21:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hai,\u00a0\n\nThanks for the response.\n\nYeah you are correct. When I tried to load back my version to draft, only pages and flows are effected but not the intends , webhooks, entities ,they are still same remain same."
			},
			{
				"Answer_created_time": "2023-01-18T12:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It looks like it is the same for my replications. I would suggest creating a public tracker for this to let dialogflow engineers investigate this issue. Please see this documentation on how to create ang issue tracker [1] the actual link for Dialogflow template for issue tracker please see here[2]\n\n[1]https://cloud.google.com/support/docs/issue-trackers#issue_reports\n\n[2]https://issuetracker.google.com/issues/new?component=956347"
			},
			{
				"Answer_created_time": "2023-01-19T01:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Created a issue tracker. Here is the link\u00a0https://issuetracker.google.com/issues/265603731\nYou can follow-up this."
			}
		]
	},
	{
		"Question_title": "Which NLU Model is used for Dialogflow CX?",
		"Question_created_time": "2023-01-13T06:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Which-NLU-Model-is-used-for-Dialogflow-CX/td-p/509932/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 94,
		"Question_body": "We assume that Sentence Embeddings are used for the intent analysis. Is this correct? If so, is it a publicly available model? We would like to display the embeddings of the training sentences graphically (e.g. with t-SNE) for the optimization of our training.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-17T19:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Excalibur,\n\nWelcome to Google Cloud Community!\n\nCan you clarify sentence embedding that you are pertaining to and how are you going to use it in the training?"
			}
		]
	},
	{
		"Question_title": "Translation API Request - Detailed info",
		"Question_created_time": "2023-01-17T09:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Translation-API-Request-Detailed-info/td-p/510906/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 60,
		"Question_body": "Hi all\u00bfCan we get the detailed information in monitoring, about all API requests? for instance users id which has make the request, volume (in MB) of the requests, grouped by user id. \u00bfIs it possible to get for Translation API?Thanks in advance for your guidance",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-17T10:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The first thing I would suggest we look at is the audit logging from the Cloud Translation service that is documented here:\n\nhttps://cloud.google.com/translate/docs/audit-logging\n\nWhen Google cloud API services (such as translation) are executed, they can generate audit logs.\u00a0 Some are enabled by default but others have to be explicitly turned on.\u00a0 I would say that you are likely going to want to look at the \"Data Access audit logs\" which are off by default.\u00a0 I'd suggest switching those on and running some of your tests and examine the resulting audit records found in Cloud Logging.\u00a0 Let's see if those contain the records/data you are looking for.\u00a0 Realize that switching on extra auditing records results in more consumption of Cloud Logging so make sure that you monitor your costs if you exceed the free allocation of Cloud Logging records.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-17T10:05:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The first thing I would suggest we look at is the audit logging from the Cloud Translation service that is documented here:\n\nhttps://cloud.google.com/translate/docs/audit-logging\n\nWhen Google cloud API services (such as translation) are executed, they can generate audit logs.\u00a0 Some are enabled by default but others have to be explicitly turned on.\u00a0 I would say that you are likely going to want to look at the \"Data Access audit logs\" which are off by default.\u00a0 I'd suggest switching those on and running some of your tests and examine the resulting audit records found in Cloud Logging.\u00a0 Let's see if those contain the records/data you are looking for.\u00a0 Realize that switching on extra auditing records results in more consumption of Cloud Logging so make sure that you monitor your costs if you exceed the free allocation of Cloud Logging records."
			}
		]
	},
	{
		"Question_title": "Capture a parameter from an annotated training phrase",
		"Question_created_time": "2023-01-10T01:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Capture-a-parameter-from-an-annotated-training-phrase/td-p/508526/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 59,
		"Question_body": "When the user input is    \"I want to order a new project called SalesPitch\"I want Dialogflow CX to determine that the intent is to order a new project, and also to capture the text \"SalesPitch\" in a parameter called projectName. I created a custom entity type for the project name, a regex, called projectNameText,  and created a training phrase:    \"I want to order a new project called Annabel\"and I annotated the training phrase so that Annabel was labelled as a projectNameText entity, all in the DialogFlow-CX console, as described in the documentation. But that instantly creates a parameter with Parameter id projectNameText. And I cannot edit that Parameter id. I can't require that when Dialogflow matches that training phrase and extracts an entity of type projectNameText, it puts it into the parameter projectName. Dialogflow demands that it goes into a parameter called projectNameText, or projectNameText1 if there is more than one. When I run the simulator and type input that matches that training phrase, Dialogflow does indeed correctly extract the entity, but will only create a parameter named projectNameText - I can see the name and value in the simulator.This StackOverflow answer implies that I can send the matched entity into any parameter I want. That would be sensible. But how do I do it? I can't find any way to edit the parameter name in the Intent editor. All it gives me is this:and I cannot change the Parameter Id.I must be missing something really basic. Hints, please?I asked this same question on StackOverflow but very few people read it so I am trying again here.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-17T07:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "It is indeed not possible to edit the Parameter Id in the Intent Editor directly.\n\nInstead, in the Intent Editor from the `Build` tab, accept the default Parameter Id, and `Save` the modified intent.\n\nThen go to the tabs on the left of the Dialogflow CX console and choose the `Manage` tab. Choose `Intents` from the menu and find the intent you have just edited from the menu. Click the intent name to be given a different version of the Intent Editor. Same fields, same data, different functionality. In this different Intent Editor, click the Parameter Id you want to edit. It is now editable. Do not forget to hit `Save` after editing it.\n\nFrom start to end, that took thirteen days to find. I posted here and received no answer. I finally subscribed to Google Cloud paid support and raised a support case and was given the answer in a video call with Google India. Perhaps it should be in the Google Documentation.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-17T07:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "It is indeed not possible to edit the Parameter Id in the Intent Editor directly.\n\nInstead, in the Intent Editor from the `Build` tab, accept the default Parameter Id, and `Save` the modified intent.\n\nThen go to the tabs on the left of the Dialogflow CX console and choose the `Manage` tab. Choose `Intents` from the menu and find the intent you have just edited from the menu. Click the intent name to be given a different version of the Intent Editor. Same fields, same data, different functionality. In this different Intent Editor, click the Parameter Id you want to edit. It is now editable. Do not forget to hit `Save` after editing it.\n\nFrom start to end, that took thirteen days to find. I posted here and received no answer. I finally subscribed to Google Cloud paid support and raised a support case and was given the answer in a video call with Google India. Perhaps it should be in the Google Documentation."
			}
		]
	},
	{
		"Question_title": "Quota increase for training on Vertex AI",
		"Question_created_time": "2023-01-13T03:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Quota-increase-for-training-on-Vertex-AI/td-p/509872/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 225,
		"Question_body": "I am trying to use the custom container method to train a simple model. I am executing a config.yaml file through which I am running a job but I received the following error. Error: \"error\": { \"code\": 429, \"message\": \"The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_n2_cpus\", \"status\": \"RESOURCE_EXHAUSTED\" } }The config.yaml file looks like this: ",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I believe what this messages is telling us is that Google had no machines of that specification (including NVIDIA_TESLA_4) available at the time the request was made.\u00a0 Quotas are limits on consumption that are in placed to protect users from over consumption.\u00a0 Resource exhaustion is the notion that Google was \"out of stock\" of your requested machine at the moment the request was made.\n\nHere is a good link that describes the story and some possible resolutions:\n\nhttps://cloud.google.com/compute/docs/troubleshooting/troubleshooting-vm-creation#resource_availabil...\n\nI'd also suggest reviewing the area:\n\nhttps://cloud.google.com/compute/resource-usage#quotas_and_resource_availability\n\nAt that link, we will read:\n\nResource usage quotas are the maximum number of resources you can create of that resource type, if those resources are available. Quotas do not guarantee that resources are always available. If a resource is not available, or if the region you choose is out of the resource, you can't create new resources of that type, even if you have remaining quota in your region or project."
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As you can see in the above image screenshot, It says 0, I have tried various zones as well as at different times to request for the resource( with caution I did not go all spammy). Here the resources arent available for me to create in the first place, and I would like to know why is it the case."
			},
			{
				"Answer_created_time": "2023-01-13T13:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This might be a useful article.\u00a0\u00a0\n\nhttps://stackoverflow.com/questions/53415180/gcp-error-quota-gpus-all-regions-exceeded-limit-0-0-glo...\n\nI also got to reading here:\n\nhttps://cloud.google.com/compute/docs/gpus/about-gpus\n\nI'm tempted to suggest trying to create a Compute Engine manually with an attached GPU and see if that works (delete the Compute Engine after creation).\u00a0 What I would look to learn from that experiment is whether the issue is that the Notebook/VertexAI is preventing you from getting a machine with a GPU or is it a configuration in your organization which might be fixed by the first link."
			},
			{
				"Answer_created_time": "2023-01-13T20:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am unable to raise a request for an increase in quota... This is a personal project with \"No organization\".\u00a0\n\n\u2003also, I am trying to execute the following from the docs.\u00a0\n\nhttps://cloud.google.com/vertex-ai/docs/training/configure-compute\n\n\u00a0\n\nHere you can see I am trying to configure the compute through workerpoolspec.\n\nThis is a quota issue and would be solved by raising a request, but I am unable to do so.\u00a0\n\nPs- this is not a free tier account and have a proper billing setup."
			},
			{
				"Answer_created_time": "2023-01-13T20:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I was reading here ...\u00a0https://nirbenz.github.io/gce-quota/\n\nAlso ... looking at your screen shot, I seem to see you are requesting GPU quota increase for ALL Google Cloud regions.\u00a0 Perhaps just try and request a quota increase for a single region.\u00a0 \u00a0I'm also seeing a possible clue in your screen shot ... it seems to say \"Based on your service usage history, you are not eligible for quota increase at this time\".\u00a0 \u00a0I'm afraid that's beyond any skills I may have to advise.\u00a0 \u00a0I did notice again on your screen shot that there is a \"link\" that says to contact the \"Sales Team\".\u00a0 I'd follow that link and fill out the form and let's see what they come back with.\u00a0 \u00a0Going back to your original post, you may have already tried this as you said \"I tried to contact several teams and it was to no help.\" ... but you didn't elaborate on who you contacted or what was the response."
			}
		]
	},
	{
		"Question_title": "Real-time feature engineering",
		"Question_created_time": "2023-01-12T10:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Real-time-feature-engineering/td-p/509639/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "I was wondering if there's a way to do real-time feature engineering with Vertex AI featurestore? An example use-case is if you needed to calculate Z-score for a transaction in real time on a huge number of transactions per second, while also keeping this feature up to date for a quick serving. If this possible, how would it be done? ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-13T11:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For starters, you may check this documentation about the overview of Vertex AI Feature Store. You can use Vertex AI Feature Store to create and manage\u00a0featurestores,\u00a0entity types, and\u00a0features.\u00a0You can also retrieve feature values in real time to perform fast online predictions.\n\nAs to how to use Vertex AI Feature Store on your sample use case, you may review this documentation about Data Model and Resources and check if this fits your purpose."
			}
		]
	},
	{
		"Question_title": "Webhooks",
		"Question_created_time": "2023-01-12T08:28:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Webhooks/td-p/509590/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 86,
		"Question_body": "I just want to clarify my understanding. I can use my own servers for calling webhooks correct (as long as they return the json structure required). The webhooks will essentially reach out another API service and return data for fulfillment. Thanks in advance for your time. ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-12T08:47:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Exactly correct.\u00a0 During the processing of a conversation, if you have a Web Hook enabled, the Dialogflow engine will call-out to the target URL passing in a JSON payload and expecting a correctly formatted JSON response.\n\nSee the following for details:\n\nhttps://cloud.google.com/dialogflow/cx/docs/concept/webhook\n\nTake care to notice that the target service MUST be callable through HTTPS which means that it has a valid SSL certificate.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2023-01-12T08:47:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Exactly correct.\u00a0 During the processing of a conversation, if you have a Web Hook enabled, the Dialogflow engine will call-out to the target URL passing in a JSON payload and expecting a correctly formatted JSON response.\n\nSee the following for details:\n\nhttps://cloud.google.com/dialogflow/cx/docs/concept/webhook\n\nTake care to notice that the target service MUST be callable through HTTPS which means that it has a valid SSL certificate."
			},
			{
				"Answer_created_time": "2023-01-12T08:53:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks! I just found that article shortly after I made the post. Thanks!!"
			}
		]
	},
	{
		"Question_title": "Google cloud speech to text : Dealing with high amount of input data in realtime",
		"Question_created_time": "2022-09-28T07:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-cloud-speech-to-text-Dealing-with-high-amount-of-input/td-p/471924/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 125,
		"Question_body": "Scenario: We are receiving huge amount of voice data streams which could be upto couple of minutes long. There will huge set of data streams as an input every second. This a real time application where I get live data from multiple users and return response from api. I am using google' AsyncStreamingRecognize to create stream. This stream open, read and write functions are blocking as I cannot proceed with other operation until success is returned by get() functions. Due to this my threads responsible for dealing with stream data are getting blocked. Is there a better way to where I can write high amount of input data without my thread being blocked for previous write function ? The code/setup works fine for a single call in real time. But I doubt how to design it well that could handle huge realtime incoming traffic",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-27T06:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi vaibhav_jio, sorry for possible inconvenience. I've noticed you might be explorer of speech recognition engine. I\u2019m owner of a small company A-Transcript, which providing Finnish and Russian native transcription crowd services within comparably small budget (might process other languages as well). (website: a-transcript.com, email: info@a-transcript.com). I do have also some work experience behind on LaMDA and speech recognition engines. Wonder if you could provide any contacts, who are qualified to discuss about collaboration and taking business proposal. Thank you in advance and wish you a great day."
			},
			{
				"Answer_created_time": "2022-11-23T10:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Do you have the capability to deliver speech-to-text subtitles on a VR glass product?"
			},
			{
				"Answer_created_time": "2022-11-24T00:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "hi hello249, thank you for request. Unfortunately at the moment I'm not familiar with such technology. My services is about providing qualified transcribers, onboard and manage them. (Type of transcription text could be subtitles as well as AI datasets (text with tags including). In order if you are agree with this, then would be glad to have calloboration. my company's email: <PII removed by staff>"
			},
			{
				"Answer_created_time": "2023-01-12T08:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Maybe put Redis in front of BigQuery?\n\nOtherwise take SaaS like getstream.com\n\nAdidas has run a successful campaign at them\n\nhttps://getstream.com/case/getstream-case-study-adidas-advertising-on-twitch/\n\nWith kind regards,\n\n\u00a0\n\nSjoerd"
			}
		]
	},
	{
		"Question_title": "LogisticRegression model to TFLite?",
		"Question_created_time": "2023-01-09T19:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/LogisticRegression-model-to-TFLite/td-p/508457/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 63,
		"Question_body": "Hi All,I need your guidance!I have trained a logistic regression model from the scikit-learn library and saved the model as .cpickle file.But the goal is to use this model in an Android app so my android team needs the model in supportive format. I am not sure if .cpickle is supportive for them or not.So my query is in which format I can save the model so that mobile team can easily integrate it with the Android app. I know about the TFLite format but how can I convert the model to this format or is there any other way?Please share your thoughts on this.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-10T10:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is more of a Tensorflow question and it would better help you to participate in their Community as well. I have found though this SO post that could be helpful on conversion to tflite."
			},
			{
				"Answer_created_time": "2023-01-11T06:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "here is a great article that describes on how to transform your model into ONNX format which can be executed on android devices.\u00a0\nhttps://towardsdatascience.com/deploying-scikit-learn-models-in-android-apps-with-onnx-b3adabe16bab"
			}
		]
	},
	{
		"Question_title": "Can we Download an AutoML Model after training?",
		"Question_created_time": "2021-08-05T11:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-we-Download-an-AutoML-Model-after-training/td-p/166258/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 281,
		"Question_body": "Is there a way to Download the AutoML Model after training?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-11T06:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Depends on the AutoML product, which one are you using?\n\nAutoMLTables models can be exported but need to be served with a dedicated serving image.\u00a0\n\nAutoML Vision allows you to train a dedicated edge model that can be exported."
			}
		]
	},
	{
		"Question_title": "PyTorch is using the GPU on a container on my local machine, but is unable to use the GPU on Vertex",
		"Question_created_time": "2022-08-23T07:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/PyTorch-is-using-the-GPU-on-a-container-on-my-local-machine-but/td-p/458980/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 149,
		"Question_body": "Expected Behavior\nI want to use a GPU on a component of Vertex AI.Actual Behavior\nUnfortunately, `torch.cuda.is_available()` is returning `False`. Also, `nvidia-smi` is not working if ran in the container of Vertex AI.\nNote: both commands also don't work locally in the container if I'm not specifying the `--gpus all` flag in the command `docker run --rm -it --gpus all ee97db5bbd98 /bin/bash`. However, I can't find any option to add the `--gpus all` flag for Vertex AI. Would this be required?Steps to Reproduce the ProblemMy YAML file: My pipeline:Visualized in the browser:",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-11T06:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Does your container properly installs the CUDA and NVIDIA drivers?\n\neu.gcr.io/uman-interns/backend:v1.7\n\nIf not it's probably best to use a cuda ready image as baseimage for your image.\u00a0\nFor example\n\nFROM nvidia/cuda:11.0-cudnn8-runtime-ubuntu18.04"
			}
		]
	},
	{
		"Question_title": "Changing the service account for an endpoint",
		"Question_created_time": "2022-11-21T01:37:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Changing-the-service-account-for-an-endpoint/td-p/491278/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 235,
		"Question_body": "Hi, I have deployed a Vertex AI model that was created using a custom image. However, when I tried deploying to an endpoint, it fails when it tries to run this line:In the logs, there was an error message that readsIt appears that the endpoint has been assigned a service account that is not associated with my account. From this documentation (https://cloud.google.com/ai-platform/prediction/docs/custom-service-account), it appears that a service account managed by AI Platform Prediction is being used when a custom container is being used. However, my account does not have the permissions to create another custom service account, as it is being managed by my client. I came across this solution (https://stackoverflow.com/questions/68456262/gcp-vertex-ai-model-access-gcs-failed) where the user had the exact same problem and solved it by changing the service account used at the endpoint. As such, I would like to ask how it will be possible for me to change the service account used by the endpoint without having to create a new service account?Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-11T06:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You need to attach a custom service account (that you create yourself and has access to Cloud Storage) to your endpoint.\u00a0\n\nsee last parameter.\u00a0\n\n\n\ngcloud ai endpoints deploy-model ENDPOINT_ID \\\n\u00a0 --region=LOCATION \\\n\u00a0 --model=MODEL_ID \\\n\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\n\u00a0 --machine-type=MACHINE_TYPE \\\n\u00a0 --min-replica-count=MIN_REPLICA_COUNT \\\n\u00a0 --max-replica-count=MAX_REPLICA_COUNT \\\n\u00a0 --traffic-split=0=100 \\\n\u00a0 --service-account=CUSTOM_SERVICE_ACCOUNT\nhttps://cloud.google.com/vertex-ai/docs/general/custom-service-account#attach_a_service_account_to_a..."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX: optional parameter capture",
		"Question_created_time": "2023-01-11T03:09:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-optional-parameter-capture/td-p/508944/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 74,
		"Question_body": "Sometimes user input is1:    I want to order a new projectand sometimes it is 2:   I want to order a new project called SalesPitchand sometimes it is 3:  I want to order a new project called SalesPitch for delivery by tomorrowInput 1 above is easy to recognise. The chatbot has to go on to ask what the project name should be, and when delivery is wanted.Input 2 can detect the intent and pick out the projectName in one go. But in the Dialogflow CX console, I cannot mix Input 2 phrases and input 1 phrases, so that projectName is optionally extracted from input. Once I start with an annotated training phrase, the intent editor seems to demand that every subsequent training phrase includes the same entity to be extracted. It automatically annotates every phrase with the entity, and I cannot delete the annotation from the training phrase.It seems I cannot mix annotated and unannotated training phrases in one intent. Is that true? Anyway, I can try and work around this by creating two intents, one that requires the parameter to be defined in the input, and one that does not.But then we come to input 3. The user can specify delivery date too. So now I have two optional parameters, either of which might be present in the input. That means I now have to have four different intents with training phrases for the four options of zero, one or two parameters in the input. And then if I have another optional parameter, I need eight different intents...Is that the way to do it?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-11T03:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sometimes user input is\n\n1: \u00a0\u00a0 I want to order a new project\n\nand sometimes it is\n\n\u00a02: \u00a0 I want to order a new project called SalesPitch\n\nand sometimes it is\n\n\u00a03:\u00a0 I want to order a new project called SalesPitch for delivery by tomorrow\n\nInput 1 above is easy to recognise. The chatbot has to go on to ask what the project name should be, and when delivery is wanted.\n\nInput 2 can detect the intent and pick out the projectName in one go. But in the Dialogflow CX console, I cannot mix Input 2 phrases and input 1 phrases, so that projectName is optionally extracted from input. Once I start with an annotated training phrase, the intent editor seems to demand that every subsequent training phrase includes the same entity to be extracted. It automatically annotates every phrase with the entity, and I cannot delete the annotation from the training phrase.\n\nIt seems I cannot mix annotated and unannotated training phrases in one intent. Is that true? Anyway, I can try and work around this by creating two intents, one that requires the parameter to be defined in the input, and one that does not.\n\nBut then we come to input 3. The user can specify delivery date too. So now I have two optional parameters, either of which might be present in the input. That means I now have to have four different intents with training phrases for the four options of zero, one or two parameters in the input. And then if I have another optional parameter, I need eight different intents...\n\nIs that the way to do it?"
			}
		]
	},
	{
		"Question_title": "Dialogflow CX: Inhibit automatic annotation of training phrases",
		"Question_created_time": "2023-01-11T02:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-Inhibit-automatic-annotation-of-training-phrases/td-p/508942/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 48,
		"Question_body": "I have training phrases like this:    I want to order a new projectand I enter many versions of that and it all works: Dialogflow understands my intent is to order a new project. But sometimes the input is    I want to order a new project called SalesPitchwhere SalesPitch has to conform to a projectName entity type - [A-Za-z0-9]+, in regex terms. When I define this custom entity, and enter a new training phrase, the Dialogflow CX console tends to break the phrase down into individual words, and annotate every word as a projectName, and create a parameter for every word. It would be much more convenient if Dialogflow would not attempt auto annotation, leaving me to annotate the word I wanted myself.Is there any way to turn off the auto annotation?Here's what happens: This gets even more ridiculous. Now I'm editing a different intent, no parameter capture at all, and this is what happens with automated annotation:",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-11T02:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have training phrases like this:\n\n\u00a0\u00a0\u00a0 I want to order a new project\n\nand I enter many versions of that and it all works: Dialogflow understands my intent is to order a new project. But sometimes the input is\n\n\u00a0\u00a0\u00a0 I want to order a new project called SalesPitch\n\nwhere SalesPitch has to conform to a projectName entity type - [A-Za-z0-9]+, in regex terms. When I define this custom entity, and enter a new training phrase, the Dialogflow CX console tends to break the phrase down into individual words, and annotate every word as a projectName, and create a parameter for every word. It would be much more convenient if Dialogflow would not attempt auto annotation, leaving me to annotate the word I wanted myself.\n\nIs there any way to turn off the auto annotation?\n\nHere's what happens:\n\n\u00a0\n\n\u2003\n\nThis gets even more ridiculous. Now I'm editing a different intent, no parameter capture at all, and this is what happens with automated annotation:"
			}
		]
	},
	{
		"Question_title": "Production readiness POC: Our Client wants to use GCP and want to make sure about few features.",
		"Question_created_time": "2023-01-09T04:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Production-readiness-POC-Our-Client-wants-to-use-GCP-and-want-to/td-p/508106/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "There is a requirement of our client in GCP that Dialog flow CX agent should support the chat history maintenance and also if it provides chatroom option to maintain different chats as per user id in webchat integration option. We have studied few features on Azure Chatbot those are supporting these requirements and client needs to see if GCP dialog flow can be customize like this or not. We are planning to do some POC for production readiness with GCP, it will be good if we can get answers to these questions ASAP",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-10T13:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for reaching out.\u00a0\n\nDialogflow CX has a conversation\u00a0history tool that provides an interface for browsing actual production conversations between your agent and end-users. As for\u00a0\n\n@Trushna\u00a0wrote:\n\n\nchatroom option to maintain different chats as per user id\n\nThere is no available documentation that there is a feature of that in DialogFlow CX. You can customize your Dialogflow Messenger though and one of its attributes is \u00a0user-id according to your use case."
			},
			{
				"Answer_created_time": "2023-01-10T23:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Anjelab,\n\nThanks for your quick reply, I will check these links you provided to see a turnaround and if we can make it, below is the list of actual requirements from client:\n\n1. Passing values to the Bot: When Bot is initialized on webpage, we want to send some values to the Bot directly (like passing values from backend), and the Bot should not need to take inputs from the user. Can we do this here with any workaround?\n\n2. Show Values in Tabular Format: Bot should display values received from API in the tabular format (or any other presentable format). Need guidance here.\n\n3.\u00a0Load existing chat history: Whenever user opens the Bot, it should load existing chat history in the chat window. Can we do it with this?\n\n4.\u00a0Send Notifications to Bot: We need to send confirmation notifications to the bot, we are connecting to the other services\u00a0\n\nIf we get initial heads up, we can spend time on exploration on this type of customization.\n\nThanks, and Regards.\n\nTrushna Pawar"
			}
		]
	},
	{
		"Question_title": "Image size for Google AutoML Training",
		"Question_created_time": "2023-01-09T18:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Image-size-for-Google-AutoML-Training/td-p/508426/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 91,
		"Question_body": "Hi, I have some images which are lager than  1024 pixels by 1024 pixels.\nFollowing this document: https://cloud.google.com/vertex-ai/docs/image-data/object-detection/prepare-data\nImage size should be 1024 pixels by 1024 pixels suggested maximum. If I resize images, the quality of images is low.\n\nWhat should I do?\nThank you and\nRegards",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-10T10:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As this is the required image size and there is a disclaimer in the documentation:\u00a0\n\nFor images much larger than 1024 pixels by 1024 pixels some image quality may be lost during Vertex AI's image normalization process.\n\n\n\nThis seems to be the limitation of image training. If you really are adamant to retain the image size more than 1024 x 1024 pixels, you may file a feature request and an engineer working closely in Cloud Vision API can assist you."
			}
		]
	},
	{
		"Question_title": "ContexualVersionConflict When Importing Aiplatform Package in Executor of Vertex AI Workbench",
		"Question_created_time": "2023-01-08T23:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/ContexualVersionConflict-When-Importing-Aiplatform-Package-in/td-p/508016/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 323,
		"Question_body": "Hi There,I had the following problem when importing aiplatform package in executor of Vertex AI workbench. This issue did not occur when I manually run the code in the Vertex AI workbench. However, the error came when I set a executor to run my code on schedule.---------------------------------------------------------------------------Here is the script---------------------------------------------------------------------------Here is the error message:-----------------------------------------------------------------------------------------------------Seeking help from everyone. Thank you  ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-09T00:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It looks like you are encountering a version conflict error when trying to import the aiplatform package from Google Cloud. This error occurs when different versions of a package or its dependencies are installed and are being imported at the same time, which can cause conflicts.\n\nOne solution to this problem is to ensure that you have the correct versions of all required packages installed. You can check the version of the aiplatform package by running the following command:\n\npip show aiplatform\n\nThis will display the version of the package that is currently installed. If the version is not what you expected, you can try uninstalling and re-installing the package with the desired version using the following command:\n\npip install aiplatform==<desired_version>"
			},
			{
				"Answer_created_time": "2023-01-10T00:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Ram56\u00a0,\n\nThanks for taking a look at this issue and the reply. I have tried your approach to upgrade the aiplatform package(!pip install google-cloud-aiplatform --upgrade), but still had the same issue. It seems that even we downgrade the google-cloud-bigquery package (!pip install google-cloud-bigquery==2.34.2 --user) to the version less than 3.0.0. The executor container would resume to be 3.4.1. which leads to the same issue again. Is there any other way you think worth a try ? Thank you"
			},
			{
				"Answer_created_time": "2023-01-10T00:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "screenshots of my code"
			}
		]
	},
	{
		"Question_title": "What you think about CHATGPT",
		"Question_created_time": "2023-01-05T07:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-you-think-about-CHATGPT/td-p/506958/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 110,
		"Question_body": "What do you all think about \"ChatGPT\" by open AI? Does it take the jobs of developers or it's just a kind of personal assistant who will make your job easier \ud83d\ude42",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-09T10:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "AI will replace the people who think it will. - Naval.\nI totally agree with what Naval tweeted about AI. In our context, ChatGPT will assist developers with many tasks but underlying decisions would still be made by humans."
			},
			{
				"Answer_created_time": "2023-01-09T19:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's a very rubbish Al...."
			}
		]
	},
	{
		"Question_title": "How to you implement Retail recommendation model?",
		"Question_created_time": "2023-01-09T08:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-you-implement-Retail-recommendation-model/td-p/508252/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "We are trying to implement a Retail recommendation model, in particular, the Recommended for you model.I share here the docs for reference: https://cloud.google.com/retail/docs/reference/rest/v2beta/projects.locations.catalogs.placements/pr...All the catalog is uploaded correctly, the model is trained and inside GCP in google Retail we get a prediction if we provide a cookie.Our problem is that we are trying to get the list of predictions  of users with the new url:\"https://retail.googleapis.com/v2beta/YOUR_MODEL_LOCATION\"But we have to provide the API Key and also the user has to be authenticated (OAuth).This is an issue because we users can't authentica themselfs in order to get the predictions.A prior version that worked had a different url:https://recommendationengine.googleapis.com/v1beta1/This one worked and no OAuth was needed, it returned a list/array with the predictions for each user correctly. Sadly this website is going to be deprecated and we have to use the new one.Has somebody encountered the same issue or implemented the Retail models correctly in a website?Thank you in advance! ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-09T08:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We are trying to implement a Retail recommendation model, in particular, the\u00a0Recommended for you model.\n\nI share here the docs for reference:\u00a0https://cloud.google.com/retail/docs/reference/rest/v2beta/projects.locations.catalogs.placements/pr...\n\nAll the catalog is uploaded correctly, the model is trained and inside GCP in google Retail we get a prediction if we provide a cookie.\n\nOur problem is that we are trying to get the list of predictions\u00a0 of users with the new url:\n\n\"https://retail.googleapis.com/v2beta/YOUR_MODEL_LOCATION\"\n\nBut we have to provide the API Key and also the user has to be authenticated (OAuth).\n\nThis is an issue because we users can't authentica themselfs in order to get the predictions.\n\nA prior version that worked had a different url:\n\nhttps://recommendationengine.googleapis.com/v1beta1/\n\nThis one worked and no OAuth was needed, it returned a list/array with the predictions for each user correctly. Sadly this website is going to be deprecated and we have to use the new one.\n\nHas somebody encountered the same issue or implemented the Retail models correctly in a website?\n\nThank you in advance!"
			}
		]
	},
	{
		"Question_title": "Cloud Vision API - Wrong line order",
		"Question_created_time": "2023-01-05T01:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Vision-API-Wrong-line-order/td-p/506806/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 111,
		"Question_body": "Hi there,I'm curious how are you handling the too common case where DOCUMENT_TEXT_DETECTION returns the text lines (or specific words) at incorrect order, despite the input being a simple paragraph. For example, line n would jump to line n+2, then backtrack to line n+1.Using \"legacy_layout\" does not usually help.thanks! ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-07T06:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is an on-going issue and the engineers are working out a solution for this. For the mean time, you may click +1 on the right pane to express that you are also encountering the same issue."
			},
			{
				"Answer_created_time": "2023-01-08T12:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks anjelab. I'm happy to learn the engineers are working on it!\n\nIs there a priority or timeline for the Vision API returning text at the correct order?\n\nThere's also a recent, more specific issue, that was opened for this problem here:\u00a0\u00a0https://issuetracker.google.com/issues/264322914\n\nWhile the team is working on solving this issue, the service is not usable for use cases that rely on correct order of the returned text, hence I'm interested in any workarounds that folks may have come up with."
			}
		]
	},
	{
		"Question_title": "How to create an endpoint using the model trained by Vertex AI?",
		"Question_created_time": "2022-12-29T23:37:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-create-an-endpoint-using-the-model-trained-by-Vertex-AI/td-p/504610/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 191,
		"Question_body": "I trained a model using Google Cloud Vertex AI. I have a dataset containing different videos and model does the video classification of these videos. In the Google Cloud Platform, I can find accuracy of the given videos after model has been trained. However, I could not find a way to create an Endpoint which is required to implement my model to the project.In other words, I have a model trained by Google Cloud\u2019s Vertex AI and I want to create an Endpoint which accepts a request with video and returns the accuracy according to the trained model. Google Cloud provides a way to do it using Endpoints but as shown below, following error occurs. This model cannot be deployed to an EndpointI could not find any documents on why this error is given to me. On the other hand, when I download the given model, code requires input type as below with the video;\u201cRESNETISH_V3: float32 Tensor, shape=(None, 128)\u201d.I do not know what is this variable type and this is basically where I am stuck. I need help deploying my trained model to any serverless environment, create a code handling posted video and return the accuracy and class of the given video to the request.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-02T13:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As this would be hard to reproduce on my end without access on your project, I suggest to raise a 1:1 support and a dedicated resource with access on your project can help you with your issue."
			},
			{
				"Answer_created_time": "2023-01-07T17:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I also have the same issue when working with time prediction. If you have any solution, please kindly let us know!"
			},
			{
				"Answer_created_time": "2023-01-07T21:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Just for supplementary information, here is whats happening . Please take a look, i cant click on the deploy buttton."
			}
		]
	},
	{
		"Question_title": "Event \"home-page-view\" not working",
		"Question_created_time": "2022-12-10T12:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Event-quot-home-page-view-quot-not-working/td-p/498253/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 72,
		"Question_body": "We set up the home-page-view event to create a recommendation system, and implemented the code on the site fitformula.ru via dataLayer according to the article https://cloud.google.com/retail/docs/record-events We also added the Cloud Retail tag in Google Tag Manager\nand selected the dataLayer source. But it does not collect the main page visit. Why?<script>\nwindow.dataLayer = window.dataLayer || [];\nwindow.dataLayer.push({\n'cloud_retail': {\n'eventType': 'home-page-view',\n'visitorId': window.gaGlobal.vid\n}\n});\n</script>",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-07T09:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Since this is specific to your project, I suggest to raise a 1:1 Support."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX - Telephony Transfer does not work with Voicemail",
		"Question_created_time": "2023-01-05T11:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-Telephony-Transfer-does-not-work-with-Voicemail/td-p/507030/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 25,
		"Question_body": "Whenever I transfer to a voicemail number, it does not work. It could be an infinite loop where I transfer to the number and the VOIP system transfers back to the dialogflow cx number but I am not sure. \n\nThis severely limits our customers who already have a voicemail system set up.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-05T11:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Whenever I transfer to a voicemail number, it does not work. It could be an infinite loop where I transfer to the number and the VOIP system transfers back to the dialogflow cx number but I am not sure.\u00a0\n\nThis severely limits our customers who already have a voicemail system set up."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX - DTMF Barge in not working",
		"Question_created_time": "2022-12-03T06:28:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-DTMF-Barge-in-not-working/td-p/495732/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 6,
		"Question_view_count": 185,
		"Question_body": "Dialogflow CX Barge in does not work for DTMF input.\n\n-- Steps to reproduce the issue:\n1. Enable Barge in at Agent level settings\n2. Create a page with a parameter with DTMF enabled\n3. Call agent via phone connector like Twilio and use phone keypad to provide parameter value\n\n-- Expected behavior  --\nThe agent prompt should stop on DTMF input, similar to how it stops on voice input.\n\n-- Actual behavior  --\nThe agent prompt does not stop on DTMF input.https://issuetracker.google.com/issues/259816857\n\nIssue report url: https://issuetracker.google.com/issues/259816857  Please upvote. \nCX phone experience is highly degraded without DTMF barge in.",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-04T12:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Is Diaglogflow CX production-ready? Lack of DTMF Barge is a blocker that makes the product unusable over phone. This issue is blocking us from going live with Dialogflow CX over phone."
			}
		]
	},
	{
		"Question_title": "I am getting a permissions error when trying to create a new Agent in DF",
		"Question_created_time": "2023-01-03T10:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/I-am-getting-a-permissions-error-when-trying-to-create-a-new/td-p/505956/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 29,
		"Question_body": "I want to create a bot for my team that will answer incoming chats.  When I go to create the Agent in DF ES I get a caller does not have permission error.  How can I gain access to do this?",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-04T11:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can check this documentation for Access Control. If you are the\u00a0IAM Project Owner or the Agent Admin you can grant access or revoke permission to your teammates. You can navigate it through Google Cloud Console and Dialogflow Console."
			}
		]
	},
	{
		"Question_title": "upload images/photos - Dialogflow",
		"Question_created_time": "2023-01-02T12:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/upload-images-photos-Dialogflow/td-p/505401/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 117,
		"Question_body": "How can an \"Action and parameters\" load an image/photo as input data. It is having the possibility of fearing an image field.\nThank youHow can an \"Action and parameters\" load an image/photo as input data. It is having the possibility of fearing an image field.\nThank youHow can an \"Action and parameters\" load an image/photo as input data. It is having the possibility of fearing an image field.\nThank you",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-03T13:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you elaborate and share encountered errors, screenshots, logs, steps to reproduce etc?"
			}
		]
	},
	{
		"Question_title": "Being charge for Translate API usage",
		"Question_created_time": "2023-01-02T05:08:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Being-charge-for-Translate-API-usage/td-p/505295/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 74,
		"Question_body": "I have some doubts about translate API usage.\n\nI thought it would be unlimited according to this image:But they have charged me 180 reais in one day because of translating texts using the API.I have two questions: How can i check how much money will i pay for X characters on each translation?Even if i was about to be charged, wheres the 300$ i had for testing it? I didnt know i was going to be charged.\n\nCan anyone helps me, please?\n\nThanks in advance! ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-03T13:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As per this pricing documentation:\n\nPrices are\u00a0pro rata\u00a0(proportional and incremental). Charges are scaled to the number of characters actually provided to Cloud Translation. For example, if you send 575,000 characters for processing within a month, you are charged $1.50. The first 500,000 characters are free, and then you are charged for the additional 75,000 characters sent for detection, translation, or both.\n\nGiven this example, each character including white spaces costs $0.00002. But again, this will depend and vary on your own use case as stated in the documentation.\n\nAs for the $300 free trial, you may contact Billing Support for tabulated information on this being consumed."
			}
		]
	},
	{
		"Question_title": "dialogflow no response if input too frequency",
		"Question_created_time": "2022-12-28T18:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/dialogflow-no-response-if-input-too-frequency/td-p/504274/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "If user input the wording too frequency.Dialogflow agent will hang and no intent message reply.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-30T09:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you share sample scenario with screenshots for me to reproduce your issue?"
			},
			{
				"Answer_created_time": "2023-01-02T18:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "https://photos.app.goo.gl/SrJqTTtTLVQ1fAQT6\u00a0\n\nhttps://photos.app.goo.gl/UdRemP5ByVt5K5eH6\u00a0\n\nIf we input the training phrases too frequent ( 2-3 phrases within 1 sec). The dialogflow will not response. You may refer to the screenshot."
			}
		]
	},
	{
		"Question_title": "dialogflow cx, how to obtain multiple answer from users",
		"Question_created_time": "2023-01-01T08:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/dialogflow-cx-how-to-obtain-multiple-answer-from-users/td-p/505122/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "hello, I am trying to build a chatbot with dialogflow cx that allows user to enter multiple answers, for example:what is your favorite color ? answer to select Black, Red, Greencurrently I have 3 suggestion chips built that allow user to click on each color,the problem I am running into is1. I tried looping back the user to the questions via route and condition like session.params.color !== null , but dialogflow wont let me do this cause the it thinks this will result in an infinite loop, what am I doing wrong?2. if the apporach above is not right, what is the best way to this problem?  Thanks in advance ",
		"Answer_list": [
			{
				"Answer_created_time": "2023-01-01T08:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "hello, I am trying to build a chatbot with dialogflow cx that allows user to enter multiple answers, for example:\n\nwhat is your favorite color ?\u00a0\n\nanswer to select\u00a0Black, Red, Green\n\ncurrently I have 3 suggestion chips built that allow user to click on each color,the problem I am running into is\n\n1. I tried looping back the user to the questions via route and condition like session.params.color !== null , but dialogflow wont let me do this cause the it thinks this will result in an infinite loop, what am I doing wrong?\n\n2. if the apporach above is not right, what is the best way to this problem?\u00a0\n\n\u00a0\n\nThanks in advance"
			}
		]
	},
	{
		"Question_title": "Cannot use listed and ready TPU from VM for pretraining",
		"Question_created_time": "2022-12-29T15:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cannot-use-listed-and-ready-TPU-from-VM-for-pretraining/td-p/504543/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "I signed up for the TRC program for the third time in two years. Now I barely created a preemptible v3-8 TPU. Before that, I could efficiently allocate five non-preemptible v3-8 TPUs. Even with that allocation, TPU is listed as READY and HEALTHY. However, when I want to access it from the pretraining script, I run into this error that I have never encountered before:\nFailed to connect to the Tensorflow master. The TPU worker may not be ready (still scheduling), or the Tensorflow master address is incorrect\nThe TPU is accessible, ready, and healthy, and the master URL is correct (it is automatically retrieved from the TPU_NAME, which I also double-checked).I also get this: ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-30T14:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Since this would be hard to reproduce on my end, you can try the workaround suggested here."
			}
		]
	},
	{
		"Question_title": "Simple but frustrating error: Google.cloud module not found",
		"Question_created_time": "2022-12-28T21:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Simple-but-frustrating-error-Google-cloud-module-not-found/td-p/504285/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 126,
		"Question_body": "Have installed Google Cloud via pip and CLI installer, yet programs cannot seem to see import statements from Google.cloud, returning the following error:line 9, in <module>\nfrom google.cloud import vision\nModuleNotFoundError: No module named 'google.cloud'Please advise and thank you for your time.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-30T09:41:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Have you tried installing google cloud vision? You can also check what python version you are using, this package is only supported in python versions 3.7 and up.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-12-30T09:41:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Have you tried installing google cloud vision? You can also check what python version you are using, this package is only supported in python versions 3.7 and up."
			}
		]
	},
	{
		"Question_title": "Save an text from Google Speech-to-text to cloud storage",
		"Question_created_time": "2022-12-26T11:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Save-an-text-from-Google-Speech-to-text-to-cloud-storage/td-p/503516/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 83,
		"Question_body": "Hi all , Is it possible to directly save the text from speech to text to cloud storage using python client library?At the moment I was able to perform this save operation only by waiting for the end of the speech-to-text operation, saving the text stack as a file and only then uploading it to the storage bucket",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-27T13:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I checked and there is no feature that can do your requirements in python client library. Suggested workaround is your current setup. You may raise a PIT if you want to have this feature."
			},
			{
				"Answer_created_time": "2022-12-30T01:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you!"
			}
		]
	},
	{
		"Question_title": "The kernel for MyTest.ipynb appears to have died. It will restart automatically.",
		"Question_created_time": "2022-11-24T11:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/The-kernel-for-MyTest-ipynb-appears-to-have-died-It-will-restart/td-p/492715/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 202,
		"Question_body": "Hello,I'm trying to run a test jupyter notebook of a LSTM model running tensorflow. I have tried setting the GPU memory limit like suggested here. But still the I get the error mentioned above. I can not find anything realted to GC vertex AI and everyone suggest setting the gpu memory in case of such errors. For reference I have tried to run this as well on my Vertex AI jupyter lab and it crashes as well. The only thing I added was this:gpus = tf.config.list_physical_devices('GPU')\nif gpus:\ntf.config.set_logical_device_configuration(\ngpus[0],\n[tf.config.LogicalDeviceConfiguration(memory_limit=12288)]\n)logical_gpus = tf.config.list_logical_devices('GPU')\nprint(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")On my personal computer it runs just fine, but it would take 13 hours to train which is not a option for me at the moment.Any help would be appriciated. Barnabas.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-28T12:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, can you share the error you encountered?"
			},
			{
				"Answer_created_time": "2022-11-29T09:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I get this when I reach the training process part."
			},
			{
				"Answer_created_time": "2022-11-29T10:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for sharing\u00a0@HBarnabas, since this seems to be an issue specific to your project, you may raise a\u00a01:1 GCP support. This kind of support has access to your internal resources and may check your issue in a more comprehensive way.\n\n0\u00a0Likes"
			},
			{
				"Answer_created_time": "2022-12-10T07:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I personnaly don't think it's a fair thing to ask a user to pay for support, while this issue only presists on GCP."
			},
			{
				"Answer_created_time": "2022-12-29T03:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have furthure investigated this issue and I can safely conclude that it's not an issue with my project. I have run the included tutorials for tensorflow 2 that are on the Vertex AI platform. I have run the entire notebook (06_rnns) and it ran everything just fine up until the point it reached the model which had LSTM layers (Exerecise 3). I have attached a screenshot of this.\n\nPlease help me figure this out."
			}
		]
	},
	{
		"Question_title": "fail counter - Dialogflow",
		"Question_created_time": "2022-12-22T07:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/fail-counter-Dialogflow/td-p/502611/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 57,
		"Question_body": "I require in dialogflow to control that when a user makes a mistake in the answer, it does not let him make a mistake more than x times. Who can have control of those failCreate a custom \"fail\" I require in dialogflow to control that when a user makes a mistake in the answer, it does not let him make a mistake more than x times. Who can have control of those failCreate a custom \"fail\" I require in dialogflow to control that when a user makes a mistake in the answer, it does not let him make a mistake more than x times. Who can have control of those failCreate a custom \"fail\"",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-23T12:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nWhat I could suggest is to look into fulfillment. This allows you to control responses programatically where you can add the logic to implement the checking of mistakes n number of times."
			},
			{
				"Answer_created_time": "2022-12-28T03:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for answering, but I don't know how to do it."
			}
		]
	},
	{
		"Question_title": "Instance schedule for TPU VM",
		"Question_created_time": "2022-12-22T06:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Instance-schedule-for-TPU-VM/td-p/502564/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 32,
		"Question_body": "Hello,I would like to attach an instance schedule to a TPU VM. I have followed these instructions, and can correctly create an instance schedule, but it seems I can only attach it to regular VMs, i.e., VMs listed under the \"instances\" of my project. Is there a way around this?Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-22T06:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI would like to attach an instance schedule to a TPU VM. I have followed these instructions, and can correctly create an instance schedule, but it seems I can only attach it to regular VMs, i.e., VMs listed under the \"instances\" of my project. Is there a way around this?\n\nThanks!"
			}
		]
	},
	{
		"Question_title": "Unable to correct labels that have been read",
		"Question_created_time": "2022-12-19T22:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-correct-labels-that-have-been-read/td-p/501393/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 80,
		"Question_body": "Hello, I used Document AI form processor to convert pdf file.I hit the confirm button to edit the label to the correct one because it was not reading correctly, but nothing responded.\n\u203b I`ve added the text and tried to save it.It seems to be a javascript error, but I don't think this has happened before.Do you have any info on this?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-20T11:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It would be better checked by engineers that have access to your project. You can file a PIT case here."
			},
			{
				"Answer_created_time": "2022-12-20T16:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "thank you for your reply.\n\nI will follow up with the link you provided."
			}
		]
	},
	{
		"Question_title": "Can I export a my model for internal usage ?",
		"Question_created_time": "2022-12-16T01:52:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-I-export-a-my-model-for-internal-usage/td-p/500236/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 382,
		"Question_body": "Hi,I created my models with Auto ML (image classification or object detection).Now, I would like to use these in my application, on local (disconnected).Is it possible to extract a model file from Auto ML that I can use (.pb for instance) ?After some researches, it seems to me that it is not possible but I would like to be sure.Else, how?Regards.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-20T01:56:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I just didn't understand that I need to select the \"edge\" option to have the \"export model\" available.\n\nThank you for the useful documentation .\n\nAs a additionnal question : can I know the version of Tensorflow used for these export model? (I have some incompatibiliy to use these in my software).\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-12-16T13:31:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I have found this documentation that enumerates the steps on exporting an image classification model in Tensorflow SavedModel for use on Docker container.\n\nAfter exporting your model to a Google Cloud Storage bucket you can use your exported model to make predictions in a Docker image. You may refer to this documentation on how to deploy to a container."
			},
			{
				"Answer_created_time": "2022-12-20T01:56:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I just didn't understand that I need to select the \"edge\" option to have the \"export model\" available.\n\nThank you for the useful documentation .\n\nAs a additionnal question : can I know the version of Tensorflow used for these export model? (I have some incompatibiliy to use these in my software)."
			},
			{
				"Answer_created_time": "2022-12-20T10:26:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "As this is more of a Tensorflow question, I suggest for you to ask in their forum."
			}
		]
	},
	{
		"Question_title": "VERTEX PIPELINE",
		"Question_created_time": "2022-12-17T12:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/VERTEX-PIPELINE/td-p/500836/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 50,
		"Question_body": "I have this problem: ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-19T12:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You may refer to this documentation regarding Vertex AI quota and limits as your guide. Custom-trained model quotas may be particularly helpful with your scenario."
			}
		]
	},
	{
		"Question_title": "Python 3.10 Wheels for Cloud TPU VM",
		"Question_created_time": "2022-12-16T05:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Python-3-10-Wheels-for-Cloud-TPU-VM/td-p/500329/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 128,
		"Question_body": "For running Cloud TPU VM workflows inside a docker container, I have the following two lines in the Dockerfile:I would like to get wheels for python:3.10. Ideally don\u2019t want to be building from source as that increases difficulty level significantly. Can developers maintaining the cloud-tpu-tpuvm-artifacts wheels please update the python versions. For reference here is a linked issue asked on the tensorflow forum",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-19T09:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nPlease create a feature request under Cloud TPU issue\u00a0to let the developers know your request. But please keep in mind that this will still be under consideration and there is no definite ETA."
			}
		]
	},
	{
		"Question_title": "Document AI does not import line_item child labels",
		"Question_created_time": "2022-12-16T06:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-does-not-import-line-item-child-labels/td-p/500346/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 139,
		"Question_body": "Hi,\nAfter exporting a dataset from one processor, we're seeing labels under line_item parent group fail to be imported for another processor, even though both processors have child labels in common.\nHow can we avoid this problem?\nThanks in advance,\nRasmus",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-16T07:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "OK, I think I found the issue. I believe there's a bug in the Document AI dataset exporter. Steps to repro:\n\nCreate a new invoice processor\nLabel a document using a child label, ie create parent label \"line_item\" and a child label \"unit\"\nExport document and inspect json. Notice unit type \"line_item/unit\" within the \"line_item\" section.\nCreate a new custom processor, setup parent label \"line_item\", child label \"unit\".\nImport document. Notice unit fails to identify.\u00a0\nLabel unit again.\nExport document and inspect json.\u00a0Notice unit type \"unit\" within the \"line_item\" section. This is the reason import didn't work.\n\nSo in order to successfully import documents from dataset of one processor to another, drop the \"line_item/\" prefix for all \"type\" fields using a text editor.\n\nDocument AI team, any chance for a fix for this?\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-12-16T07:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "OK, I think I found the issue. I believe there's a bug in the Document AI dataset exporter. Steps to repro:\n\nCreate a new invoice processor\nLabel a document using a child label, ie create parent label \"line_item\" and a child label \"unit\"\nExport document and inspect json. Notice unit type \"line_item/unit\" within the \"line_item\" section.\nCreate a new custom processor, setup parent label \"line_item\", child label \"unit\".\nImport document. Notice unit fails to identify.\u00a0\nLabel unit again.\nExport document and inspect json.\u00a0Notice unit type \"unit\" within the \"line_item\" section. This is the reason import didn't work.\n\nSo in order to successfully import documents from dataset of one processor to another, drop the \"line_item/\" prefix for all \"type\" fields using a text editor.\n\nDocument AI team, any chance for a fix for this?"
			}
		]
	},
	{
		"Question_title": "Remove Roboto Google Font from Dialogflow",
		"Question_created_time": "2022-12-13T11:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Remove-Roboto-Google-Font-from-Dialogflow/td-p/499260/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 116,
		"Question_body": "Hello dears, I have been asked to remove the call to this Google Fonthttps://fonts.gstatic.com/s/roboto/v30/KFOmCnqEu92Fr1Mu4mxK.woff2from a website using Dialogflow Messenger. I tried plugins, local loading of fonts and everything else but it is working for everything but the chatbot. Any idea how can fix this?Thank you.Lorenzo",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-14T23:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, what type of Dialogflow is the website using? CX or ES?"
			},
			{
				"Answer_created_time": "2022-12-14T23:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nES.\n\n--\n---Se il messaggio...."
			},
			{
				"Answer_created_time": "2022-12-15T12:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I suggest for you to raise a PIT ticket, an engineer that would have access on your project and is working on the product can check it in a more comprehensive way."
			}
		]
	},
	{
		"Question_title": "Vertex AI training pricing",
		"Question_created_time": "2022-03-09T22:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-training-pricing/td-p/402031/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 680,
		"Question_body": "I recently tried out Vertex AI and used AutoML to train my image classification model. I did train 3 sets and noticed from the billing that i was billed for 24 node hours. Is it so that i will be billed for the 8 node hours per model regardless if the training takes only one hour?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-15T13:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I passed by the same situation. My training was finished in 45 minutes, but the billing is reporting 8 hours of usage, that was the specified budget. 8 hours is the minimum value allowed. \ud83d\ude1e"
			},
			{
				"Answer_created_time": "2022-07-15T19:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I also used it and the same issue is there with me too.\n\nMaybe gcloud server is down."
			},
			{
				"Answer_created_time": "2022-12-13T19:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I had the same issue,and bow for image detection the minimum node hours are 20. It says if the training takes less, you will be reimbursed, but I am yet to see this."
			}
		]
	},
	{
		"Question_title": "DialogFlow CX integration with Genesys or CX One",
		"Question_created_time": "2022-12-13T07:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DialogFlow-CX-integration-with-Genesys-or-CX-One/td-p/499133/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 34,
		"Question_body": "Hi everyoneI'm trying to connect DialogFlow CX chat with some CX tools like Nice or Genesys. And the chat is working. \nBut my problem is to receive parameters from the platform and the way to use it in DialogFlow.Here an example of the input i receive on log activity.\nHow can i use the parameter firstName on DialogFlow ?    thank you for your help ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-13T07:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi everyone\n\nI'm trying to connect DialogFlow CX chat with some CX tools like Nice or Genesys. And the chat is working.\u00a0\nBut my problem is to receive parameters from the platform and the way to use it in DialogFlow.\n\nHere an example of the input i receive on log activity.\nHow can i use the parameter firstName on DialogFlow ?\u00a0\n\n\u00a0\n\n{\n  \"insertId\": \"-cggsxvqhcamq\",\n  \"jsonPayload\": {\n    \"queryParams\": {\n      \"payload\": {\n        \"context\": \"{\\n  \\\"id\\\": \\\"information\\\",\\n  \\\"lifespan\\\": \\\"2\\\",\\n  \\\"parameters\\\": {\\n    \\\"MSDynaContactId\\\": \\\"07cf2edb-ab48-ed11-bba2\\\",\\n    \\\"firstName\\\": \\\"FN\\\",\\n    \\\"lastName\\\": \\\"Kimura\\\",\\n    \\\"segment\\\": \\\"SILVER\\\",\\n    \\\"OvaloContactId\\\": \\\"OV0002\\\",\\n    \\\"mobilePhone\\\": \\\"6346373\\\",\\n    \\\"emailAddress1\\\": \\\"takashikimura.customer@gmail.com\\\",\\n    \\\"facebook\\\": \\\"\\\"\\n  }\\n}\"\n      },\n      \"parameters\": {}\n    },\n    \"queryInput\": {\n      \"languageCode\": \"fr\",\n      \"text\": {\n        \"text\": \"OV0002\"\n      }\n    },\n    \"session\": \"projects/profile-bot-dohn/locations/europe-west3/agents/3136149e-42f2-4ac5-ab47-/environments/draft/sessions/BusNo--\"\n  },\n  \"resource\": {\n    \"type\": \"global\",\n    \"labels\": {\n      \"project_id\": \"profile-bot-dohn\"\n    }\n  },\n  \"timestamp\": \"2022-12-13T13:23:25.801205Z\",\n  \"severity\": \"INFO\",\n  \"labels\": {\n    \"session_id\": \"BusNo--\",\n    \"agent_id\": \"3136149e-42f2-4ac5-ab47-\",\n    \"location_id\": \"europe-west3\"\n  },\n  \"logName\": \"projects/profile-bot-dohn/logs/dialogflow-runtime.googleapis.com%2Frequests\",\n  \"receiveTimestamp\": \"2022-12-13T13:23:26.476085542Z\"\n}\n\n\u00a0\n\n\u00a0\n\nthank you for your help"
			}
		]
	},
	{
		"Question_title": "Calling speech-to-text suddenly giving me bad transcripts ( starting 2022-Dec-1)",
		"Question_created_time": "2022-12-01T18:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Calling-speech-to-text-suddenly-giving-me-bad-transcripts/td-p/495250/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "For several months I've been using S2T to transcribe mp3 audio files (1 - 40 minutes long). It's given great results and since I'm using the gcloud CLI I can script batches of submissions.Today I submitted 10 jobs totalling 40 minutes and the results are all junk. The JSON transcript files which are normally 50-300K in size are a few hundred bytes long and just consist of a handful of individual random words. One of the files I had run on Nov-11 and it gave a good result (230K JSON file of basically correct transcriptions.) To test this, I ran the same file through the \"Create Transcription\" GUI and it gave exactly the same correct result.I modified my gcloud call (which was \"gcloud beta ml speech ....\" to remove the \"beta\" option, and the submission failed on encoding=mp3. I then added back in the \"alpha\" option after gcloud, this accepted the mp3 encoding but again returned the defective JSON transcrption file.It would really be a massive inconvenience to have to use the GUI to submit jobs one at a time. I went to the S2T \"What's new\" page and it didn't make any reference that seemed to explain this. (Incidentally there is a bug there where if you click on the \"Speech-To-Text v1\" drop down and choose \"Speech-to-Text\" under Public Features, you actually end up at page titled \"Speed-to-Text V2\" with \"Speech-To-Text On-Prem\" above it, and no information on either one. )Any suggestions will be greatly appreciated! ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-05T12:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This issue seems to be not reproducible on my end.\u00a0If you have premium support, you can check with\u00a0GCP Support\u00a0to further check your issue since this is specific to your project."
			},
			{
				"Answer_created_time": "2022-12-10T08:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks very much for taking a look at my post.\u00a0\n\nTo try to simplify things, I created a 20 sec audio file and saved as FLAC, stereo and mono MP3s. I used gcloud to invoke the recognize-long-running function. In this case I used the beta version since the standard l-r-r invocation (without alpha or beta) doesn't accept the encoding=mp3 option. The gcloud CLI I used is shown below.\n\nThe FLAC version transcribed perfectly. However the mp3 executions ran for a while and produced a small JSON file with just a word or two in it, and that with a confidence value of < 0.4 (the FLAC confidence values are all > 0.9 as they should be).\n\nThen I went to the cloud services web page that offers a simple demo (at\u00a0Speech-to-Text: Automatic Speech Recognition \u00a0|\u00a0 Google Cloud\u00a0\"Put Speech-to-Text into action\") This demo accepted all three versions of my file and produced a correct transcription.) So I wish I could talk to whoever wrote the code for that and ask what API they used, and why my gcloud invocation which should have done the same thing failed.\u00a0\n\nIn addition to that, since I'm a registered Cloud user, there's a more advanced GUI job submitter at\u00a0https://console.cloud.google.com/speech/overview?project=vernal-design-355021\n\nThis tool also processes all three files just fine.\n\nSo if I could only talk to the programmers who wrote the code for these demo tools, maybe I could learn why my gcloud invocations, that should do the same thing, fail for mp3 files.\n\nHere's the command I used in the gcloud shells (SDK and even cygwin) :\n\ngcloud beta ml speech recognize-long-running gs://debug_cli/files/221209_1758-mono.mp3 --language-code=en_US --async --encoding=mp3 --channel-count=1 --no-separate-channel-recognition --model=latest_long\n\nThe demo files I used are in a bucket called \"debug_cli\" but I haven't yet figured out an easy way to make that public."
			}
		]
	},
	{
		"Question_title": "Audio export bucket - only save incoming audio",
		"Question_created_time": "2022-12-08T06:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Audio-export-bucket-only-save-incoming-audio/td-p/497482/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 105,
		"Question_body": "Hello everyoneWe're using dialogflow cx for a call bot. For later use we load the exported audio from the dialogflow bucket and display it in a seperate frontend.As it is written in the tooltip help \"Configure the incoming audio to be automatically exported to the specific Google Cloud Storage destination by Dialogflow.\"I expect to only get the incoming audio files (what the user says on the phone). But I also get the responses from the agent defined in \"Agent says\":Is there a way to prevent this from happening? Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-09T13:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "As this is specific to your project, you may file a 1:1 Support. A dedicated resource will work with you on this scenario.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-12-09T13:01:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "As this is specific to your project, you may file a 1:1 Support. A dedicated resource will work with you on this scenario."
			}
		]
	},
	{
		"Question_title": "Wrong no-inputs transition",
		"Question_created_time": "2022-12-09T02:04:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Wrong-no-inputs-transition/td-p/497783/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 105,
		"Question_body": "For quite a long time we have been facing a problem connected to a wrong transition between Event Handlers (\"no-input-1\"/\"no-input-2\"/\"no-input-n\") OR even a missing events.\nThat is recurring problem that appears from time to time on different agents/flows.\n\n---\n\n#1 Example\n\nWrong transition between no-inputs. From \"no-input-1\" we went to \"no-input-default\" instead of \"no-input-2\", that was present on the Page.\n\nAgent structure:\n1) Agent starts with a \"Default Start Flow\", that forwards customer to another 4 flows.\n2) These flows have the same construction \u2014 one by one questions with some input from a user.\nQuestion 1: Please name the letters for Zinc (Zn);\nQuestion 2: Please name the letter for Carbon (C);\nQuestion n: ...\n3) On the page we have several no-input handlers (no-input-1, no-input-2, ...).\n4) The last no-input event moves user to the \"End Session\" Page.\n\n\nExpected Agent behaviour:\n1) If user is silent for the first time -> GDF raises no-input-1 event\n   and replies with a predefined Fulfilment.\n2) If user is silent for the second time -> GDF raises no-input-2 event\n   and replies with a predefined Fulfilment + moves customer to the \"End Session\" page.\n\n\nReal Agent behaviour:\n1) If user is silent for the first time -> GDF raises no-input-1 event\n   and replies with a predefined Fulfilment.\n2) If user is silent for the second time -> GDF raises \"no-input-default\" event (instead of \"no-input-2\")\n   and moves user to the \"Start\" page in the \"Default Start Flow\".\n\n---\n\n#2 Example\n\nMissed one event handler. We had two Event Handlers on a single Page: \"no-input-1\", \"no-input-2\".\nOn empty user input, \"no-input-2\" raised instead of \"no-input-1\"\n\n---\n\nTemporary solution (but sometimes it doesn't work!)\n\n1) Go to \"Agent Settings\" => \"ML\"\n2) Select all flows and press \"Train\"\n\n---\n\n\u2757Has someone faced this issue? How can we fix it?\u2757",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-09T10:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This issue seems to be specific to your project. You may raise a 1:1 support ticket here."
			}
		]
	},
	{
		"Question_title": "Access public saved query in \"Using BigQuery ML to predict basketball outcomes\" tutorial",
		"Question_created_time": "2022-12-08T11:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Access-public-saved-query-in-quot-Using-BigQuery-ML-to-predict/td-p/497608/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 28,
		"Question_body": "I am trying to complete the tutorial at this link: https://cloud.google.com/bigquery-ml/docs/bigqueryml-ncaa but cannot access the public saved queries for the feature input,  training, or evaluation data. Here is the link to the feature input query: https://bigquery.cloud.google.com/savedquery/1057666841514:77711b21274b4c6485c907483ef2f6feIt just redirects me to the BigQuery console. Please let me know if you have any tips or how to access the query. Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-09T10:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have tried to follow the documentation and there seems to be a problem with schema import level, and probably it would affect accessing the saved query on my end as well. I have provided feedback on the step that I am encountering the problem. I encourage you to provide feedback regarding your issue as well with the documentation so that the team working on creating these documentation can alter and provide a working setup on our end."
			}
		]
	},
	{
		"Question_title": "Google Cloud Api TTS",
		"Question_created_time": "2022-12-07T17:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Api-TTS/td-p/497285/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 72,
		"Question_body": "I am using the TTS and STT services of Google Cloud API together. I am inquiring because an error has occurred only in the TTS function since last week without code modification or redistribution. When I checked the data from the backend log, I found no abnormality and the Google Cloud server is sending 400 errors. There is no problem with STT, but only TTS is experiencing problems. Please check.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-09T02:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You may raise an issue in this Public Issue Tracker. Engineers that is working closely to the said products will check your case."
			}
		]
	},
	{
		"Question_title": "Automl training process",
		"Question_created_time": "2022-12-07T08:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Automl-training-process/td-p/497090/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 70,
		"Question_body": "Hey folk,\nI'm using automl approach to train some classification text models. I had trained two models with the same data set and got a big difference in the performance metrics between the models. e.g. recall model 1: 0.80, recall model 2: 0.65.\nSo, I have a question about how the training process works, and if the model depends on a stochastic process (seed).",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-09T01:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For starters, you may read this documentation about precision and recall.\u00a0Recall tells us, from all the test examples that should have had the label assigned, how many were actually assigned the label. On the other hand, Precision tells us, from all the test examples that were assigned a label, how many actually were supposed to be categorized with that label. Your optimization on either of the two will depend on these use cases.\u00a0\n\nIf you really are after a comparison of the accuracy of your models, the recommended metric is Average Precision.\u00a0\u00a0It measures how well your model performs across all score thresholds and this is under the precision-recall curve."
			}
		]
	},
	{
		"Question_title": "Empty Search result from previously Indexed Product set",
		"Question_created_time": "2022-11-29T21:18:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Empty-Search-result-from-previously-Indexed-Product-set/td-p/494580/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 74,
		"Question_body": "Hi everyone,\nI have a problem with vision product search. Will the index become invalid after being idle for a period of time, and will there be charges for searching under the invalid index? And how long will the index become invalid after being idle?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-01T11:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThe to answer your questions above:\n\n1. ) Will the index become invalid after being idle for a period of time?\u00a0And how long will the index become invalid after being idle?\n\nUnfortunately this information cannot be shared publicly hence there is no detailed information posted on the public documentation. But to confirm, your observation is correct. The index becomes invalid after being idle for a period of time. Just make sure to send requests to keep the index available for you to prevent this from happening.\n\n2.) Will there be charges for searching under the invalid index?\n\nAFAIK you won't be billed if Google did not use any resources to process your request. But if your request pushed through and it returned data (status 200) you will be billed for this."
			},
			{
				"Answer_created_time": "2022-12-09T01:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\nNot to hijack the thread, but I have the same issue, so what can we do to get the products indexed again ?\nDo we need to re-create the whole structure or can we somehow kickstart the indexing of the products again ?"
			}
		]
	},
	{
		"Question_title": "CX Phone gateway caller_id lost when forwarding to US-number",
		"Question_created_time": "2022-10-06T04:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/CX-Phone-gateway-caller-id-lost-when-forwarding-to-US-number/td-p/475112/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 126,
		"Question_body": "Hello everyoneWe use dialogflow cx with the cx phone gateway. But because we aren't based in the US we used a local phone number which we forward to the US-number from the gateway.This worked great for some days, but now we are experiencing the following issue:\nThe caller_id from the original caller does not get sent in the webhook response anymore. It only sends the number which forwards the call.Does someone know how to prevent this from happening?Thanks for your help!- Federico",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-08T06:14:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "looks like this is a problem with the local network/phone provider, which does not transfer the caller_id correctly.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-10-07T08:39:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "In this Stackoverflow post\u00a0you might find useful information. As Dialogflow CX Phone Gateway is in Preview. You may also consider reporting this issue here\u00a0to improve this feature. Take a look at this issue tracker information about how to report issues. Because it uses the number of \"stars\" (people who have indicated interest in an issue) to prioritize work on the platform, you should search existing issues before you make a new entry."
			},
			{
				"Answer_created_time": "2022-10-10T00:29:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thanks for your response.\nUnfortunately the Stackoverflow post you mentioned does not help me because I already get a caller_id but its not the right one."
			},
			{
				"Answer_created_time": "2022-12-08T06:14:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "looks like this is a problem with the local network/phone provider, which does not transfer the caller_id correctly."
			}
		]
	},
	{
		"Question_title": "How to retrive transcript of virtual agent and customer in dialogflowcx?",
		"Question_created_time": "2022-12-01T03:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-retrive-transcript-of-virtual-agent-and-customer-in/td-p/494977/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 9,
		"Question_score_count": 0,
		"Question_view_count": 302,
		"Question_body": "I have two questions?1. How to retrieve transcript of virtual agent and customer in dialogflowcx? After the virtual agent has triggered the \"live agent handoff\" the call reaches our internal contact centre. I want to fetch the transcript of the conversation so far to show to the real agent. Using API/Client library/anything.2. When live agent hand off happens, then there are certain parameters passed on. There is also an option to app a JSON in the dialogue option. Is there a way to retrieve this JSON/Parameters using an API call?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-02T10:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThere is no feature available as of the moment to extract the conversation using the API. But as a workaround you can get this information from the generated logs of Dialogflow. The logs are located in Cloud Logging as shown in this example. The provided example exports data manually, but you can use Cloud Logging API to programatically export the logs and automate this process.\u00a0\n\nYou can\u00a0\u00a0create a feature request in GCPs\u00a0public issue tracker\u00a0regarding your request as this is a useful feature to have.\u00a0Please keep in mind that when you create a feature request, it still needs to be analyzed and considered by the product team and a definite ETA is not guaranteed."
			},
			{
				"Answer_created_time": "2022-12-04T23:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @ricconoel\u00a0Thanks for your quick reply.\n\nI was able to fetch the transcript using Conversations resource. This resource exists under the umbrella of Dialogflow ES v2beta1 and is not a part of Dialogflow CX. Is it a suggested way? I am asking as we are planning to use in a production app and want to avoid a situation where there this does not work, or stopped, etc.\n\nI am attaching the code snippet for your better understanding\n\n\u00a0\n\nfrom google.cloud import dialogflow_v2beta1\nfrom google.cloud.dialogflowcx_v3.services.agents.client import AgentsClient\n\n# Set the project and location IDs for your Dialogflow CX project\nproject_id = \"ccai-flex-aman\"\nlocation_id = \"us-central1\"\nagent_id = \"8bf5e181-f10f-4336-a831-c7936aa55585\"\nagent = f\"projects/{project_id}/locations/{location_id}/agents/{agent_id}\"\nparent = f\"projects/{project_id}/locations/{location_id}\"\n\nagent_components = AgentsClient.parse_agent_path(agent)\n\nlocation_id = agent_components[\"location\"]\nif location_id != \"global\":\n    api_endpoint = f\"{location_id}-dialogflow.googleapis.com:443\"\n    print(f\"API Endpoint: {api_endpoint}\\n\")\n    client_options = {\"api_endpoint\": api_endpoint}\n\n# Set up the Dialogflow CX client\nclient = dialogflow_v2beta1.ConversationsClient(client_options=client_options)\n\n# Use the API to retrieve a list of all the conversations in the project\nconversations = client.list_conversations(\n    parent=f\"projects/{project_id}/locations/{location_id}\",\n)\n\n# Print the list of conversations\nfor conversation in conversations:\n    print(conversation)\n\n# Select the conversation for which you want to fetch the transcript\nconversation_id = \"081mM5l9Ry8Qeu_BFg8XjuZ8Q\"\n\n# Use the API to retrieve the full conversation history\nmessages = client.list_messages(\n    parent=f\"projects/{project_id}/locations/{location_id}/conversations/{conversation_id}\"\n)\n\n# Format the conversation history into a transcript\ntranscript = \"\"\nfor message in messages:\n    # Add the message text and timestamp to the transcript\n    transcript += f\"{message.content} ({message.create_time})\\n\"\n\n# Print the transcript\nprint(transcript)\n\n\u00a0\n\nOutput( 2nd part):\n\nYou have not provided a confirmation number yet. Let me transfer you to a live agent to further assist you. (2022-12-01 08:17:25.522847+00:00)\n (2022-12-01 08:17:25.483295+00:00)\n (2022-12-01 08:17:21.059599+00:00)\n (2022-12-01 08:17:21.021359+00:00)\nWhat is your flight confirmation number? (2022-12-01 08:17:14.826351+00:00)\nlooking for my flight (2022-12-01 08:17:14.800063+00:00)\nHi! How are you doing? (2022-12-01 08:17:09.682911+00:00)"
			},
			{
				"Answer_created_time": "2022-12-05T08:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dialogflow ES and Dialogflow CX have a huge difference in its architecture. So it really depends on your needs. For example Dialogflow CX implements flows and Dialogflow ES does not. If you won't be needing features of Dialoflow CX then you can go with Dialogflow ES. See reference for CX and ES comparison."
			},
			{
				"Answer_created_time": "2022-12-06T02:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@ricconoel\u00a0Please note that the above code snippet is fetching the conversation history of a Dialogflow CX agent using Dialogflow ES API. Is that an accepted approach?\n\nfrom google.cloud import dialogflow_v2beta1\n\nThis directs to Dialogflow ES, but it can fetch conversations from CX"
			},
			{
				"Answer_created_time": "2022-12-06T08:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I see. I never have thought of that. If you are able to fetch using this method I think you can continue using it."
			},
			{
				"Answer_created_time": "2022-12-06T23:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Casually asking - Is this is a bug or a feature? Asking because we are planning to use it in a large scale application and if Google decides to remove this then there will be a huge impact on our customers."
			},
			{
				"Answer_created_time": "2022-12-07T10:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Based on the Conversation History in CX, it is still in \"preview\" and is not recommended to use this yet in production environments. I suggest to wait for this feature to be in \"general availability\" before using it in production.\u00a0\n\nAlso it might be better to create an issue in GCP issue tracker\u00a0to confirm to the engineering team if using Diagloflow ES Conversations with Dialogflow CX Conversations is an expected behavior."
			},
			{
				"Answer_created_time": "2022-12-05T01:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Also, another question. How long does the conversation history remain in the Google systems? Is there a timeline before these conversations are purged?"
			},
			{
				"Answer_created_time": "2022-12-05T08:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It should be retained for 400 days as per document."
			}
		]
	},
	{
		"Question_title": "Attempt to Populate System Parameters with API Response from Dialogflow Webhook Not Working?",
		"Question_created_time": "2022-12-05T09:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Attempt-to-Populate-System-Parameters-with-API-Response-from/td-p/496266/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "Hi, \n\nI've been able to make a webhook that calls my own nodejs api where i maintain a database of faux employees. The API responds, and I have been able to populate the Agent's response in the fulfillmentResponse Object. I've tried formatting the sessionInfo Object in a number of ways and can't update system parameters like @sys.last-name (Note: I also tried \"@sys.last-name\" as the parameters key, but it didn't work)? Is there something wrong w/ my JSON formatting? Here's my JSON formatting, and the console.log of what the app sent:     ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-06T10:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Just to confirm, you are using Dialogflow CX? Can you provide a sample code on your webhook that can be used to reproduce the issue? Can you try separating the `sessionInfo` object with the `fulfillmentResponse` object just to check if it has the same behavior?"
			}
		]
	},
	{
		"Question_title": "Issues with importing aiplatform",
		"Question_created_time": "2022-11-14T01:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Issues-with-importing-aiplatform/td-p/489087/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 0,
		"Question_body": "Hi, I am following this tutorial on model deployment (https://codelabs.developers.google.com/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.When running \"from google.cloud import aiplatform\", I get the following error message:The versions of the concerned libraries are shown below.I have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?Thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-14T17:15:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-14T15:37:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI tried the code labs link and I did not encounter your error.\u00a0 I ran the code in a \"Managed Notebook\" as shown in the labs. Here are the versions of the libraries in my notebook:\n\ngoogle-api-core ==\u00a02.8.0\ngoogle-api-python-client ==\u00a02.65.0\ngoogle-cloud-aiplatform == 1.18.2\ngrpcio == 1.50.0\ngrpcio-gcp == 0.2.2\ngrpcio-status == 1.48.2\n\nJust to confirm, where did you run the code?"
			},
			{
				"Answer_created_time": "2022-11-14T17:15:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!"
			},
			{
				"Answer_created_time": "2022-11-15T08:18:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "You can mark this as solved so future readers can easily determine the solution."
			},
			{
				"Answer_created_time": "2022-11-30T03:31:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "in process ImportError: cannot import name 'aiplatform' from 'google.cloud' this error I am getting while I am importing it into apache beam- dataflow pipeline"
			},
			{
				"Answer_created_time": "2022-12-06T05:35:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "@ricconoel\u00a0can you help me here"
			}
		]
	},
	{
		"Question_title": "AutoML Table models not deploying",
		"Question_created_time": "2022-12-02T11:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Table-models-not-deploying/td-p/495557/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 62,
		"Question_body": "I have a bunch of AutoML table models. I have been able to deploy them and do online prediction till last week. But starting this week, when I try to deploy the models for doing online prediction, I see an INTERNAL error every time. There has been no change in the models in the meantime. Any idea how this can be fixed?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-05T10:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you encounter an INTERNAL error, contact GCP support so proper troubleshooting can be performed using their internal tools to inspect your project."
			},
			{
				"Answer_created_time": "2022-12-05T10:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@ricconoel\u00a0I don't have a paid support plan. I only have Basic Support. Is there any way I can contact Google Support using this plan? If yes, can you send me a link where I can contact them?"
			},
			{
				"Answer_created_time": "2022-12-05T10:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I see. The next best thing for you to contact support is creating a public issue tracker\u00a0since as you have mentioned there is a sudden internal error occurring."
			}
		]
	},
	{
		"Question_title": "DialogFlow CX triggering no-match when adding words like \"issue\" or \"working\".",
		"Question_created_time": "2022-11-29T15:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DialogFlow-CX-triggering-no-match-when-adding-words-like-quot/td-p/494494/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 114,
		"Question_body": "I am having an issue with DialogFlow CX not recognizing certain words. For example, if I test \"Internet\" it will trigger the proper intent. \"Internet not\" also triggers the proper intent. But, when I said \"Internet not working\" the system triggers a no-match. There are no other intents with the word \"working\" listed as a training phrase. Help?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-30T16:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for sharing\u00a0@pmurray123, have you tried doing the suggestions in this SO post?"
			},
			{
				"Answer_created_time": "2022-12-02T10:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes. I have tried both Advanced and Basic NLU multiple time, and train regularly. I am still having issues. The system will recognize \"help with iPad\", but \"help with an iPad\" triggers no-match. I can't figure it out!"
			},
			{
				"Answer_created_time": "2022-12-05T09:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Since the SO post was not able to offer some resolution to your scenario, I suggest to raise a\u00a0Public Issue (PIT). An engineer specialized in the product can check your issue and offer solution."
			}
		]
	},
	{
		"Question_title": "Insufficient data error in Others You May Like",
		"Question_created_time": "2022-11-29T04:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Insufficient-data-error-in-Others-You-May-Like/td-p/494198/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 131,
		"Question_body": "Hi,I'm getting the below error while training Others You May Like Model even though all the data requirements are being satisfied.INSUFFICIENT_TRAINING_DATA Recommendation model others-you_others-you_1669706866636 cannot be trained because of insufficient data.     Could anyone please help with the same?Thanks,Anusha",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@anushap2094, what type of data are you using to train your model?"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0anjelab, we are using synthetic apparel data to train the model."
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As suggested in this Google Group post, it is always recommended to train real and live data. Few thousand events or few events per day may not be able to work well even if it meets the minimum requirements on training a model."
			}
		]
	},
	{
		"Question_title": "Just curious, can I use cloud bigtable as a feature store instead of using vertex AI feature store?",
		"Question_created_time": "2022-06-28T03:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Just-curious-can-I-use-cloud-bigtable-as-a-feature-store-instead/td-p/435633/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 500,
		"Question_body": "I am trying to migrate my features table stored in bigquery to a feature store with lower latency. I'm choosing whether I should make use of vertex AI feature store or just cloud bigtable.My features tables are <10MB, and it is used for real time prediction hence a database with low latency is sufficient.Im just wondering aside from pricing, and ease of exporting data (bigtable requires more steps than vertex ai feature store), what is the difference between the 2 options?Also, what type of database (eg: bigtable or redis?) is vertex AI feature store behind the scenes, when I am creating the feature store using the web UI?  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-04T15:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, Chiayi,\n\nThe main difference between these 2, is that Vertex AI Feature Store is not considered a database as such, it is more like a product that provides a centralized repository for organizing, storing, and serving ML features [1]. In the other hand, Cloud Bigtable is a scalable NoSQL database service for large analytical and operational workloads [2]. More about [1][2].\n\n[1] https://cloud.google.com/vertex-ai/docs/featurestore/overview\n[2] https://cloud.google.com/bigtable/docs/overview"
			},
			{
				"Answer_created_time": "2022-12-05T06:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks\u00a0@comaro\u00a0\n\nI have a followup question on this as i am also in the phase of deciding between the two.\n\n`it is more like a product that provides a centralized repository for organizing, storing, and serving ML features` - but the idea of organizing, storing and serving can also be done by BT.\u00a0\n\nSo, how does one decide or what factors impact the decision-making between the two, when picking one ? I guess the criteria would be super helpful.\u00a0\n\nIn my case, I have a BigTable instance, that is currently being used for other purposes but the volumes is not large enough currently (it will be in a while) but since its run in production, i have to deploy it in a fault tolerant and scalable way, which doesn't make it cost-effective right now.\n\nSo, i am keen to understand, whether BT will make sense for my fraud-detection model for computing features on the fly, or would it be best to use the feature store. I am using Vertex AI for my ML model architecture.\n\nMy model will need to compute a few features dynamically and input into the predictor."
			}
		]
	},
	{
		"Question_title": "Cloud Translation Permission",
		"Question_created_time": "2022-11-15T07:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Translation-Permission/td-p/489632/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 666,
		"Question_body": "So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:I have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org. I did verify that the service account has the permission from the https://console.cloud.google.com/iam-admin/troubleshooter so and that my organization's admin sees that the service account is granted access through ancestor policies.  So what else can we check? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-15T08:30:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-15T08:30:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied."
			},
			{
				"Answer_created_time": "2022-12-04T02:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi Everyone,\u00a0\n\nI am facing the same error. I am using a separate cloud Linux and using nodejs client library\u00a0\n\n@Google-cloud/translate.\u00a0 I am using service account key and set it at env variable\u00a0\nGOOGLE_APPLICATION_CREDENTIALS.\n\u00a0\nCase 1) When I execute the the translation code directly as java script it does execute successfully.\nCase 2) When I call this client library inside an Express REST Application, I am getting permission denied error. I have verified that it is same service account which has IAM access role \"Cloud Translation API Admin\". This is very strange.\nComplete Error :\u00a0 \u00a0\"\"\u00a0\nError:\u00a07\u00a0PERMISSION_DENIED:\u00a0Cloud\u00a0IAM\u00a0permission\u00a0'cloudtranslate.generalModels.predict'\u00a0denied.\"\"\n\u00a0\nAny help is appreciated!"
			}
		]
	},
	{
		"Question_title": "Cloud Vision API in Vertex AI?",
		"Question_created_time": "2022-11-28T02:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Vision-API-in-Vertex-AI/td-p/493648/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 516,
		"Question_body": "  Hi,I am a newbie in Google Cloud and i have an elementary conceptual question about the dependency between Cloud Vision API and Vertex AI or the recently launched Vertex Vision AI.I have an app that makes predictions on images using Google Vision AI API ImageAnnotatorClient() Is this API going to be part of  Vertex AI  or Vertex Vision AI?Or in other words, should I modify the below code to make it part of Vertex AI/Vertex Vision AI?          ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-29T10:23:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Vision API should be affected by Vertex Vision AI and your code should stay as is. Vision API is primarily used to detect vision features like\u00a0image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content. While Vertex Vision AI is an end to end solution to ingest, analyze and store video and image data.\u00a0Vertex AI Vision lets users build and deploy applications with a simplified UX.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-29T10:23:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Vision API should be affected by Vertex Vision AI and your code should stay as is. Vision API is primarily used to detect vision features like\u00a0image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content. While Vertex Vision AI is an end to end solution to ingest, analyze and store video and image data.\u00a0Vertex AI Vision lets users build and deploy applications with a simplified UX."
			},
			{
				"Answer_created_time": "2022-12-04T00:18:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you Ricconoel, this clarifies my doubts."
			}
		]
	},
	{
		"Question_title": "What is the limit of ProductSet per location",
		"Question_created_time": "2022-11-03T13:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-is-the-limit-of-ProductSet-per-location/td-p/485590/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 66,
		"Question_body": "When using Google Vision's Product Search API, does anyone have an idea the maximum number of ProductSets allowed in a location(region)?This documentation shares limits on reference images per ProductSet, but it says nothing about ProductSets per location",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-04T11:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, iola,\n\nIn your Google Cloud Project, type \u201cCloud Vision API\u201d in the search bar.\nOn the left, click \u201cEnabled APIs and services\u201d > Quotas.\n\nIn the Quota column it is listed the different types of actions, accompanied by their limit in the Limit column.\n\nPlease tell me if this information is useful."
			},
			{
				"Answer_created_time": "2022-12-03T14:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for the response comaro. I followed the steps as you described and I could see the quota table, but none of the rows mentions the limit on the maximum Product Sets that one may have in a location(region)"
			}
		]
	},
	{
		"Question_title": "Text recognition language (and numbers only)",
		"Question_created_time": "2022-12-01T00:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Text-recognition-language-and-numbers-only/td-p/494929/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 114,
		"Question_body": "Dear Fellow Developers,I have 2 issues with the Google Vision API for handwriting recognition.First of all, I don't see an option to specify the recognition language. Sometimes it detects numbers as Russian characters. And second: sometimes I only want to recognize numbers. Is there a setting for this?I use a PHP library (Google\\Cloud\\Vision\\V1\\ImageAnnotatorClient) for handwriting recognition in an image. I get my result after calling the documentTextDetection() function, like this:\n$response->getFullTextAnnotation()->getText()",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-03T00:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you provide a sample image for this scenario? This is for us to reproduce your case."
			}
		]
	},
	{
		"Question_title": "EntityAnalysis, Version 2 model in natural language API",
		"Question_created_time": "2022-11-17T12:28:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/EntityAnalysis-Version-2-model-in-natural-language-API/td-p/490557/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 7,
		"Question_score_count": 0,
		"Question_view_count": 252,
		"Question_body": "Hello,could anyone share the python code on how to get natural language API to use version 2 for Entity  Sentiment Analysis?\nThe Demo can be run for that, but it seems like in the docs this part is missing:\nhttps://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeEntitySentiment\n\nHowever, for the classification, it is possible, as it is described here: \nhttps://www.googlecloudcommunity.com/gc/AI-ML/Version-2-model-in-natural-language-API/m-p/484641\n\nThanks ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-18T15:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "When using the classifyText method of the API, the classification models are available. However, for Entity Sentiment Analysis there is no mention of available options or models that can be declared. Can you add more details of your use case to review?"
			},
			{
				"Answer_created_time": "2022-11-22T20:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I was using a demo from:\nhttps://cloud.google.com/natural-language\n\nAnd here the API provides an example for the V2 of the model, including Entity Analysis for the German\nHowever, when I go to the docs (https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeEntitySentiment)\nI can`t see the option to use V2 model for Entity Analysis, one V1\nHowever, for the Classification Model there is an option:\u00a0https://cloud.google.com/natural-language/docs/reference/rest/v1/ClassificationModelOptions\n\nSo, my question is: can I use V2 model for the Entity Analysis?\u00a0\nThanks"
			},
			{
				"Answer_created_time": "2022-11-23T13:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The Natural Language API documentation includes all available code samples. Could you direct me to the one you refer to? Most likely there is some clarification to make, as there is no mention of a V2 model that can be used."
			},
			{
				"Answer_created_time": "2022-11-24T05:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am interested in the:\nhttps://cloud.google.com/natural-language/docs/samples/language-entities-text\n\nHowever, I want to use V2 model for Entity Analysis, as it supports German\nIs it possible to use V2 here ?\u00a0\n\nThanks"
			},
			{
				"Answer_created_time": "2022-11-25T15:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "German language support is not available for Entity Sentiment Analysis, as the only listed languages\u00a0for it are English, Japanese, and Spanish. There might have been some confusion with the V2 model for Content Classification, since it includes broader language options. However, this model is only for Content Classification, not for Entity Sentiment Analysis, which offers no such option.\n\nKeep in mind that standalone Sentiment analysis and Entity analysis do offer support for German, in case you need either of those API features instead. As a note, you can raise a Feature Request in Google\u2019s Issue Tracker. It would be brought to the attention of the appropriate teams to implement support in the future; however, there\u2019s no assurance of an ETA."
			},
			{
				"Answer_created_time": "2022-12-01T06:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you send me a link please to a mentioned standalone Entity\u00a0analysis service I can use for\u00a0 German?\u00a0\nThanks"
			},
			{
				"Answer_created_time": "2022-12-01T09:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For standalone Entity Analysis, you can review the following documentation overview. Language support for this API feature can be seen here, and German is listed under the supported languages."
			}
		]
	},
	{
		"Question_title": "Method for Photo Analysis (AI/Machine Learning/OCR/Others)",
		"Question_created_time": "2022-12-01T06:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Method-for-Photo-Analysis-AI-Machine-Learning-OCR-Others/td-p/495040/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 47,
		"Question_body": "Hi,I am very new to Google Cloud Platform, here I attached a photo of a cable being cut and measured on measuring tape. From the photo, the dimension of the cable is 5500mm. Because I am not sure which method I can use, either AI? Machine Learning? OCR engine? or others?.This photo was captured and stored in Google Drive (using Appsheet), is there any solution in GCP able to read from this photo and  analyze it, finally tell this is 5500.Anyone from community that took part in such project or maybe a similar project able to enlighten me in this?Thanks in advance for any valuable opinion!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-12-01T06:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI am very new to Google Cloud Platform, here I attached a photo of a cable being cut and measured on measuring tape. From the photo, the dimension of the cable is 5500mm. Because I am not sure which method I can use, either AI? Machine Learning? OCR engine? or others?.\n\nThis photo was captured and stored in Google Drive (using Appsheet), is there any solution in GCP able to read from this photo and\u00a0\u00a0analyze it, finally tell this is 5500.\n\nAnyone from community that took part in such project or maybe a similar project able to enlighten me in this?\n\nThanks in advance for any valuable opinion!"
			}
		]
	},
	{
		"Question_title": "Dialogflow cx sdk pricing",
		"Question_created_time": "2022-11-12T02:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-cx-sdk-pricing/td-p/488718/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 96,
		"Question_body": "Hi all! \nI'm using Dialogflox cx python libraries to make some tests with intents matching. To do these tests, I'm using the match_intent method of the SessionClient class. I digged in the pricing table of Dialogflow CX but as far as I searched I didn't understand if there's a pricing for request made with SessionClient, and I'm here to ask for your help.\n\nThank you in advance!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-14T16:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThank you for reaching out. Will look into this."
			},
			{
				"Answer_created_time": "2022-11-15T09:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I created a public issue tracker regarding your question. You can track the progress of the issue using the link I have provided.\u00a0Please keep in mind that the issue has to be analyzed and considered by the product team and I can't provide you an ETA for it."
			},
			{
				"Answer_created_time": "2022-11-30T15:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you! I deeply appriciate your help!"
			}
		]
	},
	{
		"Question_title": "DocAI - Form Processor table issue",
		"Question_created_time": "2022-11-23T22:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DocAI-Form-Processor-table-issue/td-p/492493/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 131,
		"Question_body": "Hello,\nI used Document AI form processor to convert pdf file table data into table object.\nSome table data are not converted properly.In the sample file, the 3rd table is not detected columns properly.\nCould you please throw some light on this ?Source PDF fileAfter conversion using form processor. The third one is having issue on column detection.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-28T12:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Document AI is powered by Machine Learning models and results are not always what is expected due to number of reasons (input quality, volume, etc.), but this service is continuously\u00a0improved.\n\nYou may file a PIT case and a Google Engineer that is directly working on the product can assist you."
			},
			{
				"Answer_created_time": "2022-11-28T20:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nThanks much. I will file a case and I hope I can get a solution over there."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX logs sink to BigQuery. sink error - field: value is not a record",
		"Question_created_time": "2022-11-16T07:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-logs-sink-to-BigQuery-sink-error-field-value-is/td-p/490079/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 362,
		"Question_body": "I am using google cloud logging to sink Dialogflow CX requests data to big query. BigQuery tables are auto generated when you create the sink via Google Logging.We keep getting a sink error - field: value is not a record.This is because pageInfo/formInfo/parameterInfo/value is of type String in BigQuery BUT there are values that are records, not strings. One example is @sys.date-timeHow do we fix this?We have not tried anything at this point since the BigQuery dataset is auto created via a Logging Filter. We cannot modify the logs and if we could modify the table schema, what would we change it to since most of the time \"Value\" is a String but other times it is a Record",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-18T10:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Currently working with your question."
			},
			{
				"Answer_created_time": "2022-11-18T16:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "With the information you shared, I\u2019m afraid it\u2019s not possible to provide a good solution.\n\nPlease include sufficient code and any guides you followed or your process so that we can analyze the issue."
			},
			{
				"Answer_created_time": "2022-11-19T04:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is an article that explains how to set up the logging to BigQuery:\u00a0https://medium.com/google-cloud/dialogflow-cx-response-logging-e1b77d7a9fc6\n\nThere is no code, just create a simple agent in Dialogflow CX and set up at least one parameter with entity type\u00a0@sys.date-time, turn on logging, create BigQuery sink, then test the agent a few times and you should get the error notice that the sink has been disabled.\n\nThe financial services example agent would probably show the same bug if you triggered the \"Investigate charges\" intent"
			},
			{
				"Answer_created_time": "2022-11-28T13:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "With the information you provided from your project, I would highly recommend you to review this Troubleshooting routing and sinks documentation.\n\nAdditionally, you can actually create a PIT (Public Issue Tracker - Dialogflow CX), or please engage GCP Support if you're paying or if you're interested in starting to pay for a Support Package. Please be aware that from these 2 options, the second one is the fastest."
			}
		]
	},
	{
		"Question_title": "Google Translation issue",
		"Question_created_time": "2022-10-28T08:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translation-issue/td-p/483154/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 584,
		"Question_body": "Hello Team,I am trying to document translation using Google cloud translate V3.I found some issue in below-1. Text Overlapping from German to English2.Some text position was not correct3.table column name show in bottom of pages.4.Some pages were not being Translate. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-31T16:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you please share more information about your project and the process that you are following ?"
			},
			{
				"Answer_created_time": "2022-11-03T00:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nWe are using google translate cloud Service V3 with \"BatchDocumentService\".\n\nFirst Image is in German Language and Second Image is translate into english but you can see Text overlapping in below-"
			},
			{
				"Answer_created_time": "2022-11-08T00:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Team,\n\nAny update from your side ,it is really urgent."
			},
			{
				"Answer_created_time": "2022-11-15T22:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Team,\n\nAny update?"
			},
			{
				"Answer_created_time": "2022-11-23T12:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Following this process using projects.translateText Method on Cloud Translation API, the translation worked fine.\n\nIn order to better understand how we might assist, please provide more details about how you are utilizing the Cloud Translation API if this behavior seems unique to your use-case and implementation. You may, for instance, let us know what version of the Translation API is being used. Aside from that, consider the translation process you're using. Tell us how you are gathering the data; is it through a web user interface?"
			}
		]
	},
	{
		"Question_title": "What's the training corpus of models behind GCP Natural Language APIs?",
		"Question_created_time": "2022-11-17T14:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-s-the-training-corpus-of-models-behind-GCP-Natural-Language/td-p/490614/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 61,
		"Question_body": "Hi, where can I find some information about which datasets are used for training models that power the natural language APIs for sentiment analysis, entity extraction, etc.? Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-23T08:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Natural Language API is trained using different types of datasets.\n\nPublic datasets Examples: Five crowd-flower\u00a0sentiment benchmarks\nEAP customer datasets Examples: Feefo sentiment dataset\nAcademic datasets Examples: Stanford rotten tomatoes sentences, UCI Sentiment Labeled Sentences Data Set. See.\nGoogle datasets Examples: Shopping, Play."
			}
		]
	},
	{
		"Question_title": "A 10+2 Students thrust for Programming can really enable his abilities to capture the AI?",
		"Question_created_time": "2022-11-23T08:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/A-10-2-Students-thrust-for-Programming-can-really-enable-his/td-p/492261/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 32,
		"Question_body": "Students passed out from 10+2 are desperate enough to pursue AI through certifications, bachelor degree and in many other ways.... I am wondering is it really possible for them to capture AI and navigate in an easy manner these days. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-23T08:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Students passed out from 10+2 are desperate enough to pursue AI through certifications, bachelor degree and in many other ways....\n\n\u00a0\n\nI am wondering is it really possible for them to capture AI and navigate in an easy manner these days."
			}
		]
	},
	{
		"Question_title": "Vertex AI video action recognition - can it return action timeframes instead of a timestamps?",
		"Question_created_time": "2022-11-20T13:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-video-action-recognition-can-it-return-action/td-p/491200/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 187,
		"Question_body": "My problem is that my usecase requires the AI engine I use to provide predictions with the entire duration of the action. It seems to me that vertex AI picks a random frame in the span of the action and return it as the same start/end values. Here's an excerpt from an actual response Can I make it work the way I need it to? Maybe I'm annotating in a wrong manner?Here's a mockup of what I need. Notice how timeSegmentStart and timeSegmentEnd represent a duration now:",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-21T11:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nHaving the equal values for timeSegmentStart and timeSegmentEnd is an expected behavior as seen in this sample prediction response.\n\nWhat I could suggest is to create a feature request in GCPs\u00a0public issue tracker regarding your request.\u00a0Please keep in mind that when you create the feature request, it still needs to be analyzed and considered by the product team and a definite ETA is not guaranteed."
			},
			{
				"Answer_created_time": "2022-11-23T00:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@ricconoel\u00a0thanks for your reply. I agree that according to the sample this behavior is expected, however i wanted to know if it can be altered or configured. Looks weird to me that action recognition AI would stop at giving an arbitrary timestamp and not the whole duration of the action. Is there a way to find out if this feature is *already* implemented?"
			},
			{
				"Answer_created_time": "2022-11-23T07:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Unfortunately it cannot be altered or configured as of now. Hence my suggestion to create a feature request."
			}
		]
	},
	{
		"Question_title": "Google vertex AI support is terrible",
		"Question_created_time": "2022-11-22T20:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-vertex-AI-support-is-terrible/td-p/492001/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 78,
		"Question_body": "I usually don't post things like this but I have been trying to work with Google support for Vertex AI for a while. It has been a month on a P2 ticket and no help or support so far. I guess I don't pay the big bucks to get Google's attention. So just wanted to post a warning that is helpful hopefully. Use Vertex AI at your own risk. If something fails you are on your own.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-22T20:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I pay for support by the way. Not expecting free support."
			}
		]
	},
	{
		"Question_title": "How can I explicitly authenticate to the ai-platform using the java PredictionServiceClient",
		"Question_created_time": "2022-11-21T14:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-can-I-explicitly-authenticate-to-the-ai-platform-using-the/td-p/491537/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 222,
		"Question_body": "I have a model hosted on a Google Cloud endpoint and I would like to access it via the Java client.  I've created a service account and a key for that service account with the , when I run my client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, I am able to call the service.  When I try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.  The code is as follows`````` ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-22T09:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nUpon checking your code, FixedCredentialsProvider.create()\u00a0accepts\u00a0com.google.auth.Credentials\u00a0as a parameter. Can you try a Credentials object to\u00a0FixedCredentialsProvider.create()? See code below:\n\nGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(\"/Users/ME/Downloads/XYZ.json\")).createScoped(Lists.newArrayList(\"https://www.googleapis.com/auth/cloud-platform\"));\n\n\u00a0If code above did not work, can you provide the stack trace of the error? Also what roles did you assign on your service account?\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-22T09:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nUpon checking your code, FixedCredentialsProvider.create()\u00a0accepts\u00a0com.google.auth.Credentials\u00a0as a parameter. Can you try a Credentials object to\u00a0FixedCredentialsProvider.create()? See code below:\n\nGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(\"/Users/ME/Downloads/XYZ.json\")).createScoped(Lists.newArrayList(\"https://www.googleapis.com/auth/cloud-platform\"));\n\n\u00a0If code above did not work, can you provide the stack trace of the error? Also what roles did you assign on your service account?"
			},
			{
				"Answer_created_time": "2022-11-22T13:31:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "This worked. Thank you!"
			}
		]
	},
	{
		"Question_title": "How would you model a list of an unknown number of items in DialogFlow CX?",
		"Question_created_time": "2022-11-21T19:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-would-you-model-a-list-of-an-unknown-number-of-items-in/td-p/491605/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 51,
		"Question_body": "Hi,Taking from the example at Dialogflow CX: Build a retail virtual agent , if you were to build a shopping cart where users could add unlimited items to purchase. How would you model a solution for this?That is, instead of having:Can we have something equivalent to:How?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-22T10:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nYou might want to see this example where the user created a shopping cart based using Dialogflow CX."
			}
		]
	},
	{
		"Question_title": "Glossary not found.; Failed to initialize a glossary.",
		"Question_created_time": "2022-11-17T20:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Glossary-not-found-Failed-to-initialize-a-glossary/td-p/490677/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 107,
		"Question_body": "I have created glossary to translate text using Cloud Translation API. it shows me status as running-\n\"name\": \"projects/xxx/locations/us-central1/operations/xxx\",\n\"metadata\": {\n\"@type\": \"type.googleapis.com/google.cloud.translation.v3.CreateGlossaryMetadata\",\n\"name\": \"projects/xxx/locations/us-central1/glossaries/xxx\",\n\"state\": \"RUNNING\",\n\"submitTime\": \"2022-11-18T03:59:51.876209069Z\"\n}\n}but when I am trying to use this Glossary for translation api, it shows me error as-\n\"Glossary not found.; Failed to initialize a glossary\".\nEven when I tried listing my Glossary, it doesn't show.Not sure what is the issue. Console activity dashboard shows activity as created Glossary.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-22T09:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Make sure that all permissions are appropriately established.\n\n\u00a0\n\nIn order to make sure you see it, I would suggest listing your glossaries. If you don't, at least you are aware of the problem.\n\nFurthermore, I don't believe that this is a permissions issue. Explicit permission errors ought to be returned if there is a permission problem.\n\nI advise beginning with the create glossary sample and then attempting to access the same resource using the example code you're using in order to troubleshoot."
			}
		]
	},
	{
		"Question_title": "Google Cloud AutoML Vision annotation stopped working",
		"Question_created_time": "2022-11-18T13:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-AutoML-Vision-annotation-stopped-working/td-p/490920/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 95,
		"Question_body": "Has anyone encountered the issue where the AutoML Vision annotations for datasets stopped working. This includes not being able to change labels anymore, not being able delete created labels or not save create labels. The annotations were working as expected last week, but for some reason they stopped working this week.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-21T22:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you share the screenshot?"
			},
			{
				"Answer_created_time": "2022-11-22T07:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Here is a screenshot of one of the images in my dataset. I can add annotations of the default label type, but cannot change the type of label afterwards. As you can see the default is all cropland labels but since I cannot change the label type, all of them are stuck as \"cropland\"(even though there are 3 labels available for this dataset).\n\n\u00a0The labels behaves as if they are a regular text instead of a dropdown list.\n\nAlso the delete button for each of the labels do not work, as it appears but nothing happens when you click it. Moreover, the cancel button at the bottom of the screen is clickable but does not do anything either."
			},
			{
				"Answer_created_time": "2022-11-22T08:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for sharing\u00a0@akanel, since this seems to be an issue specific to your project, you may raise a 1:1 GCP support. This kind of support has access to your internal resources and may check your issue in a more comprehensive way."
			}
		]
	},
	{
		"Question_title": "Airflow Dag for Vertex AI",
		"Question_created_time": "2022-11-20T02:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Airflow-Dag-for-Vertex-AI/td-p/491123/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 116,
		"Question_body": "Please advice on how to create airflow dag for vertex ai.**********************************************************\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom google.cloud import aiplatform\nfrom airflow.operators import CreateDatasetOperatorYESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)default_dag_args = {\n\n'start_date': YESTERDAY,\n}with models.DAG(\n'composer_sample_simple_greeting',\nschedule_interval=datetime.timedelta(weeks=2),\ndefault_args=default_dag_args) as dag:\n\ndef create_entity_type_sample(\nproject: str,\nlocation: str,\nentity_type_id: str,\nvertexai: str,\nservice_account_id: str\ntask_id: str,\nproject_id: str,aiplatform.init(project=project, location=location)my_entity_type = aiplatform.EntityType.create(\nentity_type_id=entity_type_id, vertexai=vertexai\n)my_entity_type.wait()return my_entity_typecreate_image_dataset_job = CreateDatasetOperator(\ntask_id=\"image_dataset\",\ndataset=IMAGE_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_tabular_dataset_job = CreateDatasetOperator(\ntask_id=\"tabular_dataset\",\ndataset=TABULAR_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_text_dataset_job = CreateDatasetOperator(\ntask_id=\"text_dataset\",\ndataset=TEXT_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_video_dataset_job = CreateDatasetOperator(\ntask_id=\"video_dataset\",\ndataset=VIDEO_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_time_series_dataset_job = CreateDatasetOperator(\ntask_id=\"time_series_dataset\",\ndataset=TIME_SERIES_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)create_image_dataset_job >> create_tabular_dataset_job >> create_text_dataset_job >> create_video_dataset_job >> create_time_series_dataset_job\n************************************************************************************************************************",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-21T23:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can check in this link for available Google Cloud VertexAI Operators. There are available operators for creating Datasets (in which you already used in your code), training jobs, batch prediction jobs, endpoint service and more.\u00a0\n\nYou may also check this documentation on how to run an apache airflow DAG in Cloud Composer."
			}
		]
	},
	{
		"Question_title": "Vertex AI deploy custom model error - Model server terminated: model server container terminated:",
		"Question_created_time": "2022-11-18T05:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-deploy-custom-model-error-Model-server-terminated/td-p/490796/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 396,
		"Question_body": "Hi, I'm stuck at following error message when I try to deploy custom model to vertex-ai endpoint.Command:  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-21T21:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You may check here some things you can check further when deploying your model. If this does not work, it would be helpful to file a 1:1 support case since they can check your internal resources."
			}
		]
	},
	{
		"Question_title": "Authenticating to Vertex AI deployed endpoints",
		"Question_created_time": "2022-11-10T13:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Authenticating-to-Vertex-AI-deployed-endpoints/td-p/488229/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 194,
		"Question_body": "Hello, I am a new user of Vertex AI.  I have trained and deployed a tabular data categorization model to an Vertex AI hosted endpoint.  I have successfully called it from a program running on my laptop where the \"gcloud\" cli is installed.  If I want to run this not from my desktop but have it called from another service, how do I authenticate ?  I have created a service account but I am not sure 1) what roles would need to be attached to that account and 2) how I would provide the service account credentials given that I don't have much control over how the service that will call my model is started (i.e. I can't control its environment vars).  Any help would be appreciated! ",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, for starters, you may read the basic concepts of IAM and\u00a0service accounts\u00a0\n\nYou may check this\u00a0pre-defined roles\u00a0for Vertex AI that you can attach on your service account depending on the level of permission you want to give.\u00a0\n\nFor the second question, you need to be a Service Account Admin as per this official\u00a0GCP Documentation\u00a0for you to manage a service account."
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for the response.\u00a0 Now I know I need ServiceAccountAdmin.\u00a0 The thing I'm still not clear on is whether there is some way to provide the service account credentials without referencing a file on the file system.\u00a0 For example, if I'm accessing the service from a client to whose file system I have no access, what are my options ?"
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If this is a file from a service you have no control of, the permission should be given on the side of that service."
			},
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is custom code that is deployed to an app server like environment. I can deploy code and config to the app server but I don't control the environment variables that the app servers starts up with and I don't control the file system.\u00a0 I'd like to deploy code to the app server that accesses Google hosted model endpoints.\u00a0 Is there some way to do to authenticate to Google Cloud other than setting an environment variable that points to credentials on the file system ?"
			}
		]
	},
	{
		"Question_title": "Google Vertex AI Automl Model ID is invalid. It should start with 3 letters Error",
		"Question_created_time": "2022-11-20T02:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Vertex-AI-Automl-Model-ID-is-invalid-It-should-start-with/td-p/491126/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 147,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-21T10:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nYou get the error because you are using the old AutoML API to run predictions. See model_id created when the old AutoML API is used.\n\nSince you have trained your model using Vertex AutoML Classification (you got the 18 digit number), you should use aiplatform to run your predictions. See sample prediction code."
			}
		]
	},
	{
		"Question_title": "DocAI - Response in a single json file",
		"Question_created_time": "2022-11-17T23:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DocAI-Response-in-a-single-json-file/td-p/490702/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 282,
		"Question_body": "Hello Experts,\nI'm doing BatchProcessDocument. I have 18 pages of a PDF file and tried to process this using DocumentProcessorServiceClient API. After the process, Im getting response in json file. This is perfect.\nBut the json output file is created only for the 5 pages of the source PDF file. Each 5 pages of the content are converted into a separate json file.My question here is, is it possible to have a single output json file for a PDF source file? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-18T15:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nJust to confirm, when you said \"Each 5 pages of the content are converted into a separate json file.\" does it mean that 1 json per page? or 1 json per 5 pages? Also can you provide the code and sample file that you are using? Please make sure there are no PIIs (Personal Identifiable Information) in your file when providing it here."
			},
			{
				"Answer_created_time": "2022-11-20T21:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nThanks for you response. Actually I have two points.\n\nA pdf file should be processed and the response for this file in a single json file\nI have a file with a table of around 1000 rows. This table data can not be displayed in a single page. I just want a json object for this whole table. But currently the code is working for objects in a single page.\n\nBelow is my sample source code. I could not attach my sample file to this discussion.\n\n\u00a0\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileReader;\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\n\nimport com.google.api.gax.core.FixedCredentialsProvider;\n\n// [START documentai_batch_process_document]\n\nimport com.google.api.gax.longrunning.OperationFuture;\nimport com.google.api.gax.paging.Page;\nimport com.google.auth.oauth2.GoogleCredentials;\nimport com.google.cloud.documentai.v1.BatchDocumentsInputConfig;\nimport com.google.cloud.documentai.v1.BatchProcessMetadata;\nimport com.google.cloud.documentai.v1.BatchProcessRequest;\nimport com.google.cloud.documentai.v1.BatchProcessResponse;\nimport com.google.cloud.documentai.v1.Document;\nimport com.google.cloud.documentai.v1.DocumentOutputConfig;\nimport com.google.cloud.documentai.v1.DocumentOutputConfig.GcsOutputConfig;\nimport com.google.cloud.documentai.v1.DocumentProcessorServiceClient;\nimport com.google.cloud.documentai.v1.DocumentProcessorServiceSettings;\nimport com.google.cloud.documentai.v1.GcsDocument;\nimport com.google.cloud.documentai.v1.GcsDocuments;\nimport com.google.cloud.storage.Blob;\nimport com.google.cloud.storage.BlobId;\nimport com.google.cloud.storage.Bucket;\nimport com.google.cloud.storage.Storage;\nimport com.google.cloud.storage.StorageOptions;\nimport com.google.common.collect.Lists;\nimport com.google.protobuf.util.JsonFormat;\n\npublic class CustomProcessDocument {\n\t\n\tpublic static void main(String a[]) {\n\t\t\n\t\ttry {\n\t\t\tCustomProcess();\n\t\t} catch (IOException | InterruptedException | ExecutionException | TimeoutException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\t\n\tpublic static void CustomProcess() \n\t\t\tthrows IOException, InterruptedException, ExecutionException, TimeoutException {\n\t\t\n        String projectId = \"my-project-id\";\n        String location = \"us\"; // Format is \"us\" or \"eu\".\n        String processerId = \"my-processor-id\";\n        String outputGcsBucketName = \"my-storage-bucket-name\";\n        String outputGcsPrefix = \"my-output-path\";\n        String inputGcsUri = \"gs://my-storage-bucket-name/sample-pdf-file.pdf\";\n        String tokenPath = \"credentials-json-file-path\";\n        CustomProcessDoc(projectId, location, processerId, inputGcsUri, outputGcsBucketName, outputGcsPrefix, tokenPath);\n\t}\n\n    public static void CustomProcessDoc(String projectId, String location, String processorId, String gcsInputUri, \n    \t\tString gcsOutputBucketName, String gcsOutputUriPrefix, String tokenPath) {\n    \t\n        // Initialize client that will be used to send requests. This client only needs to be created\n        // once, and can be reused for multiple requests. After completing all of your requests, call\n        // the \"close\" method on the client to safely clean up any remaining background resources.\n        try {\n        \t\n        \tGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(tokenPath)).createScoped(Lists.newArrayList(\"https://www.googleapis.com/auth/cloud-platform\"));\n        \tDocumentProcessorServiceSettings setting = DocumentProcessorServiceSettings.newBuilder().setCredentialsProvider(FixedCredentialsProvider.create(credentials)).build();\n        \tDocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(setting);\n        \t\n            // The full resource name of the processor, e.g.:\n            // projects/project-id/locations/location/processor/processor-id\n            // You must create new processors in the Cloud Console first\n            String name = String.format(\"projects/%s/locations/%s/processors/%s\", projectId, location, processorId);\n\n            GcsDocument gcsDocument = GcsDocument.newBuilder().setGcsUri(gcsInputUri).setMimeType(\"application/pdf\").build();\n\n            GcsDocuments gcsDocuments = GcsDocuments.newBuilder().addDocuments(gcsDocument).build();\n\n            BatchDocumentsInputConfig inputConfig = BatchDocumentsInputConfig.newBuilder().setGcsDocuments(gcsDocuments).build();\n\n            String fullGcsPath = String.format(\"gs://%s/%s/\", gcsOutputBucketName, gcsOutputUriPrefix);\n            GcsOutputConfig gcsOutputConfig = GcsOutputConfig.newBuilder().setGcsUri(fullGcsPath).build();\n\n            DocumentOutputConfig documentOutputConfig = DocumentOutputConfig.newBuilder().setGcsOutputConfig(gcsOutputConfig).build();\n\n            // Configure the batch process request.\n            BatchProcessRequest request = BatchProcessRequest.newBuilder().setName(name).setInputDocuments(inputConfig).setDocumentOutputConfig(documentOutputConfig).build();\n\n            OperationFuture<BatchProcessResponse, BatchProcessMetadata> future = client.batchProcessDocumentsAsync(request);\n\n            // Batch process document using a long-running operation.\n            // You can wait for now, or get results later.\n            // Note: first request to the service takes longer than subsequent\n            // requests.\n            System.out.println(\"Waiting for operation to complete...\");\n            future.get(240, TimeUnit.SECONDS);\n\n            System.out.println(\"Document processing complete.\");\n\n//            Storage storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();\n            Storage storage = StorageOptions.newBuilder().setCredentials(credentials).setProjectId(projectId).build().getService();\n            Bucket bucket = storage.get(gcsOutputBucketName);\n\n            // List all of the files in the Storage bucket.\n            Page<Blob> blobs = bucket.list(Storage.BlobListOption.prefix(gcsOutputUriPrefix + \"/\"));\n            System.out.println(\"blobs : \"+blobs);\n            int idx = 0;\n            for (Blob blob : blobs.iterateAll()) {\n                if (!blob.isDirectory()) {\n                    System.out.printf(\"Fetched file #%d\\n\", ++idx);\n                    // Read the results\n\n                    // Download and store json data in a temp file.\n                    File tempFile = File.createTempFile(\"file\", \".json\");\n                    Blob fileInfo = storage.get(BlobId.of(gcsOutputBucketName, blob.getName()));\n                    fileInfo.downloadTo(tempFile.toPath());\n\n                    // Parse json file into Document.\n                    FileReader reader = new FileReader(tempFile);\n                    Document.Builder builder = Document.newBuilder();\n                    JsonFormat.parser().merge(reader, builder);\n\n                    Document document = builder.build();\n\n                    // Get all of the document text as one big string.\n                    String text = document.getText();\n\n                    // Read the text recognition output from the processor\n                    System.out.println(\"The document contains the following paragraphs:\");\n                    Document.Page page1 = document.getPages(0);\n                    List<Document.Page.Paragraph> paragraphList = page1.getParagraphsList();\n                    for (Document.Page.Paragraph paragraph : paragraphList) {\n                        String paragraphText = getText(paragraph.getLayout().getTextAnchor(), text);\n                        System.out.printf(\"Paragraph text:%s\\n\", paragraphText);\n                    }\n\n                    // Form parsing provides additional output about\n                    // form-formatted PDFs. You must create a form\n                    // processor in the Cloud Console to see full field details.\n                    System.out.println(\"The following form key/value pairs were detected:\");\n\n                    for (Document.Page.FormField field : page1.getFormFieldsList()) {\n                        String fieldName = getText(field.getFieldName().getTextAnchor(), text);\n                        String fieldValue = getText(field.getFieldValue().getTextAnchor(), text);\n\n                        System.out.println(\"Extracted form fields pair:\");\n                        System.out.printf(\"\\t(%s, %s))\", fieldName, fieldValue);\n                    }\n\n                    // Clean up temp file.\n                    tempFile.deleteOnExit();\n                }\n            }\n        } catch (IOException | InterruptedException | TimeoutException | ExecutionException e) {\n        \te.printStackTrace();\n        }\n    }\n\n    // Extract shards from the text field\n    private static String getText(Document.TextAnchor textAnchor, String text) {\n    \t\n        if (textAnchor.getTextSegmentsList().size() > 0) {\n        \t\n            int startIdx = (int) textAnchor.getTextSegments(0).getStartIndex();\n            int endIdx = (int) textAnchor.getTextSegments(0).getEndIndex();\n            return text.substring(startIdx, endIdx);\n        }\n        return \"[NO TEXT]\";\n    }\n}"
			}
		]
	},
	{
		"Question_title": "Vertex AI - Slow Batch Predictions",
		"Question_created_time": "2022-01-20T06:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Slow-Batch-Predictions/td-p/184803/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 871,
		"Question_body": "Hi, I've been running a Vertex AI Tabular batch prediction job for about 500k rows (50MB BQ table) for nearly 5 hours now, and I can't see any reference to how it's performing anywhere. Is there an estimate for how long this should take? Or where I should look for progress?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-21T14:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Liam,\u00a0\n\nYou might be able to check results in BigQuery or Cloud Storage [1] to estimate the progress on it. If this does not meet your demand, you might consider filing a feature request per instructions at [2].\u00a0\u00a0\n\n[1]\u00a0https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions#retrieve_batch_prediction_resu...\n[2]\u00a0https://cloud.google.com/support/docs/issue-trackers"
			},
			{
				"Answer_created_time": "2022-11-20T11:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\u00a0\n\nIt's not possible to check progress by checking the GCS output location. The output files only get uploaded after the job ends."
			}
		]
	},
	{
		"Question_title": "Importing to Vertex dataset does not import labels.",
		"Question_created_time": "2022-11-02T04:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Importing-to-Vertex-dataset-does-not-import-labels/td-p/484912/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 462,
		"Question_body": "In Vertex AI I am updating an image dataset, thus:the images are uploaded to the dataset but their labels are ignored and they are classed as Unlabeled. What am I doing wrong? TIA!\n\nPS they are in a csv, like:which worked fine for the dataset creation. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-03T17:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You could check this sample code to Import data for image classification single label:\n\n\u00a0\n\nfrom google.cloud import aiplatform\n\n\ndef import_data_image_classification_single_label_sample(\n    project: str,\n    dataset_id: str,\n    gcs_source_uri: str,\n    location: str = \"us-central1\",\n    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n    timeout: int = 1800,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {\"api_endpoint\": api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.DatasetServiceClient(client_options=client_options)\n    import_configs = [\n        {\n            \"gcs_source\": {\"uris\": [gcs_source_uri]},\n            \"import_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/ioformat/image_classification_single_label_io_format_1.0.0.yaml\",\n        }\n    ]\n    name = client.dataset_path(project=project, location=location, dataset=dataset_id)\n    response = client.import_data(name=name, import_configs=import_configs)\n    print(\"Long running operation:\", response.operation.name)\n    import_data_response = response.result(timeout=timeout)\n    print(\"import_data_response:\", import_data_response)"
			},
			{
				"Answer_created_time": "2022-11-04T07:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks, but exactly the same result."
			},
			{
				"Answer_created_time": "2022-11-14T13:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "From this Tensorflow blog post:\n\nIn addition to image files, we've provided a CSV file (all_data.csv) containing the image URIs and labels. We randomly split this data into two files, train_set.csv and eval_set.csv, with 90% data for training and 10% for eval, respectively.\n\ngs://cloud-ml-data/img/flower_photos/dandelion/17388674711_6dca8a2e8b_n.jpg,dandelion\ngs://cloud-ml-data/img/flower_photos/sunflowers/9555824387_32b151e9b0_m.jpg,sunflowers\ngs://cloud-ml-data/img/flower_photos/daisy/14523675369_97c31d0b5b.jpg,daisy\ngs://cloud-ml-data/img/flower_photos/roses/512578026_f6e6f2ad26.jpg,roses\ngs://cloud-ml-data/img/flower_photos/tulips/497305666_b5d4348826_n.jpg,tulips\n\n\nWe also need a text file containing all the labels (dict.txt), which is used to sequentially map labels to internally used IDs. In this case, daisy would become ID 0 and tulips would become 4. If the label isn't in the file, it will be ignored from preprocessing and training.\n\ndaisy \ndandelion \nroses \nsunflowers \ntulips \n\n\nTherefore, you need to create the dict.txt file which will have the all the labels used as shown above.\n\nSee also:\n\nHow to classify images with TensorFlow using Google Cloud Machine Learning and Cloud Dataflow"
			},
			{
				"Answer_created_time": "2022-11-15T03:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks but that is six years old and not a Vertex AI dataset."
			},
			{
				"Answer_created_time": "2022-11-18T10:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you please raise a private thread in the issue tracker (referencing this question, as stated in the template) with the project ID, job ID and a sample data of your input CSV file (Don't want the entire file or any PII)?"
			}
		]
	},
	{
		"Question_title": "Uploading images to Object detection with bounding boxes",
		"Question_created_time": "2022-11-16T21:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Uploading-images-to-Object-detection-with-bounding-boxes/td-p/490325/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 111,
		"Question_body": "I am trying to upload images with bounding boxes and want to know is there an online tool that can calculate bounding box co-ordinates from an image specifically for Automl?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-18T10:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can have a look into this two articles:\n\nComputer Vision: Object Detection and No-Code AI with AutoML\nVisual Inspection AI: a purpose-built solution for faster, more accurate quality control"
			}
		]
	},
	{
		"Question_title": "Dialogflox cx conflict: \"intents matching\" and \"parameters form\" at the same page.",
		"Question_created_time": "2022-11-15T05:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflox-cx-conflict-quot-intents-matching-quot-and-quot/td-p/489582/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 210,
		"Question_body": "Hello everyone,I found an unexpected behavior with the following page-level configuration.CONFLICT: After the question \"What do you want? \", if the user input is not clear there will not be any intent matching, but if the user include in the sentence the word \"Value1\" which is synonym of \"Entity1\", then the parameter \"intent_param\" (entity type \"@intent\") will be collected with value \"Entity1\". When this happens I was expecting \"sys.no-match-1\" to be activated, but this did not occurred and the page state status is \"PROCESSING_FORM\" (FormFilled: false).Does anyone knows why this happens and if is there a way to avoid this behavior? In this situation I would like to continue the workflow with the  parameter collected and no intents matched.Thank you,\nMiguel.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-15T15:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you provide an example of\u00a0Value1? It would also help if you provide the whole response for investigation purposes."
			},
			{
				"Answer_created_time": "2022-11-16T06:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello, this would be an example for the previous explanation:\n\nEntity type:\nEntity1: Tarjetas\nSynonyms: \"Tarjetas\", \"Cart\u00f3n\", \"Pl\u00e1stico\"\n\nPage (not start page):\n- Agent Fulfillment: \"What do you want?\"\n- Required/Not required Parameter: \"intent_param\" (I said required in the previous explanation, in both cases I can't achieve what I was expecting)\n- Some intent Routes\n- Event handler (\"sys.no-match-1\")\n\nUser input: \"Pl\u00e1stico\"\n\nThe behavior which I was expecting is that Agent tried to match any of the intents and:\n1. If match, activates the corresponding intent route\n2. If no-match, activates the\u00a0\"sys.no-match-1\" event handler\n\nAnd I also expected that simultaneously, if the input matches with the Entity/Synonym, the parameter was also collected and mapped with the corresponding entity.\n\nSummarizing, is it possible to collect a parameter (map entity type) and try to match intents, with one unique user input at the same time?\n\nResponse for Required parameter (Parameter is collected but event handler is not activated):\n\n{\n  \"advancedSettings\": {\n    \"audioExportGcsDestination\": {},\n    \"loggingSettings\": {\n      \"enableInteractionLogging\": true\n    },\n    \"speechSettings\": {\n      \"endpointerSensitivity\": 90,\n      \"noSpeechTimeout\": \"5s\"\n    }\n  },\n  \"currentPage\": {\n    \"displayName\": \"Prueba params\",\n    \"name\": \"projects/dev-bbva-dialogflow-poc/locations/us-central1/agents/57dc60ad-5e59-4913-9c5a-dcef53842159/flows/9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8/pages/5409aed3-7c85-47a8-aa87-2a84947d8b5d\"\n  },\n  \"diagnosticInfo\": {\n    \"Alternative Matched Intents\": [\n      {\n        \"Score\": 1,\n        \"Active\": true,\n        \"Parameters\": {\n          \"intent_param\": {\n            \"resolved\": \"REDACTED\",\n            \"original\": \"REDACTED\",\n            \"type\": \"@resolve_intent\"\n          }\n        },\n        \"Type\": \"NLU_SLOT\"\n      }\n    ],\n    \"Transition Targets Chain\": [],\n    \"Session Id\": \"19db1e-467-50c-8d4-69fdb3e1a\",\n    \"Triggered Transition Names\": [],\n    \"Execution Sequence\": [\n      {\n        \"Step 1\": {\n          \"InitialState\": {\n            \"SessionParameters\": {\n              \"intent_param\": \"REDACTED\"\n            },\n            \"MatchedIntent\": {\n              \"Active\": true,\n              \"Parameters\": {\n                \"intent_param\": {\n                  \"resolved\": \"REDACTED\",\n                  \"type\": \"@resolve_intent\",\n                  \"original\": \"REDACTED\"\n                }\n              },\n              \"Type\": \"NLU_SLOT\",\n              \"Score\": 1\n            },\n            \"FlowState\": {\n              \"Name\": \"Prueba params\",\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\",\n              \"Version\": 0,\n              \"PageState\": {\n                \"Name\": \"Prueba params\",\n                \"ActiveParameter\": \"intent_param\",\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"FormFilled\": false,\n                \"Status\": \"PROCESSING_FORM\"\n              }\n            }\n          },\n          \"Type\": \"INITIAL_STATE\"\n        }\n      },\n      {\n        \"Step 2\": {\n          \"StateMachine\": {\n            \"FlowState\": {\n              \"Name\": \"Prueba params\",\n              \"Version\": 0,\n              \"PageState\": {\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"FormFilled\": true,\n                \"Name\": \"Prueba params\",\n                \"Status\": \"TRANSITION_ROUTING\"\n              },\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\"\n            }\n          },\n          \"Type\": \"STATE_MACHINE\"\n        }\n      }\n    ]\n  },\n  \"intentDetectionConfidence\": 1,\n  \"languageCode\": \"es\",\n  \"match\": {\n    \"confidence\": 1,\n    \"matchType\": \"PARAMETER_FILLING\",\n    \"parameters\": {\n      \"intent_param\": \"Tarjetas\"\n    },\n    \"parametersOriginalValues\": {\n      \"intent_param\": \"Pl\u00e1stico\"\n    },\n    \"resolvedInput\": \"Pl\u00e1stico\"\n  },\n  \"parameters\": {\n    \"intent_param\": \"Tarjetas\"\n  },\n  \"redactedParameters\": [\n    \"intent_param\"\n  ],\n  \"responseMessages\": [\n    {\n      \"interactiveVoiceResponseSettings\": {\n        \"audioExportGcsDestination\": {},\n        \"speechSettings\": {\n          \"endpointerSensitivity\": 90,\n          \"noSpeechTimeout\": \"5s\"\n        }\n      }\n    }\n  ],\n  \"text\": \"Pl\u00e1stico\"\n}\n\n\u00a0Response for no roquired parameter (Event handler is activated, but no parameter is collected):\n\n{\n  \"advancedSettings\": {\n    \"audioExportGcsDestination\": {},\n    \"loggingSettings\": {\n      \"enableInteractionLogging\": true\n    },\n    \"speechSettings\": {\n      \"endpointerSensitivity\": 90,\n      \"noSpeechTimeout\": \"5s\"\n    }\n  },\n  \"currentPage\": {\n    \"displayName\": \"End Session\",\n    \"name\": \"projects/dev-bbva-dialogflow-poc/locations/us-central1/agents/57dc60ad-5e59-4913-9c5a-dcef53842159/flows/9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8/pages/END_SESSION\"\n  },\n  \"diagnosticInfo\": {\n    \"Execution Sequence\": [\n      {\n        \"Step 1\": {\n          \"InitialState\": {\n            \"Event\": \"sys.no-match-1\",\n            \"FlowState\": {\n              \"PageState\": {\n                \"Name\": \"Prueba params\",\n                \"Status\": \"TRANSITION_ROUTING\",\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"FormFilled\": true\n              },\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\",\n              \"Name\": \"Prueba params\",\n              \"Version\": 0\n            }\n          },\n          \"Type\": \"INITIAL_STATE\"\n        }\n      },\n      {\n        \"Step 2\": {\n          \"Type\": \"STATE_MACHINE\",\n          \"FunctionExecution\": {\n            \"Responses\": [\n              {\n                \"responseType\": \"HANDLER_PROMPT\",\n                \"text\": {\n                  \"redactedText\": [\n                    \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n                  ],\n                  \"text\": [\n                    \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n                  ]\n                },\n                \"source\": \"VIRTUAL_AGENT\"\n              }\n            ]\n          },\n          \"StateMachine\": {\n            \"FlowState\": {\n              \"Name\": \"Prueba params\",\n              \"PageState\": {\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"Name\": \"Prueba params\",\n                \"FormFilled\": true,\n                \"Status\": \"TRANSITION_ROUTING\"\n              },\n              \"Version\": 0,\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\"\n            },\n            \"TriggeredEvent\": \"sys.no-match-1\",\n            \"TriggeredEventHandlerId\": \"f6760455-fb9b-49fb-acae-69142ec33188\"\n          }\n        }\n      },\n      {\n        \"Step 3\": {\n          \"StateMachine\": {\n            \"FlowState\": {\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\",\n              \"PageState\": {\n                \"Name\": \"End Session\",\n                \"PageId\": \"END_SESSION\",\n                \"Status\": \"ENTERING_PAGE\"\n              },\n              \"Name\": \"Prueba params\",\n              \"Version\": 0\n            }\n          },\n          \"Type\": \"STATE_MACHINE\"\n        }\n      }\n    ],\n    \"Alternative Matched Intents\": [],\n    \"Session Id\": \"946ef0-346-0ee-be7-ec290fb95\",\n    \"Unfulfilled Parameters\": [\n      \"$session.params.intent_param\"\n    ],\n    \"Transition Targets Chain\": [\n      {\n        \"TargetPage\": \"END_SESSION\"\n      }\n    ],\n    \"Triggered Transition Names\": [\n      \"f6760455-fb9b-49fb-acae-69142ec33188\"\n    ]\n  },\n  \"intentDetectionConfidence\": 0.3,\n  \"languageCode\": \"es\",\n  \"match\": {\n    \"confidence\": 0.3,\n    \"event\": \"sys.no-match-1\",\n    \"matchType\": \"NO_MATCH\"\n  },\n  \"responseMessages\": [\n    {\n      \"responseType\": \"HANDLER_PROMPT\",\n      \"source\": \"VIRTUAL_AGENT\",\n      \"text\": {\n        \"redactedText\": [\n          \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n        ],\n        \"text\": [\n          \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n        ]\n      }\n    },\n    {\n      \"interactiveVoiceResponseSettings\": {\n        \"audioExportGcsDestination\": {},\n        \"speechSettings\": {\n          \"endpointerSensitivity\": 90,\n          \"noSpeechTimeout\": \"5s\"\n        }\n      }\n    },\n    {\n      \"endInteraction\": {}\n    }\n  ],\n  \"text\": \"Pl\u00e1stico\"\n}"
			},
			{
				"Answer_created_time": "2022-11-18T08:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This seems to be not reproducible on my end.\u00a0If you have premium support, you can check with\u00a0GCP Support\u00a0to further check your issue since this is specific to your project."
			}
		]
	},
	{
		"Question_title": "timeSegments vs timeSegmentAnnotations",
		"Question_created_time": "2022-11-16T08:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/timeSegments-vs-timeSegmentAnnotations/td-p/490092/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 59,
		"Question_body": "timeSegments vs timeSegmentAnnotationsCan anyone explain what's the difference between these 2 fields described here? https://storage.cloud.google.com/google-cloud-aiplatform/schema/dataset/ioformat/video_action_recogn...why would I want to tag timeSegments? what's the objective of this? associate a label to a time segment?  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-17T15:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "timeSegments is a parameter that should be included to your schema's YAML and JSONL files so that it can be used to determine the timestamps of the operations, not something that should be made from the terminal. I.e., a section of the film that contains a number of actions or annotations. And when you import data into your dataset later, you add those files.\n\nHowever, I checked timeSegmentAnnotations and found that although I set the endTime to be different from startTime, the action is only labeled on the startTime, so it doesn't seem to be that useful (hence, one frame)."
			}
		]
	},
	{
		"Question_title": "Dialogflow CX Intergration with Messenger from Facebook",
		"Question_created_time": "2022-11-15T20:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-CX-Intergration-with-Messenger-from-Facebook/td-p/489873/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 106,
		"Question_body": "Hi, I have some issue when I integrate dialogflow cx with messenger. \nThe issue is I didn't get response from my bot, I have a suspect that dialogflow failed to send message to user on messenger.\nSo, my problem now is how to check sending message process in dialogflow to get the detail of error?Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-17T09:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "To see logs for DialogFlow, you would need to enable Cloud Logging\u00a0in your agent\u2019s general settings. After enabling logging, you\u2019d be able to see DialogFlow logs for requests and responses (including for webhooks). Let me know if you have additional questions."
			}
		]
	},
	{
		"Question_title": "Do Training Jobs Run in Parallel? (VERTEX AI)",
		"Question_created_time": "2022-11-15T07:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI/td-p/489639/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 144,
		"Question_body": "I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000. Thanks! ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-15T11:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-15T11:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document."
			},
			{
				"Answer_created_time": "2022-11-16T06:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Cool thanks,\n\nAre those quotas liftable or are they the hard cap.\n\n\u00a0\n\nThanks!"
			},
			{
				"Answer_created_time": "2022-11-16T09:06:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yes it is possible but is subject to approval. Kindly see this document on how to request a quota increase."
			}
		]
	},
	{
		"Question_title": "Matching Engine: Queries with filtering do not work as expected",
		"Question_created_time": "2022-11-10T07:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Matching-Engine-Queries-with-filtering-do-not-work-as-expected/td-p/488086/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 199,
		"Question_body": "I tried to run this notebook: https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engi...\nI got an issue with the filtering step:\nLet's say glove100.json is:If I try to filter in this way:I do not get any response (empty result). If from the above code I remove the \"restrict block\", it works (of course without filtering).response:But, if I add a new vector by Vertex SDK for Python (link ), in this way:and then I try to filter in the same way above with filter class == \"3\", I get the right response.It seems like the allow_tokens are \"seen\" by vertex only when I insert a new vector by Vertex SDK and not when I specify them in the initial glove100.json.Moreover, if I update a datapoint where the filter did not work, for example id=1:The filter for class=1 starts to work.Is there a way to know what is actually stored in the index? I mean a kind of \"SELECT * FROM myindex\" in order to check embeddings and tokens stored.Any ideas on how to solve this issue?Thanks in advanceSpecificationsI tried from local and from workbench, the result is the same.- Version: Python 3.7.9\n- Platform: Matching Engine. zone: europe-west1",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-11T14:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThank you for reaching out, you can create a public issue tracker for this question since it might be an unexpected behavior and you have a workaround to show for."
			},
			{
				"Answer_created_time": "2022-11-15T02:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@ricconoel\u00a0,\nThank you for replying!\u00a0\n\nSince I do not see a \"Matching Engine\" section,\u00a0 in which one do you suggest to open an issue?"
			}
		]
	},
	{
		"Question_title": "Google Cloud Vision broken for English?",
		"Question_created_time": "2022-11-12T13:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Vision-broken-for-English/td-p/488836/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 62,
		"Question_body": "Has anyone else noticed that the Google Cloud Vision OCR that processes the text in images operates starting top to bottom, then left to right for English?  And that it didn't used to?\n\nThe problem with this is generally:\n\nWe write in English like this.\nSo we want to read the lines from left to right, top to bottom.We          write\ndo           english\nnot         like\nreally      thisTop to bottom, left to right.  Which is how you're reading it.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-14T15:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "To report an issue, it is better to do it at Issue Tracker, rather than write a post at this forum.\n\nYou can submit your report here:\n\nCreate new Cloud Vision issue\nIssue reports\n\nGoogle reviews every new issue report submitted by users. Sometimes one of our staff will ask for clarification or followup. After we're able to replicate the issue, we'll tell you that it's been forwarded to the appropriate team.\n\nDepending on the circumstances, we may be able to provide periodic updates while an issue is being looked at, but usually we cannot provide too many specifics about the exact cause of an issue, or when it will be fixed.\n\nWhen we've fixed an issue in production, we'll indicate this and then we'll close the issue.\n\nSee also:\n\nWhat to expect after you've opened an issue."
			}
		]
	},
	{
		"Question_title": "Eliminacion de fondos personalizados",
		"Question_created_time": "2022-11-12T05:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Eliminacion-de-fondos-personalizados/td-p/488739/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 39,
		"Question_body": "Hay documentos que tienen un fondo personalizado ya sea con logos o texto referencial a la empresa o al proceso que se lleva a cabo, es o ser\u00e1 posible eliminar estas caracter\u00edsticas y as\u00ed poder hacer m\u00e1s eficiente el nivel de eficiencia del OCR",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-12T05:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hay documentos que tienen un fondo personalizado ya sea con logos o texto referencial a la empresa o al proceso que se lleva a cabo, es o ser\u00e1 posible eliminar estas caracter\u00edsticas y as\u00ed poder hacer m\u00e1s eficiente el nivel de eficiencia del OCR"
			}
		]
	},
	{
		"Question_title": "Month-to-date total cost (VertexA\u0131) keeps increasing even though I delete all my projects and billin",
		"Question_created_time": "2022-11-11T01:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Month-to-date-total-cost-VertexA%C4%B1-keeps-increasing-even-though-I/td-p/488385/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 92,
		"Question_body": " ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-11T01:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello. I am a student and only use GCP for learning. But even though there is no project in my profile and\nI have closed my billing account, the Month-to-date total cost (VertexA\u0131) is constantly increasing. \nI tried everything but couldn't find a solution. I would be very happy if you could help me what to do."
			}
		]
	},
	{
		"Question_title": "Python API to view/list Vertex AI Feature Store ingestion jobs",
		"Question_created_time": "2022-10-13T11:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Python-API-to-view-list-Vertex-AI-Feature-Store-ingestion-jobs/td-p/477834/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 287,
		"Question_body": "It's possible to view currently running feature ingestion jobs in the console (https://console.cloud.google.com/vertex-ai/ingestion-jobs). How can I list currently running ingestion jobs using a Python API? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-17T14:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As shown in the documentation,\n\nuse the Google Cloud console to view batch ingestion jobs.\n\nBy now, this is the way to view ingestion jobs."
			},
			{
				"Answer_created_time": "2022-10-18T08:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "OK, weird. How can I make sure that there isn't an ingestion job running on an entity_type_id before I starting a new ingestion job on the same entity_type_id?"
			},
			{
				"Answer_created_time": "2022-11-10T14:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You could request a code sample by creating a new issue at the googleapis/python-aiplatform GitHub repository.\n\nFor questions about programming languages, you could ask in Stack Overflow."
			}
		]
	},
	{
		"Question_title": "Error in GCP Doc AI project",
		"Question_created_time": "2022-11-09T03:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-in-GCP-Doc-AI-project/td-p/487561/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 58,
		"Question_body": "Good evening . My peer while try to access Document AI page is getting the below error . Facing this issue from 2 PM yesterday. We are working for a POC project from LTI organization. Basically, it should show some processors or specialized processors. Please can you guide us.Regards,Vamsi ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-10T13:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For this kind of issue, please contact Google Cloud support.\n\nFile a support case"
			}
		]
	},
	{
		"Question_title": "Vertex AI Batch Predictions: Bigquery format must be used as input and output simultaneously",
		"Question_created_time": "2022-11-01T15:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Batch-Predictions-Bigquery-format-must-be-used-as/td-p/484734/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 211,
		"Question_body": "I'm encountering an error when I try to create a batch prediction job with a bigquery table as my input, and a JSONL output in a GCS bucket. The documentation for batch predictions seems to indicate that I can do so, but I still see an error.I'm trying to create a batch prediction job on the Vertex AI console, and I see this error. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-02T16:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The documentation does seem unclear about this error, do you see the same issue if you test selecting a different output format, for instance CSV? This so I can investigate further and contact the appropriate teams with this inquiry."
			},
			{
				"Answer_created_time": "2022-11-10T08:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you use BigQuery as your batch prediction input you only can store the batch prediction results in BigQuery (input and output has to be the same)"
			}
		]
	},
	{
		"Question_title": "Vertex Matching Engine deny list tokens",
		"Question_created_time": "2022-08-24T12:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-Matching-Engine-deny-list-tokens/td-p/459559/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 1,
		"Question_view_count": 239,
		"Question_body": "How does the Vertex matching engine deny list work?\n\nLet's say I have a class fruit which will ONLY have deny list tokens (no allow) such as \"apple\", \"mango\", etc. How do I filter out \"mango\" in the query (search all fruits except mango)? I have tried the following method but it does not work as expected:\n\njson\n{\"id\": \"1\", \"embedding\":[0.002792,0.000492], \"restricts\": [{\"namespace\": \"fruit\", \"deny\": [\"mango\"]}]}\n\nquery\ndeny_namespace = match_service_pb2.Namespace()\ndeny_namespace.name = \"fruit\"\ndeny_namespace.deny_tokens.append(\"mango\")\nrequest.restricts.append(deny_namespace)\n\nI have coded this similar to allow list which has worked for me but with deny tokens it does not seem to skip deny tokens even after completely overwriting the index. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-30T08:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Based on the documentation that you shared about the filters and the code you are sharing, it looks like there\u2019s a formatting error on your code, based on the doc the JSON should look like the example I\u2019m providing you below.\n\n{\"id\": \"42\", \"embedding\": [0.5, 1.0], \"restricts\": [{\"namespace\": \"class\",\u00a0\"allow\": [\"cat\", \"pet\"]},{\"namespace\": \"category\", \"allow\": [\"feline\"]}]}"
			},
			{
				"Answer_created_time": "2022-08-31T06:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Eduardo_Ortiz\u00a0Thanks for looking into this!\nYes this is for allow list, but I'm specifically asking for deny list. When I changed the \"deny\" to \"allow\" it seems to work but it fails for \"deny\". Can you or someone from the team please help me with this?"
			},
			{
				"Answer_created_time": "2022-10-03T09:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Did you try to add deny tokens onto your case?"
			},
			{
				"Answer_created_time": "2022-11-10T07:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello @Eduardo_Ortiz\u00a0,\n\nI tried what you said but it does not work anyway. For a more details, I opened an issue on github:\u00a0 https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/1230"
			}
		]
	},
	{
		"Question_title": "Auto ML Edge failing if highest accuracy option is selected",
		"Question_created_time": "2022-10-31T17:08:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Auto-ML-Edge-failing-if-highest-accuracy-option-is-selected/td-p/484251/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "Hello all,I am doing an automl training for edge ad if I pick the higher accuracy option training fails after about 3 hours with the following error. Training completes with no issues if I pick the best trade-off option. I have opened a ticket but has received zero support from Google so posting here. Has anyone seen this issue and know how to fix it?\u2003I have tried multiple times with the same error. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-08T15:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "To report an issue, it is better to do it at Issue Tracker, rather than write a post at this forum.\n\nYou can submit your report here:\n\nCreate new Vertex AI AutoML Vision issue\nIssue reports\n\nGoogle reviews every new issue report submitted by users. Sometimes one of our staff will ask for clarification or followup. After we're able to replicate the issue, we'll tell you that it's been forwarded to the appropriate team.\n\nDepending on the circumstances, we may be able to provide periodic updates while an issue is being looked at, but usually we cannot provide too many specifics about the exact cause of an issue, or when it will be fixed.\n\nWhen we've fixed an issue in production, we'll indicate this and then we'll close the issue.\n\nSee also:\n\nWhat to expect after you've opened an issue."
			}
		]
	},
	{
		"Question_title": "GCP Idle Model Charging",
		"Question_created_time": "2022-11-07T04:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/GCP-Idle-Model-Charging/td-p/486604/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 136,
		"Question_body": "Hello!\nI would like to deploy the ML Model into GCP.Most of the time the model will be sleeping. Sometimes I should use it through Endpoint for some seconds.\nI don't want to pay for full-time GPU instance and I need fast responses at the same time, without deployment from scratch everytime I need it.Is this possible in GCP ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-08T15:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "According to the Pricing for AutoML models documentation:\n\nPricing for AutoML models\n\nFor Vertex AI AutoML models, you pay for three main activities:\n\nTraining the model\nDeploying the model to an endpoint\nUsing the model to make predictions\n\nVertex AI uses predefined machine configurations for Vertex AutoML models, and the hourly rate for these activities reflects the resource usage. ... You pay for each model deployed to an endpoint, even if no prediction is made. You must undeploy your model to stop incurring further charges. Models that are not deployed or have failed to deploy are not charged.\n\nYou pay only for compute hours used; if training fails for any reason other than a user-initiated cancellation, you are not billed for the time. You are charged for training time if you cancel the operation.\n\nCustom-trained models\nTraining\n\nThe tables Machine types and Accelerators provide the approximate price per hour of various training configurations. You can choose a custom configuration of selected machine types. To calculate pricing, sum the costs of the virtual machines you use.\n\nIf you use Compute Engine machine types and attach accelerators, the cost of the accelerators is separate. To calculate this cost, multiply the prices in the table of accelerators below by how many machine hours of each type of accelerator you use.\n\nFor further information about pricing, please refer to the Vertex AI pricing documentation, or you can connect with our sales team to get a custom quote."
			}
		]
	},
	{
		"Question_title": "Vertex AI endpoint deployment",
		"Question_created_time": "2022-11-04T04:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-endpoint-deployment/td-p/485783/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 121,
		"Question_body": "How can I utilize the mega GPU during endpoint deployment  for vertex ai work? Are there any model for examples or other resources that I can use to better grasp this?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-07T11:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What kind of model are you deploying on the endpoint? Can you clarify on what you mean in this statement \"utilizing the GPU during endpoint deployment\"?"
			},
			{
				"Answer_created_time": "2022-11-08T00:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for responding. It is a simple tensorflow tabular classification model, and deployment takes about 18-20 minutes after model registration. I decided to use a mega GPU since I want to shorten this time as much as I can; perhaps this decision will shorten the time for deployment."
			},
			{
				"Answer_created_time": "2022-11-08T09:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for clarifying. As far as I know the deployment of an endpoint is handled at the backend of GCP so it is not possible to use a GPU in order to shorten the deployment time."
			},
			{
				"Answer_created_time": "2022-11-08T11:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "okay thanks , but when I use another account for the same , it's done within 2-3 min only !!!!"
			}
		]
	},
	{
		"Question_title": "Authentication errors running vaictl in container",
		"Question_created_time": "2022-11-07T15:09:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Authentication-errors-running-vaictl-in-container/td-p/486888/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 82,
		"Question_body": "I'm trying to run vaictl on OSX inside a docker container based on these Vertex AI Vision instructions, but hitting the following auth error:  I've run gcloud auth login in the container and saved the authorization code.Are there any extra steps needed to make this work?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-08T10:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "How did you reproduce the error? Can you provide the reproduction steps? Just so I can reproduce it on my end."
			}
		]
	},
	{
		"Question_title": "Error 524 on jupyterlab",
		"Question_created_time": "2022-11-05T02:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-524-on-jupyterlab/td-p/486115/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 389,
		"Question_body": "Hi,I cannot access Jupyterlab by web interface. It still works by ssh. I've followed the support documentations, but nothing works.My best guess is that the main issue is with the opened ports of docker.       I have the full result of the following command available, but i'm not sure if it's good idea to post it in public.I think the main error is from:      Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-07T11:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "One possible solution is to perform disk check and repair using this command:\u00a0sudo fsck /dev/sdb \u00a0then reboot your machine.\n\nYou can also migrate data of\u00a0/home/jupyter/ folder and upload it to a new instance as a workaround."
			},
			{
				"Answer_created_time": "2022-11-07T21:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThis solution doesn't work\n\nPlease have a look at that thread:\u00a0https://stackoverflow.com/questions/74326566/jupyter-internal-api-is-not-active-vertex-ai-jupyterlab...\n\n\n\nthanks"
			},
			{
				"Answer_created_time": "2022-11-08T09:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Since documentation and the workaround does not help, you may file a 1:1 Cloud Support here:\u00a0https://cloud.google.com/support\u00a0\n\nA support resource that has access to your project will be able to check more thoroughly some components around your Jupyter notebook and can give you more comprehensive answers and suggestions."
			}
		]
	},
	{
		"Question_title": "Vision API Product search",
		"Question_created_time": "2022-11-02T10:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vision-API-Product-search/td-p/485081/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 1,
		"Question_view_count": 269,
		"Question_body": "Following is vision API product search request jsonFor ImageContext, ProductCategories(apparel in this request) is mandatory in API. My concern is if I want product search from all the available ProductCategories, do I need to set multiple requests?For example, addProductCategories(\"apparel\") can have only one productcategory at a time. But I want product search from all the category something like addProductCategories(\"apparel-v2\").addProductCategories(\"toys-v2\").addProductCategories(\"general-v1\") etc.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-03T17:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As shown in the documentation, it will \u201conly consider the first category\u201d:\n\nThe list of product categories to search in. Currently, we only consider the first category, and either \"homegoods-v2\", \"apparel-v2\", \"toys-v2\", \"packagedgoods-v1\", or \"general-v1\" should be specified. The legacy categories \"homegoods\", \"apparel\", and \"toys\" are still supported but will be deprecated. For new products, please use \"homegoods-v2\", \"apparel-v2\", or \"toys-v2\" for better product search accuracy. It is recommended to migrate existing products to these categories as well.\n\nSee also:\n\njava-vision/ProductSearchParams.java"
			},
			{
				"Answer_created_time": "2022-11-03T22:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes we have migrated product to\u00a0\"homegoods-v2\", \"apparel-v2\", \"toys-v2\",\u00a0\"packagedgoods-v1\",\u00a0\"general-v1\" etc. but basic issue is, if we have data-set of multiple category Vision-API don't have provision of product search. So it is dependent on product category whereas filter is optional. It is a blocker since we have to request vision API multiple times based on category and all the responses need to be combined and reprocess further for actual response. So please provide any alternates Google Team already working on this scenario."
			},
			{
				"Answer_created_time": "2022-11-06T07:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Also considered another scenario where we want safe search over this data-set, again we need to collect all the previous response, reprocess the previous response by adding\n\nFeature.Type.SAFE_SEARCH_DETECTION\n\n\u00a0Only after that we will get any results. Looks like currently vision API is limited to work on these scenario. Individual has to manage it at application level rather than API itself support it."
			},
			{
				"Answer_created_time": "2022-11-07T23:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "exactly, it make application slow by requesting multiple times and reprocess the request to display end results."
			}
		]
	},
	{
		"Question_title": "Endpoint in GCP",
		"Question_created_time": "2022-10-14T12:37:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Endpoint-in-GCP/td-p/478360/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 105,
		"Question_body": "In GCP I was deploying a model which obtain from training a dataset and after success full Vertex AI Model Registry. It takes too much time around 10 min to create endpoint for model. How can I reduce creation time when creating endpoints on GCP? What factors affected endpoint creation ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-20T16:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This video would be a good starting point:\n\nDeploying quick, cost-effective ML models with Vertex AI\n\nManaged notebook environments make it easier, faster, and more cost-effective to get high-quality models into production without having to set up infrastructure or install libraries. In this session, we\u2019ll demo how to use Vertex AI to get batch and online predictions and use the Vertex AI Python SDK to upload models to Vertex AI Model Registry and deploy to an endpoint with little code."
			},
			{
				"Answer_created_time": "2022-11-04T04:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "thanks for\u00a0 reply but given resource does not work for me !!!"
			}
		]
	},
	{
		"Question_title": "Using other API to translate PDF documents",
		"Question_created_time": "2022-11-01T23:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Using-other-API-to-translate-PDF-documents/td-p/484845/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 159,
		"Question_body": "Hello all, \n\nI am trying to find a way to translate English PDF documents to a target language(Korean) without messing up the original PDF page format(pictures, headers, tables, etc.)\n\nThe only problem with the default google translation is that many of the words that appear in the document are very industry-specific and need to be translated accordingly through AutoML translation. \n\nHowever, we'd like to use our own language model (i.e. fine-tuned GPT3) to translate just the text and feed the translated text to the output stream to get the final pdf output.\n\nI'm yet to see any other company that maintains PDF formatting as well as Google while translating, so I'd really like to use Cloud Translation API with our own translation module for optimal accuracy.\n\nIs there a way to do this? I've tried reaching out to the local Google branch to no avail. Please help!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-03T15:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It is possible to use a glossary\u00a0in Cloud Translation to provide the API with custom translations for terms that appear in texts. This would help when industry-specific terminology needs to be translated in a specific way.\n\nAs for using a custom language recognition model, you would be able to create a Feature Request for Cloud Translate API\u00a0in Google\u2019s public issue tracker."
			}
		]
	},
	{
		"Question_title": "Vocal emojis in Speech-to-Text",
		"Question_created_time": "2022-11-03T07:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vocal-emojis-in-Speech-to-Text/td-p/485418/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 45,
		"Question_body": "Hello! I am majoring in Theoretical Linguistics this year and I would like to write my dissertation on Google Cloud API and the vocal emojis supported, delving into the neural network to find out how they are translated. I have seen that my native language is missing and could build a dataset of spoken forms. Following the tutorial for using the Speech-to-Text API with Phyton I found out that very little information on this project are public.Should I contact some specific person/service via my institutional account to receive material for a study case?Thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-03T07:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello! I am majoring in Theoretical Linguistics this year and I would like to write my dissertation on Google Cloud API and the vocal emojis supported, delving into the neural network to find out how they are translated. I have seen that my native language is missing and could build a dataset of spoken forms. Following the tutorial for using the Speech-to-Text API with Phyton I found out that very little information on this project are public.\n\nShould I contact some specific person/service via my institutional account to receive material for a study case?\n\nThank you!"
			}
		]
	},
	{
		"Question_title": "Version 2 model in natural language API",
		"Question_created_time": "2022-11-01T11:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Version-2-model-in-natural-language-API/td-p/484641/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 172,
		"Question_body": "Hi, could anyone share the python code on how to get  natural language API to use version 2 classify text  categories?I can get it working well with the default (version 1) categories but can't figure out where to adapt the standard code (as here: https://cloud.google.com/natural-language/docs/samples/language-classify-text-tutorial-classify?hl=e...)  to  use model version 2.Many thanks ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-02T16:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "From the Classifying Content guide, you can include classification_model_options\u00a0within the request\u00a0dictionary argument to the classify_text()\u00a0function. In these options, you can define the model and version to use for content categories.\n\n// ...\ncontent_categories_version = (\n        language_v1.ClassificationModelOptions.V2Model.ContentCategoriesVersion.V2) // Assigning the v2 model type\n    response = client.classify_text(request = {\n        \"document\": document,\n        \"classification_model_options\": {\n            \"v2_model\": {\n                \"content_categories_version\": content_categories_version\n            }\n        }\n    })\n// ...\n\n\nYou can also check ClassificationModelOptions\u00a0reference for available options.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-11-02T16:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "From the Classifying Content guide, you can include classification_model_options\u00a0within the request\u00a0dictionary argument to the classify_text()\u00a0function. In these options, you can define the model and version to use for content categories.\n\n// ...\ncontent_categories_version = (\n        language_v1.ClassificationModelOptions.V2Model.ContentCategoriesVersion.V2) // Assigning the v2 model type\n    response = client.classify_text(request = {\n        \"document\": document,\n        \"classification_model_options\": {\n            \"v2_model\": {\n                \"content_categories_version\": content_categories_version\n            }\n        }\n    })\n// ...\n\n\nYou can also check ClassificationModelOptions\u00a0reference for available options."
			}
		]
	},
	{
		"Question_title": "Vertex AI image classification models lose accuracy when being placed in a python dictionary",
		"Question_created_time": "2022-10-26T10:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-image-classification-models-lose-accuracy-when-being/td-p/482475/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 172,
		"Question_body": "I have made a  model using vertex AI's image classification. Exported as EdgeTPU tflite model to my Raspberry pi 4 with Coral USB accelerator. When I used the Pycoral's example code https://github.com/google-coral/pycoral/blob/master/examples/classify_image.py  to run my model, I get a perfect prediction result. But when I passed them to a python dictionary in my script, the prediction accuracy is way off. https://github.com/hillyuyichu/Pycoral-python-API/blob/main/pycoral_classification.py   Here is a screenshot of the prediction results on my python classification.py:The label in row 1 is always the most active. The one in the last rows are the least active and most inaccurate.ex: In picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-31T12:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It seems Vertex AI is not supposed to be placed in a Python directory.\n\nIf this is impacting your application or your business, you can file a feature request using the following link. File the feature request, and they could assist you with the feature you are trying to implement."
			},
			{
				"Answer_created_time": "2022-11-02T09:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Comaro!\n\nThanks for the reply! I was able to find a way to work around it."
			},
			{
				"Answer_created_time": "2022-11-02T11:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you share the workaround, Hillyu?"
			}
		]
	},
	{
		"Question_title": "Get Cloud Vision API as good as Google Lens",
		"Question_created_time": "2022-06-24T13:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Get-Cloud-Vision-API-as-good-as-Google-Lens/td-p/434624/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 586,
		"Question_body": "As part of a student team, I am building a system to classify used shoes.I know that Google Lens is doing a really good job here.I came across Google Cloud Vision API (which should be a similar thing) and implemented this in python.For clean, well-angled images like this Air Force One:  I am getting really promising results:If however, i input real-world images like this old used Nike Tanjun:  Things fall apart:But if I upload the image to google lens, I could still figure out the right label:  Logo detection (Nike) almost always works. And using this, I could for example search after the most often occurring word after the Logo (Tanjun) to figure out the model.It must be mentioned that the data of our system will be better than that, there will be multiple images taken from different angles and very good lighting conditions.Now i am trying to figure out how toEITHER: Get Vision API working in the same way as Google LensOR: Acces Google Lens data in a somehow convenient way (should in the best case run from a raspberry pi)   ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-26T05:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "push. I urgently need help."
			},
			{
				"Answer_created_time": "2022-10-06T02:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm in the same shoes. The real answer is, the API does not work so well."
			},
			{
				"Answer_created_time": "2022-11-01T23:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, i'm facing the same problem, do you find any solution to it?"
			}
		]
	},
	{
		"Question_title": "Auto ML edge training failure",
		"Question_created_time": "2022-10-30T16:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Auto-ML-edge-training-failure/td-p/483746/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 50,
		"Question_body": "I am training an edge model in vertex AI. It is failing after a few hours. Details in the screenshot below. Tried 4 times and failed all 4 times. I cannot see to see any detail at all on the error. Can someone from Vertex AI please help? Training fails after about 3 hours if I pick the highest accuracy option but seems to process if I pick the 'best trade-off' options.  Screenshot of the of the failed jobs below.Screenshot upload fails just like getting any support from Google.  The training pipeline id is 2116799302125748224",
		"Answer_list": [
			{
				"Answer_created_time": "2022-11-01T15:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For this type of inquiry the appropriate support channel would be creating a support case\u00a0directly with Google Cloud. This would enable support to directly work with you and your project for solutions, specially when logs are not available for the community in this forum."
			}
		]
	},
	{
		"Question_title": "AI/ML Research Paper Publish",
		"Question_created_time": "2022-10-31T08:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AI-ML-Research-Paper-Publish/td-p/484051/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 29,
		"Question_body": "How should get into research about AI/ML to get some international research intern offer?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-31T08:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "How should get into research about AI/ML to get some international research intern offer?"
			}
		]
	},
	{
		"Question_title": "Vertex AI quota policy exceed when training custom model",
		"Question_created_time": "2022-09-26T02:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-quota-policy-exceed-when-training-custom-model/td-p/470907/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 430,
		"Question_body": "Hello team,Can anyone please help me with this,I have been trying to run the custom model training in vertex ai and gives an error saying\"Training pipeline failed with error message: The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_cpus\"Followed the below steps to solve it but didn't help me at all,1. Changed the region (As it mentioned in one comment of Stack Overflow for this error)2. Increased CPU instances in the work pool as well as notebooks but didn't help me at all.I have gone through the IAM & API Services, and then when I checked the quotas for the Vertex AI API for all resources in it, none of them had exceeded the quota limit. I'm still confused as to why it was showing a quota exceed error when I was training the custom model.Please help me on this issue, how to solve it.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-27T11:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You need to contact Billing Support."
			},
			{
				"Answer_created_time": "2022-10-30T14:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have the same problem, can you tell me how solved it??\u00a0@Praneeth5533"
			}
		]
	},
	{
		"Question_title": "Dialogflow cx caller abandoned call event",
		"Question_created_time": "2022-10-26T05:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-cx-caller-abandoned-call-event/td-p/482361/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 125,
		"Question_body": "Is there a way to trigger a webhook, as soon as the caller abandons the call?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-27T14:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Currently working with your question."
			},
			{
				"Answer_created_time": "2022-10-28T11:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In regards to your main question, the best option would be to create a Cloud Function. You can actually review the Quickstart: Create a webhook.\n\nWhen the call ends, there must be some variable that you can grab to start the function.\n\nIf we get more information for your main issue, it will be shared with you as soon as possible."
			}
		]
	},
	{
		"Question_title": "deploying model on vertex ai deploymentResourcePool to an endpoint located in another project.",
		"Question_created_time": "2022-10-26T00:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/deploying-model-on-vertex-ai-deploymentResourcePool-to-an/td-p/482309/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 141,
		"Question_body": "I'am trying to deploy a custom trained model to a deployment resource pool that is located in project-1 to an endpoint located in project-2 , I have granted the editor role for project-1 to user account (u1) which also has editor role in project-2. when I try to deploy the model from user account (u1) ,I get the following error:grpc_message:\"DeploymentResourcePool 'projects/{project-1}/locations/us-central1/deploymentResourcePools/drlpool' does not exist.*the deployment resource pool (drlpool) exists and also deploys successfully if the endpoint and the deployment Resource Pool are in the same project.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-27T14:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you please check the roles granted to your service account as advised in this Stack Overflow question:\n\nFor example you have Project A and Project B, assuming that Project A hosts the model.\n\nAdd service account of Project B in Project A and provide at least roles/aiplatform.user predefined role. See predefined roles and look for roles/aiplatform.user to see complete roles it contains.\n\nThis role contains aiplatform.endpoints.* and aiplatform.batchPredictionJobs.* as these are the roles needed to run predictions.\n\nSee IAM permissions for Vertex AI\n\n|Resource|Operation|Permissions needed| |---|---|---| |batchPredictionJobs|Create a batchPredictionJob|aiplatform.batchPredictionJobs.create (permission needed on the parent resource)| |endpoints|Predict an endpoint|aiplatform.endpoints.predict (permission needed on the endpoint resource)|\n\nWith this set up, Project B will be able to use the model in Project A to run predictions.\n\nNote: Just make sure that the script of Project B points to the resources in Project A like project_id and endpoint_id.\n\nIf after that are you still having issuesIf after that you are still having issues it would be better to export the model from project-1 and import into project-2, as shown in the documentation:\n\nThe Model and Endpoint components expose the functionalities of the Vertex AI endpoint and model resources. You can import existing model resources that you've trained outside of Vertex AI, or that you've trained using Vertex AI and exported. After you import your model, this resource is available in Vertex AI. You can deploy this model to an endpoint and then send prediction requests to this resource."
			}
		]
	},
	{
		"Question_title": "Dialogflow cx v3 DetectIntentRequest returning no-match",
		"Question_created_time": "2022-10-26T13:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-cx-v3-DetectIntentRequest-returning-no-match/td-p/482545/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 81,
		"Question_body": "I am trying to create and manage agents using exclusively the api.  I have created an agent with one intent only:name: \"projects/???/locations/global/agents/1828e34b-78bc-48f5-9212-6dd83497d409/intents/b9e91883-f358-46ea-9661-a8a39c7d2557\"\ndisplay_name: \"test-age\"\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 23 \"\nparameter_id: \"p0\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" my age is \"\n}\nparts {\ntext: \" 68 \"\nparameter_id: \"p1\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 44 \"\nparameter_id: \"p2\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age \"\n}\nparts {\ntext: \" 81 \"\nparameter_id: \"p3\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age is \"\n}\nparts {\ntext: \" 35 \"\nparameter_id: \"p4\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" the age is \"\n}\nparts {\ntext: \" 29 \"\nparameter_id: \"p5\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 37 \"\nparameter_id: \"p6\"\n}\nparts {\ntext: \" years of age \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 45 \"\nparameter_id: \"p7\"\n}\nparts {\ntext: \" years \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 52 \"\nparameter_id: \"p8\"\n}\nparts {\ntext: \" years old \"\n}\nrepeat_count: 1\n}\nparameters {\nid: \"p0\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p1\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p2\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p3\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p4\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p5\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p6\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p7\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\nparameters {\nid: \"p8\"\nentity_type: \"projects/-/locations/-/agents/-/entityTypes/sys.number-integer\"\n}\npriority: 500000However, when I try to detect that intent using the DetectIntentRequest as shown in the github samples I keep getting no-match results:====================\nQuery Text: ' I am 55 '\nDetected Intent: text: \" I am 55 \"\nlanguage_code: \"en\"\nresponse_messages {\ntext {\ntext: \"Sorry, could you say that again?\"\n}\n}\ncurrent_page {\nname: \"projects/???/locations/global/agents/1828e34b-78bc-48f5-9212-6dd83497d409/flows/00000000-0000-0000-0000-000000000000/pages/START_PAGE\"\ndisplay_name: \"Start Page\"\n}\nintent_detection_confidence: 0.3\ndiagnostic_info {\nfields {\nkey: \"Alternative Matched Intents\"\nvalue {\nlist_value {\n}\n}\n}I did train the agent in advance. Appreciate any help",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-27T10:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, mihai527,\n\nI understand that you are experiencing the issue with the transcription which is not responding as expected, and this is a transient issue. Instead of identifying the user age, it is Invoking the No-Match event. Please let me know if I have misunderstood.\n\nHere it seems like an issue with the speech adaption. In regards to that, can you please Enable speech adaptation? The auto speech adaptation feature improves the speech recognition accuracy of your agent by automatically using conversation state to pass relevant entities and training phrases as speech context hints for all detect intent requests. This feature is disabled by default.\n\nYou may follow this doc\u00a0to achieve this. Please let me know if this resolves your issue or not. Also, you may follow this doc, which talks about Voice agent design best practices.\n\nPlease let me know if you have any questions regarding the information provided above. I will be happy to assist you.\n\nI will be looking forward to your response."
			}
		]
	},
	{
		"Question_title": "Using RegEx Entity with other entitiy in Traning Phrase gives empty value for the parameter",
		"Question_created_time": "2022-10-19T02:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Using-RegEx-Entity-with-other-entitiy-in-Traning-Phrase-gives/td-p/479671/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 272,
		"Question_body": "n Dialogflow ES I am using a Regular Expression for the date format (DD/MM/YYYY) validation. If I have the training phrase as 22/05/2021 it works perfectly.But I need to have the training phrase as 22/05/2021 sample@sample.com. When I use like that, for the date it gives empty value.Not even for this mydate validation, this is not working if I use any RegEx entity along with other entity in the same training phrase.myDigit RegEx is ^[0-9]$So can't it be used Regex entity along with other entity in the same training phrase in Dialogflow?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-26T10:18:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Sorry for the delay in replying, I see that your regular expression is matching the end of the line with \u201c$\u201d after the digit (^[0-9]$). This would explain why you are able to capture the data when the training phrase contains only the digit, but not when additional data is located after (for example, \u201c2 sample@sample.com\u201d does not contain the EOL metacharacter after \u201c2\u201d, but having only \u201c2\u201d as the training phrase does).\n\nIf your date regular expression matches the end of the line as well, you\u2019d see the same behavior. I made a quick intent using this expression to match \u201cDD/MM/YYYY\u201d: ^[0-9]{2}\\/[0-9]{2}\\/[0-9]{4}. As shown below, it captures both the date and Email:\n\n\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-10-20T11:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Do you experience this problem when adding multiple entities of any kind into a single phrase? Do you also see any difference if you edit the regular expression?"
			},
			{
				"Answer_created_time": "2022-10-20T20:12:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yes. I only face this issue when adding RegExp entity with multiple other entities in single training phrase. It works fine if I have only this RegExp entity in the training phrase. I tried changing the regular expression; but the issue remains the same."
			},
			{
				"Answer_created_time": "2022-10-26T10:18:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Sorry for the delay in replying, I see that your regular expression is matching the end of the line with \u201c$\u201d after the digit (^[0-9]$). This would explain why you are able to capture the data when the training phrase contains only the digit, but not when additional data is located after (for example, \u201c2 sample@sample.com\u201d does not contain the EOL metacharacter after \u201c2\u201d, but having only \u201c2\u201d as the training phrase does).\n\nIf your date regular expression matches the end of the line as well, you\u2019d see the same behavior. I made a quick intent using this expression to match \u201cDD/MM/YYYY\u201d: ^[0-9]{2}\\/[0-9]{2}\\/[0-9]{4}. As shown below, it captures both the date and Email:"
			},
			{
				"Answer_created_time": "2022-10-26T22:23:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you very much for your solution. It worked perfectly when I removed $ from my Regular Expression.\n\nI am posting the RegExp used in case someone else needs it (only get the valid dates only, avoid dates like 31/02/2022, 35/18/2022). Got it from https://stackoverflow.com/a/20773444/1719133\n\n(^(((0[1-9]|1[0-9]|2[0-8])[\\/](0[1-9]|1[012]))|((29|30|31)[\\/](0[13578]|1[02]))|((29|30)[\\/](0[4,6,9]|11)))[\\/](19|[2-9][0-9])\\d\\d)|(^29[\\/]02[\\/](19|[2-9][0-9])(00|04|08|12|16|20|24|28|32|36|40|44|48|52|56|60|64|68|72|76|80|84|88|92|96))"
			}
		]
	},
	{
		"Question_title": "Vonage Smart Number",
		"Question_created_time": "2022-10-25T00:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vonage-Smart-Number/td-p/481819/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 127,
		"Question_body": "Trying to find a way to link a Vonage Smart Number (Vonage Communications, not API) to Dialogflow ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-26T08:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can use Contact Center AI\n\nContact Center AI (CCAI) is an extension of Dialogflow services that helps create contact center solutions. You need to request access to Contact Center AI documentation. For information, see the Contact Center AI solution overview.\n\nAs shown in this Google Cloud Blog entry Deliver an exceptional customer experience with Contact Center AI, now GA:\n\nYou can now integrate Contact Center AI with your existing workflows and start seeing results within 3-6 months, thanks to integrations with partners such as Avaya and Mitel, who are GA today, as well as 8x8, Cisco, Five9, Genesys, NICE inContact, Salesforce, Twilio, and Vonage.\n\n\u2026\n\nTo find out how Contact Center AI can increase CSAT, deflection rates, and operational efficiency, visit our site, contact your Google sales representative, or request to be contacted."
			}
		]
	},
	{
		"Question_title": "What's the order for the labels in txt file after I have exported my tflite model from Vertex AI",
		"Question_created_time": "2022-10-21T12:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-s-the-order-for-the-labels-in-txt-file-after-I-have/td-p/480804/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 379,
		"Question_body": "I have exported my trained tflite model. But I noticed the order of the labels in the txt file matters. I'm using image classification models. The ones with only two labels, it's an easy fix. I just switch the two. But when I have more than two labels, I notice the predictions are way off. Does it say in Vertex AI or is there a general rule to what label should go first, second, third..etc in the txt file that we create on our own? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-25T15:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "After reviewing more about Export AutoML Edge models, you can see the following TensorFlow documentation to learn more about extracting this information.\n\nTensorFlow Lite inference with metadata\nGenerate model interfaces with TensorFlow Lite code generator\nAdding metadata to TensorFlow Lite models\n\nThe documentation that might help more for your question is the last one \u201cAdding metadata to TensorFlow Lite Models\u201d.\n\nBut what I can suggest to you is to send an email to tensorflow-enterprise-support@google.com with your question, and hopefully they can give you a direct solution to your concerns.\n\nAdditionally, I found this Stack Overflow question to create labels.txt manually.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-10-25T10:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I am still reviewing documentation to determine if there is one way to create a rule.\n\nAs soon as I collect more information about your main concern, I will share it with you as soon as possible."
			},
			{
				"Answer_created_time": "2022-10-25T15:27:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "After reviewing more about Export AutoML Edge models, you can see the following TensorFlow documentation to learn more about extracting this information.\n\nTensorFlow Lite inference with metadata\nGenerate model interfaces with TensorFlow Lite code generator\nAdding metadata to TensorFlow Lite models\n\nThe documentation that might help more for your question is the last one \u201cAdding metadata to TensorFlow Lite Models\u201d.\n\nBut what I can suggest to you is to send an email to tensorflow-enterprise-support@google.com with your question, and hopefully they can give you a direct solution to your concerns.\n\nAdditionally, I found this Stack Overflow question to create labels.txt manually."
			},
			{
				"Answer_created_time": "2022-10-25T17:31:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks! I was able to use unzip command in my terminal and extract the label file."
			}
		]
	},
	{
		"Question_title": "Converting TIFF files to base64 using Java",
		"Question_created_time": "2022-10-24T08:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Converting-TIFF-files-to-base64-using-Java/td-p/481476/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 197,
		"Question_body": "Hi I have two instances where I'm converting a TIFF file before sending the data to google vision AI. Using CLI, the base64 string works fine. I get the expected responseWhen I use Java I'm getting a different out, which causes my API call to fail. The base64 output is different.  I'm looking for suggestions on how to convert a file (tiff, pdf, png) to base64 using plain Java ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-25T14:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You could convert your images as shown in this sample code from this Stack Overflow question:\n\n    File File = new File(args[2]);\n    String ImageString;\n\n    FileInputStream fileInputStreamReader = new FileInputStream(File);\n    byte[] bytes = new byte[(int)File.length()];\n    fileInputStreamReader.read(bytes);\n    ImageString = Base64.getEncoder().encodeToString(bytes);\n\n\nSee also:\n\nBase64 Encoding - Using client libraries"
			}
		]
	},
	{
		"Question_title": "Confirmation page / custom text",
		"Question_created_time": "2022-10-22T03:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Confirmation-page-custom-text/td-p/480915/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 50,
		"Question_body": "Hi All,I'm just migrating over from AWS to CX, and so far I think its great. However - the tutorial section I'm working through kind of hit a 'draw the rest of the owl' meme - if you don't know it, look it up.The difficult bit is where it gets to 'confirmation page' in the quick start - here: ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-25T13:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In order to fulfill the create confirmation page step, you could follow the instructions given in the Create the location page section.\n\nIn order to get the example working, please double check that all steps are completed and the variable names are the same as used in order to test the completed agent.\n\nThis article could be helpful to create A Conversational Agent with Dialogflow.\n\nSome important concepts used in the quickstart:\n\nIntent\nParameters\nEntity types\nSession parameters\nConditions"
			}
		]
	},
	{
		"Question_title": "Will Google provide MTQP in Cloud Translation API?",
		"Question_created_time": "2022-10-25T12:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Will-Google-provide-MTQP-in-Cloud-Translation-API/td-p/482115/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 1,
		"Question_view_count": 70,
		"Question_body": "Hi, I discovered with interest that your Google Translation Hub advanced tier offers document post-editing features, and, as part of that, includes an MTQP quality prediction score on a segment by segment basis. \n\nThis would be a very interesting feature to include in Cloud Translation API, particularly for TMS and CAT tools like Trados/MemoQ/Memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional Translation Memory technology.\n\nIt could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.So my question is whether Google is looking at making this available in the API, or whether Google is treating that as proprietary information that you guys do not want to make available outside of your Google Translation Hub?  I really hope the answer is the former, not the latter...\n\nThank you.\n\nMichel Farhi\nPrincipal Localization Engineer\nNI (formerly National Instruments)",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-25T12:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\u00a0\n\nI discovered with interest that your Google Translation Hub advanced tier offers document post-editing features, and, as part of that, includes an MTQP quality prediction score on a segment by segment basis.\u00a0\n\nThis would be a very interesting feature to include in Cloud Translation API, particularly for TMS and CAT tools like Trados/MemoQ/Memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional Translation Memory technology.\n\nIt could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.\n\nSo my question is whether Google is looking at making this available in the API, or whether Google is treating that as proprietary information that you guys do not want to make available outside of your Google Translation Hub?\u00a0 I really hope the answer is the former, not the latter...\n\nThank you.\n\nMichel Farhi\nPrincipal Localization Engineer\nNI (formerly National Instruments)"
			}
		]
	},
	{
		"Question_title": "Short glossary terms not respected in the Translation API",
		"Question_created_time": "2022-10-20T11:04:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Short-glossary-terms-not-respected-in-the-Translation-API/td-p/480322/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 109,
		"Question_body": "We are seeing that some glossary terms are not respected in the translation API. For example, we have \"IT\" defined in our glossary for both English and Spanish. However,  it is being translated to \"TI\" when translating from English to Spanish. Other glossary terms are behaving as expected. Is anyone else seeing this? It's causing a lot of issues with our translations.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-24T07:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This behavior is caused by the word \u201cit\u201d being included as an English stopword\u00a0for the Translate API. Any stopwords used as entries in a glossary will be ignored\u00a0by the API. I created a glossary to replicate this, and I also saw the same results.\n\nGiven that entries containing only the stopword will be ignored, you could instead pair the word \u201cIT\u201d in multiple entries with additional words (e.g. \u201cIT industry\u201d). Another possibility would be to use another abbreviation for \u201cIT\u201d such as \u201cI.T.\u201d"
			}
		]
	},
	{
		"Question_title": "SSML to read Date in German Language not working",
		"Question_created_time": "2022-04-21T14:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/SSML-to-read-Date-in-German-Language-not-working/td-p/415908/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 121,
		"Question_body": "Hi - I am working on converting Text to Speech using SSML via Google Speech API. Below is the request to the API. This perfectly works when language Code is En-US , however for code de-DE and to hear in German voice, the output is totally random. Please help me in checking this issue,  TTS Request JSON :: {\"voice\":{\"ssmlGender\":\"MALE\",\"name\":\"de-DE-Wavenet-E\",\"languageCode\":\"de-DE\"},\"input\":{\"ssml\":\"<speak><say-as interpret-as=\\\"date\\\" format=\\\"yyyymmdd\\\"> 20220506<\\/say-as><\\/speak>\"},\"audioConfig\":{\"sampleRateHertz\":8000,\"volumeGainDb\":0,\"speakingRate\":1,\"audioEncoding\":\"LINEAR16\",\"pitch\":0,\"effectsProfileId\":[\"telephony-class-application\"]}} The same request when changeing the name and Language code works perfectly.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-06T07:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You will want to see this documentation where it\u2019s explained how to use different languages instead of english in SSML they also provide some code examples as the one i\u2019m sharing where it explains how to use the Voice tag in SSML.\n\n\u00a0\n\n<speak>The dog is friendly<voice name=\"fr-CA-Wavenet-B\">mais la chat est\u00a0mignon</voice><break time=\"250ms\"/> said a pet shop\u00a0owner</speak>"
			},
			{
				"Answer_created_time": "2022-10-22T09:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Eduardo,\n\nyou may have a look at this as well. The google asr has still problems with german date transcription:\n\nhttps://stackoverflow.com/questions/66799469/google-speech-recognition-weak-date-transcription\n\nBest Regards\n\nAndre"
			}
		]
	},
	{
		"Question_title": "Vertex Workbench Managed Notebook vs Quotas",
		"Question_created_time": "2022-08-03T08:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-Workbench-Managed-Notebook-vs-Quotas/td-p/450055/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 206,
		"Question_body": "Hi to allA little help need it... Im trying to create a Managed Notebook with the next configuration for what I request a quota increase as you can see in the following imagesa2-highgpu-1g (Accelerator Optimized: 1 NVIDIA Tesla A100 GPU, 12 vCPUs, 85GB Ram) The problem is that although I got the quota increase I have not been able to create the notebook in any us regions as:  US Central (Iowa1), Us West (Oregon), always get the same error: Could not create instance: Quota limit 'GPUsA100PerProjectPerRegion' has been exceeded. Limit: 0 in region us-central1.What Im doing wrong? How much quota do I need to get so I can use this type of machine?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-04T14:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For quotas error you can contact Billing support."
			},
			{
				"Answer_created_time": "2022-10-22T05:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's the same problem for me. Let us know if you were able to solve it"
			}
		]
	},
	{
		"Question_title": "Working with Context for different intent",
		"Question_created_time": "2022-09-19T10:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Working-with-Context-for-different-intent/td-p/468253/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 145,
		"Question_body": "Hello,\ni'm really new in DialogFlow and would like to dig deep my knowledge about this topic.\nMy question right now is about context. So for my case, i build a Conversational Bot for a Fitness Center and trying to create intent related to an individual's program goal\n\nSo for the training phrases would be\"i want to get ideal weight\"\n\"i want to build muscle\"\n\"i want to be more healthy\"\n\"i want to lose weight\"\n\"i want to gain weight\"\n\nI Called the entity \"Individual-goal\"\nTha Output Context for this is \"Fitness-Goal\"Now for my question:\ni would like to segment OR create the \"Sub-Context\" for this Fitness Goal, in following category:\nHealth - Gain Weight\nHealth - Lose Weight\nHealth - General\nFitness - Muscle Building\nFitness - General\n\nFor this case:\n1. Is it better for me to create multiple Intent ?\n2. Is there a way to put a context based on the response, like \"IF Individual goal contain 'gain weight' then Output Context set to \"Health - Gain Weight\"\n\nWhats the best scenario for this ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-28T11:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It is better to create separate intents as shown in the examples of this documentation."
			},
			{
				"Answer_created_time": "2022-10-21T08:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Its a while since you asked this so you may have already worked it out. You probably need to create an intent for each category like 'gain weight; so another one for 'fitness building'.\n\nAs CX is a 'state machine' (think of it this as lots of rooms off a corridor) the conversation depends what room your in. So if your in a 'lose weight room' your intent keyword of 'weight' will mean 'lose weight in that room'. But in a 'gain weight' room any conversation around weight will pick up trained phrases that interpret this in a way that relates to adding on weight.\u00a0\n\nHence you need to chose how many separate 'rooms' (different topics) you need and build the dialog specific to each room through intent to get you in there, and then build up your fulfilment from it too. Each room is standalone really. CX only finds the trained answers for that specific room when it is in it. This is the 'state machine' in action.\n\nFrom that map of rooms you then link your rooms together through pages / routes.\u00a0\n\nThe map starts in the 'default start page' so all rooms connected to start page ('routed') lead off it like a big entrance hall. User initially chatting 'gain weight' will be taken into that room (state) by CX. Once in there, unless you build a route directly from the gain weight room to the lose weight room, the visitor won't be able to access lose weight. They'd have to go back to the entrance hall to have a door into lose weight.\n\nSo you'll need to build a network of rooms through intent and pages, route them off a central entrance hallway and if you want them to interconnect you'll have to build a route network between them too.\n\nIt can get messy but it's genius when you get your head round it. Do the logical design first or frankly it can hurt \ud83d\ude42"
			}
		]
	},
	{
		"Question_title": "Meetup on machine translation for low-resource languages this Friday!",
		"Question_created_time": "2022-10-19T12:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Meetup-on-machine-translation-for-low-resource-languages-this/td-p/479955/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 1,
		"Question_view_count": 31,
		"Question_body": "The last machine translation meetup featured a PM for the Google Cloud Translation API in person.The next machine translation meetup is all about low-resource machine translation and it'll be online.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-19T12:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The last machine translation meetup featured a PM for the Google Cloud Translation API in person.\n\nThe next machine translation meetup is all about low-resource machine translation\u00a0and it'll be online.\n\n\u00a0\nmachinetranslate.org/meetup\n\u00a0\nThe 25-minute panel features guests from Meta AI,\u00a0NeuralSpace, LoResMT, and Masakhane!\n\nRegister to join us\u00a0this Friday at 8am PST"
			}
		]
	},
	{
		"Question_title": "Too many pages",
		"Question_created_time": "2022-10-17T05:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Too-many-pages/td-p/478806/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 54,
		"Question_body": "I sent a 13 page pdf thru a document ai parser.  GCP, instead of populating the errors collection of the result with an error indicating too many pages, instead throws a runtime error causing a crash.Is try...except... really the best solution for this as I've not seen use of try...except in any of Google parser examples.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-17T16:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For this kind of issues, you could file a report as shown in the documentation:\n\n\nIssue reports\n\nGoogle reviews every new issue report submitted by users. Sometimes one of our staff will ask for clarification or follow up. After we're able to replicate the issue, we'll tell you that it's been forwarded to the appropriate team.\n\nDepending on the circumstances, we may be able to provide periodic updates while an issue is being looked at, but usually we cannot provide too many specifics about the exact cause of an issue, or when it will be fixed.\n\nWhen we've fixed an issue in production, we'll indicate this and then we'll close the issue.\n\nYou can create a new Document AI issue here."
			}
		]
	},
	{
		"Question_title": "No results with is_final true for single utterance set to true",
		"Question_created_time": "2022-10-11T06:18:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/No-results-with-is-final-true-for-single-utterance-set-to-true/td-p/476809/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 120,
		"Question_body": "I am using below configuration to identify my voice input stream (Hindi language)  :",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-12T09:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Have you confirmed whether changing the set_interim_results()\u00a0property to `false` changes the output result? Based on the documentation, using false\u00a0as the argument will only return results with is_final=true."
			},
			{
				"Answer_created_time": "2022-10-13T06:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for your response.\n\nI do not get any results if I change set_interim_results() property to false.\u00a0\n\nI did get some interim results when the property was set to true and\u00a0\n\nspeech_event_type() did return 1. Usually, I get response with is_final set to true, but this does not happen in this case . Below is the log from my application when\u00a0set_interim_results() is set to true\u00a0:\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nhlpr_start_stream() ----> !!!!!!!!!!!!!! Vaibhav :: single_utternace :  ======== : 0\n\n2022-10-11 09:24:15:824114 [INFO]    hlpr_start_stream() --> printing results : 0.900000\n\n2022-10-11 09:24:15:824126 [INFO]   hlpr_start_stream() ---> !!!!!!!!!!!!!! inside 2nd if -------------------------------------\n\n2022-10-11 09:24:15:824132 [INFO]    ########## hlpr_start_stream() printing interim results transcript  #####: \u0907\u0938\n\n2022-10-11 09:24:15:824139 [INFO]    hlpr_start_stream() -----Vaibhav :: time_since_epoch() = 0 \n\n2022-10-11 09:24:18:257357 [INFO]   hlpr_start_stream() ----> !!!!!!!!!!!!!! Vaibhav :: single_utternace :  ======== : 1\n\n\u00a0\n\nMy application keeps on waiting for response till I close the stream."
			},
			{
				"Answer_created_time": "2022-10-17T13:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As your application remains waiting until you close the stream, have you verified that the final result of the transcription is not merely taking additional time? As an example, this issue\u00a0shows similar behavior involving not receiving the final transcription quickly, depending on the language. Are you experiencing the same behavior even when using a different language?"
			}
		]
	},
	{
		"Question_title": "Poor OCR results from PDF files compared to TIFFs",
		"Question_created_time": "2022-10-14T03:18:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Poor-OCR-results-from-PDF-files-compared-to-TIFFs/td-p/478060/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 93,
		"Question_body": "Hi,We're using DOCUMENT_TEXT_DETECTION in production to perform OCR on documents. We've found  the quality of OCR of PDF documents compared to the exact same TIFF to be very poor (with missing characters, extra whitespace etc).I've attached an example test image in both PDF and TIFF formats. You can see the text is very legible and the OCR from the TIFF is 100% correct. The OCR from the PDF has multiple missing characters.This leads me to believe that the internal rendering of PDFs performed by the cloud vision API is buggy.Can anyone shed any light?Correct OCR results from TIFF:Poor read from PDF:See missing hyphen, missing 'ME' from 'PAYMENT', and various lost hash/pound characters with extra newlines.The pdf and tiff can be found in this shared gdrive: https://drive.google.com/drive/folders/1M4VZ3cT3YDoEn5o565fdWP6_47Y_KISL?usp=sharingHere's a screenshot of the PDF for ease: ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-17T09:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, phildrip,\n\nCould you try TEXT_DETECTION instead of DOCUMENT_TEXT_DETECTION and share your results?\n\nTo update your model, simply set the 'model' value to \"builtin/latest\", e.g code sample:\n\nclient = vision.ImageAnnotatorClient()\nfeature = vision.types.Feature(\ntype=vision.enums.Feature.Type.TEXT_DETECTION, model=\"builtin/latest\")\n\nI will be awaiting your response."
			}
		]
	},
	{
		"Question_title": "How to connect Elasticsearch to Jupyter Notebook on GCP",
		"Question_created_time": "2022-10-08T02:28:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-connect-Elasticsearch-to-Jupyter-Notebook-on-GCP/td-p/475882/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 234,
		"Question_body": "Hi,I am working on an ML project which requires to use Transformers and Elasticsearch.For Transformers, I have created a Jupyter Notebook instance on GCP.For Elasticsearch I will create another instance on CGP.As part of project requirement, I need to access Elasticsearch in Jupyter Notebook through port 9200. I need to ingest data in Elasticsearch and run search queries which can fetch relevant information from Elasticsearch DB and give it in Notebook.My question is that if I create two separate instances of Notebook and Elasticsearch and if i try to connect Elasticsearch through Jupyter Notebook via port 9200, will i be able to connect and perform the above mentioned operations ?If NO, then what is the procedure to do so ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-10T15:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you elaborate on how you have set up the Elasticsearch and Jupyter notebook instances? On which compute service is your Elasticsearch instance running?"
			},
			{
				"Answer_created_time": "2022-10-12T02:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nBelow is the Screenshots of the way i have setup Jupyter Notebook instance and Elasticsearch instance.\n\n1.\n\nVertex AI Workbench - Jupyter Notebook\n\nThis is a link of the screenshot of the Vertex AI workbench where I have created a Jupyter Notebook instance.\n\n2. Below is the screenshots of the way I have accessed Elasticsearch through marketplace. Once I click on the service provider, the second screenshot is the landing page of Elasticsearch Cloud where also, I have signed and created the account.\n\nElasticsearch in Market Place\nLanding Page of Elasticsearch once we click on provider\n\n\u00a03. I want to access the Elasticsearch through \"http\" request made through my Jupyter Notebook and the screenshot of the codes which I want to run in my Jupyter Notebook is attached in below links.\n\nCode Snippet 3\n\n\u00a0\n\nCode Snippet 2\n\n\u00a0\n\nCode Snippet 1\n\n\u00a0Please Note that I want to access Elasticsearch version 8 onwards, as version 8 onwards, there is a facility to ingest \"Dense Vector\" in the database."
			},
			{
				"Answer_created_time": "2022-10-14T15:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Using the Elasticsearch managed service from the GCP marketplace, you would need to use the [provided guide\u00a0to connect using a Python environment (in this case being the Jupyter notebook). Additional methods of connecting to the JSON API of the service are also included\u00a0in the documentation.\n\nIf you would like to exclusively connect directly to the deployment using port 9200, creating a deployment within a VM instance in GCE\u00a0should be more appropriate. This is due to the fact that a Vertex Workbench Notebook is also hosted on a GCE instance, and can be set up to be in the same VPC as a VM hosting Elasticsearch."
			}
		]
	},
	{
		"Question_title": "Does Vertex AI support multiple model instances in Same Endpoint Node.",
		"Question_created_time": "2021-09-13T04:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Does-Vertex-AI-support-multiple-model-instances-in-Same-Endpoint/td-p/169614/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 120,
		"Question_body": "We are trying to deploy the model in Vertex Endpoint with GPU support. \nHere we are facing two problems, GPU memory is fully reserved by a single model but GPU poweris underutilize. \n\nSo can we deploy multiple Workers in the Same Node and also how to allow the worker to reserve VRAM only up to it required?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-14T15:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can deploy more than one model to the same endpoint (documentation), however the resources are\u00a0associated with the model rather than the endpoint."
			}
		]
	},
	{
		"Question_title": "How to set multiple series identifier columns on tabular forecast?",
		"Question_created_time": "2021-08-11T22:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-set-multiple-series-identifier-columns-on-tabular/td-p/166959/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 178,
		"Question_body": "Hello,I tried BigQuery ML's ARIMA+ to predict sales data, but the results were not particularly good.So, I wanted to try adding weather as a feature to the dataset. This requires the use of Vertex AI Tabular forecast (AutoML).The dataset looks like this.When using ARIMA+, multiple columns can be specified by using the following statement. How to set multiple series identifier columns on AutoML? Should I consider merging the store and product columns into one column(eg: tokyo_pixel6)?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-14T15:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I found this section of the documentation, which might be helpful:\u00a0\n\nOne of your columns in your training data for a forecasting model must be specified as the time series identifier. Forecasting training data usually includes multiple time series, and the identifier tells Vertex AI which time series a given observation in the training data is part of. All of the rows in a given time series have the same value in the time series identifier column.\n\nSome common time series identifiers might be the product ID, a store ID, or a region. When you have multiple time series in your training data, there should be a specific column that differentiates them.\n\nYou can train a forecasting model on a single time series (in other words, the time series identifier column contains the same value for all rows). However, Vertex AI is a better fit for training data that contains two or more time series. For best results, you should have at least 10 time series for every column used to train the model."
			}
		]
	},
	{
		"Question_title": "How to use Recommendations AI(Retail API) for multiple stores?",
		"Question_created_time": "2022-10-12T04:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-use-Recommendations-AI-Retail-API-for-multiple-stores/td-p/477225/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 95,
		"Question_body": "Hi Guys,We are trying to build an e-commerce personalized recommendation system. we want to make it worthwhile for our clients.  We are trying to use the recommendations API(retail API) for multiple e-commerce stores. But in the retail API, it seems we can use retail API under one project per store. Importing catalogs, creating models, and getting recommendations are only applicable to a single store under one project.One solution is to create separate projects per store only to use retail API, which is not the right way for numerous customers.So, is there any way to do this or any other GCP service that we can go for? Please suggest. Thanks in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-14T11:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you are currently working to build an e-commerce system, one suggestion as an alternative GCP service is to follow the Building an e-commerce recommendation system by using BigQuery ML.\n\n\nAdditionally, here is a guide written by Polong Lin\u00a0using the documentation referred above."
			}
		]
	},
	{
		"Question_title": "Exempt few words within paragraph while translating through Google Translation clod API",
		"Question_created_time": "2022-10-13T08:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Exempt-few-words-within-paragraph-while-translating-through/td-p/477744/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 71,
		"Question_body": "I have a use case where I need to translate biographies of some Doctors. I am using cloud API for translation but I want to exempt few words like degree or school of degree and some other specific terms and some html tags. How can we restrict those not to be translated. Glossary works only for exact match. How can I exempt words within paragraph? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-14T09:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As mentioned\u00a0here,\u00a0you can use the following HTML tags:\n\n<span translate=\"no\"> </span>\n\n<span class=\"notranslate\"> </span>\n\nThis functionality requires the source text to be submitted in HTML.\n\nMore on this can be found in\u00a0this Stack Overflow post\u00a0and\u00a0this group post"
			}
		]
	},
	{
		"Question_title": "Dialogflow should have its own official facebook app for integration",
		"Question_created_time": "2022-10-13T21:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-should-have-its-own-official-facebook-app-for/td-p/477998/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 39,
		"Question_body": "Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflowI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the settings for Webhooks channels it needI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experienceps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-13T21:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflow\n\nI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the\u00a0settings for\u00a0Webhooks channels it need\n\nI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experience\n\nps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label"
			}
		]
	},
	{
		"Question_title": "Speech-to-Text billing optimization",
		"Question_created_time": "2022-10-11T20:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-Text-billing-optimization/td-p/477135/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 51,
		"Question_body": "Hi AllAnyone knows if with the Speech-to-Text API \u00bfCan we do a committed consume contract similar with other GCP services to get fees or billing optimization by month?Thanks in advance for your response",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-11T20:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi All\n\nAnyone knows if with the Speech-to-Text API \u00bfCan we do a committed consume contract similar with other GCP services to get fees or billing optimization by month?\n\nThanks in advance for your response"
			}
		]
	},
	{
		"Question_title": "Due to an error, Vertex AI was unable to train model \"some_model",
		"Question_created_time": "2022-10-09T07:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Due-to-an-error-Vertex-AI-was-unable-to-train-model-quot-some/td-p/476128/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 114,
		"Question_body": "Hi Team\nWe are trying to train the model, but we are getting the below error after running 2 hrs.Region       : us-centerl1(IOWA) Algorithm : AutoML\nObjective  : Image classification (Single-label)\nData split:   Randomly assigned (80/10/10)Due to an error, Vertex AI was unable to train model \"some_model\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \nprojects/1096088445304/locations/us-central1/trainingPipelines/8154185764406558720\nError Messages: INTERNALKindly help us to resolve the issue. \nThanks & Regards\nJambu ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-11T08:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you please share more information on the error message that you are receiving, since the information you are sharing isn\u2019t enough to properly help with the issue you are facing.\n\nThe internal errors occur when there\u2019s an issue with your system. The error could be transient, try to resubmit the CustomJob, HyperparameterTuningJob or TrainingPipeline, if the error persists what is recommended that you do is to contact support."
			}
		]
	},
	{
		"Question_title": "Tabular Forecasting Model in Vertex AI - Cannot deploy model to endpoint",
		"Question_created_time": "2022-10-08T03:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Tabular-Forecasting-Model-in-Vertex-AI-Cannot-deploy-model-to/td-p/475893/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 137,
		"Question_body": "I'm getting started with Vertex AI, and the model I'd like to use is a Tabular Forecasting Model. After spending hours tweaking the model that I wanted to deploy, I came across this error message. \"The default version cannot be deployed\". I can deploy a normal Tabular model to an endpoint, but not the Tabular Forecasting model. Does anyone know if there is a way to deploy a Tabular Forecasting Model? If not, is Google planning on adding this functionality anytime soon? Thanks in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-10T14:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You are able to use Tabular Forecasting on Vertex AI the issue you are facing might be occurring since it was omitted one step from the guide that google offers. I recommend you to check the hyperlink I attached since it outlines how to properly achieve this.\n\nAlso a couple of points that you might want to be aware of are the next ones:\n\nYou can export AutoML tabular classification and regression models only. Exporting AutoML tabular forecasting models is not supported.\nVertex Explainable AI is not available using exported tabular models. If you need to use Vertex Explainable AI, you must serve predictions from a model hosted by Vertex AI.\nThe exported tabular model can run only on x86 architecture CPUs that support Advanced Vector Extensions (AVX) instruction sets."
			}
		]
	},
	{
		"Question_title": "Does Vertex AI support multi model endpoints",
		"Question_created_time": "2021-07-07T03:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Does-Vertex-AI-support-multi-model-endpoints/td-p/163169/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 7,
		"Question_score_count": 2,
		"Question_view_count": 0,
		"Question_body": "We have 100's of models and deploying each one to its independent endpoint is very expensive.We are looking for a way to deploy multiple models to a single endpoint.Our docker image will have all the models and we will be having custom logic to invoke the models based on the request from the endpoint.Similar functionality is available in AWS SageMaker.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-07-08T06:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I read the following Vertex AI documentation page:\n\nhttps://cloud.google.com/vertex-ai/docs/general/deployment#models-endpoint\n\nThis page seems to say that we can deploy multiple models to the same endpoint.\u00a0 If I understand that correctly, you can then serve multiple models from the same endpoint nodes."
			},
			{
				"Answer_created_time": "2021-07-08T07:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I think this means deploying multiple versions of the same model and not completely independent model."
			},
			{
				"Answer_created_time": "2021-07-14T07:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi there,\n\nYou may deploy totally different models to the same endpoint on Vertex AI and split the traffic as you wish. There is no technical restriction. From a business point of view, you may prefer to have the same (or similar) targeting goals for the models in order to support your decisions."
			},
			{
				"Answer_created_time": "2021-11-07T16:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, how would that work though, as in, if the endpoint is the same, how do we make sure that we request a specific model prediction. For example, if we deploy 2 different models, say model1 and model2, to the same endpoint, with a traffic split of 50%, then what this means is that all requests to this endpoint are split to the two models with a probability of 0.5, i.e., if a we make a request, sometimes we will be served by model1 and sometimes by model2. How do we make sure we are served by a specific model in this scenario?"
			},
			{
				"Answer_created_time": "2021-11-16T08:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "you can actually deploy a multi model endpoint and to call a specific model, just add the argument \"TargetModel\": 'yourmodelname.tar.gz'.\n\nfor more information refer to this link :\u00a0https://towardsdatascience.com/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f"
			},
			{
				"Answer_created_time": "2021-11-16T09:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes that option is available in amazon sagemaker as the article suggests.Is it also possible with vertex ai"
			},
			{
				"Answer_created_time": "2022-10-10T06:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nCould you please suggest how to do this using the Python API? (https://googleapis.dev/python/aiplatform/latest/aiplatform.html)\n\nI have been trying but when specifying a `traffic_split` dict, the keys of this dict have to be\u00a0Deployed Model IDs, which makes no sense because the models are not deployed yet when calling `model.deploy()`\n\nThank you!"
			}
		]
	},
	{
		"Question_title": "AI Augmented Sensory Headset",
		"Question_created_time": "2022-10-07T20:18:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AI-Augmented-Sensory-Headset/td-p/475836/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 28,
		"Question_body": "Wondering when Google will develop olfactory sensor addition to VR headsets and technology. In laymens terms, adding the sense of smell to VR headsets using an add on similar to a printer ink cartridge, but designed specifically for the sense of smell. Theoretically, it is possible, but to manufacture it in a large scale. It can change the way programs, especially helping boost the food and hospitality industry as well as giving everyday people a very good reason to smell fresh food and drink... from their phone! Where and how can we further this research for this wonderful idea?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-07T20:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Wondering when Google will develop olfactory sensor addition to VR headsets and technology. In laymens terms, adding the sense of smell to VR headsets using an add on similar to a printer ink cartridge, but designed specifically for the sense of smell. Theoretically, it is possible, but to manufacture it in a large scale. It can change the way programs, especially helping boost the food and hospitality industry as well as giving everyday people a very good reason to smell fresh food and drink... from their phone! Where and how can we further this research for this wonderful idea?"
			}
		]
	},
	{
		"Question_title": "Text to speech data residency",
		"Question_created_time": "2022-10-05T12:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Text-to-speech-data-residency/td-p/474727/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 104,
		"Question_body": "Compliance team is asking for Data residency for text to speech data. On Google website, it says,\u201cText-to-Speech is both stateless and resourceless. This means Data Access and System Event data don't apply. As a result, Text-to-Speech is out of the scope of Client Access Licenses (CAL). Google does not log any customer Text-to-Speech text or audio data.\u201dDoes logging here refer to data storage. We do not want to store any data in the cloud. Regards,",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-07T08:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Reviewing the documentation you shared, you might also want to review Data Logging.\n\nYou will find that if you enable the information log, it helps to enhance speech to text, but not all the information you will use will be preserved, so, the log will only be saved if you actually want to use it or not."
			}
		]
	},
	{
		"Question_title": "Advanced NLU v/s Standard NLU",
		"Question_created_time": "2022-10-06T08:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Advanced-NLU-v-s-Standard-NLU/td-p/475233/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 281,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-06T16:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As shown in the Dialogflow CX - Agents documentation, in the NLU type section:\n\nNLU type\n\nThis can be one of:\n\nStandard\nStandard NLU technology.\nAdvanced\nAdvanced NLU technology. This NLU type works better than standard, especially for large agents and flows. Model training takes longer, so automatic training is disabled. You need to train the flows manually or via API.\n\nTherefore, the NLU advanced is recommended for large agents and flows. Note that the training model takes longer, so automatic training is disabled, and should be done through manual flows or using the API.\n\nThis article discusses further about Dialogflow CX."
			}
		]
	},
	{
		"Question_title": "Google Vision API pricing",
		"Question_created_time": "2022-06-10T09:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Vision-API-pricing/td-p/430454/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 344,
		"Question_body": "Hello,I'm currently using the service of the Google Cloud Vision API. On the website it says that the first 1000 Request are for free every month. But for that I need a Billing account which is not for free if I understand correctly.  So basically you can't use the Cloud Vision API completely for free. Am I right or can you use the service without any costs?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-21T07:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "A Cloud Billing account is set up in Google Cloud and is used to define who pays for a given set of Google Cloud resources.This is needed in Google Vision API because if it passes the request limit these needs to be charged. The Billing account is needed because it tracks all of the costs (charges and usage credits) incurred by your Google Cloud usage."
			},
			{
				"Answer_created_time": "2022-10-06T02:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The first 1000 credits can be used each month without charge, but a billing account is required."
			}
		]
	},
	{
		"Question_title": "Vertex AI - Custom Job (with GPU) froze without errors",
		"Question_created_time": "2022-10-05T10:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Custom-Job-with-GPU-froze-without-errors/td-p/474701/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "Hi all,I ran into an issue yesterday when submitting a custom job in Vertex AI. The job successfully started (as evident by the logs reported) but at some point, just before the script starts using the GPU on the machine, we stopped receving any logs. I let the job run for 20 minutes, but it did not procide any more logs - as well as there was no indication of the machine having any issues.  I then stopped the job manually,  re-created the exact same job by running the same script (using the google-cloud-aiplatform package in Python) with the exact same parameters, and the job ran successfully.Is there any way I can figure out what went wrong in the first job? I am looking for a stable solution to manage custom jobs, but the fact that this happened within my first 5 runs seems very concerning to me, especially since there was no indication that the job was frozen as it could have ran until it hit the max time which would have costed a lot of money.Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-05T10:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi all,\n\nI ran into an issue yesterday when submitting a custom job in Vertex AI. The job successfully started (as evident by the logs reported) but at some point, just before the script starts using the GPU on the machine, we stopped receving any logs. I let the job run for 20 minutes, but it did not procide any more logs - as well as there was no indication of the machine having any issues.\u00a0 I then stopped the job manually,\u00a0 re-created the exact same job by running the same script (using the google-cloud-aiplatform package in Python) with the exact same parameters, and the job ran successfully.\n\nIs there any way I can figure out what went wrong in the first job? I am looking for a stable solution to manage custom jobs, but the fact that this happened within my first 5 runs seems very concerning to me, especially since there was no indication that the job was frozen as it could have ran until it hit the max time which would have costed a lot of money.\n\nThanks!"
			}
		]
	},
	{
		"Question_title": "C# handwritten text detection using google.cloud.Vision.v1 Api. why can't detect language?",
		"Question_created_time": "2022-10-02T23:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/C-handwritten-text-detection-using-google-cloud-Vision-v1-Api/td-p/473437/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 161,
		"Question_body": "hello mam/sir,I was used google.cloud.Visiion.v1 for handwritten text recognition for Indian languages .  this code is work but only for Marathi, Hindi languages. but other language like Malayalam, Tamil, Kaneda Telegu its not return a 100 percent result.For Example- i have a 12 months handwritten name(Malayalam, Kaneda etc.) but its recognize only 7 to 8 correct word detection .can you please help me to 100 percent  accurate word detection.please give me a solution .thanks and regards,Bhagyashri ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-10-03T13:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Bjagyashri, I think that you would find a better help on StackOverflow\u00a0since your issue seems to be on your code and people there is specialized on programming."
			},
			{
				"Answer_created_time": "2022-10-03T23:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "okay\n\nthank you for advice."
			}
		]
	},
	{
		"Question_title": "exporting a google autoML translate model",
		"Question_created_time": "2022-09-27T12:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/exporting-a-google-autoML-translate-model/td-p/471646/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 133,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-30T15:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request/Issues.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-09-29T08:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "At the moment AutoML Translation does not support ML model exporting. If you are using AutoML Tables, you may export your ML Model."
			},
			{
				"Answer_created_time": "2022-09-30T02:17:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I see, thank you so much for your reply. If I'm not using AutoML tables, is there a way to convert my model to AutoML tables?\n\nAlso, is exporting AutoML Tranlsation models a feature you would consider adding some time in the near future?"
			},
			{
				"Answer_created_time": "2022-09-30T15:57:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request/Issues."
			}
		]
	},
	{
		"Question_title": "Mount gcsfuse in gcloud ai custom-jobs local-run",
		"Question_created_time": "2022-09-28T00:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Mount-gcsfuse-in-gcloud-ai-custom-jobs-local-run/td-p/471834/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 233,
		"Question_body": "When locally testing my custom-job through \"gcloud ai custom-jobs local-run\" command, I would like to have access to a bucket mounted though gcsFuse as it happens when I launch the same containerized job from GCloud console. Is there the option to have the same access locally?Thank you for helping",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-30T08:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "What you could do is use cloud storage as a file system within ai training, since while using fuse your training jobs on both of the platforms can access your data that is stored on Cloud Storage as files on your local file system, also the documentation I shared provides you useful information as the problems you might encounter, permissions, a brief description of cloud storage fuse, performance related information, the restrictions this method has and also how you can make use of the logs.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-09-30T08:43:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "What you could do is use cloud storage as a file system within ai training, since while using fuse your training jobs on both of the platforms can access your data that is stored on Cloud Storage as files on your local file system, also the documentation I shared provides you useful information as the problems you might encounter, permissions, a brief description of cloud storage fuse, performance related information, the restrictions this method has and also how you can make use of the logs."
			}
		]
	},
	{
		"Question_title": "How does google cloud speech to text api deals with invalid inputs ?",
		"Question_created_time": "2022-09-25T10:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-does-google-cloud-speech-to-text-api-deals-with-invalid/td-p/470810/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 161,
		"Question_body": "I am using google cloud stream(AsyncStreamingRecognize) for speech to text conversion in my applications. I have gone through the below link to understand the structure of response returned by the apis :StreamingRecognizeResponse  I can have various scenarios where I can end up with various invalid scenarios and I do not understand what could be the responses. I can invalid scenarios like :- User speaks in a different language than what is passed in configuration .- User does not speak anything / no input- Only noise gets passed / Data lossIs there any parameter inside my response which can point to above scenarios ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-27T14:56:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "1.- If a user speaks a different language you can use language recognition in audio requests. Speech-to-Text supports alternative language codes for all speech recognition methods. Also, one good practice is to show a phrase that can be used or advice on what language you select to be recognized by Speech-to-Text.\u00a0\n\n2.- There are multiple ways that Speech to text can return an empty response. The source of the problem could be the RecognitionConfig\u00a0or the audio itself.\n\n3.-To avoid that only the noise gets passed and the data is lost you can pre-process the audio just as the best practices doc\u00a0mentions.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-09-27T14:56:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "1.- If a user speaks a different language you can use language recognition in audio requests. Speech-to-Text supports alternative language codes for all speech recognition methods. Also, one good practice is to show a phrase that can be used or advice on what language you select to be recognized by Speech-to-Text.\u00a0\n\n2.- There are multiple ways that Speech to text can return an empty response. The source of the problem could be the RecognitionConfig\u00a0or the audio itself.\n\n3.-To avoid that only the noise gets passed and the data is lost you can pre-process the audio just as the best practices doc\u00a0mentions."
			}
		]
	},
	{
		"Question_title": "Emotional mobiles",
		"Question_created_time": "2022-09-26T23:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Emotional-mobiles/td-p/471342/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 25,
		"Question_body": "My idea is to create emotional mobiles. Were we cannot buy mobiles with only money,mobile must choose us for buy and unique emotional between specific person and his new mobile . An intimacy between mobile and human. Like a puppy or understanding couples mobile and human sinking using AI. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-26T23:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "My idea is to create emotional mobiles. Were we cannot buy mobiles with only money,mobile must choose us for buy and unique emotional between specific person and his new mobile . An intimacy between mobile and human. Like a puppy or understanding couples mobile and human sinking using AI."
			}
		]
	},
	{
		"Question_title": "What's the route after conversion action's sunset",
		"Question_created_time": "2022-09-22T09:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-s-the-route-after-conversion-action-s-sunset/td-p/469678/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 68,
		"Question_body": "Hello,i learn dialogflow with one of the goal to connect with Google Assistant. But i just found out the conversion actions will be sunset.\n\nCan anyone route me on the path to take for the next step of dialogflow integration?\n\nThanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-26T15:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In the next Documentation I\u2019m sharing with you, you can read what are the next steps you might want to follow when conversation actions sunsets."
			}
		]
	},
	{
		"Question_title": ".",
		"Question_created_time": "2022-09-22T14:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/unreadable-title/td-p/469789/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": ".",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-26T09:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I found this stackoverflow question that already has an answer and I thought it might help you, since it\u2019s a similar use case, as the one you are presenting."
			}
		]
	},
	{
		"Question_title": "Getting started DialogFlow CX",
		"Question_created_time": "2022-09-24T12:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Getting-started-DialogFlow-CX/td-p/470603/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 94,
		"Question_body": "Looking into starting a project using DialogFlow CX. Seems rather promising but have one issue I cannot seem to find an answer for. The agent will be connected to via IVR (from Flex/Callcenter). I need to gather some information on start so that I can identify the hotel/property that will be referenced in the conversation.  I found session parameters but those are isolated to the session from start to finish but not passed to the start of a session. We are starting with about 60 properties and when the agent starts, it needs to \"know\" what property it is dealing with. Another quick question - will I need a separate telephony integration number to run multiple concurrent instances? I am really new to all this so my language may be off. Thanks in advance!!Robert ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-24T13:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Howdy Robert,\n\nI think the answer to the puzzle will be to look in detail at how the integration from your IVR to DialogFlow will happen.\u00a0 From what I can tell, at the DialogFlow API level, there is a call called \"detectIntent\" ... this is where the audio/text is passed in and the DialogFlow engine processes a part of the conversation.\u00a0 Looking at the API we find:\n\nhttps://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.environments...\n\nwhich has a query parameters set of options.\u00a0 That all said, can you explain some more about how you see the overall flow happening?\u00a0 Will the hotel/property be somehow passed in before the call or will the identification of the hotel be part of the start of the conversation?\u00a0 Depending on your overall goals, you might also consider contacting Google Cloud sales.\u00a0 They will always be delighted to hear from a prospective customer and be able to guide you on how to understand and get assistance with CCAI.\u00a0 \u00a0It is also possible that your IVR supplier may be able to offer assistance and have an existing relationship with Google Cloud for services and support."
			}
		]
	},
	{
		"Question_title": "Vertex AI - Text entity extraction response format",
		"Question_created_time": "2022-09-18T11:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Text-entity-extraction-response-format/td-p/467926/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 172,
		"Question_body": "Hello!I would like to ask if it's possible to get the same output format from the Vertex AI entity extraction REST API as from the Google Natural Language API?Because now in the response of Vertex AI we get only a list with the confidence scores, displayNames and the start/end offsets of the textSegments, but the entities itself are not in the json (in the NLP response there is also \"content\" with the entity). So this is how I would like to get the response from the Vertex AI for each entity:{\n\"annotationSpecId\": \" \",\n\"displayName\": \"date\",\n\"textExtraction\": {\n\"score\": 0.69745916,\n\"textSegment\": {\n\"startOffset\": \"382\",\n\"endOffset\": \"392\",\n\"content\": \"12.07.2022\"\n}Can you help please how I could achive this?Thank you for your help in advance!Zsolt",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-23T12:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "So if my understanding is correct you want the result to be the same as GNL while using vertex. You could read more about how to properly prepare the text you will use on the documentation that I\u2019m providing you."
			},
			{
				"Answer_created_time": "2022-09-24T09:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Dear Eduardo,\n\nthank you for your reply!\nNo, I didn't mean the input. I already have a dataset with labels and annotations and I already trained a model, but when I make an API call with a text, the field \"content\" is missing in the response. I only get a list of \"displayNames\", \"startoffsets\" and \"endoffsets\", but I also need the field \"content\", where I see the entity that belongs to those \"displayName\", \"startoffset\" and \"endoffset\" fields.\nIs it possible to get a json from the Vertex AI API that also has this \"content\" field?"
			}
		]
	},
	{
		"Question_title": "Short polish inputs recognized but not returned by ASR.",
		"Question_created_time": "2022-09-22T01:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Short-polish-inputs-recognized-but-not-returned-by-ASR/td-p/469439/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 50,
		"Question_body": "Hi,",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-23T15:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "For some languages STT engine requires a longer time to decide the end of a single utterance, so you could use some Fast recognition."
			}
		]
	},
	{
		"Question_title": "How to use ARIMA coefficients from BigQuery",
		"Question_created_time": "2022-09-19T12:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-use-ARIMA-coefficients-from-BigQuery/td-p/468311/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 93,
		"Question_body": "I am trying to use Auto ARIMA from BigQuery and I just want to understand the results. That's what BigQuery is giving me:Store ACoeficients from Store AI trained the model using weekly incomeHow to fit this information in an equation?",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "All I could find is this documentation that explains how to properly use the arima coefficient function on BigQuery."
			}
		]
	},
	{
		"Question_title": "Custom container image not found by Vertex AI for model upload",
		"Question_created_time": "2022-09-16T07:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Custom-container-image-not-found-by-Vertex-AI-for-model-upload/td-p/467487/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 124,
		"Question_body": "Hi,1. I have pushed a custom container to gcr.io/<project id>/reponame/imagename:latest\nfrom gcloud cli on local WSL + podman.2. Then from google console\\ vertex ai\\ model registry, i tried to import the custom container as new model and new version. I'm able to browse to the container image and select the image URI.3. Then once I click finish, i get error container Image not found.\nSame is the error with gcloud  ai models upload command executed from the notebook.Please suggest, how to debug the issue and identify root cause.Thanks in advance.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-23T11:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can read how to properly export a custom container into vertex AI on the documentation I\u2019m sharing with you.\n\nAlso to use a model with a custom container you should also provide a docker container image as the basis of that container."
			}
		]
	},
	{
		"Question_title": "Billing & Cloud Vision API issue with \"Recognize Text\" on Android system",
		"Question_created_time": "2022-09-21T23:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Billing-amp-Cloud-Vision-API-issue-with-quot-Recognize-Text-quot/td-p/469422/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 1,
		"Question_view_count": 41,
		"Question_body": "Hi there,Here I got a billing problem with Cloud Vision API.First, I follow this link to setup my Firebase project to enable the feature of \"Recognize Text\".https://firebase.google.com/docs/ml/android/recognize-textThen all the functions used are normal. I call the function of \"annotateImage\" in Cloud Functions to invoke the Cloud Vision API, then can also used successful.Absolutely, I have trace the flow and requests on Cloud Vision API, it is just looks reasonable.But the issue I encountered is, \"it still charges when I'm not using it\", also when it has no any flow and requests! Billing, September 1-22, 2022 (the project has billing alerts set up now) :Cloud Vision API, 30 days to 9/22/2022 : It would be so gratefull if there any good suggestions !",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-21T23:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi there,\n\nHere I got a billing problem with Cloud Vision API.\n\nFirst, I follow this link to setup my Firebase project to enable the feature of \"Recognize Text\".\n\nhttps://firebase.google.com/docs/ml/android/recognize-text\n\nThen all the functions used are normal. I call the function of \"annotateImage\" in Cloud Functions to invoke the Cloud Vision API, then can also used successful.\n\nAbsolutely, I have trace the flow and requests on Cloud Vision API, it is just looks reasonable.But the issue I encountered is, \"it still charges when I'm not using it\", also when it has no any flow and requests!\u00a0\n\nBilling, September 1-22, 2022 (the project has billing alerts set up now) :\n\nCloud Vision API, 30 days to 9/22/2022 :\n\n\u00a0\n\nIt would be so gratefull if there any good suggestions !"
			}
		]
	},
	{
		"Question_title": "vertex AI Workbench is hanging with error \"Opening notebook with JupyterLab\" for more than a day",
		"Question_created_time": "2022-09-08T08:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/vertex-AI-Workbench-is-hanging-with-error-quot-Opening-notebook/td-p/464300/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 331,
		"Question_body": "I am trying to follow instructions in https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks (vertex AI Jupyter Notebooks tutorials). Steps done1. For the first notebook \"Text Classification model\" I have clicked on \"Vertex AI Workbench\". It takes me to GCP console & workbench.2. I am supposed to click on the \"Create\" button, which I did.3. THen the message \"Opening notebook with JupyterLab\" will come. But it is there for past 1 day, and still it hasn't finished creating. So I canceled the same. I tried once more the same thing happens. Not sure why?I have screen shots, but can't see anywhere to attach.Have anyone tried this tutorial, especially in workbench? Thanks,",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-08T22:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\u00a0\n\nAnybody active on these forums?\n\nIdeally some GCP reps should be there. Especially with newer offering like vertexAI - fundamental issues should be easy to solve!!"
			},
			{
				"Answer_created_time": "2022-09-09T00:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Today I have retried the same. It worked at least creation of notebook.\n\nBut when executing step\n\nInstall additional packages\n\nInstall the following packages for executing this notebook.\n\nI am getting error:\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-recommendations-ai 0.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.8.1 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\n\n\u00a0\n\nAny help?"
			},
			{
				"Answer_created_time": "2022-09-20T14:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "After searching for a solution for your case, it seems to be more an issue of the package version.\n\nI found a GitHub repository dealing with a similar problem to yours; there, you will likely find solutions to resolve it."
			}
		]
	},
	{
		"Question_title": "How to disable TLS 1.0 and 1.1 in dialogflow?",
		"Question_created_time": "2022-09-16T08:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-disable-TLS-1-0-and-1-1-in-dialogflow/td-p/467528/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 52,
		"Question_body": "Need to disable TLS 1.0 and 1.1 for oauth api and events api in the dialogflow. We get those apis while integrating with slack.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-16T08:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Need to disable TLS 1.0 and 1.1 for oauth api and events api in the dialogflow. We get those apis while integrating with slack."
			}
		]
	},
	{
		"Question_title": "Google translator is free or has any kind of pricing?",
		"Question_created_time": "2022-09-05T23:52:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-translator-is-free-or-has-any-kind-of-pricing/td-p/463225/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 87,
		"Question_body": "I'm using this code for translating my website in my angular project. I'm not using translate API provided by google cloud. So, I just need to confirm that the source I'm using is paid for publicly available (free)?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-15T11:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "TRANSLATION is a basic free Google service for users to translate their website content on the web browser side. There are some disadvantages using this such as the Search Engine Optimization and there are limits while using it."
			}
		]
	},
	{
		"Question_title": "How does it work underhood: Predictions of multiple instances (Batch) to Vertex AI online serving",
		"Question_created_time": "2022-08-31T21:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-does-it-work-underhood-Predictions-of-multiple-instances/td-p/462022/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 68,
		"Question_body": "Hello,Vertex AI online serving:When multiple instances are passed for prediction to an endpoint, Does prepackaged container serve the inferences in the same manner as TFX Serving does with enable_batching.  If so how do we optimize batching parameters with multiple instances sent to Vertex AI online.If multi_instances prediction is different from TFX serving batching, how do we gain GPU resources efficient usage optimization with prepackaged serving container.On a general note, how to handle efficient GPU usage for both prepackaged container and custom container using a custom trained model.Please guide.Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-12T11:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Depending on how you perform the custom training you\u2019ll need to set the WorkerPoolSpec. See this document\u00a0to see what how to create a custom job and what it includes."
			}
		]
	},
	{
		"Question_title": "Vertex AI Notebook deleting cells",
		"Question_created_time": "2022-08-30T08:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Notebook-deleting-cells/td-p/461480/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "Hello everyone, I have been facing an issue for the past few months where on occasion my vertex AI notebooks will completely wipe and delete all the cells in a .ipynb file. This happens at what appears to be random times. Context to reproduce: N96 High Memory instance 624 gb of ramIdle time: 1440single user only notebook What happens: notebook with shutdown in the midst of running. Once the notebook is back up and running all the cells in the ipynb file are gone. There is no error message If anyone has faced this issue in the past and knows how to resolve I would really appreciate and information! Thank you ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-09T14:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This seems that it\u2019s an issue that you are facing only applies to your use case, so what I would recommend to you is that you raise a support ticket to receive better support."
			}
		]
	},
	{
		"Question_title": "Output filename for Translate Document API",
		"Question_created_time": "2022-08-26T08:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Output-filename-for-Translate-Document-API/td-p/460318/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 67,
		"Question_body": "Hello,I have been developing an iOS mobile app to translate PDF documents and currently using Google Translate documents API without any issue.Is there any way, we can update or change the output filename coming from the API? Currently, the output file name includes all the attributes starting from the project name, bucket name and finally the file name. This won't be appropriate for the users to showcase these things.Is there any configuration to change this filename to more user friendly format?e.g. Input file name - Test_Translation_En_Fr_Sp.pdfOutput filename from Translate API -scantranslatorapp.appspot.com_uploaded_documents_FuOrb0L4tudAZvhZ99IwsFYg83M2_Test_Translation_En_Fr_Sp_fr_translations.pdfIt would be appropriate to showcase only - Test_Translation_En_Fr_Sp_fr_translations.pdf",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-09T08:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can change the output file prefix for this you need to do a REST request to the as a POST to the API URL:\n\n\n\nhttps://translation.googleapis.com/v3/projects/PROJECT_NUMBER_OR_ID/locations/LOCATION:translateDocument\n\nWith the following request JSON body:\n\n\n\n{\n  \"source_language_code\": \"SOURCE_LANGUAGE\",\n  \"target_language_code\": \"TARGET_LANGUAGE\",\n  \"document_input_config\": {\n    \"mimeType\": \"MIME_TYPE\",\n    \"content\": \"INPUT_BYTE_STREAM\"\n  },\n  \"document_output_config\": {\n    \"gcsDestination\": {\n      \"outputUriPrefix\": \"gs://OUTPUT_FILE_PREFIX\"\n    }\n  }\n}"
			}
		]
	},
	{
		"Question_title": "Vertex pipeline model training component stuck running forever because of metadata issue",
		"Question_created_time": "2022-09-09T02:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-pipeline-model-training-component-stuck-running-forever/td-p/464631/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 169,
		"Question_body": "'m attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.The error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:I've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:protoPayload.methodName=\"google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph\"protoPayload.resourceName=\"projects/724306335858/locations/europe-west4/metadataStores/defaultLeading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.I've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-09T02:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "'m attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.\n\nThe error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:\n\nRetryable error reported. System is retrying.\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=ABORTED, message=Specified Execution `etag`: `1662555654045` does not match server `etag`: `1662555533339`, cause=null System is retrying.\n\nI've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:\n\nprotoPayload.methodName=\"google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph\"\n\nprotoPayload.resourceName=\"projects/724306335858/locations/europe-west4/metadataStores/default\n\nLeading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.\n\nI've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck."
			}
		]
	},
	{
		"Question_title": "Time to deploy model",
		"Question_created_time": "2022-09-06T07:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Time-to-deploy-model/td-p/463368/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 44,
		"Question_body": "I asked to deploy my AutoML model over an hour ago but it is processing the request....  How long should this take?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-06T08:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Answering my own question: \u00a0it took almost 2 hours to deploy a very simple model."
			},
			{
				"Answer_created_time": "2022-09-08T23:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes. I also have seen such behaviour. Not sure whether this is because in most of the cloud services there is a concept of most used services will work fast. They are already in memory and works - probably currently the vertexAPI usage is low now, so the delay."
			}
		]
	},
	{
		"Question_title": "Batch prediction on custom model",
		"Question_created_time": "2022-07-13T11:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Batch-prediction-on-custom-model/td-p/442147/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 875,
		"Question_body": "Hi,I used custom containers for training and prediction to create a model on Vertex AI. Now I want to run batch prediction against it but get error message that says \"Unable to start batch prediction job due to the following error: A model using a third-party image must specify PredictRoute and HealthRoute in ContainerSpec.\"I checked documentation, AIP_HEALTH_ROUTE = /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODELDoes this mean that the model has to be deployed to an endpoint in order to generate the value of the AIP_ENDPOINT_ID variable?However, the documentation \u201cGet batch predictions\u201d says \u201cRequesting a batch prediction is an asynchronous request (as opposed to online prediction, which is a synchronous request). You request batch predictions directly from the model resource; you don't need to deploy the model to an endpoint.I am confused whether in my situation, the model has to be deployed first. Also, is there any resources regarding hosting custom models for batch predictions?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-18T11:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If you are using a custom container, you can read this information about how to use a custom container for prediction.\n\nAbout your confusion, if you are using an API to create batch prediction, you need to send the request to a\u00a0 service endpoint.\n\n\u00a0\u201cTo create batch predictions, we recommend that you select input and output locations that are in the same region as your model. If you use the API to create batch predictions, send requests to a service endpoint (such as https://us-central1-aiplatform.googleapis.com) that is in the same region or geographically close to your input and output locations.\u201d"
			},
			{
				"Answer_created_time": "2022-07-19T10:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the reply. The\u00a0custom\u00a0container link you shared is about using a custom container for (online) prediction. Now my confusion is that, if I only want the model to be trained to serve batch predictions rather than online predictions, do I still need a custom prediction container. Would a deployed model with only training container suffice?"
			},
			{
				"Answer_created_time": "2022-08-03T12:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can upload your models in two ways:\n\n1. With a pre-build container (supported TensorFlow, XGBoost, scikit-learn)\n2. With a custom container\n\nBoth options support batch predictions. With batch predictions, you don't need to deploy your model to an endpoint. Uploading it to Vertex AI is enough.\u00a0\n\nA custom container is only needed if you use another ML framework that is not supported with the pre-build containers. Or you need additional logic as part of your prediction like pre or post processing for example."
			},
			{
				"Answer_created_time": "2022-09-08T05:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I don't know if u have solved your probem but hopefully this helps. I can see how this is confusing, it was the same for me. So batch prediction under the hood is similar to vertex ai endpoint prediction. When you start a batch job a\u00a0a model endpoint to serve model predictions, and a Dataflow job to fetch the data is created, This is then split it into batches, get predictions from the endpoint, and return the results to GCS or BigQuery. All of this is done in a Google-managed project, so you won\u2019t see the model endpoint or the Dataflow job in your own project. So in the custom container you will need to have your model server code that runs your model. You can build your own model server using flask or Fastapi. Or you can also use custom prediction routines which does all that for you and u can focus only on the model logic. So to answer your question for the predict route and health route u need to mention '/predict' and '/health' or whatever name you are giving your routes. I am also working on this currently. So if I am wrong about anything I have told above pls correct me."
			}
		]
	},
	{
		"Question_title": "Using Service Account Keys on different machines",
		"Question_created_time": "2022-08-25T00:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Using-Service-Account-Keys-on-different-machines/td-p/459716/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 56,
		"Question_body": "It is a little unclear to me on how to use the AutoML translation model I created on different machines. I generated a service account key and reference it when calling the API and it works fine on my machine. However, when another user uses the same service account key on their machine, occasionally the following error occurs. The odd part is it doesn't happen all the time.Status(StatusCode=Unavailable, Detail=\"Permission Denied: automl.models.predict\"I had two questions:\n1. When using the service account key on another machine, is there anything else I need to do to authenticate the user?\n2. I know Google advises against sharing keys, however I am using a third-party translation software that requires you set a JSON file key to use the API. Any advice on other methods for allowing other users to use the AutoML translation model would be appreciated.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-01T12:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I will try to answer both questions.\n\nNo, you don\u2019t need anything more to do to authenticate the user, just provide the correct service account and the key to the user.\nNo, the only way to do this is by using the JSON that IAM provides, but also note that sharing the service account key is like sharing a password."
			},
			{
				"Answer_created_time": "2022-09-06T18:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Eduardo_Ortiz\u00a0\nThanks for answering my question. I was able to solve my permission problem by generating separate JSON for each machine. From my observations, it appears if you try to use the same JSON key on different machines permission issues occur randomly although this is not documented anywhere."
			}
		]
	},
	{
		"Question_title": "Need help for compute engine pricing",
		"Question_created_time": "2022-09-06T05:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Need-help-for-compute-engine-pricing/td-p/463295/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 56,
		"Question_body": "GPU: nvidia-a100-80gb has no pricing but  nvidia-tesla-a100 has",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-06T05:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "GPU:\u00a0nvidia-a100-80gb has no pricing but\u00a0\u00a0nvidia-tesla-a100 has"
			}
		]
	},
	{
		"Question_title": "hi-Latn Language Detect Error",
		"Question_created_time": "2022-09-05T01:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/hi-Latn-Language-Detect-Error/td-p/463002/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 158,
		"Question_body": "I keep getting this language as detected language code but i can't seem to find this code in official docs of google translate api Please Help",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-05T01:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I keep getting this language as detected language code but i can't seem to find this code in official docs of google translate api\n\n\u00a0\n\nPlease Help"
			}
		]
	},
	{
		"Question_title": "What difference environments/domains does vertex ai is being in use ?",
		"Question_created_time": "2022-09-02T02:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-difference-environments-domains-does-vertex-ai-is-being-in/td-p/462390/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 40,
		"Question_body": "I wanna learn more about how to use vertex ai in more domains than in recognition and things , so i can learn how to use Vertex ai in other fields and domain , I want to learn like projects using live vertex ai auto ml or realated to those .",
		"Answer_list": [
			{
				"Answer_created_time": "2022-09-02T02:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I wanna learn more about how to use vertex ai in more domains than in recognition and things , so i can learn how to use Vertex ai in other fields and domain ,\u00a0\n\nI want to learn like projects using live vertex ai auto ml or realated to those ."
			}
		]
	},
	{
		"Question_title": "Google Cloud Translate API & Referer Restriction Issue",
		"Question_created_time": "2021-12-31T17:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Translate-API-amp-Referer-Restriction-Issue/td-p/181705/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 0,
		"Question_body": "Hello Dear Community !I have a frustrating issue with the Google Cloud Translate API.I set up correctly the restriction of the API key to some domains including *.example.com/* I launch the script on the URL https://www.example.com/translate and i have the following message :     When i remove the restriction, everything works, but i need the restriction to avoid misuse/abuse.\nI tried to change the restriction to *.example.com, www.example.com, example.com/*, even with the dedicated URL, but nothing works, always the same error message.Do you have any ideas or any ways to investigate better this issue ? How i can know the referrer Google get when i launch my request ? It's driving me crazy !Thanks a lot and happy new year to everybody !!!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-03T13:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The API key being used seems to have some type of HTTP restriction [1], and needs to be properly referred.\n\nYou should recreate a new API key, without an IP referrer, as it doesn't seem related to either roles/permissions.\n\nYou could also try adding the full URL, including \"https\" as mentioned in the examples [2].\n\nHere are some Stack Overflow examples [3][4].\n\n[1] https://cloud.google.com/docs/authentication/api-keys#adding_http_restrictions\n[2] https://cloud.google.com/docs/authentication/api-keys#:~:text=Allow%20any%20subdomain,least%20two%20....\n[3] https://stackoverflow.com/questions/70013973/error-when-using-iap-external-identities-sign-in-page\n[4] https://stackoverflow.com/questions/69803590/error-api-key-http-referrer-blocked-ocurred-at-using-yo..."
			},
			{
				"Answer_created_time": "2022-01-12T10:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for your reply, i created a new API Key with no restriction => It worked\nThen i added a basic restriction : *.domain.com/* and it stopped to work with the same error message as before.\u00a0\nI tried to add the exact full URL with https included, and same error also.\n\nI don't know what to do. In the meantime, we implemented another translate solution from your main competitor and it worked in 5 minutes...\n\nSo it's a bit depressing, especially when we speak about billions of caracters to translate each month with money for you (Google) at the end.\u00a0\n\nThe last time we had an issue with Google API it was for the Maps API, hopefully there was a a chat support included and it was an issue from Google' side regarding country restriction linked to our account.\u00a0 It was solved in few minutes by your technical staff...\n\nBut for the translation API we feel a bit lonely to fix that..."
			},
			{
				"Answer_created_time": "2022-01-08T02:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Same issue still no fix to this.\n\n\u00a0\n\nMyCCPay"
			},
			{
				"Answer_created_time": "2022-09-01T06:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have a key google translate message like this but don't know how to handle it, please help.\n\n\n{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Requests from referer \\u003cempty\\u003e are blocked.\",\n    \"errors\": [\n      {\n        \"message\": \"Requests from referer \\u003cempty\\u003e are blocked.\",\n        \"domain\": \"global\",\n        \"reason\": \"forbidden\"\n      }\n    ],\n    \"status\": \"PERMISSION_DENIED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_HTTP_REFERRER_BLOCKED\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"translate.googleapis.com\",\n          \"consumer\": \"projects/281599394813\"\n        }\n      }\n    ]\n  }\n}"
			}
		]
	},
	{
		"Question_title": "Unable to use audio to text transcribe",
		"Question_created_time": "2022-03-20T00:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-use-audio-to-text-transcribe/td-p/405132/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 162,
		"Question_body": "I am new to this Google Audio transcription and I have set up the whole Google Free Trial thing and I have tried to use the function of Google's Audio to Speech transcript and well so far my customer experience has been so hard.  I have two files and *.mpa and a *.mp4 file and no matter what i do i keep getting an error that it cannot transcribe.Can someone  please help me with this.  Here are the errors I am getting.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-31T23:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can try another variant of\u00a0audio transcription -Audext.\u00a0I like that the software supports various audio file formats like Mp3, WAV, and M4A and it allows editing of the transcript."
			}
		]
	},
	{
		"Question_title": "Deep Learning VM Config to connect to Google Colab",
		"Question_created_time": "2022-05-19T10:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Deep-Learning-VM-Config-to-connect-to-Google-Colab/td-p/424603/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 523,
		"Question_body": "Morning to allIm trying to connect a google colab file to a Google Deep Learning VM with any results. My guess is that I need to configure something inside the VM or the google console but not sure how to do so.I get the error that you will find in image 1 that says:\"The VM requested does not exist. Check out our guide to set up GCE VMs in Colab\"and has a the next link in which theres not much info on how to solve the situation:  https://research.google.com/colaboratory/marketplace.htmlOn image 2 and 3 you will find the info that I add to the colab file that is the same as the VM configuration that you will fins on image 3.What I\u00b4m doing wrong? Do I need to asing special permits to the VM? Any comments or advice is more than appreciatedImage 1Image 2\nImage 3",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-20T17:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Did you confirm that the VM does exist in the project you typed down? If so, is its name correctly spelled and is the VM turned on?"
			},
			{
				"Answer_created_time": "2022-06-02T02:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I solve the issus by changing VM configuration(decrease or increase ram for exemple)"
			},
			{
				"Answer_created_time": "2022-08-30T16:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi All,\n\nI too am experiencing the same issue as the original poster\u00a0@holguinmora\u00a0\n\nWould be very helpful to know the steps/configuration to overcome this issue\u00a0@bentalla"
			}
		]
	},
	{
		"Question_title": "Commercial usage of Google Cloud TTS",
		"Question_created_time": "2022-08-21T18:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Commercial-usage-of-Google-Cloud-TTS/td-p/458420/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 2,
		"Question_view_count": 363,
		"Question_body": "Hi,I wish to use Google cloud's Wavenet TTS (TextToSpeech) for commercial use for my company. Can anyone please confirm whether it is allowed or not?RegardsUtkarsh Dubey",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-21T22:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am sure its not allowed, you can use it only if you choose to use for informative or project purpose, if you wanna use it for commercial purpose get it by being in touch with google cloud partner coordinator here.\u00a0\u00a0Enroll and proceed with further steps.\n\n- Nihal"
			},
			{
				"Answer_created_time": "2022-08-22T00:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What is covered under informative/project use?\nBy commercial use, i meant we will be using it to develop voice-over for our videos. The videos will be used in marketing. Need confirmation on wether it allowed or not."
			},
			{
				"Answer_created_time": "2022-08-30T10:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I actually have this exact same question. Someone from chat support was meant to get back to me but I never heard back \ud83d\ude1e"
			}
		]
	},
	{
		"Question_title": "Translation of MySQL data in 6 different language",
		"Question_created_time": "2022-08-17T02:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Translation-of-MySQL-data-in-6-different-language/td-p/454758/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 122,
		"Question_body": "I have 20K record in 1 Table of MySQL DBThis table is having a Description column and 6 different columns as TLang1, TLang2, Tlang3....I have to translate the data in Description column in 6 Different Languages and insert them in TLang1, TLang2, Tlang3.... columns in the same row.What approach I can use to do this since the current approach is taking too long.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-24T09:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What is your current approach?"
			},
			{
				"Answer_created_time": "2022-08-26T02:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am taking 1 row at a time and calling the translate API 6 times to get 6 different text translations and then updating the row."
			},
			{
				"Answer_created_time": "2022-08-29T09:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You could do a Program that uses a Loop to translate each record and keep it in a Dataframe and then update the fields or replace the Table. There could be a case that you'll need to do this by parts by the Limits that Translation API have."
			}
		]
	},
	{
		"Question_title": "Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
		"Question_created_time": "2022-08-27T00:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project/td-p/460565/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 308,
		"Question_body": "Hi, I'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.  I have enabled aiplatform.googleapis.com.Command:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifierUsing endpoint [https://us-central1-aiplatform.googleapis.com/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.Please suggest what am I missing.   ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-27T04:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-08-27T04:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views."
			}
		]
	},
	{
		"Question_title": "Cloud translations with glossary drops words after glossary item.",
		"Question_created_time": "2022-08-12T05:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-translations-with-glossary-drops-words-after-glossary-item/td-p/453337/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 161,
		"Question_body": "We use the cloud translation API, supplemented with a glossary, to translate English to Dutch. On some occasions the glossary translation drops part of the translation if it is a two part ducht construct, where part 2 follows immediately after the glossary item.For example \"add\" in English becomes \"voeg toe\" in Dutch.  In a sentence with a glossary item for \"tomato passata -> \"tomaten passata\" this becomes:English: Then add the tomato passata and simmer for 10 minutes.Dutch with glossary: Voeg vervolgens de tomaten passata en laat 10 minuten sudderen.Dutch without glossary: Voeg vervolgens de tomatenpassata toe en laat 10 minuten sudderen.In this case the \"toe\" is required but somehow missing in the glossary translation. Any idea what goes wrong here?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-22T14:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There are sometimes that the translations can be inaccurate. For this you can become a contributor\u00a0and help translate to be improved.\n\nNote: While using the translation from dutch to English the phrase given from Dutch with no glossary returns the English value and the Dutch with glossary returns also the same English value, so this could be an interpretation mismatch while using glossaries."
			},
			{
				"Answer_created_time": "2022-08-24T23:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\u00a0\n\nThanks\u00a0@josegutierrez\u00a0 for the response.\n\nIn this particular case the issue is not so much with the (quality) of the translation itself but with the usage of the glossary and then specifically the non-glossary text around the glossary item.\u00a0\n\nI have looked at contributing but I don't think that will help with the specific issue we are experiencing.\u00a0 As a paying customer of the translate API we would appreciate if someone is able to provide some input on the specific issue of words being dropped in the translated glossary sentences ,while they are okay when translating the same sentence without the glossary. At this moment this is really hurting the value we are getting from the google translate api."
			},
			{
				"Answer_created_time": "2022-08-25T11:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As I mentioned before Translation can sometimes be inaccurate, and this could be an interpretation mismatch while using glossaries. Please file a support case, so we can do a detailed review and help you with this issue."
			}
		]
	},
	{
		"Question_title": "Bigquery ML billiing support",
		"Question_created_time": "2022-08-19T01:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Bigquery-ML-billiing-support/td-p/455616/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 51,
		"Question_body": "I have been working on Demand Forecasting in Bigquery ML and i have been creating model with different datasets but recently for a particular dataset price spiked up for the CREATE MODEL query while working on the dataset can anyone help me regarding training model it is because of dataset or any other matter associated with it",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-25T11:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Please contact Cloud Billing regarding this issue."
			}
		]
	},
	{
		"Question_title": "computational instances at the tool Vertex AI",
		"Question_created_time": "2022-08-18T04:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/computational-instances-at-the-tool-Vertex-AI/td-p/455213/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 55,
		"Question_body": "Hello,\nI have a question about the computational instances at the tool Vertex AI in the field of image classification. What are the underlying instances of the process or where can I find out? I'm looking for Information comparable to these syntax for example: Virtual Machine (CPU: Intel Xeon E5-2690 v3, 6 vCPUs, GPU: NVIDIA Tesla K80, 56 GB RAM, 380 GB SSD)\nThanks\nArndt",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-24T09:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You could look at this comparison table\u00a0of machine types, I think the machine types you are looking for are the N1+GPU and the A2 since these two are the VMs that supports GPU and that can be used for Vertex.\n\nAdditionally check the GPUs document\u00a0to see the details of every GPU."
			}
		]
	},
	{
		"Question_title": "WARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it",
		"Question_created_time": "2022-08-04T06:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/WARN-BlockManager-Block-rdd-6-0-already-exists-on-this-machine/td-p/450462/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 121,
		"Question_body": "Hi there.\nI am working with Vertex AI Jupyterlab Notebook.\nThere were a few such warningson this as the model was getting trained.\nMay I know if we are safe to ignore this?\nWhat do they mean actually?\nThanks in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-23T10:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "No, you should not worry since this is just a warning that tells you that those two blocks will not be re added to your notebook.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-08-23T10:20:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "No, you should not worry since this is just a warning that tells you that those two blocks will not be re added to your notebook."
			},
			{
				"Answer_created_time": "2022-08-23T11:51:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for confirming."
			}
		]
	},
	{
		"Question_title": "I am trying to use google vision api in c++ to read local images",
		"Question_created_time": "2022-08-12T15:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/I-am-trying-to-use-google-vision-api-in-c-to-read-local-images/td-p/453466/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 86,
		"Question_body": "I have tried using c++ to use google vision api to read local files, but I have no experience with google, and I am mainly a c++ developer, so changing languages is not that of an option. Do any one knows how to do it in this language. Furthermore, any written documentation or tutorial in my language(c++) will be extremely helpful.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-22T08:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There is a library of Vision API in C++ you can use google-cloud-cpp quickstart."
			}
		]
	},
	{
		"Question_title": "Batch prediction forecasting",
		"Question_created_time": "2022-08-08T09:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Batch-prediction-forecasting/td-p/451758/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 80,
		"Question_body": "Dear ALL;I have made a sales and demand forecasting autoML model. It trained well and is working. I am looking for a way to format input data for batch prediction forecasting where I would like to do more than one forecast horizon predictions.Basically my model uses monthly data granularity with 12 month context length and 6 month forecast horizon. (I have 15 covariate features and predict for 14 separate identifiers).What I would like to do is configure the input data for a batch forecasting where the forecasting would start 3 month earlier than where the feature data end and make forecasts for these three time periods AND when the feature data ends do the normal 6 month forecasting so I end up with 3 month forecasting where I know the actuals and the normal 6 month forecasting.At this point no matter how I format my input data I only get the 6 month forecast horizon.Thanks ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-17T11:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I searched for a document which explain how batch prediction results work within Google Cloud, and I hope it helps you."
			}
		]
	},
	{
		"Question_title": "Google ML kit",
		"Question_created_time": "2022-08-10T08:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-ML-kit/td-p/452579/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 132,
		"Question_body": "I know Google provides an ML kit supported by android that we can integrate into an app. The ML Kit provides many Vision and NLP APIs that can help us make our own Google-like Lens.Anyone can give me more information on how to get the ML kit?I am the CEO and I am looking for a CTO to my company, must be good in Python, A.I., Machine Learning, IoT and Robotics.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-15T10:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "ML Kit is a mobile SDK that brings Google's on-device machine learning expertise to Android and iOS apps. To use ML Kit on Android you\u2019ll need to add the libraries to your module's app-level gradle file. To use on Ios you need to include the ML Kit pods in your Podfile.\n\nYou can use this document\u00a0to see the whole product's quickstarts.\n\nAdditionally see the left menu guides\u00a0for each product and how to use it on Ios or Android."
			},
			{
				"Answer_created_time": "2022-08-16T20:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello\n\nThank you for the information."
			}
		]
	},
	{
		"Question_title": "Join us on August 4! Machine Learning Day on Google Open Source Live",
		"Question_created_time": "2022-07-28T06:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Join-us-on-August-4-Machine-Learning-Day-on-Google-Open-Source/td-p/447714/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 3,
		"Question_view_count": 97,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-16T20:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello @Lauren_vdv\u00a0\n\nThank you for the post, can I register on demand??"
			}
		]
	},
	{
		"Question_title": "Google Translate javascript API",
		"Question_created_time": "2022-08-06T01:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translate-javascript-API/td-p/451250/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 251,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-11T13:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If my understanding is correct, you only want to use that part of the script to translate a web page, right?\u00a0\n\nIf yes I think you can use it as long as you don\u2019t meet the quotas described on this documentation free of charge, and also if you have doubts on how to implement the script you are sharing, you could look up for tutorials online on how to properly make use of it."
			},
			{
				"Answer_created_time": "2022-08-16T17:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThank you for your kind reply.\u00a0\n\nYes, you're right, I only want to use the script to translate a webpage.\n\nI'll go through the document you have referred to.\n\nMinoru Kume"
			}
		]
	},
	{
		"Question_title": "WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB",
		"Question_created_time": "2022-08-04T06:52:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/WARN-DAGScheduler-Broadcasting-large-task-binary-with-size-2-2/td-p/450466/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 0,
		"Question_body": "Hi there.\nThere were quite a number of such warnings as the model was getting trained.May I know if we are safe to ignore them?\nWhat does it mean actually?\nThanks in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-15T12:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There is a limit of MB while broadcasting a task (10 MB), while using your VM that has enough resources if you don\u2019t pass this limit its going to be OK, but if your VM has low resources this could create a timeout.\n\nYou can mitigate it by reducing the task size => reduce the data its handling\n\nFirst, check number of partitions in dataframe via df.rdd.getNumPartitions() After, increase partitions: df.repartition(100)"
			}
		]
	},
	{
		"Question_title": "AutoML Translation models response time",
		"Question_created_time": "2022-08-04T05:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Translation-models-response-time/td-p/450442/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 130,
		"Question_body": "Hi,We have several AutoML Translation models and we are facing timeout issues when the first translation requests are sent. We have to retry a second time to get the translations back. After this first request, it seems the model is kept  \"online\", and subsequent requests to the same model are performing well.What we don't really know is how long the models are kept online and ready for quick response times and how many models can be online simultaneously. We would like to have more information about this in order to handle the translation requests in a proper and controlled way.Thank you,Julian ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-12T11:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Normally when we use the custom models, we will load the model to the chip, if there are more frequent custom models, the least frequent models will be evicted from the chip, and the next when we use the evicted custom models, we need to reload the model to the chip, it will take around 15s. So what happens to the batch translation is that the customers tried to call batch translation with custom models, however, this is the first time for the model to be loaded into the chip, it will take 15s to be loaded, because they have the empty sentences in the output. But for the second try, the model has already been loaded, so we don't need to wait for 15s and we have all translated sentences in the output.\n\nIf there is an inconsistency should be related to the replicas, if we have multiple replicas loading the models at the same time, it is possible that one replica loads the model successfully before others and it starts serving the first request, however the second request is routed to other replicas."
			}
		]
	},
	{
		"Question_title": "Unable to get textStyle in JSON response with Document ai",
		"Question_created_time": "2022-08-04T05:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-get-textStyle-in-JSON-response-with-Document-ai/td-p/450434/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 73,
		"Question_body": "I was trying to get text style or Font style with document ai but was getting null list..This is the file I wanted text style to be extractedThis was the response I received.can someone help me with this?",
		"Answer_list": [
			{
				"Answer_created_time": "",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I previously answered on this Stack Overflow post"
			}
		]
	},
	{
		"Question_title": "Unable to create model",
		"Question_created_time": "2022-08-04T02:08:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unable-to-create-model/td-p/450348/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 111,
		"Question_body": "I am working on demand forecasting where my timestamp duration is 15 minutes and i have attached sample output to below documents.The issue i am facing is despite setting DATA_FREQUENCY = [AUTO_FREQUENCY].ii am getting the error \"Invalid time series: the finest data frequency supported is PER_MINUTE. All input time intervals must be at least one minute\" and the query for create model is given below  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-08T15:48:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "What is happening is that using \u201cAUTO_FREQUENCY\u201d is trying to send the Information as \u201cPER_MINUTE\u201d because of your data, and this needs to have an interval value per minute in each HOUR. You could try with \u201cHOURLY\u201d instead of \u201cAUTO_FREQUENCY\u201d, and it should work.\n\nInstead of:\nDATA_FREQUENCY = 'AUTO_FREQUENCY'\n\nUse \u201cHOURLY\u201d or any other DATA_FREQUENCY:\u00a0\n\nDATA_FREQUENCY = 'HOURLY'\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-08-08T15:48:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "What is happening is that using \u201cAUTO_FREQUENCY\u201d is trying to send the Information as \u201cPER_MINUTE\u201d because of your data, and this needs to have an interval value per minute in each HOUR. You could try with \u201cHOURLY\u201d instead of \u201cAUTO_FREQUENCY\u201d, and it should work.\n\nInstead of:\nDATA_FREQUENCY = 'AUTO_FREQUENCY'\n\nUse \u201cHOURLY\u201d or any other DATA_FREQUENCY:\u00a0\n\nDATA_FREQUENCY = 'HOURLY'"
			}
		]
	},
	{
		"Question_title": "Next Step from Google Colab +Pro",
		"Question_created_time": "2022-05-10T08:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Next-Step-from-Google-Colab-Pro/td-p/421797/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 370,
		"Question_body": "Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some modelsWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?Should I get a Google Compute Engine and try to connect the google colab files to it?Should I up load the model to vertex AI?What characteristics should I need to take into consideration before I select any of the different tools?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-13T11:42:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps://stackoverflow.com/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps://neptune.ai/blog/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-05-13T11:42:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps://stackoverflow.com/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps://neptune.ai/blog/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards"
			},
			{
				"Answer_created_time": "2022-05-16T15:14:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks a lot!!!"
			},
			{
				"Answer_created_time": "2022-05-17T13:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Ulisses hi, had the oportunity to review all the links that you send me but still not very sure what to do next.\n\nI have a ML model that runs in a Google Colab Pro+ that always went down do to Ram consumption.\n\nThe next step in order to be able to run the entire Google Colab Notebook is to connect the notebook to a Google Cloud virtual machine? Should I upload the notebook to a container and then to\u00a0 Vertex AI and see if all the functions from the notebook runs?\n\nWhat will be your recommendation if Im looking for a step by step escalation?"
			},
			{
				"Answer_created_time": "2022-08-08T12:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello\u00a0holguinmora\n\nThe\u00a0Google Workspace Community\u00a0is the most appropriate\u00a0place to\u00a0ask troubleshooting questions and get answers from Google Workspace\u00a0product experts and other administrators considering Google Colab Pro+ is a Workspace product."
			}
		]
	},
	{
		"Question_title": "Relation Google node hour to Azure computing hour",
		"Question_created_time": "2022-08-05T06:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Relation-Google-node-hour-to-Azure-computing-hour/td-p/450911/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 31,
		"Question_body": "Hey there,I am writing my masters thesis at the moment. In my master thesis I compare the machine learning services of Google and Microsoft for image classification. This also includes the costs. Google uses node hours and Microsoft computing hours for the calculation. Is it possible to compare these units? This would be a crucial part of the comparison.\nThanks a lot! ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-05T06:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hey there,\n\nI am writing my masters thesis at the moment.\u00a0In my master thesis I compare the machine learning services of Google and Microsoft for image classification. This also includes the costs. Google uses node hours and Microsoft computing hours for the calculation. Is it possible to compare these units? This would be a crucial part of the comparison.\nThanks a lot!"
			}
		]
	},
	{
		"Question_title": "Document AI Form Processor Parse Table Structure Incorrectly",
		"Question_created_time": "2022-08-01T20:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-Form-Processor-Parse-Table-Structure-Incorrectly/td-p/449252/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 149,
		"Question_body": "Hi All,We are currently using Document AI for form parsing some PDF document and half of times the default former processor either missing a col or messed up some col structure.Let's say the expected file headerSales | Dollar Volume | Average Price For example, I saw cases like1. Missing HeaderSales|Average Price2. Wrong structureSalesDollar|Volume|Average PriceThe content of first two cols are messed up as well. The cell could be missing value or incomplete value.Any recommendation to improve this? If no easy way, any guidance with examples to train or deploy one's own form processor? PS: the document has the same structure. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-04T10:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can improve the data results by using Document AI Parser with AI Platform Notebooks. Also you can use Vision AI to create your own Parser."
			}
		]
	},
	{
		"Question_title": "Feature Engineering Vertex AI/AutoML",
		"Question_created_time": "2022-07-28T09:18:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Feature-Engineering-Vertex-AI-AutoML/td-p/447814/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 131,
		"Question_body": "Hey There,I am writing my Master Thesis at the moment. I am comparing AutoML products for image classification. There I compare the product Vertex AI with Azure from Microsoft. However, I can't find the concrete methods of feature engineering and model selection from the documentation. Does anybody know these methodes used for Google AutoML for image classification?Thanks a lot!Arndt",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-03T15:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Think you are looking for this documentation it describes how feature engineering works within autoML and how it supports it in different ways."
			}
		]
	},
	{
		"Question_title": "Model Selection / Feature Engineering",
		"Question_created_time": "2022-08-02T05:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Model-Selection-Feature-Engineering/td-p/449387/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 69,
		"Question_body": "Hey There,I am writing my Master Thesis at the moment. I am comparing AutoML products for image classification. There I compare the product Vertex AI with to Azure. However, I can't find the concrete methods of feature engineering and model selection from the documentation.Thanks a lot!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-03T14:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Using Vertex Ai You can train models on Vertex AI by using AutoML, or if you need the wider range of customization options available in AI Platform Training, use custom training.\n\nIn custom training, you can select from among many different machine types to power your training jobs, enable distributed training, use hyperparameter tuning, and accelerate with GPUs. See the full custom training documentation here.\u00a0\n\nAdditionally there are 4 other models:\n\nImage Data.\nTabular Data.\nText Data.\nVideo Data."
			}
		]
	},
	{
		"Question_title": "Document AI fails for one particular image, else works great",
		"Question_created_time": "2022-05-02T13:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Document-AI-fails-for-one-particular-image-else-works-great/td-p/419233/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 7,
		"Question_score_count": 0,
		"Question_view_count": 135,
		"Question_body": "We are delivering a platform to a customer based on Document AI. The use case it to send a lottery ticket via API and return the structure information using Document AI. We tried for several hundred images and the Document AI OCR worked great (95%+ times captured right string, only errors were line feeds and Q turning into O etc. that we could resolve using a post-processor). But for one set of images (from DC), the OCR fails miserably.  This is a corner case that seems to throw the Document AI engine off the mark.I will appreciate greatly if anyone can help explain it.See one particular image which is the most problematic.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-09T11:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you please share the output that you are receiving, and what errors are the ones that you are presenting?"
			},
			{
				"Answer_created_time": "2022-05-10T00:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Eduardo,\n\nThe error is that several characters that are on the image are not captured by OCR (whereas it does capture in case of several other images).\n\nRefer to the screenshot attached. See the right side after line \"DCLOTTERY.COM\".\u00a0 You will notice lines \"8\", \"B. 4\" etc.\n\nLine \"8\", the rest of the characters 0 1 1 4 STRAIGHT ... are missing.\n\nLine \"4 0\", characters 2 8 4 STRAIGHT ... are missing.\n\nSame for following 3 lines.\n\n----"
			},
			{
				"Answer_created_time": "2022-05-13T16:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I could find this guide that might seem helpful for your case, if not, please give me more time so I can provide you a proper answer for the issue you are facing."
			},
			{
				"Answer_created_time": "2022-05-13T22:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sorry, you missed the entire point, the issue is that the core OCR engine is failing to process the image properly. If the product team takes a look at the image and result, it may give a clue. Hopefully they may be able to find a corner case that will improve the OCR results.\n\nWe are quite familiar with the documents and how to parse the result of the DOcument AI."
			},
			{
				"Answer_created_time": "2022-06-21T07:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Anil, sorry it took me so long to answer you, couldn't find any information on why the OCR is failing for that image that you specify, so my best recommendation for you, is that you file an issue tracker or open a support ticket since this seems like an issue that you are only facing."
			},
			{
				"Answer_created_time": "2022-06-27T21:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What DocAI processor are you using?\u00a0 What, if anything, is returned from the processing?"
			},
			{
				"Answer_created_time": "2022-08-03T12:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Deskewing the image in a pre processing step is doing the trick"
			}
		]
	},
	{
		"Question_title": "How to determin which GCP VM do I need for ML",
		"Question_created_time": "2022-07-26T13:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-determin-which-GCP-VM-do-I-need-for-ML/td-p/447075/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 99,
		"Question_body": "Hi to allIm trying to run a procedure looking to reduce the number of features for a model.The first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM n1-highmem-8 that has: GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)and still not getting the process done.The question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-01T10:31:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-08-01T10:31:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs."
			},
			{
				"Answer_created_time": "2022-08-03T06:39:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I sincerely appreciate your response, you cannot imagine how important and valuable your help has been"
			}
		]
	},
	{
		"Question_title": "How to assign specialist for specialist",
		"Question_created_time": "2022-07-27T09:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-assign-specialist-for-specialist/td-p/447380/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 74,
		"Question_body": "Hello,I configured Document AI processor with HITL, No filter(self-validate). Using a python code, I am sending specific document to hitl queue to be processed by a specialist.I can clearly see that there are documents to be reviewed as \"Queued for review\" column with 2 documents.I also configured the specialist assignment assigning to all tasks (P0, audit, P1) to all the available specialists as it is shown in the image:Howevere, accesing to specialist platform, I cannot see any of the documents in the queue.What am I missing here?Thanks for your help. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-02T13:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Follow the steps from 4 to 7 from the following codelab."
			},
			{
				"Answer_created_time": "2022-08-02T14:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks"
			}
		]
	},
	{
		"Question_title": "Tenserflow model not detecting plants correctly",
		"Question_created_time": "2022-07-27T05:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Tenserflow-model-not-detecting-plants-correctly/td-p/447276/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 80,
		"Question_body": "Hi there,We are using AutoML skd with Tenserflow model (https://tfhub.dev/google/lite-model/aiy/vision/classifier/plants_V1/3) for detecting plants. The model return results, but they are not accurate. I wanted to see if there are any pre-trained TenserFlow models for detecting plant type? Similar to plant.id. Thanks  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-08-02T11:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There is a TensorFlow Model for Flowers you can see this quickstart\u00a0using this model.\n\nMore specific about the model you can see the training in the following document."
			}
		]
	},
	{
		"Question_title": "Google translate API gave a mixed language translation result?",
		"Question_created_time": "2022-07-11T00:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-translate-API-gave-a-mixed-language-translation-result/td-p/440841/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 248,
		"Question_body": "I am using Google Translate API for translating a Japanese sentence to Portuguese.On July 6 to 8, the translation result was a sentence of mixed English and Portuguese words, but on July 9 the result seems to be a correct Portuguese sentence.Was there any event on July 6 to 8 such that Google Translate API gave a mixed language translation result?Thank you for your time. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-13T15:33:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The issue was generated due our service update and multiple service languages including Portuguese were affected. Our Translate API engineer team detected the root cause of the issue, and it was mitigated by a roll back recently performed.\n\nOur engineers confirmed that the issue is officially mitigated and you should not be experiencing any service misbehavior at this point.\n\nWe apologize for any inconvenience this may have caused on your operation.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-07-13T15:33:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The issue was generated due our service update and multiple service languages including Portuguese were affected. Our Translate API engineer team detected the root cause of the issue, and it was mitigated by a roll back recently performed.\n\nOur engineers confirmed that the issue is officially mitigated and you should not be experiencing any service misbehavior at this point.\n\nWe apologize for any inconvenience this may have caused on your operation."
			},
			{
				"Answer_created_time": "2022-07-31T06:46:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Dear josequtierrez,\n\nThank you very much for your response and explanation.\n\nWe really appreciate it.\n\nThank you and best regards,\n\nIvan"
			}
		]
	},
	{
		"Question_title": "Strange behaviour of ARIMA model",
		"Question_created_time": "2022-07-22T08:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Strange-behaviour-of-ARIMA-model/td-p/445988/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 93,
		"Question_body": "Hi guys, I'm working with ARIMA Model and I found a strange behaviour.I have two dataset called Then I create two model in this waysample_10_arimasample_11_arimaThen I call the ML.FORECAST function for both in that wayResult for sample_10:Result for sample_11:In the first case sample_10_arima the standard_error is low (around 2.8) but in the sample_11_arima the standard_error is high (between 60 and 101). Why this difference occour? The time series are very similarThanks, Marcello",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-29T14:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you please file an issue\u00a0 at issue tracker according to this behavior shown?"
			}
		]
	},
	{
		"Question_title": "Vertex AI data lost on VM stop",
		"Question_created_time": "2022-07-22T08:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-data-lost-on-VM-stop/td-p/445990/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 277,
		"Question_body": "I am new to Vertex AI and wanted to try it out for a Kaggle competition. I was able to get a GPU machine up and running, as well as download the data to the machine. The download script was automatically generated when uploading my notebook to Vertex AI. I ran the script and 5 hours later all of the data was there successfully (to the boot disk -  standard persistent disk with 1000 GB). I then ran a first iteration of my model and everything worked great. When I was done, I went back to GCP and stopped my VM, assuming all of my data would be saved. It was not!I then started over and once the data was on the machine I took a snapshot so I wouldn't have to redownload the data a third time. I then made some edits to my model and ran it again. After I was done, I again stopped my VM to not leave it running. All of the data was lost again, but less surprisingly this time. I thought a snapshot could be used as a backup to the original machine, but the documentation makes it seem like it is only for creating a new VM from the boot disk. I then made a new machine but cannot figure out how to use it. I also tried looking for a way to make a new notebook on Vertex with the disk snapshot, but it did not look possible. Questions: ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-29T08:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Create a snapshot\n\ngcloud compute snapshots create SNAPSHOT_NAME \\\n\n--source-disk SOURCE_DISK \\\n\n--source-disk-zone SOURCE_DISK_ZONE\n\nssh instance and run command: sudo umount /dev/disk/by-id/google-<INSTANCE NAME>\nStop the instance\nDetach data disk\n\ngcloud compute instances detach-disk $INSTANCE_NAME --disk $DATA_DISK_NAME --zone $ZONE\n\nDelete data disk\n\ngcloud compute disks delete $DATA_DISK_NAME --zone $ZONE\n\nCreate the new disk using the snapshot created: gcloud compute disks create $DATA_DISK_NAME $DATA_DISK_SIZE --source-snapshot=$SNAPSHOT_NAME $DATA_DISK_TYPE --zone $ZONE\nAttach the disk into the notebook instances: gcloud compute instances attach-disk $INSTANCE_NAME --disk $DATA_DISK_NAME --zone $ZONE\nCreate directory that serves as the mount point sudo mkdir -p /mnt/disks/MOUNT_DIR\nMount the disk sudo mount -o discard,defaults /dev/DEVICE_NAME /mnt/disks/MOUNT_DIR\nStart the VM"
			},
			{
				"Answer_created_time": "2022-07-29T09:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thanks, that is helpful but I still do not understand why the data was deleted in the first place? It says it is a persistent disk"
			}
		]
	},
	{
		"Question_title": "VertexAI- Auto ML training model failed without giving the reason",
		"Question_created_time": "2022-07-21T00:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/VertexAI-Auto-ML-training-model-failed-without-giving-the-reason/td-p/445439/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 171,
		"Question_body": "After an hour of training Auto ML with Vertex AI, it failed without mentioning the reason. I have received the following email;\n\"Due to an error, Vertex AI was unable to train model \"some_model\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \nprojects/xxxxxxxxxxxxxxx/locations/region/trainingPipelines/xxxxxxxxxxxxxxxxxxxxxxxx\nError Messages: Internal error occurred. Please retry in a few minutes. If \nyou still experience errors, contact Vertex AI.\"Would you please help me with it?\nThanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-27T10:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There was an issue with Europe West 2 Servers during that day, does your training model was in that region?\nIs this still an issue or is it fixed now?"
			},
			{
				"Answer_created_time": "2022-07-28T06:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Not at that region and still the same error."
			},
			{
				"Answer_created_time": "2022-07-28T13:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What could be happening is due to a permission error.\n\nFix custom training permission issues.\n1. use default compute account of model preprocessing tenant projects to run training jobs\n2. Grant default compute account storage.admin role to batch prediction/prediction/training tps during provisioning"
			}
		]
	},
	{
		"Question_title": "VM Ram vs Google Colab Ram",
		"Question_created_time": "2022-07-27T12:51:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/VM-Ram-vs-Google-Colab-Ram/td-p/447466/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 88,
		"Question_body": "Hi, @Eduardo_Ortiz  @josegutierrez sorry to bother but I`m completely lostDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.VM Configuration : GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)Ram obtained in Google Colab from the VM: 1.31 Gb / 51.01 Gb Disc 43.79 / 186.52As you realized,  althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....What could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?In the next screen shot the ram and disck that I got from Google Colab:Thanks a lot for any help ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-27T12:51:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, @Eduardo_Ortiz\u00a0 @josegutierrez\u00a0sorry to bother but I`m completely lost\n\nDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.\n\nVM Configuration : GPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nRam obtained in Google Colab from the VM: 1.31 Gb / 51.01 Gb Disc 43.79 / 186.52\n\nAs you realized,\u00a0 althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....\n\nWhat could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?\n\nIn the next screen shot the ram and disck that I got from Google Colab:\n\nThanks a lot for any help"
			}
		]
	},
	{
		"Question_title": "Error while trying to get explanation from (custom container) model deployed on Vertex AI",
		"Question_created_time": "2022-07-26T01:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-while-trying-to-get-explanation-from-custom-container/td-p/446817/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 315,
		"Question_body": "Hi,I created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF/SKL/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get explainable predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.Please take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations#custom-container(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-26T01:38:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF/SKL/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get\u00a0explainable\u00a0predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\n\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\n\nEnvironment details\nGoogle Cloud Notebook\nPython version: 3.7.12\npip version: 21.3.1\ngoogle-cloud-aiplatform\u00a0version: 1.15.0\nReference\n\nhttps://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations#custom-container\n\nexplanation-metadata.json\n\n(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)\n\n{\n    \"inputs\": {\n        \"A\": {},\n        \"B\": {}\n    },\n    \"outputs\": {\n        \"Y\": {}\n    }\n}\nModel upload with explanation parameters and metadata\n! gcloud ai models upload \\\n  --region=$REGION \\\n  --display-name=$MODEL_NAME \\\n  --container-image-uri=$PRED_IMAGE_URI \\\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\n  --explanation-method=sampled-shapley \\\n  --explanation-path-count=10 \\\n  --explanation-metadata-file=explanation-metadata.json\nPrediction/Explanation Input\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\n# Prediction (works fine):\nendpoint.predict(instances=instances)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\n\n# Another example\ninstances_2 = [[1.1,20], [2.2,21]]\n# Prediction (works fine):\nendpoint.predict(instances=instances_2)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances_2) # Returns error\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\nPrediction Server (Flask)\n# Custom Flask server to serve online predictions\n# Input for prediction\nraw_input = request.get_json()\ninput = raw_input['instances']\ndf = pd.DataFrame(input, columns = ['A', 'B'])\n# Prediction from model (loaded from GCP bucket)\npredictions = model.predict(df).tolist() # [0, 1]\nresponse = jsonify({\"predictions\": predictions})\nreturn response\nStack trace of error (1)\n---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     49         try:\n---> 50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n\n/opt/conda/lib/python3.7/site-packages/grpc/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--> 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n/opt/conda/lib/python3.7/site-packages/grpc/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--> 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n/tmp/ipykernel_2590/4024017963.py in <module>\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\n\n~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\n   1563             parameters=parameters,\n   1564             deployed_model_id=deployed_model_id,\n-> 1565             timeout=timeout,\n   1566         )\n   1567 \n\n~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\n    917             retry=retry,\n    918             timeout=timeout,\n--> 919             metadata=metadata,\n    920         )\n    921 \n\n/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[\"metadata\"] = metadata\n    153 \n--> 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n---> 52             raise exceptions.from_grpc_error(exc) from exc\n     53 \n     54     return error_remapped_callable\n\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\n---------------------------------------------------------------------------\n# https://github.com/googleapis/python-aiplatform/issues/1526"
			}
		]
	},
	{
		"Question_title": "503 on translations",
		"Question_created_time": "2022-07-20T05:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/503-on-translations/td-p/445069/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 182,
		"Question_body": "I started to see this error on multiple clusters in America. But there is nothing in the status page. I don't think we had any updates to our code.google.api_core.exceptions.ServiceUnavailable: 503 POST https://translation.googleapis.com/language/translate/v2?prettyPrint=false: The service is unavailable at this time.I guess I need to wait, but posting here just to raise it",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-25T15:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Sometimes this issue happens because the Product is getting an Update.\nOthers it's because you are sending too many requests, and to mitigate this issue the recommendation is to split up the code."
			}
		]
	},
	{
		"Question_title": "Google Cloud Translation language support for bcp-47",
		"Question_created_time": "2022-07-20T20:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Translation-language-support-for-bcp-47/td-p/445359/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 124,
		"Question_body": "Google Speech to Text supports languages using bcp 47 codes like es-MX for mexican spanish and pt-BR for Brazilian Portugese.I am using transcription and translation in a pipeline.Is there any support for bcp 47 languages in Google Cloud Translation. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-25T15:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Any BCP47 input or most legacy versions language codes should just work and there is no need to convert to a particular standard.\nAdditionally you can see here the full list of particular ISO languages codes.\n\nNote that ISO 639-1 on its own isn't sufficient to differentiate between written languages; mix up zh-CH and zh-TW ."
			}
		]
	},
	{
		"Question_title": "Streaming Ingestion into Vertex AI Feature Store",
		"Question_created_time": "2022-07-12T09:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Streaming-Ingestion-into-Vertex-AI-Feature-Store/td-p/441577/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 346,
		"Question_body": "I'm just wondering if Vertex AI Feature Store supports streaming ingestions rather than just batch ingestion as seen here (https://cloud.google.com/vertex-ai/docs/featurestore/ingesting-batch). I figured that the presence of an online store (https://cloud.google.com/vertex-ai/pricing) means that there is a way to store the most up-to-date data and serve them.Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-14T12:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello, at the moment it\u2019s impossible to stream ingestion onto vertex, what you could do is to create a feature request.\n\nIf you find an issue or feature request that matches yours, star it.\n\nIf you don't see a matching issue or feature request, you can create one:\n\nIn the following tables of issue trackers, locate the product.\nClick on the link to create a new issue.\nIn the form's Template drop-down menu, select either Defect report to report an issue or Feature request to request a feature.\nIn the Description text box, complete the rest of the form using the prompts provided.\nClick Create."
			},
			{
				"Answer_created_time": "2022-07-15T03:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Eduardo,\n\nThanks for the reply! Then could I check, what features would populate the online store then? Would it just be the latest features from any batch ingestion?"
			},
			{
				"Answer_created_time": "2022-07-15T03:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Also, what is the online store based on? Is it a BigTable?"
			},
			{
				"Answer_created_time": "2022-07-21T16:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You would need to wait for the Release note since i don't really have an answer for your first question, secondly, the store is based on cloud storage if i'm not mistaken."
			}
		]
	},
	{
		"Question_title": "Retail API predict call saves the userEvent. It should NOT!",
		"Question_created_time": "2022-07-17T17:45:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Retail-API-predict-call-saves-the-userEvent-It-should-NOT/td-p/443911/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 89,
		"Question_body": "According to document, the userEvent sent as part of the predict body is not recorded.  https://cloud.google.com/retail/docs/predict#recommendHowever, I noticed this was not TRUE.  Here is how to reproduce thisBecause both \"FAKE_SESSION_ID_1\" and \"FAKE_SESSION_ID_2\" are never used before this experient.   The recommendation result for the same sku should be same or very similar.  But they diff a lot.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-20T12:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I reproduce this and it gives me similar results from what you say, so I decided to investigate this, and I found that this usually is not a great test of the prediction capabilities unless you use real, live data, but you can try this using sample data but it is possible that you get different results.\n\n\nFor using sample data to test prediction capabilities it is recommended to use the following demo."
			}
		]
	},
	{
		"Question_title": "Run Colab with Mulitple GPUs using Drive Files OR workaround",
		"Question_created_time": "2022-07-18T15:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Run-Colab-with-Mulitple-GPUs-using-Drive-Files-OR-workaround/td-p/444346/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 747,
		"Question_body": "Hi,I am trying to run my ML model in Colab utilizing a custom VM with multiple GPUs. I can successfully spin up a 2 GPU DeepLearning VM and connect to a Colab notebook via port-forwarding to a locally-hosted connection (Jupyter Notebook), as shown here.Although I can connect to custom runtimes directly WITHOUT port-forwarding to a locally-hosted connection, I can only access 1 of the 2 GPUs this way (i.e. I can successfully connect to a locally-hosted, port-forwarded runtime and verify that the notebook can access the 2 GPUs; however I am running into issues when trying to mount my Google Drive.I know that ocamlfuse was offered as a suggestion to this Drive issue,  however, none of the download options work. Specifically, it seems like a locally-hosted port-forwarded runtime doesn't allow terminal inputs, so I can't \"Press [ENTER]\" to allow the download, as shown below:User import cursor shows up for a direct connection to a custom or hosted runtime:User import cursor fails to show up/accept inputs in a locally-hosted, port-forwarded custom VM.In general, it seems like terminal commands don't work in Colab in a locally-hosted runtime.  Another option is PyDrive, which I've used in the past. However, since PyDrive relies on authentication through a local port, I can't get it to work on my locally-hosted custom VM.In short I'm looking for tips/suggestions for any of the following issues:1) An alternative workflow to run my ML model using multiple GPUs (i.e. that's not through port-forwarding to a locally-hosted connection)2) How to get that user cursor to show up (enabling me to download ocamlfuse)3) How to authenticate in PyDrive, given I'm already using a local port connection to host my runtime.4) Alternatives to accessing my Drive/Drive files.  Thank you so much!     ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-18T15:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI am trying to run my ML model in Colab utilizing a custom VM with multiple GPUs. I can successfully spin up a 2 GPU DeepLearning VM and connect to a Colab notebook via port-forwarding to a locally-hosted connection (Jupyter Notebook), as shown here.\n\nAlthough I can connect to custom runtimes directly WITHOUT port-forwarding to a locally-hosted connection, I can only access 1 of the 2 GPUs this way (i.e.\u00a0\n\nlen(tf.config.list_physical_devices('GPU')) always returns 1); hence, I'm tied to port-forwarding, unless there's an alternative option.\n\nI can successfully connect to a locally-hosted, port-forwarded runtime and verify that the notebook can access the 2 GPUs; however I am running into issues when trying to mount my Google Drive.\n\nI know that ocamlfuse was offered as a suggestion to this Drive issue,\u00a0 however, none of the download options work. Specifically, it seems like a locally-hosted port-forwarded runtime doesn't allow terminal inputs, so I can't \"Press [ENTER]\" to allow the download, as shown below:\n\nUser import cursor shows up for a direct connection to a custom or hosted runtime:\n\nUser import cursor fails to show up/accept inputs in a locally-hosted, port-forwarded custom VM.\n\nIn general, it seems like terminal commands don't work in Colab in a locally-hosted runtime.\n\n\u00a0\n\n\u00a0\n\nAnother option is\u00a0PyDrive, which I've used in the past. However, since\u00a0PyDrive\u00a0relies on authentication through a local port, I can't get it to work on my locally-hosted custom VM.\n\nIn short I'm looking for tips/suggestions for any of the following issues:\n\n1) An alternative workflow to run my ML model using multiple GPUs (i.e. that's not through port-forwarding to a locally-hosted connection)\n\n2) How to get that user cursor to show up (enabling me to download\u00a0ocamlfuse)\n\n3) How to authenticate in PyDrive, given I'm already using a local port connection to host my runtime.\n\n4) Alternatives to accessing my Drive/Drive files.\u00a0\n\n\u00a0\n\nThank you so much!"
			}
		]
	},
	{
		"Question_title": "Retail product catalog not sync with Merchant Center after initial import",
		"Question_created_time": "2022-07-17T15:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Retail-product-catalog-not-sync-with-Merchant-Center-after/td-p/443901/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 63,
		"Question_body": "I followed the documents to import product catalog from Merchant Center Sync. It seems to work at the beginning - but I recently notice it has not synced for last seven days. There is no error I can see that explains why.- There is a warnig on the Product Catalog integration page saying Last import is more than 1 week old.- If I click Import button, select merchant account and branch 0 . It says The branch already has a data source. But it is not syncing at all.I have daily new products added to merchant center so this is serious issue for me as many events become unjoined and recommendations on those new products are all messed up.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-17T15:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "And the Import Activity tab shows no activity of any Product catalog import activity. In fact I never see any entry here despite it was syncing last week."
			}
		]
	},
	{
		"Question_title": "Normalised Text from TTS",
		"Question_created_time": "2022-07-13T01:38:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Normalised-Text-from-TTS/td-p/441871/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 76,
		"Question_body": "Is there a way for the Google TTS service to return the normalised text along with the generated speech file? For example, if \"This is 1993\" is sent to the service, can it return the verbalised for \"This is nineteen ninety three\"?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-15T12:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes, you can verbalize the year like ordinal numbers or cardinal numbers. You can see this example:\n\nI can speak in cardinals. Your number is <say-as interpret-as=\"cardinal\">10</say-as>.\n\nOr I can speak in ordinals. You are <say-as interpret-as=\"ordinal\">10</say-as> in line.\n\n\u00a0\n\nHere is the official documentation about how Text-to-Speech synthesizes the text:"
			}
		]
	},
	{
		"Question_title": "About the extended model \"phone_call\" of Speech-to-Text",
		"Question_created_time": "2022-07-11T18:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/About-the-extended-model-quot-phone-call-quot-of-Speech-to-Text/td-p/441346/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 75,
		"Question_body": "I am using Speech-to-Text from a server application developed with Node.js using gRPC.When using the extended model phone_call in Japanese and recognizing it for a long time,\nThe voice recognition result of the intermediate result may be rewound, or if you think that it does not return for about 1 to 2 seconds, it may return at once in one sentence.I've incorporated the Speech-to-Text API into my C # app before, and I used the extended model phone_call as well, but I didn't see anything like this in my C # app.\nDo you know what is causing it?The API setting value (Recognition Config) is the same for both Node.js and C #.Thank you.(I am sorry if it is rude or inappropriate to you, since I use the online translation site.)",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-15T12:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can see this document about Speech-To-Text troubleshooting. It has information about unexpected results from speech recognition, and this document is about best practices."
			}
		]
	},
	{
		"Question_title": "Vertex AI explain with a custom trained scikit-learn classification model",
		"Question_created_time": "2022-06-30T09:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-explain-with-a-custom-trained-scikit-learn/td-p/436711/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 416,
		"Question_body": "Hi Google Community,I was wondering, has anyone been able to successfully train and deploy a custom trained scikit-learn classification model and deploy it to a vertex endpoint with the feature attribution through the explain endpoint working?Every time i define my instances, predictions and explanation_spec while uploading my model, i get errors on the endpoint for the :explain method. Specifically, i get '400 bad request' with no information on why it was a bad request.I am using the v1beta1 ai platform python SDK and also am using a custom basic serving container. The custom container works for :predict but :explain does not work. Is there some example code out there? Is scikit-learn not supported for feature attribution? Thanks! Ryan",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-14T16:51:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "For anyone looking back on this, i was able to use the following notebook to solve my problem. It seems we need to use encoding BAG_OF_FEATURES. I am not to sure why this is required, but it seems to have done the trick for me.\n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage4...\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-07-06T13:32:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The easiest (and recommended) way to create a training application package uses gcloud to package and upload the application when you submit your training job.\n\nHere\u00a0you can see documentation that will guide you through all of the steps that you need to follow to implement your scikit trained model."
			},
			{
				"Answer_created_time": "2022-07-14T16:51:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "For anyone looking back on this, i was able to use the following notebook to solve my problem. It seems we need to use encoding BAG_OF_FEATURES. I am not to sure why this is required, but it seems to have done the trick for me.\n\nhttps://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage4..."
			}
		]
	},
	{
		"Question_title": "[Vertex AI] Bug - Failed to download file",
		"Question_created_time": "2022-07-07T07:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Bug-Failed-to-download-file/td-p/439222/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 339,
		"Question_body": "Vertex AI recently fails to download any file greater than 30M. Any downloaded file will be trimmed at 30M. The download speed is also way slower recently (200k/s). It was working a few days ago. (downloads files of 100+M at 5M/s) Any ideas?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-14T07:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can see this documentation about troubleshooting with vertex, it mentions working with files that are truncated or do not complete downloading and possible solutions."
			}
		]
	},
	{
		"Question_title": "A100 ram limitations",
		"Question_created_time": "2022-07-07T10:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/A100-ram-limitations/td-p/439296/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 94,
		"Question_body": "Can someone please help me understand why the best GPUs google offer (A100) have a fixed CPU RAM of 85GB (only 2x that of the GRAM) and all the other poorer GPU options can go over 300GB. It's terribly frustrating for large dataset training pipelines. Especially when mmdetection libraries don't work well on mutiple GPUs and would rather just use the 1",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-13T15:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The NVIDIA A100 GPU must use the a2 Machine series, the lower CPU that a `a2 Machine` is 85GB and the highest a2 machine memory is 1360GB.\n\nAlso, You can attach up to 257 TB of local storage to these machine types in this series for applications that require higher storage performance.\n\nYou can also use Optional local SSD support: you can get up to 3 TB of Local SSD with `a2 machine` types. This can be used as fast scratch disks or for feeding data into the A100 GPUs while preventing I/O bottlenecks.\n\nYou can see further details about the `a2 Machine` series here."
			}
		]
	},
	{
		"Question_title": "Using Vison ML via REST API",
		"Question_created_time": "2022-07-06T02:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Using-Vison-ML-via-REST-API/td-p/438619/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 210,
		"Question_body": "I'm taking my first steps with Vision ML and using the REST interface (https://vision.googleapis.com/v1/files:annotate). As API key I provide the key from the Firebase project settings. In the Authorization Bearer, I supply the token from Firebase-Auth after sign-in.When accessing Annotate I get a 403 (Permission_Denied) error message back:\nError opening file: gs://######.appspot.com/MyFile.tiff.The object is available in the corresponding bucket and it is not blocked due to the Firebase Storage rules.Can I pass a Firebase token in this REST interface at all?How do I make sure that the service account can access the storage?\n\nThank you for any hint\n\nAuthor of FB4D GitHub Project (A Delphi Library for access Firebase Services via REST).",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-11T10:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The issue seems to be with the permissions or the Firebase rules that are being used. Could you please share the permissions and Firebase rules that you are using?\u00a0\u00a0\u00a0\n\nHere is document that you can use for the Firebase security rules:\n\nhttps://firebase.google.com/docs/storage/security\u00a0\n\nhttps://stackoverflow.com/questions/38671444/user-does-not-have-permission-to-access-this-object-fir...\n\nHere is another document that you can use for the IAM permissions:\n\nhttps://cloud.google.com/storage/docs/access-control/iam-permissions\u00a0\n\nYou can also use a service account to authenticate to Firebase storage:\n\nhttps://stackoverflow.com/questions/72565059/cloud-api-product-search-asked-for-storing-the-images-i..."
			},
			{
				"Answer_created_time": "2022-07-12T01:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "That was also my first thought, so I have the rule that only a known user can use the storage, removed for all types of readers released.\n\nrules_version = '2';\nservice firebase.storage {\n  match /b/{bucket}/o {\n    match /testML/{img} {\n      allow read: if true; // if request.auth != null;\n      allow write: if request.auth != null;\n    }\n  }\n}\n\nI suppose this rules out your first assumption.\n\nOn the second point, yes, I've also tried giving more privileges to the executing service account. In the Google Cloud Console, I see three other principals in addition to my email address as the owner. Which one is used by the ML vision Service?\n\n<ProjectID>@appspot.gserviceaccount.com\nfirebase-adminsdk-zog2s@<ProjectID>.iam.gserviceaccount.com\nfirebase-service-account@firebase-sa-management.iam.gserviceaccount.com\n\nI have already assigned \"Storage admin\" rights to all these 3 accounts. Unfortunately, this did not solve the problem.\n\nThank you for a further hint."
			}
		]
	},
	{
		"Question_title": "Can i show alias instead of voice name?",
		"Question_created_time": "2022-06-22T23:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-i-show-alias-instead-of-voice-name/td-p/434016/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 139,
		"Question_body": "[ ko-KR-Wavenet-A ] This name is so awkward for me.Can i show alias instead of that voice name?like this. [ ko-KR-Wavenet-A ] -> [ Jinsung ]------------------------------------------------------I'm developing a web service that can edit videos on the web.I will provide Google TTS on that web service.I show the Google (source of the voice) on the side, just wanna alias.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-07-01T13:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The issue you are facing is happening since somewhere within your code something is calling the API in that form and not on the one you are trying to do so, also google cloud community isn\u2019t the best place to make questions about coding, my recommendation for you would be that you post your question on StackOverflow."
			}
		]
	},
	{
		"Question_title": "Memory issue",
		"Question_created_time": "2022-06-21T02:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Memory-issue/td-p/433242/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 477,
		"Question_body": "Hi friends, Iam facing this error recently - The replica workerpool0-0 ran out-of-memory and exited with a non-zero status of 137(SIGKILL). Kindly help me , i am using 800GB , still getting this error   ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-29T07:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This error normally indicates an issue with the code rather than being an Out Of Memory Exception in the service side.\n\nIs there any other error that you can share that is showing in the VM logs?"
			}
		]
	},
	{
		"Question_title": "Error uploading csv file to Vertex DataSets",
		"Question_created_time": "2022-06-17T09:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Error-uploading-csv-file-to-Vertex-DataSets/td-p/432499/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 198,
		"Question_body": "Hi to allTrying to upload a .csv file to AutoMl for training.Not sure what Im doing wrong, I save the file as csv encode utf 8 and values separated by comma and with both cases getting the error that you will find in the next image.Do I need to upload the files to Cloud Storage or Google BigQuery before using them for training? When trying to create and train the model got the warning from the next image:",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-27T14:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This could be occurring because you have formatting errors in your dataset, could you trim newlines and extra white spaces\u00a0on your dataset, remember that Vertex wants the text inside of your CSV to look like the examples I'm sharing.\n\n\"this is a sentence, with a comma\", 0\n\nAlso which dataframe are you using? Are you for any chance using Pandas?"
			}
		]
	},
	{
		"Question_title": "Google Translate API - Laotian Translation Failures",
		"Question_created_time": "2022-06-13T12:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translate-API-Laotian-Translation-Failures/td-p/431021/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 125,
		"Question_body": "Description:    Inquiries: Code Example:Resulting Translation:",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-22T12:54:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you please raise an issue tracker, since I couldn\u2019t find any bugs regarding the outage you are presenting, it might be due to something you are doing, or could you please share your code.\n\nAlso note that the community maintains google translate."
			},
			{
				"Answer_created_time": "2022-06-23T13:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I have just updated this with a code example and the resulting translations and bugs. Additionally, I have create an issue tracker here.\u00a0\n\nPlease let me know what else we can provide to help get this resolved, thanks!"
			}
		]
	},
	{
		"Question_title": "Google Vision release notes",
		"Question_created_time": "2022-06-14T00:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Vision-release-notes/td-p/431130/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 107,
		"Question_body": "From here https://cloud.google.com/vision/docs/release-notes it says that there was an upgrade on OCR model for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION. What is the recent model improvement compare to legacy model (is there some metrics used)?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-21T15:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As part of the best effort, can I make a suggestion for you on modifying the model field of Feature object into either \"builtin/legacy\" or \"builtin/latest\". Though the OCR model will automatically be upgraded to the new model when there's an update on \"builtin/stable\" (default). This means that google implemented the newest versions of TEXT_DETECTION and DOCUMENT_TEXT_DETECTION to work without a problem for future updates"
			}
		]
	},
	{
		"Question_title": "AI scientist",
		"Question_created_time": "2022-06-19T15:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AI-scientist/td-p/432867/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 51,
		"Question_body": "I'm from Ukraine. Therefore, I write with the help of a translator. I immediately apologize for any mistakes.I am a doctor. I am interested in many areas of science that are related to medicine. But because of their volume and complexity, it is impossible to learn by one person.I propose to create an AI that will analyze information on the Internet (video lectures, articles, books, audio books, images ...) and find relationships. For example, the electrophysical properties of DNA are analyzed through all known theories of physics. And a concrete example: Academician P. Garyaev's \"Linguistic Wave Genome\" through V. Atsyukovsky's \"Ether Theory\".This tool needs to be made multifunctional and accessible to all users. This will revolutionize science by combining all knowledge.\nIt is important that there is a convenient voice interface and a personal account where studies are saved.Thank you for attention. Sincerely, Sukhachov Denis.\u042f \u0441 \u0423\u043a\u0440\u0430\u0438\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043f\u0438\u0448\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0447\u0438\u043a\u0430. \u0421\u0440\u0430\u0437\u0443 \u0438\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.\u042f \u0434\u043e\u043a\u0442\u043e\u0440. \u041c\u0435\u043d\u044f \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u043c\u043d\u043e\u0433\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0443\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u043e\u0439. \u041d\u043e \u0438\u0437-\u0437\u0430 \u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u0430 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438\u0445 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0432\u043e\u0438\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0443.\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0418\u0418, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 (\u0432\u0438\u0434\u0435\u043e\u043b\u0435\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043d\u0438\u0433\u0438, \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f...) \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0414\u041d\u041a \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u043e\u0440\u0438\u0439. \u0418 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u00ab\u041b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u043e\u043b\u043d\u043e\u0432\u043e\u0439 \u0433\u0435\u043d\u043e\u043c\u00bb \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u043a\u0430 \u041f. \u0413\u0430\u0440\u044f\u0435\u0432\u0430 \u0447\u0435\u0440\u0435\u0437 \u00ab\u0422\u0435\u043e\u0440\u0438\u044e \u044d\u0444\u0438\u0440\u0430\u00bb \u0412. \u0410\u0446\u044e\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e.\u042d\u0442\u043e\u0442 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u042d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u0442 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0432 \u043d\u0430\u0443\u043a\u0435, \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0435 \u0437\u043d\u0430\u043d\u0438\u044f.\n\u0412\u0430\u0436\u043d\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0438 \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430, \u0433\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0437\u0430\u043d\u044f\u0442\u0438\u044f.\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435. \u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0421\u0443\u0445\u0430\u0447\u0435\u0432 \u0414\u0435\u043d\u0438\u0441.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-19T15:22:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm from Ukraine. Therefore, I write with the help of a translator. I immediately apologize for any mistakes.\n\nI am a doctor. I am interested in many areas of science that are related to medicine. But because of their volume and complexity, it is impossible to learn by one person.\n\nI propose to create an AI that will analyze information on the Internet (video lectures, articles, books, audio books, images ...) and find relationships. For example, the electrophysical properties of DNA are analyzed through all known theories of physics. And a concrete example: Academician P. Garyaev's \"Linguistic Wave Genome\" through V. Atsyukovsky's \"Ether Theory\".\n\nThis tool needs to be made multifunctional and accessible to all users. This will revolutionize science by combining all knowledge.\nIt is important that there is a convenient voice interface and a personal account where studies are saved.\n\nThank you for attention. Sincerely, Sukhachov Denis.\n\n\u042f \u0441 \u0423\u043a\u0440\u0430\u0438\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043f\u0438\u0448\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0447\u0438\u043a\u0430. \u0421\u0440\u0430\u0437\u0443 \u0438\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.\n\n\u042f \u0434\u043e\u043a\u0442\u043e\u0440. \u041c\u0435\u043d\u044f \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u043c\u043d\u043e\u0433\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0443\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u043e\u0439. \u041d\u043e \u0438\u0437-\u0437\u0430 \u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u0430 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438\u0445 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0432\u043e\u0438\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0443.\n\n\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0418\u0418, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 (\u0432\u0438\u0434\u0435\u043e\u043b\u0435\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043d\u0438\u0433\u0438, \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f...) \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0414\u041d\u041a \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u043e\u0440\u0438\u0439. \u0418 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u00ab\u041b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u043e\u043b\u043d\u043e\u0432\u043e\u0439 \u0433\u0435\u043d\u043e\u043c\u00bb \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u043a\u0430 \u041f. \u0413\u0430\u0440\u044f\u0435\u0432\u0430 \u0447\u0435\u0440\u0435\u0437 \u00ab\u0422\u0435\u043e\u0440\u0438\u044e \u044d\u0444\u0438\u0440\u0430\u00bb \u0412. \u0410\u0446\u044e\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e.\n\n\u042d\u0442\u043e\u0442 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u042d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u0442 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0432 \u043d\u0430\u0443\u043a\u0435, \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0435 \u0437\u043d\u0430\u043d\u0438\u044f.\n\u0412\u0430\u0436\u043d\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0438 \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430, \u0433\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0437\u0430\u043d\u044f\u0442\u0438\u044f.\n\n\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435. \u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0421\u0443\u0445\u0430\u0447\u0435\u0432 \u0414\u0435\u043d\u0438\u0441."
			}
		]
	},
	{
		"Question_title": "BigQuery ARIMA Model - What changed?",
		"Question_created_time": "2022-05-09T12:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/BigQuery-ARIMA-Model-What-changed/td-p/421507/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 7,
		"Question_score_count": 0,
		"Question_view_count": 125,
		"Question_body": "I have published an open source project using the ARIMA model available in Big Query to predict the price of BTC using a free API.The results are displayed in a Datastudio dashboard.Something changed on the 28th of March of 2022. I suggest selecting the date range: from 2022-03-26 to 2022-03-31.I failed to find the answer in any of Google's documentation.Does anyone know what changed?Regards,",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-12T13:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello, I searched for documentation that I could share for your case but I couldn't find anything as you mentioned. What you could do is raise an issue tracker and wait for them to tell you an in-depth reason why it isn\u2019t working."
			},
			{
				"Answer_created_time": "2022-05-13T06:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi @Eduardo_Ortiz\u00a0,\n\nThank you for your answer. How can I raise an issue tracker?\n\nThank you!"
			},
			{
				"Answer_created_time": "2022-05-13T07:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello\u00a0@CaueBertolino\u00a0\n\nNo problem, you can raise an issue tracher in the next link\n\nhttps://issuetracker.google.com/"
			},
			{
				"Answer_created_time": "2022-05-16T09:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you,\u00a0@Eduardo_Ortiz.\n\nI have raised an issue tracker, I will post the solution here once I have an answer."
			},
			{
				"Answer_created_time": "2022-06-15T09:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Caue, was your issue tracker resolved?"
			},
			{
				"Answer_created_time": "2022-06-17T03:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Eduardo_Ortiz,\u00a0thanks for following up.\n\nNo answer yet on the issue tracker. Please, find the link for the issue created here:\u00a0https://issuetracker.google.com/issues/232742210\n\nThanks,"
			},
			{
				"Answer_created_time": "2022-06-17T06:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "No problem\u00a0@CaueBertolino\n\nThank you for providing the issuetracker link, i'll keep an eye on it."
			}
		]
	},
	{
		"Question_title": "Node hours vs actual time",
		"Question_created_time": "2022-06-16T01:10:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Node-hours-vs-actual-time/td-p/431897/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 294,
		"Question_body": "What is meant by node hours in VertexAI?I set the budget in VertexAI AUtoML to a 1 node hour but my model has been training for 1 hr and 30+ minutes. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-16T01:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What is meant by node hours in VertexAI?\n\nI set the budget in VertexAI AUtoML to a 1 node hour but my model has been training for 1 hr and 30+ minutes."
			}
		]
	},
	{
		"Question_title": "NLP Classification Categories",
		"Question_created_time": "2022-06-07T11:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/NLP-Classification-Categories/td-p/429420/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 121,
		"Question_body": "When using the NLP API and in particular the documents.classifyText, it will obviously be classified under one of the categories listed here. My question is, do we know what was used to create these categories? Were they created from different datasets/corpora like Wikipedia, Gigaword, and Freebase? Does the Word2Vec term embedding relate to category embeddings at all? Any information, references or resources would be greatly appreciated.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-13T12:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "By using NLP, text classification can automatically analyze text and then assign a set of predefined tags or categories based on its context. NLP is used for sentiment analysis, topic detection, and language detection. There are mainly three text classification approaches.\n\n\u00a0\n\nRule-based System,\nMachine System\nHybrid System."
			}
		]
	},
	{
		"Question_title": "Authentication for the Document AI",
		"Question_created_time": "2022-06-03T10:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Authentication-for-the-Document-AI/td-p/428530/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 172,
		"Question_body": "First off, please be kind, as I'm not a developer and may struggle with some basics concepts.\n\nI'm trying to build a AI Invoice reader to collect invoice data in a spreadsheet, using Integromat / make.com (no-code platform) and Google Cloud Services.\nUsually, there are integrations for what I need in Integromat or I use simple REST calls. \n\nWith the Document AI, afaik I have to use OAuth. I have my \"processor\" and the I've been searching the Google Cloud documentation for a while, but for a non-dev it's quite confusing. Where can I find the two URLs needed?\n\nThank you very much for your help! \ud83d\ude03",
		"Answer_list": [
			{
				"Answer_created_time": "2022-06-10T14:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can see this post[1] you can see here how they get the environment endpoint that I think that what you mean by Authorize URI also here is a step on how to get the Access Token (Token URI) additionally the step to get the access token is here in the documentation[2] you have to use the Cloud SDK and use the following command:\n\n\u00a0 \u00a0 - gcloud auth application-default print-access-token\n\n[1]https://clincher.medium.com/create-a-document-ai-service-rest-api-processor-in-google-cloud-2710f583...\u00a0\n\n[2]https://cloud.google.com/document-ai/docs/setup#auth-test"
			}
		]
	},
	{
		"Question_title": "cloud vision API",
		"Question_created_time": "2022-05-23T08:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/cloud-vision-API/td-p/425445/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 170,
		"Question_body": "Hello everyone,\nMy question is really hypothetical.  I am right now using Cloud vision api with feature web detection. Desc: Detect topical entities such as news, events, or celebrities within the image, and find similar images on the web using the power of Google Image Search.Does profile login effects the results that web detection will return. For example if I type in google search python, I will get python programming language, while my mum will snake. Does the same logic works for cloud vision API web detection ? \n\nWhy this question even raised, because same image executed from 2 different profiles ( I have 2 accounts on cloud Vision API) the model return different probabilities. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-30T10:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yes, it is indeed an intended behavior getting different results even if you use the same image.\n\nThe API returns matching results with their respective scores. The scenario you are experiencing would entail that the scores are relatively close to one another and the API cannot properly select which is a better representation of the image being provided. For more accurate results, we would advise using product reference images with bounding poly coordinates[1].\n\n[1]https://cloud.google.com/vision/product-search/docs/tutorial#5_create_a_products_reference_image\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-05-30T10:25:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Yes, it is indeed an intended behavior getting different results even if you use the same image.\n\nThe API returns matching results with their respective scores. The scenario you are experiencing would entail that the scores are relatively close to one another and the API cannot properly select which is a better representation of the image being provided. For more accurate results, we would advise using product reference images with bounding poly coordinates[1].\n\n[1]https://cloud.google.com/vision/product-search/docs/tutorial#5_create_a_products_reference_image"
			},
			{
				"Answer_created_time": "2022-05-30T23:47:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Grateful for your answer Jose !"
			}
		]
	},
	{
		"Question_title": "speech-to-text demo",
		"Question_created_time": "2021-11-25T10:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/speech-to-text-demo/td-p/176523/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 336,
		"Question_body": " https://cloud.google.com/speech-to-text\n\nspeech to text demo doesn't work for a while now ! annoying .. I'd like to play with it. Can you fix, please.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-25T15:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for reporting this. I forwarded your issue to the product team and you can follow this thread\u00a0as all\u00a0further updates should occur there."
			},
			{
				"Answer_created_time": "2022-05-26T06:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Google Cloud Text-to-Speech is not working again.\n\nthe language tab is not clickable"
			}
		]
	},
	{
		"Question_title": "Text-to-Speech",
		"Question_created_time": "2022-05-25T02:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Text-to-Speech/td-p/425961/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 68,
		"Question_body": "Hi every one, Google Text-to-Speech seems not to be working again",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-26T06:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "That's right.\n\nThe language tab is not clickable"
			}
		]
	},
	{
		"Question_title": "The new languages are missing",
		"Question_created_time": "2022-05-17T01:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/The-new-languages-are-missing/td-p/423648/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 122,
		"Question_body": "Google cloud translation have added new languages. About 24 new languages has been added to Google Translate. Very good job, well done. But they are not listed on this link.\nhttps://cloud.google.com/translate/docs/languages\n\nI tried to access it using basic v2 API code, but no response came to my translation request. When will this new languages be available to be accessed by v2 APIs? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-18T16:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https://blog.google/products/translate/24-new-languages/\u00a0\n\n[2]https://arxiv.org/pdf/2205.03983.pdf\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-05-18T16:00:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https://blog.google/products/translate/24-new-languages/\u00a0\n\n[2]https://arxiv.org/pdf/2205.03983.pdf"
			},
			{
				"Answer_created_time": "2022-05-22T09:33:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for the update. I hope they do it soon. That would be good."
			}
		]
	},
	{
		"Question_title": "Imbalance DataSet for Tabular AutoML",
		"Question_created_time": "2022-04-18T10:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Imbalance-DataSet-for-Tabular-AutoML/td-p/414630/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 328,
		"Question_body": "Hi, I would like to know if in case of having a tabular database,  with binary data (class 0 and Class 1), that has an imbalance between class 0 and class 1, as it occurs in scenarios of fraud in financial transactions.Does AutoML solves automatically the imbalance situation? Or is it possible to add SMOTE or ADASYN to the AutoML model?  Any comments to advice more than appreciated",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-21T09:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There are several ways of handling imbalanced datasets:\n\nUpsampling and/or Downsampling: In case of Upsampling, instances from the minority classes are duplicated in the training dataset at random. In case of Downsampling, certain instances of the majority classes are randomly left out of the training dataset. Upsampling of minority class and downsampling of the majority class can be done at the same time.\n\n\nUpweighting and/or Downweighting: In Upweighting, sample weight greater than 1 is given to instances from the minority classes. In case of Downweighting, sample weight less than 1 is given to instances from the majority classes. The sample weights are taken into account when computing the loss function. Upweighting and Downweighting can be used together.\n\n\nData Augmentation: In this approach, data augmentation techniques are used to generate synthetic instances of the minority class to better balance the training dataset."
			},
			{
				"Answer_created_time": "2022-05-02T06:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Jos\u00e9 hi, thanks for your answer but is not very clear.....\n\nThe question is if I can upload a data set with imbalance situation to AutoML or I need to fix somehow the situation before uploading the data into AutoML or AutoML can handle in very good way Imbalance data sets?"
			},
			{
				"Answer_created_time": "2022-05-20T04:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am also interested in the same question."
			}
		]
	},
	{
		"Question_title": "When will Hebrew language be available in Text-To-Speech API?",
		"Question_created_time": "2022-05-18T06:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/When-will-Hebrew-language-be-available-in-Text-To-Speech-API/td-p/424088/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 165,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-19T08:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There is no information about when Hebrew will be available in Cloud Text-to-Speech you can file a feature request in Issue Tracker[1].\n\n[1] https://issuetracker.google.com/issues/new?component=451645&template=1161363"
			}
		]
	},
	{
		"Question_title": "Feature Store Calculations",
		"Question_created_time": "2022-05-12T13:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Feature-Store-Calculations/td-p/422580/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 98,
		"Question_body": "Hi,I have a Google Colab notebook with some functions (Python) that been used to calculate the features for a model.The functions use as inputs data from an API.The question is if I can or should calculate the features inside a Features Store and feed the results to the Model?Or in which Instance do I need to make the calculations and then feed the results into the model?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-17T11:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It is possible to use Vertex AI Features Store to Fetch the data, so you can use it as a part of the Vertex AI Workflow to train Custom or AutoML models in Vertex.\n\nYou can see here[1] the Vertex AI workflow.\n\n[1]https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide#workflow"
			},
			{
				"Answer_created_time": "2022-05-17T13:04:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thanks for your comments\n\nAfter reading the documents, I understand that the data that should be\nstored in the Feature Store is \"static data\".\n\nBy static I mean data from previously loaded databases and not calculated\nwithin the Feature Store.\n\nFor example I wanted to add a simple average of the last 30 data entries\nobtained from an API that sends data in real time in 5 minute intervals I\nshould::\n\nConnect the API to the feature Store, store each data entry from the API\nand then calculate the average inside the Feature Store?\n\nOr should I connect the API to Goolge BiGQuery, store the data in Google\nBigQuery, calculate the average and then send the data to the model deploy\nin the end point?\n\nOr connect a google colab notebook to the API, perform the calculations,\nupload the Notebook to a container and send the data to the endpoint in\nwhich the model was deployed?\n\nAnce again thanks for your help"
			}
		]
	},
	{
		"Question_title": "Vision AI labels",
		"Question_created_time": "2022-05-12T13:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vision-AI-labels/td-p/422564/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 67,
		"Question_body": "Where can I find list of all labels what could be detected in Vision AI ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-17T09:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You can read this documentation\u00a0about labels with Vision AI."
			}
		]
	},
	{
		"Question_title": "AutoML Features",
		"Question_created_time": "2022-05-12T12:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Features/td-p/422551/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 52,
		"Question_body": "HiAssume that I create a model using AutoML with 50 features from the Vertex AI Feature Store and after training I found that from the 50 original features, 10 has a very low incidence over the model.Looking to increase the accuracy, reduce the consumption of resources and increase the speed of the model:Do I need to remove the 10 features from the Feature Store and deploy the model to the endpoint?Should I retrain the model with the 40 features and deploy it to the end point?Any comments more than appreciated",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-17T08:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I wouldn't delete the features because there are some features that can be used to share, discover, and re-use ML features at scale, which can increase the velocity of developing and deploying new ML applications."
			}
		]
	},
	{
		"Question_title": "Trying to do multiple voice files with speech-to-text",
		"Question_created_time": "2022-05-11T18:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Trying-to-do-multiple-voice-files-with-speech-to-text/td-p/422295/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 135,
		"Question_body": "Hello.I'm someone who's trying to make speech-to-text work without being a coder in any way whatsoever. I have let's say hundreds of individual audio files and they go from 30 seconds to a minute and a half. The problem is that uploading them to the bucket makes it so there's hundreds of individual ones. And I need to create a transcriptions individually. what do I do? can I not just transcribe everything in one folder?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-13T22:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It requires coding - you load files in a bucket (say in 'input' folder) and run a background job to produce a \"txt\" file for each using speech-to-text API (say in 'output' folder).\u00a0 If you have files formats such as mp4 then use transcoding. This is the step roughly."
			},
			{
				"Answer_created_time": "2022-05-15T21:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Understood. But do you have any idea how I'd code that? Of course I am\u00a0 unfamiliar with how to code... but anyway no problem at all if you cannot help there."
			},
			{
				"Answer_created_time": "2022-05-15T22:10:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Coding it will not be easy if you are not a programmer. It requires a bunch of technology and tools. Roughly steps would be:\n\n1. Using gsutil tool of the GCP, upload files to a bucket.\n\n2. Write a program to read file from the bucket and invoke\u00a0 Speech-to-Text API. It will require you to acquire an access-token (OAuth2).\u00a0\n\n3. If files are small then you could do 2 without uploading files to the bucket.\u00a0\n\nYou can find the example programs here:\u00a0https://cloud.google.com/speech-to-text/docs/samples"
			}
		]
	},
	{
		"Question_title": "Total Novice",
		"Question_created_time": "2022-05-10T19:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Total-Novice/td-p/421957/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 59,
		"Question_body": "Friends,Can I submit a file for conversion from speech to text without having to learn computer coding - even if it is at a very simple level? Can I just submit the file somewhere for transcription?Thank You,Just, simply, a consumer",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-11T18:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes you can and I would also appreciate help.\n\nMake a bucket, go through the settings and be sure it's a private bucket.\n\nSearch up text to speech in the main screen. It's probably one audio channel and for hz you can either open the audio file with alt + enter to see if it shows you what the Hz is or use audacity audio software and load the file into audacity.\n\nAs someone who doesn't understand anything about code, my problem is that my audio files are and should all be contained individually, each in one file."
			},
			{
				"Answer_created_time": "2022-05-14T07:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi ,\n\nYes you can do that. Just upload your audio file\u00a0 , click button and download your transcript audio file.\n\nhttps://cloud.google.com/speech-to-text"
			}
		]
	},
	{
		"Question_title": "Form Parsing in Document AI",
		"Question_created_time": "2022-05-05T02:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Form-Parsing-in-Document-AI/td-p/420076/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 269,
		"Question_body": "Hi All,We are currently using Document AI for form parsing scanned documents and we are now required to capture the checkboxes data from the form.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-06T16:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I think that you would find this laboratory that google offers helpful since it explain step by step how form parsing works within google Document AI."
			},
			{
				"Answer_created_time": "2022-05-10T00:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for your response. I have gone through the lab course but still couldn't find answers on the checkbox count limitation of the Form-parsing using Document AI.\nIs that limitation due to Pricing? Can you please help me with this?"
			},
			{
				"Answer_created_time": "2022-05-13T15:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I found this guide that you might found useful, i couldn't find any information about limitations, so i would say that it shouldn't limit you, or maybe the documentation wasn't processed succsesfully."
			}
		]
	},
	{
		"Question_title": "Why is sample rate optional only for FLAC or WAV file and not other formats?",
		"Question_created_time": "2022-04-21T05:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Why-is-sample-rate-optional-only-for-FLAC-or-WAV-file-and-not/td-p/415704/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 0,
		"Question_view_count": 138,
		"Question_body": "So for example at my work we are using WEBM_OPUS encoding, which from what I understand, specificies the sample rate in audio stream metadata itself? Yet from here: https://cloud.google.com/speech-to-text/docs/basics#sample-rates it says the field is only optional  for FLAC or WAV formats.And indeed, when I try the GSTT API with some example code (Streaming Recognition and a WEBM_OPUS encoded at 48000 sample rate), the GSTT actually accepts sample rates other than 48000 - and depending on the recognition model, produces different results depending on the sample rate selected!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-06T13:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Did you try to put 8000 hz within your data that you are sending to speech-to-text? Answering your question those two files are optionals since they are the most commonly used."
			},
			{
				"Answer_created_time": "2022-05-09T05:59:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "What \"two files\"? ..."
			},
			{
				"Answer_created_time": "2022-05-09T06:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I was refering to WAV and FLAC audio files"
			},
			{
				"Answer_created_time": "2022-05-10T04:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes, it's optional for WAV and FLAC - but my question was about using WEBM_OPUS format..."
			},
			{
				"Answer_created_time": "2022-05-13T12:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's because google uses auto defined sample rates that are the next ones.\u00a0Sample rate must be one of 8000 Hz, 12000 Hz, 16000 Hz, 24000 Hz, or 48000 Hz, also you can see this documentation here."
			}
		]
	},
	{
		"Question_title": "Why does Jupyter Notebooks not recognize changes in my .py files?",
		"Question_created_time": "2022-05-09T11:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Why-does-Jupyter-Notebooks-not-recognize-changes-in-my-py-files/td-p/421489/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 26,
		"Question_body": "Hello, When working locally, I usually put routine tasks inside functions held in a .py file and import those.  When I need to make a change, I change the function, reimport and moving on with the main script.   GCP's jupyter instance does not recognize when I make the change an re-import the function.  I have to restart the kernel each time.  Is there a way around this?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-09T11:24:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\u00a0\n\nWhen working locally, I usually put routine tasks inside functions held in a .py file and import those.\u00a0 When I need to make a change, I change the function, reimport and moving on with the main script.\u00a0\u00a0\n\n\u00a0\n\nGCP's jupyter instance does not recognize when I make the change an re-import the function.\u00a0 I have to restart the kernel each time.\u00a0 Is there a way around this?"
			}
		]
	},
	{
		"Question_title": "Steps to get Real life data into the features section",
		"Question_created_time": "2022-04-29T15:39:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Steps-to-get-Real-life-data-into-the-features-section/td-p/418695/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 55,
		"Question_body": "Hi, I already create a tabular classification model using AutoML. All ready have the features created in a Goolge colab and now I need to get the real life data from a Public API to feed the features, then pass the features into the Model and finally get the classifications.The question is are the steps and which tools should I use in order to connect to the API in order to receive the real time data? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-09T11:05:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I think you are looking for this documentation that can help you understand how you can create datasets and import the data into AutoML. The documentation will provide you with the steps you need to follow."
			}
		]
	},
	{
		"Question_title": "Create an instance of TextToSpeechClient() and ApplicationDefaultCredentials ...",
		"Question_created_time": "2022-05-01T16:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Create-an-instance-of-TextToSpeechClient-and/td-p/418964/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 382,
		"Question_body": "Hi Folks,My first post here. This was posted on stackoverflow without much feedback - it is a little specific to the TextToSpeechClient and using ApplicationDefaultCredentials.  The link to the stackoverflow article is below just for reference.https://stackoverflow.com/questions/72074724/trying-to-create-an-instance-of-googles-class-texttospe...I'm attempting to Create an instance of TextToSpeechClient() and an getting an exception - Could not construct ApplicationDefaultCredentials. I was able to get the php sample code provided on your github site running from the command line. I'm now executing in a browser session on an apache server. I have added the putenv() function to set the GOOGLE_APPLICATION_CREDENTIALS value.Below is the code sample <?php\nheader(\"Content-Type: application/json; charset=UTF-8\");\nheader(\"Access-Control-Allow-Methods: POST\");\nheader(\"Access-Control-Max-Age: 3600\");\nheader(\"Access-Control-Allow-Headers: Content-Type, Access-Control-Allow- Headers, Authorization, X-Requested-With\");require_once '/home/macgowan/vendor/autoload.php';// [START tts_synthesize_text]\nuse Google\\Cloud\\TextToSpeech\\V1\\AudioConfig;\nuse Google\\Cloud\\TextToSpeech\\V1\\AudioEncoding;\nuse Google\\Cloud\\TextToSpeech\\V1\\SsmlVoiceGender;\nuse Google\\Cloud\\TextToSpeech\\V1\\SynthesisInput;\nuse Google\\Cloud\\TextToSpeech\\V1\\TextToSpeechClient;\nuse Google\\Cloud\\TextToSpeech\\V1\\VoiceSelectionParams;putenv('GOOGLE_APPLICATION_CREDENTIALS=/Users/macgowan/google_cloud/service-account-text-to-speech-test-00.json');try\n{putenv('GOOGLE_APPLICATION_CREDENTIALS=/Users/macgowan/google_cloud/service-account-text-to-speech-test-00.json');\n// $client->useApplicationDefaultCredentials();$ip = getenv('GOOGLE_APPLICATION_CREDENTIALS');\nprintf(\"Get env var - GOOGLE_APPLICATION_CREDENTIALS: %s<br />\", $ip);$ip = getenv('APACHE_RUN_USER');\nprintf(\"Get env var - APACHE_RUN_USER: %s<br />\", $ip);// *** FAILS HERE ***\n$client = new TextToSpeechClient();$text = \"Hello Joe\";print('Set input text using the SynthesisInput() object' . PHP_EOL);\n$input_text = (new SynthesisInput())\n->setText($text);$voice = (new VoiceSelectionParams())\n->setLanguageCode('en-US')\n->setSsmlGender(SsmlVoiceGender::FEMALE);$audioConfig = (new AudioConfig())\n->setAudioEncoding(AudioEncoding::MP3);$response = $client->synthesizeSpeech($input_text, $voice, $audioConfig);\n$audioContent = $response->getAudioContent();file_put_contents('/home/macgowan/output.mp3', $audioContent);\n$client->close();\n}\ncatch (Exception $e)\n{\nprintf(\"Caught exception: %s<br />\", $e->getMessage());\n}\n?>Thanks for your help - Chris   ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-09T09:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, this might be occurring because you don\u2019t have the proper composer package in your composer json file. You need to have the next package installed within your JSON..\n\n{\n\n\u00a0 \u00a0\u00a0\"require\": {\u00a0\n\n\u00a0 \u00a0 \u00a0 \u00a0\"google/cloud-text-to-speech\": \"^1.0\"\n\n\u00a0 \u00a0}\n\n\u00a0}"
			}
		]
	},
	{
		"Question_title": "translate api may give different translation variations in response to the same request",
		"Question_created_time": "2022-04-27T05:54:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/translate-api-may-give-different-translation-variations-in/td-p/417588/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 67,
		"Question_body": "I am using Google cloud translate api v3 NMT to translate sentences from English to Hebrew. I noted that for the same source sentence I may get slightly different results on subsequent calls. I see these going back and forth in a short time frame, so it is NOT a result of the model being updated. For my application I would like to either get all possible variations or at least get reproducible results. Is there an option to get all variations or to set the random seed? Or must I resort to multiple polling and caching on my side?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-09T09:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, what I would recommend to you is, as you said in your second question, to implement multiple polling and store it on your side so that you have a better knowledge of what's going on on your end."
			}
		]
	},
	{
		"Question_title": "Vertex AI Custom Training Job Container not finding my module: Error while finding module for '...'",
		"Question_created_time": "2022-04-25T02:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Custom-Training-Job-Container-not-finding-my-module/td-p/416620/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 398,
		"Question_body": "Hello,I have a PyTorch training job that I am packaging in a Python software distribution (.tar.gz file). I upload the sdist to a GCS bucket and run it in a container using the gcloud ai custom-jobs create CLI.Up until a couple of weeks ago this worked fine but in recent days my jobs consistently fail with messages like these appearing in their logs:Running command: python3 -m MyPackage.MyModule --job-dir=gs://my-bucket/my-job/model --model-name=my-model\n\n/opt/conda/bin/python3: Error while finding module specification for 'MyPackage.MyModule' (ModuleNotFoundError: No module named 'MyPackage.MyModule') MyPackage.MyModule is my module where my training code runs, naturally.As I've mentioned above the same procedure worked until recently. There have not been any changes to it and I can clearly see that MyModule.py is located under MyPackage in my .tar.gz file.The container image that I am using is us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-9:latest and from what I can tell it has not changed since the time I successfully used it before.Why is the Vertex AI container not finding my training module? How can I further debug and fix this?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-06T15:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Check this documentation[1] to see how to fix ModuleNotFoundError.\n\n[1] https://towardsdatascience.com/how-to-fix-modulenotfounderror-and-importerror-248ce5b69b1c"
			},
			{
				"Answer_created_time": "2022-05-08T05:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Jose,\n\nThank you for trying to help. Alas, I've already followed all suggestions in the linked article, to no avail. There is something funky going on between the Vertex AI python code that looks for my module and the way I structured my .tar.gz. At this point, without being able to access the Vertex AI code, I don't see how to debug this."
			}
		]
	},
	{
		"Question_title": "Appsheet with Google Cloud Vision",
		"Question_created_time": "2022-04-26T05:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Appsheet-with-Google-Cloud-Vision/td-p/417105/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 125,
		"Question_body": "Hello,\n\nHow can I integrate Google Cloud Vision in Appsheet to create  a Facial recognition CHECK IN system where the staff can just take photos of themselves on a device and the image captured is compared with the stored image in the Google Drive. If the Face is detected or matched with the stored image, it should automatically CHECK IN OR CHECK OUT the staff.\n\nPlease kindly help.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-06T16:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Please have a look into this documentation that appsheet offers."
			}
		]
	},
	{
		"Question_title": "Product Specifications or Product Recommendation in the Support Section",
		"Question_created_time": "2022-04-21T08:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Product-Specifications-or-Product-Recommendation-in-the-Support/td-p/415770/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 41,
		"Question_body": "HiAny ones knows if I could get Product Specifications or Product Recommendation in the Support Section from Google Cloud: ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-06T07:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, you should be able to get product specifications and product recommendations every time you contact Google support and also chat with a support agent via chat."
			}
		]
	},
	{
		"Question_title": "Connect API to AutoML Model",
		"Question_created_time": "2022-05-02T06:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Connect-API-to-AutoML-Model/td-p/419081/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 126,
		"Question_body": "Hi, I already have the classification model created using AutoML, I have a Google Colab file in which I calculate the different features based from the information received from a public APIWhat Google tool should I use to be able to connect Google Vertex to the API?What module / section of Vertex AI should I connect the API to in order to receive live data?To which instance / section should I upload the Google Colab notebook with the calculations of the features?What tool should I use to connect the model created in AutoML to the feature data?Finally, if I wanted to export the predictions to an API, which tool should I use?Thanks alot for any help you may provide",
		"Answer_list": [
			{
				"Answer_created_time": "2022-05-05T14:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The next Google tutorial will provide you a step by step guide in how to connect the API into AutoML.\n\nIt explains how you can connect the vision API into AutoML, how yo can install your custom library and also how to upload them."
			}
		]
	},
	{
		"Question_title": "Deploying AutoML tabular model changes feature column types to text",
		"Question_created_time": "2022-03-15T03:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Deploying-AutoML-tabular-model-changes-feature-column-types-to/td-p/403658/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 62,
		"Question_body": "I\u2019ve trained an AutoML tabular model using a pretty simple CSV file of numeric data. When I ran the training I ensured each feature column was set as numeric. When viewing the column meta data of the trained model, all columns show as numeric. However, when I deploy the model they all show as text and will only accept strings. What am I doing wrong?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-29T07:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The possible cause may be the incorrect format of the used data type .You can find some examples of the the valid and invalid numeric formats in documentation [1] that can be used in AutoML Tables dataset.\n\n[1] https://cloud.google.com/automl-tables/docs/data-types#numeric"
			}
		]
	},
	{
		"Question_title": "Removed voices from German standard text to speech (tts)",
		"Question_created_time": "2022-02-17T06:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Removed-voices-from-German-standard-text-to-speech-tts/td-p/394397/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 164,
		"Question_body": "We have a problem related to the Cloud text to speech API.\nWe develop an AI based chatbot system, and we have lot of different chatbot which speak in English and German also.\nWe are using two different voices 'de-DE-Standard-B' (male) and 'de-DE-Standard-C' (female) in the case of German bots, but both bots speak in same vois at now.\nWe detected the problem at 2022-02-16.Could you give me some information about this problem?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-22T12:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI understand you have selected 2 different German Language voices[ 'de-DE-Standard-B' (male) and 'de-DE-Standard-C' (female) ] from the list of available voices[0] that can be used for synthetic speech, however both voices are coming out the same.\n\nAs you have rightly indicated, these voices are different. However, it will be nice to understand how you are creating the voice audio files[1]. As indicated in the article[1], it is not only possible selecting a unique voice, you can also make certain modifications depending on your implementation. For example, you can modulate the output in pitch, volume, speaking rate, and sample rate. If you are using SSML in your audio synthesis, you would even have a finer-grain control over how the audio output.\n\nSo, please give more insight to your setup and how these voices are selected.\n\n\u00a0\n\n[0]https://cloud.google.com/text-to-speech/docs/voices\n\n[1]https://cloud.google.com/text-to-speech/docs/create-audio#text-to-speech-ssml-java"
			},
			{
				"Answer_created_time": "2022-04-21T14:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Oakinlaja - There seem to be an issue when using SSML to read date using the German langauge. It plays randomly different message. Please help. This is the below request to Google speech to text.\n\n\u00a0\n\nTTS Request JSON :: {\"voice\":{\"ssmlGender\":\"MALE\",\"name\":\"de-DE-Wavenet-E\",\"languageCode\":\"de-DE\"},\"input\":{\"ssml\":\"<speak><say-as interpret-as=\\\"date\\\" format=\\\"yyyymmdd\\\"> 20220506<\\/say-as><\\/speak>\"},\"audioConfig\":{\"sampleRateHertz\":8000,\"volumeGainDb\":0,\"speakingRate\":1,\"audioEncoding\":\"LINEAR16\",\"pitch\":0,\"effectsProfileId\":[\"telephony-class-application\"]}}"
			},
			{
				"Answer_created_time": "2022-04-21T14:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "In English it works correctly. For english , i pass languageCode : En-US"
			}
		]
	},
	{
		"Question_title": "Google assistant and cloud speech API not working",
		"Question_created_time": "2022-04-06T04:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-assistant-and-cloud-speech-API-not-working/td-p/410871/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 159,
		"Question_body": "I am having problem using Google cloud platform.I bought Google AIY voice kit from AliExpress.com. I discovered it was an old version. Two weeks ago, I used Etcher to flash aiyprojects-2021-04-02.img.xz from GitHub on an SD card and set up my voice kit. Hardware testing was good. I then created a project, named \u201cVoice Kit\u201d, on google cloud following directions given on \"aiyprojects.withgoogle.com/voice-v1/\". I had the following experience:It would be appreciated if I could be educated on the following:",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-20T18:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI see you have concerns relating to the Google AIY Projects and your configurations. I think your Questions will be better supported by the Google AIY Support team[0]. The help page[1] also includes help links on various forums that may have the type of information that you need. For example, your Question about your Question 2, you explained that you are unsure which of the Cloud Speech APIs to use. Whether the Cloud speech-to-text API or the Cloud text-to speech API? Well, the help page seems to include an hyperlink[2] for the Cloud Speech API to use.\n\nAgain, I think your Questions will be better supported by the Google AIY Support team.\n\nI hope this information helps.\n\n[0]support-aiyprojects@google.com.\n[1]https://aiyprojects.withgoogle.com/help/\n[2]https://cloud.google.com/speech-to-text/docs/"
			}
		]
	},
	{
		"Question_title": "Google Cloud Platform - Vertex AI - Workbench JupyterLab - Spark/Hadoop - JAVA_HOME is not set error",
		"Question_created_time": "2022-04-13T15:12:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Cloud-Platform-Vertex-AI-Workbench-JupyterLab-Spark/td-p/413482/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 646,
		"Question_body": "Hi All,I am trying to connect to a SparkSession on Vertex AI's Workbench JupyterLab, but receive this error. Locally, my JAVA_HOME system environments and path environments are already set, and can work when I run Jupyter locally. But only on Vertex AI's Workbench JupyterLab I get this error. Code: \n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.appName('Jupyter BigQuery Storage')\\\n.config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n.getOrCreate()Full Error:Do let me know if you have advice or help, thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-20T13:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You would need to have Java installed on your Mac, Linux or Windows, without Java installation & not having JAVA_HOME environment variable set with Java installation path or not having PYSPARK_SUBMIT_ARGS, you would get this Exception.\n\n\u00a0\n\nYou need to Set PYSPARK_SUBMIT_ARGS with master, this resolves Exception: Java gateway process exited before sending the driver its port number.\n\n\u00a0\n\nexport PYSPARK_SUBMIT_ARGS=\"--master local[3] pyspark-shell\"\n\nvi ~/.bashrc , add the above line and reload the bashrc file using source ~/.bashrc\n\n\u00a0\n\nIn case the issue is still not\u00a0 resolved, check your Java installation and JAVA_HOME environment variable.\n\n\u00a0\n\nYou can see this troubleshooting documentation[1].\n\n\n[1] https://cloud.google.com/vertex-ai/docs/general/troubleshooting"
			}
		]
	},
	{
		"Question_title": "Deep Reinforcement Learning",
		"Question_created_time": "2022-04-13T06:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Deep-Reinforcement-Learning/td-p/413277/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 84,
		"Question_body": "Hi is it possible to implement Deep Reinforcement Learning for structured data frames? If son can someone help me with an example?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-20T13:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Deep Learning delivers a seamless notebook experience with integrated support for JupyterLab[1], so you can load data frames as a normal notebook. Additionally, it depends on what instance you are using Deep Learning.\u00a0\n\nIf you are using TensorFlow, you can see this[2] to know how to load a data frame to TensorFlow.\n\nIf you are using Pytorch tensor, you can see this[3] example of how to load the data frame.\n\n\u00a0\n\n[1] https://cloud.google.com/deep-learning-vm/docs/jupyter\u00a0\n\n[2] https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\u00a0\n\n[3] https://stackoverflow.com/a/50308132/16929358\u00a0\n\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-04-20T13:38:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Deep Learning delivers a seamless notebook experience with integrated support for JupyterLab[1], so you can load data frames as a normal notebook. Additionally, it depends on what instance you are using Deep Learning.\u00a0\n\nIf you are using TensorFlow, you can see this[2] to know how to load a data frame to TensorFlow.\n\nIf you are using Pytorch tensor, you can see this[3] example of how to load the data frame.\n\n\u00a0\n\n[1] https://cloud.google.com/deep-learning-vm/docs/jupyter\u00a0\n\n[2] https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\u00a0\n\n[3] https://stackoverflow.com/a/50308132/16929358"
			}
		]
	},
	{
		"Question_title": "Pipeline failed to deploy model: \"service_account cannot be specified for deploying AutoML models\"",
		"Question_created_time": "2022-04-11T12:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Pipeline-failed-to-deploy-model-quot-service-account-cannot-be/td-p/412645/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 166,
		"Question_body": "I made a pipeline that almost mirrors step 6 of Intro to Vertex Pipelines which has managed to get past every step up until the model deployment side of things. The code snippet for my model deploy op is here:   And the associated error message in the logs for the deployment part of the pipeline was:RuntimeError: Failed to create the resource. Error: {'code': 400, 'message': 'service_account cannot be specified for deploying AutoML Models.', 'status': 'FAILED_PRECONDITION'} Does it have to do with a specific permission I need to give my service account? I don't know how to interpret this error.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-18T14:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You are sure you can specify a service account here like this? Do you have any reference that uses service account here?\n\n[1] https://cloud.google.com/blog/topics/developers-practitioners/use-vertex-pipelines-build-automl-..."
			}
		]
	},
	{
		"Question_title": "Over fitting during RL or DRL when using tabular data",
		"Question_created_time": "2022-04-18T10:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Over-fitting-during-RL-or-DRL-when-using-tabular-data/td-p/414640/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 83,
		"Question_body": "HiI will like to know how the Vertex Api handles or warns about models with over fitting conditions when using Reinforcement Learning or Deep Reinforcement Learning ? If so can you help me with the documents where you explain this situations when using the Vertex Api for tabular dataframes?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-18T10:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\n\nI will like to know how the Vertex Api handles or warns about models with over fitting conditions when using Reinforcement Learning or Deep Reinforcement Learning ? If so can you help me with the documents where you explain this situations when using the Vertex Api for tabular dataframes?"
			}
		]
	},
	{
		"Question_title": "Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
		"Question_created_time": "2022-02-27T02:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very/td-p/397685/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 595,
		"Question_body": "Hello,I am trying to run a Custom Training Job in the Vertex AI Training service.The job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).When I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:$BASE_GPU_IMAGE=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"gcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.Granted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?Please note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image. Thanks,\nurig",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-17T08:03:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-04-07T10:36:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi Urig,\n\nIs it possible that you have local files in the current directory such as data or log files that are getting picked up, specifically this line local-package-path=.\n\nIf this persists, I highly recommend for you to file a Public Issue\u00a0as you can high a private thread created to you and we would be able to further support you there."
			},
			{
				"Answer_created_time": "2022-04-17T08:03:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig"
			}
		]
	},
	{
		"Question_title": "Optimization Variables / Inputs",
		"Question_created_time": "2022-04-12T07:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Optimization-Variables-Inputs/td-p/412877/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 57,
		"Question_body": "Hi is it possible to implement Optimization Problems using AutoML or other Google Cloud application different than regular google Colab / Jupiter Notebooks? ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-14T09:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Could you try creating AutoML models using the Command Line using [1]?\n\n[1] https://cloud.google.com/vertex-ai/docs/training/automl-api#train_an_automl_model_using_the_api"
			},
			{
				"Answer_created_time": "2022-04-16T13:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, saw the link but did not found anything related to optimization\nprocess, is it possible to send more detail information?\n\n--\nSaludos\n\n*Alejandro Holguin M **/ Business Developer*\n*Cel +57 3106668252 / correo electr\u00f3nico: alejandro@inflexion.com.co\n*\n*Skype: holguinmora*"
			}
		]
	},
	{
		"Question_title": "Translating streaming audio into text",
		"Question_created_time": "2022-04-11T14:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Translating-streaming-audio-into-text/td-p/412679/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 46,
		"Question_body": "Hi, I'm using @Google-cloud/media-translation in node with express js server. I want to translate media file (\".wav\" format) with media-translation. At first, i got an error because of authentication and I fixed it with env variable as specified in documentation, I followed each and every step exactly told in the documentation but I'm getting no response from server. When i looked into APIs & Services tab it only recorded my failed auth attempts no other API calls are recorded. Please help because there is no help available online about this product and it doesn't even send error responses so i can debug. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-15T13:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nWould you please share with us the error message ? Please make sure there are PII in it.\n\nYou can also share the reproduction steps?\n\nThanks"
			}
		]
	},
	{
		"Question_title": "Vision API quota/budget limit and API key help",
		"Question_created_time": "2022-04-01T09:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vision-API-quota-budget-limit-and-API-key-help/td-p/409566/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 103,
		"Question_body": "Hello, I have never used Vision API before but I recently found it very powerful for a project of mine. However I have two concerns regarding its budget limiting, in order to not get an unexpected bill:Thanks everyone for any help!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-14T12:31:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0\n\nYou can set a cap of api usage\u00a0or a billing budget might help you on this issue. Does it meet your needs?\n\nAbout your security question about \"hard code the key in Android/iOS\", I recommend you to discuss\u00a0on Stack Overflow for better help."
			}
		]
	},
	{
		"Question_title": "How should the input JSONL look for a batch prediction job?",
		"Question_created_time": "2022-04-04T10:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-should-the-input-JSONL-look-for-a-batch-prediction-job/td-p/410193/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 239,
		"Question_body": "I can't find any examples online of how an input jsonl is supposed to look for a batch training job. When I tried with this:  I got an error email saying  Error Messages: BatchPrediction could not start because no valid instances \nwere found in the input file. Is there some other way this should look for it to work? Maybe like      ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-08T13:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hell sangersteel,\nIt is not possible to use a JSONL file for batch prediction of text classification. Only a CSV file format is accepted for text classification. This is indicated in the [1] [AutoML Natural Language documentation] The CSV file should only contain 1 file (input file) per row. The CSV file and each input file needs to be stored in your Cloud Storage bucket.\n\n[1]\u00a0https://cloud.google.com/natural-language/automl/docs/predict#batch_prediction."
			}
		]
	},
	{
		"Question_title": "Speech-to-Text for many langages",
		"Question_created_time": "2022-03-28T06:33:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-Text-for-many-langages/td-p/407723/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 67,
		"Question_body": "Hello,I'm testing bunch of variety of the Speech-to-Text API to transcribe audio from microphone but I'm going through two issues:Note: I didn't see anywhere how to use utf-8 for transcriptionHow can I fix it I used the code found here https://github.com/googleapis/python-speech/blob/main/samples/microphone/transcribe_streaming_infini...",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-05T10:20:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I solved the first part of my problem, if anyone is going through the same problem\n\nIn windows terminal you can type: chcp 1256 . This allow arabic characters in terminal"
			}
		]
	},
	{
		"Question_title": "Action Needed | OAuth Google Cloud platform | multiple unique domains",
		"Question_created_time": "2022-04-04T02:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Action-Needed-OAuth-Google-Cloud-platform-multiple-unique/td-p/410024/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 117,
		"Question_body": "Hi,I am carrying out the OAuth verification in Google Cloud Platform, I received an email that said:\n\"Thanks for your patience while we reviewed your project.Your project pc-api-XXXXXXXXXXXXXXX-XX has multiple unique domains in the redirect URI and origin URLs, many of which have unrelated applications. This is in direct violation of the Google API Services: User Data Policy, which requires that projects accurately represent their identity and intent to Google and to our users when they request access to Google user data.Please follow the instructions on the Google API Console to:You can find more information in the OAuth Application Verification FAQ.  To make sure we don't miss your messages, respond directly to this email to continue with the verification process.\"I have a web server, which checks the validity (domain-1.com) in-app purchases, and I also have a site with a different domain containing: privacy-policy and terms-of-service (domain-2.com).My settings are as follows:OAuth consent screen:\n- Home page application: https://www.domain-2.com/\n- Privacy Policy: https://www.domain-2.com/privacy-policy/\n- Terms of Service: https://www.domain-2.com/terms-of-service/\n\nAuthorized domains:\n- domain-2.com\n- domain-1.comID client OAuth 2.0 -> Authorized Redirect URIs:\n- https://game.domain-1.com:8443I have a working service account.\nI have successfully verified all 2 domains.Where is the mistake?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-04-04T21:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I have found the solution:\n\nI had 2 Google Cloud Platform projects for the same application.\n\nI deleted a Google Cloud Platform.\nI implemented all the configurations of the deleted project in the other project, and it worked!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-04-04T21:08:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I have found the solution:\n\nI had 2 Google Cloud Platform projects for the same application.\n\nI deleted a Google Cloud Platform.\nI implemented all the configurations of the deleted project in the other project, and it worked!"
			}
		]
	},
	{
		"Question_title": "Create TPU Node - Malformed Name",
		"Question_created_time": "2022-02-26T05:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Create-TPU-Node-Malformed-Name/td-p/397566/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 5,
		"Question_score_count": 1,
		"Question_view_count": 257,
		"Question_body": "Hi! Im trying to create a Google Cloud TPU node using TPU client API and I cannot figure out the parent resource name of a TPU node in Google Cloud.I tried all the possible combinations, for example:And I always get the same error (google.api_core.exceptions.InvalidArgument: 400 Malformed name) :        Below you can find the full code I'm using to create the node. Im using Python 3.8, google-cloud-tpu v1.2.1, on a Conda virtualenv. Any help would be much apprecciated!",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-02T06:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It appears that you have created a StackOverflow thread where a Google Cloud Platform Engineer has already replied.\u00a0\n\nHe has suggested you that\u00a0you can find the expected format of\u00a0parent\u00a0in the documentation for the underlying API method:\u00a0projects.locations.nodes.create.parent\u00a0should be formatted as\u00a0projects/*/locations/*. That is, change\u00a0zones\u00a0to\u00a0locations\u00a0and remove the\u00a0/tpus\u00a0from the end which you had included at the StackOverflow thread.\n\nThe Google Cloud Platform Engineer has further suggested you to remove\u00a0nodes\u00a0from the path. i.e. change\u00a0projects/my-project-id/locations/europe-west4-a/nodes/\u00a0that is shown at the stack trace to\u00a0projects/my-project-id/locations/europe-west4-a/."
			},
			{
				"Answer_created_time": "2022-03-03T23:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!\n\nAs I answered in the same stackoverflow, it appears that following the recommended parent=projects/*/locations/* (to be 100% clear: without /nodes/ ) does not work and gives the error actually shared by the authors.\n\nWe cannot remove a /nodes/ that we do not set in the first place.\n\nLibraries version:\ngoogle-api-core 2.6.0\ngoogle-auth 2.6.0\ngoogle-cloud-tpu 1.3.1\ngoogleapis-common-protos 1.55.0"
			},
			{
				"Answer_created_time": "2022-03-07T13:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The Google Cloud Engineer has updated the response along with the code here. Please let us know if you can use the code and whether that works."
			},
			{
				"Answer_created_time": "2022-03-11T12:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, it worked.\n\nWhen cleaning the resources though, there seems to be an issue with the lib:\n\n\u00a0\n\nNAME = f\"projects/{manifest.tpu.gcpProject}/locations/{manifest.tpu.gcpZone}/nodes/{manifest.name}\"\n\nclient = tpu_v2alpha1.TpuClient()\n        \nrequest = tpu_v2alpha1.DeleteNodeRequest(\n    name=NAME,\n)\n\n# Make the request\noperation = client.delete_node(request=request)\n\nlogging.info(\"Waiting for operation to complete...\")\nresponse = operation.result()\n\n\u00a0\n\n\nThe TPU VM is successfully deleted, but the python code eventually fails:\n\n\u00a0\n\nCleaning TPU\nWaiting for operation to complete...\nTraceback (most recent call last):\nFile \"/argo/staging/script\", line 29, in <module>\nresponse = operation.result()\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/future/polling.py\", line 132, in result\nself._blocking_poll(timeout=timeout, **kwargs)\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/future/polling.py\", line 110, in _blocking_poll\nretry_(self._done_or_raise)(**kwargs)\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/retry.py\", line 283, in retry_wrapped_func\nreturn retry_target(\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/retry.py\", line 190, in retry_target\nreturn target()\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/future/polling.py\", line 88, in _done_or_raise\nif not self.done(**kwargs):\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/operation.py\", line 170, in done\nself._refresh_and_update(retry)\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/operation.py\", line 159, in _refresh_and_update\nself._set_result_from_operation()\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/operation.py\", line 130, in _set_result_from_operation\nresponse = protobuf_helpers.from_any_pb(\nFile \"/root/.local/lib/python3.9/site-packages/google/api_core/protobuf_helpers.py\", line 65, in from_any_pb\nraise TypeError(\nTypeError: Could not convert Any to Node"
			},
			{
				"Answer_created_time": "2022-03-23T07:40:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@Mohammad_I\u00a0could you please have a look at my response, I also created a github ticket:\u00a0https://github.com/googleapis/python-tpu/issues/92\nIt creates wrong fail warnings in our Pipelines today"
			}
		]
	},
	{
		"Question_title": "Recover deleted Vertex AI resources",
		"Question_created_time": "2022-03-22T22:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Recover-deleted-Vertex-AI-resources/td-p/405934/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 429,
		"Question_body": "Hi,In order to save on billing, I deleted most of the resources in the data sources, workbench, pipelines in Vertex AI.Is there a way I can recover them??",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-22T22:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nIn order to save on billing, I deleted most of the resources in the data sources, workbench, pipelines in Vertex AI.\n\nIs there a way I can recover them??"
			}
		]
	},
	{
		"Question_title": "Text to speech Google Cloud Python",
		"Question_created_time": "2022-03-22T18:48:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Text-to-speech-Google-Cloud-Python/td-p/405883/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 325,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-22T18:48:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I would like to calculate the time duration for sentences when I convert text to speech in Google Cloud in Python. For example, if I have three sentences converted to audio, I would like to know when the first sentence starts in the audio, the second one, etc.\n\nExample:\n\ntext= 'Hello, World. I can speak any language. I would like to help you.'\n\nHello, World: starts 00:00 ends 00:03\n\nI can speak any language: starts 00:04 ends 00:09\n\nI would like to help you: starts 00:10 ends 00:13\n\nIs there something for that in python? here is the main code:\n\n\u00a0\n\n\"\"\"Synthesizes speech from the input string of text or ssml.\n\nNote: ssml must be well-formed according to:\n    https://www.w3.org/TR/speech-synthesis/\n\"\"\"\nfrom google.cloud import texttospeech\n\n# Instantiates a client\nclient = texttospeech.TextToSpeechClient()\n\n# Set the text input to be synthesized\nsynthesis_input = texttospeech.types.SynthesisInput(text=\"Hello, World. I can speak any language. I would like to help you.\")\n\n# Build the voice request, select the language code (\"en-US\") and the ssml\n# voice gender (\"neutral\")\nvoice = texttospeech.types.VoiceSelectionParams(\n    language_code=\"en-US\", ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL\n)\n\ntexttospeech_v1beta1.types.cloud_tts_pb2\n\n# Select the type of audio file you want returned\naudio_config = texttospeech.types.AudioConfig(\n    audio_encoding=texttospeech.enums.AudioEncoding.MP3\n)\n\n# Perform the text-to-speech request on the text input with the selected\n# voice parameters and audio file type\nresponse = client.synthesize_speech(\n    input_=synthesis_input, voice=voice, audio_config=audio_config\n)\n\n# The response's audio_content is binary.\nwith open(\"./output.mp3\", \"wb\") as out:\n    # Write the response to the output file.\n    out.write(response.audio_content)\n    print('Audio content written to file \"output.mp3\"')"
			}
		]
	},
	{
		"Question_title": "How can I deploy a pretrained fasttext model?",
		"Question_created_time": "2022-03-13T01:31:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-can-I-deploy-a-pretrained-fasttext-model/td-p/403114/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 346,
		"Question_body": "Hi, I have this code : \"",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-16T17:06:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nSeeing that there is already an existing post [1] on Stack Overflow, kindly follow up on that post to avoid any work duplication.\u00a0\n\n\u00a0\n\n[1]\u00a0https://stackoverflow.com/questions/71455187/how-can-i-deploy-a-fasttext-model-on-google-cloud"
			},
			{
				"Answer_created_time": "2022-03-17T22:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi! Thanks for answering, that question actually is mine, and there hasn't been an answer yet. Anyways I have a question, all I want to do, is deploy a couple pretrained fasttext models :\u00a0\n\nimport fasttext\nft = fasttext.load_model('pretrainedmodelpath')\nJust that, a couple times and be able to have an api get word vectors for words, which google cloud service would best be suited for this task? Most of the google cloud services want me to train my own model and don't let me just load a pretrained one. And in for example Dataflow, I can't just send text via the API and get an answer because it wants a file input. Any ideas? Thanks!"
			}
		]
	},
	{
		"Question_title": "[Vertex AI] Bug - Failed to create endpoint due to the error: INTERNAL",
		"Question_created_time": "2022-03-15T06:40:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Bug-Failed-to-create-endpoint-due-to-the-error/td-p/403711/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 443,
		"Question_body": "When attempting to create a new Vertex AI endpoint in us-central1 using a healthy model, I keep getting the error: \"Failed to create endpoint \"NAME\" due to the error: INTERNAL\"\n\nI expected the endpoint to get deployed successfully.  In fact, up to about 7 days ago, this operation worked perfectly.\n\nSteps to reproduce:\nAttempt to deploy a health Vertex AI model to a new endpoint in us-central1\n\nI'm currently trying to figure out if this INTERNAL error is specific to a region (or not), but it will take me hours before I can determine if the region is a factor.  I suspect there's some other global issue that's the problem.Has anyone else encountered this problem?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-17T10:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0Darien,\u00a0\n\nI see that you have already reported this on our Public Issue Tracker\u00a0here and I am glad that you are no longer having this issue."
			}
		]
	},
	{
		"Question_title": "hello custom training tutorial failed on cloud function deploy",
		"Question_created_time": "2022-03-10T15:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/hello-custom-training-tutorial-failed-on-cloud-function-deploy/td-p/402689/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 608,
		"Question_body": "Hi guys I'm following this tutorial to get my had around Vertex AI - https://cloud.google.com/vertex-ai/docs/tutorials/image-recognition-custom/ On step  - https://cloud.google.com/vertex-ai/docs/tutorials/image-recognition-custom/serving#2_deploy_a   since I'm new on GCP anyone tried this tutorial and have the same error? any tips on how to fix this?thank you very much guys",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-15T18:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "The gcloud tool comes with a set of options which are not easily to spot but offer features like verbosity. Add -- verbosity debug to your deplyment command in order to debug the deployment process with more meaningful logs.\n\nFind all the options here:[1]\n\n[1] https://cloud.google.com/sdk/gcloud/reference/functions/deploy"
			}
		]
	},
	{
		"Question_title": "AutoML tables - sample size of an average",
		"Question_created_time": "2022-02-25T06:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-tables-sample-size-of-an-average/td-p/397276/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 76,
		"Question_body": "Hi everyonesI'm new to google automl tables and have a basic question about which data is worthwhile including in the training of my model.I have a dataset of golfers and will be looking at the averages of scores over different periods. For example, average over the past 3 months, 6 months, 1 year etc.My question is, is it worthwhile also including the sample size for each date range for each player. For example, over the past 3 months, some players will have a sample size of 28 while some will only have 2. Those players that have 28 rounds will have more accurate averages than those with 2. However, I didn't know whether google automl tables would pick up this link automatically, whether I could create a different weighting/reliability variable, or whether there's a way to specify a link between columns? Or if this automated type of automl isn't really suitable or just leave out that sample size variable?Thanks in advance",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-11T11:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nGoogle Groups are reserved for general product discussion, StackOverflow for technical questions whereas Issue Tracker for product bugs (unexpected behaviors) and feature requests. To get a better support you should post to the relevant forum, thus please read the Community Support article for better understanding."
			}
		]
	},
	{
		"Question_title": "Save audio file from speech to text stream",
		"Question_created_time": "2022-03-03T07:35:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Save-audio-file-from-speech-to-text-stream/td-p/398993/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 90,
		"Question_body": "I am using @Google-cloud/speech for streaming audio from the browser to my nodejs backend.\nI would like to save the recorded audio.\nI see no option to do so. Any suggestions? Thanks.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-08T13:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hey,\u00a0\n\nYou shall probably use other packages for recording such as recordrtc as mentioned at [1].\u00a0\u00a0\n\n[1]\u00a0https://www.leeboonstra.dev/chatbots/building-your-own-voice-ai-3/"
			}
		]
	},
	{
		"Question_title": "Being told to contact Vertex AI support but we don't have a support contract?!",
		"Question_created_time": "2022-03-08T11:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Being-told-to-contact-Vertex-AI-support-but-we-don-t-have-a/td-p/401449/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 75,
		"Question_body": "Getting an internal error when training a model on Vertex AI.I have gotten repeated emails from Google telling me to contact Vertex AI support about this.We don't pay for a support contract.It seems odd that there is no way to report issues like this to Vertex AI without a support contract.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-08T11:34:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Getting an internal error when training a model on Vertex AI.\n\nI have gotten repeated emails from Google telling me to contact Vertex AI support about this.\n\nWe don't pay for a support contract.\n\nIt seems odd that there is no way to report issues like this to Vertex AI without a support contract."
			}
		]
	},
	{
		"Question_title": "Dialogflow quota reset",
		"Question_created_time": "2022-03-02T01:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Dialogflow-quota-reset/td-p/398513/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 65,
		"Question_body": "Hello,I have a Dialogflow ES agent, and I'm sending a few thousand (~5k) DetectIntent requests asynchronously. Our quota is 9k requests per minute, so it shouldn't be a problem. However what I'm seeing is that even after waiting for 10 minutes or more, when I run another batch (also ~5k), I get a resource exhausted error. If the quota is 9k per minute, why is the resource still exhausted after 10 minutes? And is there a way to know by what time I should try again?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-02T01:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI have a Dialogflow ES agent, and I'm sending a few thousand (~5k) DetectIntent requests asynchronously. Our quota is 9k requests per minute, so it shouldn't be a problem. However what I'm seeing is that even after waiting for 10 minutes or more, when I run another batch (also ~5k), I get a resource exhausted error. If the quota is 9k per minute, why is the resource still exhausted after 10 minutes? And is there a way to know by what time I should try again?"
			}
		]
	},
	{
		"Question_title": "load a .h5 trained model directly from GCS ?",
		"Question_created_time": "2022-02-21T03:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/load-a-h5-trained-model-directly-from-GCS/td-p/395442/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 4,
		"Question_score_count": 0,
		"Question_view_count": 340,
		"Question_body": "Hello, it's the fist time I actually try to put in a production environment a locally trained .h5 model. I have a website hosted on a cloud run container and I'm trying to run an Image processing pipeline every-time a file is uploaded to GCS via the website (that's why I want to use a cloud function that triggers when a new file is created).my issue:I have found a way to load my .h5 model from GCS but It's taking way too mush time and I'm sure there's surely a better way to do what i'm trying to do:almost 1 minute to load on my local machine. Do you have any recommendation on how to trigger the prediction of my trained model + (pre/post processing) easily upon file upload from my website (in a serverless context) ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-23T17:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nCan you kindly advise of your complete workflow. Additionally, how big are these h5 files?\n\nRegards"
			},
			{
				"Answer_created_time": "2022-02-28T12:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I am using a could run docker container. And instead of packaging the model inside the container I would prefer to tell the code to load it directly from GCS (in python). Model .h5 size is 22.4Mo"
			},
			{
				"Answer_created_time": "2022-03-01T17:17:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nGCS does offer low latency which applies to all storage classes. If your app and GCS buckets are in the same region, I suggest to file an official support [1] so we can take a look at it in details using our internal tooling.\n\nRegards,\n\n[1]\u00a0https://cloud.google.com/support-hub"
			},
			{
				"Answer_created_time": "2022-03-01T17:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Additionally,\u00a0\n\nHave you tried comparing the results when fetching the same file directly through gsutil?"
			}
		]
	},
	{
		"Question_title": "Use GCP Endpoints as reverse proxy for Vertex Ai Endpoint",
		"Question_created_time": "2022-02-24T08:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Use-GCP-Endpoints-as-reverse-proxy-for-Vertex-Ai-Endpoint/td-p/396912/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 587,
		"Question_body": "I am using GCP Endpoints to work as a reverse proxy to a Vertex Ai Endpoint. I can authenticate to GCP Endpoints with api keys, service account... but I get the following error code. Yet, am able to get a successful response from Vertex Ai Endpoint directly. # Error code when requesting to GCP Endpoints (API is authenticated)   Even using the flag \"--allow-unauthenticated\" when setting up ESPv2 still fails. The request   openapi.json (host and address removed for privacy)   Any help would be greatly appreciated",
		"Answer_list": [
			{
				"Answer_created_time": "2022-03-01T07:37:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nCould be for different reasons:\n\nFor testing purposes, grant the service account the \u2018owner\u2019 role as advice in [1] document, and see if you still get the error.\u00a0\nIf everything was working fine before, please try to generate another <access_token> for your service account as mentioned in [2].\nMake sure you followed all the steps form document [3]\n\nThank you\n\n[1]: https://cloud.google.com/vertex-ai/docs/tutorials/image-recognition-automl#before_you_begin\n\n[2] https://developers.google.com/identity/protocols/oauth2#5.-refresh-the-access-token,-if-necessary.\n\n[3]: https://cloud.google.com/vertex-ai/docs/general/custom-service-account"
			}
		]
	},
	{
		"Question_title": "AI & MLOps Garage (Demos + Hands-on w/ Prizes) - March 9th",
		"Question_created_time": "2022-02-27T10:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AI-amp-MLOps-Garage-Demos-Hands-on-w-Prizes-March-9th/td-p/397728/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 2,
		"Question_view_count": 103,
		"Question_body": " Join Google Cloud Industry experts for a half-day dedicated to the possibilities of MLOps & AI. Dig deeper into how you can mature your current Machine Learning & AI practices. Not a Data Scientist or a ML Engineer? No worries! Let us help you get started with modern ML technologies like AutoML and ML w/ SQL. End the day\u2019s learning by building an MLOps pipeline to automate data engineering, model training & model deployment.   Registration Link : https://inthecloud.withgoogle.com/machine-learning-ai-garage-series/register.html   Through conversations and hands-on workshops, you\u2019ll explore:   The newest AI and ML innovations, use cases, and best practices How to build high-quality ML models with minimal effort How to use automation to your advantage    Running AI and ML solutions both in the cloud and on-premises  LinkedIn Event  Registration Page",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-27T10:11:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Join Google Cloud Industry experts for a half-day dedicated to the possibilities of MLOps & AI. Dig deeper into how you can mature your current Machine Learning & AI practices. Not a Data Scientist or a ML Engineer? No worries! Let us help you get started with modern ML technologies like AutoML and ML w/ SQL. End the day\u2019s learning by building an MLOps pipeline to automate data engineering, model training & model deployment.\n\n\u00a0\n\n\u00a0\n\n\u00a0Registration Link : https://inthecloud.withgoogle.com/machine-learning-ai-garage-series/register.html\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThrough conversations and hands-on workshops, you\u2019ll explore:\n\n\u00a0\n\n\u00a0\n\n\u00a0The newest AI and ML innovations, use cases, and best practices\n\n\u00a0How to build high-quality ML models with minimal effort\n\n\u00a0How to use automation to your advantage\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0Running AI and ML solutions both in the cloud and on-premises\n\n\u00a0\n\n\u00a0LinkedIn Event\u00a0\n\n\u00a0Registration Page"
			}
		]
	},
	{
		"Question_title": "Viewing model architecture",
		"Question_created_time": "2022-02-23T14:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Viewing-model-architecture/td-p/396608/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 77,
		"Question_body": "Hello! How can I interpret the feedforward NN model architecture described in AutoML logs (after training a model using AutoML in VertexAI)?I understand the base structure described in https://cloud.google.com/automl-tables/docs/loggingBut I am not sure this describes the full architecture. For example, if num_neurons = 256, and num_layers = 2, how do I know how many neurons on each layer? Or for dropout = 0.5, in which layer is the dropout happening?Any sources your recommend that might explain this a bit better?Thank you very much in advance for your help! I have been researching this and have found no clear explanation ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-26T13:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nIt seems the doc[1] you shared is the only one I can find\u00a0 that describes the feedforward NN model architecture.\n\nKeep in mind that AutoML Tables \u00a0is covered by the Pre-GA Offerings Terms of the Google Cloud Terms of Service.\u00a0\u00a0\n\nBut I encourage you to file a feature request here[2] to have the full\u00a0 model architecture details added to the log entries.\n\n[1]:\u00a0https://cloud.google.com/automl-tables/docs/logging\n\n[2]: https://cloud.google.com/support/docs/issue-trackers?hl=en"
			}
		]
	},
	{
		"Question_title": "Vertex AI integration with mlflow ?",
		"Question_created_time": "2022-02-18T03:57:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-integration-with-mlflow/td-p/394738/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 0,
		"Question_body": "Is there any way to integrate vertex AI with mlflow ? \nAny articles or resources I can go through ?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-18T04:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0avinashbhawnani,\u00a0\n\nI would suggest to have a look at MLflow plugin for Google Cloud Vertex AI\n\nhttps://pypi.org/project/google-cloud-mlflow/\n\nFeel free to reach out in case of questions\n\nThanks"
			},
			{
				"Answer_created_time": "2022-02-20T23:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0\u00a0ilnardo92,\n\nThanks for sharing the resource, will have a look into it and reach out in case of any queries.\n\nThanks"
			},
			{
				"Answer_created_time": "2022-02-26T06:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi!... i saw right now this post and tanks to sharing...\u00a0\n\nYou know if is possible to share another container registry how to \"nexus\" instead of GCR ?"
			}
		]
	},
	{
		"Question_title": "Vertex AI workbench and Google cloud storage problems accesing files",
		"Question_created_time": "2022-02-07T05:27:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-workbench-and-Google-cloud-storage-problems-accesing/td-p/390712/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 536,
		"Question_body": "I am trying to create a ML project in which the job is a classification task of videos, so I upload those videos in Google cloud storage, and then I create a notebook on the workbench of vertex AI, for making data balancing, and then train my respective ML algorithm. But I have this problem:1. I want to use the video files from GCS without the need of downloading them again(that was the purpose of uploading them in GCS), but I don't know how can i do this?.I also try uploading de videos into the dataset space of the vertex AI workbench but still don't know how to acces to this files without downloading them again.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-09T06:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "If I'm not mistaken, you simply need to provide a schema file to set the structure and then provide the json input file with the right paths according to the schema file you've uploaded.\nWhat do you mean by this?:\n\nwithout the need of downloading them again\n\nI'm not aware of any requirement to download a file in order to use it in Vertex AI, the learning algorithm should access the files on Cloud Storage and use them as needed."
			}
		]
	},
	{
		"Question_title": "No more Wavenet for fr-FR lang",
		"Question_created_time": "2022-02-09T00:32:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/No-more-Wavenet-for-fr-FR-lang/td-p/391415/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 91,
		"Question_body": "Hi,I have been using Google cloud API for text-to-speech to generate audio based on text for some days using the Wavenet voices and it worked great. The vast majority of my text is French and I have been using the fr-FR-Wavenet-C voice for it. I can't find it anymore. Even the page https://cloud.google.com/text-to-speech/ doesn't show up in the demo section. That's seems to be the case for all fr-FR-Wavenet voices. Have they been deleted?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-02-09T00:32:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nI have been using Google cloud API for text-to-speech to generate audio based on text for some days using the Wavenet voices and it worked great. The vast majority of my text is French and I have been using the fr-FR-Wavenet-C voice for it. I can't find it anymore. Even the page https://cloud.google.com/text-to-speech/ doesn't show up in the demo section. That's seems to be the case for all fr-FR-Wavenet voices. Have they been deleted?"
			}
		]
	},
	{
		"Question_title": "Where is Visual Inspection AI?",
		"Question_created_time": "2022-01-04T06:23:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Where-is-Visual-Inspection-AI/td-p/181914/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 212,
		"Question_body": "Does anybody know where to get started with the Visual Inspection AI?  It has been advertised for more than 6 months, but I cannot find where it is available.   The landing page is here: https://cloud.google.com/solutions/visual-inspection-ai  However, it has never shown up in my Google Cloud Platform Console. ",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-17T08:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It involves the analysis of products on the production line for quality control. Visual inspection can also be used for internal and external assessment of the various equipment in a production facility, such as storage tanks, pressure vessels, piping, and other equipment.\nArtificial intelligence (AI) is one of the most exciting and fastest-growing fields in computer science. If you're starting to learn about it, you'll need a solid E-Learning Platform by Edureka. This is an Artificial Intelligence course program accredited by E&ICT academy, NIT Warangal, India."
			},
			{
				"Answer_created_time": "2022-02-02T16:09:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I think you should ask Google's sales team."
			}
		]
	},
	{
		"Question_title": "Make deep learning VM JupyterLab publicly available?",
		"Question_created_time": "2022-01-27T11:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Make-deep-learning-VM-JupyterLab-publicly-available/td-p/386576/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 133,
		"Question_body": "I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https://[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-31T13:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-01-31T14:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I think the problem was that I had selected the option to enable access via URL and this only grants access to users who are Editors or Owners in the workspace (step 9 here)\u00a0\n\n\u00a0\n\nhttps://cloud.google.com/deep-learning-vm/docs/tensorflow_start_instance#creating_ainstance_from_the\n\u00a0\n\n\nWhen I created a new VM and left this option unchecked, I was able to use my public SSH cert to get access.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-01-31T13:58:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!"
			},
			{
				"Answer_created_time": "2022-01-31T14:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I think the problem was that I had selected the option to enable access via URL and this only grants access to users who are Editors or Owners in the workspace (step 9 here)\u00a0\n\n\u00a0\n\nhttps://cloud.google.com/deep-learning-vm/docs/tensorflow_start_instance#creating_ainstance_from_the\n\u00a0\n\n\nWhen I created a new VM and left this option unchecked, I was able to use my public SSH cert to get access."
			}
		]
	},
	{
		"Question_title": "How can I use this specific voice? (English (en-gb-x-gbg-network)",
		"Question_created_time": "2022-01-20T22:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-can-I-use-this-specific-voice-English-en-gb-x-gbg-network/td-p/184984/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 329,
		"Question_body": "Goal: Given text, generate mp3 files using Google Cloud tts servicesProblem: Unable to find specific voice I am used to hearing English (en-gb-x-gbg-network). Other info: I've been using this tts app on android in which I can select the aforementioned Voice Type from the Google TTS engine on android. I have since created a Google Cloud account, and followed the tutorial to setup a project to which I can use their selection of voices. However, when I went through the list of voice that I can use, the en-gb-x-gbg-network was not available to use. AFAIK, en-gb-x-gbg-network is not a premium WaveNet voice type. I suspect it has something to do with android but I can't not see why I can't use this voice on the Google Cloud Platform. Many thanks for any helpful info or any nudge that can point me to the right directionCheers, Welp",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-24T07:14:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Welp,\n\nI will like to point you to this article[0] that highlights the supported voices and languages available in Cloud Text-to-Speech on GCP.\n\nApparently, while there are other en-gb voices supported, the en-gb-x-gbg-network is not supported at this time. This explains why you could not find this specific voice[en-gb-x-gbg-network] on your GCP project because it seems unsupported at this time.\n\nPerhaps you can go through the supported voices and the samples provided on the link[0] to check for other alternatives?\n\n[0]https://cloud.google.com/text-to-speech/docs/voices"
			}
		]
	},
	{
		"Question_title": "Speech changes to a more robotic feel if I use certain phonemes",
		"Question_created_time": "2022-01-17T00:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-changes-to-a-more-robotic-feel-if-I-use-certain-phonemes/td-p/184068/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 55,
		"Question_body": "I am using phonemes for some speaks, to make them sound more natural, but some phonemes change the rest of the sentence feel. I have made a demo here.Using these settings:In the speak below I have two identical speaks, where one word is replaced with a phoneme. Notice how the end of the sentence changes to a more robotic feel in the first. How do I avoid this?<speak>\nIn this training, you will learn more about how you sell <phoneme alphabet=\"ipa\" ph=\"k\u0251\u02d0d\">placeholder</phoneme> as a solution for companies that want to minimize out-of-pocket spending and have better control of company spending by employees.In this training, you will learn more about how you sell card as a solution for companies that want to minimize out-of-pocket spending and have better control of company spending by employees.\n</speak>",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-20T04:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi! I see there is an issue already open regarding this. I\u2019ve tested your SSML and I understand what the issue is and it seems like unintended behavior. I tried to workaround it but I didn\u2019t find a way to do so. Any updates regarding your issue will be on the issue tracker"
			},
			{
				"Answer_created_time": "2022-01-23T23:28:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Yes. I created the issue. Thx for the reply"
			}
		]
	},
	{
		"Question_title": "BatchAnnotateImagesResponse images info (context support) for web based images",
		"Question_created_time": "2021-12-23T01:21:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/BatchAnnotateImagesResponse-images-info-context-support-for-web/td-p/181129/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 155,
		"Question_body": "Hi all. How could one know in what exact web based image the text was detected when multiple web based images are sent to the cloud vision api in a single request using BatchAnnotateImagesRequest? BatchAnnotateImagesResponse doesn't return that information which is kinda odd... It has ImageAnnotationContext, which holds image details, but it's reserved only for files and not web based images.\n\nIs there some way to do this? Maybe like preserving order of images in request / response or something down that line.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-27T13:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nlooking to our documentation [1] the batch annotation images response return an Individual responses to image annotation requests within the batch as AnnotateImageResponse [2].\n\nImageAnnotationContext, if present, contextual information is needed to understand where this image comes from.[1]https://cloud.google.com/vision/docs/reference/rpc/google.cloud.vision.v1#batchannotateimagesrespons...\n[2]https://cloud.google.com/vision/docs/reference/rpc/google.cloud.vision.v1#annotateimageresponse"
			},
			{
				"Answer_created_time": "2022-01-11T00:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, abdelilahf\n\nSorry to see response a bit late. I was on vacation.\nAs I wrote, last time we checked, contextual information was just implemented to return image information about file based images, not web based images, and we only can check batches of images fetched from image service via api \"on the fly\".\n\nDo you maybe know if this will be implemented any time soon for web images? Is there some way to request this feature from Google? Also, is there some workaround in the meantime to cover our use case?"
			},
			{
				"Answer_created_time": "2022-01-18T06:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Solved with this answer. Thanks!"
			}
		]
	},
	{
		"Question_title": "Order of entries in BatchAnnotateImagesResponse",
		"Question_created_time": "2022-01-11T01:43:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Order-of-entries-in-BatchAnnotateImagesResponse/td-p/182949/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 117,
		"Question_body": "Given a list of image urls I want to annotate each image, i.e. extract text from each image. For that, I want to use Google Cloud Vision API client library in Java. Here is my pseudocode:Now from batchResponse I can get a list of AnnotateImageResponse. The questions are, does the number of AnnotateImageResponse correspond to the number of requests? Does the order of responses correspond to the order of requests? Can I safely assume that by doing soI will get annotations for the right image on each iteration of the for loop? This is something that is not clear to me from the documentation.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-12T15:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nHave you tried to use AsyncBatchAnnotateImages instead of batchAnnotateImages? As the response contains the context of each image mentioning the uri, eg:\n\n\u00a0\"context\": {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"uri\": \"gs://cloud-samples-data/vision/document_understanding/image1.png\"\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\n\u00a0\n\nThe responses should be in the order of the requests."
			},
			{
				"Answer_created_time": "2022-01-13T00:45:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Nasif,\n\nAsyncBatchAnnotateImages doesn't fit to my usecase. If you say the responses should be in the order of the request, then I'll go for it with\u00a0BatchAnnotateImagesResponse."
			}
		]
	},
	{
		"Question_title": "Google Speech-to-text for live audio",
		"Question_created_time": "2022-01-10T04:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Speech-to-text-for-live-audio/td-p/182518/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 147,
		"Question_body": "I was working with google speech to text for transcribing live audio. I was able to use the auto detect feature to detect user's language he/she is speaking in. it worked perfectly when transcribing an audio file but i was not able to achieve the same result when doing the same with live audio. I followed every sample and documentation made available by google but still no luck.\nPlatform: Python 3.9\nHere is my snippet:Any help will be appreciated. Thanks",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-12T15:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's not much clear what is not same with live audio, can you please clarify a bit.\u00a0\n\n1) What differences you noticed in your results?\n\n2) Which documentation you followed?\n\nThere are whole lot of different issues for example:\n\n- with the speaker diarization (multiple speaker recognition), can not identify different speakers\n\n- can not determine the spaces/pauses, start or end of the speech.\u00a0\n\n- can not recognize some special words. etc.\n\n\u00a0\n\nThere are a lot of factors and RecognitionConfig parameters for example:encoding, sampleRateHertz, language code, speechContext, length of the speech etc. that take into accounts while transcribing an audio.\n\n\u00a0\n\nYou may find the following documentation helpful:\n\n[1] https://cloud.google.com/speech-to-text/docs/concepts\n[2] https://cloud.google.com/speech-to-text/docs/basics\n[3] https://cloud.google.com/speech-to-text/docs/best-practices\n[4] https://cloud.google.com/speech-to-text/docs/adaptation-model\n[5] https://cloud.google.com/architecture/architecture-for-production-ready-live-transcription-tutorial\n[6] https://github.com/googleapis/python-speech/blob/main/samples/microphone/transcribe_streaming_infini..."
			}
		]
	},
	{
		"Question_title": "AutoML Vision - Error type - No valid preprocessed examples",
		"Question_created_time": "2022-01-11T13:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Vision-Error-type-No-valid-preprocessed-examples/td-p/183067/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 89,
		"Question_body": "I have set up an Image classification (Single-label).The model trained for 18 min 25 sec before I recieved the following error:Due to one or more errors, this training job was canceled on Jan 11, 2022 at 07:34AM Batch prediction job GAF-prediction-test encountered the following errors: No valid preprocessed examples.There is no documentation that I could find that explains this error type. Anyone with any ideas what this means?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-12T13:24:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, Thank you for reporting this behavior.\u00a0 \u00a0\n\nPlease note that\u00a0Groups is reserved for general product discussions. If you require further technical support it is recommended to post your detailed question on Stack Overflow which i can see that you have correctly did. [1].\n\n\u00a0\n\n[1]:https://stackoverflow.com/questions/70673890/automl-vision-error-no-valid-preprocessed-examples\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2022-01-12T13:24:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi, Thank you for reporting this behavior.\u00a0 \u00a0\n\nPlease note that\u00a0Groups is reserved for general product discussions. If you require further technical support it is recommended to post your detailed question on Stack Overflow which i can see that you have correctly did. [1].\n\n\u00a0\n\n[1]:https://stackoverflow.com/questions/70673890/automl-vision-error-no-valid-preprocessed-examples"
			}
		]
	},
	{
		"Question_title": "CUstom Container in Vertex AI pipeline",
		"Question_created_time": "2022-01-07T06:16:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/CUstom-Container-in-Vertex-AI-pipeline/td-p/182237/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 188,
		"Question_body": "Hi,I wanted to check that is it possible to create a custom container in a Vertex AI pipeline and Push it to Artifact registery?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-12T06:16:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "It's a good question and i hope yes, because artifact registry is a \"new version\" of container registry. But is intersting read how to transition to artifact..\u00a0\n\n\u00a0\n\nhttps://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr"
			}
		]
	},
	{
		"Question_title": "DTMF working in dialogflow cx telephony gateway",
		"Question_created_time": "2022-01-05T22:56:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/DTMF-working-in-dialogflow-cx-telephony-gateway/td-p/182110/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 281,
		"Question_body": "Hi,\nI want to know that whether we can integrate  bot to Dialogflow CX Phone Gateway and then we can make something like press 1 for this and press 2 for this? and then that bot should work on numbers entered by user? Actually i have done a research regarding this and found that we have dtmf option which let us take input from the user but that is not working , so can you please let me know if something like this is supported?",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-10T17:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I would recommend you to have clarification for the issue and give a detailed use case with any documentation that you are following.\n\nGoogle Groups are reserved for general product discussion, StackOverflow for technical questions whereas Issue Tracker for product issues (unexpected behaviors) and feature requests.\n\nAs this is quite a technical question I would recommend you using the StackOverflow channel, the scope of the questions reach a greater tech community so they are likely to be answered faster.\n\nPlease read the Community Support article [1] for better understanding.\n\n[1] https://cloud.google.com/support/docs/community"
			}
		]
	},
	{
		"Question_title": "Google Translate API",
		"Question_created_time": "2021-12-30T10:46:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translate-API/td-p/181620/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 346,
		"Question_body": "Hi,I would like to use Google Translate API in plain javascript.As far as I understand from this guide, the supported languages are : ... and some additional languages :Does translate API is supported for Javascript as well? If so, where is the guide?Thanks in advance.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-10T14:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Here is a third-party solution you may find useful [1].\u00a0You may also report it to the Public Issue Tracker (PIT) [2]\u00a0 as well as feature request.\n\n[1]\u00a0 https://github.com/topics/javascript-translate\n\n[2]\u00a0https://cloud.google.com/support/docs/issue-trackers\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2021-12-30T17:04:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "I could not find any official documentation/tutorial on Google Translate API for plain JavaScript. It looks like the previously used \"Google Transliterate API\" which was officially deprecated as of May 26, 2011 had support for plain JavaScript [1].\nThe client libraries are currently available for seven popular programming languages \u2013 C#, Go, Java, Node.js, PHP, Python, and Ruby.\n\nHowever, the following un-official link [2] (July 20, 2021) may help.\n\n[1] https://developers.google.com/transliterate/v1/getting_started\n[2] https://rapidapi.com/blog/google-translate-api-tutorial/"
			},
			{
				"Answer_created_time": "2021-12-31T05:36:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks for the reply sf.\u00a0\n\nHow can it be that there is not documentation for one of the most popular programming languages such as Javascript?"
			},
			{
				"Answer_created_time": "2022-01-10T14:09:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Here is a third-party solution you may find useful [1].\u00a0You may also report it to the Public Issue Tracker (PIT) [2]\u00a0 as well as feature request.\n\n[1]\u00a0 https://github.com/topics/javascript-translate\n\n[2]\u00a0https://cloud.google.com/support/docs/issue-trackers"
			}
		]
	},
	{
		"Question_title": "Vision API - Text detection training",
		"Question_created_time": "2022-01-05T03:59:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vision-API-Text-detection-training/td-p/182002/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 0,
		"Question_view_count": 92,
		"Question_body": "Hi there,I'd like to train my Vision Project to improve document text detection for manuscript books. I couldn't find the solution anywhere. The current result is awful. The language is Portuguese.Please advise. Thanks.",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-10T06:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\u00a0\n\nI hope I understand your concern. The issue seems to be about the Quality of the result observed.\u00a0\n\nCould you give more insights and details on what you observed? As per this doc[0], Portuguese is one of the Supported languages by the Cloud Vision API's text recognition feature. So, if the result is not what you expected, perhaps sharing more info about your use-case, your image/file and the output observed, will help to understand how we could help.\u00a0You can share a screenshot as an example, while describing what you observed and what you expected.\n\n\u00a0\n\n[0]https://cloud.google.com/vision/docs/languages"
			},
			{
				"Answer_created_time": "2022-01-10T06:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Please ensure that whatever Documents or Screenshots added to this thread does not include PII or SPII.\u00a0\n\nThank you."
			},
			{
				"Answer_created_time": "2022-01-10T12:49:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Oakinlaja,\n\nThanks for replying. I'm sending to you my code, image file and result. It does not include PII or SPII. Please advise.\nhttps://1drv.ms/u/s!Ap4FzqZLgHXGpYlNTp8Y8Q6eP_7jiw?e=3tGSIx"
			}
		]
	},
	{
		"Question_title": "BatchPredict could not start due to empty input CSV file",
		"Question_created_time": "2021-12-31T03:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/BatchPredict-could-not-start-due-to-empty-input-CSV-file/td-p/181685/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 60,
		"Question_body": "Hello,\nI have a problem with batch prediction for text classification.\nAccording to the documentation I have created a csv file in which every single line refers to a PDF in my bucket. However I get the error message \"InvalidArgument: 400 BatchPredict could not start due to empty input CSV file\".I would be infinitely grateful for help in this case....",
		"Answer_list": [
			{
				"Answer_created_time": "2022-01-03T09:35:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hey,\u00a0\n\nI would suggest that you consider reaching the\u00a0StackOverflow for technical questions as\u00a0Google Groups are reserved for general product discussion.\u00a0\n\nTo get a better support you should post to the relevant forum, thus please read the Community Support article for better understanding."
			}
		]
	},
	{
		"Question_title": "Can Google Cloud Vision work as fast as Google lens for OCR?",
		"Question_created_time": "2021-12-29T12:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-Google-Cloud-Vision-work-as-fast-as-Google-lens-for-OCR/td-p/181569/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 656,
		"Question_body": "Hello, I am using Google Cloud Vision for text recognition, but the processing speed is quite slow (5 to 15 seconds). I would like to know how does Google lens work so fast and if there's any way to make Google Vision as fast.Edit: My photos that go through Vision are stored in Firebase Storage. (As I've read in some posts this is the quickest way to process them). Thanks in advance!",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-30T11:55:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "=> For uploading\u00a0 images to Google Cloud\u00a0 Vision you can use one of the following options:\n\n1) Directly upload the image as binary (is the slowest)\n\n2) Upload the image as base64 encoded (is ~25% faster)\n\n3) Use a pre-uploaded image stored on Google Cloud Storage [1] (is the fastest)\n\n- Please refer to the documentation [2] for best practices for Cloud Vision API\n\n- Also, the response time of Vision API depends on the resource status or network latency.\u00a0 (network IO, file transfer to Vision API, etc).\u00a0\n\n[1] https://cloud.google.com/storage/\n\n\u00a0[2] https://cloud.google.com/vision/docs/supported-files"
			},
			{
				"Answer_created_time": "2022-01-03T02:21:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Google Cloud Vision API work? Google image recognition API\u00a0will identify images from pre-trained models on large datasets of images and then it classifies the images into thousands of categories to detect the objects, places, people and faces in the images.\n\n\u00a0\n\nMyAccountAccess"
			}
		]
	},
	{
		"Question_title": "BigQueryML Explainability Apparently Not Working",
		"Question_created_time": "2021-12-22T09:25:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/BigQueryML-Explainability-Apparently-Not-Working/td-p/181036/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 450,
		"Question_body": "I'm using BigQueryML to train an XGBoost model on some of my data. When I create the model, I set the ENABLE_GLOBAL_EXPLAIN flag to TRUE, the model then trains properly and I can evaluate it. However there is no Interpretability tab on the model's page, and when I try to query the model with the ML.GLOBAL_EXPLAIN command, I get an error that says: Is this a bug or am I doing something wrong?Here's my create model code: ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-29T17:47:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "You are using the right syntax for creating the model\u00a0 as per the examples mentioned in documentation[1].\n\n=> Some possibilities for the error are :\n\n- Models with the same id were re-training again with the script during the evaluation.\n\n- Evaluation data was not provided (might have been explicitly disabled)\n\nYou can \u00a0open up details about the model in the UI in BigQuery, under Evaluation and verify what details it shows.\u00a0Also, you can try to rerun the model again (creating, training, evaluating) and verify if it works for you.\n\n[1] https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_mod..."
			},
			{
				"Answer_created_time": "2021-12-30T09:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@dikaur\u00a0\n\nThanks so much for your response!\n\nI did try to train new models with new names, and as you can tell from the code above, we do provide the \"RANDOM\" method for data evaluation. In addition, on the model's page, there is information about the model's evaluation, so I believe it is receiving an evaluation dataset. I've also tried several other models with several different datasets and unfortunately they are all seeing the same error.\n\nThis was working about 2 months ago so it's possible that something changed recently.\n\nAre there any other possible explanations for something that I'm doing wrong?\n\nThanks!"
			}
		]
	},
	{
		"Question_title": "VertexAI: a way to post-process results without usign custom containers?",
		"Question_created_time": "2021-12-24T10:42:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/VertexAI-a-way-to-post-process-results-without-usign-custom/td-p/181244/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 84,
		"Question_body": "Hello,I have been searching on how to deploy models on VertexAI in AI Platform manner. Most tutorials shows using pre-built container which seems to load the model and return inference results.My current requirement needs to post-process. This was easy to do with AI platform's Predictor class. Is something like that doable with pre-built containers on VertexAI (where we upload a package for inference by inheriting Predictor class and specifyinh the class name)? Using custom container makes it complex to handle and response to requests.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-29T18:18:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Pre-built containers are convenient to deploy models as Vertex AI provides many pre-built containers but they don\u2019t offer you a chance to customize your workflow beyond the SavedModel. For your scenario, a custom container is the solution as you can build it in your preferred way but it has to comply with several rules required by Vertex AI. [1]\n\n[1] https://blog.ml6.eu/deploy-ml-models-on-vertex-ai-using-custom-containers-c00f57efdc3c"
			}
		]
	},
	{
		"Question_title": "Google Translate Cloud API",
		"Question_created_time": "2021-12-06T05:03:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-Translate-Cloud-API/td-p/177308/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 294,
		"Question_body": "Hi All,I have recently started using Google cloud translate API with python. M having trouble converting this word in the Telugu language which is written in English \"parishkaram chesamu\".  In general internet or mobile the application google translate which we use is giving correctly. But API is returning the same word again.Google Cloud translate API:Input text: parishkaram chesamuOutput text: parishkaram chesamuparameters : text ='''parishkaram chesamu'''\ntarget = \"en\"\noutput = translate_client.translate(text)print(output) --> {'translatedText': 'parishkaram chesamu', 'detectedSourceLanguage': 'te', 'input': 'parishkaram chesamu'}================================Mobile or Internet google translate:Input text : parishkaram chesamuOutput text: We have solved",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-28T14:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Aditya,\n\nYou are writing Telugu language in English \"parishkaram chesamu\" instead of \"\u0c2a\u0c30\u0c3f\u0c37\u0c4d\u0c15\u0c3e\u0c30\u0c02 \u0c1a\u0c47\u0c38\u0c3e\u0c2e\u0c41\"\n\nCurrently the Translate app has the \"Spell check feature\" that responds with both the \"Did you mean\" phrase identified and the corresponding translated result, as oppose to the Translation API.\n\nThe \"did you mean\" text comes from Google's spell check API (the same one that suggests alternative searches when using Google search). When you use the translate API directly you are skipping the spell checking, and that's why you don't get a translation.\n\nThere's already a feature request\u00a0to have a spell check feature in the Translation API response."
			}
		]
	},
	{
		"Question_title": "Speech to Text for Tamil audio file",
		"Question_created_time": "2021-12-23T22:24:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-Text-for-Tamil-audio-file/td-p/181208/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 211,
		"Question_body": "Hi,I am trying to convert an audio file to text.Language: TamilAudio type: Phone callBut when  I look at the support documentation for this API the Model=\"phone call\" is not available for this language. Because of this I am getting very low accuracy after the conversion. please let me know this model would be available in the future. And is there any alternate way to achieve this. ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-27T15:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nUnfortunately the Tamil Language is not supported for phone call as the documentation is showing [1].\n\nI would recommend you to file a feature request by following the steps in the documentation [2].\n\nThanks.\n\n[1]https://cloud.google.com/speech-to-text/docs/languages\n[2]https://cloud.google.com/support/docs/issue-trackers#feature_requests"
			}
		]
	},
	{
		"Question_title": "NLP",
		"Question_created_time": "2021-12-27T02:06:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/NLP/td-p/181365/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 63,
		"Question_body": "https://colab.research.google.com/drive/1U26EA63hocAzyGFLsNP3u-nQUCFYVbpz",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-27T10:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@Shivaay\u00a0can you provide more context?"
			}
		]
	},
	{
		"Question_title": "How can I avoid being charged for Tensorboard?",
		"Question_created_time": "2021-12-21T12:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-can-I-avoid-being-charged-for-Tensorboard/td-p/180658/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 470,
		"Question_body": "Today I received an email from GCP saying that my account will be charged for using Vertex AI Tensorboard from February. It is quite expensive and I want to stop using the service and avoid being charged.How can I do that? There is no option for Tensorboard in the API dashboard (just one for Vertex AI generally). I only have \"basic support\" so I cannot contact technical support, and I am not the billing administrator so I cannot contact billing support. Is there any way I can disable Tensorboard?Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-23T12:57:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nThanks for reaching out regarding Vertex AI TensorBoard pricing, and disabling the service.\u00a0\n\nTensorBoard is currently free of charge as it is a preview product. Vertex AI will launch a new generally available (GA) pricing model for Vertex AI TensorBoard in February 2022, as you mentioned, which will be charged per user, per month.\u00a0\n\n\n\n=> You will be charged for a user if they view the Vertex AI TensorBoard webapp during a given billing period, and the VertexAI product teams plan will be to have this gated by a custom IAM role.\u00a0\n\n=> Given that Tensorboard is part of the VertexAI API, it cannot be disabled without also disabling\u00a0all of VertexAI. It may however be possible to avoid\u00a0being charged by setting all the TensorBoard quotas to zero in the Google Cloud Platform Quotas page [1], and restricting\u00a0what users get assigned\u00a0the IAM role once it is available.\n\n\u00a0\n\n[1] GCP quotas:\u00a0console.cloud.google.com/iam-admin/quotas"
			},
			{
				"Answer_created_time": "2021-12-23T14:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I think I have a super user role, so it sounds like I should set the quotas to zero when they become available. Thank you slando."
			}
		]
	},
	{
		"Question_title": "Cloud Vision model change",
		"Question_created_time": "2021-12-13T05:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Cloud-Vision-model-change/td-p/178017/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 112,
		"Question_body": "Hello, The changes for the Vision API's model from latest -> stable and stable -> legacy are scheduled around Dec 30th/Jan 1st. Is there a more concrete date and time for this planned change?I'd like to use the currently stable model for the time being, which would involve switching from \"stable\" to \"legacy\". Since this could have an impact on some environments, I will need to make this swap shortly after the model references are being changed. Thanks!",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-22T09:12:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Please follow the Release Notes for official updates, as Google does not disclose any updates otherwise."
			}
		]
	},
	{
		"Question_title": "Python Code example to transcribe 2 audio inputs into speech at the same time",
		"Question_created_time": "2021-12-21T00:07:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Python-Code-example-to-transcribe-2-audio-inputs-into-speech-at/td-p/180446/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 152,
		"Question_body": "I'm trying to create a piece of python code that can take 2 audio inputs, 1. from my microphone2. virtual input from zoomat the same timehowever, i am not sure how to transcribe them simultaneously.any help would be appreciated, thank you!",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-21T17:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nThe following doc [1]\u00a0describes how to use Speech-to-Text to transcribe audio files that include more than one channel. Multi-channel recognition is available for most, but not all, audio encodings supported by Speech-to-Text.\n\nI hope this would help, thanks.\n\n[1]https://cloud.google.com/speech-to-text/docs/multi-channel#speech_transcribe_multichannel_gcs-python"
			},
			{
				"Answer_created_time": "2021-12-21T19:53:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, thank you for the reply, but if I am not wrong, this only applies to 1 audio input/audio file. Or could this code be modified to use 2 audio streams?"
			}
		]
	},
	{
		"Question_title": "Text-to-Speech seems to ignore SSML input",
		"Question_created_time": "2021-12-17T08:20:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Text-to-Speech-seems-to-ignore-SSML-input/td-p/179629/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 227,
		"Question_body": "Greetings, all! Since getting started with the TTS service, I have had good success with submitting JSON files that specify simple text input. I am using the instructions for PowerShell as described here:https://cloud.google.com/text-to-speech/docs/quickstart-protocol#windowsWhen submitting JSON files that specify SSML input, however, it seems that some of the SSML elements are being ignored by the speech synthesizer. I'd like to use the <prosody> and <emphasis> elements, but the output isn't reflecting the values I specified. Here's an example:It doesn't seem to matter how I specify the rate and pitch attributes\u2014the output comes back with no alteration.Thank you for taking the time to read this. If you have information or suggestions, please reply with your ideas!",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-20T14:41:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "When submitting JSON files with SSML input, you can use the Prosody tag in the following format[1][2].\n\nExample:\n\n```\n\n<prosody rate=\"slow\" pitch=\"-2st\">Can you hear me now?</prosody>\n\n```\n\n[1] https://cloud.google.com/text-to-speech/docs/ssml#prosody\n\n[2] https://www.w3.org/TR/speech-synthesis11/#:~:text=3.2.4%20prosody%20Element"
			}
		]
	},
	{
		"Question_title": "Getting Error Deadline Exceeded when deploying model from Cloud Firestoer functions",
		"Question_created_time": "2021-12-03T11:17:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Getting-Error-Deadline-Exceeded-when-deploying-model-from-Cloud/td-p/177128/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 0,
		"Question_body": "Hello,I currently have a firebase function that is set to deploy my AutoML tables model everyday at 5am. This has been working fine for the past month, up until the last week. I have been getting the following error below when the function attempts to deploy the model.I watched a google tutorial and it recommend to return a promise from my cloud function. That seemed to work for 1 day, but I received the error again this morning.I am going to try to implement a retry function, but I figured I would ask on here as well. Also, I am thinking that moving from autoML to VertexAI might help alleviate my issues. Any guidance here is helpful.See below for my deploy model code:  ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-03T12:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Is deploying the model fails via Cloud Functions only, or does it also fail when the model is deployed manually, or by any other means?"
			},
			{
				"Answer_created_time": "2021-12-03T15:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Manually (through the cloud console) it usually works. I have been testing using an http call in Firebase functions. Sometimes (not consistantly) that fails if I have recently undeployed the model. Although, I do not get any type of error notification, I just know it fails by checking the cloud console.\n\nThe deploy model function only runs once a day though, and the model has typically been undeployed for at least 20 hours before that, so I don't think I am getting that error because I am calling it too often."
			},
			{
				"Answer_created_time": "2021-12-09T10:39:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "So after further investigation it looks like the model is failing when I am requesting predictions through firebase functions. I get the same error \"DEADLINE_EXCEEDED\". This is not consistent though, it worked for the previous 2 days before this and today failed again. I haven't changed anything.\n\nI have 2 questions:\n\n1) is it possible that congestion on the network is causing these to fail? Would it help if I moved the prediction to a different time? Currently I have it set at 6am PST.\n\n2) Since autoML is beta, would it help if I moved the model to VertexAI? I can make that move if it helps"
			}
		]
	},
	{
		"Question_title": "How to use trained vision model in App Inventor",
		"Question_created_time": "2021-12-05T04:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-use-trained-vision-model-in-App-Inventor/td-p/177222/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 178,
		"Question_body": "I have trained a cloud image model with a set of images, I want to use this model to make a mobile app through MIT App Inventor 2. Please suggest how to do this.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-08T16:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\nAccording to this MIT App Inventor response\u00a0you can use an exported model that has a format compatible with Tensorflow.js.\nPlease find here how to export\u00a0your model in a TensorFlow.js format with\u00a0AutoML Vision Edge and further details about\u00a0building and deploying TensorFlow.js models with AutoML\u00a0here."
			}
		]
	},
	{
		"Question_title": "\u062a\u062d\u0633\u064a\u0646 \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0628\u062d\u062b",
		"Question_created_time": "2021-12-01T21:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/%D8%AA%D8%AD%D8%B3%D9%8A%D9%86-%D8%B9%D9%85%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A8%D8%AD%D8%AB/td-p/176885/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 45,
		"Question_body": "\u0627\u0633\u0639\u062f \u0627\u0644\u0644\u0647 \u0635\u0628\u0627\u062d\u0643\u0645\u0641\u064a \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0628\u062d\u062b \u0641\u064a \u0627\u0644\u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u0634\u0627\u0626\u0639\u0629 \u0648 \u0627\u0644\u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u062a\u064a \u062a\u062c\u0639\u0644 \u0627\u0644\u0634\u062e\u0635 \u0644\u0627 \u064a\u0639\u0628\u0631 \u0639\u0646 \u0633\u0624\u0627\u0644\u0647 \u0623\u0648 \u0645\u0648\u0636\u0648\u0639\u0647 \u0647\u0648 \u0639\u062f\u0645 \u0627\u0644\u0648\u0635\u0648\u0644 \u0625\u0644\u0649 \u0627\u0644\u0643\u062a\u0627\u0628\u0647 \u0627\u0644\u062e\u0637\u064a\u0629 \u0628\u0634\u0643\u0644 \u0635\u062d\u064a\u062d \u0623\u0648 \u0639\u0646\u062f\u0645\u0627 \u064a\u0633\u0623\u0644 \u0633\u0624\u0627\u0644 \u0644\u0627 \u064a\u0633\u062a\u0637\u064a\u0639 \u0634\u0631\u062d\u0647\u0627 \u0639\u0646 \u0637\u0631\u064a\u0642 \u0627\u0644\u0643\u0644\u0627\u0645)\u0627\u0642\u062a\u0631\u062d \u0639\u0646\u062f\u0645\u0627 \u064a\u062a\u0643\u0644\u0645 \u0627\u0644\u0628\u0627\u062d\u062b \u0639\u0646 \u0645\u0639\u0644\u0648\u0645\u0629 \u0623\u0648 \u0633\u0624\u0627\u0644 \u064a\u062a\u0643\u0644\u0645\u0647\u0627 \u0627\u0644\u0628\u0627\u062d\u062b \u0628\u0627\u0644\u0635\u0648\u062a \u0648\u0639\u0644\u0649 \u0637\u0631\u064a\u0642\u062a\u0647 \u0627\u0644\u0639\u0627\u0645\u064a\u0629 \u0648\u0627\u0644\u0643\u0644\u0627\u0645 \u0627\u0644\u0645\u062a\u062f\u0627\u0648\u0644 \u0639\u0644\u064a\u0647 \u0641\u064a \u0645\u0646\u0637\u0642\u062a\u0647 \u0648\u064a\u0643\u0648\u0646 \u0647\u0646\u0627\u0644\u0643 \u0627\u0634\u062e\u0627\u0635  \u0645\u0646 \u0646\u0641\u0633 \u0627\u0644\u0645\u0646\u0637\u0642\u0629 \u064a\u0641\u0647\u0645 \u0644\u063a\u0629 \u0627\u0644\u0645\u062a\u0643\u0644\u0645 \u0648\u064a\u062c\u064a\u0628\u0647 \u0639\u0644\u0649 \u0627\u0633\u0627\u0633\u0647\u0627 \u0648\u064a\u0648\u062c\u062f \u0627\u0634\u062e\u0627\u0635 \u0643\u062b\u0631 \u0645\u062a\u0637\u0648\u0639\u064a\u0646 \u0641\u064a \u0646\u0634\u0631 \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0629 \u0648\u0627\u0644\u062e\u064a\u0631 \u0628\u0627\u0644\u0645\u062c\u0627\u0646 \u0633\u0648\u0627\u0621 \u0643\u0627\u0646\u062a \u0627\u0644\u0637\u0628 \u0627\u0648 \u0627\u0644\u0647\u0646\u062f\u0633\u0629 \u0627\u0648 \u0639\u0644\u0645 \u0645\u0639\u064a\u0646 \u0627\u0648 \u0627\u064a \u0639\u0644\u0645 \u0648\u0645\u0639\u0644\u0648\u0645\u0629\u0627\u0644\u0645\u062e\u062a\u0635\u0631 \u0639\u0646\u062f\u0645\u0627 \u0627\u062a\u0643\u0644\u0645 \u0645\u0646 \u0627\u0644\u062e\u0627\u062f\u0645 \u062d\u0648\u062c\u0644 \u0635\u0648\u062a \u0644\u0627 \u064a\u062a\u0643\u0644\u0645 \u0628\u0627\u0644\u0644\u063a\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0641\u0635\u062d\u0649 \u0648\u0644\u0643\u0646 \u064a\u0648\u062c\u062f \u0627\u0634\u062e\u0627\u0635 \u0643\u0627\u062f\u0645 \u062d\u0648\u062c\u0644 \u064a\u062d\u0644\u0644\u0648\u0646 \u0627\u0644\u0643\u0644\u0627\u0645 \u0628\u0644\u063a\u0629 \u0627\u0644\u0634\u062e\u0635 \u0627\u0644\u0645\u062a\u0643\u0644\u0645 \u0643\u0644\u064b \u062d\u0633\u0628 \u0645\u0646\u0637\u0642\u062a\u0647..\u0627\u062a\u0645\u0646\u0627 \u0648\u0635\u0644\u062a \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0629 \u0648\u0627\u0630\u0627 \u0628\u062f\u0643\u0645 \u0627\u062d\u0643\u064a\u0647\u0627 \u0635\u0648\u062a \u0648\u0627\u0634\u0631\u062d\u0647\u0627 \u0627\u0641\u0636\u0644 \u064a\u0627\u0631\u064a\u062a \u062a\u062e\u0628\u0631\u0648\u0646\u064a \u0648\u0627\u062a\u0648\u0627\u0635\u0644 \u0645\u0639\u0643\u0645 ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-07T09:07:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I would like to inform you that Arabic\u00a0 is not a supported language [1]. I would recommend you to re-formulate your issue in English and we will be very happy to help you.\n\n\u00a0\n\nThanks\n\n[1]\n\nhttps://cloud.google.com/support/docs/language-working-hours#language_support"
			},
			{
				"Answer_created_time": "2021-12-07T09:08:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, I would like to inform you that Arabic\u00a0 is not a supported language [1]. I would recommend you to re-formulate your issue in English and we will be very happy to help you.\n\n\u00a0\n\nThanks\n\n[1]\n\nhttps://cloud.google.com/support/docs/language-working-hours#language_support"
			}
		]
	},
	{
		"Question_title": "Unstructured data in Vertex AI feature store",
		"Question_created_time": "2021-11-30T16:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Unstructured-data-in-Vertex-AI-feature-store/td-p/176796/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 410,
		"Question_body": "Does Vertex AI feature store support ingestion, transformation and storage of unstructured data like images and audio?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-01T13:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nAs per source data requirements [1] regarding Vertex AI Feature Store, it does not look like that you'll be able to ingest unstructured data.\n\n\"Vertex AI Feature Store can ingest data from tables in BigQuery or files in Cloud Storage. For files in Cloud Storage, they must be in the Avro or CSV format.\"\n\n[1]\u00a0https://cloud.google.com/vertex-ai/docs/featurestore/source-data"
			}
		]
	},
	{
		"Question_title": "3D object detection using mobile camera or 3D scanner using cloud vision",
		"Question_created_time": "2021-11-23T00:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/3D-object-detection-using-mobile-camera-or-3D-scanner-using/td-p/176320/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 72,
		"Question_body": "The use case is, I want detect the 3d model through mobile camera or 3d scanner with dimensions to verify the scanned model is available or not in cloud storage. If model is not available it should list the model with approx model with percentage. ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-12-01T07:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\n\nHere is a general guide on training Edge models, that you can export to Edge devices, for on-prem use.\n\nI also found this list of more detailed guides on training, and deploying the model either on an Edge device, or online. It includes this page which talks about training Edge exportable models."
			}
		]
	},
	{
		"Question_title": "About Object Localization in Vision API",
		"Question_created_time": "2021-11-25T02:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/About-Object-Localization-in-Vision-API/td-p/176494/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 0,
		"Question_view_count": 167,
		"Question_body": "Hi,We are considering using the Object Localization feature in the Vision API, however, we cannot find any information about supported object classes. Is the information open to the public? If Yes, where can we find the information?Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-29T12:13:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\n\nwould you please give more details about the object classes and its meaning? if you give an example with other services will be very helpful. The bellow information is my findings and I need to confirm if that what you have being asking about?\n\nThe public documentation [1], mentions only that Object localization identifies multiple objects in an image and provides a LocalizedObjectAnnotation for each object in the image.\n\nThe only place where classes are mentioned, for instance:\n.net client library, all operations are performed through the following client classes: ImageAnnotatorClient, ProductSearchClient\n\nJava client library, shows the classes supported [3].\n\n\u00a0\n\n[1]https://cloud.google.com/vision/docs/object-localizer\n[2]https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest/index\n[3]https://googleapis.dev/java/google-cloud-vision/latest/index.html"
			},
			{
				"Answer_created_time": "2021-11-29T18:42:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\u00a0abdelilahf,\n\nThanks for your detailed information. Sorry for not giving a clear description of my inquiry. I would like to know what kind of objects (for example, bicycles and doors) will be detected in the Object Localization feature of the Vision API. If Google can provide a list of supported objects, that would be helpful.\n\nThank you."
			}
		]
	},
	{
		"Question_title": "Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
		"Question_created_time": "2021-11-28T16:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI/td-p/176629/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 83,
		"Question_body": "I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-29T12:59:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https://github.com/GoogleCloudPlatform/vertex-ai-samples\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2021-11-29T12:59:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https://github.com/GoogleCloudPlatform/vertex-ai-samples"
			}
		]
	},
	{
		"Question_title": "Manage Labeling Assignments on DataCompute",
		"Question_created_time": "2021-11-11T20:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Manage-Labeling-Assignments-on-DataCompute/td-p/175499/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 799,
		"Question_body": "Our team has started to use the DataCompute console to assign labelers to labeling tasks created in Vertex AI. Currently, the Assignments tab requires the Labeling Manager to Populate the Specialists Column and Populate the Tasks Column I wanted to highlight some issues we are facing and ask if there's any plan to implement fixes.Issues: 1. The dropdown for task selection does not order the tasks alphabetically so it is difficult to find a specific task.2. There's no \"Select All\" option, instead, the manager must select each task individually.3. There is no drop down for the specialist emails even though they are available under the Specialists tab.Generally, it would be nice to see the entire assignment table by default rather than nothing on this page.Let me know if some of these issues can be addressed! ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-25T15:25:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Thank you for your input. Can you provide the reproduction steps so we can update it on this thread\u00a0\nPlease note that such issues are usually submitted and handled by the product teams via Public Issue Tracker. Therefore, I just submitted your request to the Vertex AI product team on\u00a0this thread, and I recommend you to star it as all future updates will occur there."
			}
		]
	},
	{
		"Question_title": "is there any list of brands/logos supported by google vision api?",
		"Question_created_time": "2021-11-10T21:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/is-there-any-list-of-brands-logos-supported-by-google-vision-api/td-p/175425/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 324,
		"Question_body": "is there anyplace i can see the list of brands/logos that are currently supported by the google vision api's logo recognition service?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-23T11:41:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team.\u00a0\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2021-11-23T11:41:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team."
			}
		]
	},
	{
		"Question_title": "Speech to Text using Microphone",
		"Question_created_time": "2021-11-21T22:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-Text-using-Microphone/td-p/176201/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 119,
		"Question_body": "Hi there, Was trying to use convert speech to text using a microphone, getting a pop-up security error saying \"failed to construct 'worker': script at ..........\"Any idea why?Also, how can I use the speech to text service with microphone input, I tested uploading a video file and worked perfectly, but no idea how to use the service with microphone input?Please let me know AJ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-22T15:56:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "- The error message [1] can be seen in Chrome as Chrome doesn't let you load web workers when running scripts from a local file. You can try using a different web browser like Firefox and verify if it works for you.\n\n- In order to read from microphone, you can install PyAudio in your machine.\n\nYou can refer to the documentation [2] [3] to use the speech to text service with microphone input.\n\n[1] \"failed to construct 'worker': script at\n\n[2] https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python\n\n[3] https://pypi.org/project/SpeechRecognition/"
			}
		]
	},
	{
		"Question_title": "How to package custom prediction code and serve it using an Endpoint in Vertex AI ?",
		"Question_created_time": "2021-10-25T11:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-package-custom-prediction-code-and-serve-it-using-an/td-p/173876/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 2,
		"Question_view_count": 238,
		"Question_body": "Goal: serve prediction request from a Vertex AI Endpoint by executing custom prediction logic.Expected Workflow:1. Upload a pretrained image_quality.pb model (developed in a non vertex-ai pythonic environment) in a gcs bucket2. Port existing image inference logic into a container and serve the prediction functionality through a vertex AI endpoint. 3. Use Vertex AI api for logging and capturing metrics inside the  custom inference logic.4. Finally we want to pass a list of images (stored in another gcs bucket) to that endpoint.5. We also want to see the logs and metrics in tensorboard.Existing Vertex AI code samples provide examples for custom training , invoking model.batch_predict / endpoint.predict , but don't mention how to execute custom prediction code.It would be great if someone can provide guidelines and links to documents/code in order to implement the above steps.Thanks  ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-19T07:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Please find the following guides respective of the points\n\n1.\n[1] Import model:\u00a0https://cloud.google.com/vertex-ai/docs/general/import-model\n[2] What cannot be migrated:\u00a0https://cloud.google.com/vertex-ai/docs/start/migrating-to-vertex-ai#migration-exceptions\n2.\n[3] Custom containers:\u00a0https://cloud.google.com/vertex-ai/docs/training/containers-overview\n[4] https://cloud.google.com/vertex-ai/docs/training/create-custom-container\n3.\n[5] About metrics:\u00a0https://cloud.google.com/vertex-ai/docs/general/monitoring-metrics\n4.\n[6] Passing list of images:\u00a0https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api\n5.\n[7] Metrics in Tensorboard:\u00a0https://cloud.google.com/architecture/ml-on-gcp-best-practices?hl=en#use-vertex-tensorboard-to-visua...\n\n\nAs there is no existing unifying guide for these operations, I created a documentation feature request to have one, and asked the documentation team to post updates here."
			}
		]
	},
	{
		"Question_title": "ML",
		"Question_created_time": "2021-11-12T04:50:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/ML/td-p/175533/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 335,
		"Question_body": "How to start my journey for being an ML/AI or data science engineer?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-12T04:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "How to start my journey for being an ML/AI or data science engineer?"
			}
		]
	},
	{
		"Question_title": "Speech to text work",
		"Question_created_time": "2021-11-08T00:14:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Speech-to-text-work/td-p/175139/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 352,
		"Question_body": "  I am a user of Speech to Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually. So, in Russian it works fine, however, 20-30 percents of the words are incorrect. Moreover, there are no Russian punctuation at all.  So I get the speech to text transcript, then I create the perfect transcript out of this with correct words and punctuations. All I want to know is how I can improve Speech toText by using the perfect transcript I have already corrected? Where I can send that data to?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-09T09:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\u00a0\n\nIf you're using the Google Cloud Speech-to-Text API [1] and encounter text quality problem, I would suggest that you can report the issue at the Issue Tracker [2] with the reproduction details for the support to further look into issue with you to improve the quality.\u00a0\n\n[1]\u00a0https://cloud.google.com/speech-to-text\n[2]\u00a0https://cloud.google.com/support/docs/issue-trackers"
			}
		]
	},
	{
		"Question_title": "speech-to-text improvements",
		"Question_created_time": "2021-11-01T09:36:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/speech-to-text-improvements/td-p/174445/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 343,
		"Question_body": "Good afternoon!I am a user of Speech-to-Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually.So, in Russian it works fine, however, 30-40 percents of the words are incorrect. Moreover, there are no Russian punctuation at all.  So I get the speech-to-text transcript, then I create the perfect transcript out of this with correct words and punctuations.All I want to know is how I can improve Speech-to-Text by using the perfect transcript I have already corrected? Where I can send that data to?P.S. Sorry for my English. I hope You can understand me \ud83d\ude42",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-05T07:50:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi,\u00a0\n\nIf you're using the Google Cloud Speech-to-Text API [1] and encounter text quality problem, I would suggest that you can report the issue at the Issue Tracker [2] with the reproduction details for the support to further look into issue with you to improve the quality.\u00a0\n\n[1]\u00a0https://cloud.google.com/speech-to-text\n[2]\u00a0https://cloud.google.com/support/docs/issue-trackers"
			}
		]
	},
	{
		"Question_title": "Recommendations AI - catalog update not reflecting in console",
		"Question_created_time": "2021-11-02T22:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Recommendations-AI-catalog-update-not-reflecting-in-console/td-p/174619/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 338,
		"Question_body": "In Recommendations AI, I tried uploading the catalog from GCS. On uploading the catalog, I could see the catalog products showing up in the console but there is a warning notification stating the catalog is not integrated and the total product count is always zero.I have tried both recommended methods of uploading the catalog data from GCS directly through the GCP console and also through the CLI but still, this issue persists. I have followed every instruction provided in this GCP documentation but still, I couldn't figure out the actual issue and not much information is available in the public domain as well \n\nCan someone help with this??\nThanks ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-04T18:30:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nCan you go over the troubleshooting guide\u00a0and see if you can get more details on the issue from the Cloud logging logs or from any API errors there might be?\n\nYou might also try to inspect your browser's log network activity when retrying to upload the catalog with the console method."
			}
		]
	},
	{
		"Question_title": "Does Vertex AI support labels for counting?",
		"Question_created_time": "2021-10-25T08:47:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Does-Vertex-AI-support-labels-for-counting/td-p/173840/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 444,
		"Question_body": "I have an image where I have to do a multi-label classification and additionally count the number of a specific item in each image. I'm trying to setup a labeling task so I can enter a continuous number (0-100 for example), but there doesn't seem to be support for it.  Additionally, does the labeling have capabilities to pre-choose a \"default\"  value? Does anyone have an idea?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-04T16:36:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nAs you have rightly mentioned, Vertex AI does not presently support object counting in an image. Please feel free to reach out to submit a feature request via the issue tracker link[0] to the Vertex AI product team about such implementations.\n\nAs mentioned in this article[1], there are only three ways to assign labels to your training data items:\n\n-- Add the data items to your dataset with their labels already assigned, for example using a commercially available dataset\n-- Assign labels to the data items using the Cloud Console\n-- Request to have human labelers add labels to the data items\n\nAt this time, none of these options provide ways to pre-choose a \"default\" value. May be there may be workarounds to explore using your own human labelers via your instructions.\n\n\n[0]https://developers.google.com/issue-tracker/#public_users\n[1]https://cloud.google.com/vertex-ai/docs/datasets/data-labeling-job"
			}
		]
	},
	{
		"Question_title": "Is there a way to resend invitation emails to labelers inside a Vertex AI Labeler group?",
		"Question_created_time": "2021-10-26T09:53:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Is-there-a-way-to-resend-invitation-emails-to-labelers-inside-a/td-p/173962/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 385,
		"Question_body": "I have added a Labeler Group to my Labeling task in Vertex AI but none of the members of the labeler group received an invite email.How can I ensure this invite email is sent?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-04T13:03:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi izvonkov,\n\nYou can try to delete and re-add labelers in the Manager Console under 'Specialists' page.\n\nPlease keep in mind that it takes some time for Vertex AI to generate the resources for labelers to work with. Once the resources are ready, your designated group manager and labelers will receive the email notification with the link to access their Specialist page.\n\nI hope the provided information is useful for you.\n\nBest Regards,\nKailong\nGoogle Cloud Platform Support, Montreal"
			}
		]
	},
	{
		"Question_title": "Model adaptation - Speech-to-Text - GA?",
		"Question_created_time": "2021-10-19T08:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Model-adaptation-Speech-to-Text-GA/td-p/173368/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 405,
		"Question_body": "Hello,I'd like to know if Model adaptation feature is ready to use in production (custom classes, phrase sets, etc). Official web documentation (https://cloud.google.com/speech-to-text/docs/model-adaptation) says it is a preview feature (Pre-GA). Also, REST resources are inside namespace v1p1beta1 (https://cloud.google.com/speech-to-text/docs/reference/rest).On the other hand, release notes web page (https://cloud.google.com/speech-to-text/docs/release-notes#May_07_2021) says \"The Speech-to-Text model adaptation  feature is now a GA feature\".Thank you very much,Pablo Gomez",
		"Answer_list": [
			{
				"Answer_created_time": "2021-11-01T10:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hello Pablo\n\nThe documentation [1] was just updated 2021-10-27 UTC, and there is no 'preview feature' mentioned in it.\n\nAlso release notes are always more reliable source of information regarding feature updates for Google Cloud Platform\n\nBest Regards,\nKailong\nGoogle Cloud Platform Support, Montreal\n\n[1] https://cloud.google.com/speech-to-text/docs/model-adaptation"
			}
		]
	},
	{
		"Question_title": "Model changes",
		"Question_created_time": "2021-10-19T12:13:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Model-changes/td-p/173396/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 337,
		"Question_body": "It appears that the default model for GCP Cloud Vision API has changed. Specifically, the current API results lack localized objects that were available previously, say around June 2021. How does the team communicate, if at all, what entities the model supports? Thanks.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-10-29T15:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "As per the release notes [1], it doesn't seem like there was any change to the default model.\n\nFrom all of the available models [2], for your use case you can use the object localizer [3].\n\n[1] https://cloud.google.com/vision/docs/release-notes\n[2] https://cloud.google.com/vision/docs/reference/rest/v1/Feature#type\n[3] https://cloud.google.com/vision/docs/object-localizer"
			}
		]
	},
	{
		"Question_title": "AutoML Tables for model where comparison is required?",
		"Question_created_time": "2021-09-29T05:05:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-Tables-for-model-where-comparison-is-required/td-p/171520/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 376,
		"Question_body": "Hi there,I have used GCP for a while now, and have trained quite a few models using AutoML Tables - all of these have been fairly simple datasets with probably a maximum of 20 columns. I now have a problem that I would like to solve, but the dataset is a lot more complicated. I want to be able to predict the results of Greyhound Racing, or at least the % chances of each Greyhound winning a given race, compared to the other greyhounds running in that same race. To be able to do this I need to feed multiple pieces of data for each Greyhound in each given race, to be able to predict the winning chance of that greyhound in that day's race.However, I am very stuck on how to structure my data. Using AutoML Tables - would I need to structure the data in a tabular form with many columns? Or is there a better way to tackle this problem.Here is an example of the data I would be using:Race:Example data for each Greyhound in the race: Does anyone please have any advice of how to tackle this kind of problem, and how best to structure the data to attempt to predict the winning chance of each Greyhound for that day's race, based on that greyhound's previous data, compared to the other greyhounds in that day's race? Thanks,\nRob",
		"Answer_list": [
			{
				"Answer_created_time": "2021-10-16T08:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi, this is tabular data and hence has to be structured as a tabular data. I know this will call for lot of data prep work."
			}
		]
	},
	{
		"Question_title": "Google cloud text to speech",
		"Question_created_time": "2021-09-26T06:11:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Google-cloud-text-to-speech/td-p/171231/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 806,
		"Question_body": "I registered myself for the google cloud text-to-speech service recently. Speech Studio worked just fine for the first few days, but today, to my dismay, there is distortion in the text reader's voice.What can I do about it?Thanks.  ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-10-01T14:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Speech Studio is not a Google product. TTS voices are always the same though and do not change over time if the same one is selected."
			}
		]
	},
	{
		"Question_title": "About Speech-to-Text support area",
		"Question_created_time": "2021-09-26T04:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/About-Speech-to-Text-support-area/td-p/171226/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 336,
		"Question_body": "Does Speech-to-Text have any nodes in Hong Kong?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-10-01T14:26:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Speech to text is available as a global or multi regional service. You can select a region by specifying a particular endpoint.\u00a0\u00a0https://cloud.google.com/speech-to-text/docs/endpoints\n\n\u00a0\n\nHong Kong is not currently available as a standalone region."
			}
		]
	},
	{
		"Question_title": "Issues with Handover Protocols - Facebook & Dialogflow",
		"Question_created_time": "2021-09-29T18:02:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Issues-with-Handover-Protocols-Facebook-amp-Dialogflow/td-p/171609/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 354,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2021-09-29T18:02:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "We're currently experiencing an ongoing issue where Bot conversations in our Facebook inbox are moving from \u2018Main\u2019 to \u2018Done\u2019 without any manual agent involvement. This means that conversations being escalated to the Main folder from the Bot will revert to the Done folder and the Bot will answer again.\n\u00a0\nOnce the conversation with the bot has been escalated to a human, there should be no further Bot involvement.\n\u00a0\nThis issue originally started after we noticed a failed payment on our Dialogflow account, where we went in, updated payment information and successfully charged the card to resume services. However, once we initiated a few test conversations, we noticed the above.. any advice or suggestions?"
			}
		]
	},
	{
		"Question_title": "Vertex AI dataset permissions",
		"Question_created_time": "2021-09-20T07:15:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-dataset-permissions/td-p/170536/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 461,
		"Question_body": "Is there a way to assign IAM roles to datasets in Vertex AI so only certain people have access to certain datasets?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-09-20T07:15:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Is there a way to assign IAM roles to datasets in Vertex AI so only certain people have access to certain datasets?"
			}
		]
	},
	{
		"Question_title": "Fast Start GPU for AI training",
		"Question_created_time": "2021-09-02T02:41:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Fast-Start-GPU-for-AI-training/td-p/168768/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 354,
		"Question_body": "Is there a way to fast start-up GPU (like Cloud RUN) if there is training request come-in?Due to GPU cost is high, turn-on 24 hours/day does not make sense.Pre-empted GPU cloud be another option but offer only 1st minute free.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-09-16T01:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I made a 4000-word guide for people looking to build Nvidia Ampere prosumer workstations and servers, including:\n\n\u00a0\n\nDifferent budget tiers\n\nWhere to place them, home, office, data center, etc.\n\nConstraints with consumer GPUs\n\n\u00a0\n\nMy CC Pay"
			}
		]
	},
	{
		"Question_title": "cloud vision text coordinates format",
		"Question_created_time": "2021-09-15T23:01:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/cloud-vision-text-coordinates-format/td-p/170059/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 354,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2021-09-15T23:01:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi Team,\n\u00a0\nI was going through the vision api document https://cloud.google.com/vision/docs/pdf to understand the format of the coordinates of the text.\u00a0 I was a bit confused regarding the normalized values.\n\u00a0\nCould you please clarify on how to find the original coordinates with respect the dimension of the document.\n\u00a0\nThanks,\nArun"
			}
		]
	},
	{
		"Question_title": "Artificial Intelligence - list of APIs",
		"Question_created_time": "2021-07-23T18:30:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Artificial-Intelligence-list-of-APIs/td-p/164618/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_score_count": 1,
		"Question_view_count": 607,
		"Question_body": "Good day.Is it possible to list the Google APIs in the Artificial Intelligence, especially in the Voice recognitions, comparison, analytics etc? Thank you.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-07-25T12:52:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Let's try this link for a start ...\nhttps://cloud.google.com/products/ai\n\nThis appears to list all the various GCP products related to AI and, if one were to delve into each product, one would find the corresponding APIs."
			},
			{
				"Answer_created_time": "2021-08-02T03:27:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "And take a look at the \"Google Cloud in\u00a0 Words\" cheat sheet\n\nhttps://github.com/gregsramblings/google-cloud-4-words/blob/master/Brochure.pdf"
			},
			{
				"Answer_created_time": "2021-08-22T20:43:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "@pascal_reddig\u00a0wrote: mcdvoice\n\n\nAnd take a look at the \"Google Cloud in\u00a0 Words\" cheat sheet\n\nhttps://github.com/gregsramblings/google-cloud-4-words/blob/master/Brochure.pdf\n\nCarry on, don\u2019t stop posting like this"
			}
		]
	},
	{
		"Question_title": "How to advise Safe Search",
		"Question_created_time": "2021-08-20T14:00:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/How-to-advise-Safe-Search/td-p/167893/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 408,
		"Question_body": "I'm using Safe Search API in Cloud Vision to detect adult or harmful pictures and don't let them pass to be published in my project. But sometimes pictures are recognized wrong - not adult picture tagged so. Is it possible to \"teach\" Vision API, or mark a file as safe for my project? ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-08-20T14:00:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "I'm using Safe Search API in Cloud Vision to detect adult or harmful pictures and don't let them pass to be published in my project. But sometimes pictures are recognized wrong - not adult picture tagged so. Is it possible to \"teach\" Vision API, or mark a file as safe for my project?"
			}
		]
	},
	{
		"Question_title": "enhanced speech feature",
		"Question_created_time": "2021-08-19T01:34:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/enhanced-speech-feature/td-p/167747/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 442,
		"Question_body": "Hi I have a queryIn Dailogflow if we enable enhanced speech feature, specifically, credit card info (i.e. number), if that is spoken by user, is that stored by Google. Please help",
		"Answer_list": [
			{
				"Answer_created_time": "2021-08-20T06:59:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https://cloud.google.com/dialogflow/es/docs/speech-enhanced-models#data-security\n[1]https://cloud.google.com/dialogflow/docs/data-logging-terms\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2021-08-20T06:59:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https://cloud.google.com/dialogflow/es/docs/speech-enhanced-models#data-security\n[1]https://cloud.google.com/dialogflow/docs/data-logging-terms"
			}
		]
	},
	{
		"Question_title": "NLP is hard",
		"Question_created_time": "2021-08-15T22:44:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/NLP-is-hard/td-p/167242/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 337,
		"Question_body": "Who else thinks NLP is the hardest subset of AI to build?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-08-15T22:44:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Who else thinks NLP is the hardest subset of AI to build?"
			}
		]
	},
	{
		"Question_title": "Voice/language options during conversion of long text files to speech",
		"Question_created_time": "2021-08-02T19:55:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Voice-language-options-during-conversion-of-long-text-files-to/td-p/165947/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_score_count": 0,
		"Question_view_count": 386,
		"Question_body": "The voice/language options during conversion of long text files to speech. Can anyone help with the doc/sample for the same.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-08-03T15:53:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps://cloud.google.com/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice/language query in your question, we'll get back to you ASAP.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2021-08-03T15:53:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps://cloud.google.com/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice/language query in your question, we'll get back to you ASAP."
			}
		]
	},
	{
		"Question_title": "7 days left: learn new skills with free access to data, ML and AI labs!",
		"Question_created_time": "2021-08-02T09:58:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/7-days-left-learn-new-skills-with-free-access-to-data-ML-and-AI/td-p/165897/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 375,
		"Question_body": "Don't miss this great, free ML/AI learning opportunity! Crossposting from the Learning Forums:https://www.googlecloudcommunity.com/gc/Learning-Forums/Your-mission-should-you-choose-to-accept-it/...  ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-08-02T09:58:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Don't miss this great, free ML/AI learning opportunity!\n\n\u00a0\n\nCrossposting from the Learning Forums:\n\nhttps://www.googlecloudcommunity.com/gc/Learning-Forums/Your-mission-should-you-choose-to-accept-it/..."
			}
		]
	},
	{
		"Question_title": "Thanks",
		"Question_created_time": "2021-07-25T10:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Thanks/td-p/164694/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 0,
		"Question_view_count": 2,
		"Question_body": "",
		"Answer_list": [
			{
				"Answer_created_time": "2021-07-25T10:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": ""
			}
		]
	},
	{
		"Question_title": "Should I custom split my image data?",
		"Question_created_time": "2021-07-05T04:26:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Should-I-custom-split-my-image-data/td-p/163031/jump-to/first-unread-message",
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_score_count": 1,
		"Question_view_count": 481,
		"Question_body": "Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML? And what difference does it make?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-07-14T06:16:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba.\n\nView solution in original post"
			},
			{
				"Answer_created_time": "2021-07-14T06:16:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba."
			},
			{
				"Answer_created_time": "2021-07-15T23:26:00",
				"Answer_has_accepted": true,
				"Answer_score_count": 0,
				"Answer_body": "Thanks so much for the response."
			}
		]
	},
	{
		"Question_title": "AutoML - pre-trained models?",
		"Question_created_time": "2021-07-01T12:49:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AutoML-pre-trained-models/td-p/162864/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_score_count": 1,
		"Question_view_count": 559,
		"Question_body": "I know that for AutoML, the user has to train the model. But are there existing \"pre-trained\" models that you can leverage to identify sentiments or classifications like profanity, irony, and bullying? ",
		"Answer_list": [
			{
				"Answer_created_time": "2021-07-01T23:46:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Can you please add more details about the sample text/document that you are trying?. The powerful pre-trained models of the\u00a0Natural Language API\u00a0\u00a0empowers developers to easily apply natural language understanding (NLU) to their applications with features including sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis.\n\nSamples: https://cloud.google.com/natural-language/automl/docs/samples"
			}
		]
	},
	{
		"Question_title": "Vertex AI - Any trainings ?",
		"Question_created_time": "2021-06-23T16:22:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/Vertex-AI-Any-trainings/td-p/161842/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 2,
		"Question_score_count": 2,
		"Question_view_count": 564,
		"Question_body": "Hi CommunityFirstly I would like to congratulate all for the announcement of VertexAI.I am interested to know that with the advent of VertexAI, is the AI-Platform planned to be deprecated completely ? How can existing AIP ML engineers easily transit to VertexAI as there's a huge list of topics and things which VertexAI has brought in and has changed the way GCP is doing ML going forward.Do we have any free Qwiklabs notebooks specifically for VertexAI as now I find myself a lot unaware about new GCP ML( VertexAI ) stack even though I am a GCP ML certified Engineer. This is I am sure an issue with a lot of customers and engineers who were building on AIP since a long time.Hope someone can understand and provide their advice. @rseshadri @Former Community Member what are your thoughts ?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-06-24T00:23:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "There has been data cloud summits workshops/events recently by google and you can get a lot of information there. And they are all on-demand."
			},
			{
				"Answer_created_time": "2021-06-24T15:33:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Hi\u00a0@rafiqhasan\u00a0Check out some of these resources:\n\nVideos:\n\nWhat is Vertex AI?\u00a0\nIntro to Vertex I/O technical session\nML Summit\n\nCodelabs:\n\nTraining a tabular model with AutoML\u00a0\nBuild an AutoML forecasting model with Vertex AI\u00a0\n\nGood luck!"
			}
		]
	},
	{
		"Question_title": "What are you looking forward to in Google Cloud NEXT '21?",
		"Question_created_time": "2021-06-23T09:19:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/What-are-you-looking-forward-to-in-Google-Cloud-NEXT-21/td-p/161539/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 2,
		"Question_view_count": 381,
		"Question_body": "Google Cloud Next conference is happening this year on October 12-14, 2021!What exciting new technologies are you looking forward to?",
		"Answer_list": [
			{
				"Answer_created_time": "2021-06-23T09:19:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "Google Cloud Next conference is happening this year on October 12-14, 2021!\n\nWhat exciting new technologies are you looking forward to?"
			}
		]
	},
	{
		"Question_title": "AI/ML",
		"Question_created_time": "2021-06-18T13:29:00",
		"Question_link": "https://www.googlecloudcommunity.com/gc/AI-ML/AI-ML/td-p/31/jump-to/first-unread-message",
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_score_count": 10,
		"Question_view_count": 549,
		"Question_body": "This is the discussion space to talk about all things AI/ML related.",
		"Answer_list": [
			{
				"Answer_created_time": "2021-06-18T13:29:00",
				"Answer_has_accepted": false,
				"Answer_score_count": 0,
				"Answer_body": "This is the discussion space to talk about all things AI/ML related."
			}
		]
	}
]