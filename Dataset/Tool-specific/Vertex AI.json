[
    {
        "Question_title":"Error from Vertex AI Getting predictions from custom trained models",
        "Question_tag_count":1,
        "Question_created_time":1681326780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-from-Vertex-AI-Getting-predictions-from-custom-trained\/m-p\/543292#M1647",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":609,
        "Question_body":"I am trying to call an API to inference from a model I have uploaded to vertex AI.\n\nI have tried three methods, and none worked so far.\n\nAt first, I was following a youtube from standford university,\u00a0https:\/\/www.youtube.com\/watch?v=fw6NMQrYc6w&t=3876s\u00a0which uses ai platform.\n\n1. I also tried that, but I think google is trying to get rid of AI platform, and although I succesfully uploaded the model, it doesn't allow me to make a new version, basically allows me nothing.\n\n2. I tried to work this tutorial,\u00a0https:\/\/codelabs.developers.google.com\/vertex-p2p-predictions#5\u00a0\n\nand it keeps complaining that my payload is above 1.5MB limit, but my image is only 49KB, so it's ridiculous. maybe something happened in this code, but it's from the tutorial, so the tutorial must be wrong then.\n\n\u00a0\n\nIMAGE_PATH = \"test-image.jpg\"\nim = Image.open(IMAGE_PATH)\nx_test = np.asarray(im).astype(np.float32).tolist()\nendpoint.predict(instances=x_test).predictions\n\n\u00a0\n\n3. Last, I've been trying to call the API from the sample code,\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\n\nbut it gives me a json format error.\n\nI have referenced from this website to get the json format.\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\u00a0\n\nThe error I am getting is as is :\u00a0\n\n400 { \"error\": \"Failed to process element: 0 key: instances of 'instances' list. Error: Invalid argument: JSON object: does not have named input: instances\" }\n\u00a0\nThe code that I have used is :\n\u00a0\n\n\u00a0\n\n    encoded_content = base64.b64encode(image).decode(\"utf-8\")\n    instances = {\"instances\": {\"image\": {\"b64\": encoded_content}}, \"key\": \"0\"}\n\n\u00a0\n\nand the 400 error comes from API and the log from vertex AI is not very useful when debugging.\n\nHonestly I have been struggling with this issue for days and in my opinion, this should not be this difficult. My experience with GCP and vertex AI is very disappointing and I'm considering to explore other options. Please let me know if any of you have any advices. Thanks",
        "Question_closed_time":1682031900000,
        "Answer_score_count":0.0,
        "Answer_body":"I actually solved this error by applying this document.\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-predict-image-classification-sample.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Google ML kit",
        "Question_tag_count":2,
        "Question_created_time":1660119120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-ML-kit\/m-p\/452579#M498",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":156,
        "Question_body":"I know Google provides an ML kit supported by android that we can integrate into an app. The ML Kit provides many Vision and NLP APIs that can help us make our own Google-like Lens.\n\nAnyone can give me more information on how to get the ML kit?\n\nI am the CEO and I am looking for a CTO to my company, must be good in Python, A.I., Machine Learning, IoT and Robotics.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GPU shortage on europe-west1",
        "Question_tag_count":1,
        "Question_created_time":1678181940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GPU-shortage-on-europe-west1\/m-p\/529808#M1385",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":279,
        "Question_body":"Hi everyone,\n\nI am trying to launch a Vertex AI CustomJob Training on europe-west1 using a T4 GPU.\n\nIt's been two days I keep receiving a \"Insufficient Ressources\" denial and after like 50 trials, I wonder if I am the only one experimenting this issue. Does anyone managed to trigger a training job on T4 ?\n\nI've checked and my quotas are ok.\u00a0\n\nThanks for the feedback",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX bot does not display the fulfilment text when called by api",
        "Question_tag_count":2,
        "Question_created_time":1677233400000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-bot-does-not-display-the-fulfilment-text-when\/m-p\/525966#M1339",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":137,
        "Question_body":"Hi guys, I'm struggling with a problem related to calling the Dialogflow API. I'm using an API to call a specific page using the following payload:\n\n\u00a0\n\n\u00a0\n\n{\n\tsession: \"projects\/xxxx\/locations\/us-east1\/agents\/xxxx\/session\/xxx\",\n\tqueryParams: {\n\t  currentPage: \"projects\/xxxx\/locations\/us-east1\/agents\/xxxx\/flows\/xxxx\/pages\/xxxx\",\n\t  parameters: {\n\t\tfields: {},\n\t  },\n\t},\n\tqueryInput: {\n\t  text: {\n\t\ttext: \"\",\n\t  },\n\t  \"en\",\n\t},\n}\n\n\u00a0\n\nand to call it, I use:\n\n\u00a0\n\nconst assistant = new dialogflow.SessionsClient(config);\nconst [response] = await assistant.detectIntent(request);\n\n\u00a0\n\nI'm trying to call this specific page:\n\nBut I don't get any response. No text is sent back.\n\nNow, if I use a parameter, I'll get the response:\n\nBut here's the thing, I won't use parameters everywhere. I need to get the fulfillment message.\n\nDoes anyone have any idea what I can do? Thank you all!!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deep Reinforcement Learning",
        "Question_tag_count":3,
        "Question_created_time":1649832960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Reinforcement-Learning\/m-p\/413277#M269",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hi is it possible to implement Deep Reinforcement Learning for structured data frames? If son can someone help me with an example?",
        "Question_closed_time":1650461880000,
        "Answer_score_count":0.0,
        "Answer_body":"Deep Learning delivers a seamless notebook experience with integrated support for JupyterLab[1], so you can load data frames as a normal notebook. Additionally, it depends on what instance you are using Deep Learning.\u00a0\n\nIf you are using TensorFlow, you can see this[2] to know how to load a data frame to TensorFlow.\n\nIf you are using Pytorch tensor, you can see this[3] example of how to load the data frame.\n\n\u00a0\n\n[1] https:\/\/cloud.google.com\/deep-learning-vm\/docs\/jupyter\u00a0\n\n[2] https:\/\/www.tensorflow.org\/tutorials\/load_data\/pandas_dataframe\u00a0\n\n[3] https:\/\/stackoverflow.com\/a\/50308132\/16929358\u00a0\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI quota policy exceed when training custom model",
        "Question_tag_count":3,
        "Question_created_time":1664159880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-quota-policy-exceed-when-training-custom-model\/m-p\/470907#M601",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":618,
        "Question_body":"Hello team,\n\nCan anyone please help me with this,\n\nI have been trying to run the custom model training in vertex ai and gives an error saying\n\n\"Training pipeline failed with error message:\u00a0The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus\"\n\nFollowed the below steps to solve it but didn't help me at all,\n\n1. Changed the region (As it mentioned in one comment of Stack Overflow for this error)\n\n2. Increased CPU instances in the work pool as well as notebooks but didn't help me at all.\n\nI have gone through the IAM & API Services, and then when I checked the quotas for the Vertex AI API for all resources in it, none of them had exceeded the quota limit. I'm still confused as to why it was showing a quota exceed error when I was training the custom model.\n\nPlease help me on this issue, how to solve it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Receiving quota error when trying to use the Embedding for Image model in Model Garden",
        "Question_tag_count":3,
        "Question_created_time":1686603780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-the-Embedding-for-Image\/m-p\/602429#M2139",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":42,
        "Question_body":"Hi, I am working on building an application around image search. I am using the Embeddings for Image model\u00a0https:\/\/console.cloud.google.com\/vertex-ai\/publishers\/google\/model-garden\/5?project=applied-ai-labs.\n\n\u00a0\n\nBut, when I am trying to run it from the colab notebook attached,\u00a0 I am getting this error.\u00a0\n\n429 Quota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: multimodalembedding. Please submit a quota increase request.\n\nI checked the quota list, but can't find\u00a0multimodalembedding.\u00a0\n\nPlease help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"only getting error_validation and not results when using vertex for forecasting",
        "Question_tag_count":3,
        "Question_created_time":1688438520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/only-getting-error-validation-and-not-results-when-using-vertex\/m-p\/609090#M2283",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":64,
        "Question_body":"i have been trying to get batch predicitions to a dataset i am only getting error-validation but no predictions can someone help me out asap?\n\nThe date format is according to the format specified in the document",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI\/ML",
        "Question_tag_count":14,
        "Question_created_time":1624022940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-ML\/m-p\/31#M1",
        "Question_answer_count":0,
        "Question_score_count":10,
        "Question_view_count":576,
        "Question_body":"This is the discussion space to talk about all things AI\/ML related.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"NotFound: 404 Publisher Model `publishers\/google\/models\/chat-bison@001` is not found",
        "Question_tag_count":1,
        "Question_created_time":1686315660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NotFound-404-Publisher-Model-publishers-google-models-chat-bison\/m-p\/601725#M2115",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":601,
        "Question_body":"Hi - I'm using on-prem jupyter notebook to run the following code and am getting the error \"NotFound: 404 Publisher Model `publishers\/google\/models\/chat-bison@001` is not found\". Has anyone run into this issue? Thank you.\u00a0\n\ndef predict_large_language_model_sample(\nproject_id: str,\nmodel_name: str,\ntemperature: float,\nmax_output_tokens: int,\ntop_p: float,\ntop_k: int,\nlocation: str = \"us-central1\",\n) :\n\"\"\"Predict using a Large Language Model.\"\"\"\nvertexai.init(project=project_id, location=location)\n\nchat_model = ChatModel.from_pretrained(model_name)\nparameters = {\n\"temperature\": temperature,\n\"max_output_tokens\": max_output_tokens,\n\"top_p\": top_p,\n\"top_k\": top_k,\n}\n\nchat = chat_model.start_chat(\nexamples=[]\n)\nresponse=chat.send_message('''What version of PaLM are you?''',**parameters)\nprint(response.text)\n\npredict_large_language_model_sample(PROJECT_ID, \"chat-bison@001\", 0, 256, 0.8, 40, LOCATION)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ERROR log for health checks, liveness checks, prediction requests for Vertex AI endpoints?",
        "Question_tag_count":3,
        "Question_created_time":1688047620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ERROR-log-for-health-checks-liveness-checks-prediction-requests\/m-p\/607886#M2263",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":50,
        "Question_body":"They seem to log as ERROR, which makes filtering for actual errors in a container really difficult. Is there some way I can change the logging for these so I can actually find errors in my container?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"The kernel for MyTest.ipynb appears to have died. It will restart automatically.",
        "Question_tag_count":1,
        "Question_created_time":1669287660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-kernel-for-MyTest-ipynb-appears-to-have-died-It-will-restart\/m-p\/492715#M860",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":280,
        "Question_body":"Hello,\n\nI'm trying to run a test jupyter notebook of a LSTM model running tensorflow. I have tried setting the GPU memory limit like suggested here. But still the I get the error mentioned above. I can not find anything realted to GC vertex AI and everyone suggest setting the gpu memory in case of such errors.\u00a0\n\nFor reference I have tried to run this\u00a0as well on my Vertex AI jupyter lab and it crashes as well. The only thing I added was this:\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\ntf.config.set_logical_device_configuration(\ngpus[0],\n[tf.config.LogicalDeviceConfiguration(memory_limit=12288)]\n)\n\nlogical_gpus = tf.config.list_logical_devices('GPU')\nprint(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n\nOn my personal computer it runs just fine, but it would take 13 hours to train which is not a option for me at the moment.\n\nAny help would be appriciated.\u00a0\n\nBarnabas.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Request-Response Logging Not Working on Vertex AI Endpoints",
        "Question_tag_count":1,
        "Question_created_time":1685520000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Request-Response-Logging-Not-Working-on-Vertex-AI-Endpoints\/m-p\/598682#M2055",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":134,
        "Question_body":"I'm having trouble with request-response logging on Vertex AI endpoints. Previously, with AI Platform endpoints, I was able to enable request-response logging via gcloud, which would then store the request-response data in a chosen BigQuery table. However, with the newer Vertex AI endpoints, the process requires using the REST API to enable request-response logging.\n\nI followed the official documentation and made a PATCH request with the structure provided. Despite receiving a confirmation that the PATCH request was successful (as it displayed the updated configuration for the specified endpoint), the request-response logs are not appearing in BigQuery.\n\nWith the AI Platform endpoints, enabling logging resulted in a corresponding BigQuery insert job for every prediction request. However, with Vertex AI endpoints, no such job is created even after enabling the logging.\n\nHere's the PATCH request I used:\n\n\n\ncurl -X PATCH \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application\/json\" \\\n\"https:\/\/$REGION-aiplatform.googleapis.com\/v1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID\"\n  -d '{\n      \"predict_request_response_logging_config\": {\n      \"enabled\": true,\n        \"sampling_rate\": 1,\n        \"bigquery_destination\": {\n          \"output_uri\": \"bq:\/\/PROJECT_ID.DATASET_NAME.TABLE_NAME\"\n        }\n      }\n  }'\n\n\u00a0\n\nI've also tried using gcloud ai and gcloud beta ai-platform versions update commands, but these do not recognize\/find the endpoint. Furthermore, I have also created the endpoint with the above config using a POST request, which also did not work.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Image size for Google AutoML Training",
        "Question_tag_count":3,
        "Question_created_time":1673287800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Image-size-for-Google-AutoML-Training\/m-p\/508426#M1035",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":134,
        "Question_body":"Hi, I have some images which are lager than\u00a0\u00a01024 pixels by 1024 pixels.\nFollowing this document:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/image-data\/object-detection\/prepare-data\nImage size should be\u00a01024 pixels by 1024 pixels suggested maximum. If I resize images, the quality of images is low.\n\nWhat should I do?\nThank you and\nRegards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"3D object detection using mobile camera or 3D scanner using cloud vision",
        "Question_tag_count":4,
        "Question_created_time":1637628240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/3D-object-detection-using-mobile-camera-or-3D-scanner-using\/m-p\/176320#M90",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"The use case is, I want detect the 3d model through mobile camera or 3d scanner with dimensions to verify the scanned model is available or not in cloud storage. If model is not available it should list the model with approx model with percentage.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error: Input sets do not have the same labels.",
        "Question_tag_count":1,
        "Question_created_time":1671031140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-Input-sets-do-not-have-the-same-labels\/m-p\/499698#M942",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":179,
        "Question_body":"I have tried to train a model with over 50,000 images and 227 labels (with bounding boxes) I loaded the images using a csv file with each image left as UNASSIGNED for ML use.\u00a0\n\nEach time I try to train it, it gets to the 28 minute mark and errors out with\u00a0Training pipeline failed with error message:\u00a0Input sets do not have the same labels.\n\nI am at my wits end trying to work out what is the issue. I cannot find anyone else with the same issue (or I have found one, but there was no solution) and no matter what I do, it errors out at the same point (I have removed some labels etc)\u00a0\n\nDoes anyone know what the issue is (with more detail) so I can try to fix it? I assume it is talking about the\u00a0\n\nDataset and the Annotation set, but I cannot work out what is causing the issue.\n\u00a0\nI have followed all bounding box details, but to no avail. Any help would be appreciated. I am on the trial at the moment, so do not want to pay for support until I have",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Triton on Vertex AI does not support multiple models?",
        "Question_tag_count":2,
        "Question_created_time":1661411700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Triton-on-Vertex-AI-does-not-support-multiple-models\/m-p\/459822#M533",
        "Question_answer_count":6,
        "Question_score_count":2,
        "Question_view_count":454,
        "Question_body":"Currently, I want to deploy a Triton server to Vertex AI endpoint. However I received this error message.\n\n\"failed to start Vertex AI service: Invalid argument - Expect the model repository contains only a single model if default model is not specified\"\n\nIs this mean that the Triton server deploy only support one model? It is different from what I have read in this document about concurrent model execution\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-nvidia-triton",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Auto ML Edge failing if highest accuracy option is selected",
        "Question_tag_count":2,
        "Question_created_time":1667236080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Auto-ML-Edge-failing-if-highest-accuracy-option-is-selected\/m-p\/484251#M708",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":58,
        "Question_body":"Hello all,\n\nI am doing an automl training for edge ad if I pick the higher accuracy option training fails after about 3 hours with the following error. Training completes with no issues if I pick the best trade-off option. I have opened a ticket but has received zero support from Google so posting here. Has anyone seen this issue and know how to fix it?\u2003I have tried multiple times with the same error.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Idle shutdown for user-managed notebook (vertex-AI)",
        "Question_tag_count":1,
        "Question_created_time":1654153800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Idle-shutdown-for-user-managed-notebook-vertex-AI\/m-p\/428171#M366",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":970,
        "Question_body":"There are two types of notebooks in Vertex-AI\n\n1) managed notebook:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/introduction\n\n2) user-managed notebook:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/introduction\n\nI see that the former has a useful function called \"idle shutdown\" that\u00a0help manage costs:\u00a0managed notebooks instances shut down after being idle for a specific time period by default.\n\nWhy we didn't make it available for user-managed notebook as well? Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How can I deploy a pretrained fasttext model?",
        "Question_tag_count":2,
        "Question_created_time":1647135060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-deploy-a-pretrained-fasttext-model\/m-p\/403114#M235",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":401,
        "Question_body":"Hi, I have this code :\u00a0\n\n\"\n\nimport fasttext\nft = fasttext.load_model('pretrained model location')\n\"\nIs there any way I could deploy this model using the google cloud platform? I've been looking around and there doesn't seem to be a way to do it. When I want to create a model it wants me to choose scikit or tensorflow but nothing allows me to deploy this model. Can anyone help? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Need help for compute engine pricing",
        "Question_tag_count":1,
        "Question_created_time":1662441060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Need-help-for-compute-engine-pricing\/m-p\/463295#M562",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"GPU:\u00a0nvidia-a100-80gb has no pricing but\u00a0\u00a0nvidia-tesla-a100 has",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI scientist",
        "Question_tag_count":3,
        "Question_created_time":1655652120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-scientist\/m-p\/432867#M381",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"I'm from Ukraine. Therefore, I write with the help of a translator. I immediately apologize for any mistakes.\n\nI am a doctor. I am interested in many areas of science that are related to medicine. But because of their volume and complexity, it is impossible to learn by one person.\n\nI propose to create an AI that will analyze information on the Internet (video lectures, articles, books, audio books, images ...) and find relationships. For example, the electrophysical properties of DNA are analyzed through all known theories of physics. And a concrete example: Academician P. Garyaev's \"Linguistic Wave Genome\" through V. Atsyukovsky's \"Ether Theory\".\n\nThis tool needs to be made multifunctional and accessible to all users. This will revolutionize science by combining all knowledge.\nIt is important that there is a convenient voice interface and a personal account where studies are saved.\n\nThank you for attention. Sincerely, Sukhachov Denis.\n\n\u042f \u0441 \u0423\u043a\u0440\u0430\u0438\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043f\u0438\u0448\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0447\u0438\u043a\u0430. \u0421\u0440\u0430\u0437\u0443 \u0438\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.\n\n\u042f \u0434\u043e\u043a\u0442\u043e\u0440. \u041c\u0435\u043d\u044f \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u043c\u043d\u043e\u0433\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0443\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u043e\u0439. \u041d\u043e \u0438\u0437-\u0437\u0430 \u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u0430 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438\u0445 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0432\u043e\u0438\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0443.\n\n\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0418\u0418, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 (\u0432\u0438\u0434\u0435\u043e\u043b\u0435\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043d\u0438\u0433\u0438, \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f...) \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0414\u041d\u041a \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u043e\u0440\u0438\u0439. \u0418 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u00ab\u041b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u043e\u043b\u043d\u043e\u0432\u043e\u0439 \u0433\u0435\u043d\u043e\u043c\u00bb \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u043a\u0430 \u041f. \u0413\u0430\u0440\u044f\u0435\u0432\u0430 \u0447\u0435\u0440\u0435\u0437 \u00ab\u0422\u0435\u043e\u0440\u0438\u044e \u044d\u0444\u0438\u0440\u0430\u00bb \u0412. \u0410\u0446\u044e\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e.\n\n\u042d\u0442\u043e\u0442 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u042d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u0442 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0432 \u043d\u0430\u0443\u043a\u0435, \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0435 \u0437\u043d\u0430\u043d\u0438\u044f.\n\u0412\u0430\u0436\u043d\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0438 \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430, \u0433\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0437\u0430\u043d\u044f\u0442\u0438\u044f.\n\n\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435. \u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0421\u0443\u0445\u0430\u0447\u0435\u0432 \u0414\u0435\u043d\u0438\u0441.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BigQuery ARIMA plus changes TIME_SERIES_DATA_COL data",
        "Question_tag_count":3,
        "Question_created_time":1689638940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQuery-ARIMA-plus-changes-TIME-SERIES-DATA-COL-data\/m-p\/613188#M2389",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"I am using ARIMA plus model to predict anomalies. My TIME_SERIES_DATA_COL parameter is number of incidents received per day and is integer value in input. But the ARIMA plus model in output converts\u00a0 this to fraction value in some rows which is incorrect as number of incident received cannot be fractional. How to correct this behavior. My Query is as follows:\n\n\u00a0\n\nCREATE OR REPLACE MODEL `test_inputs_arima.ARIMA_15mins_model`\nOPTIONS\n(MODEL_TYPE = 'ARIMA_PLUS',\nTIME_SERIES_TIMESTAMP_COL = 'opened_at',\nTIME_SERIES_DATA_COL = 'number',\nTIME_SERIES_ID_COL = 'Issue_Category',\nHOLIDAY_REGION = ['GLOBAL'])\nAS\nSELECT\nopened_at,\nnumber,\nIssue_Category\nFROM\ntest_inputs_arima.15_mins_7days;\n\nCREATE OR REPLACE TABLE test_inputs_arima.ARIMA_15mins_out\nAS\n(SELECT\nIssue_Category,opened_at,number,is_anomaly,lower_bound,upper_bound,anomaly_probability\nFROM\nML.DETECT_ANOMALIES(\n  MODEL `test_inputs_arima.ARIMA_15mins_model`,\n  STRUCT(0.9 AS anomaly_prob_threshold)\n));\n\nUpdate test_inputs_arima.ARIMA_15mins_out\n set is_anomaly = false\nwhere number<lower_bound;\n\n\nSELECT * FROM test_inputs_arima.ARIMA_15mins_out;",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech to text work",
        "Question_tag_count":9,
        "Question_created_time":1636330440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-work\/m-p\/175139#M79",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":374,
        "Question_body":"I am a user of Speech to Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually.\n\n\u00a0\n\nSo, in Russian it works fine, however, 20-30 percents of the words are incorrect. Moreover, there are no Russian punctuation at all. \u00a0So I get the speech to text transcript, then I create the perfect transcript out of this with correct words and punctuations.\n\n\u00a0\n\nAll I want to know is how I can improve Speech toText by using the perfect transcript I have already corrected? Where I can send that data to?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What is the difference between vertexai.preview.language_models and google-generativeai?",
        "Question_tag_count":2,
        "Question_created_time":1684940520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-is-the-difference-between-vertexai-preview-language-models\/m-p\/596702#M2016",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":460,
        "Question_body":"as asked in the title what is the difference between these two python packages? how can I get my hands on PaLM 2?",
        "Question_closed_time":1685372040000,
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@morpheus,\n\nWelcome to Google Cloud Community!\n\n1.\u00a0The google.generativeai is more general and can be used to perform a variety of generative AI tasks such as generation of text, embedding and chat services, whereas the vertexai.preview.language_models is used to access the preview language models of PALM API in vertex AI. (E.g. migration from Azure OpenAi to Palm API).\nFor more information you can check this link:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/generative-ai\/migrate-from-azure\nhttps:\/\/developers.generativ...\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/generative-ai\/learn\/overview\n\n2. Please note that it is still in public preview and currently it is only available in the United States. If you want to have access to this public preview, you can use this link to register in the waitlist:\u00a0https:\/\/makersuite.google.com\/waitlist\nFor more information about the public preivew, you can check this link:\nhttps:\/\/developers.generativeai.google\/guide\/preview_faq\n\nHope this helps!\n\n\n\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Export Vertex model error 400",
        "Question_tag_count":3,
        "Question_created_time":1686991080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Export-Vertex-model-error-400\/m-p\/604143#M2180",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":59,
        "Question_body":"Hi,\n\nhi try to export my vertex model. in the 3 case if i choose TF lite, container or Tensorflow, one time i choose the file in my bucket and i click export i have this error message:\u00a0Exporting artifact for model ` ` in format `tf-js` is not supported.\u00a0\n\nAny solution please ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"The DAG failed because some tasks failed. The failed tasks are: [exit-handler-1];....",
        "Question_tag_count":3,
        "Question_created_time":1685407560000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-DAG-failed-because-some-tasks-failed-The-failed-tasks-are\/m-p\/598103#M2045",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":407,
        "Question_body":"Vertex ai text classification was interrupted with this error. What does it mean and how can I resolve it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI endpoint updating model version",
        "Question_tag_count":2,
        "Question_created_time":1681069800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-endpoint-updating-model-version\/m-p\/542061#M1611",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":107,
        "Question_body":"Hi, currently I'm working on vertex ai pipelines using kfp. I'm confused on how to update my model version on the same endpoint(not updating model version on model registry) using python script, is there a way to do it? Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Generative AI Read File",
        "Question_tag_count":4,
        "Question_created_time":1688695200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Generative-AI-Read-File\/m-p\/609978#M2318",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":92,
        "Question_body":"Hi Team,\n\nI am looking for steps to create a generative AI mode that reads the file and prompt Q&A example.\nBasically, I am trying for chatbot that answers based on the input file contents.\n\nKindly help.\n\nThanks,\n\nRajavelu",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Adding Dialogflow es library to python virtualenv",
        "Question_tag_count":3,
        "Question_created_time":1675953780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Adding-Dialogflow-es-library-to-python-virtualenv\/m-p\/520568#M1260",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"First, thank you to those that contribute to this forum.\n\nI am having a hard time trying to get the library imported into my python environment. I've tried all different methods and nothing seems to work.\n\nI've upgraded my environment. My python version is 3.11. I have isolated the environment in its own virtualenv. I've tried uninstalling\/reinstalling. I attempted the different imports.\n\nI would greatly appreciate any direction, links, articles, kind words. Thank you.\n\nimport dialogflow_v2 as dialogflow\n# import google_cloud_dialogflow as dialogflow",
        "Question_closed_time":1676204700000,
        "Answer_score_count":0.0,
        "Answer_body":"Thank you for your reply. I had a few things off. I needed to install the SDK and write the following into my code:\u00a0 I was using the first two (commented out with #) and they didn't work perhaps because they were deprecated. Once I did this, I was able to connect to the agent.\n\n# import dialogflow_v2 as dialogflow\n# import google_cloud_dialogflow as dialogflow\nfrom google.cloud import dialogflow_v2beta1 as dialogflow\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"torch.cuda.is_available() returns False on Vertex AI",
        "Question_tag_count":3,
        "Question_created_time":1686146760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/torch-cuda-is-available-returns-False-on-Vertex-AI\/m-p\/601055#M2097",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":533,
        "Question_body":"I run a custom model on vertex AI. Model is a simple FastAPI app that loads a whisper model. The beginning\u00a0of the app looks like this.\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nif torch.cuda.is_available():\nprint(\"GPU is available =)\")\nmodel = whisper.load_model(model_name).cuda()\nelse:\nprint(\"GPU is not available =(\")\nmodel = whisper.load_model(model_name)\n\n\u00a0\n\n\u00a0\n\nWhen running on vertex AI\u00a0\n\n\u00a0\n\n\u00a0\n\ngcloud ai endpoints deploy-model [ENDPOINT_NAME] \\\n     --region=europe-west4 \\\n     --model=[MODEL_NAME] \\\n     --machine-type=n1-standard-2 \\\n     --accelerator=type=nvidia-tesla-t4,count=1 \\\n\n\u00a0\n\n\u00a0\n\n\ntorch.cuda.is_available() always returns false.\n\nThere is also a log message prio to that\n\n\n\u00a0\n\n\u00a0\n\n\/app\/.venv\/lib\/python3.10\/site-packages\/torch\/cuda\/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ..\/c10\/hip\/HIPFunctions.cpp:110.)\n\n\u00a0\n\n\u00a0\n\nCan you advice me a direction to look into. I'm running out of ideas how to set the app for the GPU support.\u00a0\n\nThis very same docker image works on Compute Engine Vm and can find nvidia drivers. Why can it not do it on Vertex AI.\n\nDocker base image is this btw\n\n\u00a0\n\n\u00a0\n\nFROM nvidia\/cuda:11.7.0-base-ubuntu22.04\n\nENV PYTHON_VERSION=3.10\nENV POETRY_VENV=\/app\/.venv\n\nRUN export DEBIAN_FRONTEND=noninteractive \\\n  && apt-get -qq update \\\n  && apt-get -qq install --no-install-recommends \\\n  python${PYTHON_VERSION} \\\n  python${PYTHON_VERSION}-venv \\\n  python3-pip \\\n  ffmpeg \\\n  && rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN ln -s -f \/usr\/bin\/python${PYTHON_VERSION} \/usr\/bin\/python3 && \\\n  ln -s -f \/usr\/bin\/python${PYTHON_VERSION} \/usr\/bin\/python && \\\n  ln -s -f \/usr\/bin\/pip3 \/usr\/bin\/pip\n\nRUN python3 -m venv $POETRY_VENV \\\n  && $POETRY_VENV\/bin\/pip install -U pip setuptools \\\n  && $POETRY_VENV\/bin\/pip install poetry\n\nENV PATH=\"${PATH}:${POETRY_VENV}\/bin\"\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN poetry config virtualenvs.in-project true\nRUN poetry install\n\nRUN $POETRY_VENV\/bin\/pip install torch==1.13.0 -f https:\/\/download.pytorch.org\/whl\/torch\n\nEXPOSE 8080\nENV PORT 8080\n\nCMD exec gunicorn --bind :${PORT} --workers 1 --threads 8 --timeout 0 app.webservice:app -k uvicorn.workers.UvicornWorker",
        "Question_closed_time":1686198120000,
        "Answer_score_count":1.0,
        "Answer_body":"I think it's best to use the official pytorch gpu image e.g. this:\nhttps:\/\/hub.docker.com\/layers\/pytorch\/pytorch\/1.13.1-cuda11.6-cudnn8-runtime\/images\/sha256-1e26efd42...\n\nJust make sure that you're not doing pip install pytorch again as that image already comes with Pytorch pre-installed with GPU set up, or you'll be overriding it and potentially disable GPUs.\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI Model Deployment Error",
        "Question_tag_count":1,
        "Question_created_time":1669872180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Model-Deployment-Error\/m-p\/495020#M885",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":450,
        "Question_body":"Hi, I just got started using vertex ai with google cloud console. I am trying to deploy this mode to an endpoint. https:\/\/tfhub.dev\/tensorflow\/efficientnet\/lite0\/feature-vector\/2 I successfully imported it into a google storage bucket and uploaded it to the model registry. However, when I attempt to deploy the model to an endpoint, I receive the following error.\n\nHello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to create endpoint \"Feature Vectors\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \n**path to project**\nError Messages: Model server terminated: model server container terminated: \nexit_code:       255\nreason: \"Error\"\nstarted_at {\n   seconds: 1669817118\n}\nfinished_at {\n   seconds: 1669817421\n}\n. Model server logs can be found at \n**some link**\n\nI have attempted to change the TensorFlow version and the folder that i import (I attempted to import the containing folder instead of the model folder) however nothing seems to help. Any suggestions would be greatly appreciated. Thank you!",
        "Question_closed_time":1676994480000,
        "Answer_score_count":0.0,
        "Answer_body":"Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \".\/efficientnet_lite0_feature-vector_2\/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to \/tmp\/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='.\/efficientnet_lite0_feature-vector_2\/info.meta')\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Using text recognition while also using object detection",
        "Question_tag_count":1,
        "Question_created_time":1668635280000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-text-recognition-while-also-using-object-detection\/m-p\/490329#M802",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":83,
        "Question_body":"Can you use visionAI text recognition while using object detection, or will they have to be two separate calls the Vertex AI?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to compare and evaluate all trial models in AutoML?",
        "Question_tag_count":2,
        "Question_created_time":1680716400000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-compare-and-evaluate-all-trial-models-in-AutoML\/m-p\/541098#M1596",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"I have ran an AutoML forecasting job to predict sales. The training job modeled 69 models (from trial models) and 9 of them were selected to create an ensemble model as final model.\n\nQ1: I want to evaluate and compare all trial models that autoML performed in the training, is it possible?\nQ2: From the logs, I have all 69 models with their hyperparameters, how do I recreate all these model and get another predictions over it?\n\nThank you for your interest and please let me know if you have more questions.\n\n\u00a0\n\nlogs from the trial training job with total 69 trial models and hyperparameters",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI: Getting a GRPC Exception when sending a prediction request in Java",
        "Question_tag_count":1,
        "Question_created_time":1686108720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Getting-a-GRPC-Exception-when-sending-a-prediction\/m-p\/600835#M2093",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":221,
        "Question_body":"Hi,\n\nI have deployed a custom model (from a Docker image) in a Vertex AI endpoint.\n\nWhen I try to get a prediction in Java with the following code:\n\n\u00a0\n\nprivate PredictResponse predict(String endpointId, String query, String project, String location) throws IOException {\n    try (PredictionServiceClient serviceClient = getPredictionServiceClient()) {\n        EndpointName endpointName = EndpointName.of(project, location, endpointId);\n        ListValue.Builder listValue = ListValue.newBuilder();\n        JsonFormat.parser().merge(query, listValue);\n        List<Value> instanceList = listValue.getValuesList();\n        PredictRequest request = PredictRequest.newBuilder()\n                .setEndpoint(endpointName.toString())\n                .addAllInstances(instanceList)\n                .build();\n        return serviceClient.predict(request);\n    }\n}\n\n\u00a0\n\nI got a\u00a0io.grpc.StatusRuntimeException: INTERNAL: RST_STREAM closed stream. HTTP\/2 error code: INTERNAL_ERROR.\n\nI see no error when I check the logs in the Google Cloud Console. It looks like the request has been processed without error.\n\nThe strange part is that everything is working if:\n\nI call another Vertex model endpoint (with a simpler model inside a Docker image)\nI use a HTTP REST request (with curl) to get the prediction from the initial model (instead of Java code)\n\nThe request in CLI works so it seems that the model+endpoint is working.\n\nThe request in Java to another endpoint is working, so the code seems correct.\n\nAny idea how to debug\/fix this?",
        "Question_closed_time":1687254540000,
        "Answer_score_count":0.0,
        "Answer_body":"I have found an open issue on this topic:\u00a0https:\/\/issuetracker.google.com\/issues\/234474507\n\nI have posted a workaround that also solves the bug in the Java SDK.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Palm2 not able to read urls?",
        "Question_tag_count":2,
        "Question_created_time":1684147860000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Palm2-not-able-to-read-urls\/m-p\/553579#M1915",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"I've tried this in GenerativeAI studio and via API, but it's either giving me a blank message, or not showing me via policy (in the studio).\n\n\"View this repo at <https:\/\/github.com\/aaronn\/slack-gpt>, what is it?\"\n\nHowever, this works perfectly on bard.google.com\n\nAny way to fix this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Exported edge model always returns the same values",
        "Question_tag_count":2,
        "Question_created_time":1674109260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Exported-edge-model-always-returns-the-same-values\/m-p\/511682#M1091",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":88,
        "Question_body":"Hi,\n\nI trained a binary classification model with AutoML, and I have exported it for use in a container.\n\nI have followed this article to the letter https:\/\/cloud.google.com\/automl-tables\/docs\/model-export and the prediction server is running OK. However, it doesn't matter what values I submit to it as the inputs, the output is always absolutely the same value.\n\nI should specify that when deploying the same model to a VertexAI endpoint, it works great. I thought the exported model and container was supposed to work the same as using an endpoint.\n\nAny ideas would be great.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Kubeflow pipeline fails on GKE cluster",
        "Question_tag_count":2,
        "Question_created_time":1675411680000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Kubeflow-pipeline-fails-on-GKE-cluster\/m-p\/517717#M1197",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":169,
        "Question_body":"Need to create a Kubeflow pipeline for ML use-cases on GKE cluster, currently working on recommendation.\u00a0Have made the Vertex AI pipeline for the same but vertex being serverless takes time to make the containers up and then run the pipeline.\u00a0We needed a platform where the kubeflow pipelines can be executed faster - especially during development the results are needed instantly so that we can change inputs or tune the model.\n\nAfter uploading the yaml file, creating a run for the pipeline. After the first component the file read fails to write on the given bucket with below error.\n\nFileNotFoundError: [Errno 2] No such file or directory: '\/gcs\/bucket\/reco_v2\/637705bf-381e-40fa-8597-91089e700aaf\/pipeline\/reco_v2\/637705bf-381e-40fa-8597-91089e700aaf\/get-dataframe\/df_path.csv'39F0203 13:57:24.664574\u00a0 \u00a0 \u00a0 18 main.go:50] Failed to execute component: exit status 140time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact \/tmp\/outputs\/test_df_path\/data\" argo=true error=\"stat \/tmp\/outputs\/test_df_path\/data: no such file or directory\"41time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact \/tmp\/outputs\/train_df_path\/data\" argo=true error=\"stat \/tmp\/outputs\/train_df_path\/data: no such file or directory\"42Error: exit status 1\n\nTried running the sample tutorial kubeflow pipelines, they are running fine and does not throw any such file write error.\n\nWe have created a cluster and integrated Kubeflow pipelines from the GCP marketplace -> https:\/\/console.cloud.google.com\/marketplace\/details\/google-cloud-ai-platform\/kubeflow-pipelines\n\nNeed to know if there is a version issue or code is not proper or anything else. Have tried running a kfp v2 code too(from tutorials online) that worked well too.\n\nThe kubeflow pipeline should have worked and saved outputs to the bucket given.\n\nPlease help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Vision API in Vertex AI?",
        "Question_tag_count":2,
        "Question_created_time":1669604340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-API-in-Vertex-AI\/m-p\/493648#M867",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":779,
        "Question_body":"Hi,\n\nI am a newbie in Google Cloud and i have an elementary conceptual question about the dependency between\u00a0Cloud Vision API and Vertex AI or the recently launched Vertex Vision AI.\n\nI have an app that makes predictions on images using Google Vision AI API\u00a0ImageAnnotatorClient()\u00a0\n\nIs this API going to be part of\u00a0 Vertex AI\u00a0 or Vertex Vision AI?\n\nOr in other words, should I modify the below code to make it part of Vertex AI\/Vertex Vision AI?\n\n\u00a0\n\nfrom google.cloud import vision\n\ndef detect_labels_uri(uri):\n    client = vision.ImageAnnotatorClient()\n    image = vision.Image()\n    image.source.image_uri = uri\n\n    response = client.label_detection(image=image)\n    labels = response.label_annotations\n    return(labels)",
        "Question_closed_time":1669717380000,
        "Answer_score_count":1.0,
        "Answer_body":"Vision API should be affected by Vertex Vision AI and your code should stay as is. Vision API is primarily used to detect vision features like\u00a0image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content. While Vertex Vision AI is an end to end solution to ingest, analyze and store video and image data.\u00a0Vertex AI Vision lets users build and deploy applications with a simplified UX.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Custom Training Job with custom container failed with error 'Cant find specification for module...'",
        "Question_tag_count":2,
        "Question_created_time":1675288800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-Training-Job-with-custom-container-failed-with-error-Cant\/m-p\/516766#M1172",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":161,
        "Question_body":"Hi everyone,\n\nFor some reason, my custom training Jobs with custom container keep failing on vertex AI but the local run is working fine (I verified by running with local run as indicated in the docs here. I also built the image and run it manually and it works fine).\u00a0\u00a0\n\nError log when running custom job:\u00a0\n\n<code>\n\n{\n\"insertId\": \"2s7rqvfjzoq4v\",\n\"jsonPayload\": {\n\"attrs\": {\n\"tag\": \"workerpool0-0\"\n},\n\"message\": \"\/opt\/conda\/bin\/python: Error while finding module specification for 'trainer.train' (ModuleNotFoundError: No module named 'trainer')\\n\",\n\"levelname\": \"ERROR\"\n},\n\n<\/code>",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cannot open jupyter notebook in vertexAI",
        "Question_tag_count":1,
        "Question_created_time":1688717040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-open-jupyter-notebook-in-vertexAI\/m-p\/610067#M2319",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":113,
        "Question_body":"I recently started to use GCP with the free trial. The interface and the functionalities are messed up. Often the texts collide with each other, and it also doesn't open Jupyter Notebook in Vertex AI. Even the customer support page looks like in the picture below. How can I fix that? Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How start Learn GENAI?",
        "Question_tag_count":2,
        "Question_created_time":1686210000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-start-Learn-GENAI\/m-p\/601296#M2106",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":191,
        "Question_body":"How to start learning GENAI using Google with free?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Does Vertex AI support multi model endpoints",
        "Question_tag_count":2,
        "Question_created_time":1625630160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-multi-model-endpoints\/m-p\/163169#M11",
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":0,
        "Question_body":"We have 100's of models and deploying each one to its independent endpoint is very expensive.We are looking for a way to deploy multiple models to a single endpoint.Our docker image will have all the models and we will be having custom logic to invoke the models based on the request from the endpoint.\n\nSimilar functionality is available in AWS SageMaker.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Workbench JSONL format",
        "Question_tag_count":3,
        "Question_created_time":1687442460000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Workbench-JSONL-format\/m-p\/605833#M2218",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":77,
        "Question_body":"I am trying to use Generative AI studio to tune a model. Can someone send me the example of JSONL file that I should use. I build following file and I am getting an error:\n\n\u00a0\n\n{\n'input_text':'This is input1','output_text':'This is output1'\n'input_text':'This is input2','output_text':'This is output2'\n'input_text':'This is input3','output_text':'This is output3'\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI create pipeline file content error",
        "Question_tag_count":2,
        "Question_created_time":1684882080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-pipeline-file-content-error\/m-p\/596343#M2009",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":154,
        "Question_body":"Have been trying to build a pipeline using several generic examples. Python created the following yaml file, but when I upload it into vertex ai pipeline page, I always get the following error \"Invalid File Content\".\n\n\u00a0\n\n\u00a0\n\napiVersion: argoproj.io\/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: tfrecord-processing-pipeline-\n  annotations: {pipelines.kubeflow.org\/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org\/pipeline_compilation_time: '2023-05-23T21:47:19.096627',\n    pipelines.kubeflow.org\/pipeline_spec: '{\"inputs\": [{\"default\": \"gs:\/\/aa.bb.cc.dd\/data\",\n      \"name\": \"tfrecord_dir\", \"optional\": true, \"type\": \"String\"}], \"name\": \"TFRecord\n      Processing Pipeline\"}'}\n  labels: {pipelines.kubeflow.org\/kfp_sdk_version: 1.8.22}\nspec:\n  entrypoint: tfrecord-processing-pipeline\n  templates:\n  - name: load-tfrecord-dataset\n    container:\n      args:\n      - \"\\n            import tensorflow as tf\\n            file_pattern = \\\"{{inputs.parameters.tfrecord_dir}}\\\"\\\n        \\ + '\/*.block'\\n            files = tf.io.gfile.glob(file_pattern)\\n     \\\n        \\       for file in files:\\n                dataset = tf.data.TFRecordDataset(file)\\n\\\n        \\            \"\n      command: [python, -c]\n      image: tensorflow\/tensorflow:2.6.0\n    inputs:\n      parameters:\n      - {name: tfrecord_dir}\n    metadata:\n      labels:\n        pipelines.kubeflow.org\/kfp_sdk_version: 1.8.22\n        pipelines.kubeflow.org\/pipeline-sdk-type: kfp\n        pipelines.kubeflow.org\/enable_caching: \"true\"\n  - name: tfrecord-processing-pipeline\n    inputs:\n      parameters:\n      - {name: tfrecord_dir}\n    dag:\n      tasks:\n      - name: load-tfrecord-dataset\n        template: load-tfrecord-dataset\n        arguments:\n          parameters:\n          - {name: tfrecord_dir, value: '{{inputs.parameters.tfrecord_dir}}'}\n  arguments:\n    parameters:\n    - {name: tfrecord_dir, value: 'gs:\/\/aa.bb.cc.dd\/data'}\n  serviceAccountName: pipeline-runner\n\n\u00a0\n\n\u00a0Any thoughts?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Authentication errors running vaictl in container",
        "Question_tag_count":1,
        "Question_created_time":1667833740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authentication-errors-running-vaictl-in-container\/m-p\/486888#M744",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":112,
        "Question_body":"I'm trying to run\u00a0vaictl on OSX inside a docker container based on these Vertex AI Vision instructions, but hitting the following auth error:\n\n\u00a0\n\nroot@dc89ced3ac47:\/# vaictl -p backyard-camera -l us-central1 -c application-cluster-0 -v list streams\nE1107 23:01:37.006251735     858 google_default_credentials.cc:434] Could not create google default credentials: {\"created\":\"@1667862097.001779783\",\"description\":\"Failed to load file\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/load_file.cc\",\"file_line\":72,\"filename\":\"\/root\/.config\/gcloud\/application_default_credentials.json\",\"referenced_errors\":[{\"created\":\"@1667862097.001778260\",\"description\":\"No such file or directory\",\"errno\":2,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/load_file.cc\",\"file_line\":45,\"os_error\":\"No such file or directory\",\"syscall\":\"fopen\"}]}\nE20221107 23:01:37.007882   858 resource_manager_app.cc:312] INVALID_ARGUMENT: Invalid credentials.; while listing streams; while listing streams\n\n\u00a0\n\nI've run gcloud auth login\u00a0in the container and saved the authorization code.\n\nAre there any extra steps needed to make this work?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Submit Job But Never Ran",
        "Question_tag_count":3,
        "Question_created_time":1684493940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Submit-Job-But-Never-Ran\/m-p\/554999#M1963",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"I am trying to test a bash script on the cloud, but when I submit it, it repeatedly switch between SCHEDULE and QUEUE and never ran. Here is the bash script that I am trying to run:\u00a0\n\n\u00a0\n\n#!\/bin\/bash\n\n\nsource \/opt\/conda\/etc\/profile.d\/conda.sh\nconda activate WaifuDiffusion\n\nexport FOLDER=\/home\/kannachan\/SeaSalt-Downloader\n\npython $FOLDER\/main.py -u \"https:\/\/danbooru.donmai.us\/posts?tags=mika_pikazo\" \\\n       --scraper danbooru \\\n       --filter tag_filter animated video \\\n       --saver folder mika_pikazo       \n\n\u00a0\n\n\nI config my job as follows:\n\n\u00a0\n\n{\n    \"taskGroups\": [\n        {\n            \"taskSpec\": {\n                \"runnables\": [\n                    {\n                       \"script\": {\n\t\t\t      \"text\": \"#!\/bin.bash\\n bash ..\/run.sh\" \n\t\t\t}\n                    }\n                ],\n                \"computeResource\": {\n                    \"cpuMilli\": 2000,\n                    \"memoryMib\": 16\n                },\n                \"maxRetryCount\": 2,\n                \"maxRunDuration\": \"3600s\"\n            },\n            \"taskCount\": 4,\n            \"parallelism\": 2\n        }\n    ],\n    \"allocationPolicy\": {\n        \"instances\": [\n            {\n                \"policy\": { \"machineType\": \"g2-standard-8\" }\n            }\n        ]\n    },\n    \"labels\": {\n        \"department\": \"ai\",\n        \"env\": \"web-scrapper\"\n    },\n    \"logsPolicy\": {\n        \"destination\": \"CLOUD_LOGGING\"\n    }\n}\n\n\u00a0\n\n\u00a0\n\nBoth run.sh and config.json is in the same directory. I ran:\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json\n\n\u00a0\n\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Exceeded limit 'QUOTA_FOR_INSTANCES' on resource 'dataflow-tabular-stats-and-e .... Limit: 24",
        "Question_tag_count":1,
        "Question_created_time":1684234980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Exceeded-limit-QUOTA-FOR-INSTANCES-on-resource-dataflow-tabular\/m-p\/553942#M1927",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":172,
        "Question_body":"I'm trying to run Vertex AI using the Multiclass3_mod.csv example and I get the error bellow:\n\nThe same happens with bank-marketing.csv example\n\nAny help?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow ES Bad update mask fields",
        "Question_tag_count":2,
        "Question_created_time":1677648180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-Bad-update-mask-fields\/m-p\/527470#M1362",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"Hello everyone,\n\nI am encountering an error while trying to update the text_to_speech_settings.enable_text_to_speech parameter using the Dialogflow API. The error message I'm getting is \"Bad update mask fields: [text_to_speech_settings.enable_text_to_speech]\".\n\nI'm not sure what is causing this error, and I've already checked my API request and made sure that the resource name and update mask are correct. Can someone please help me troubleshoot this issue?\n\nHere are some additional details about my setup:\n\n\u00a0\n\nThank you in advance for your help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"need help in figuring out how to make the Vertex AI DAG wait for the task to complete?",
        "Question_tag_count":1,
        "Question_created_time":1688986860000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/need-help-in-figuring-out-how-to-make-the-Vertex-AI-DAG-wait-for\/m-p\/610794#M2336",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":67,
        "Question_body":"Hi team,\n\n\u00a0\n\ni am trying to create the big query table in Vertex AI but the current implementation makes it difficult for the DAG to wait for the Big query table to be created and hence the DAG fails ultimately.\n\n\u00a0\n\ncan you tell me the way on how to wait for the resource creation before moving forward with the next step in DAG?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ContexualVersionConflict When Importing Aiplatform Package in Executor of Vertex AI Workbench",
        "Question_tag_count":3,
        "Question_created_time":1673220900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ContexualVersionConflict-When-Importing-Aiplatform-Package-in\/m-p\/508016#M1030",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":645,
        "Question_body":"Hi There,\n\nI had the following problem when importing aiplatform\u00a0package in executor of Vertex AI workbench. This issue did not occur when I manually run the code in the Vertex AI workbench. However, the error came when I set a executor to run my code on schedule.\n\n---------------------------------------------------------------------------\n\nHere is the script\n\nfrom google.cloud import aiplatform\n\n---------------------------------------------------------------------------\n\nHere is the error message:\n\nContextualVersionConflict                 Traceback (most recent call last)\n\/tmp\/ipykernel_295\/605285511.py in <module>\n     63 \n     64 # Google cloud\n---> 65 from google.cloud import aiplatform\n     66 from datetime import datetime\n     67 from google.cloud import bigquery\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     29 from google.auth.exceptions import GoogleAuthError\n     30 \n---> 31 from google.cloud.aiplatform import compat\n     32 from google.cloud.aiplatform.constants import base as constants\n     33 from google.cloud.aiplatform import utils\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/compat\/__init__.py in <module>\n     16 #\n     17 \n---> 18 from google.cloud.aiplatform.compat import services\n     19 from google.cloud.aiplatform.compat import types\n     20 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/compat\/services\/__init__.py in <module>\n     16 #\n     17 \n---> 18 from google.cloud.aiplatform_v1beta1.services.dataset_service import (     19     client as dataset_service_client_v1beta1,\n     20 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/__init__.py in <module>\n     15 #\n     16 \n---> 17 from .services.dataset_service import DatasetServiceClient\n     18 from .services.dataset_service import DatasetServiceAsyncClient\n     19 from .services.deployment_resource_pool_service import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/__init__.py in <module>\n     14 # limitations under the License.\n     15 #\n---> 16 from .client import DatasetServiceClient\n     17 from .async_client import DatasetServiceAsyncClient\n     18 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/client.py in <module>\n     55 from google.protobuf import struct_pb2  # type: ignore\n     56 from google.protobuf import timestamp_pb2  # type: ignore\n---> 57 from .transports.base import DatasetServiceTransport, DEFAULT_CLIENT_INFO\n     58 from .transports.grpc import DatasetServiceGrpcTransport\n     59 from .transports.grpc_asyncio import DatasetServiceGrpcAsyncIOTransport\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/transports\/__init__.py in <module>\n     17 from typing import Dict, Type\n     18 \n---> 19 from .base import DatasetServiceTransport\n     20 from .grpc import DatasetServiceGrpcTransport\n     21 from .grpc_asyncio import DatasetServiceGrpcAsyncIOTransport\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/transports\/base.py in <module>\n     40     DEFAULT_CLIENT_INFO = gapic_v1.client_info.ClientInfo(\n     41         gapic_version=pkg_resources.get_distribution(\n---> 42             \"google-cloud-aiplatform\",\n     43         ).version,\n     44     )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in get_distribution(dist)\n    469         dist = Requirement.parse(dist)\n    470     if isinstance(dist, Requirement):\n--> 471         dist = get_provider(dist)\n    472     if not isinstance(dist, Distribution):\n    473         raise TypeError(\"Expected string, Requirement, or Distribution\", dist)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in get_provider(moduleOrReq)\n    345     \"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\n    346     if isinstance(moduleOrReq, Requirement):\n--> 347         return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    348     try:\n    349         module = sys.modules[moduleOrReq]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in require(self, *requirements)\n    889         included, even if they were already activated in this working set.\n    890         \"\"\"\n--> 891         needed = self.resolve(parse_requirements(requirements))\n    892 \n    893         for dist in needed:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in resolve(self, requirements, env, installer, replace_conflicting, extras)\n    780                 # Oops, the \"best\" so far conflicts with a dependency\n    781                 dependent_req = required_by[req]\n--> 782                 raise VersionConflict(dist, req).with_context(dependent_req)\n    783 \n    784             # push the new requirements onto the stack\n\nContextualVersionConflict: (google-cloud-bigquery 3.3.6 (\/opt\/conda\/lib\/python3.7\/site-packages), Requirement.parse('google-cloud-bigquery<3.0.0dev,>=1.15.0'), {'google-cloud-aiplatform'})\n\n-----------------------------------------------------------------------------------------------------\n\nSeeking help from everyone. Thank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error 403 in Vertex Ai Workbench",
        "Question_tag_count":1,
        "Question_created_time":1678243800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-403-in-Vertex-Ai-Workbench\/m-p\/530061#M1390",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":384,
        "Question_body":"I am facing an issue where if I click on the \"OPEN JUPITER LAB\" link on my instance it directs me to a Error 403 page with the text: \"403.\u00a0That\u2019s an error.\u00a0That\u2019s all we know.\"\n\nWhat access am I missing?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What you think about CHATGPT",
        "Question_tag_count":1,
        "Question_created_time":1672903740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-you-think-about-CHATGPT\/m-p\/506958#M1023",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":132,
        "Question_body":"What do you all think about \"ChatGPT\" by open AI? Does it take the jobs of developers or it's just a kind of personal assistant who will make your job easier",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GPU Machines in Vertex AI Pipelines failing with RuntimeError: NCCL Error 2: unhandled system error",
        "Question_tag_count":1,
        "Question_created_time":1681302240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GPU-Machines-in-Vertex-AI-Pipelines-failing-with-RuntimeError\/m-p\/543230#M1645",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":127,
        "Question_body":"Hi AI platform, I am using Vertex AI Pipelines to train detection models, and a since today my pipelines are failing due to a NCCL error on the GPU machine, I am creating the task as follows:\n\n\n\ntrain_task = train_model(\n        centernet_container_trainer_uri=centernet_uri_task.output,\n        train_dataset=train_importer_task.output,\n        test_dataset=test_importer_task.output,\n        categories_json=categories_importer_task.output,\n        pretrained_model=pretrained_model.output,\n        num_iters=num_iters,\n        batch_size=batch_size,\n        lr=lr,\n        num_epochs=num_epochs,\n        lr_step=lr_step,\n        gpus=gpus,\n        num_workers=num_workers,\n        val_intervals=val_intervals,\n    )\n    train_task.set_display_name(\"Train model\").set_cpu_limit(\"12\").set_memory_limit(\n        \"170G\"\n    ).add_node_selector_constraint(\"NVIDIA_TESLA_A100\").set_gpu_limit(\n        \"2\"\n    ).set_env_variable(\n        name=\"NCCL_SHM_DISABLE\", value=\"1\"\n    )\n\nThen after a few minutes my training step fails due to:\n\nNCCL Error 2: unhandled system error\n\nWhich seems to be related to shm size on the container. I have tried setting and unsetting the variable `NCCL_SHM_DISABLE` but to no avail, any idea on how to get support for this issue?\u00a0\n\nThanks\n\nI am using vertex and kubeflow",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI User Managed Notebooks all broken?",
        "Question_tag_count":1,
        "Question_created_time":1689240000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-User-Managed-Notebooks-all-broken\/m-p\/612020#M2362",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"We are unable to run our existing Jupyter notebooks, and get an error \"Restarting notebook does not have enough resources available to fulfill the request. Retry later or try another zone in your configurations\". We have tried in multiple different regions and zones.\u00a0\n\nWe just tried spinning up a new Jupyter notebook, and that failed as well. Are other people seeing these errors?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Turn off text normalization in Speech to Text API",
        "Question_tag_count":3,
        "Question_created_time":1680747600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Turn-off-text-normalization-in-Speech-to-Text-API\/m-p\/541225#M1598",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"I am using Speech to Text API to transcribe audio files. I see that the output contains a lot of characters which might be possibly occurring due to inverse text normalization somewhere. Symbols like $ for dollars and other currency symbols and also numbers written in numeric format rather than words. Is there some option in RecognitionConfig which gives me verbatim output in words instead of numbers and symbols?\u00a0\n\nI see there's a \"transcriptNormalization\" option in the config but then I have to provide my own rules.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deploying AutoML tabular model changes feature column types to text",
        "Question_tag_count":3,
        "Question_created_time":1647314460000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deploying-AutoML-tabular-model-changes-feature-column-types-to\/m-p\/403658#M236",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":73,
        "Question_body":"I\u2019ve trained an AutoML tabular model using a pretty simple CSV file of numeric data. When I ran the training I ensured each feature column was set as numeric. When viewing the column meta data of the trained model, all columns show as numeric. However, when I deploy the model they all show as text and will only accept strings. What am I doing wrong?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Agent Assist - Simulator view - feature FAQ, article suggestion CHECKBOX disabled",
        "Question_tag_count":1,
        "Question_created_time":1674702180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Agent-Assist-Simulator-view-feature-FAQ-article\/m-p\/514223#M1143",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":129,
        "Question_body":"While working on simulator of Google Agent assist, the checkbox to enable FAQ or Article Suggestion or smart reply option is DISABLED.\n\nThe chat on simulator is working with virtual agent I have configured via Dialog flow with intents that I have built, but can't use agent assist feature since I am not able to enable it.\n\nPlease note I have configured smart reply, article suggestion and Knowledge Base option in the agent assist in my google project.\n\nLooking for help to fix the issue.",
        "Question_closed_time":1675226640000,
        "Answer_score_count":0.0,
        "Answer_body":"Smart reply, FAQ and Articlet Suggestion of google agent assist became visible after I disabled the \"Choose to use Dialogflow\" option in my selected \"Conversation Profile\".\n\nHowever there is another issue. The \"smart reply\"\/\"FAQ\"\/\"Article Suggestion\" though enabled and visible in UI but they are not working. I am not getting any suggestion or smart reply for the customer chat. Any idea what could be wrong ?\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"JupyterLab classic UX not Vertex Ai",
        "Question_tag_count":3,
        "Question_created_time":1688653500000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/JupyterLab-classic-UX-not-Vertex-Ai\/m-p\/609860#M2314",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":50,
        "Question_body":"I created Notebook\n\nvm_image_family = \"common-cpu-debian-11-py310\"\nand got Classic interface JupyterLab.\n\n\u00a0\n\nI saw a lot of guides and official docs with the Vertex Ai interface JupyterLab.\nI rebuilt a lot of time Notebook aka VM and still a classic interface without GCS, memory usage and etc.\n\u00a0\nHow I can set\u00a0Vertex Ai interface JupyterLab?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Usage of spot machines while training in Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1674129120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Usage-of-spot-machines-while-training-in-Vertex-AI\/m-p\/511862#M1094",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":587,
        "Question_body":"Hello GCP community, I have the following question, I am training in Vertex using a custom container, I am porting pipelines that were in Kubeflow to vertex and using this to train:\n\n\n\nfrom google.cloud import aiplatform\n\njob = aiplatform.CustomContainerTrainingJob(display_name=\"training-job\", container_uri=container_uri)\n# define training code arguments\ntraining_args = [\"--num-epochs\", \"2\", ]\nmodel = job.run(\nreplica_count=1,\nmachine_type=\"n1-standard-8\",\naccelerator_type=\"NVIDIA_TESLA_V100\",\naccelerator_count=1,\nargs=training_args,\nsync=False,\n)\n\nIt looks ok, but here is my question is there anyway in which I can do the training but in a SPOT machine to try to reduce my training costs.\n\nThanks!",
        "Question_closed_time":1674130140000,
        "Answer_score_count":1.0,
        "Answer_body":"Hi David\n\nNo unfortunately there is no support for spot \/ preemptible instances with Vertex AI.\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI predictions fail without details about the error",
        "Question_tag_count":2,
        "Question_created_time":1687496640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-predictions-fail-without-details-about-the-error\/m-p\/605995#M2219",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":126,
        "Question_body":"I am a very new Vertex AI user, and I started using it with very basic AutoML regression model. I performed evey action using web UI, but something goes wrong.\n\nBasically I uploaded my dataset, trained a new model and then deployed to an endpoint to make first predictions. For doing I used the web interface available in Deploy and use -> Model registry -> Deploy and test -> Test your model... but every time I try to make some prediction I get the following error \"The prediction did not succeed due to the following error:\", and no further detail is available... what can I do to debug the error and hopefully make some prediction?\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"best DS solution for the use case below",
        "Question_tag_count":2,
        "Question_created_time":1690257240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/best-DS-solution-for-the-use-case-below\/m-p\/615452#M2451",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":9,
        "Question_body":"we are looking to set a pipeline that will contain two layers , we are thinking a creating a dag that combines the two below layers, but we are still not sure what tools to use for the data science layer\n\ndata generation layer: Generation of a Table in bigquery containing around hundreds of thousands of rows (product) and around 26 fields based on a sql logic (bigquery)\u00a0 \u00a0, baring in mind that the number of rows might increase to millions in the near future\nData science layer :prediction of possible outcomes of each product\u00a0 \u00a0by a data science model in\u00a0 written in python.\u00a0 (for each product we need to predict what are the next possible stages\u00a0 for it),\u00a0there is a lot of\u00a0 computation done by the model which requires 20 types of gaussian mixture fittings, also the performance\u00a0 will depend on the amount of input products\/ output outcomes\u00a0\n\nThe solution would be running hourly , every day. Priorities of criterias whilst looking for a solution for the data science layer of the pipeline\u00a0 are as follows:\n\nInference :\u00a0possibility of making\u00a0 the model scale horizontally \u00a0( increase number of samples x ) or vertically (number of producst n ) \u00a0in order to \u00a0produce x possible outcomes for each single product , the scaling will be in the hand of the data scientist\nCosts\nPossibility of having a model registry ( similar to images registry , which will\u00a0 keep a history of artifacts of the models that can be deployed) \u00a0\nTraining whilst doing inference\nPossibility of giving the end user the choice of \u00a0input output (one or more specific product id as input\/and to choose \u00a0\u00a0the number of samples \u00a0as output for those input products)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BigQuery ML Evaluation Metrics",
        "Question_tag_count":1,
        "Question_created_time":1679911140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQuery-ML-Evaluation-Metrics\/m-p\/537516#M1512",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"Hi,\n\nWondering for BigQuery ML model evaluation, is it possible to pass custom metrics for model evaluation or model comparison purpose while using BigQuery. I am planning to do a binary classification and have some self defined metrics that I want to use to compare and pick the best model.\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Looking for a Tool to Refine Object Recognition AI Model on Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":1682227500000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Looking-for-a-Tool-to-Refine-Object-Recognition-AI-Model-on\/m-p\/546397#M1728",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"I would like to know if there is a tool that would allow me to refine the image object recognition artificial intelligence model developed on vertex AI that I made. I recognize several objects in an image.\nI would like to send thousands of images to this model and depending on the model's response, I can validate the response, or modify it by adding or removing the label frames.\nI would also like to be able to display the answers according to their scores.\nI don't know if Google offers such a tool or if I have to go through a specialized company.\n\nThank you for your help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About Vertex AI Billing",
        "Question_tag_count":1,
        "Question_created_time":1687821240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Vertex-AI-Billing\/m-p\/606790#M2238",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":147,
        "Question_body":"There are four things I would like to know about Vertex AI billing.\n\n\u00a0\n\n1. Is the 300$\/month fee for Vertex AI Tensorboard a one-time charge?\n2. How can I get a breakdown of Vertex AI fees?\u00a0I would like to know the fee per pipeline and endpoint.\n3. Is there any lag in billing? I started up Tensorboard instance on the 26th, but it looks like I was billed on the 25th.\n4.\u00a0If I activate Tensorboard on 6\/26, am I correct that I will not be billed until 7\/26?\nPlease tell me additionally, is Tensorboard instance pay-as-you-go billing? Is the cost per month 300$ no matter how much you use?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deleting a failed dataset",
        "Question_tag_count":2,
        "Question_created_time":1677567600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deleting-a-failed-dataset\/m-p\/527077#M1360",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":126,
        "Question_body":"Under the Vertex AI - a dataset failed to create due to a constraint applied to the organization. It does not allow for the deletion of the dataset, I attempted using python (Delete a dataset \u00a0|\u00a0 Vertex AI \u00a0|\u00a0 Google Cloud) and the response was -\u00a0\"...is in failure state and cannot be deleted. It will be deleted automatically after a few days.\"\u00a0\u00a0but it didn't delete. There is not a gcloud command to correct. Short of a support request..how can the dataset be removed as I foresee this occuring as others attempt experiments. I have addressed the issue with the constraint.",
        "Question_closed_time":1677681300000,
        "Answer_score_count":1.0,
        "Answer_body":"I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How to build a data to AI solution with BigQuery and Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1684756920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-build-a-data-to-AI-solution-with-BigQuery-and-Vertex-AI\/m-p\/595708#M1991",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":149,
        "Question_body":"In this article, learn how to build an end-to-end data to AI solution on Google Cloud, including a practical example of a real-time fraud detection system and the architecture behind it. You'll also discover how to train, deploy, and monitor machine learning models in production.\n\nThis article is based on a recent Cloud OnBoard session.\u00a0Register here to watch on demand.\u00a0\u00a0\u00a0\n\nIf you have any questions, please leave a comment on the blog (or below) and someone from the Community or Google Cloud team will be happy to help.\n\nRead the blog",
        "Question_closed_time":1684833900000,
        "Answer_score_count":2.0,
        "Answer_body":"Great read!\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"What's the most efficient way to load data for training?",
        "Question_tag_count":2,
        "Question_created_time":1685603700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-most-efficient-way-to-load-data-for-training\/m-p\/599080#M2059",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"I currently have a training task that loads sharded CSV files from GCS using TorchData library (training code in Pytorch).\n\n\u00a0\n\nHowever I notice that my GPU usage has like ~ 2-3 minutes of 0% utilisation after each epochs, which I presume is due to I\/O issues of streaming data from GCS and starving my GPU.\n\nWhat's the most efficient way of getting around this? Would it be to download all my files from GCS to my compute instance, then loading the data directly?",
        "Question_closed_time":1685985720000,
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@shengy90,\n\nWelcome to Google Cloud Community!\n\nYou can validate the following suggestions:\n\n1.\u00a0You try implementing WebDatasets. It shards and compiles multiple data files into POSIX tar archive files, it doesn't do any format conversion and the data format is the same in the tar file as it is on the disk, and it can be created with the tar command. WebDataset is a great way to achieve Sequential I\/O since it will read the individual files in the tar file. This will be helpful since the data is collected in GCS which is in a remote setting, it will provide faster I\/O of objects over the network and will reduce potential bottlenecks.\u00a0You can check this blog post for more information regarding WebDatasets:\u00a0https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/scaling-deep-learning-workloads-pytorc...\n\n2. Google Cloud Storage Fuse is used for accessing data on Cloud Storage for Vertex AI training. This will allow you to access Google Cloud Storage as a local file system which provides high throughput, by simply using the code:\u00a0\n\nfile = open('\/gcs\/bucket-name\/object-path', 'r')\n\nYou can use this link to learn more:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/code-requirements#fuse\n\u00a0\n3. Also for best performance, your bucket must reside in the region where you are performing the custom training. You can use this link to learn more:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/code-requirements#loading-data\n\n4. You can also use this blog as a guide on how to efficiently train with vertex AI, you can check the demonstration using this link:\u00a0https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/efficient-pytorch-training-with-vertex-ai\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How do you get access to bard Api",
        "Question_tag_count":1,
        "Question_created_time":1687292760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-you-get-access-to-bard-Api\/m-p\/605111#M2201",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":49,
        "Question_body":"How do you get access to bard Api",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error with \"import vertexai\"",
        "Question_tag_count":2,
        "Question_created_time":1683845940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-with-quot-import-vertexai-quot\/m-p\/552708#M1877",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi,\n\nI'm trying to use GCP: Vertex Ai. I already installed the Vertex AI SDK for Python package successfully.\n\nI can \"from google.cloud import aiplatform\" successfully. However, \"import vertexai\" caused error as follows: No module named 'vertexai'\n\nI also already installed the module vertexai in the linux machine successfully.\n\nWould you help to show me what caused the error? Thanks so much in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1658799480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-while-trying-to-get-explanation-from-custom-container\/m-p\/446817#M447",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":388,
        "Question_body":"Hi,\n\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get\u00a0explainable\u00a0predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\n\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\n\nEnvironment details\nGoogle Cloud Notebook\nPython version: 3.7.12\npip version: 21.3.1\ngoogle-cloud-aiplatform\u00a0version: 1.15.0\nReference\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\n\nexplanation-metadata.json\n\n(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)\n\n{\n    \"inputs\": {\n        \"A\": {},\n        \"B\": {}\n    },\n    \"outputs\": {\n        \"Y\": {}\n    }\n}\nModel upload with explanation parameters and metadata\n! gcloud ai models upload \\\n  --region=$REGION \\\n  --display-name=$MODEL_NAME \\\n  --container-image-uri=$PRED_IMAGE_URI \\\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\n  --explanation-method=sampled-shapley \\\n  --explanation-path-count=10 \\\n  --explanation-metadata-file=explanation-metadata.json\nPrediction\/Explanation Input\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\n# Prediction (works fine):\nendpoint.predict(instances=instances)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\n\n# Another example\ninstances_2 = [[1.1,20], [2.2,21]]\n# Prediction (works fine):\nendpoint.predict(instances=instances_2)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances_2) # Returns error\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\nPrediction Server (Flask)\n# Custom Flask server to serve online predictions\n# Input for prediction\nraw_input = request.get_json()\ninput = raw_input['instances']\ndf = pd.DataFrame(input, columns = ['A', 'B'])\n# Prediction from model (loaded from GCP bucket)\npredictions = model.predict(df).tolist() # [0, 1]\nresponse = jsonify({\"predictions\": predictions})\nreturn response\nStack trace of error (1)\n---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     49         try:\n---> 50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--> 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--> 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\n\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\n   1563             parameters=parameters,\n   1564             deployed_model_id=deployed_model_id,\n-> 1565             timeout=timeout,\n   1566         )\n   1567 \n\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\n    917             retry=retry,\n    918             timeout=timeout,\n--> 919             metadata=metadata,\n    920         )\n    921 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[\"metadata\"] = metadata\n    153 \n--> 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n---> 52             raise exceptions.from_grpc_error(exc) from exc\n     53 \n     54     return error_remapped_callable\n\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\n---------------------------------------------------------------------------\n# https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Pipeline: The replica workerpool0-0 exited with a non-zero status of 13",
        "Question_tag_count":3,
        "Question_created_time":1678245300000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Pipeline-The-replica-workerpool0-0-exited-with-a-non\/m-p\/530067#M1391",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":452,
        "Question_body":"I'm trying to run a vertex AI pipeline (AutoML) using default setting and required service account permission but every time i run that pipeline, i will get failed with above error.\u00a0\n\n\n\u00a0\n\n\nsame i had run with custom will success\u00a0but when running with AutoML for Tabular Classification \/ Regression it will get failed after long process time.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Infinte long process",
        "Question_tag_count":2,
        "Question_created_time":1686207720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Infinte-long-process\/m-p\/601274#M2103",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"Hi,\n\nI sent an audio for transcription, and didn't get the result yet (should be in a bucket)\n\nthis is the process id: 7346104001135815711\n\nany ideas how I can debug it and find why it fails?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Receiving quota error when trying to use bison chat model in Vertex AI",
        "Question_tag_count":3,
        "Question_created_time":1683790740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-bison-chat-model-in\/m-p\/552421#M1857",
        "Question_answer_count":43,
        "Question_score_count":7,
        "Question_view_count":0,
        "Question_body":"Hi, I want to try out the new bison chat model. However, when I'm asking anything I'm receiving this error:\u00a0\n\nQuota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: chat-bison. Please submit a quota increase request.",
        "Question_closed_time":1684921140000,
        "Answer_score_count":2.0,
        "Answer_body":"UPDATE: We have raised the default quotas for everyone.\u00a0 This roll out may take a day to reach everyone so.\u00a0 Thank you everyone for your patience and flagging this to us!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Method for Photo Analysis (AI\/Machine Learning\/OCR\/Others)",
        "Question_tag_count":3,
        "Question_created_time":1669877400000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Method-for-Photo-Analysis-AI-Machine-Learning-OCR-Others\/m-p\/495040#M887",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":60,
        "Question_body":"Hi,\n\nI am very new to Google Cloud Platform, here I attached a photo of a cable being cut and measured on measuring tape. From the photo, the dimension of the cable is 5500mm. Because I am not sure which method I can use, either AI? Machine Learning? OCR engine? or others?.\n\nThis photo was captured and stored in Google Drive (using Appsheet), is there any solution in GCP able to read from this photo and\u00a0\u00a0analyze it, finally tell this is 5500.\n\nAnyone from community that took part in such project or maybe a similar project able to enlighten me in this?\n\nThanks in advance for any valuable opinion!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to implement custom evaluation jobs (with custom metrics for custom learning tasks) in Vertex AI",
        "Question_tag_count":3,
        "Question_created_time":1684396920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-implement-custom-evaluation-jobs-with-custom-metrics-for\/m-p\/554593#M1954",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":90,
        "Question_body":"From what I can see in GCP's official docs:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/evaluation\/introduction\n\nEvaluation job comes with predefined metrics for predefined tasks. If I have a training task that's slightly more bespoke with metrics\/ objective outside the scope of the available tasks\/ metrics, how would I implement them?\n\nExamples of custom metrics are MMD or KL Divergence loss etc.\n\nMy current thinking is:\n\nCreate a custom evaluation app in a docker container\nCreate a CustomJob that runs my evaluation app\nOutput the evaluation artefacts (e.g. results, plots) to GCS etc?\n\nMany thanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud VM Instances",
        "Question_tag_count":7,
        "Question_created_time":1686348480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-VM-Instances\/m-p\/601789#M2121",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"hello everyone, I just sign up on google cloud yesterday and got a 300$ free trial also for 90 days but now when I am creating my very first new VM Instance I am getting an error \"The C2-CPUS-per-project-region quota maximum in region europe-west4 has been exceeded\" almost tries all regions, please anyone help me guide me.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Stable Diffusion 1.5 from Vertex AI's model garden: what parameters and instance values are allowed?",
        "Question_tag_count":2,
        "Question_created_time":1689109800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Stable-Diffusion-1-5-from-Vertex-AI-s-model-garden-what\/m-p\/611394#M2341",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":138,
        "Question_body":"I have deployed stable diffusion 1.5 from Vertex AI's model garden. I have an endpoint on which I am successfully running online inference with a request payload like the following:\n\n{ \"instances\": [{ \"prompt\": interesting_prompt }] }\n\nStable diffusion should be able to accept many parameters, including width\/height, sampling steps, n_samples\/batch size, etc.\u00a0 But I've been unable to figure out how to pass these parameters in a Vertex AI payload. It makes the most sense to me to pass them as key\/value pairs alongside the prompt in the same object in `instances`, but no key names besides prompt that I've tried have changed the output at all. I haven't found any documentation to support these use cases.\n\nHow can I pass these additional parameters for online inference?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"computational instances at the tool Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1660795320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/computational-instances-at-the-tool-Vertex-AI\/m-p\/455213#M514",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":72,
        "Question_body":"Hello,\nI have a question about the computational instances at the tool Vertex AI in the field of image classification. What are the underlying instances of the process or where can I find out? I'm looking for Information comparable to these syntax for example: Virtual Machine (CPU: Intel Xeon E5-2690 v3, 6 vCPUs, GPU: NVIDIA Tesla K80, 56 GB RAM, 380 GB SSD)\nThanks\nArndt",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Rate Limiting on Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1678375320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Rate-Limiting-on-Vertex-AI\/m-p\/530748#M1408",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":178,
        "Question_body":"I'm preparing to launch a prod endpoint using vertex ai automl text solution. I want to understand more prelaunch how many requests\/second my endpoint will handle.\u00a0\n\n\u00a0\n\nI see that\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/quotas\u00a0this implies I can run 30,000\/minute. Is that the limit or will things break down before then? Looking for some best practices.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Just curious, can I use cloud bigtable as a feature store instead of using vertex AI feature store?",
        "Question_tag_count":2,
        "Question_created_time":1656387780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Just-curious-can-I-use-cloud-bigtable-as-a-feature-store-instead\/m-p\/435633#M395",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":615,
        "Question_body":"I am trying to migrate my features table stored in bigquery to a feature store with lower latency. I'm choosing whether I should make use of vertex AI feature store or just cloud bigtable.\n\nMy features tables are <10MB, and it is used for real time prediction hence a database with low latency is sufficient.\n\nIm just wondering aside from pricing, and ease of exporting data (bigtable requires more steps than vertex ai feature store), what is the difference between the 2 options?\n\nAlso, what type of database (eg: bigtable or redis?) is vertex AI feature store behind the scenes, when I am creating the feature store using the web UI?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Getting charged for VertexAI but can't find resources to turn off.",
        "Question_tag_count":1,
        "Question_created_time":1689228720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-charged-for-VertexAI-but-can-t-find-resources-to-turn\/m-p\/611940#M2373",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":46,
        "Question_body":"I'm being charged about $500 a month for VertexAI services but I can't for the life of me locate which services are turned on.\n\nI used Google Colab to run through some notebooks, and I think I upgraded to a certain more dedicated system there, but I can't locate where to downgrade this, or even know which notebook it was.\n\nCan someone help?!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How Should I Cite the AutoML Model for Tabular Data?",
        "Question_tag_count":2,
        "Question_created_time":1687803840000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-Should-I-Cite-the-AutoML-Model-for-Tabular-Data\/m-p\/606699#M2233",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":37,
        "Question_body":"Hello Community,\n\nI've been utilizing Google AutoML's Tabular-form dataset classifier for my study and am trying to find an official citation for the model or method. I've come across papers that cite Google's AI blog (Lu, Y. An end-to-end AutoML solution for tabular data at KaggleDays. Google AI Blog, 2019. URL: (\u00a0http:\/\/ai.googleblog.com\/2019\/05\/an-end-to-end-automl-solution-for.html ) or the website URL ( https:\/\/cloud.google.com\/vertex-ai\/docs\/training-overview#tabular ), but I'm uncertain as to which is the correct approach. It would be much appreciated if anyone could provide insight on this matter.\n\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Scheduling Vertex AI Pipeline - Error 503",
        "Question_tag_count":2,
        "Question_created_time":1683717540000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Scheduling-Vertex-AI-Pipeline-Error-503\/m-p\/552124#M1849",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":244,
        "Question_body":"Hi,\n\nI successfully trained and deployed a pipeline in Vertex AI using Kubeflow for a retrieval model, Two Towers.\n\nNow I want to schedule this pipeline run every 8 minutes. Here's my code:\n\n\u00a0\n\nfrom kfp.v2.google.client import AIPlatformClient\napi_client = AIPlatformClient(project_id='my-project', region='us-central1')\n\napi_client.create_schedule_from_job_spec(\n    job_spec_path='vacantes_pipeline.json',\n    schedule=\"\/8 * * * *\", # every 8 minutes\n    time_zone='America\/Sao_Paulo',\n    parameter_values={\n        \"epochs_\": 5,\n    \"embed_length\":768,  \n        \"maxsplit_\" : 130\n    }\n)\n\n\u00a0\n\nThe JSON is successfuly created, but the Scheduler Job fails immediately.\n\nLogging tells me the httpRequest has an error 503 plus:\n\n\u00a0\n\njsonPayload: {\n@type: \"type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished\"\njobName: \"projects\/my-project\/locations\/us-central1\/jobs\/pipeline_vacantes-pipeline-with-deployment_c7e98a8f_59-14-a-a-a\"\nstatus: \"UNAVAILABLE\"\ntargetType: \"HTTP\"\nurl: \"https:\/\/us-central1-bogotatrabaja.cloudfunctions.net\/templated_http_request-v1\"\n}\n\n\u00a0\n\nAny ideas on how to solve this issue ?",
        "Question_closed_time":1683971520000,
        "Answer_score_count":0.0,
        "Answer_body":"I solved with Compute Engine and cron jobs.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Unable to deploy model to endpoint",
        "Question_tag_count":2,
        "Question_created_time":1684285800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-deploy-model-to-endpoint\/m-p\/554155#M1939",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":247,
        "Question_body":"Hi,\n\nI am trying to deploy a simple model on an endpoint in order to start making predictions.\n\nI followed these steps:\n\n1. Create the model, create a docker image, push it into Artifact Registry\n\n2. Upload the model to Vertex AI:\n\n\u00a0\n\n! gcloud ai models upload --container-image-uri=<REGION>-docker.pkg.dev\/<PROJECT>\/<REPOS>\/<MODEL>:latest --region=<REGION> --display-name=my-model\n\n\u00a0\n\n3. Create an endpoint on Vertex AI:\n\n\u00a0\n\n! gcloud ai endpoints create --display-name=my-model-endpoint --region=<REGION>\n\n\u00a0\n\n4. Deploy the model to the endpoint:\n\n\u00a0\n\n! gcloud ai endpoints deploy-model <ENDPOINT_ID> --model <MODEL_ID> -display-name my-model --traffic-split=0=100 --region <REGION> --machine-type=n1-standard-8 --enable-access-logging \n\n\u00a0\n\nHere's the message:\n\nModel server terminated: model server container terminated: exit_code: 0 reason: \"Completed\" started_at { seconds: 1684309447 } finished_at { seconds: 1684309449 } . Model server logs can be found at xxxxx\n\nWhen i check the logs:\u00a0\n\nAs you can see i have no errors.\n\nWhat i tried:\n\n- upsized the machine-type\u00a0\n\n- deployed with a python script\u00a0\n\n- deployed manually using the portal\u00a0\u00a0\n\n- changed my model to a simple linear regression to test",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Got Welcome email, but doesn't have permission Makersuite.google.com!",
        "Question_tag_count":1,
        "Question_created_time":1684311660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Got-Welcome-email-but-doesn-t-have-permission-Makersuite-google\/m-p\/554306#M1942",
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":249,
        "Question_body":"I got an email with the following content.\n\n\u00a0\n\nWelcome to the\nPaLM API and MakerSuite\nThanks for putting your name on the waitlist for access to the PaLM API and MakerSuite. We\u2019ve been opening up to small groups of people gradually and now it\u2019s your turn. Open MakerSuite to enable API access or start prototyping today.\n\nBut when I click on the \"Visit MakerSuite\", I get a message saying that my gmail account\u00a0doesn't have permission to see this page.\n\nTried adding my email back into the waiting list, but got \"You're all set!\" page with the \"Get Started\" button. Clicking on \"Get Started\" results in the same error message\n\nAny idea how to resolve this issue?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Should I custom split my image data?",
        "Question_tag_count":2,
        "Question_created_time":1625459160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Should-I-custom-split-my-image-data\/m-p\/163031#M10",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":502,
        "Question_body":"Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML?\n\n\u00a0\n\nAnd what difference does it make?",
        "Question_closed_time":1626243360000,
        "Answer_score_count":1.0,
        "Answer_body":"Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training\/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps:\/\/cloud.google.com\/vision\/automl\/docs\/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Download \/ Print Evaluation report",
        "Question_tag_count":2,
        "Question_created_time":1689576660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Download-Print-Evaluation-report\/m-p\/612912#M2384",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":54,
        "Question_body":"How can I print or download all of my evaluation data, and precision curves for publication?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Language Tuning Model - RESOURCE EXHAUSTED message",
        "Question_tag_count":2,
        "Question_created_time":1687946640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Language-Tuning-Model-RESOURCE-EXHAUSTED-message\/m-p\/607459#M2255",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"While tuning language model, the process stops every time at llm model tuning step with the below error\u00a0 code=resource_exhausted, message=the following quota metrics exceed quota limits:\"\n\nWhen the Quota status is checked, the assigned quota limit for\u00a0restricted_image_training_tpu_v3_pod\u00a0\u00a0is 0. How to increase this value?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard API",
        "Question_tag_count":1,
        "Question_created_time":1684669440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-API\/m-p\/595340#M1988",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":157,
        "Question_body":"Hello, I know that there's the PaLM-2 API. My question is if there will ever be a BARD API. So a model with the ability to surf the internet.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"User level Gen AI restriction",
        "Question_tag_count":3,
        "Question_created_time":1689654180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/User-level-Gen-AI-restriction\/m-p\/613219#M2391",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":28,
        "Question_body":"Hi All,\n\nI created a chatbot using generative AI model chat-bison with some business pdf data and created a customized UI.\n\nNow I want to restrict the user access. Like only few users can get response to the Project A\u00a0 Q&A in chat and other group can see project B document content in Q&A. How to bring user authorization concept into this?\n\nKindly help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google vertex AI support is terrible",
        "Question_tag_count":1,
        "Question_created_time":1669150740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-vertex-AI-support-is-terrible\/m-p\/492001#M846",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":130,
        "Question_body":"I usually don't post things like this but I have been trying to work with Google support for Vertex AI for a while. It has been a month on a P2 ticket and no help or support so far. I guess I don't pay the big bucks to get Google's attention. So just wanted to post a warning that is helpful hopefully. Use Vertex AI at your own risk. If something fails you are on your own.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
        "Question_tag_count":2,
        "Question_created_time":1638115980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI\/m-p\/176629#M96",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":102,
        "Question_body":"I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
        "Question_closed_time":1638190740000,
        "Answer_score_count":1.0,
        "Answer_body":"Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Getting error in fetching the text-bison model for custom tuning using python.",
        "Question_tag_count":7,
        "Question_created_time":1687253640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-error-in-fetching-the-text-bison-model-for-custom-tuning\/m-p\/604919#M2197",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":288,
        "Question_body":"I am trying to tune a model using Python. Incorporated the code that has been given in the documentation as below:\n\n\u00a0\n\nfrom __future__ import annotations\n\n\n\nfrom google.auth import default\nimport pandas as pd\nimport vertexai\nfrom vertexai.preview.language_models import TextGenerationModel\n\n\ncredentials, _ = default(scopes=[\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"])\n\n\n\ndef tuning(\n\u00a0 \u00a0 project_id: str,\n\u00a0 \u00a0 location: str,\n\u00a0 \u00a0 training_data: pd.DataFrame | str,\n\u00a0 \u00a0 train_steps: int = 10,\n) -> None:\n\u00a0 \u00a0 \"\"\"Tune a new model, based on a prompt-response data.\n\u00a0 \u00a0 \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n\u00a0 \u00a0 (for example: training_data=f'gs:\/\/bb_bucket2023\/Vertex AI data (3).jsonl'), or a pandas\n\u00a0 \u00a0 DataFrame. Each training example should be JSONL record with two keys, for\n\u00a0 \u00a0 example:\n\u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \"input_text\": <input prompt>,\n\u00a0 \u00a0 \u00a0 \u00a0 \"output_text\": <associated output>\n\u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 or the pandas DataFame should contain two columns:\n\u00a0 \u00a0 \u00a0 ['input_text', 'output_text']\n\u00a0 \u00a0 with rows for each training example.\n\u00a0 \u00a0 Args:\n\u00a0 \u00a0 \u00a0 project_id: GCP Project ID, used to initialize vertexai\n\u00a0 \u00a0 \u00a0 location: GCP Region, used to initialize vertexai\n\u00a0 \u00a0 \u00a0 training_data: GCS URI of jsonl file or pandas dataframe of training data\n\u00a0 \u00a0 \u00a0 train_steps: Number of training steps to use when tuning the model.\n\u00a0 \u00a0 \"\"\"\n\u00a0 \u00a0 vertexai.init(project=project_id, location=location, credentials=credentials)\n\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n\n\n\u00a0 \u00a0 model.tune_model(\n\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\n\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\n\u00a0 \u00a0 \u00a0 \u00a0 train_steps=train_steps,\n\u00a0 \u00a0 \u00a0 \u00a0 tuning_job_location=\"europe-west4\", \u00a0# Only supported in europe-west4 for Public Preview\n\u00a0 \u00a0 \u00a0 \u00a0 tuned_model_location=location,\n\u00a0 \u00a0 )\n\n\n\u00a0 \u00a0 print(model._job.status)\n\u00a0 \u00a0 # [END aiplatform_sdk_tuning]\n\u00a0 \u00a0 return model\n\n\n\nif __name__ == \"__main__\":\n\u00a0 \u00a0 tuning(project_id=\"xxxxxxxxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 location=\"europe-west4\", \u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 training_data='gs:\/\/xxxxxx\/Vertex AI data (3).jsonl')\n\nAfter running this code I get an error as below:\n\n\n---------------------------------------------------------------------------\n\n_InactiveRpcError                         Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     71         try:\n---> 72             return callable_(*args, **kwargs)\n     73         except grpc.RpcError as exc:\n\n\n11 frames\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Publisher Model `publishers\/google\/models\/text-bison@001` is not found.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.8.95:443 {created_time:\"2023-06-20T16:09:33.304268493+00:00\", grpc_status:5, grpc_message:\"Publisher Model `publishers\/google\/models\/text-bison@001` is not found.\"}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\n\nNotFound                                  Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     72             return callable_(*args, **kwargs)\n     73         except grpc.RpcError as exc:\n---> 74             raise exceptions.from_grpc_error(exc) from exc\n     75 \n     76     return error_remapped_callable\n\n\nNotFound: 404 Publisher Model `publishers\/google\/models\/text-bison@001` is not found.\n\nPlease guide me through the steps that can resolve the above issue.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Code Chat Generative AI model",
        "Question_tag_count":3,
        "Question_created_time":1689549240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Code-Chat-Generative-AI-model\/m-p\/612812#M2380",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":48,
        "Question_body":"Hi All,\n\nI would like to train Gen AI model which can answer\/create code based on the table fed as input.\nHow to achieve this?\n\nExample: If i ask prompt to write a sql query to join two tables. It should join two tables from the bigquery table which i mentioned and the required fields.\n\nThanks,\n\nRajavelu",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Steps to get Real life data into the features section",
        "Question_tag_count":2,
        "Question_created_time":1651246740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Steps-to-get-Real-life-data-into-the-features-section\/m-p\/418695#M297",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":68,
        "Question_body":"Hi,\u00a0\n\nI already create a tabular classification model using AutoML. All ready have the features created in a Goolge colab and now I need to get the real life data from a Public API to feed the features, then pass the features into the Model and finally get the classifications.\n\nThe question is are the steps and which tools should I use in order to connect to the API in order to receive the real time data?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to correct labels that have been read",
        "Question_tag_count":2,
        "Question_created_time":1671487980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-correct-labels-that-have-been-read\/m-p\/501393#M970",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":95,
        "Question_body":"Hello,\u00a0\n\nI used Document AI form processor to convert pdf file.\n\nI hit the confirm button to edit the label to the correct one because it was not reading correctly, but nothing responded.\n\u203b I`ve added the text and tried to save it.\n\nIt seems to be a javascript error, but I don't think this has happened before.\n\nDo you have any info on this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can't enable generativelanguage API even on Admin account for GCP",
        "Question_tag_count":2,
        "Question_created_time":1684192200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-t-enable-generativelanguage-API-even-on-Admin-account-for\/m-p\/553710#M1921",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":37,
        "Question_body":"Is there a way to resolve this? As I need it enable for use with Langchain.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"reCAPTCHA IA",
        "Question_tag_count":1,
        "Question_created_time":1675771980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/reCAPTCHA-IA\/m-p\/519498#M1242",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":77,
        "Question_body":"Hello,\u00a0Captcha has an artificial intelligence model that allows it to learn to improve security scores and thus know when it asks for validation or not? my question is related to a user who always accesses from the same pc and the same ip to a site that has reCAPTCHA validation and always asks for validation, is it possible to avoid this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GETTING A PART OF PROMPT IN COMPLETION WHEN FINETUNING TEXT DAVINCI MODEL",
        "Question_tag_count":1,
        "Question_created_time":1686603720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GETTING-A-PART-OF-PROMPT-IN-COMPLETION-WHEN-FINETUNING-TEXT\/m-p\/602427#M2138",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":21,
        "Question_body":"Hi,I tried fine tuning the text davinci model with a classification data set. Although the model is trained multiple number if times im getting a part of prompt in the completion.\u00a0For example if the prompt is given as \"Classify the following text into one of the following classes: [spam,ham] Text:\u00a0 Are you this much buzy\" the completion is given as : \"of the following classes: [spam,ham] Text:\u00a0 Are you this much buzy\"\u00a0 ham\". Kindly help with this",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How should the input JSONL look for a batch prediction job?",
        "Question_tag_count":2,
        "Question_created_time":1649068920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-should-the-input-JSONL-look-for-a-batch-prediction-job\/m-p\/410193#M256",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":316,
        "Question_body":"I can't find any examples online of how an input jsonl is supposed to look for a batch training job. When I tried with this:\n\n\u00a0\n\n{'instances': {'mimeType': 'text\/plain', 'content': '0'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '1'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '1'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '2'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '8'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'a'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'a'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'h'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'q'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 's'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'y'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'y'}}\n\n\u00a0\n\nI got an error email saying\u00a0\n\n\u00a0\n\nError Messages: BatchPrediction could not start because no valid instances\u00a0\nwere found in the input file.\n\n\u00a0\n\nIs there some other way this should look for it to work? Maybe like\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n{\n\n'instances': [\n\n{'mimeType': 'text\/plain', 'content': 'a'}\n\n{'mimeType': 'text\/plain', 'content': 'bc'}\n\n{'mimeType': 'text\/plain', 'content': 'de'}]\n\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Quota not listed in Vertex AI pipelines?",
        "Question_tag_count":3,
        "Question_created_time":1675635060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-not-listed-in-Vertex-AI-pipelines\/m-p\/518466#M1211",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":388,
        "Question_body":"Hi everyone I have a step in vertex ai pipelines that looks like this:\n\ntranscribe_task = transcribe_audios(audio_files=download_task.output)\ntranscribe_task.set_cpu_limit(\"2\").set_memory_limit(\n\"8G\"\n).add_node_selector_constraint(\"NVIDIA_TESLA_T4\").set_gpu_limit(\"1\")\n\nyet that task is not executed due to:\n\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\u00a0aiplatform.googleapis.com\/custom_model_training_nvidia_t4_gpus, cause=null; Failed to create custom job for the task.\n\nBut that quota is not listed anywhere in the quota manager, how can I enable GPU in Vertex AI pipelines?",
        "Question_closed_time":1675667340000,
        "Answer_score_count":1.0,
        "Answer_body":"I solved it, it is quite not easy to find:\n\nSo for anyone with the same problem, go to Quotas and use the following filters:\n\n\n\nHope it can help anyone\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"VERTEX PIPELINE",
        "Question_tag_count":3,
        "Question_created_time":1671279720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VERTEX-PIPELINE\/m-p\/500836#M966",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"I have this problem:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GenerativeAI Studio is flagging this as inappropriate due to policy...",
        "Question_tag_count":2,
        "Question_created_time":1684194240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GenerativeAI-Studio-is-flagging-this-as-inappropriate-due-to\/m-p\/553719#M1922",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":38,
        "Question_body":"\"View this repo at https:\/\/github.com\/aaronn\/slack-gpt\n\nWhat is it about?\"\n\nThis points to a Github repo that has code for a Slackbot.\n\nOn bard.google.com, it runs fine.\n\nWhat's causing this though?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"403 error when trying to access PalmAPI",
        "Question_tag_count":2,
        "Question_created_time":1684179660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/403-error-when-trying-to-access-PalmAPI\/m-p\/553667#M1917",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":70,
        "Question_body":"Dear community,\n\nI got an invite to try out PalmAPI through makesuite, but unfortunately it is showing me the following error: The account <my-email>@gmail.com doesn't have permission to see this page.\nContact your admin for access or use a different account.\n\nis there something I can do to fix this? note that I am trying to log in with the account I got the invite for.\n\n\u00a0\n\nappreciate all the insights",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model Garden - Tag Recognizer",
        "Question_tag_count":1,
        "Question_created_time":1686632100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-Garden-Tag-Recognizer\/m-p\/602560#M2144",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":83,
        "Question_body":"I would like to test out and play around with a model I found in the Goocle Cloud model garden, namely the Tag Recognizer Model. Im trying to figure out how I could actually add this model to my model registry and train it using my images\/labelling. When I select the model in the model garden, it does not give me any options to make use for it. Am I missing something?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Access to PaLM 2 unicorn",
        "Question_tag_count":2,
        "Question_created_time":1687836060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Access-to-PaLM-2-unicorn\/m-p\/606875#M2243",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":351,
        "Question_body":"Hi,\n\nI already use Vertex AI bison model, but the results are not as good as using BARD (which I understand that works with the bigger\u00a0text-unicorn@001\u00a0model).\n\nHow can I get access to the unicorn model?",
        "Question_closed_time":1688128860000,
        "Answer_score_count":2.0,
        "Answer_body":"Good day,\n\nUnfortunately this is not yet available on Vertex AI and there is no ETA when it will be available, you can check the\u00a0current available foundation models in Vertex AI using this link:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/generative-ai\/learn\/models#foundation_models\nBut if you like this feature, I would suggest that you submit a feature request using this link:\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\n\nYou can also try to register in a waitlist for a public preview of Palm API for experimentation and internal prototyping without any cost, but please note that the estimated preview period will end in H2 2023. You can check this link for more information regarding the public preview:\u00a0https:\/\/developers.generativeai.google\/guide\/preview_faq\nYou can register using this link:\u00a0https:\/\/developers.generativeai.google\/\n\nI hope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Cloud Video Intelligence API Still Relevant?",
        "Question_tag_count":2,
        "Question_created_time":1687765380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Video-Intelligence-API-Still-Relevant\/m-p\/606541#M2230",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"Hi, it seems like there's a big push towards Vertex as the future of Google's AI platform. This said, the cloud video intelligence API provides the functionality that I need for my project. Is it a bad idea to start a new project building on this platform? Are there any plans to sunset the service in the foreseeable future? I'm building a business around the app I'm creating, so it's important to know that Google will continue to invest in this service.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I want to monitor user input text with Vertex AI Model Monitoring.",
        "Question_tag_count":1,
        "Question_created_time":1686704580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-want-to-monitor-user-input-text-with-Vertex-AI-Model\/m-p\/602854#M2152",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":79,
        "Question_body":"I am using the ML model to predict toxicity or not for a user entered text. If I enable Vertex AI Model Monitoring for that user-entered text, can I successfully detect skew and drift?\nI am thinking that the string type data is treated as a categorical feature and the user entered text is not the same as the training data, which would trigger an alert for skew or drift.\n\nIf not, is there a way to detect skew and drift for different user-entered text each time?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ML Pipeline parameter_values does not work well.",
        "Question_tag_count":1,
        "Question_created_time":1689288360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ML-Pipeline-parameter-values-does-not-work-well\/m-p\/612195#M2367",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":114,
        "Question_body":"I am currently running an ML pipeline on Vertex AI and it is not working as expected, so I am asking this question.\n\n\nI have the following pipeline defined right now To explain, when I run the pipeline, I want to use timestamp via paramter_values (e.g. 20230714100000) when executing the pipeline. I want the custom training job to output the models in the timestamp directory, and I want the importer to import the models in that directory. However, when I run it, GCS creates a directory named bucket_name\/{{channel:task=;name=timestamp;type=String;}}. How can this be avoided?\n\n\n\n```\nimport google.cloud.aiplatform as aip\nimport kfp\n\n\n@kfp.dsl.pipeline(name=\"xxx\")\ndef pipeline(\n\u00a0 \u00a0 timestamp: str = \"notimestamp\"\n\u00a0 \u00a0 from google_cloud_pipeline_components.types import artifact_types\n\u00a0 \u00a0 from google_cloud_pipeline_components.v1.custom_job import \\\n\u00a0 \u00a0 \u00a0 \u00a0 CustomTrainingJobOp\n\u00a0 \u00a0 from kfp.components import importer_node\n\n\n\u00a0 \u00a0 custom_job_task = CustomTrainingJobOp(\n\u00a0 \u00a0 \u00a0 \u00a0 project=\"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 display_name=f\"xxx-{timestamp}\",\n\u00a0 \u00a0 \u00a0 \u00a0 worker_pool_specs=[\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"containerSpec\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"env\": [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"name\": \"BUCKET_NAME\", \"value\": f\"bucket_name\/{timestamp}\"},\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"imageUri\": \"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"replicaCount\": \"1\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machineSpec\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machineType\": \"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_type\": xxx,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_count\": 1,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 \u00a0 \u00a0 ],\n\u00a0 \u00a0 )\n\n\n\u00a0 \u00a0 importer_node.importer(\n\u00a0 \u00a0 \u00a0 \u00a0 artifact_uri=f\"bucket_name\/{timestamp}\",\n\u00a0 \u00a0 \u00a0 \u00a0 artifact_class=artifact_types.UnmanagedContainerModel,\n\u00a0 \u00a0 \u00a0 \u00a0 metadata={\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"containerSpec\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"imageUri\": \"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 ).after(custom_job_task)\n\n\n```",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error in Vertex AI - Language",
        "Question_tag_count":3,
        "Question_created_time":1689921480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-in-Vertex-AI-Language\/m-p\/614381#M2417",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":48,
        "Question_body":"Hello everyone! I\u00a0have an issue with using the Vertex AI Language on my project. When using Science Chatbot, we need to ask questions in Portuguese and receive answers in Portuguese, but we are receiving the following error when performing the procedure: \"\nI'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback\". Can you help me with this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Incremental ingestion into Feature Store",
        "Question_tag_count":1,
        "Question_created_time":1674560700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Incremental-ingestion-into-Feature-Store\/m-p\/513484#M1129",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"Hello evryone,\n\nHow to set up a incremental ingestion for online serving using Feature Store?\n\nI would like to use this flow:\u00a0\n\nQuery into The BigQuery with delta time\n\nIngestion into FeatureStore\u00a0 witho only this incremental values\n\nUsing the online serving with Incremental + Previous data into FeatureStore\n\nHow Can I?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I run my custom model training from vscode",
        "Question_tag_count":1,
        "Question_created_time":1680145080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-run-my-custom-model-training-from-vscode\/m-p\/538798#M1531",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":239,
        "Question_body":"I have run a custom model training on workbench. I want to execute this training from VSCode. Is that possible and if yes, how do i do it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Calling vertex API for embeddings in asia region does not work",
        "Question_tag_count":1,
        "Question_created_time":1689336900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Calling-vertex-API-for-embeddings-in-asia-region-does-not-work\/m-p\/612417#M2374",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"I'm trying to use Vertex AI to generate embeddings.\n\nMaking a request to\u00a0 POST https:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/PROJECT_ID\/locations\/us-central1\/publishers\/google\/models\/textembedding-gecko:predict works as per this documentation.\n\nHowever, I can't seem to retrieve embeddings from a model running in a region closer to asia.\n\nI have tried the following API call, POST https:\/\/asia-east1-aiplatform.googleapis.com\/v1\/projects\/PROJECT_ID\/locations\/asia-east1\/publishers\/google\/models\/textembedding-gecko:predict\u00a0but I\u00a0get a 404 error saying\u00a0Publisher Mode not found\n\nDo I need to do something to enable Vertex AI API in other regions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Month-to-date total cost (VertexA\u0131) keeps increasing even though I delete all my projects and billin",
        "Question_tag_count":3,
        "Question_created_time":1668131160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Month-to-date-total-cost-VertexA%C4%B1-keeps-increasing-even-though-I\/m-p\/488385#M764",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":106,
        "Question_body":"Hello. I am a student and only use GCP for learning. But even though there is no project in my profile and\nI have closed my billing account, the Month-to-date total cost (VertexA\u0131) is constantly increasing. \nI tried everything but couldn't find a solution. I would be very happy if you could help me what to do.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Why does Jupyter Notebooks not recognize changes in my .py files?",
        "Question_tag_count":1,
        "Question_created_time":1652095440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Why-does-Jupyter-Notebooks-not-recognize-changes-in-my-py-files\/m-p\/421489#M316",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":39,
        "Question_body":"Hello,\u00a0\n\nWhen working locally, I usually put routine tasks inside functions held in a .py file and import those.\u00a0 When I need to make a change, I change the function, reimport and moving on with the main script.\u00a0\u00a0\n\n\u00a0\n\nGCP's jupyter instance does not recognize when I make the change an re-import the function.\u00a0 I have to restart the kernel each time.\u00a0 Is there a way around this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Endpoint in GCP",
        "Question_tag_count":3,
        "Question_created_time":1665751020000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Endpoint-in-GCP\/m-p\/478360#M653",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":135,
        "Question_body":"In GCP I was deploying a model which obtain from training a dataset and after success full Vertex AI Model Registry. It takes too much time around 10 min to create endpoint for model. How can I reduce creation time when creating endpoints on GCP? What factors affected endpoint creation ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Selecting a region for Cloud Talent Solution",
        "Question_tag_count":2,
        "Question_created_time":1677732780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Selecting-a-region-for-Cloud-Talent-Solution\/m-p\/527976#M1366",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"Starting using Cloud Talent Solution service but i am not able to see an option to configure a default region for this service. I would like to choose the region us-west-1 (oregon), is there a way i can do that?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create a Tuned model - No response",
        "Question_tag_count":2,
        "Question_created_time":1689355140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-a-Tuned-model-No-response\/m-p\/612495#M2375",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":188,
        "Question_body":"Hi\u00a0\n\nI am trying to follow the youtube video for parameter based tuning - I created a .jsonl file with the input_text and output_text data and uploaded the same as in the Created tuned model Step - 1 - uploaded this file to the Bucket and selected the dataset location. This file was processed without any errors and the flow went to the #2 step - Model details - i provided a model name, adjusted the train steps and learning rate with the base model pre-populated as text-bison and finally selected the destination work directory.\n\nThe Start Tuning button becomes active as soon as i complete all the steps - but after i click on that one - there is a blip and then nothing happens - i do not get a success or failure dialog as shown in the youtube video.\n\nNot sure what i am missing here - any help will be highly appreciated.\n\nThis is the generated code\n\n\u00a0\n\nfrom google.cloud import storage\nimport vertexai\nfrom vertexai.preview.language_models import TextGenerationModel\n\ndataset = \"\"\"my .jsonl text data goes here with the correct format of input_text and output_text\"\"\"\n\n#Uploads dataset to cloud storage\nstorage_client = storage.Client()\nbucket = storage_client.bucket(\"cloud-ai-platform-MY_BUCKETID\")\nblob = bucket.blob(\"input.jsonl\")\nblob.upload_from_string(dataset)\ntraining_data = \"gs:\/\/cloud-ai-platform-MY_ID\/input.jsonl\"\n\nvertexai.init(project=\"PROJECT_ID\", location=\"us-central1\")\nmodel = TextGenerationModel.from_pretrained(\"text-bison@001\")\nmodel.tune_model(\n    training_data=training_data,\n    train_steps=100,\n    tuning_job_location=\"europe-west4\",\n    tuned_model_location=\"us-central1\",\n)\n\n\u00a0\n\nsomewhere in the documentation they have mentioned that Preview happens only in europe-west4 and if i get a 500 error i need to run a piece of code - in my case i do not get 500 or internal server error and the generated code also shows the tuning job location is europe-west4.\n\nany pointers around why this is not working and what am i missing or the youtube video is missing as i am trying to follow it step by step.\n\nThanks in advance\n@kvandres\u00a0- tagging you on this post as you helped someone on a related issue .\n\nSai",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to connect Elasticsearch to Jupyter Notebook on GCP",
        "Question_tag_count":2,
        "Question_created_time":1665196080000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-connect-Elasticsearch-to-Jupyter-Notebook-on-GCP\/m-p\/475882#M630",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":318,
        "Question_body":"Hi,\n\nI am working on an ML project which requires to use Transformers and Elasticsearch.\n\nFor Transformers, I have created a Jupyter Notebook instance on GCP.\n\nFor Elasticsearch I will create another instance on CGP.\n\nAs part of project requirement, I need to access Elasticsearch in Jupyter Notebook through port 9200. I need to ingest data in Elasticsearch and run search queries which can fetch relevant information from Elasticsearch DB and give it in Notebook.\n\nMy question is that if I create two separate instances of Notebook and Elasticsearch and if i try to connect Elasticsearch through Jupyter Notebook via port 9200, will i be able to connect and perform the above mentioned operations ?\n\nIf NO, then what is the procedure to do so ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What's the training corpus of models behind GCP Natural Language APIs?",
        "Question_tag_count":3,
        "Question_created_time":1668695760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-training-corpus-of-models-behind-GCP-Natural-Language\/m-p\/490614#M807",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hi, where can I find some information about which datasets are used for training models that power the natural language APIs for sentiment analysis, entity extraction, etc.? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Relation Google node hour to Azure computing hour",
        "Question_tag_count":1,
        "Question_created_time":1659682680000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Relation-Google-node-hour-to-Azure-computing-hour\/m-p\/450911#M490",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":46,
        "Question_body":"Hey there,\n\nI am writing my masters thesis at the moment.\u00a0In my master thesis I compare the machine learning services of Google and Microsoft for image classification. This also includes the costs. Google uses node hours and Microsoft computing hours for the calculation. Is it possible to compare these units? This would be a crucial part of the comparison.\nThanks a lot!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Time taking to take A Job in a Cluster on GCP DataProc",
        "Question_tag_count":2,
        "Question_created_time":1676986020000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Time-taking-to-take-A-Job-in-a-Cluster-on-GCP-DataProc\/m-p\/524763#M1305",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":43,
        "Question_body":"Dear Googlians,\n\nThere was a problem relevant to the time taken to create a cluster on GCP Dataproc. What may cause such a problem?\n\nRegards.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Too many pages",
        "Question_tag_count":1,
        "Question_created_time":1665984720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Too-many-pages\/m-p\/478806#M657",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":68,
        "Question_body":"I sent a 13 page pdf thru a document ai parser.\u00a0\u00a0\n\nGCP, instead of populating the errors collection of the result with an error indicating too many pages, instead throws a runtime error causing a crash.\n\nIs try...except... really the best solution for this as I've not seen use of try...except in any of Google parser examples.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Virtual Machine Configuration Error for User-managed Notebook on Vertex AI Workbench",
        "Question_tag_count":1,
        "Question_created_time":1688467500000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Virtual-Machine-Configuration-Error-for-User-managed-Notebook-on\/m-p\/609199#M2286",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":87,
        "Question_body":"I am creating a user-managed notebook on the Vertex AI Workbench, and when choosing the Machine Type Configuration, I put\u00a0g2-standard-8, which comes with a Graphics Optimized NVIDIA L4 GPU, 8 vCPUs, and 32GB of RAM. However, when I created the notebook, I got an error saying \"Instances with guest accelerators do not support live migration.: Something went wrong. Sorry about that.\" I would greatly appreciate it if someone could help to solve this issue so I am able to use the g2-standard-8 configuration. Thank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I enable generativelanguage.googleapis.com?",
        "Question_tag_count":2,
        "Question_created_time":1683968700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-enable-generativelanguage-googleapis-com\/m-p\/553160#M1894",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":261,
        "Question_body":"I already have API Access to Vertex AI, and the Python code works with the google AI SDK.\n\nHowever, I'm using this with Langchain, and it requires\u00a0\n\ngoogle.generativeai\u00a0module along with a Google API Key. However I'm unable to enable\n\u00a0\ngenerativelanguage.googleapis.com, due to a permission denied issue. Is there any way to resolve this? Since I already have Enterprise access to Vertex AI.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI - Text entity extraction response format",
        "Question_tag_count":3,
        "Question_created_time":1663501800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Text-entity-extraction-response-format\/m-p\/467926#M584",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":298,
        "Question_body":"Hello!\n\nI would like to ask if it's possible to get the same output format from the Vertex AI entity extraction REST API as from the Google Natural Language API?\n\nBecause now in the response of Vertex AI we get only a list with the confidence scores, displayNames and the start\/end offsets of the textSegments, but the entities itself are not in the json (in the NLP response there is also \"content\" with the entity). So this is how I would like to get the response from the Vertex AI for each entity:\n\n{\n\"annotationSpecId\": \" \",\n\"displayName\": \"date\",\n\"textExtraction\": {\n\"score\": 0.69745916,\n\"textSegment\": {\n\"startOffset\": \"382\",\n\"endOffset\": \"392\",\n\"content\": \"12.07.2022\"\n}\n\nCan you help please how I could achive this?\n\nThank you for your help in advance!\n\nZsolt",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What are you looking forward to in Google Cloud NEXT '21?",
        "Question_tag_count":1,
        "Question_created_time":1624439940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-are-you-looking-forward-to-in-Google-Cloud-NEXT-21\/m-p\/161539#M3",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":403,
        "Question_body":"Google Cloud Next conference is happening this year on October 12-14, 2021!\n\nWhat exciting new technologies are you looking forward to?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text to SQL : How to get access",
        "Question_tag_count":2,
        "Question_created_time":1690014960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-SQL-How-to-get-access\/m-p\/614692#M2430",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":66,
        "Question_body":"Hi, we need to convert queries from natural languages to the SQL. Backend is MariaDB\/MySQL with relational database. Please advise:\n\n1) How and where to get access?\n\n2) The documentation and links.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocAI Warehouse UI provision problem",
        "Question_tag_count":1,
        "Question_created_time":1675520820000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Warehouse-UI-provision-problem\/m-p\/518134#M1205",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":192,
        "Question_body":"hi. have the problem with\u00a0Provision DocAI Warehouse. I can complete all 4 steps but on the last step when all is don and I see\n\nDocAI Warehouse UI\nVisit the link below to the DocAI Warehouse UI (Preview) to finalize setup of your schema, ACLs, AI processor mappings, and notifications.\n\nhttps:\/\/documentwarehouse.cloud.google.com\/provision\/***********\n\nWhen I enter to this link to complete my provision. On the first step I see the project number and the following message \"Request had insufficient authentication scopes. Logout and re-login will fix this problem.\" - it somehow log out from my Google account and provision page somehow on the unlogged page and I am stuck. On the provision page all other steps 2-4 are grey. according the docs all have to be easy -\u00a0https:\/\/cloud.google.com\/document-warehouse\/docs\/administer-warehouse\u00a0\n\non the first step the author on the top right corner of the screen have Search, project name, admin, and his Google account. In my case I have only Search and project name. Admin and my Google account from no reason does not exist. Can you please help me? I have the project owner right.\u00a0\n\n\"\"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"text-bison@001 tuned model serving",
        "Question_tag_count":1,
        "Question_created_time":1687422660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/text-bison-001-tuned-model-serving\/m-p\/605737#M2212",
        "Question_answer_count":11,
        "Question_score_count":0,
        "Question_view_count":710,
        "Question_body":"Hi! I would like to tune a model based on text-bison@001\u00a0and have it run online inferences. The documentation about how to tune is very clear. However, I can't figure out how Vertex serves my model for inference.\n\nDo I need to deploy the tuned model to an endpoint and pay hourly? If so, what instance type is necessary to support the tuned model?\n\nAlternatively, is the tuned model hosted \"serverlessly\" and I pay the same (or different) per-character rate as for regular requests to the base text-bison@001\u00a0model?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI not training custom jobs in batches larger than 20?",
        "Question_tag_count":3,
        "Question_created_time":1675596180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-not-training-custom-jobs-in-batches-larger-than-20\/m-p\/518322#M1210",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":274,
        "Question_body":"Whenever I try to train more than 20 jobs (custom containers) in parallel it seems like the jobs enter a queue such that only 20 run at one time, my quotas should allow me to do much more (cpu and vertex ai api way under limit, same with all my compute stuff).\n\nIs there any way to see why those jobs aren't starting instantly or to increase the number of custom jobs running at one time? I don't even see them in pending but when I submit them all using sync=False in the workbench they get submitted with no error instantly.\n\nI also tried to throttle the start of each training job incase its a per min request limit thats causing the limit of 20 parallel jobs but that didnt solve it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Importing to Vertex dataset does not import labels.",
        "Question_tag_count":3,
        "Question_created_time":1667362320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Importing-to-Vertex-dataset-does-not-import-labels\/m-p\/484912#M716",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":544,
        "Question_body":"In Vertex AI I am updating an image dataset, thus:\n\nfrom google.cloud import aiplatform\nimport_schema_uri = aiplatform.schema.dataset.ioformat.image.single_label_classification\ndataset_id = \"my_ds_id\"\n\nds = aiplatform.ImageDataset(dataset_id)))\nds.import_data(gcs_source=DATASET_PATH, import_schema_uri=import_schema_uri)\n\nthe images are uploaded to the dataset but their labels are ignored and they are classed as\u00a0Unlabeled. What am I doing wrong? TIA!\n\nPS they are in a csv, like:\n\ngs:\/\/path\/to\/file\/barnacles.jpg,label1\n\nwhich worked fine for the dataset creation.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to use Vertex AI training pipelines",
        "Question_tag_count":1,
        "Question_created_time":1684802880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-use-Vertex-AI-training-pipelines\/m-p\/595873#M2005",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":63,
        "Question_body":"Hi!I have been trying to use Vertex AI for training custom ML models. I requested for a quota increase and did all the procedures as requested(Made\u00a0 a $10 transaction). I got an email that my quota has increased. However, I'm still getting the following errors:\n\nHow can I resolve this?\n\n2. I also wanted to upgrade to a standard support package so as to get prompt responses to my queries on email. However, that also tells me I dont have sufficient permissions.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Authenticating to Vertex AI deployed endpoints",
        "Question_tag_count":2,
        "Question_created_time":1668088620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authenticating-to-Vertex-AI-deployed-endpoints\/m-p\/488229#M762",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":436,
        "Question_body":"Hello, I am a new user of Vertex AI.\u00a0 I have trained and deployed a tabular data categorization model to an Vertex AI hosted endpoint.\u00a0 I have successfully called it from a program running on my laptop where the \"gcloud\" cli is installed.\u00a0 If I want to run this not from my desktop but have it called from another service, how do I authenticate ?\u00a0 I have created a service account but I am not sure 1) what roles would need to be attached to that account and 2) how I would provide the service account credentials given that I don't have much control over how the service that will call my model is started (i.e. I can't control its environment vars).\u00a0 Any help would be appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI custom training job never finished",
        "Question_tag_count":1,
        "Question_created_time":1684281540000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-custom-training-job-never-finished\/m-p\/554143#M1938",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":144,
        "Question_body":"Hi,\u00a0\n\nI am trying to implement custom model training on Vertex AI.\n\nI pushed custom training job to Vertex AI and met the freezing issue at the\u00a0trainer.train()\u00a0 process.\u00a0\n\n\u00a0Command:\n```\n\ngcloud ai custom-jobs create \\\n--region=us-west1 \\\n--display-name=test_model \\\n--worker-pool-spec=machine-type=n1-highmem-2,accelerator-type=NVIDIA_TESLA_T4,accelerator-count=1,replica-count=1,executor-image-uri=asia-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-13:latest, local-package-path=src, script=task.py\n\n```\n\nVersion\n\n```\n\nimage:\u00a0asia-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-13:latest\n\ntransformers v4.28.0\u00a0 (changed to other versions but got the same freeze issue)\n\n```\n\nLog in Logging is below without error messages.\n\n```\n\nMap: 0%| | 0\/72 [00:00<?, ? examples\/s]\n\n```\n\nThe process succeeded two month ago with vertex-ai\/training\/pytorch-gpu.1-11:latest and the same script task.py.\n\u00a0\nIs there anyone who've been facing a same issue or have solved before?\nThanks,",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to create an endpoint using the model trained by Vertex AI?",
        "Question_tag_count":4,
        "Question_created_time":1672357020000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-create-an-endpoint-using-the-model-trained-by-Vertex-AI\/m-p\/504610#M1003",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":343,
        "Question_body":"I trained a model using Google Cloud Vertex AI. I have a dataset containing different videos and model does the video classification of these videos. In the Google Cloud Platform, I can find accuracy of the given videos after model has been trained. However, I could not find a way to create an Endpoint which is required to implement my model to the project.\n\nIn other words, I have a model trained by Google Cloud\u2019s Vertex AI and I want to create an Endpoint which accepts a request with video and returns the accuracy according to the trained model. Google Cloud provides a way to do it using Endpoints but as shown below, following\u00a0error\u00a0occurs.\u00a0This model cannot be deployed to an Endpoint\n\nI could not find any documents on why this error is given to me. On the other hand, when I download the given model, code requires input type as below with the video;\n\n\u201cRESNETISH_V3: float32 Tensor, shape=(None, 128)\u201d.\n\nI do not know what is this variable type and this is basically where I am stuck. I need help deploying my trained model to any serverless environment, create a code handling posted video and return the accuracy and class of the given video to the request.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"VertexAI: a way to post-process results without usign custom containers?",
        "Question_tag_count":1,
        "Question_created_time":1640342520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-a-way-to-post-process-results-without-usign-custom\/m-p\/181244#M132",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":103,
        "Question_body":"Hello,\n\nI have been searching on how to deploy models on VertexAI in AI Platform manner. Most tutorials shows using pre-built container which seems to load the model and return inference results.\n\nMy current requirement needs to post-process. This was easy to do with AI platform's Predictor class. Is something like that doable with pre-built containers on VertexAI (where we upload a package for inference by inheriting Predictor class and specifyinh the class name)? Using custom container makes it complex to handle and response to requests.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Internal error (code 13) when tuning a model in Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1690278660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Internal-error-code-13-when-tuning-a-model-in-Vertex-AI\/m-p\/615702#M2457",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":104,
        "Question_body":"When processing a model tuning in Vertex AI, during the pipeline creation, i recieved an Internal error message.:\n\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects\/xxxxxxxxx\/locations\/xxxxx-xxxxx\/pipelineJobs\/tune-large-model-xxxxxxxxxxx current state:\nPipelineState.PIPELINE_STATE_PENDING\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-591a9a561b04> in <cell line: 4>()\n      2 vertexai.init(project=\"xxxxxxxxx\", location=\"us-central1\")\n      3 model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n----> 4 model.tune_model(\n      5     training_data=training_data,\n      6     train_steps=100,\n\n3 frames\n\/usr\/local\/lib\/python3.10\/dist-packages\/google\/cloud\/aiplatform\/pipeline_jobs.py in _block_until_complete(self)\n    500         # JOB_STATE_FAILED or JOB_STATE_CANCELLED.\n    501         if self._gca_resource.state in _PIPELINE_ERROR_STATES:\n--> 502             raise RuntimeError(\"Job failed with:\\n%s\" % self._gca_resource.error)\n    503         else:\n    504             _LOGGER.log_action_completed_against_resource(\"run\", \"completed\", self)\n\nRuntimeError: Job failed with:\ncode: 13\nmessage: \"Internal error encountered. Please try again\"\n\nThe error doesn't show more details, so i can't figure out what's wrong, any ideas about what is wrong?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI workbench and Google cloud storage problems accesing files",
        "Question_tag_count":1,
        "Question_created_time":1644211620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-workbench-and-Google-cloud-storage-problems-accesing\/m-p\/390712#M196",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":610,
        "Question_body":"I am trying to create a ML project in which the job is a classification task of videos, so I upload those videos in Google cloud storage, and then I create a notebook on the workbench of vertex AI, for making data balancing, and then train my respective ML algorithm. But I have this problem:\n\n1. I want to use the video files from GCS without the need of downloading them again(that was the purpose of uploading them in GCS), but I don't know how can i do this?.\n\nI also try uploading de videos into the dataset space of the vertex AI workbench but still don't know how to acces to this files without downloading them again.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Does vertex ai support simultaneous ingestion (batch) from multiple sources?",
        "Question_tag_count":1,
        "Question_created_time":1675545600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-vertex-ai-support-simultaneous-ingestion-batch-from\/m-p\/518165#M1206",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"I read here that it supports ingestion from BigQuery and GCS, however, I was wondering if it supports simultaneous ingestions from various sources",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"RESOURCE_EXHAUSTED at custom-training-job.",
        "Question_tag_count":3,
        "Question_created_time":1674324600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/RESOURCE-EXHAUSTED-at-custom-training-job\/m-p\/512495#M1111",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":167,
        "Question_body":"I have trouble to execute this below vertex pipeline example from google in vertexAI workbench notebook.\n\nhttps:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/goog...\n\nI have below error code which mention\u00a0RESOURCE_EXHAUSTED at custom-training-job.\n\n\"com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\"\n\nI change kind of cpu or memory . however it is meaningless for me.\n\nCould you please give me some idea?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GCP Idle Model Charging",
        "Question_tag_count":5,
        "Question_created_time":1667794200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-Idle-Model-Charging\/m-p\/486604#M740",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":167,
        "Question_body":"Hello!\nI would like to deploy the ML Model into GCP.\n\nMost of the time the model will be sleeping. Sometimes I should use it through Endpoint for some seconds.\nI don't want to pay for full-time GPU instance and I need fast responses at the same time, without deployment from scratch everytime I need it.\n\nIs this possible in GCP ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to use multiple data files in vertex AI",
        "Question_tag_count":1,
        "Question_created_time":1688967900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-multiple-data-files-in-vertex-AI\/m-p\/610696#M2333",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":52,
        "Question_body":"Hi\n\nI am looking to implement Vertex AI on product. The use case is very simple. The end user uploads some files (mostly CSV or xls containing 50 to 1000 entries.). Then user can request info using prompt.\n\nHere is an example:\n\nUser 1 -> uploads a csv having pricelist -> tshirt- $10, book - $15, bag - $20\nUser 2 -> uploads a csv having pricelist -> tshirt- $25, cap - $12, shorts - $15\n\nNow if user 1 prompts and asks for value of tshirt. He should see $10. Also if he requests price of cap he should get an error.\n\nSimilarly if\u00a0 user 2 prompts and asks for value of tshirt. He should see $25. Also if he requests price of book he should get an error.\n\nI was wondering how can I implement this on vertex AI in a way that data\/files uploaded by one user is not accessible to other user. I am not looking create a fine tuned model for each client as it is not cost effective and also the client data does not require any special learning.\n\nPlease help.\n\nThanks in advance.\n\nRegards\nYogendra",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to increase quota for base_mode: chat-bison and text-bison",
        "Question_tag_count":1,
        "Question_created_time":1684231860000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-increase-quota-for-base-mode-chat-bison-and-text-bison\/m-p\/553927#M1926",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":144,
        "Question_body":"Hi,\n\nI have paid GCP account. I am unable to use any Vertex AI services where\u00a0chat-bison and text-bison are required. The quota is set at 0 and I am not able to increase it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Tabular Forecasting Model in Vertex AI - Cannot deploy model to endpoint",
        "Question_tag_count":1,
        "Question_created_time":1665200280000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Tabular-Forecasting-Model-in-Vertex-AI-Cannot-deploy-model-to\/m-p\/475893#M631",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":207,
        "Question_body":"I'm getting started with Vertex AI, and the model I'd like to use is a Tabular Forecasting Model. After spending hours tweaking the model that I wanted to deploy, I came across this error message. \"The default version cannot be deployed\". I can deploy a normal Tabular model to an endpoint, but not the Tabular Forecasting model.\n\n\u00a0\n\nDoes anyone know if there is a way to deploy a Tabular Forecasting Model? If not, is Google planning on adding this functionality anytime soon?\n\n\u00a0\n\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI image classification models lose accuracy when being placed in a python dictionary",
        "Question_tag_count":5,
        "Question_created_time":1666779180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-image-classification-models-lose-accuracy-when-being\/m-p\/482475#M690",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":225,
        "Question_body":"I have made a\u00a0 model using vertex AI's image classification. Exported as EdgeTPU tflite model to my Raspberry pi 4 with Coral USB accelerator. When I used the Pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py\u00a0 to run my model, I get a perfect prediction result. But when I passed them to a python dictionary in my script, the prediction accuracy is way off.\u00a0https:\/\/github.com\/hillyuyichu\/Pycoral-python-API\/blob\/main\/pycoral_classification.py\u00a0\n\n\u00a0\n\n\u00a0\n\nHere is a screenshot of the prediction results on my python classification.py:\n\nThe label in row 1 is always the most active. The one in the last rows are the least active and most inaccurate.\n\nex: In picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Product Specifications or Product Recommendation in the Support Section",
        "Question_tag_count":3,
        "Question_created_time":1650528420000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Product-Specifications-or-Product-Recommendation-in-the-Support\/m-p\/415770#M288",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"Hi\n\nAny ones knows if I could get\u00a0Product Specifications or Product Recommendation in the Support Section from Google Cloud:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"A brief introduction to Convolutional Neural Network with Tensorflow",
        "Question_tag_count":1,
        "Question_created_time":1679141100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/A-brief-introduction-to-Convolutional-Neural-Network-with\/m-p\/534073#M1439",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":35,
        "Question_body":"Dear Learners' Community,\n\nI hope you're having a good day. I've created a notebook on Kaggle to simplify CNN with tf. You can review it here:\n\nhttps:\/\/www.kaggle.com\/code\/eslamfouad\/introduction-to-convolutions-with-tensorflow?scriptVersionId=...\n\nRegards.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocAI - Form Processor table issue",
        "Question_tag_count":2,
        "Question_created_time":1669240800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Form-Processor-table-issue\/m-p\/492493#M856",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":149,
        "Question_body":"Hello,\nI used Document AI form processor to convert pdf file table data into table object.\nSome table data are not converted properly.\n\nIn the sample file, the 3rd table is not detected columns properly.\nCould you please throw some light on this ?\n\nSource PDF file\n\nAfter conversion using form processor. The third one is having issue on column detection.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Is it possible to use Vertex AI experiments without Tensorboard?",
        "Question_tag_count":1,
        "Question_created_time":1683855060000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Is-it-possible-to-use-Vertex-AI-experiments-without-Tensorboard\/m-p\/552738#M1884",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":387,
        "Question_body":"Is it possible to use Vertex AI experiments without Tensorboard?\n\nWhen I programmatically init the SDK and start experiment run as described in docs, Tensorboard is automatically provisioned.\n\nI would like to use only summary metrics and parameters tracking (i.e. not time-series metrics), and AFAIK it doesn't require Tensorboard.\n\nHere's the code snippet:\n\n\u00a0\n\naiplatform.init(experiment=\"foo\", location=\"us-central1\")\naiplatform.start_run(run=\"bar\")\naiplatform.log_metrics({\"score\": 0.42})",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Matching engine not accepting Batch Prediction embeddings and generating error",
        "Question_tag_count":2,
        "Question_created_time":1688879340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Matching-engine-not-accepting-Batch-Prediction-embeddings-and\/m-p\/610409#M2325",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":54,
        "Question_body":"I am attempting to build a question answering bot using a model from TensorFlow Hub, specifically the model located at https:\/\/tfhub.dev\/google\/universal-sentence-encoder-qa\/3. I have successfully uploaded the model to the model registry and performed batch predictions, resulting in two text\/plain files as output. I am now working on creating a matching index using the DOT_PRODUCT method. Below is the code I am using for this task.\n\nBut getting errorr as\u00a0\n\nShow More\n\nHow do I resolve this error?, Do I need to convert the predictions to json format?.\n\nIts very urgent for me, if anyone can help please let us know",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I would like to receive notification of the start of the ML pipeline",
        "Question_tag_count":1,
        "Question_created_time":1686077640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-would-like-to-receive-notification-of-the-start-of-the-ML\/m-p\/600721#M2091",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":68,
        "Question_body":"I have constructed Email notification of ML pipeline according to the URL below.\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/email-notifications?hl=ja\n\nAll I receive is notification of success or failure.\u00a0I would like to receive notification of the start of the ML pipeline.\u00a0Is that possible?\n\nIt looks like there are only email notifications, can I switch to Slack notifications?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"vetex ai tensorboard absurd charges",
        "Question_tag_count":4,
        "Question_created_time":1689137520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/vetex-ai-tensorboard-absurd-charges\/m-p\/611509#M2344",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":38,
        "Question_body":"Hey,\n\nAfter enabling Vertex services and initiated training i was charged at once of 300$ which later i got to know was Tensorboard fee per user per month. first of all this was totally shocking to me that google is charging this much for logging metrics (seriously) and secondly not even a warning that a Platform where all the pricing is directly proportional to disk space consumed\/month or by the hour (training), google would charge this absurd amount basically for a tool which i can host locally for free.\n\nGoogle is revising this fee in augest from 300 to 10$\/month (finally someone understood there) ,is there any way this fee can be reimbursed to my account if i open a\u00a0 case\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Time to train a new model in Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":1689173700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Time-to-train-a-new-model-in-Vertex-AI\/m-p\/611730#M2348",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":99,
        "Question_body":"Hello community,\u00a0\n\nI'm trying to train my first model to classify images with Vertex AI. I set 8 hours as the budget for maximum node hours but the process run for 18 hours and I wasn't sure if I was going to be billed for the 18 hours so I canceled the training. The \"Enable early stopping\" was enabled.\u00a0\nIn addition, I used 13 images by label but I read that the recommended quantity is 100. I guess that the quantity of images might impact the training time.\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create User Managed Workbook with A100 40gb",
        "Question_tag_count":2,
        "Question_created_time":1680503160000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-User-Managed-Workbook-with-A100-40gb\/m-p\/540188#M1570",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":85,
        "Question_body":"I'm trying to create a user managed workbook with an a100 40gb GPU, but I am always running into this same error:\n\nProject-Name:\u00a0The zone 'projects\/project-name\/zones\/us-east1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later.: Something went wrong. Sorry about that.\n\nI am pretty sure I have the appropriate quotas, and I don't have any other workbooks so my global gpu quota shouldn't be the chokepoint. (Do I need the Preemptible Nvidia A100 GPUs quota? All of those are set to 0.) I have the \"Managed Notebooks NVIDIA A100 GPUs per region\" set to 1, but I guess those only apply to, well, managed notebooks. Any tips?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to create a private instance of Bard and feed it private data to help answer questions",
        "Question_tag_count":1,
        "Question_created_time":1685920380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-create-a-private-instance-of-Bard-and-feed-it-private\/m-p\/599980#M2078",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":210,
        "Question_body":"Hi Google support,\n\n\u00a0 \u00a0 I'm trying to figure out how I can create a private Bard instance and feed it private data to train it so that it can help me answer questions in an informative way. There is a lot of content out there, but I feel it's quite convoluted. Please let me know if you can guide me in any way.\u00a0\n\nThank you.\u00a0\nRegards,\n\nTraolly Xiong",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Node hours vs actual time",
        "Question_tag_count":2,
        "Question_created_time":1655341800000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Node-hours-vs-actual-time\/m-p\/431897#M387",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":391,
        "Question_body":"What is meant by node hours in VertexAI?\n\nI set the budget in VertexAI AUtoML to a 1 node hour but my model has been training for 1 hr and 30+ minutes.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Datasets best practices",
        "Question_tag_count":2,
        "Question_created_time":1684932660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Datasets-best-practices\/m-p\/596635#M2015",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":63,
        "Question_body":"I am new to ML and VertexAI. I have some questions about an app I am building that requires image classification labels. The closest example I can think of is that mobile app which identifies plants, like PlantNet. You take a photo, and it returns the type of plant, ideally with a relationship from parent species.\n\nI chose Vertex because it includes the Google Bucket storage, allows for custom labels, and having more than 1 label per image. I plan to have a single endpoint to query against, across all my data.\n\nIn terms of what should go inside of a dataset, are there best practices for setting up the datasets in Vertex?\u00a0 Should I put all images (categories) under the same\u00a0dataset\u00a0OR can I add multiple datasets that query across a single endpoint?\n\nShould there be 1 dataset, or multiple?\n\nIE: a separate dataset for trees and a dataset for flowers?\n\nin this case trees would include photos with labels \"oak\", \"pine\", \"maple\", and would include `none_of_these` label associated to things like \"roses\" and \"poison ivy\" and \"grass\"\n\nor a single large database that would include all the labels for all the things?\n\nWhat about model deployment? How can I set a budget on that? It's darn pricey at 1.375 USD per hour\n\nWhat about training hours? Is that a bit more ambiguous because it's based on the training output ratings?\n\nit's also pricey at 3.465 USD per hour",
        "Question_closed_time":1685378220000,
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@lucksp,\n\nWelcome to Google Cloud Community!\n\nThis will be depending on your use case, here are some suggestions for your questions:\n\n1. You can use a single dataset with all the categories if the categories you're working with are closely related and you want your model to distinguish between them. In your example, you should use a single dataset with all of these labels if you want the model to differentiate the photos. This will enable the model to understand the variations among all the categories and produce more accurate predictions. If you wish to create two different datasets, I would suggest that if the tasks are not related to one another, then you can create two datasets with two different models.\u00a0If you wish to sustain modularity and have various models for various tasks, this method may be helpful.\u00a0\n\n2. You can limit the number of compute nodes in your model settings during deployment but this will also limit its performance. You can check this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/image-recognition-automl\/deploy-predict\nYou can also track this ongoing feature request of auto scaling to zero:\u00a0https:\/\/issuetracker.google.com\/206042974\nYou can check this link also if you want to learn more about the considerations when deploying a model:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment\n\n3.\u00a0 You can try configuring the training budget of the training but please note that the pricing is based on the node hour, and you can also enable the early stopping. Disabling early stopping will train the model until your training budget is exhausted. It is also important to know that this will still depend:\n\nModel training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one.\u00a0You can use this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-\ndata\/forecasting\/train-model\n\nYou can use this link for more information about the training pricing:\u00a0https:\/\/cloud.google.com\/vertex-ai\/pricing#automl_models\u00a0\n\nFor best practices on creating training data, this will be helpful to increase the quality of the model. you can check this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/bp-tabular\n\nHope this helps!\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI: Workbench - Failing to Created a schedule based recurrent execution",
        "Question_tag_count":2,
        "Question_created_time":1670887320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Workbench-Failing-to-Created-a-schedule-based\/m-p\/498980#M934",
        "Question_answer_count":8,
        "Question_score_count":0,
        "Question_view_count":324,
        "Question_body":"Hello,\n\nI am trying to create a schedule-based recurrent execution via the Vertex AI workbench. It fails with error code 2- unknown error. Attaching the error snippet.\u00a0\n\nAny help to understand the root cause is highly appreciated.\u00a0\nappreciated.\u00a0\n\nNote: I can do standalone execution successfully. It's only when trying to create a schedule based event, this error occurs.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Bard API",
        "Question_tag_count":2,
        "Question_created_time":1685278140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Bard-API\/m-p\/597656#M2036",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"Hello, I am student currently working on a chatbot project and I would love to use the Bard API, please can someone assist me in how I can",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error while deploying hugging pytorch model (ROBERTA) to Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":1684722720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-while-deploying-hugging-pytorch-model-ROBERTA-to-Vertex-AI\/m-p\/595498#M1989",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":150,
        "Question_body":"Hello\n\nI am new to the vertex AI. I have created a binary classfication model using hugging face Pytorch model (ROBERTA).\u00a0 Now I am following the google vertex AI documentation to deploy but I am facing somer errors while deploying.\n\nERROR 2023-05-22T08:08:18.241378784Z usage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\nERROR 2023-05-22T08:08:18.241437196Z [--model-store MODEL_STORE]\nERROR 2023-05-22T08:08:18.241444587Z [--workflow-store WORKFLOW_STORE]\nERROR 2023-05-22T08:08:18.241451025Z [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\nERROR 2023-05-22T08:08:18.241456985Z [--log-config LOG_CONFIG] [--foreground]\nERROR 2023-05-22T08:08:18.241462945Z [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\nERROR 2023-05-22T08:08:18.241468906Z torchserve: error: unrecognized arguments: --handler \/home\/jupyter\/Container\/hugging_face_handler.py\nERROR 2023-05-22T08:08:22.568594694Z usage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\nERROR 2023-05-22T08:08:22.568649768Z [--model-store MODEL_STORE]\nERROR 2023-05-22T08:08:22.568657159Z [--workflow-store WORKFLOW_STORE]\nERROR 2023-05-22T08:08:22.568664073Z [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\nERROR 2023-05-22T08:08:22.568670034Z [--log-config LOG_CONFIG] [--foreground]\nERROR 2023-05-22T08:08:22.568676233Z [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\nERROR 2023-05-22T08:08:22.568681955Z torchserve: error: unrecognized arguments: --handler \/home\/jupyter\/Container\/hugging_face_handler.py\n\n\u00a0\n\nDocker Image code\n\nFROM pytorch\/torchserve:latest\n\n# Install additional dependencies if required\n# RUN pip install transformers\n\n# Copy the requirements file to the container\nCOPY requirements.txt \/home\/jupyter\/Container\/requirements.txt\n\n# Install the requirements\nRUN pip install -r \/home\/jupyter\/Container\/requirements.txt\n# Copy your model and inference code to the container\nCOPY cls \/home\/jupyter\/Container\/cls\nCOPY hugging_face_handler.py \/home\/jupyter\/Container\/hugging_face_handler.py\n\n# Set the working directory\nWORKDIR \/home\/jupyter\/Container\/\n\n# Expose the port used by TorchServe (default: 8080)\nEXPOSE 8080\n\n# Start TorchServe with your custom model and handler\nCMD [\"torchserve\", \"--start\", \"--model-store\", \".\", \"--models\", \"my_model=\/home\/jupyter\/Container\/cls\/pytorch_model.bin\", \"--handler\", \"\/home\/jupyter\/Container\/hugging_face_handler.py\"]\n\n\u00a0\n\n\u00a0\n\nHandler file code:\n\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\nimport torch\nfrom sklearn import preprocessing\nimport numpy as np\n\nclass TransformersClassifierHandler(BaseHandler):\n\"\"\"\nThe handler takes an input string and returns the classification text\nbased on the serialized transformers checkpoint.\n\"\"\"\ndef __init__(self):\nsuper(TransformersClassifierHandler, self).__init__()\nself.initialized = False\nself.model = None\nself.tokenizer = None\nself.device = None\n\ndef initialize(self, ctx):\n# self.manifest = ctx.manifest\n# properties = ctx.system_properties\nmodel_dir = \"\/home\/jupyter\/Container\/cls\" # Set the model directory path\nself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load model\nself.model = RobertaForSequenceClassification.from_pretrained(model_dir)\nself.model.to(self.device)\nself.model.eval()\nlogger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n\n# Ensure to use the same tokenizer used during training\nself.tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n\nself.initialized = True\n\ndef preprocess(self, data):\ntext = data.get(\"data\", [\"\"])[0]\nsentences = text.decode('utf-8')\nlogger.info(\"Received text: '%s'\", sentences)\n\n# Tokenize the texts\ntokenizer_args = ((sentences,))\ninputs = self.tokenizer(*tokenizer_args,\npadding='max_length',\nmax_length=512,\ntruncation=True,\nreturn_tensors=\"pt\")\nreturn inputs\n\ndef inference(self, inputs):\nwith torch.no_grad():\noutputs = self.model(**inputs)\nlogits = outputs.logits\nprobabilities = torch.softmax(logits, dim=1)\npredicted_labels = torch.argmax(probabilities, dim=1).item()\nreturn ['Y' if predicted_labels == 1 else 'N']\n\n\ndef postprocess(self, inference_output):\nreturn inference_output\n\n\u00a0\n\n\u00a0\n\nI am not sure where I am going wrong over here and getting the above mentioned error. So any help in this topic would be of great help\n\nThanks\n\nJagdish\n\n(PII Removed by Staff)\n\n(PII Removed by Staff)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Point pip on a artifact registry",
        "Question_tag_count":1,
        "Question_created_time":1687421700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Point-pip-on-a-artifact-registry\/m-p\/605726#M2210",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":125,
        "Question_body":"Hello to evryone. I'm new to Vertex AI and I'm learning step by step.\u00a0\n\nI'm wondering, it is possible to use PIP on Vertex AI workbench but instead of pointing at PyPI by default, pointing to a python artifact registry containing pre-installed python packages? This is due to the fact that for project constraints, it is not possible to direcly reach outside internet.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Quota increase for training on Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":1673579700000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-increase-for-training-on-Vertex-AI\/m-p\/509872#M1061",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":394,
        "Question_body":"I am trying to use the custom container method to train a simple model. I am executing a config.yaml file through which I am running a job but I received the following error.\u00a0\n\nError:\u00a0\"error\": { \"code\": 429, \"message\": \"The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_n2_cpus\", \"status\": \"RESOURCE_EXHAUSTED\" } }\n\nThe config.yaml file looks like this:\u00a0\n\nworkerPoolSpecs:\n\u00a0\u00a0machineSpec:\n\u00a0\u00a0\u00a0\u00a0machineType:\u00a0n1-standard-4\n\u00a0\u00a0\u00a0\u00a0acceleratorType:\u00a0NVIDIA_TESLA_T4\n\u00a0\u00a0\u00a0\u00a0acceleratorCount:\u00a01\n\u00a0\u00a0replicaCount:\u00a01\n\u00a0\u00a0containerSpec:\n\u00a0\u00a0\u00a0\u00a0imageUri:\u00a0gcr.io\/vertexAIdemo\/vertexai-bert-training:latest\n\u00a0\n\u00a0\nI tried to increase quota and it cant be increased. I tried to contact several teams and it was to no help. Please provide me a way to how I can be able to increase the quota to execute the job.\n\u00a0\nPS: no vertex Ai Api quota is available to edit and is currently 0 for all regions be it CPU or GPU",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialog flow cx Integration Twilio",
        "Question_tag_count":2,
        "Question_created_time":1676103780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialog-flow-cx-Integration-Twilio\/m-p\/521061#M1267",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":148,
        "Question_body":"Dialog flow CX having problems in integration twilio options is no more visible.\nPreviously I was able to change\/update the integration easily till 11-02-2023.\n\nNow it just doesn't show any option\u00a0for TWILIO.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocumentAI - How to highlight bounding box after ProcessDocument(Request)",
        "Question_tag_count":3,
        "Question_created_time":1681066680000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocumentAI-How-to-highlight-bounding-box-after-ProcessDocument\/m-p\/542055#M1610",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":171,
        "Question_body":"Hello,\n\nAfter following this example with success:\u00a0https:\/\/cloud.google.com\/document-ai\/docs\/libraries#client-libraries-usage-csharp\n\nQuestion is: How to highlight each detected returned value on source PDF as Google does in their example? If I press F12 on their example it shows tags like:\n\nai-document-view\nai-labeling-layout\nai-annotated-document-view\nai-document-tree-row-annotation-panel\nai-document-row-annotation\nai-connecting-line\n\nI'd like to perform something similar so my users will be able to fix values with low confidence or just let them improving the results viewing the highlighted results on PDF.\n\nAny clue or code snipped will be appreciated.\n\nkind regards,\n\nFrancisco",
        "Question_closed_time":1681070820000,
        "Answer_score_count":0.0,
        "Answer_body":"If we look at the genetic response from a Document AI processing output we see that it is a structured document (JSON).\u00a0 Within the JSON output we get indications of what was detected but also\u00a0where\u00a0it was detected.\u00a0 This makes me think that the PDF is converted into an image representation and the image processed using OCR.\u00a0 The results would then be the bounding polygons of an image representation of where each items was found.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex Batch prediction with BigQuery table as input. Predict API receives input without column name",
        "Question_tag_count":3,
        "Question_created_time":1672290600000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-Batch-prediction-with-BigQuery-table-as-input-Predict-API\/m-p\/504368#M1001",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":274,
        "Question_body":"Hello All!\n\n\nWhen we use BigQuery as input for batch prediction, then my custom container predict API receives requests in the below format\u00a0\n\n\n{\n\u00a0 \u00a0\"instances\": [[1.23, \"xxx\", 379],[2.43, \"yyy\", 184]]\n}\n\n\nThere is no column name in the above example and that's why the request failed with the error \"missing column names\".\nBut it should be in the below format based on documentation,\n\n\n{\n\u00a0\"instances\": [\n\u00a0\u00a0\u00a0 {\n\u00a0\u00a0\u00a0\u00a0 \"feat1\": 1.23,\n\u00a0\u00a0\u00a0\u00a0 \"feat2\": \"xxx\",\n\u00a0\u00a0\u00a0\u00a0 \"feat3\": 379,\n},\n{\n\u00a0\u00a0\u00a0\u00a0 \"feat1\": 2.43,\n\u00a0\u00a0\u00a0\u00a0 \"feat2\": \"yyy\",\n\u00a0\u00a0\u00a0\u00a0 \"feat3\": 184,\n},\n]\n}\n\n\nHow can I fix it as I can't control the code which picks the data from the big query, converts it to the JSON and then sends it to my predict API?\nThanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I would like to assign a custom domain to Vertex AI Endpoints.",
        "Question_tag_count":1,
        "Question_created_time":1688595360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-would-like-to-assign-a-custom-domain-to-Vertex-AI-Endpoints\/m-p\/609623#M2308",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":70,
        "Question_body":"When I deploy a model to Vertex AI Endpoints, it is assigned a default domain ({region}-aiplatform.googleapis.com). How do I switch from the default domain to the custom domain?",
        "Question_closed_time":1689233280000,
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@YoshinaoMori,\n\nWelcome back to Google Cloud Community!\n\nUnfortunately, this is not yet possible when you are creating an endpoint in vertex ai. However if you want this feature you can submit a request using this link:\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\nIf you are really looking for a way to assign a custom domain to the endpoint URL, you can try applying a reverse proxy that will accept the request from the client then it will forward the request to the endpoint URL.\n\nBut if you want to secure a connection for Vertex AI prediction service, It is possible to create a private endpoint for prediction, the prediction URI will look a little bit different from the Vertex AI public endpoint. Here is the format:\n\n\u00a0\n\nhttp:\/\/ENDPOINT_ID.aiplatform.googleapis.com\/v1\/models\/DEPLOYED_MODEL_ID:predict\n\n\u00a0\n\nYou can use this link as a guide on how to create a private endpoint:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\n\nHope this is useful!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Memory issue",
        "Question_tag_count":8,
        "Question_created_time":1655778180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Memory-issue\/m-p\/433242#M383",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":587,
        "Question_body":"Hi friends,\n\n\u00a0\n\nIam facing this error recently -\u00a0\n\nThe replica workerpool0-0 ran out-of-memory and exited with a non-zero status of 137(SIGKILL).\u00a0\n\nKindly help me , i am using 800GB , still getting this error",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI endpoint deployment",
        "Question_tag_count":4,
        "Question_created_time":1667537580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-endpoint-deployment\/m-p\/485783#M734",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":142,
        "Question_body":"How can I utilize the mega GPU during endpoint deployment\u00a0 for vertex ai work? Are there any model for examples or other resources that I can use to better grasp this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex Matching Engine deny list tokens",
        "Question_tag_count":2,
        "Question_created_time":1661345760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-Matching-Engine-deny-list-tokens\/m-p\/459559#M528",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":284,
        "Question_body":"How does the Vertex matching engine deny list work?\n\nLet's say I have a class fruit which will ONLY have deny list tokens (no allow) such as \"apple\", \"mango\", etc. How do I filter out \"mango\" in the query (search all fruits except mango)? I have tried the following method but it does not work as expected:\n\njson\n{\"id\": \"1\", \"embedding\":[0.002792,0.000492], \"restricts\": [{\"namespace\": \"fruit\", \"deny\": [\"mango\"]}]}\n\nquery\ndeny_namespace = match_service_pb2.Namespace()\ndeny_namespace.name = \"fruit\"\ndeny_namespace.deny_tokens.append(\"mango\")\nrequest.restricts.append(deny_namespace)\n\nI have coded this similar to allow list which has worked for me but with deny tokens it does not seem to skip deny tokens even after completely overwriting the index.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"text-bison@001model usage",
        "Question_tag_count":1,
        "Question_created_time":1687745880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/text-bison-001model-usage\/m-p\/606457#M2229",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":24,
        "Question_body":"Hi I tried to use\u00a0text-bison@001\u00a0to generate chapter title from\u00a0 subtitle transcript.\u00a0 But I always I got no result\u00a0 for some chunks, which never happens with openai's model. Anyone can help me on this? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Feature Store Calculations",
        "Question_tag_count":3,
        "Question_created_time":1652363340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-Calculations\/m-p\/422580#M329",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":113,
        "Question_body":"Hi,\n\nI\u00a0have a Google Colab notebook with some functions (Python) that been used to calculate the features for a model.\n\nThe functions use as inputs data from an API.\n\nThe question is if I can or should calculate the features inside a Features Store and feed the results to the Model?\n\nOr in which Instance do I need to make the calculations and then feed the results into the model?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Custom Tune of LLM in Generative AI Studio - training time\/parameters - Costs",
        "Question_tag_count":1,
        "Question_created_time":1689969480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-Tune-of-LLM-in-Generative-AI-Studio-training-time\/m-p\/614610#M2428",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":49,
        "Question_body":"Hi,\n\nI want to tune a LLM in Vertex AI Generative Studio (text-bison001\u00a0) and I know it has 137B parameters. I investigated the costs and they are:\n\nTuning jobs in us-central1 use eight A100 80GB GPUs. Tuning jobs in europe-west4 use 64 cores of the TPU v3 pod custom model training resource, only available upon request. Using a fast calculation, eight A100 80GB will cost 40.22 USD\/hour and the TPU V3 64 cores, supposing is the double of 32 cores, will cost 64 USD\/hour.\n\nI have a proper JSONL dataset with 1,000 to 52,000 examples and I want to train for 300 epochs.\n\nThe issue here is that I need to know how much time usually the tune of text-bison001 takes (1 hour, 10 hours), or at least how many parameters will be tuned, to have an idea about costs involved.\n\nThis information is not provided in Vertex AI Pricing, it is not provided in Generative AI Studio Language documentation. Should I consider the regular Vertex AI pricing in calculator ? Maybe this is not the case, as 8 A100 GPUs will be used.\n\nThanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error 524 on jupyterlab",
        "Question_tag_count":2,
        "Question_created_time":1667614740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-524-on-jupyterlab\/m-p\/486115#M738",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":579,
        "Question_body":"Hi,\n\nI cannot access Jupyterlab by web interface. It still works by ssh. I've followed the support documentations, but nothing works.\n\nMy best guess is that the main issue is with the opened ports of docker.\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\ncurl http:\/\/127.0.0.1:8080\/api\/kernelspecs\ncurl: (7) Failed to connect to 127.0.0.1 port 8080: Connection refused\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nI have the full result of the following command available, but i'm not sure if it's good idea to post it in public.\n\nsudo -i\ncd \/opt\/deeplearning\/bin\/\n.\/diagnostic_tool.sh\n\nI think the main error is from:\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n{\"log\":\"2022\/11\/05 08:47:34 Health Check request failed: Get \\\"http:\/\/localhost:8080\/api\/kernelspecs\\\": dial tcp [::1]:8080: connect: connection refused\\n\",\"stream\":\"stderr\",\"time\":\"2022-11-05T08:47:34.588994439Z\"}\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"PipelineJob using a dedicated service account",
        "Question_tag_count":1,
        "Question_created_time":1678409520000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/PipelineJob-using-a-dedicated-service-account\/m-p\/530893#M1412",
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":353,
        "Question_body":"When I submit my pipelinejob (submit)\n\njob = PipelineJob(...)\njob.submit(\n    service_account='<PII removed by staff>'\n)\n\nI get the following error message:\n\ngrpc_message:\"You do not have permission to act as service_account: <PII removed by staff>. (or it may not exist).\"\n\nI am the owner of the project and when I go through the console to create a pipeline job, there is no issue in specifying another service account to run as. It all works if I remove the service_account parameter, so the default account is used. Any idea what the issue can be?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Batch prediction forecasting",
        "Question_tag_count":3,
        "Question_created_time":1659951120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Batch-prediction-forecasting\/m-p\/451758#M494",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":98,
        "Question_body":"Dear ALL;\n\nI have made a sales and demand forecasting autoML model. It trained well and is working. I am looking for a way to format input data for batch prediction forecasting where I would like to do more than one forecast horizon predictions.\n\nBasically my model uses monthly data granularity with 12 month context length and 6 month forecast horizon. (I have 15 covariate features and predict for 14 separate identifiers).\n\nWhat I would like to do is configure the input data for a batch forecasting where the forecasting would start 3 month earlier than where the feature data end and make forecasts for these three time periods AND when the feature data ends do the normal 6 month forecasting so I end up with 3 month forecasting where I know the actuals and the normal 6 month forecasting.\n\nAt this point no matter how I format my input data I only get the 6 month forecast horizon.\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Impossible to create vertex Ai model version with gcloud ai models",
        "Question_tag_count":2,
        "Question_created_time":1686209580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Impossible-to-create-vertex-Ai-model-version-with-gcloud-ai\/m-p\/601294#M2105",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"I use\u00a0\n\n\u00a0\n\ngcloud ai models upload\n\n\u00a0\n\nto create a model from a custom docker image. But I do not see a way to create a new version of that model in command line? Only with UI console.\n\nIs the feature missing or am I missing something?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Resource Exhaustion Error but resources are not even close to exhausted?...",
        "Question_tag_count":3,
        "Question_created_time":1683166920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Resource-Exhaustion-Error-but-resources-are-not-even\/m-p\/550175#M1784",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":120,
        "Question_body":"I'm attempting to run a basic pipeline using Kubeflow in Vertex AI. However, when I run it I receive a\u00a0RESOURCE_EXHAUSTED error in the logs relating to\u00a0aiplatform.googleapis.com\/custom_model_training_cpus.\n\nChecking my quotas, I can see that I am not anywhere close to exhausting any of the quotas under\u00a0\u00a0aiplatform.googleapis.com\/custom_model_training_cpus\u00a0(including the region I'm using - us-central1).\n\nHas anyone had a similar issue and know what is going on here?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"External url retrieved for locust web interface for ai load testing is not accessible in browser",
        "Question_tag_count":2,
        "Question_created_time":1689209580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/External-url-retrieved-for-locust-web-interface-for-ai-load\/m-p\/611811#M2350",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"Hello Folks,\n\nI am trying to implement \"Load testing and monitoring AI Platform models\" using Vertex AI Workbench. I have deployed the ML model and validated it I have also created the locust image as per docs i followed the steps now the challenge I am facing is that when the external load balancer url is retrieved and I am trying to access that in browser to simulate my load test on the ML API I am getting site cant be reached error .\nthe below url is the external load balancer url where locust is installed and hosted I am trying to access that by providing the load balancer server name as given in docs : -https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/load-testing-and-monitoring-aiplatform-models#:~:text=Locust%20is%20used%20to%20implement,testing%20the%20model%20prediction%20service. .Does anyone have any idea how to resolve this or have anyone faced a similar kind of an issue while trying the steps ? Is there a possibility of some firewall settings that can be tweaked ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Matching Engine: Queries with filtering do not work as expected",
        "Question_tag_count":3,
        "Question_created_time":1668065460000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Matching-Engine-Queries-with-filtering-do-not-work-as-expected\/m-p\/488086#M758",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":304,
        "Question_body":"I tried to run this notebook: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/matching_engi...\nI got an issue with the filtering step:\nLet's say glove100.json is:\n\n{\"id\":\"0\",\"embedding\":[-0.99544,-2.3651],\"restricts\":[{\"namespace\": \"class\", \"allow_list\": [\"0\"]}],\"crowding_tag\":\"a\"}\n{\"id\":\"1\",\"embedding\":[0.42052,-1.1817],\"restricts\":[{\"namespace\": \"class\", \"allow_list\": [\"1\"]}],\"crowding_tag\":\"b\"}\n{\"id\":\"2\",\"embedding\":[-0.10185,0.59817],\"restricts\":[{\"namespace\": \"class\", \"allow_list\": [\"2\"]}],\"crowding_tag\":\"a\"}\n\nIf I try to filter in this way:\n\nfor val in query:\n    request.float_val.append(val)\n\nrestrict = match_service_pb2.Namespace()\nrestrict.name = \"class\"\nrestrict.allow_tokens.append(\"1\")\nrequest.restricts.append(restrict)\nresponse = stub.Match(request)\nresponse\n\n\nI do not get any response (empty result). If from the above code I remove the \"restrict block\", it works (of course without filtering).\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n   request.float_val.append(val)\n# restrict = match_service_pb2.Namespace()\n# restrict.name = \"class\"\n# restrict.allow_tokens.append(\"1\")\n# request.restricts.append(restrict)\nresponse = stub.Match(request)\nresponse\n\n\nresponse:\n\nneighbor {\nid: \"0\"\ndistance: 17.592369079589844\n}\nneighbor {\nid: \"1\"\ndistance: 17.592369079589844\n}\n\nBut, if I add a new vector by Vertex SDK for Python (link\u00a0), in this way:\n\ninsert_datapoints_payload = aiplatform_v1.IndexDatapoint(\ndatapoint_id=\"3\",\nfeature_vector=query,\nrestricts=[{\"namespace\": \"class\", \"allow_list\": [\"3\"]}],\n# crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"), <-- this does not seem to change anything\n)\n\nupsert_request = aiplatform_v1.UpsertDatapointsRequest(\nindex=INDEX_RESOURCE_NAME, datapoints=[insert_datapoints_payload]\n)\n\nindex_client.upsert_datapoints(request=upsert_request)\n\nand then I try to filter in the same way above with filter class == \"3\", I get the right response.\n\nIt seems like the allow_tokens are \"seen\" by vertex only when I insert a new vector by Vertex SDK and not when I specify them in the initial glove100.json.\n\nMoreover, if I update a datapoint where the filter did not work, for example id=1:\n\nupdate_datapoints_payload = aiplatform_v1.IndexDatapoint(\ndatapoint_id=\"1\",\nfeature_vector=embedding,\nrestricts=[{\"namespace\": \"class\", \"allow_list\": [\"1\"]}],\n# crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"),\n)\n\nupsert_request = aiplatform_v1.UpsertDatapointsRequest(\nindex=INDEX_RESOURCE_NAME, datapoints=[update_datapoints_payload]\n)\n\nindex_client.upsert_datapoints(request=upsert_request)\n\nresponse = stub.Match(request)\nresponse\n\n\nThe filter for class=1 starts to work.\n\nIs there a way to know what is actually stored in the index? I mean a kind of \"SELECT * FROM myindex\" in order to check embeddings and tokens stored.\n\nAny ideas on how to solve this issue?\n\nThanks in advance\n\nSpecifications\n\nI tried from local and from workbench, the result is the same.\n\n- Version: Python 3.7.9\n- Platform: Matching Engine. zone: europe-west1",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to train Vertex AI: Generative AI studio on custom dataset?",
        "Question_tag_count":3,
        "Question_created_time":1687735980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-train-Vertex-AI-Generative-AI-studio-on-custom-dataset\/m-p\/606423#M2227",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":121,
        "Question_body":"How to input own dataset for generative ai?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Enconding $session.params.[parameter] on Dialog Flow CX ERROR: ByteString is not valid UTF8",
        "Question_tag_count":3,
        "Question_created_time":1678900380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Enconding-session-params-parameter-on-Dialog-Flow-CX-ERROR\/m-p\/533056#M1429",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":266,
        "Question_body":"I am having a problem to display a message of type $session.params.[parameter] on DialogFlow CX\n\nI am reading a json file from Google Bucket and when the Json is with no special characters it works fine example\u00a0Convencao.\n\nBut when I put some special character example\u00a0Conven\u00e7\u00e3o\u00a0it not working giving me the error below:\n\n   \"FunctionExecution\": {\n    \"Webhook\": {\n      \"URL\": \"example.com\",\n      \"Status\": {\n        \"ErrorCode\": \"INTERNAL\",\n        \"ErrorMessage\": \"Response body [<ByteString@40a8d64c size=182 contents=\\\"{\\\\n  \\\\\\\"session_info\\\\\\\": {\\\\n    \\\\\\\"parameters\\\\\\\": {\\\\n     ...\\\">] is not valid UTF8.\"\n      },\n      \"DisplayName\": \"example\",\n      \"Latency\": \"170 ms\",\n      \"ID\": \"a8660475-f28b-440e-ad4c-4e8506cb6795\"\n    },\n    \"Responses\": [\n      {\n        \"responseType\": \"ENTRY_PROMPT\",\n        \"text\": {\n          \"text\": [\n            \"Escolha o evento:\\n1. $session.params.evento1\\n2. $session.params.evento2\\n3. $session.params.evento3\\n4. Outros Eventos\"\n          ],\n          \"redactedText\": [\n            \"Escolha o evento:\\n1. $session.params.evento1\\n2. $session.params.evento2\\n3. $session.params.evento3\\n4. Outros Eventos\"\n          ]\n        },\n        \"source\": \"VIRTUAL_AGENT\"\n      }\n    ]\n  }\n}\n\n},\n\nThis is the .json that work\n\n{\n   \"eventname1\":{\n      \"eventname\":\"Test Diamond Summit\",\n      \"eventdate\":\"07 at\u00e9 12 de Fevereiro\",\n      \"eventlocation\":\"Kenya\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname2\":{\n      \"eventname\":\"Test de Incentivo 2023\",\n      \"eventdate\":\"08 at\u00e9 12 de Mar\u00e7o\",\n      \"eventlocation\":\"Ilha de Comandatuba\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname3\":{\n      \"eventname\":\"Convencao Florescer 2023\",\n      \"eventdate\":\"Sess\u00f5es complementares: 17 de maio de 2023 - Conven\u00e7\u00e3o: 18, 19 e 20 de maio 2023\",\n      \"eventlocation\":\"Rio de Janeiro \/ RJ\",\n      \"eventsite\":\"example.com\"\n   },\n   \"edicao\":{\n      \"edicaoname\":\"Motiva\u00e7\u00e3o\",\n      \"duracao\":\"1\u00b0 de Dezembro \u00e0 28 de Fevereiro\",\n      \"link\":\"example.com\"\n   },\n   \"businesshourPT\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   },\n   \"businesshourES\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   }\n}\n\nThis is the .json that not work\n\n{\n   \"eventname1\":{\n      \"eventname\":\"Test Diamond Summit\",\n      \"eventdate\":\"07 at\u00e9 12 de Fevereiro\",\n      \"eventlocation\":\"Kenya\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname2\":{\n      \"eventname\":\"Test de Incentivo 2023\",\n      \"eventdate\":\"08 at\u00e9 12 de Mar\u00e7o\",\n      \"eventlocation\":\"Ilha de Comandatuba\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname3\":{\n      \"eventname\":\"Conven\u00e7\u00e3o Florescer 2023\",\n      \"eventdate\":\"Sess\u00f5es complementares: 17 de maio de 2023 - Conven\u00e7\u00e3o: 18, 19 e 20 de maio 2023\",\n      \"eventlocation\":\"Rio de Janeiro \/ RJ\",\n      \"eventsite\":\"example.com\"\n   },\n   \"edicao\":{\n      \"edicaoname\":\"Motiva\u00e7\u00e3o\",\n      \"duracao\":\"1\u00b0 de Dezembro \u00e0 28 de Fevereiro\",\n      \"link\":\"example.com\"\n   },\n   \"businesshourPT\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   },\n   \"businesshourES\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   }\n}\n\nThis is my java code\n\n\/\/Used to collecting bucket data (all parameterized data)\npublic JSONObject readBucket() throws IOException {\n    Storage storage = StorageOptions.getDefaultInstance().getService(); \n    BlobId blobId = BlobId.of(\"test\", \"test_properties.json\");\n    byte[] content = storage.readAllBytes(blobId);\n    String contentString = new String(content,  StandardCharsets.UTF_8);\n\n    JSONObject jsonObj = new JSONObject(contentString.toString());\n    return jsonObj;\n}\n\n\n\/\/Used to build webhookResponse for Dialog Flow CX\npublic JsonObject setSessionParameter(JsonObject parameterObject){\n    \/\/ Constructs the webhook response object\n    JsonObject parameterSessionObject = new JsonObject();\n    parameterSessionObject.add(\"parameters\", parameterObject); \n    JsonObject webhookResponse = new JsonObject();\n    webhookResponse.add(\"session_info\", parameterSessionObject);   \n    return webhookResponse;\n}\n\n\n\n\n@Override\n  public void service(HttpRequest request, HttpResponse response) throws Exception {    \n    \/\/Create DialogFlow Objects\n    Gson gson = new GsonBuilder().setPrettyPrinting().create();\n    Map<String, Object> requestBody = gson.fromJson(request.getReader(), Map.class);\n    Map<String, Object> sessionInfo = (Map<String, Object>) requestBody.get(\"sessionInfo\");\n    Map<String, Object> parameters = (Map<String, Object>) sessionInfo.get(\"parameters\");\n    Map<String, Object> fulfillmentInfo = (Map<String, Object>) requestBody.get(\"fulfillmentInfo\");\n    \n    System.out.println(\"parameters: \"+parameters);\n\n    \/\/Extracting Values of DialogFlow\n    String ani = (String) parameters.get(\"ani\");\n    System.out.println(\"ani: \"+ani);\n    String tag = (String) fulfillmentInfo.get(\"tag\");\n    System.out.println(\"tag: \"+tag);\n    \n    JSONObject con;\n    String status;\n    JsonObject parameterObject;\n    String jsonResponseObject;\n    BufferedWriter writer;\n    String intervalName;\n    JSONObject properties;\n    String eventname;\n    String eventname1value;\n    String eventname2value;\n    String eventname3value;\n    String eventdate;\n    String eventlocation;\n    String eventsite;\n    JSONObject edicao;\n    String edicaoduracao;\n    String edicaonome;\n    String edicaolink;\n    JSONObject eventname1;\n    JSONObject eventname2;\n    JSONObject eventname3;\n    JSONObject eventnamedtmf;\n    JSONObject bhour;\n    String iaeventmenudtmf = null;\n    String intervalNameFormat;\n    \n\n    \/\/Checking Routes\n    try {\n        switch(tag) {\n        case \"11040\":\n            properties = readBucket();\n\n            \/\/Writing values on Objects that will became Parameters\n        eventname1 = properties.getJSONObject(\"eventname1\");\n        eventname1value = eventname1.getString(\"eventname\");\n        eventname2 = properties.getJSONObject(\"eventname2\");\n        eventname2value = eventname2.getString(\"eventname\");\n        eventname3 = properties.getJSONObject(\"eventname3\");\n        eventname3value = eventname3.getString(\"eventname\");\n    \n            \/\/Creating Response Objects\n            parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"evento1\", eventname1value.toString());    \n            parameterObject .addProperty(\"evento2\", eventname2value.toString());    \n            parameterObject .addProperty(\"evento3\", eventname3value.toString());\n            jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n            \n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObject.toString());\n            break;\n    \n        case \"11100\":\n            \/\/Get user option to concat file bucket file\n            iaeventmenudtmf = (String) parameters.get(\"iaeventmenu\");\n            System.out.println(\"iaeventmenudtmf: \"+iaeventmenudtmf);\n              \n            properties = readBucket();\n\n            \/\/Writing values on Objects that will became Parameters\n            eventnamedtmf = properties.getJSONObject(\"eventname\"+iaeventmenudtmf);\n        eventname = eventnamedtmf.getString(\"eventname\");\n        eventdate = eventnamedtmf.getString(\"eventdate\");\n        eventlocation = eventnamedtmf.getString(\"eventlocation\");\n        eventsite = eventnamedtmf.getString(\"eventsite\");\n            \n        \/\/Creating Response Objects\n            parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"evento\", eventname.toString());   \n            parameterObject .addProperty(\"eventoData\", eventdate.toString());   \n            parameterObject .addProperty(\"eventoLocal\", eventlocation.toString());      \n            parameterObject .addProperty(\"eventoSite\", eventsite.toString());\n            jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n            \n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObject.toString());\n            break;\n              \n        case \"12040\":\n            properties = readBucket(); \n    \n            \/\/Writing values on Objects that will became Parameters\n            edicao = properties.getJSONObject(\"edicao\");\n            edicaonome = edicao.getString(\"edicaoname\");\n            edicaoduracao = edicao.getString(\"duracao\");\n            edicaolink = edicao.getString(\"link\");\n             \n            \/\/Creating Response Objects\n            parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"edicao\", edicaonome.toString());      \n            parameterObject .addProperty(\"duracao\", edicaoduracao.toString());      \n            parameterObject .addProperty(\"link\", edicaolink.toString());\n            jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n            \n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObject.toString());\n            break;\n                \n        default:\n            break;\n            \n        }\n    }catch(Exception e) {\n        parameterObject  = new JsonObject();\n        parameterObject .addProperty(\"error\", \"999 Error on request - Class:  \" + getClass().getName() + \": Message\" + e.getMessage()+ \" Cause: \"+e.getCause() + \" Stack: \" +Arrays.toString(e.getStackTrace()));       \n        jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n        System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n          \n        \/\/Sends the responseObject\n        writer = response.getWriter();\n        writer.write(jsonResponseObject.toString());\n    }\n  }\n}\n\nThis is my return on Google Cloud Function Log\n\njsonResponseObject: {   \"session_info\": {\n    \"parameters\": {\n      \"evento1\": \"Blue Diamond Summit\",\n      \"evento2\": \"Viagem de Incentivo 2023\",\n      \"evento3\": \"Conven\u00e7\u00e3o Florescer 2023\"\n    }   } }\n\nWhen it passes\u00a0Conven\u00e7\u00e3o\u00a0I receive the error above when it is\u00a0Convencao\u00a0it displays the message with success.\n\nI tried some enconding UTF-8 ways without suceess\n\nI tried some solutions below\n\n            byte[] jsonResponseObjectb = jsonResponseObject.getBytes(StandardCharsets.UTF_8);\n        String jsonResponseObjects = new String(jsonResponseObjectb, StandardCharsets.UTF_8);\n\n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObjects);\n\n\n        \/\/handle encoding error on Dialog Flow CX\n        CharsetDecoder decoder = StandardCharsets.UTF_8.newDecoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);\n\n        byte[] eventname1valueb = eventname1value.getBytes(StandardCharsets.UTF_8);\n        String eventname1values = decoder.decode(ByteBuffer.wrap(eventname1valueb)).toString();\n            \n        byte[] eventname2valueb = eventname2value.getBytes(StandardCharsets.UTF_8);\n        String eventname2values = decoder.decode(ByteBuffer.wrap(eventname2valueb)).toString();\n\n        byte[] eventname3valueb = eventname3value.getBytes(StandardCharsets.UTF_8);\n        String eventname3values = decoder.decode(ByteBuffer.wrap(eventname3valueb)).toString(); \n            \n            \n                \/\/Creating Response Objects\n        JsonObject parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"evento1\", eventname1value.toString());    \n            parameterObject .addProperty(\"evento2\", eventname2value.toString());    \n            parameterObject .addProperty(\"evento3\", eventname3value.toString());\n            String jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            \n                \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObjects);\n\n\n            byte[] jsonResponseObjectb = jsonResponseObject.getBytes(StandardCharsets.UTF_8);\n        String jsonResponseObjects = new String(jsonResponseObjectb, StandardCharsets.UTF_8);\n            String encodedString = URLEncoder.encode(jsonResponseObjects, \"UTF-8\");\n\n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(encodedString);\n\n\n            CharsetDecoder decoder = StandardCharsets.UTF_8.newDecoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);\nbyte[] \n                jsonResponseObjectb = jsonResponseObject.getBytes(StandardCharsets.UTF_8);\n        String jsonResponseObjects = decoder.decode(ByteBuffer.wrap(eventname3valueb)).toString();\n\n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObjects);\n\nMy return must be\n\nChoose the event:\n\nTest Diamond Summit\nTest de Incentivo 2023\nConven\u00e7\u00e3o\u00a0Test 2023\nOther Events\n\nbut in it is\u00a0Conven\u00e7\u00e3o\u00a0I received\n\nChoose the event:\n\n$session.params.evento1\n$session.params.evento2\n$session.params.evento3\nOther Events\n\nbut in it is\u00a0Convencao\u00a0I received\n\nChoose the event:\n\nTest Diamond Summit\nTest de Incentivo 2023\nConvencao\u00a0Test 2023\nOther Events\n\nNo one of them worked\n\nI do not know more what I could try to do to solve this error\n\nCould you help me please?\n\nTried to use some enconding methods that I know",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Over fitting during RL or DRL when using tabular data",
        "Question_tag_count":5,
        "Question_created_time":1650279180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Over-fitting-during-RL-or-DRL-when-using-tabular-data\/m-p\/414640#M280",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"Hi\n\nI will like to know how the Vertex Api handles or warns about models with over fitting conditions when using Reinforcement Learning or Deep Reinforcement Learning ? If so can you help me with the documents where you explain this situations when using the Vertex Api for tabular dataframes?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"OCR text recognition for changing images.",
        "Question_tag_count":2,
        "Question_created_time":1673974140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/OCR-text-recognition-for-changing-images\/m-p\/511081#M1077",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":249,
        "Question_body":"Hi,\u00a0\n\nI'm new to Cloud vision and am looking into how I can Scan an image and detect a specific Job Number E.g. 28379 that has a unique colour. We have images of steel which have lots of text and we figured if we write the Job number in a different colour, we could extract only that information to then rename the images to this job number and if there is multiple of the same job number append the data with a (1),(2),(3) etc on the end of each image. My goal is to automate renaming a full folder of photos based of the Job number detected in each image.\u00a0\n\nWe are using Power Automate for most of our automation process, so ideally we would like to access the Cloud via Power Automate (Text detection via API Gateway key to access Google Cloud SDK Shell).\u00a0\n\nDoes anyone know how to do this or any advice\/tips on how to get this working.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard: The Next Generation of AI",
        "Question_tag_count":3,
        "Question_created_time":1686051660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-The-Next-Generation-of-AI\/m-p\/600583#M2088",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":33,
        "Question_body":"Hey guys,\n\nI published an article on Bard with everything you should know about Google's collaborative AI service that will help boost your productivity, and bring your ideas to life.\n\nYou can check it out here.\n\nGoogle Bard: The Next Generation of AI (hashnode.dev)\n\nhttps:\/\/emmanuelogebe.hashnode.dev\/google-bard-the-next-generation-of-ai",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Experiment tracking - Metadata store",
        "Question_tag_count":2,
        "Question_created_time":1680487440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Experiment-tracking-Metadata-store\/m-p\/540132#M1567",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":75,
        "Question_body":"Hey,\n\ni got a question regarding the MLOps-Principles. Is the metadatastore one component of experiment tracking or are those different aspects of MLOps. What i thought, experiment tracking tracks the metadata for the experiments in creating a training pipeline and the metadata store tracks the information about the pipeline runs in production. Or is everything of that included in experiment tracking?\n\nCan somebody help me with that? Thanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Batch Predictions: Bigquery format must be used as input and output simultaneously",
        "Question_tag_count":2,
        "Question_created_time":1667318280000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Batch-Predictions-Bigquery-format-must-be-used-as\/m-p\/484734#M713",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":250,
        "Question_body":"I'm encountering an error when I try to create a batch prediction job with a bigquery table as my input, and a JSONL output in a GCS bucket. The documentation\u00a0for batch predictions seems to indicate that I can do so, but I still see an error.\n\nI'm trying to create a batch prediction job on the Vertex AI console, and I see this error.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How does it work underhood: Predictions of multiple instances (Batch) to Vertex AI online serving",
        "Question_tag_count":1,
        "Question_created_time":1661981100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-does-it-work-underhood-Predictions-of-multiple-instances\/m-p\/462022#M554",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":81,
        "Question_body":"Hello,\n\nVertex AI online serving:\n\nWhen multiple instances are passed for prediction to an endpoint, Does prepackaged container serve the inferences in the same manner as TFX Serving does with enable_batching.\u00a0 If so how do we optimize batching parameters with multiple instances sent to Vertex AI online.\n\nIf multi_instances prediction is different from TFX serving batching, how do we gain GPU resources efficient usage optimization with prepackaged serving container.\n\nOn a general note, how to handle efficient GPU usage for both prepackaged container and custom container using a custom trained model.\n\nPlease guide.\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"PyTorch is using the GPU on a container on my local machine, but is unable to use the GPU on Vertex",
        "Question_tag_count":1,
        "Question_created_time":1661240400000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/PyTorch-is-using-the-GPU-on-a-container-on-my-local-machine-but\/m-p\/458980#M523",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":237,
        "Question_body":"Expected Behavior\nI want to use a GPU on a component of Vertex AI.\n\nActual Behavior\nUnfortunately, `torch.cuda.is_available()` is returning `False`. Also, `nvidia-smi` is not working if ran in the container of Vertex AI.\nNote: both commands also don't work locally in the container if I'm not specifying the `--gpus all` flag in the command `docker run --rm -it --gpus all ee97db5bbd98 \/bin\/bash`. However, I can't find any option to add the `--gpus all` flag for Vertex AI. Would this be required?\n\nSteps to Reproduce the Problem\n\nMy YAML file:\n\nname: Processing\ndescription: Process all the found HTML\n\ninputs:\n- name: friendly_name\ntype: String\ndescription: The name of the company\n- name: language\ntype: String\ndescription: The language to process\n- name: models\ntype: String\ndescription: The models that will be used (all, genre, or standard)\nimplementation:\ncontainer:\nimage: eu.gcr.io\/uman-interns\/backend:v1.7\ncommand:\n[\npython,\nbackend\/pages\/III_Process_website_data\/process_website_data.py,\n--friendly_name,\n{ inputValue: friendly_name },\n--language,\n{ inputValue: language },\n--models,\n{ inputValue: models }\n]\n\n\u00a0\n\nMy pipeline:\n\nrom kfp.v2 import compiler, dsl\nimport kfp.components as comp\n\nfrom config import GCS_ARTIFACT_BUCKET, VARS\n\nprocessing = comp.load_component_from_file(\"scraping\/components\/processing.yaml\")\nembeddings = comp.load_component_from_file(\"scraping\/components\/embeddings.yaml\")\n\n\ndef compile_pipeline(file_name: str, tag: str):\n@dsl.pipeline(\nname=\"scraping\",\ndescription=\"Scrape a site and extract meaningful topics\",\npipeline_root=f\"gs:\/\/{GCS_ARTIFACT_BUCKET}\/scraping\/{tag}\",\n)\ndef pipeline(\nfriendly_name: str, url: str, language: str, google: bool, models: str) :\nPROJECT_ID = VARS[\"PROJECT_ID\"]\n\nprocess = (\nprocessing(friendly_name, language, models)\n.set_display_name(\"URL processing\")\n.set_env_variable(\"PROJECT_ID\", PROJECT_ID)\n.set_caching_options(enable_caching=False)\n.set_cpu_limit(\"4\")\n.set_memory_limit(\"16G\")\n.add_node_selector_constraint(\n\"cloud.google.com\/gke-accelerator\", \"NVIDIA_TESLA_T4\"\n)\n.set_gpu_limit(1)\n).after(crawling)\nembed = (\nembeddings(friendly_name, language, models)\n.set_display_name(\"Create embeddings\")\n.set_env_variable(\"PROJECT_ID\", PROJECT_ID)\n.set_caching_options(enable_caching=False)\n.set_cpu_limit(\"4\")\n.set_memory_limit(\"16G\")\n.add_node_selector_constraint(\n\"cloud.google.com\/gke-accelerator\", \"NVIDIA_TESLA_T4\"\n)\n.set_gpu_limit(1)\n).after(process)\n\ncompiler.Compiler().compile(pipeline, file_name)\n\n\nVisualized in the browser:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI deploy custom model error - Model server terminated: model server container terminated:",
        "Question_tag_count":2,
        "Question_created_time":1668750420000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-deploy-custom-model-error-Model-server-terminated\/m-p\/490796#M811",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":590,
        "Question_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to deploy custom model to vertex-ai endpoint.\n\nCommand:\n\n\u00a0\n\ngcloud ai endpoints deploy-model {ENDPOINT_ID}\\\n  --region={REGION} \\\n  --model={MODEL_ID} \\\n  --display-name={DEPLOYED_MODEL_NAME} \\\n  --machine-type=n1-standard-2 \\\n  --enable-access-logging \\\n  --enable-container-logging \\\n  --min-replica-count=1 \\\n  --max-replica-count=100 \\\n  --traffic-split=0=100\n\n\u00a0\n\nError:\nUsing endpoint [https:\/\/europe-west3-aiplatform.googleapis.com\/]\nERROR: (gcloud.beta.ai.endpoints.deploy-model) Model server terminated: model server container terminated: exit_code: 0\nreason: \"Completed\"\nstarted_at { seconds: 1668599448 }\nfinished_at { seconds: 1668599448 } .\n\nThe same error appears if I try to deploy from UI.\nNote: I was able to import my custom model, and also I can create a new endpoint, but if I try to deploy model or make batch prediction this error appears.\u00a0Where am I wrong?\n\nPlease help me",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"how to update model version on model registry",
        "Question_tag_count":1,
        "Question_created_time":1686014760000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/how-to-update-model-version-on-model-registry\/m-p\/600399#M2086",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"Hi, currently I'm working on vertex ai pipelines. I'm confused on how to update my model version on model registry.\n\nVersions:\n\n\u00a0\n\nkfp                                    2.0.0rc1\nkfp-pipeline-spec                      0.2.2\nkfp-server-api                         2.0.0rc1\ngoogle_cloud_pipeline_components       2.0.0b\n\n\u00a0\n\nHere's the code snippet:\n\n\u00a0\n\nmodels = aiplatform.Model.list(filter=(\u201cdisplay_name={}\u201c).format(MODEL_DISPLAY_NAME))\nparent_model = models[0].resource_name\nmodel_upload_op = ModelUploadOp(\nproject=PROJECT_ID,\ndisplay_name=MODEL_DISPLAY_NAME,\nparent_model=,\nunmanaged_container_model=unmanaged_model_importer.outputs[\u201cartifact\u201d],\n).after(unmanaged_model_importer)\n\n\u00a0\n\nError message:\n\n\n\u00a0\n\nInconsistentTypeException: Incompatible argument passed to the input 'parent_model' of component 'model-upload':\nArgument type 'STRING' is incompatible with the input type 'google.VertexModel@0.0.1'\n\n\u00a0\n\nIs there any way to resolve this?\u00a0\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Batch prediction on custom model",
        "Question_tag_count":3,
        "Question_created_time":1657712340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Batch-prediction-on-custom-model\/m-p\/442147#M414",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi,\n\nI used custom containers for training and prediction to create a model on Vertex AI. Now I want to run batch prediction against it but get error message that says \"Unable to start batch prediction job due to the following error: A model using a third-party image must specify PredictRoute and HealthRoute in ContainerSpec.\"\n\nI checked documentation,\u00a0AIP_HEALTH_ROUTE =\u00a0\/v1\/endpoints\/ENDPOINT\/deployedModels\/DEPLOYED_MODEL\n\nDoes this mean that the model has to be deployed to an endpoint in order to generate\u00a0the value of the AIP_ENDPOINT_ID variable?\n\nHowever, the documentation \u201cGet batch predictions\u201d says \u201cRequesting a batch prediction is an asynchronous request (as opposed to online prediction, which is a synchronous request). You request batch predictions directly from the model resource; you don't need to deploy the model to an endpoint.\n\nI am confused whether in my situation, the model has to be deployed first. Also, is there any resources regarding hosting custom models for batch predictions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Bigquery ML billiing support",
        "Question_tag_count":3,
        "Question_created_time":1660874340000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Bigquery-ML-billiing-support\/m-p\/455616#M516",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"I have been working on Demand Forecasting in Bigquery ML and i have been creating model with different datasets but recently for a particular dataset price spiked up for the CREATE MODEL query while working on the dataset can anyone help me regarding training model it is because of dataset or any other matter associated with it",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Question_tag_count":3,
        "Question_created_time":1668498960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/m-p\/489639#M786",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":196,
        "Question_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000.\n\n\u00a0\n\nThanks!",
        "Question_closed_time":1668513420000,
        "Answer_score_count":0.0,
        "Answer_body":"Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"What's the order for the labels in txt file after I have exported my tflite model from Vertex AI",
        "Question_tag_count":4,
        "Question_created_time":1666354380000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-order-for-the-labels-in-txt-file-after-I-have\/m-p\/480804#M674",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":498,
        "Question_body":"I have exported my trained tflite model. But I noticed the order of the labels in the txt file matters. I'm using image classification models. The ones with only two labels, it's an easy fix. I just switch the two. But when I have more than two labels, I notice the predictions are way off. Does it say in Vertex AI or is there a general rule to what label should go first, second, third..etc in the txt file that we create on our own?",
        "Question_closed_time":1666711620000,
        "Answer_score_count":0.0,
        "Answer_body":"After reviewing more about Export AutoML Edge models, you can see the following TensorFlow documentation to learn more about extracting this information.\n\nTensorFlow Lite inference with metadata\nGenerate model interfaces with TensorFlow Lite code generator\nAdding metadata to TensorFlow Lite models\n\nThe documentation that might help more for your question is the last one \u201cAdding metadata to TensorFlow Lite Models\u201d.\n\nBut what I can suggest to you is to send an email to tensorflow-enterprise-support@google.com with your question, and hopefully they can give you a direct solution to your concerns.\n\nAdditionally, I found this Stack Overflow question to create labels.txt manually.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Cannot increase machine type on AI Platform Prediction",
        "Question_tag_count":1,
        "Question_created_time":1686125100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-increase-machine-type-on-AI-Platform-Prediction\/m-p\/600914#M2094",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":149,
        "Question_body":"We are running GPU-based online predictions on our AI Platform.\nWe are applying auto-scaling, but wanted to improve the machine type because the CPU load is higher than the GPU.\n\nThe original machine type we were using was n1-standard-4 and we tried to raise it from there to n1-standard-8 or higher, but we got an error on versions create.\nThe details of the error are below.\n```\n\"error\": {\n\"code\": 429,\n\"message\": \"The requested number of n1-standard-8 exceeds the quota limit. Current usage\/limit: 0\/20, Requested: 24.\",\n\"status\": \"RESOURCE_EXHAUSTED\"\n}\n```\nI thought I could apply to increase the quota, but I don't know which quota it corresponds to.\n\nOtherwise, I tried the following\n1. if minNodes=1 in n1-standard-8, I was able to deploy, but could not find the resource that was being used more. 2.\n2. we were able to create a new n1-standard-16 (with GPU) in Compute Engine.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"When will Generative AI Studio have access to the internet?",
        "Question_tag_count":2,
        "Question_created_time":1688385480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Generative-AI-Studio-have-access-to-the-internet\/m-p\/608924#M2275",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":104,
        "Question_body":"Hey everyone, does anyone know\u00a0when Generative AI Studio will have access to the internet?\n\nWhen I prompted Generative AI Studio with \"When did the Queen died?\", its giving me silly answers, so this tells me Generative AI Studio doesn't currently have access to the internet (unlike Google Bard which has access to the internet).\n\nIf\u00a0 Generative AI Studio isn't having this feature anytime soon, what's the best way for me to add this feature into my Python code?\n\nWould really appreciate if anyone can give any pointers. Thanks!",
        "Question_closed_time":1688432640000,
        "Answer_score_count":2.0,
        "Answer_body":"There is no public information about when Generative AI Studio will have access to the internet. However, you can add this feature to your Python code by using a library like Google Search API or DuckDuckGo API. These libraries allow you to search the internet and get the results back in JSON format. You can then use this JSON data to provide more accurate and up-to-date information to Generative AI Studio.\n\nHere is an example of how you can use Google Search API to add internet access to Generative AI Studio:\n\nPython\n\u00a0\nimport googlesearch def get_internet_info(query): \"\"\"Gets information from the internet about the given query.\"\"\" results = googlesearch.search(query, num_results=1) return results[0].url def generate_text(prompt, internet_info): \"\"\"Generates text using Generative AI Studio with internet access.\"\"\" return gca.generate_text(prompt, internet_info) if __name__ == \"__main__\": query = \"When did the Queen died?\" internet_info = get_internet_info(query) text = generate_text(query, internet_info) print(text)\n\nThis code will first use Google Search API to get the URL of the first search result for the given query. It will then use this URL to provide internet access to Generative AI Studio. Finally, it will generate text using Generative AI Studio with internet access.\n\nYou can also use other libraries like DuckDuckGo API to add internet access to Generative AI Studio. The process is similar to using Google Search API.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Next Step from Google Colab +Pro",
        "Question_tag_count":3,
        "Question_created_time":1652171400000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Next-Step-from-Google-Colab-Pro\/m-p\/421797#M322",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":475,
        "Question_body":"Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some models\n\nWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?\n\nShould I get a Google Compute Engine and try to connect the google colab files to it?\n\nShould I up load the model to vertex AI?\n\nWhat characteristics should I need to take into consideration before I select any of the different tools?",
        "Question_closed_time":1652442120000,
        "Answer_score_count":0.0,
        "Answer_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How can I explicitly authenticate to the ai-platform using the java PredictionServiceClient",
        "Question_tag_count":1,
        "Question_created_time":1669041720000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-explicitly-authenticate-to-the-ai-platform-using-the\/m-p\/491537#M833",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":446,
        "Question_body":"I have a model hosted on a Google Cloud endpoint and I would like to access it via the Java client.\u00a0 I've created a service account and a key for that service account with the , when I run my client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, I am able to call the service.\u00a0 When I try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.\u00a0\u00a0\n\nThe code is as follows\n\n```\n\nPredictionServiceSettings predictionServiceSettings =\n        PredictionServiceSettings.newBuilder().setEndpoint(location + \"-aiplatform.googleapis.com:443\")\n                .setCredentialsProvider(FixedCredentialsProvider.create(ServiceAccountCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\"))))\n                .build();\npredictionServiceClient = PredictionServiceClient.create(predictionServiceSettings);\nendpointName = EndpointName.of(project, location, endpointId);\nValue featureVal = Value.newBuilder().setStructValue(features).build();\nPredictResponse response =  predictionServiceClient.predict(\n        endpointName,\n        Collections.singletonList(featureVal),\n        Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build());\n\n\n\n```",
        "Question_closed_time":1669109220000,
        "Answer_score_count":0.0,
        "Answer_body":"Hi,\n\nUpon checking your code, FixedCredentialsProvider.create()\u00a0accepts\u00a0com.google.auth.Credentials\u00a0as a parameter. Can you try a Credentials object to\u00a0FixedCredentialsProvider.create()? See code below:\n\nGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\")).createScoped(Lists.newArrayList(\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"));\n\n\u00a0If code above did not work, can you provide the stack trace of the error? Also what roles did you assign on your service account?\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI -- text to image",
        "Question_tag_count":2,
        "Question_created_time":1688051220000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-text-to-image\/m-p\/607902#M2264",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":160,
        "Question_body":"I saw a YouTube video showing the incredible capability to use Vertex AI as a tool for prompt to image (similar to Leonardo AI).\u00a0 But I cannot seem to find that capability when going to my Vertex AI landing page.\u00a0 All I see under my Generative AI heading is Language and Speech.\u00a0 How to I get to the Vertex AI Image Generation page?\u00a0 Thx.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using tensorflow in python, can I use vertex?",
        "Question_tag_count":1,
        "Question_created_time":1687150440000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-tensorflow-in-python-can-I-use-vertex\/m-p\/604448#M2186",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"Hello,\u00a0\n\nI have a script:\u00a0https:\/\/github.com\/Louvivien\/ProductWatcher\/blob\/master\/estimatepriceforgivendays_anyproduct.py\n\nThat i have deployed on Google Cloud Run. I like that it is link to my github and deployed when I push. But when I run it it is very slow. Is there a way to use GPU or to use Vertex for this?\u00a0\n\nWhat is the best way to use python script with Google Cloud Run?\u00a0\n\nRegards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issues with importing aiplatform",
        "Question_tag_count":1,
        "Question_created_time":1668388200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/m-p\/489087#M771",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.\n\nWhen running \"from google.cloud import aiplatform\", I get the following error message:\n\nImportError                               Traceback (most recent call last)\n\/tmp\/ipykernel_22080\/3236611779.py in <module>\n      4 #!python #3.7.12\n      5 \n----> 6 from google.cloud import aiplatform\n      7 \n      8 import tensorflow as tf\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     24 \n     25 from google.api_core import client_options\n---> 26 from google.api_core import gapic_v1\n     27 import google.auth\n     28 from google.auth import credentials as auth_credentials\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/__init__.py in <module>\n     17 from google.api_core.gapic_v1 import config_async\n     18 from google.api_core.gapic_v1 import method\n---> 19 from google.api_core.gapic_v1 import method_async\n     20 from google.api_core.gapic_v1 import routing_header\n     21 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method_async.py in <module>\n     20 import functools\n     21 \n---> 22 from google.api_core import grpc_helpers_async\n     23 from google.api_core.gapic_v1 import client_info\n     24 from google.api_core.gapic_v1.method import _GapicCallable\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers_async.py in <module>\n     23 \n     24 import grpc\n---> 25 from grpc import aio\n     26 \n     27 from google.api_core import exceptions, grpc_helpers\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/aio\/__init__.py in <module>\n     21 \n     22 import grpc\n---> 23 from grpc._cython.cygrpc import (init_grpc_aio, shutdown_grpc_aio, EOF,     24                                  AbortError, BaseError, InternalError,\n     25                                  UsageError)\n\nImportError: cannot import name 'shutdown_grpc_aio' from 'grpc._cython.cygrpc' (\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_cython\/cygrpc.cpython-37m-x86_64-linux-gnu.so)\n\nThe versions of the concerned libraries are shown below.\n\ngoogle-api-core                       2.10.1\ngoogle-api-python-client              2.55.0\ngoogle-cloud-aiplatform               1.17.0\n\ngrpcio                                1.33.1\ngrpcio-gcp                            0.2.2\ngrpcio-status                         1.47.0\n\nI have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?\n\nThank you!",
        "Question_closed_time":1668446100000,
        "Answer_score_count":1.0,
        "Answer_body":"Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"How do I deploy my custom model I have trained on workbench GCP?",
        "Question_tag_count":2,
        "Question_created_time":1683597300000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-deploy-my-custom-model-I-have-trained-on-workbench-GCP\/m-p\/551556#M1834",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"I have trained a detectron2 model on vertex ai workbench. i have NOT used tensorflow, xgboost or scikit-learn.\u00a0\n\ni have a model.pth file and a metrics.json file stored in my bucket when i run the model.\u00a0\n\nHow do i deploy this model on GCP and further evaluate it? Is it possible for me to create an endpoint and directly deploy my model on this or will i have to use something like cloud build?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Pipeline Template Schema Issue\/Question",
        "Question_tag_count":3,
        "Question_created_time":1681897320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Pipeline-Template-Schema-Issue-Question\/m-p\/545368#M1704",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":211,
        "Question_body":"Hi all,\n\nWhen compiling our artifacts to yaml file, we are expecting the following schema format in our runtime parameter value :\u00a0https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/api\/v2alpha1\/pipeline_spec.proto#L677\u00a0\n\nWhen I first tried to upload my yaml file with the following schema above and try to run the pipeline template, the runtime value treats everything as a string instead of converting it to it's appropriate data type.\u00a0\n\nHere is the yaml file I tried to upload and run:\n```\n\ncomponents:\n\u00a0 input_data:\n\u00a0 \u00a0 executorLabel: input_data_executor\n\u00a0 \u00a0 inputDefinitions:\n\u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 outputDefinitions:\n\u00a0 \u00a0 \u00a0 artifacts:\n\u00a0 \u00a0 \u00a0 \u00a0 output_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifactType:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instanceSchema: 'title: tfx.String\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: object\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '\ndeploymentSpec:\n\u00a0 executors:\n\u00a0 \u00a0 input_data_executor:\n\u00a0 \u00a0 \u00a0 container:\n\u00a0 \u00a0 \u00a0 \u00a0 args:\n\u00a0 \u00a0 \u00a0 \u00a0 - --executor_class_path\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx_common.components.bigquery.executor.BigQueryComponentExecutor\n\u00a0 \u00a0 \u00a0 \u00a0 - --json_serialized_invocation_args\n\u00a0 \u00a0 \u00a0 \u00a0 - '{{$}}'\n\u00a0 \u00a0 \u00a0 \u00a0 - --project=projectid\n\u00a0 \u00a0 \u00a0 \u00a0 - --region=us-central1\n\u00a0 \u00a0 \u00a0 \u00a0 - --temp_location=gs:\/\/tmp\n\u00a0 \u00a0 \u00a0 \u00a0 - --runner=DataflowRunner\n\u00a0 \u00a0 \u00a0 \u00a0 - --experiments=use_runner_v2\n\u00a0 \u00a0 \u00a0 \u00a0 - --sdk_container_image=image:latest\n\u00a0 \u00a0 \u00a0 \u00a0 command:\n\u00a0 \u00a0 \u00a0 \u00a0 - python\n\u00a0 \u00a0 \u00a0 \u00a0 - -m\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor\n\u00a0 \u00a0 \u00a0 \u00a0 image: image:latest\npipelineInfo:\n\u00a0 name: hello-world\nroot:\n\u00a0 dag:\n\u00a0 \u00a0 tasks:\n\u00a0 \u00a0 \u00a0 input_data:\n\u00a0 \u00a0 \u00a0 \u00a0 cachingOptions:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enableCache: true\n\u00a0 \u00a0 \u00a0 \u00a0 componentRef:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 \u00a0 \u00a0 \u00a0 inputs:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 intValue: 1\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 intValue: 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringValue: CREATE_IF_NEEDED\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: custom-config\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringValue: 'null'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: input-table\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 intValue: 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringValue: WRITE_EMPTY\n\u00a0 \u00a0 \u00a0 \u00a0 taskInfo:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 inputDefinitions:\n\u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 custom-config:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 input-table:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\nschemaVersion: 2.1.0\nsdkVersion: tfx-1.12.0\n```\n\u00a0\nResult:\n\nBut what I'm expecting is something like this (this run is submitted through the ai platform python library):\n\nAs an alternative solution to get the above screenshot (the expected one) - I changed the yaml file as a POC to the following:\n\n```\n\ncomponents:\n\u00a0 input_data:\n\u00a0 \u00a0 executorLabel: input_data_executor\n\u00a0 \u00a0 inputDefinitions:\n\u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 outputDefinitions:\n\u00a0 \u00a0 \u00a0 artifacts:\n\u00a0 \u00a0 \u00a0 \u00a0 output_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifactType:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instanceSchema: 'title: tfx.String\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: object\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '\ndeploymentSpec:\n\u00a0 executors:\n\u00a0 \u00a0 input_data_executor:\n\u00a0 \u00a0 \u00a0 container:\n\u00a0 \u00a0 \u00a0 \u00a0 args:\n\u00a0 \u00a0 \u00a0 \u00a0 - --executor_class_path\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx_common.components.bigquery.executor.BigQueryComponentExecutor\n\u00a0 \u00a0 \u00a0 \u00a0 - --json_serialized_invocation_args\n\u00a0 \u00a0 \u00a0 \u00a0 - '{{$}}'\n\u00a0 \u00a0 \u00a0 \u00a0 - --project=projectid\n\u00a0 \u00a0 \u00a0 \u00a0 - --region=us-central1\n\u00a0 \u00a0 \u00a0 \u00a0 - --temp_location=gs:\/\/tmp\n\u00a0 \u00a0 \u00a0 \u00a0 - --runner=DataflowRunner\n\u00a0 \u00a0 \u00a0 \u00a0 - --experiments=use_runner_v2\n\u00a0 \u00a0 \u00a0 \u00a0 - --sdk_container_image=image:latest\n\u00a0 \u00a0 \u00a0 \u00a0 command:\n\u00a0 \u00a0 \u00a0 \u00a0 - python\n\u00a0 \u00a0 \u00a0 \u00a0 - -m\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor\n\u00a0 \u00a0 \u00a0 \u00a0 image: image:latest\npipelineInfo:\n\u00a0 name: hello-world\nroot:\n\u00a0 dag:\n\u00a0 \u00a0 tasks:\n\u00a0 \u00a0 \u00a0 input_data:\n\u00a0 \u00a0 \u00a0 \u00a0 cachingOptions:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enableCache: true\n\u00a0 \u00a0 \u00a0 \u00a0 componentRef:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 \u00a0 \u00a0 \u00a0 inputs:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\u00a01\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\u00a00\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue: CREATE_IF_NEEDED\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: custom-config\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue: 'null'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: input-table\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue: 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\u00a0WRITE_EMPTY\n\u00a0 \u00a0 \u00a0 \u00a0 taskInfo:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 inputDefinitions:\n\u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 custom-config:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 input-table:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\nschemaVersion: 2.1.0\nsdkVersion: tfx-1.12.0\n```\n\u00a0\nand reran it however, I got this instead:\n\nIt ran successfully in this use case, but for other components we are expecting an `int` as `int` not `double`. For example we are running the chicago taxi pipeline as a template but we got the following error:\n\n```\n\nThe replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs:\u00a0https:\/\/console.cloud.google.com\/logs\/viewer?project=636071587074&resource=ml_job%2Fjob_id%2F1886953...\n\n```\n\nMy question here, is there a way in vertex when uploading our yaml file to pipeline template, we can specify the parameter type as well? if so how? if not, what are our alternate approach? (Note here, we are also using TFX to generate these artifacts)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI - Custom Job (with GPU) froze without errors",
        "Question_tag_count":1,
        "Question_created_time":1664967240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Custom-Job-with-GPU-froze-without-errors\/m-p\/474701#M620",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"Hi all,\n\nI ran into an issue yesterday when submitting a custom job in Vertex AI. The job successfully started (as evident by the logs reported) but at some point, just before the script starts using the GPU on the machine, we stopped receving any logs. I let the job run for 20 minutes, but it did not procide any more logs - as well as there was no indication of the machine having any issues.\u00a0 I then stopped the job manually,\u00a0 re-created the exact same job by running the same script (using the google-cloud-aiplatform package in Python) with the exact same parameters, and the job ran successfully.\n\nIs there any way I can figure out what went wrong in the first job? I am looking for a stable solution to manage custom jobs, but the fact that this happened within my first 5 runs seems very concerning to me, especially since there was no indication that the job was frozen as it could have ran until it hit the max time which would have costed a lot of money.\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"text-bison@001 IAM_PERMISSION_DENIED",
        "Question_tag_count":1,
        "Question_created_time":1686544860000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/text-bison-001-IAM-PERMISSION-DENIED\/m-p\/602198#M2133",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":384,
        "Question_body":"Hello,\n\nI am currently facing an issue with calling the text-bison@001 API in my App Engine deployment. While the API works perfectly fine on my local machine, I encounter the following error message when calling the API from the deployed app:\n\n```\n403 Permission 'aiplatform.endpoints.predict' denied on resource '\/\/aiplatform.googleapis.com\/projects\/tr-media-analysis\/locations\/us-central1\/publishers\/google\/models\/text-bison@001' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"]\n```\n\nI have already checked the Service Account associated with the App Engine, and it has been granted the necessary AI Platform Developer rights.\n\nI would greatly appreciate any insights or suggestions on how to resolve this problem. Are there any additional permissions or configurations that I might be missing?\n\nThank you in advance for your assistance!\n\nBest regards,\nLudovico",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI fine tuning LLM model",
        "Question_tag_count":3,
        "Question_created_time":1689236040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-fine-tuning-LLM-model\/m-p\/611982#M2359",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":189,
        "Question_body":"Hi,\n\n\u00a0\nI'm new to GCP and I was trying to run a LLM tuning process in Vertex AI.\u00a0\nI upload my data in the jsonl format in a bucket and selected it to start the tuning process.\u00a0During the pipeline, I got this error:\n\u00a0\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\u00a0aiplatform.googleapis.com\/restricted_image_training_tpu_v3_pod, cause=null; Failed to create custom job.Project number: 162269030045, Job id: 8911904581961646080, Task id: -5724177380969283584, Task name: large-language-model-tuner, Task state: DRIVER_SUCCEEDED, Execution name: projects\/162269030045\/locations\/europe-west4\/metadataStores\/default\/executions\/9317118956493817351; Failed to create external task or refresh its state. Task:Project number: 162269030045, Job id: 8911904581961646080, Task id: -5724177380969283584, Task name: large-language-model-tuner, Task state: DRIVER_SUCCEEDED, Execution name: projects\/162269030045\/locations\/europe-west4\/metadataStores\/default\/executions\/9317118956493817351; Failed to handle the pipeline task. Task: Project number: 162269030045, Job id: 8911904581961646080, Task id: -5724177380969283584, Task name: large-language-model-tuner, Task state: DRIVER_SUCCEEDED, Execution name: projects\/162269030045\/locations\/europe-west4\/metadataStores\/default\/executions\/9317118956493817351\n\u00a0\nSo I looked online for a solution\/work around to this problem. I found that some users were resolving it by updating their quotas. Then in the error message I've seen that my limit are reached for the\u00a0europe-west4\u00a0(if I've understood correctly). That's what I'm trying to do right now. Do some of you guys got the same error and can give me some advice to fix it?\n\u00a0\nI look forward to hearing back from you!\nThank you so much!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"The model monitoring job is not executed.",
        "Question_tag_count":1,
        "Question_created_time":1685656260000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-model-monitoring-job-is-not-executed\/m-p\/599304#M2065",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":95,
        "Question_body":"I followed the URL below to deploy the model to the endpoint.\n\nhttps:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/goog...\n\nManually I enabled monitoring for the model, but the monitoring job did not run and I got the message \"The first run of the monitoring job is pending.\"\u00a0\n\nI tried both Training-serving skew detection and Prediction drift detection, but they did not run.\n\nHow do I run a monitoring job?\n\nI requested 20 predictions after deploying.\u00a0Need more requests?\n\n\u25cbSettings\n\nMonitoring window length: 1hour\n\nSampling rate: 100%\n\nPrediction input schema: not specified",
        "Question_closed_time":1686218040000,
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@YoshinaoMori,\n\nWelcome to Google Cloud Community!\n\nYou need to provide the schema of your table dataset in order for it to correctly parse the input payload. To provide your own input schema, you must create a YAML file. you can refer to this link for an example:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/schemas#custom-input-schemas-online\nIf you are trying to use automatic schema parsing where the model monitoring automatically parses the input schema, you might need 1,000 input requests in order for the model monitoring to determine its schema. It will work best if the input requests are in key-value pairs. Here is an example:\n\n{\"Name\":\"Mike\", \"age\":\"30\", \"gender\":\"M\", \"ethnicity\":\"latin american\"}\n\nYou can check this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/schemas#automatic-schema-parsing-online\n\nYou can also check this guide on how to turn on monitoring models:\u00a0https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/monitor-models-training-serving-skew-v...\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Airflow Dag for Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":1668910920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Airflow-Dag-for-Vertex-AI\/m-p\/491123#M821",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":153,
        "Question_body":"Please advice on how to create airflow dag for vertex ai.\n\n**********************************************************\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom google.cloud import aiplatform\nfrom airflow.operators import CreateDatasetOperator\n\nYESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)\n\n\ndefault_dag_args = {\n\n'start_date': YESTERDAY,\n}\n\nwith models.DAG(\n'composer_sample_simple_greeting',\nschedule_interval=datetime.timedelta(weeks=2),\ndefault_args=default_dag_args) as dag:\n\ndef create_entity_type_sample(\nproject: str,\nlocation: str,\nentity_type_id: str,\nvertexai: str,\nservice_account_id: str\ntask_id: str,\nproject_id: str,\n\naiplatform.init(project=project, location=location)\n\nmy_entity_type = aiplatform.EntityType.create(\nentity_type_id=entity_type_id, vertexai=vertexai\n)\n\nmy_entity_type.wait()\n\nreturn my_entity_type\n\n\ncreate_image_dataset_job = CreateDatasetOperator(\ntask_id=\"image_dataset\",\ndataset=IMAGE_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_tabular_dataset_job = CreateDatasetOperator(\ntask_id=\"tabular_dataset\",\ndataset=TABULAR_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_text_dataset_job = CreateDatasetOperator(\ntask_id=\"text_dataset\",\ndataset=TEXT_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_video_dataset_job = CreateDatasetOperator(\ntask_id=\"video_dataset\",\ndataset=VIDEO_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_time_series_dataset_job = CreateDatasetOperator(\ntask_id=\"time_series_dataset\",\ndataset=TIME_SERIES_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\n\ncreate_image_dataset_job >> create_tabular_dataset_job >> create_text_dataset_job >> create_video_dataset_job >> create_time_series_dataset_job\n************************************************************************************************************************",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"VertexAI notebook does not obtain Shared VPC IP",
        "Question_tag_count":1,
        "Question_created_time":1683273000000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-notebook-does-not-obtain-Shared-VPC-IP\/m-p\/550677#M1799",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"Hello,\u00a0\n\nwhen I create a new VertexAI notebook I can select:\n\nnetworking:\u00a0 Shared network (from host project: XXXC)\u00a0\n\nGCP then asks me the network and the subnet.\nIn the subnet dropdown the tooltip reads:\n\nAssigns the notebook an IPv4 address from the subnetwork's range. Notebooks in different subnetworks can communicate with each other using their internal IPs as long as they belong to the same network.\n\nBut this does not seem to work at all ?\n\nOnce I create the notebook I can run ifconfig in the terminal and there it's clear that the notebook never got an IP from the subnet I selected above.\n\nAnyone knows how I can use this feature?\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Expense Parser issue importing labels",
        "Question_tag_count":2,
        "Question_created_time":1688015940000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Expense-Parser-issue-importing-labels\/m-p\/607714#M2259",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":58,
        "Question_body":"We're trying to import labels for the Expense Parser, and this is the example JSON request we're using:\n\n{\n\"uri\": \"\",\n\"mimeType\": \"application\/pdf\",\n\"text\": \"11\/10\/2021, 16:42\\nOffice DEPOT\\nOfficeMax\\nLOS GATOS (408) 356-3757\\n05\/30\/2020 9:42 AM\\nV2VT5X3P5XY56YX66\\nSALE\\n5379432 PRNTER, ET-4760\\nSubtotal:\\n***\\nSales Tax:\\nTotal:\\nVisa\\n950-1-1844-473229-20.5.2\\noffice-depot-redacted.png\\nAUTH CODE 083396\\nTDS Chip Read\\nAID A0000000031010 CITI VISA\\nTVR 0800008000\\nCVS PIN Verified\\n499.99 SS\\n90499.99\\n19 45.00\\n544.99\\n544.99\\nhttps:\/\/mail.google.com\/chat\/u\/0\/#chat\/dm\/qimyvgAAAAE\\n5730827812\\nPlease create your online rewards\\naccount at officedepot.com\/rewards.\\nYou must complete your account to\\nclaim your rewards and view your\\nstatus.\\nShop online at www.officedepot.com\\nWE WANT TO HEAR FROM YOU!\\nVisit survey.officedepot.com\\nand enter the survey code below:\\n1508 QP9G OY41\\n****\\n**\\n****\\n1\/1\",\n\"page\": [],\n\"entities\": [\n{\n\"mentionText\": \"receipt_number\",\n\"type\": \"receipt_number\"\n},\n{\n\"mentionText\": \"amount\",\n\"type\": \"total_amount\"\n},\n{\n\"mentionText\": \"currency\",\n\"type\": \"currency\"\n},\n{\n\"mentionText\": \"date\",\n\"type\": \"receipt_date\"\n}\n]\n}\n\nThis errors out in the following way:\n\n\u201cinputGcsSource\u201d: \u201cgs:\/\/receipts321\/jsonimport\/output\u00a02\/2301149.json\u201d,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u201cstatus\u201d: {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u201ccode\u201d: 3,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u201cmessage\u201d: \u201cRequest contains an invalid argument.\u201d\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\nIt doesn't give any details what about what is wrong in the request. Any help appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Failed to submit prompt Error message: \"Internal error encountered.\" Status: 500 Error code: 500",
        "Question_tag_count":1,
        "Question_created_time":1686279660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Failed-to-submit-prompt-Error-message-quot-Internal-error\/m-p\/601579#M2111",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hi all,\n\nCreated following a sample JSONL format,\nUploaded in the GCS bucket. gs:\/\/xxx\/newsam.jsonl\nwe have created our own custom model using the curl command.\n\ncurl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application\/json; charset=utf-8\" -d \"@model-request.json\"\n\n\nThe pipeline was successfully executed and deployed in the model registry.\nModel name is : LLM\n\nFrom the model registry, we have created the end point.\nEnd point name :\nsample1\nEndpoint id: 59XXX079194845184\n\nWhen we test the end point using the curl command, it throws \"internal error encountered,\" error 500.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"error message when using vertex AI with jsonl file",
        "Question_tag_count":2,
        "Question_created_time":1687947660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/error-message-when-using-vertex-AI-with-jsonl-file\/m-p\/607470#M2256",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":70,
        "Question_body":"Hi,\u00a0\n\n\u00a0 I am using Generative AI studio to tune model, my jsonL file is like this:\n\n{\n\"input_text\": \"Create a description for Plantation Palms\",\n\"output_text\": \"Enjoy some fun in the sun at Gulf Shores.\"\n}\n\nwhen I submit the task, it said \"invalid data, missing one or more required fields.\"\n\nWonder what could be wrong?\n\nthanks, Helen",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Tensorflow Distributed ParameterServer setup",
        "Question_tag_count":1,
        "Question_created_time":1689759120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Tensorflow-Distributed-ParameterServer-setup\/m-p\/613712#M2400",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":25,
        "Question_body":"Hi,\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0Earlier I asked the Tensorflow forum but didn't get a practical answer. I read the Tensorflow documentation and set up a simple\u00a0\n\ndistribution = tf.distribute.MultiWorkerMirroredStrategy()\n\nThe cluster spec. is\u00a0\n\ncluster_spec = { \"worker\":[\"127.0.0.1:9901\",\n                           \"127.0.0.1:9902\"]\n               }\n\nIt produced an appropriated result when I trained using the MNIST dataset. I documented what worked in https:\/\/branetheory.org\/2022\/05\/25\/distributed-training-using-tensorflow-federated\/\n\nBut I never understood how to use a truly distributed ParameterServer. It isn't documented because it involves set up of compute VMs, GPUs etc. I think.\n\nWhen I read the paper \"Monolith: Real Time Recommendation System With\nCollisionless Embedding Table\" this came up again. This is the diagram.\n\nCan anyone point out instructions to set up this and execute a simple training task ? I am mainly\n\ninterested in the TensorFlow distributed set up.\n\nI may set up Kafka and Flink as described in the paper for learning later.\n\nThanks,\n\nMohan",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Custom AI Model with product documents",
        "Question_tag_count":2,
        "Question_created_time":1687218660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-AI-Model-with-product-documents\/m-p\/604674#M2194",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"We intend to create an AI model that possesses contextual knowledge about any given product. The model will be trained using various documents associated with the product like requirement documents, Functional Specifications, Technical Design, etc. The trained AI model should maintain the context of the product and provide solutions to product-specific queries or problems. Currently, we are not able to train AI models with huge product docs without losing the context. The LLMs which we used are splitting the docs into multiple portions and the responses are losing context due to that. Also, we are facing token and character limitations when we wanted to use Vertex AI.\n\nIs this possible with vertex AI? If possible how to implement this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"nearest neighbors predict pipeline",
        "Question_tag_count":2,
        "Question_created_time":1684196100000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/nearest-neighbors-predict-pipeline\/m-p\/553729#M1923",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":49,
        "Question_body":"Hi All,\n\nI am trying to build pipeline for nearestneighbors from scikit learn. As there is no \"predict\" method in this, i need to write a custom predictor class.\u00a0\n\ncan any1 help me on how to call this predict class on model is deployed on endpoint?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Co-hosting Pytorch models on Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":1687400580000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Co-hosting-Pytorch-models-on-Vertex-AI\/m-p\/605590#M2204",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":58,
        "Question_body":"Hi,\u00a0\n\nI'm trying to co-host pytorch models on vertex following\u00a0https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/introducing-co-hosting-models-on-the-vert...\u00a0from July 2022. On the article it say support is only for TF models. Is this still the case? if so what is the ETA on Pytorch support? Also, is the a current manual workaround for Pytorch models?",
        "Question_closed_time":1687542480000,
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@xolisani,\n\nWelcome to Google Cloud Community!\n\nAs of now, it only supports Tensorflow models, but please note that a feature request was filed to include pytorch models and the request was already forwarded to the\u00a0Vertex AI Engineering Team for evaluation although currently there is no exact ETA when it will be available but you can track the progress using this link:\u00a0https:\/\/issuetracker.google.com\/255271990\nAs a workaround, you can try packing all your models in one container and use a custom HTTP server logic\u00a0 for prediction requests that uses the parameters field in the prediction request body in order to send it to the correct model. You can check this link that I found for more information, please note that this is not supported by Google but I am sending it to you since it might be helpful to your case:\u00a0https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\n\nHope this is useful!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"[VERTEX AI] Is it possible to load in GCP Vertex AI Dataset a set of files along with their training",
        "Question_tag_count":2,
        "Question_created_time":1688957040000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VERTEX-AI-Is-it-possible-to-load-in-GCP-Vertex-AI-Dataset-a-set\/m-p\/610653#M2330",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":268,
        "Question_body":"I have to report here my homonym StackOverflow question:\u00a0https:\/\/stackoverflow.com\/questions\/76413616\/is-it-possible-to-load-in-gcp-vertex-ai-dataset-a-set-o...\u00a0\n\nDoes anybody know how to solve it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issue with DialogFlow ES Caching and Entity Annotation",
        "Question_tag_count":3,
        "Question_created_time":1685418900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-with-DialogFlow-ES-Caching-and-Entity-Annotation\/m-p\/598132#M2047",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":50,
        "Question_body":"Hi everyone,\n\nI'm running into a perplexing issue with DialogFlow ES - the system seems to be holding onto old data, even though I've removed certain entities.\n\nHere's the situation: I've created new composite entities, one of which has an alias that contains the entry \"Technical Manager\". When I annotate a training phrase to this entity, I'm hit with a validation error, saying \"Technical Manager\" doesn't correspond to the entity, even though it should. If I add the phrase without annotation, the system links it to an old, deleted entity which indicates that the system still remember this old entity.\n\nAdditionally, during training, I noticed the agent initiates multiple training cycles with only seconds between them.\n\nAs a result the behavior of the agent is quite unpredicted, sometimes it fill slots with \"NO\" or even empty \"\" values, I think there's some sort of caching issue on sever side that does this and affects the training process.\n\nMy attempts to resolve this (backing up and restoring the agent, even with different agents) haven't worked. Oddly, I don't see the old entity that the system still remembers in the JSON files of the backup.\n\nAnyone encountered this or have any ideas to fix it?\n\nThanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Datasets & batch predictions",
        "Question_tag_count":2,
        "Question_created_time":1658670780000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Datasets-amp-batch-predictions\/m-p\/446346#M443",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":462,
        "Question_body":"Hi all,\n\nI just started to play with Vartex AI. I am working with \"Tabular\" - \"Forecasting\" and currently struggling with few things and i hope you can help me in order i can continue. I tried to organize my questions to three categories:\n\n1) Datasets for training:\u00a0\n\n\u00a0 \u00a0 \u00a0a) \"series identifier\" define to which time series data are belonging ... lets assume that i have two series identifier - one is called \"A\" and one is called \"B\". Does this means that AI treats them as completely separate and noncorrelated - this means any data that belongs to series A don't have any correlation to B, right? This give me possibility to train different dataset with one shot right? Otherwise i would need to make (in my case) two trainings - one for A and one for B.\n\n2) Training new model --> Model details\n\na) Is possible to predict more then one target column?\u00a0\n\nb) lets assume that my dataset data granularity is 1 day. Can i use data granularity of \"5min\" for Forecast configuration or can this setup decrease quality of my forecast? Should it be more correct to use already at beginning lets say dataset granularity of 5 minute and afterwards it could be more flexible when setting data granularity for forecast configuration without influencing forecast quality?\n\nc) If I set Forecast horizon of 7 and context window 30, does this means that this setting limit my forecast to maximum 7 time steps and requesting always exactly 30 time steps of historical data as input when forecasting on existing trained model?\n\n3) Batch predictions\n\na) Batch Source file\n\nLets assume that i have data with 15 columns from which one is \"serial identifier\" , one is \"time step\" - actually date and one of those columns is target column. Rest of 12 columns are used as influencer and used to train my model. I know that i need to have same structure for batch source file - i read that i can use same file as i used for training, but i just need to add in my case a 2x7 new rows with adding 7 dates and serial identification (in my case 7x A and 7xB) and target column need to be empty. But what should i do with data of rest 12 columns? Do i need manually to enter data for those new rows (2x7) of those 12 columns which values should be for future? But what if i don't have those data? Does this means i cannot do prediction?\n\nI tried to make prediction without those future data and i got following message:\n\n\"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\"\n\nI hope you can help me with above questions.\n\n\u00a0\n\nTnx in advance!\n\n\u00a0\n\nRegards,\n\n\u00a0\n\nArny",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What difference environments\/domains does vertex ai is being in use ?",
        "Question_tag_count":4,
        "Question_created_time":1662084180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-difference-environments-domains-does-vertex-ai-is-being-in\/m-p\/462390#M559",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"I wanna learn more about how to use vertex ai in more domains than in recognition and things , so i can learn how to use Vertex ai in other fields and domain ,\u00a0\n\nI want to learn like projects using live vertex ai auto ml or realated to those .",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Notebook deleting cells",
        "Question_tag_count":1,
        "Question_created_time":1661849640000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Notebook-deleting-cells\/m-p\/461480#M550",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hello everyone, I have been facing an issue for the past few months where on occasion my vertex AI notebooks will completely wipe and delete all the cells in a .ipynb file. This happens at what appears to be random times.\u00a0\n\nContext to reproduce:\u00a0\n\nN96 High Memory instance 624 gb of ram\n\nIdle time: 1440\n\nsingle user only notebook\u00a0\n\nWhat happens: notebook with shutdown in the midst of running. Once the notebook is back up and running all the cells in the ipynb file are gone. There is no error message\u00a0\n\nIf anyone has faced this issue in the past and knows how to resolve I would really appreciate and information!\u00a0\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Fine tuning text-bison model",
        "Question_tag_count":2,
        "Question_created_time":1688373180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Fine-tuning-text-bison-model\/m-p\/608823#M2272",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":338,
        "Question_body":"Hi everyone\n\nI currently have a pipeline using OpenAI where I pass information about my internal company database tables as a prompt, and then ask a user defined question, and then get an SQL query and a response.\n\nAs you might have guessed, this takes alot of tokens since I need to describe my tables in the prompt and costs alot.\n\nI am trying to now fine tune a text-bison model by passing it training examples of the input text along with an appropriate output response. For the training, I can pass the same prompt as the OpenAI pipeline, where I describe my tables and then ask the model to generate a query.\n\nBut, the Vertex AI page on fine tuning says to use training examples which will be the same as you would get an input in production. This would mean that I also pass the whole table description in the production pipeline as well, and this is exactly what I am trying to avoid.\n\nAs an example:\n\nFor training:\n\nContext: You have the following tables to gather data from:\n\nTable 1 description\nTable 2 description\n\nInput: What is the price of Lockheed Martin stocks?\n\nOutput: The price of stock is X.\n\nIn the above example, the model knows the tables through the context and then finds the appropriate table for the text it was given and generates a response.\n\nBut in a production environment, I want to give only the 'input', and not the table descriptions (i.e. context), since that would take up tokens and cost more, and that is what I am trying to avoid in the first place.\n\nAny idea how to go about this or am I approaching the problem in the wrong way?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About Vertex AI Model Monitoring billing",
        "Question_tag_count":1,
        "Question_created_time":1687989840000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Vertex-AI-Model-Monitoring-billing\/m-p\/607607#M2258",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":112,
        "Question_body":"BigQuery's pricing page says the analysis fee is $5.00 per TB, and Model Montoring's pricing page says it is $3.50 per GB. Does this only cost $3.50 per GB when Model Monitoring is enabled? Or does both apply?",
        "Question_closed_time":1688683320000,
        "Answer_score_count":1.0,
        "Answer_body":"Hello YoshinaoMori,\n\n\nGoogle Cloud follows a pay-as-you-go pricing structure, meaning, you only pay for the services you use. If both Analysis and Model Monitoring features are active then both will apply.\n\nThe Analysis fee covers the processing of queries, user-defined functions, scripts, and certain data manipulation language (DML) and data definition language (DDL) statements while the Model Monitoring fee cover the processing of data used to monitor your models.\n\nIn the event that you disabled the Model Monitoring feature, then you will only be charged for the Analysis fee.\n\nI hope I was able to answer your question.\n\n\nUsable resource:\n- Pricing | Vertex AI Model Monitoring\n- Pricing | BigQuery\n- Google Cloud Pricing Calculator\n\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Question_tag_count":1,
        "Question_created_time":1645930620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/m-p\/397685#M214",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":728,
        "Question_body":"Hello,\n\nI am trying to run a Custom Training Job in the Vertex AI Training service.\n\nThe job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).\n\nWhen I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:\n\n\n$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"\n\n\n\n... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.\n\nGranted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?\n\nPlease note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image.\n\n\u00a0\n\nThanks,\nurig",
        "Question_closed_time":1650182580000,
        "Answer_score_count":0.0,
        "Answer_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Data from Azure Storage Gen2 and Synapse dedicate pool to train models in VertexAI",
        "Question_tag_count":3,
        "Question_created_time":1687908960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Data-from-Azure-Storage-Gen2-and-Synapse-dedicate-pool-to-train\/m-p\/607275#M2251",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"How can we create the models in Vertex AI when data is Azure Storage Gen2 and\u00a0Synapse dedicate pool WITHOUT moving the data to the Google cloud (BQ, Cloud storage etc.) and consume the model outputs in Azure. Can someone point me to the reference architecture?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google cloud functions",
        "Question_tag_count":1,
        "Question_created_time":1685766480000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-functions\/m-p\/599739#M2073",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":59,
        "Question_body":"Hi guys how to solve this error plz\n\nERROR: (gcloud.functions.deploy) Uncompressed deployment is 1486838328B, bigger than maximum allowed size of 536870912B.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.",
        "Question_tag_count":1,
        "Question_created_time":1684641120000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-GPUS-ALL-REGIONS-exceeded-Limit-0-0-globally\/m-p\/595266#M1987",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":129,
        "Question_body":"Hi,\n\nI have notebook like this\n\nWhen trying to start is I get error \"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.\"\n\nI upgraded from free trial, but it didn't help. Next I went to quota page and found no limit:\n\nWhat's the issue?\n\nThanks to all.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model training never finished",
        "Question_tag_count":2,
        "Question_created_time":1682293620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-training-never-finished\/m-p\/546578#M1729",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"Hi,\n\n\u00a0\nFrom the document, it says that the initial model training and tuning takes 2-5 days to complete. But I created few models (Similar items, Frequently bought together,\u00a0Buy it again) and started training on Apr 16, and it's not finished yet after over 8 days. Anybody knows what the problem may be? Thank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Python: Google Vision doesn't (cannot) read and convert photocopied books, from PDF into TXT",
        "Question_tag_count":2,
        "Question_created_time":1687090320000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-Google-Vision-doesn-t-cannot-read-and-convert-photocopied\/m-p\/604310#M2182",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":105,
        "Question_body":"THIS CODE HAVE NO ERROR. The OUPTUT done.\n\nThe problem is that the converted files (.txt) are 0 bytes. Seems that google cloud vision cannot\u00a0read and convert photocopied books, from PDF into TXT\n\n\u00a0\n\nimport\u00a0os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"d:\/doc\/doc\/MY-KEY.json\"\n\nfrom\u00a0google.cloud\u00a0import\u00a0vision\n\nfrom\u00a0google.cloud.vision_v1\u00a0import\u00a0types\n\nfrom\u00a0google.oauth2.service_account\u00a0import\u00a0Credentials\n\n#\u00a0from\u00a0google.cloud\u00a0import\u00a0storage\n#\u00a0client\u00a0library\n#\u00a0storage_client\u00a0=\u00a0storage.Client()\n\n#\u00a0Set\u00a0up\u00a0the\u00a0Google\u00a0Cloud\u00a0Vision\u00a0client\u00a0with\u00a0service\u00a0account\u00a0credentials\n#\u00a0credentials\u00a0=\u00a0Credentials.from_service_account_file('d:\/doc\/doc\/bebe-1084-992b240528be.json')\n#\u00a0client\u00a0=\u00a0vision.ImageAnnotatorClient(credentials=credentials)\n\n#pip\u00a0install\u00a0google-cloud-vision\n\n#\u00a0Set\u00a0up\u00a0the\u00a0Google\u00a0Cloud\u00a0Vision\u00a0client\n\nclient\u00a0=\u00a0vision.ImageAnnotatorClient()\n\n\n#\u00a0Directory\u00a0containing\u00a0the\u00a0PDF\u00a0files\n\npdf_directory\u00a0=\u00a0\"d:\/doc\/doc\"\n\n\n#\u00a0Output\u00a0directory\u00a0for\u00a0the\u00a0TXT\u00a0files\n\noutput_directory\u00a0=\u00a0\"d:\/doc\/doc\"\n\n\n#\u00a0Get\u00a0a\u00a0list\u00a0of\u00a0PDF\u00a0files\u00a0in\u00a0the\u00a0directory\n\npdf_files\u00a0=\u00a0[file\u00a0for\u00a0file\u00a0in\u00a0os.listdir(pdf_directory)\u00a0if\u00a0file.endswith(\".pdf\")]\n\n\n#\u00a0Process\u00a0each\u00a0PDF\u00a0file\n\nfor\u00a0pdf_file\u00a0in\u00a0pdf_files:\n\n\u00a0\u00a0\u00a0\u00a0pdf_path\u00a0=\u00a0os.path.join(pdf_directory,\u00a0pdf_file)\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Create\u00a0the\u00a0output\u00a0TXT\u00a0file\u00a0path\n\n\u00a0\u00a0\u00a0\u00a0txt_file\u00a0=\u00a0os.path.splitext(pdf_file)[0]\u00a0+\u00a0\".txt\"\n\n\u00a0\u00a0\u00a0\u00a0txt_path\u00a0=\u00a0os.path.join(output_directory,\u00a0txt_file)\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Read\u00a0the\u00a0PDF\u00a0file\u00a0as\u00a0bytes\n\n\u00a0\u00a0\u00a0\u00a0with\u00a0open(pdf_path,\u00a0'rb')\u00a0as\u00a0file:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0content\u00a0=\u00a0file.read()\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Convert\u00a0PDF\u00a0to\u00a0image\u00a0using\u00a0Google\u00a0Cloud\u00a0Vision\u00a0API\n\n\u00a0\u00a0\u00a0\u00a0input_image\u00a0=\u00a0types.Image(content=content)\n\n\u00a0\u00a0\u00a0\u00a0response\u00a0=\u00a0client.document_text_detection(image=input_image)\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Extract\u00a0text\u00a0from\u00a0the\u00a0response\u00a0and\u00a0save\u00a0it\u00a0as\u00a0TXT\n\n\u00a0\u00a0\u00a0\u00a0text\u00a0=\u00a0response.full_text_annotation.text\n\n\u00a0\u00a0\u00a0\u00a0with\u00a0open(txt_path,\u00a0'w',\u00a0encoding='utf-8')\u00a0as\u00a0file:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0file.write(text)\n\n\n\u00a0\u00a0\u00a0\u00a0print(f\"Converted\u00a0{pdf_file}\u00a0to\u00a0{txt_file}\")",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Analyzing the image's text with the Natural Language API",
        "Question_tag_count":3,
        "Question_created_time":1687158360000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Analyzing-the-image-s-text-with-the-Natural-Language-API\/m-p\/604500#M2188",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":83,
        "Question_body":"HI\u00a0\n\n\u00a0i try many times to run this code but some thing wrong ,\u00a0\n\nTo set up the API request, create a\u00a0nl-request.json\u00a0file with the following:\n\n{\n  \"document\":{\n    \"type\":\"PLAIN_TEXT\",\n    \"content\":\"your_text_here\"\n  },\n  \"encodingType\":\"UTF8\"\n}\nCopied!\ncontent_copy\n\nIn the request, you're telling the Natural Language API about the text you're sending:\n\ntype:\u00a0supported type values are\u00a0PLAIN_TEXT\u00a0or\u00a0HTML.\n\ncontent:\u00a0pass the text to send to the Natural Language API for analysis. The Natural Language API also supports sending files stored in Cloud Storage for text processing. To send a file from Cloud Storage, replace\u00a0content\u00a0with\u00a0gcsContentUri\u00a0and use the value of the text file's uri in Cloud Storage.\n\nencodingType:\u00a0tells the API which type of text encoding to use when processing the text. The API will use this to calculate where specific entities appear in the text.\n\nRun this Bash command in Cloud Shell to copy the translated text into the content block of the Natural Language API request:\n\nSTR=$(jq .data.translations[0].translatedText  translation-response.json) && STR=\"${STR\/\/\\\"}\" && sed -i \"s|your_text_here|$STR|g\" nl-request.json\nCopied!\ncontent_copy\n\nThe\u00a0nl-request.json\u00a0file now contains the translated English text from the original image. Time to analyze it!\n\nCall the\u00a0analyzeEntities\u00a0endpoint of the Natural Language API with this\u00a0curl\u00a0request:\n\ncurl \"https:\/\/language.googleapis.com\/v1\/documents:analyzeEntities?key=${API_KEY}\" \\\n  -s -X POST -H \"Content-Type: application\/json\" --data-binary @nl-request.json\n\ni have this issus\n\nstudent_00_fc5405542deb@cloudshell:~ (qwiklabs-gcp-01-39d2648c29b0)$ curl \"https:\/\/language.googleapis.com\/v1\/documents:analyzeEntities?key=${API_KEY}\" \\\n-s -X POST -H \"Content-Type: application\/json\" --data-binary @nl-request.json\n{\n\"error\": {\n\"code\": 403,\n\"message\": \"The request is missing a valid API key.\",\n\"status\": \"PERMISSION_DENIED\"\n}\n}\n\n\u00a0\n\nhelp plz",
        "Question_closed_time":1687431540000,
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@IBRAHIM-K,\n\nWelcome to Google Cloud Community!\n\nYou are encountering this error since your\u00a0API_KEY variable is invalid or missing. You can try checking your variable if it contains your API_KEY by running the following:\n\n\n\nprintenv API_KEY\n\nIf it does not contain your API_KEY, it is possible you were not able to export your API keys as a variable but if your variable API_KEY was able to produce an output that contains an API key, it is possible that your API key is invalid. To solve this problem, here is a step by step process on how to create an API KEY and export it as a variable.\u00a0\n\n1. In your console > Click Navigation Menu > Select API & Services > Select Credentials\n2. Click Create Credentials > Select API key (This will generate an API key) > Copy the API key that was generated\n3. In your Cloud Shell, run the following:\u00a0\n\nexport API_KEY = <PASTE YOUR COPIED API KEY HERE>\n\nPlease note that you need to remove the symbols <\u00a0 and >\n\nYou can try checking again if you're able to export the variable API_KEY by running the following:\n\nprintenv API_KEY\u200b\n\nIt should output your API keys, after that you should be able to call\u00a0the\u00a0analyzeEntities\u00a0endpoint.\n\nHope this is useful!\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Optimization Variables \/ Inputs",
        "Question_tag_count":3,
        "Question_created_time":1649746860000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Optimization-Variables-Inputs\/m-p\/412877#M268",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":68,
        "Question_body":"Hi\n\n\u00a0\n\nis it possible to implement Optimization Problems using AutoML or other Google Cloud application different than regular google Colab \/ Jupiter Notebooks?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vision API Product search",
        "Question_tag_count":2,
        "Question_created_time":1667386140000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-Product-search\/m-p\/485081#M718",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":363,
        "Question_body":"Following is vision API product search request json\n\n{\n \"requests\": [\n{\n  \"image\": {\n    \"content\": base64-encoded-image\n  },\n  \"features\": [\n    {\n      \"type\": \"PRODUCT_SEARCH\",\n      \"maxResults\": 5\n    }\n  ],\n  \"imageContext\": {\n    \"productSearchParams\": {\n      \"productSet\": \"projects\/project-id\/locations\/location-id\/productSets\/product-set-id\",\n      *\"productCategories\": [\n           \"apparel\"\n      ]*,\n      \"filter\": \"style = womens\"\n    }\n  }\n}\n]\n}\n\nFor ImageContext, ProductCategories(apparel in this request) is mandatory in API. My concern is if I want product search from all the available\u00a0ProductCategories, do I need to set multiple requests?\n\n ImageContext imageContext =\n    ImageContext.newBuilder()\n        .setProductSearchParams(\n            ProductSearchParams.newBuilder()\n                .setProductSet(productSetPath)\n                .addProductCategories(\"apparel-v2\")\n                .setFilter(filter))\n        .build();\n\nFor example, addProductCategories(\"apparel\") can have only one productcategory at a time. But I want product search from all the category something like addProductCategories(\"apparel-v2\").addProductCategories(\"toys-v2\").addProductCategories(\"general-v1\") etc.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard API",
        "Question_tag_count":2,
        "Question_created_time":1680093180000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-API\/m-p\/538517#M1526",
        "Question_answer_count":39,
        "Question_score_count":17,
        "Question_view_count":0,
        "Question_body":"My request to join the Bard preview was recently accepted and I've begun evaluating it. So far it looks amazing and I'm anxious to begin working with it as a developer. I asked Bard if there was an API that I can start to use and it responded with the information below. I cannot seem to find the API waitlist sign-up page in the Bard website, nor can I enable the API from my Cloud Account (https:\/\/cloud.google.com\/),\n\nIs there any additional action I should take to request to join the wait list for access to the Bard API? Perhaps I am too anxious and just need to wait.\u00a0\n\nThanks in advance for any advice.\n\nResponse from Bard itself:\n\nYes, there is an API for Google Bard. It is currently in beta and is available to a limited number of users. You can sign up for the waitlist on the Google AI website.\n\nThe Google Bard API allows you to access the power of Google's large language models to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n\nYou can use the Google Bard API to create a variety of applications, including:\n\nChatbots that can hold conversations with users\nGenerators that can create content for websites and social media\nTranslators that can translate text from one language to another\nAnswerers that can answer questions about a variety of topics\n\nTo use the Google Bard API, you will need to create a Google Cloud Platform project and enable the Google Bard API. You will then need to create a service account and download the JSON key file.\n\nOnce you have done this, you can start using the Google Bard API in your applications. You can find more information about the Google Bard API on the Google AI website.",
        "Question_closed_time":1681909860000,
        "Answer_score_count":14.0,
        "Answer_body":"Hi\u00a0@DeanS - thank you for your patience! We are excited to hear that you are ready to begin working with Google LLM API's as a developer.\u00a0 Check out the information below:\u00a0\n\nEnterprise customers can sign up to Vertex AI LLM Trusted Testers program here:\u00a0https:\/\/cloud.google.com\/ai\/earlyaccess\/join?hl=en\nIndependent developers can sign up the MakerSuite and PalmAPI waitlist here:\u00a0https:\/\/makersuite.google.com\/waitlist.\n\u00a0\nRegister for the Google Cloud Innovators Program to stay informed on the latest updates and roadmap sessions.\n\u00a0\nI hope this helps. Thanks!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"AI Augmented Sensory Headset",
        "Question_tag_count":2,
        "Question_created_time":1665173880000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-Augmented-Sensory-Headset\/m-p\/475836#M629",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":41,
        "Question_body":"Wondering when Google will develop olfactory sensor addition to VR headsets and technology. In laymens terms, adding the sense of smell to VR headsets using an add on similar to a printer ink cartridge, but designed specifically for the sense of smell. Theoretically, it is possible, but to manufacture it in a large scale. It can change the way programs, especially helping boost the food and hospitality industry as well as giving everyday people a very good reason to smell fresh food and drink... from their phone! Where and how can we further this research for this wonderful idea?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issue Link between Vertex AI and Appsheet",
        "Question_tag_count":1,
        "Question_created_time":1679257980000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-Link-between-Vertex-AI-and-Appsheet\/m-p\/534403#M1442",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":155,
        "Question_body":"Hello,\n\nI am working on quantity counting by combining appsheet and vertex AI.\nI have a json file link at GCP, deploy model of vertex AI, but when I go to the appsheet, it can't count the number and can't be identified.\n\n\u00a0\n\nWhere am I doing wrong. Please help me.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB",
        "Question_tag_count":1,
        "Question_created_time":1659595920000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-DAGScheduler-Broadcasting-large-task-binary-with-size-2-2\/m-p\/450466#M487",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi there.\nThere were quite a number of such warnings as the model was getting trained.\n\n22\/08\/04 07:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1139.5 KiB\n22\/08\/04 07:08:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n\nMay I know if we are safe to ignore them?\nWhat does it mean actually?\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AIML",
        "Question_tag_count":1,
        "Question_created_time":1671511740000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AIML\/m-p\/501497#M973",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":98,
        "Question_body":"Show More",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Features",
        "Question_tag_count":3,
        "Question_created_time":1652358960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Features\/m-p\/422551#M326",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"Hi\n\nAssume that I create a model using AutoML with 50 features from the Vertex AI Feature Store and after training I found that from the 50 original features, 10 has a very low incidence over the model.\n\nLooking to increase the accuracy, reduce the consumption of resources and increase the speed of the model:\n\nDo I need to remove the 10 features from the Feature Store and deploy the model to the endpoint?\n\nShould I retrain the model with the 40 features and deploy it to the end point?\n\nAny comments more than appreciated",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI integration with mlflow ?",
        "Question_tag_count":3,
        "Question_created_time":1645156620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-integration-with-mlflow\/m-p\/394738#M201",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Is there any way to integrate vertex AI with mlflow ?\u00a0\nAny articles or resources I can go through ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Correct JSONL format for Vertex AI training.",
        "Question_tag_count":3,
        "Question_created_time":1686813960000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Correct-JSONL-format-for-Vertex-AI-training\/m-p\/603463#M2163",
        "Question_answer_count":7,
        "Question_score_count":1,
        "Question_view_count":423,
        "Question_body":"Tried to upload data in JSONL format in Vertex AI, based on the example given. It still gives an error -\u00a0Invalid data. Missing one or more required fields.\nWhat can be the issue here.\u00a0\n\nData format pushed -\u00a0\n{\"input_text\": \"Please provide fresh website content for upto <5-page> website. Each page should not exceed more than 200 words of content.\", \"output_text\": \"Page 1 Home - Title (30 to 50 Characters) RCA & Asociados, Professional Services, Chula Vista, California. \\n Description (280 to 300 Characters) RCA is a regional Foodservice Disposables and packaging broker servicing company in Mexico and Southern California. Our sales representatives are all bilingual and travel in and out of Mexico and the US. We are associated with both large and small independent distributors, chain accounts, and C-Stores. \\n Title Foodservice and Packaging Broker with 10+ Years of Experience in CA \\n Hero Text We Focus On Manufacturers Looking To Increase Sales & Help Them Grow Customers In CA & Mexico! Contact Us (Link to Contact Page Button) \\n (H1) (20 to 70 Characters) RCA \u2013 Profile, Mission & Vision RCA is a regional foodservice disposable and packaging broker servicing company in Mexico and Southern California. We have been in business since 2006 and are associated with both large and small independent distributors, chain accounts, and C-Stores. \\n We have offices, both in Mexico and the US, and our sales representatives are bilingual and weekly travel in and out of Mexico and the US. \\n (H2) Our Strategic Mission. Our goal is to represent and focus only on a handful of manufacturers who are looking to expand sales in Mexico and Southern California. We help them to grow their market share and increase their selective list of valued customers in Mexico, who are strategic and smart. \\n (H2) Our Business Vision \\n We establish long term relationships with our suppliers and distributors, together, building a successful partnership benefiting our mutual interests.\\n Page 2 Services - Title (30 to 50 Characters) RCA & Asociados, Services, Chula Vista, California \\n Description (280 to 300 Characters) RCA is an established, professional, and reputable company having good relationships with a wide range of distributors in Mexico and the US. We cater to the Hispanic and Asian Markets in the US and are exploring the opportunity to represent and work with your company to help you improve your market range. Contact us. \\n (H1) (20 to 70 Characters) Well Organized, Professional & Reputed Organization \\n RCA is an established, professional, and reputable company having good relationships with a wide range of distributors in Mexico and the US. We cater to the Hispanic and Asian Markets in the US and are exploring the opportunity to represent and work with your company to help you improve your market range. \\n (H2) Your Local Salesforce \\n We provide our clients who are manufacturers and who have never been to Mexico with transparency and authenticity. We bring in experts to help create relationships with their consumers and grow their client base. \\n (H2) Our Valued Relationships \\n We have established relationships with many markets in Mexico and can help you connect with foodservice distributors, retail, grocery, and chain accounts, depending on your needs. \\n View our Clients (Links to Our Clients) \\n Page 3 Our Clients \\n Title (30 to 50 Characters) RCA & Asociados, Our Clients, Chula Vista, California \\n Description (280 to 300 Characters) RCA & Asociados has been active in foodservice disposables and packaging brokerage since 2006 and is headquartered in Chula Vista, CA. Our primary focus is to help companies grow market share and, with a joint effort, successfully penetrate the market. Know our customers. \\n (H1) (20 to 70 Characters) RCA Clientele \\n Page 4 Contact Us \\n Title (30 to 50 Characters) RCA & Asociados , Contact Us , Chula Vista, California \\n Description (280 to 300 Characters) RCA & Asociados believes in its vision of becoming the premier brokerage company in Mexico & the US. We offer experienced and professional sales representation. We specialize in selling and promoting US and Mexican products within the foodservice and packaging industry. \\n (H1) (20 to 70 Characters) Get in Touch With Us.\"}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can I see the models vertex AI AutoML has used to train my model?",
        "Question_tag_count":3,
        "Question_created_time":1677033240000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-I-see-the-models-vertex-AI-AutoML-has-used-to-train-my-model\/m-p\/524954#M1310",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":162,
        "Question_body":"I have trained a few object detection models on vertex ai automl. is there a way for me to see what ML model has been used by vertex ai to train my data?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"hi-Latn Language Detect Error",
        "Question_tag_count":4,
        "Question_created_time":1662342660000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/hi-Latn-Language-Detect-Error\/m-p\/463002#M560",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":251,
        "Question_body":"I keep getting this language as detected language code but i can't seem to find this code in official docs of google translate api\n\n\u00a0\n\nPlease Help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error- Warning message: Annotation label is deduped",
        "Question_tag_count":2,
        "Question_created_time":1676248200000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-Warning-message-Annotation-label-is-deduped\/m-p\/521402#M1274",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":328,
        "Question_body":"I have uploaded a csv for image classification. I have checked the csv for any errors or duplicate values. But when I import the csv, i get the error that annotation is deduped and only a few images out of all the images are getting imported.\n\nWhat could be the possible cause for this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Tables for model where comparison is required?",
        "Question_tag_count":3,
        "Question_created_time":1632891900000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Tables-for-model-where-comparison-is-required\/m-p\/171520#M55",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":400,
        "Question_body":"Hi there,\n\nI have used GCP for a while now, and have trained quite a few models using AutoML Tables - all of these have been fairly simple datasets with probably a maximum of 20 columns.\n\n\u00a0\n\nI now have a problem that I would like to solve, but the dataset is a lot more complicated. I want to be able to predict the results of Greyhound Racing, or at least the % chances of each Greyhound winning a given race, compared to the other greyhounds running in that same race.\n\n\u00a0\n\nTo be able to do this I need to feed multiple pieces of data for each Greyhound in each given race, to be able to predict the winning chance of that greyhound in that day's race.\n\nHowever, I am very stuck on how to structure my data. Using AutoML Tables - would I need to structure the data in a tabular form with many columns? Or is there a better way to tackle this problem.\n\nHere is an example of the data I would be using:\n\nRace:\n\nExample data for each Greyhound in the race:\n\n\u00a0\n\nDoes anyone please have any advice of how to tackle this kind of problem, and how best to structure the data to attempt to predict the winning chance of each Greyhound for that day's race, based on that greyhound's previous data, compared to the other greyhounds in that day's race?\n\n\u00a0\n\nThanks,\nRob",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Changing the service account for an endpoint",
        "Question_tag_count":1,
        "Question_created_time":1668994620000,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Changing-the-service-account-for-an-endpoint\/m-p\/491278#M826",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":470,
        "Question_body":"Hi, I have deployed a Vertex AI model that was created using a custom image. However, when I tried deploying to an endpoint, it fails when it tries to run this line:\n\nbucket = client.get_bucket(\"my-project-id\")\n\nIn the logs, there was an error message that reads\n\ngoogle.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/{my project ID}?projection=noAcl&prettyPrint=false: custom-online-prediction@{some random numbers}-tp.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\n\nIt appears that the endpoint has been assigned a service account that is not associated with my account. From this documentation (https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/custom-service-account), it appears that a\u00a0service account managed by AI Platform Prediction is being used when a custom container is being used. However, my account does not have the permissions\u00a0to create another custom service account, as it is being managed by my client.\u00a0\n\nI came across this solution (https:\/\/stackoverflow.com\/questions\/68456262\/gcp-vertex-ai-model-access-gcs-failed) where the user had the exact same problem and solved it by changing the service account used at the endpoint. As such, I would like to ask how it will be possible for me to change the service account used by the endpoint without having to create a new service account?\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    }
]