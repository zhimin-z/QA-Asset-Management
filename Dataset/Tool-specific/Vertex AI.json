[
    {
        "Question_title":"Does Vertex AI support labels for counting?",
        "Question_tag_count":2,
        "Question_created_time":"2021-10-25T08:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-labels-for-counting\/m-p\/173840#M66",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":472,
        "Question_body":"I have an image where I have to do a multi-label classification and additionally count the number of a specific item in each image. I'm trying to setup a labeling task so I can enter a continuous number (0-100 for example), but there doesn't seem to be support for it.\u00a0\n\n\u00a0\n\nAdditionally, does the labeling have capabilities to pre-choose a \"default\"\u00a0 value?\n\n\u00a0\n\nDoes anyone have an idea?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Save an text from Google Speech-to-text to cloud storage",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-26T11:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Save-an-text-from-Google-Speech-to-text-to-cloud-storage\/m-p\/503516#M991",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":98,
        "Question_body":"Hi all ,\u00a0\n\nIs it possible to directly save the text from speech to text to cloud storage using python client library?\n\nAt the moment I was able to perform this save operation only by waiting for the end of the speech-to-text operation, saving the text stack as a file and only then uploading it to the storage bucket",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Getting started DialogFlow CX",
        "Question_tag_count":2,
        "Question_created_time":"2022-09-24T12:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-started-DialogFlow-CX\/m-p\/470603#M598",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":118,
        "Question_body":"Looking into starting a project using DialogFlow CX. Seems rather promising but have one issue I cannot seem to find an answer for. The agent will be connected to via IVR (from Flex\/Callcenter). I need to gather some information on start so that I can identify the hotel\/property that will be referenced in the conversation.\u00a0 I found session parameters but those are isolated to the session from start to finish but not passed to the start of a session. We are starting with about 60 properties and when the agent starts, it needs to \"know\" what property it is dealing with.\u00a0\n\nAnother quick question - will I need a separate telephony integration number to run multiple concurrent instances?\u00a0\n\nI am really new to all this so my language may be off. Thanks in advance!!\n\nRobert",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Why is sample rate optional only for FLAC or WAV file and not other formats?",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-21T05:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Why-is-sample-rate-optional-only-for-FLAC-or-WAV-file-and-not\/m-p\/415704#M287",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":152,
        "Question_body":"So for example at my work we are using WEBM_OPUS encoding, which from what I understand, specificies the sample rate in audio stream metadata itself? Yet from here: https:\/\/cloud.google.com\/speech-to-text\/docs\/basics#sample-rates it says the field is only optional\u00a0 for FLAC or WAV formats.\n\nAnd indeed, when I try the GSTT API with some example code (Streaming Recognition and a\u00a0WEBM_OPUS encoded at 48000 sample rate), the GSTT actually accepts sample rates other than 48000 - and depending on the recognition model, produces different results depending on the sample rate selected!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"vetex ai tensorboard absurd charges",
        "Question_tag_count":4,
        "Question_created_time":"2023-07-12T04:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/vetex-ai-tensorboard-absurd-charges\/m-p\/611509#M2344",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":36,
        "Question_body":"Hey,\n\nAfter enabling Vertex services and initiated training i was charged at once of 300$ which later i got to know was Tensorboard fee per user per month. first of all this was totally shocking to me that google is charging this much for logging metrics (seriously) and secondly not even a warning that a Platform where all the pricing is directly proportional to disk space consumed\/month or by the hour (training), google would charge this absurd amount basically for a tool which i can host locally for free.\n\nGoogle is revising this fee in augest from 300 to 10$\/month (finally someone understood there) ,is there any way this fee can be reimbursed to my account if i open a\u00a0 case\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google assistant and cloud speech API not working",
        "Question_tag_count":2,
        "Question_created_time":"2022-04-06T04:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-assistant-and-cloud-speech-API-not-working\/m-p\/410871#M260",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":184,
        "Question_body":"I am having problem using Google cloud platform.\n\nI bought Google AIY voice kit from AliExpress.com. I discovered it was an old version. Two weeks ago, I used Etcher to flash aiyprojects-2021-04-02.img.xz from GitHub on an SD card and set up my voice kit. Hardware testing was good. I then created a project, named \u201cVoice Kit\u201d, on google cloud following directions given on \"aiyprojects.withgoogle.com\/voice-v1\/\". I had the following experience:\n\nI used \u201csrc\/examples\/voice\/assistant_library_demo.py\u201d command, and signed in with my google account. After some warnings and authentication, google assistant responded to commands. It worked.\nI used \u201csrc\/examples\/voice\/assistant_grpc_demo.py\u201d command and got error when I pressed the button. The error ended with \u201cgrpc_message\u201d: \u201cinvalid \u2018dialog_state_in\u2019: unsupported language_code.\u201d,\u201dgrpc_status\u201d: 3}\u201d\nI created a billing account and a service account, but my cloud_speech.json credentials did not download after following the instructions stated at aiyprojects.withgoogle.com\/voice-v1. I checked \u201dservice account\u201d on google cloud platform. Under the e-mail column, my-billing-account@voice kit\u2026. was listed. Status column had Ok. Name column had \u201cmy billing account\u201d. However, the key ID column had \u201cNo keys\u201d.\n\nIt would be appreciated if I could be educated on the following:\n\nHow do I get cloud_speech_credentials to download automatically as stated in the instructions I am following? Don\u2019t I need keys? If I do, how do I get them?\nThe instructions on aiyprojects.withgoogle.com\/voice-v1 asked us to enter \u201cCloud Speech API\u201d into the search bar, then click the name to enable. But one sees both speech-to-text, and text-to speech APIs after the search. Do we enable both?\nThe instructions ask us to add ourselves as test users, but the token for test users expire after 7 days. I will want to continue using my assistant for years. What do I do to ensure that I do not end up with expired token again after seven days?\nHow do I correct the fault I am having with the \u201csrc\/examples\/voice\/assistant_grpc_demo.py\u201d command?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-21T00:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow\/m-p\/524405#M1302",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hi, is it possible to localise my bot to a specific group of users (Maybe IP) when I integrate on the web?\n\nThis is for the Dialogflow ES bot",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"A 10+2 Students thrust for Programming can really enable his abilities to capture the AI?",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-23T08:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/A-10-2-Students-thrust-for-Programming-can-really-enable-his\/m-p\/492261#M850",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":42,
        "Question_body":"Students passed out from 10+2 are desperate enough to pursue AI through certifications, bachelor degree and in many other ways....\n\n\u00a0\n\nI am wondering is it really possible for them to capture AI and navigate in an easy manner these days.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"CX - Can only select 1 language and 1 voice?",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-08T00:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/CX-Can-only-select-1-language-and-1-voice\/m-p\/551213#M1819",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"In the DialogFlow CX agent settings, I can only select 1 voice, the Default 1. At the same time, the GCP supports many Dutch voices. Have I configured something wrong here?\n\nCX\u00a0\u2003settings:\n\nGCP Dutch voices:",
        "Question_closed_time":"05-08-2023 02:32 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Not natively, but you can do that by integrating the TTS on your webhook and returning an audio file at anytime:\u00a0https:\/\/stackoverflow.com\/questions\/65186184\/how-to-send-audio-back-from-dialogflow-cx-webhook\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"I am trying to use google vision api in c++ to read local images",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-12T15:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-am-trying-to-use-google-vision-api-in-c-to-read-local-images\/m-p\/453466#M503",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"I have tried using c++ to use google vision api to read local files, but I have no experience with google, and I am mainly a c++ developer, so changing languages is not that of an option. Do any one knows how to do it in this language. Furthermore, any written documentation or tutorial in my language(c++) will be extremely helpful.\n\nThanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issues with importing aiplatform",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-14T01:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/m-p\/489087#M771",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.\n\nWhen running \"from google.cloud import aiplatform\", I get the following error message:\n\nImportError                               Traceback (most recent call last)\n\/tmp\/ipykernel_22080\/3236611779.py in <module>\n      4 #!python #3.7.12\n      5 \n----> 6 from google.cloud import aiplatform\n      7 \n      8 import tensorflow as tf\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     24 \n     25 from google.api_core import client_options\n---> 26 from google.api_core import gapic_v1\n     27 import google.auth\n     28 from google.auth import credentials as auth_credentials\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/__init__.py in <module>\n     17 from google.api_core.gapic_v1 import config_async\n     18 from google.api_core.gapic_v1 import method\n---> 19 from google.api_core.gapic_v1 import method_async\n     20 from google.api_core.gapic_v1 import routing_header\n     21 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method_async.py in <module>\n     20 import functools\n     21 \n---> 22 from google.api_core import grpc_helpers_async\n     23 from google.api_core.gapic_v1 import client_info\n     24 from google.api_core.gapic_v1.method import _GapicCallable\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers_async.py in <module>\n     23 \n     24 import grpc\n---> 25 from grpc import aio\n     26 \n     27 from google.api_core import exceptions, grpc_helpers\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/aio\/__init__.py in <module>\n     21 \n     22 import grpc\n---> 23 from grpc._cython.cygrpc import (init_grpc_aio, shutdown_grpc_aio, EOF,     24                                  AbortError, BaseError, InternalError,\n     25                                  UsageError)\n\nImportError: cannot import name 'shutdown_grpc_aio' from 'grpc._cython.cygrpc' (\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_cython\/cygrpc.cpython-37m-x86_64-linux-gnu.so)\n\nThe versions of the concerned libraries are shown below.\n\ngoogle-api-core                       2.10.1\ngoogle-api-python-client              2.55.0\ngoogle-cloud-aiplatform               1.17.0\n\ngrpcio                                1.33.1\ngrpcio-gcp                            0.2.2\ngrpcio-status                         1.47.0\n\nI have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?\n\nThank you!",
        "Question_closed_time":"11-14-2022 05:15 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Python API to view\/list Vertex AI Feature Store ingestion jobs",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-13T11:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-API-to-view-list-Vertex-AI-Feature-Store-ingestion-jobs\/m-p\/477834#M648",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":342,
        "Question_body":"It's possible to view currently running feature ingestion jobs in the console (https:\/\/console.cloud.google.com\/vertex-ai\/ingestion-jobs). How can I list currently running ingestion jobs using a Python API?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI integration with mlflow ?",
        "Question_tag_count":3,
        "Question_created_time":"2022-02-18T03:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-integration-with-mlflow\/m-p\/394738#M201",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Is there any way to integrate vertex AI with mlflow ?\u00a0\nAny articles or resources I can go through ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Is it possible to use Vertex AI experiments without Tensorboard?",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-12T01:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Is-it-possible-to-use-Vertex-AI-experiments-without-Tensorboard\/m-p\/552738#M1884",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":329,
        "Question_body":"Is it possible to use Vertex AI experiments without Tensorboard?\n\nWhen I programmatically init the SDK and start experiment run as described in docs, Tensorboard is automatically provisioned.\n\nI would like to use only summary metrics and parameters tracking (i.e. not time-series metrics), and AFAIK it doesn't require Tensorboard.\n\nHere's the code snippet:\n\n\u00a0\n\naiplatform.init(experiment=\"foo\", location=\"us-central1\")\naiplatform.start_run(run=\"bar\")\naiplatform.log_metrics({\"score\": 0.42})",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Co-hosting Pytorch models on Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-22T02:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Co-hosting-Pytorch-models-on-Vertex-AI\/m-p\/605590#M2204",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":51,
        "Question_body":"Hi,\u00a0\n\nI'm trying to co-host pytorch models on vertex following\u00a0https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/introducing-co-hosting-models-on-the-vert...\u00a0from July 2022. On the article it say support is only for TF models. Is this still the case? if so what is the ETA on Pytorch support? Also, is the a current manual workaround for Pytorch models?",
        "Question_closed_time":"06-23-2023 05:48 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@xolisani,\n\nWelcome to Google Cloud Community!\n\nAs of now, it only supports Tensorflow models, but please note that a feature request was filed to include pytorch models and the request was already forwarded to the\u00a0Vertex AI Engineering Team for evaluation although currently there is no exact ETA when it will be available but you can track the progress using this link:\u00a0https:\/\/issuetracker.google.com\/255271990\nAs a workaround, you can try packing all your models in one container and use a custom HTTP server logic\u00a0 for prediction requests that uses the parameters field in the prediction request body in order to send it to the correct model. You can check this link that I found for more information, please note that this is not supported by Google but I am sending it to you since it might be helpful to your case:\u00a0https:\/\/stackoverflow.com\/questions\/69878915\/deploying-multiple-models-to-same-endpoint-in-vertex-ai\n\nHope this is useful!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Deep Reinforcement Learning",
        "Question_tag_count":3,
        "Question_created_time":"2022-04-13T06:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Reinforcement-Learning\/m-p\/413277#M269",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":98,
        "Question_body":"Hi is it possible to implement Deep Reinforcement Learning for structured data frames? If son can someone help me with an example?",
        "Question_closed_time":"04-20-2022 01:38 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Deep Learning delivers a seamless notebook experience with integrated support for JupyterLab[1], so you can load data frames as a normal notebook. Additionally, it depends on what instance you are using Deep Learning.\u00a0\n\nIf you are using TensorFlow, you can see this[2] to know how to load a data frame to TensorFlow.\n\nIf you are using Pytorch tensor, you can see this[3] example of how to load the data frame.\n\n\u00a0\n\n[1] https:\/\/cloud.google.com\/deep-learning-vm\/docs\/jupyter\u00a0\n\n[2] https:\/\/www.tensorflow.org\/tutorials\/load_data\/pandas_dataframe\u00a0\n\n[3] https:\/\/stackoverflow.com\/a\/50308132\/16929358\u00a0\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Airflow Dag for Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-20T02:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Airflow-Dag-for-Vertex-AI\/m-p\/491123#M821",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":151,
        "Question_body":"Please advice on how to create airflow dag for vertex ai.\n\n**********************************************************\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom google.cloud import aiplatform\nfrom airflow.operators import CreateDatasetOperator\n\nYESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)\n\n\ndefault_dag_args = {\n\n'start_date': YESTERDAY,\n}\n\nwith models.DAG(\n'composer_sample_simple_greeting',\nschedule_interval=datetime.timedelta(weeks=2),\ndefault_args=default_dag_args) as dag:\n\ndef create_entity_type_sample(\nproject: str,\nlocation: str,\nentity_type_id: str,\nvertexai: str,\nservice_account_id: str\ntask_id: str,\nproject_id: str,\n\naiplatform.init(project=project, location=location)\n\nmy_entity_type = aiplatform.EntityType.create(\nentity_type_id=entity_type_id, vertexai=vertexai\n)\n\nmy_entity_type.wait()\n\nreturn my_entity_type\n\n\ncreate_image_dataset_job = CreateDatasetOperator(\ntask_id=\"image_dataset\",\ndataset=IMAGE_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_tabular_dataset_job = CreateDatasetOperator(\ntask_id=\"tabular_dataset\",\ndataset=TABULAR_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_text_dataset_job = CreateDatasetOperator(\ntask_id=\"text_dataset\",\ndataset=TEXT_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_video_dataset_job = CreateDatasetOperator(\ntask_id=\"video_dataset\",\ndataset=VIDEO_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\ncreate_time_series_dataset_job = CreateDatasetOperator(\ntask_id=\"time_series_dataset\",\ndataset=TIME_SERIES_DATASET,\nregion=REGION,\nproject_id=PROJECT_ID,\n)\n\ncreate_image_dataset_job >> create_tabular_dataset_job >> create_text_dataset_job >> create_video_dataset_job >> create_time_series_dataset_job\n************************************************************************************************************************",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Problems with using ko-KR-Neural2 in tts API.",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-11T23:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Problems-with-using-ko-KR-Neural2-in-tts-API\/m-p\/602121#M2131",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"SynthesisInput synthesisInput = SynthesisInput.newBuilder()\n\t\t\t.setText(textToSpeechRequest.getMessage())\n\t\t\t.build();\n\t\tAudioConfig audioConfig = AudioConfig.newBuilder()\n\t\t\t.setAudioEncoding(AudioEncoding.LINEAR16)\n\t\t\t.build();\n\n\t\tVoiceSelectionParams voiceSelectionParams = VoiceSelectionParams.newBuilder()\n\t\t\t.setLanguageCode(\"ko-KR\")\n\t\t\t.setSsmlGender(SsmlVoiceGender.MALE)\n\t\t\t\/\/ .setName(\"ko-KR-Standard-A\")\n\t\t\t.build();\n\t\tSynthesizeSpeechRequest synthesizeSpeechRequest = SynthesizeSpeechRequest.newBuilder()\n\t\t\t.setInput(synthesisInput)\n\t\t\t.setAudioConfig(audioConfig)\n\t\t\t.setVoice(voiceSelectionParams)\n\t\t\t.build();\n\n\t\tSynthesizeSpeechResponse speechResponse = textToSpeechClient.synthesizeSpeech(synthesizeSpeechRequest);\n\n\u00a0\n\n\u00a0\n\nThis is my tts code.\u00a0 Currently, I have two problems that I can't solve.\n\nproblem\u00a01\n\n\nError when calling tts api using Neutral2.\n\n[INVALID_ARGUMENT: This request contains sentences that are too long. To fix, split up long sentences with sentence ending punctuation e.g. periods.]\nMy string\n\n\ub3d9\ud574\ubb3c\uacfc \ubc31\ub450\uc0b0\uc774 \ub9c8\ub974\uace0 \ub2f3\ub3c4\ub85d \ud558\ub290\ub2d8\uc774 \ubcf4\uc6b0\ud558\uc0ac \uc6b0\ub9ac\ub098\ub77c\ub9cc\uc138 \ubb34\uad81\ud654 \uc0bc\ucc9c\ub9ac \ud654\ub824\uac15\uc0b0 \ub300\ud55c\uc0ac\ub78c \ub300\ud55c\uc73c\ub85c \uae38\uc774 \ubcf4\uc804\ud558\uc138 \ub0a8\uc0b0\uc704\uc5d0 \uc800 \uc18c\ub098\ubb34 \ucca0\uac11\uc744 \ub450\ub978\ub4ef \ubc14\ub78c\uc11c\ub9ac \ubd88\ubcc0\ud568\uc740 \uc6b0\ub9ac\uae30\uc0c1 \uc77c\uc138 \ubb34\uad81\ud654 \uc0bc\ucc9c\ub9ac \ud654\ub824\uac15\uc0b0 \ub300\ud55c\uc0ac\ub78c \ub300\ud55c\uc73c\ub85c \uae38\uc774\ubcf4\uc804\ud558\uc138 \uac00\uc744\ud558\ub298 \uacf5\ud65c\ud55c\ub370 \ub192\uace0 \uad6c\ub984\uc5c6\uc774 \ubc1d\uc740\ub2ec\uc740 \uc6b0\ub9ac\uac00\uc2b4 \uc77c\ud3b8\ub2e8\uc2ec\uc77c\uc138 \ubb34\uad81\ud654 \uc0bc\ucc9c\ub9ac \ud654\ub824\uac15\uc0b0 \ub300\ud55c\uc0ac\ub78c \ub300\ud55c\uc73c\ub85c \uae38\uc774\ubcf4\uc804\ud558\uc138\nThe above problem is the same in DEMO.\n\nBut, when using ko-KR-Standard, it works normally. Should I write Standard?\u00a0Can't I use Neural2?\n\nproblem\u00a02\n\nError on call after setting languageCode to KR and SsmlGender to MALE. \u00a0\nINVALID_ARGUMENT: Requested male voice, but voice ko-KR-Neural2-A is a female voice.\u200b\nIf name is not set, language_code and gender are not selected.\n\nShouldn't I bring the male voice ko-KR-Nural2-C because I made setSsmlGender MALE? Let me know if I'm wrong.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Analyzing the image's text with the Natural Language API",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-19T07:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Analyzing-the-image-s-text-with-the-Natural-Language-API\/m-p\/604500#M2188",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"HI\u00a0\n\n\u00a0i try many times to run this code but some thing wrong ,\u00a0\n\nTo set up the API request, create a\u00a0nl-request.json\u00a0file with the following:\n\n{\n  \"document\":{\n    \"type\":\"PLAIN_TEXT\",\n    \"content\":\"your_text_here\"\n  },\n  \"encodingType\":\"UTF8\"\n}\nCopied!\ncontent_copy\n\nIn the request, you're telling the Natural Language API about the text you're sending:\n\ntype:\u00a0supported type values are\u00a0PLAIN_TEXT\u00a0or\u00a0HTML.\n\ncontent:\u00a0pass the text to send to the Natural Language API for analysis. The Natural Language API also supports sending files stored in Cloud Storage for text processing. To send a file from Cloud Storage, replace\u00a0content\u00a0with\u00a0gcsContentUri\u00a0and use the value of the text file's uri in Cloud Storage.\n\nencodingType:\u00a0tells the API which type of text encoding to use when processing the text. The API will use this to calculate where specific entities appear in the text.\n\nRun this Bash command in Cloud Shell to copy the translated text into the content block of the Natural Language API request:\n\nSTR=$(jq .data.translations[0].translatedText  translation-response.json) && STR=\"${STR\/\/\\\"}\" && sed -i \"s|your_text_here|$STR|g\" nl-request.json\nCopied!\ncontent_copy\n\nThe\u00a0nl-request.json\u00a0file now contains the translated English text from the original image. Time to analyze it!\n\nCall the\u00a0analyzeEntities\u00a0endpoint of the Natural Language API with this\u00a0curl\u00a0request:\n\ncurl \"https:\/\/language.googleapis.com\/v1\/documents:analyzeEntities?key=${API_KEY}\" \\\n  -s -X POST -H \"Content-Type: application\/json\" --data-binary @nl-request.json\n\ni have this issus\n\nstudent_00_fc5405542deb@cloudshell:~ (qwiklabs-gcp-01-39d2648c29b0)$ curl \"https:\/\/language.googleapis.com\/v1\/documents:analyzeEntities?key=${API_KEY}\" \\\n-s -X POST -H \"Content-Type: application\/json\" --data-binary @nl-request.json\n{\n\"error\": {\n\"code\": 403,\n\"message\": \"The request is missing a valid API key.\",\n\"status\": \"PERMISSION_DENIED\"\n}\n}\n\n\u00a0\n\nhelp plz",
        "Question_closed_time":"06-22-2023 10:59 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@IBRAHIM-K,\n\nWelcome to Google Cloud Community!\n\nYou are encountering this error since your\u00a0API_KEY variable is invalid or missing. You can try checking your variable if it contains your API_KEY by running the following:\n\n\n\nprintenv API_KEY\n\nIf it does not contain your API_KEY, it is possible you were not able to export your API keys as a variable but if your variable API_KEY was able to produce an output that contains an API key, it is possible that your API key is invalid. To solve this problem, here is a step by step process on how to create an API KEY and export it as a variable.\u00a0\n\n1. In your console > Click Navigation Menu > Select API & Services > Select Credentials\n2. Click Create Credentials > Select API key (This will generate an API key) > Copy the API key that was generated\n3. In your Cloud Shell, run the following:\u00a0\n\nexport API_KEY = <PASTE YOUR COPIED API KEY HERE>\n\nPlease note that you need to remove the symbols <\u00a0 and >\n\nYou can try checking again if you're able to export the variable API_KEY by running the following:\n\nprintenv API_KEY\u200b\n\nIt should output your API keys, after that you should be able to call\u00a0the\u00a0analyzeEntities\u00a0endpoint.\n\nHope this is useful!\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"building a custom jupyter notebook container that supports python 3.10",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-18T09:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/building-a-custom-jupyter-notebook-container-that-supports\/m-p\/554623#M1955",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":152,
        "Question_body":"the off the shelf notebooks only support python 3.7. This doesn't work for us as many of the libraries we use require a minimum of python 3.8 or higher.\n\nIs there a way to create a customer notebook where we can configure the correct environment for our needs?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI not training custom jobs in batches larger than 20?",
        "Question_tag_count":3,
        "Question_created_time":"2023-02-05T11:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-not-training-custom-jobs-in-batches-larger-than-20\/m-p\/518322#M1210",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":270,
        "Question_body":"Whenever I try to train more than 20 jobs (custom containers) in parallel it seems like the jobs enter a queue such that only 20 run at one time, my quotas should allow me to do much more (cpu and vertex ai api way under limit, same with all my compute stuff).\n\nIs there any way to see why those jobs aren't starting instantly or to increase the number of custom jobs running at one time? I don't even see them in pending but when I submit them all using sync=False in the workbench they get submitted with no error instantly.\n\nI also tried to throttle the start of each training job incase its a per min request limit thats causing the limit of 20 parallel jobs but that didnt solve it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Instance schedule for TPU VM",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-22T06:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Instance-schedule-for-TPU-VM\/m-p\/502564#M981",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"Hello,\n\nI would like to attach an instance schedule to a TPU VM. I have followed these instructions, and can correctly create an instance schedule, but it seems I can only attach it to regular VMs, i.e., VMs listed under the \"instances\" of my project. Is there a way around this?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow cx sdk pricing",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-12T02:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-sdk-pricing\/m-p\/488718#M767",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":107,
        "Question_body":"Hi all!\u00a0\nI'm using Dialogflox cx python libraries to make some tests with intents matching. To do these tests, I'm using the match_intent method of the SessionClient class. I digged in the pricing table of Dialogflow CX but as far as I searched I didn't understand if there's a pricing for request made with SessionClient, and I'm here to ask for your help.\n\nThank you in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Strange behaviour of ARIMA model",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-22T08:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Strange-behaviour-of-ARIMA-model\/m-p\/445988#M441",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":110,
        "Question_body":"Hi guys,\u00a0\n\nI'm working with ARIMA Model and I found a strange behaviour.\n\nI have two dataset called\u00a0\n\nanomaly_detection_poc.sample10 (the values are these\u00a0302.77562,302.0675,300.03862,297.24675,295.94825,293.798,291.0855,29.294441,40.031734,160.875 in loop every minute starting from 2022-07-01)\nanomaly_detection_poc.sample11 (the values are these\u00a0\n302.307,302.77562,302.0675,300.03862,297.24675,295.94825,293.798,291.0855,29.294441,40.031734,160.875 in loop every minute starting from 2022-07-01)\n\nThen I create two model in this way\n\nsample_10_arima\n\nCREATE OR REPLACE MODEL\n`anomaly_detection_poc.sample_10_arima`\nOPTIONS(MODEL_TYPE = 'ARIMA_PLUS'\n    , TIME_SERIES_TIMESTAMP_COL = 'ts'\n    , TIME_SERIES_DATA_COL = 'value'\n    , HORIZON = 30\n    , AUTO_ARIMA = TRUE \n    , AUTO_ARIMA_MAX_ORDER = 5\n    , DATA_FREQUENCY = 'PER_MINUTE'\n    , CLEAN_SPIKES_AND_DIPS = FALSE\n    , MAX_TIME_SERIES_LENGTH = 10080\n    )\nAS \nSELECT *\nFROM `anomaly_detection_poc.sample10`\nWHERE ts <= '2022-07-03 02:00:00'\nORDER BY ts DESC\nLIMIT 500;\n\nsample_11_arima\n\nCREATE OR REPLACE MODEL\n`anomaly_detection_poc.sample_11_arima`\nOPTIONS(MODEL_TYPE = 'ARIMA_PLUS'\n    , TIME_SERIES_TIMESTAMP_COL = 'ts'\n    , TIME_SERIES_DATA_COL = 'value'\n    , HORIZON = 30\n    , AUTO_ARIMA = TRUE \n    , AUTO_ARIMA_MAX_ORDER = 5\n    , DATA_FREQUENCY = 'PER_MINUTE'\n    , CLEAN_SPIKES_AND_DIPS = FALSE\n    , MAX_TIME_SERIES_LENGTH = 10080\n    )\nAS \nSELECT *\nFROM `anomaly_detection_poc.sample11`\nWHERE ts <= '2022-07-03 02:00:00'\nORDER BY ts DESC\nLIMIT 500;\n\nThen I call the ML.FORECAST function for both in that way\n\nSELECT\n    f.forecast_timestamp,\n    f.forecast_value,\n    s10.value,\n    f.standard_error\nFROM\n    ML.FORECAST(MODEL `anomaly_detection_poc.sample_10_arima`,\n                STRUCT(30 AS horizon, 0.95 AS confidence_level)) f\nINNER JOIN `anomaly_detection_poc.sample10` s10 on s10.ts = f.forecast_timestamp;\n\nResult for sample_10:\n\n[{\n  \"forecast_timestamp\": \"2022-07-03T02:01:00Z\",\n  \"forecast_value\": \"306.41173285451686\",\n  \"value\": \"304.609972052415\",\n  \"standard_error\": \"2.8199310058400839\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:02:00Z\",\n  \"forecast_value\": \"305.37788426343457\",\n  \"value\": \"303.95136089787633\",\n  \"standard_error\": \"2.8211256733849956\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:03:00Z\",\n  \"forecast_value\": \"306.739091217752\",\n  \"value\": \"303.6495107813119\",\n  \"standard_error\": \"2.8223198352358345\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:04:00Z\",\n  \"forecast_value\": \"300.3377701753941\",\n  \"value\": \"298.6072872324707\",\n  \"standard_error\": \"2.8235134920342255\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:05:00Z\",\n  \"forecast_value\": \"300.10617792916611\",\n  \"value\": \"294.65681486796137\",\n  \"standard_error\": \"2.8247066444204409\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:06:00Z\",\n  \"forecast_value\": \"298.31753620063341\",\n  \"value\": \"299.76196578250261\",\n  \"standard_error\": \"2.8258992930334\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:07:00Z\",\n  \"forecast_value\": \"30.122356218709314\",\n  \"value\": \"29.451189674897449\",\n  \"standard_error\": \"2.8270914385106733\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:08:00Z\",\n  \"forecast_value\": \"40.60273774219533\",\n  \"value\": \"41.3004774222539\",\n  \"standard_error\": \"2.82828308148849\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:09:00Z\",\n  \"forecast_value\": \"165.86050548201936\",\n  \"value\": \"162.47927425294355\",\n  \"standard_error\": \"2.8294742226017382\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:10:00Z\",\n  \"forecast_value\": \"307.674541548093\",\n  \"value\": \"314.076719817158\",\n  \"standard_error\": \"2.8306648624839696\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:11:00Z\",\n  \"forecast_value\": \"311.2102908255867\",\n  \"value\": \"305.12615161719276\",\n  \"standard_error\": \"2.8318550017674053\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:12:00Z\",\n  \"forecast_value\": \"306.226013674404\",\n  \"value\": \"313.07125067705323\",\n  \"standard_error\": \"2.833044641082938\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:13:00Z\",\n  \"forecast_value\": \"300.75685873687024\",\n  \"value\": \"304.717664646629\",\n  \"standard_error\": \"2.8342337810601355\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:14:00Z\",\n  \"forecast_value\": \"305.61417943694067\",\n  \"value\": \"310.23180786201976\",\n  \"standard_error\": \"2.8354224223272473\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:15:00Z\",\n  \"forecast_value\": \"303.68840718603383\",\n  \"value\": \"303.64800323499333\",\n  \"standard_error\": \"2.8366105655112044\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:16:00Z\",\n  \"forecast_value\": \"301.47419763922971\",\n  \"value\": \"296.79744824800957\",\n  \"standard_error\": \"2.8377982112376268\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:17:00Z\",\n  \"forecast_value\": \"29.893389212037278\",\n  \"value\": \"29.610424285628419\",\n  \"standard_error\": \"2.8389853601308261\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:18:00Z\",\n  \"forecast_value\": \"41.396011596661424\",\n  \"value\": \"41.5123352698129\",\n  \"standard_error\": \"2.8401720128138077\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:19:00Z\",\n  \"forecast_value\": \"163.35720284915556\",\n  \"value\": \"168.06883872246493\",\n  \"standard_error\": \"2.8413581699082777\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:20:00Z\",\n  \"forecast_value\": \"308.69299234826951\",\n  \"value\": \"302.82608922667845\",\n  \"standard_error\": \"2.8425438320346439\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:21:00Z\",\n  \"forecast_value\": \"304.99294433140392\",\n  \"value\": \"305.39582570271523\",\n  \"standard_error\": \"2.8437289998120212\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:22:00Z\",\n  \"forecast_value\": \"310.90293309247426\",\n  \"value\": \"302.20232853605353\",\n  \"standard_error\": \"2.8449136738582346\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:23:00Z\",\n  \"forecast_value\": \"304.67148335427134\",\n  \"value\": \"302.14484060782462\",\n  \"standard_error\": \"2.8460978547898241\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:24:00Z\",\n  \"forecast_value\": \"304.85940187532356\",\n  \"value\": \"308.93581009174005\",\n  \"standard_error\": \"2.8472815432220462\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:25:00Z\",\n  \"forecast_value\": \"301.24253166854521\",\n  \"value\": \"295.05089549592532\",\n  \"standard_error\": \"2.8484647397688811\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:26:00Z\",\n  \"forecast_value\": \"298.28522662702073\",\n  \"value\": \"293.5837686256578\",\n  \"standard_error\": \"2.8496474450430318\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:27:00Z\",\n  \"forecast_value\": \"30.56919200039917\",\n  \"value\": \"29.483629896147075\",\n  \"standard_error\": \"2.8508296596559326\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:28:00Z\",\n  \"forecast_value\": \"40.988491674274059\",\n  \"value\": \"40.678949191269574\",\n  \"standard_error\": \"2.8520113842177488\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:29:00Z\",\n  \"forecast_value\": \"163.75582623522916\",\n  \"value\": \"163.35355503598811\",\n  \"standard_error\": \"2.853192619337384\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:30:00Z\",\n  \"forecast_value\": \"313.92758659728719\",\n  \"value\": \"305.91143123619014\",\n  \"standard_error\": \"2.8543733656224797\"\n}]\n\nResult for sample_11:\n\n[{\n  \"forecast_timestamp\": \"2022-07-03T02:01:00Z\",\n  \"forecast_value\": \"100.53693546817703\",\n  \"value\": \"41.135001562854711\",\n  \"standard_error\": \"60.184482472084568\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:02:00Z\",\n  \"forecast_value\": \"198.78980079180036\",\n  \"value\": \"161.18096707015462\",\n  \"standard_error\": \"73.23520166522043\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:03:00Z\",\n  \"forecast_value\": \"274.06282751495371\",\n  \"value\": \"309.11862890065476\",\n  \"standard_error\": \"73.965335710915454\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:04:00Z\",\n  \"forecast_value\": \"314.73754129211278\",\n  \"value\": \"317.59056591831012\",\n  \"standard_error\": \"89.992938647114883\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:05:00Z\",\n  \"forecast_value\": \"231.57017561331438\",\n  \"value\": \"302.83582623865914\",\n  \"standard_error\": \"101.16968833227945\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:06:00Z\",\n  \"forecast_value\": \"245.46052654438029\",\n  \"value\": \"309.55330946160922\",\n  \"standard_error\": \"101.46381053942768\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:07:00Z\",\n  \"forecast_value\": \"243.14060397122429\",\n  \"value\": \"310.52346182933485\",\n  \"standard_error\": \"101.47200274626169\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:08:00Z\",\n  \"forecast_value\": \"243.52807011748283\",\n  \"value\": \"308.20566493507033\",\n  \"standard_error\": \"101.47223125532678\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:09:00Z\",\n  \"forecast_value\": \"243.46335674461702\",\n  \"value\": \"306.47721383658927\",\n  \"standard_error\": \"101.47223762949433\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:10:00Z\",\n  \"forecast_value\": \"243.47416496791112\",\n  \"value\": \"304.5239238135868\",\n  \"standard_error\": \"101.47223780729935\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:11:00Z\",\n  \"forecast_value\": \"243.47235981256838\",\n  \"value\": \"29.60521677814036\",\n  \"standard_error\": \"101.47223781225917\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:12:00Z\",\n  \"forecast_value\": \"243.47266130391648\",\n  \"value\": \"41.807929278280589\",\n  \"standard_error\": \"101.47223781239751\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:13:00Z\",\n  \"forecast_value\": \"243.47261094978307\",\n  \"value\": \"166.22813087686413\",\n  \"standard_error\": \"101.47223781240136\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:14:00Z\",\n  \"forecast_value\": \"243.4726193597715\",\n  \"value\": \"311.120674277285\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:15:00Z\",\n  \"forecast_value\": \"243.47261795516178\",\n  \"value\": \"314.0272359705304\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:16:00Z\",\n  \"forecast_value\": \"243.47261818975525\",\n  \"value\": \"305.7107591848029\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:17:00Z\",\n  \"forecast_value\": \"243.47261815057419\",\n  \"value\": \"310.24411130736274\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:18:00Z\",\n  \"forecast_value\": \"243.47261815711809\",\n  \"value\": \"310.7495483682959\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:19:00Z\",\n  \"forecast_value\": \"243.47261815602516\",\n  \"value\": \"309.75593252507184\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:20:00Z\",\n  \"forecast_value\": \"243.47261815620769\",\n  \"value\": \"307.30120520293525\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:21:00Z\",\n  \"forecast_value\": \"243.47261815617719\",\n  \"value\": \"305.22389101135269\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:22:00Z\",\n  \"forecast_value\": \"243.47261815618231\",\n  \"value\": \"30.201925796580159\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:23:00Z\",\n  \"forecast_value\": \"243.47261815618145\",\n  \"value\": \"41.982169328730954\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:24:00Z\",\n  \"forecast_value\": \"243.4726181561816\",\n  \"value\": \"166.40265635454585\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:25:00Z\",\n  \"forecast_value\": \"243.47261815618157\",\n  \"value\": \"313.89491002223463\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:26:00Z\",\n  \"forecast_value\": \"243.47261815618157\",\n  \"value\": \"304.88350360153481\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:27:00Z\",\n  \"forecast_value\": \"243.47261815618157\",\n  \"value\": \"312.68737586889137\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:28:00Z\",\n  \"forecast_value\": \"243.47261815618157\",\n  \"value\": \"306.59293265266768\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:29:00Z\",\n  \"forecast_value\": \"243.47261815618157\",\n  \"value\": \"302.44991069056488\",\n  \"standard_error\": \"101.47223781240149\"\n}, {\n  \"forecast_timestamp\": \"2022-07-03T02:30:00Z\",\n  \"forecast_value\": \"243.47261815618157\",\n  \"value\": \"306.66045835707689\",\n  \"standard_error\": \"101.47223781240149\"\n}]\n\nIn the first case sample_10_arima the standard_error is low (around 2.8) but in the sample_11_arima the standard_error is high (between 60 and 101). Why this difference occour? The time series are very similar\n\nThanks,\u00a0\n\nMarcello",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Quota increase for training on Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-13T03:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-increase-for-training-on-Vertex-AI\/m-p\/509872#M1061",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":375,
        "Question_body":"I am trying to use the custom container method to train a simple model. I am executing a config.yaml file through which I am running a job but I received the following error.\u00a0\n\nError:\u00a0\"error\": { \"code\": 429, \"message\": \"The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_n2_cpus\", \"status\": \"RESOURCE_EXHAUSTED\" } }\n\nThe config.yaml file looks like this:\u00a0\n\nworkerPoolSpecs:\n\u00a0\u00a0machineSpec:\n\u00a0\u00a0\u00a0\u00a0machineType:\u00a0n1-standard-4\n\u00a0\u00a0\u00a0\u00a0acceleratorType:\u00a0NVIDIA_TESLA_T4\n\u00a0\u00a0\u00a0\u00a0acceleratorCount:\u00a01\n\u00a0\u00a0replicaCount:\u00a01\n\u00a0\u00a0containerSpec:\n\u00a0\u00a0\u00a0\u00a0imageUri:\u00a0gcr.io\/vertexAIdemo\/vertexai-bert-training:latest\n\u00a0\n\u00a0\nI tried to increase quota and it cant be increased. I tried to contact several teams and it was to no help. Please provide me a way to how I can be able to increase the quota to execute the job.\n\u00a0\nPS: no vertex Ai Api quota is available to edit and is currently 0 for all regions be it CPU or GPU",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Translation models response time",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-04T05:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Translation-models-response-time\/m-p\/450442#M485",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":145,
        "Question_body":"Hi,\n\nWe have several AutoML Translation models and we are facing timeout issues when the first translation requests are sent. We have to retry a second time to get the translations back. After this first request, it seems the model is kept\u00a0 \"online\", and subsequent requests to the same model are performing well.\n\nWhat we don't really know is how long the models are kept online and ready for quick response times and how many models can be online simultaneously. We would like to have more information about this in order to handle the translation requests in a proper and controlled way.\n\nThank you,\n\nJulian",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vision API - Text detection training",
        "Question_tag_count":1,
        "Question_created_time":"2022-01-05T03:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-Text-detection-training\/m-p\/182002#M158",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":107,
        "Question_body":"Hi there,\n\nI'd like to train my Vision Project to improve document text detection for manuscript books. I couldn't find the solution anywhere. The current result is awful. The language is Portuguese.\n\nPlease advise.\u00a0Thanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to replace whitespace with an empty string in Dialog flow ES",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-19T05:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-replace-whitespace-with-an-empty-string-in-Dialog-flow-ES\/m-p\/545247#M1698",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":70,
        "Question_body":"Hi,\n\n\u00a0\n\nI would like to replace the whitespace with an empty string in Dialog flow ES.\n\nI tried with SUBSTIUTE and REPLACE method but no luck. Looks like these 2 options are for Dialog flow CX and not for ES.\n\n\u00a0\n\nLets say, If caller says, ab1234 then getting an output 'a b 1 2 3 4' instead of ab1234.\n\nI have configured the regular expression like this\u00a0^[a-zA-Z]{2}[0-9]*$\n\n\u00a0\n\nLooking out for an assistance.\n\n\u00a0\n\nRegards,\n\nSathish",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Parsing single digit in table with Form parser",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-18T20:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Parsing-single-digit-in-table-with-Form-parser\/m-p\/545112#M1692",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":91,
        "Question_body":"I am trying to scan the following image with the form parser (pretrained-form-parser-v2.0-2022-11-10)\u00a0\n\nbut it does not parse the single-digit number correctly. Any good idea to work around this issue? Can I expect this will be solved in the next coming version?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document Text Detection Google Vision API English Language Only",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-20T09:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-Text-Detection-Google-Vision-API-English-Language-Only\/m-p\/604918#M2196",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":150,
        "Question_body":"Hi All,\n\nI am using Google Vision API Document Text Detection for OCR to extract serial Numbers from an Image. The Serial Number contains only numbers and english alphabets. I have already set language Hint as English. However the extracted texts sometimes contains texts from other langugages such as \"BKO\" in Turkish or Cyrril language.\u00a0 The Code snippet is given below:\n\n\u00a0\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 success, encoded_image = cv2.imencode('.jpg', crop_img1)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 content = encoded_image.tobytes()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image = vision_v1.types.Image(content=content)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image_context = vision_v1.ImageContext(language_hints=[\"en\"])\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 status_label.config(text=\"Extracting Texts\")\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response = client.document_text_detection(image=image)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 texts = response.text_annotations\n\u00a0\nIs there a way I can tell Google Vision API that the text you are extracting is from English, You should not look for other languages.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI Training Errors - JSON and Schema not in agreement",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-07T12:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Training-Errors-JSON-and-Schema-not-in-agreement\/m-p\/519506#M1243",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":183,
        "Question_body":"Hello,\n\nI'm a novice user in the Document.AI world and while attempting to train a processor I've encountered the \"Training stopped due to errors\" message. When I investigate this error I observe sections of the JSON similar to:\n\n\u00a0\n\n              \"@type\": \"type.googleapis.com\/google.rpc.ErrorInfo\",\n              \"reason\": \"INVALID_DOCUMENT\",\n              \"domain\": \"documentai.googleapis.com\",\n              \"metadata\": {\n                \"num_fields\": \"0\",\n                \"annotation_name\": \"union\",\n                \"num_fields_needed\": \"1\",\n                \"field_name\": \"entities.text_anchor.text_segments\",\n                \"document\": \"b2c7cb53fbb0bd58.json\"\n              }\n\n\u00a0\n\nThe field union is set in the schema as \"optional once\" and so the metadata's report that 0 are found and 1 required seems off.\n\n\u00a0\n\nI understand there is a UI bug currently being investigated regarding these text_segments errors but it's unclear if I can work around this. I've been at a standstill for a week now, and shy of only identifying a single field per image it's not obvious to me what I'm doing on these particular records that is causing the error to appear.\n\nSome of my identified fields overlap, which someone suggested could be the cause, however, the handwriting does overflow the typical fields and sometimes overlaps, so guidance would be appreciated.\n\n\u00a0\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Java API to Document AI - Proxy-Configuration",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-20T00:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Java-API-to-Document-AI-Proxy-Configuration\/m-p\/512059#M1102",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":199,
        "Question_body":"Hello,\nmy question is: How can I configure a Proxy for the Document-AI-Client by using Java-Code, not using\u00a0Java-System-Property.\n\nI am using this code for creating the Document-AI-Client.\n\nDocumentProcessorServiceClient client;\nString endpoint = \"eu-documentai.googleapis.com:443\";\ntry (InputStream jsonCredentialsFile = getPropertyInputStream(getClass(), \"jsonCredentialsFile\")) {\n   GoogleCredentials credentials = GoogleCredentials.fromStream(jsonCredentialsFile)\n     .createScoped(Lists.newArrayList(\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"));\n   client = DocumentProcessorServiceClient.create(DocumentProcessorServiceSettings.newBuilder()\n     .setEndpoint(endpoint)\n     .setCredentialsProvider(FixedCredentialsProvider.create(credentials))\n     .build()\n   );\n}\nProcessRequest request = ProcessRequest.newBuilder()\n.setName(processorName)\n.setSkipHumanReview(true)\n.setFieldMask(FieldMask.newBuilder().build())\n.setRawDocument(rawDocument)\n.build();\n\n\/\/ Process document\nProcessResponse result = client.processDocument(request);\nDocument documentResponse = result.getDocument();\n\nIf I use Java-System-Property \"-D http.proxy .... \" Everything works fine.\n\nBut I have to access other Systems in the LAN by using e.g. Apache httpclient and other clients.\nThe global\u00a0Java-System-Property interferes with the other kinds of clients, because all of them are using the Java-System-Property. I cannot Mix \"Proxy\" and \"no Proxy\".\n\n\u00a0Of cause the other clients can be separately configured to use a Proxy or not.\nAnd the Document-AI-Client can use the\u00a0Java-System-Property.\nBut it would be much better if I could configure the Proxy for\u00a0the Document-AI-Client alike.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Failed to set data set source for tabular data in Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-17T05:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Failed-to-set-data-set-source-for-tabular-data-in-Vertex-AI\/m-p\/612883#M2382",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":37,
        "Question_body":"Hello,\n\nI am having issues in setting any csv as a tabular data source. I have tried a simple CSV (Housing.csv) downloaded from Kaggle, but I keep having this error\n\nFailed to set data set source\nThe attempted action failed. Please try again.\n\nI don't think it is a problem with permission as I have successfully created a dataset for images and trained a model.\n\nI have checked the CSV format and seems to be compliant with the recommendations\n\nprice,area,bedrooms,bathrooms,stories,mainroad,guestroom,basement,hotwaterheating,airconditioning,parking,prefarea,furnishingstatus\n13300000,7420,4,2,3,yes,no,no,no,yes,2,yes,furnished\n12250000,8960,4,4,4,yes,no,no,no,yes,3,no,furnished\n12250000,9960,3,2,2,yes,no,yes,no,no,2,yes,semi-furnished\n12215000,7500,4,2,2,yes,no,yes,no,yes,3,yes,furnished\n\nAny idea about what could be causing this?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Recommendations AI - catalog update not reflecting in console",
        "Question_tag_count":3,
        "Question_created_time":"2021-11-02T22:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Recommendations-AI-catalog-update-not-reflecting-in-console\/m-p\/174619#M73",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":356,
        "Question_body":"In Recommendations AI, I tried uploading the catalog from GCS. On uploading the catalog, I could see the catalog products showing up in the console but there is a warning notification stating the catalog is not integrated and the total product count is always zero.\n\nI have tried both recommended methods of uploading the catalog data from GCS directly through the GCP console and also through the CLI but still, this issue persists. I have followed every instruction provided in this GCP documentation but still, I couldn't figure out the actual issue and not much information is available in the public domain as well\u00a0\n\nCan someone help with this??\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I have asked for a 2 channel (2 voices podcast) transcription. But all i get is a monologue",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-24T02:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-have-asked-for-a-2-channel-2-voices-podcast-transcription-But\/m-p\/615036#M2439",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":18,
        "Question_body":"I have experimented with the local upload of podcast episodes to make transcriptions for blogging and easier editing.\n\nBut unlike other transcription tools I can't get google to recognise \/ tag the different speakers.\n\nAudio is in wav format 48k and very clear recording\u00a0\n\nIt recognises 2 channels on the upload but on the transcription it is just duplicated and all voiced mixed into the same monologue.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud translation API not working in \"en\" to \"en\" OR \"he\" to \"he\" OR \"he\" to \"iw\". same language to",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-11T03:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-translation-API-not-working-in-quot-en-quot-to-quot-en\/m-p\/508958#M1048",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":192,
        "Question_body":"Hello Google Team,\n\nI am stuck in same language translation\n\nEnvironment details\nOS: Linux\nPHP version: 7.2.34\n\nSteps to reproduce\njust pass the same language code in $source_language and $target_language (\"en\",\"en\")\n\nCode example\n$text = \"I Love google translation\";\n$tl = 'en';\n$sl = 'en';\n$project_id = 'MY_PROJECT_ID';\n$glossary_id = 'MY_GLOSSARY_ID';\n\nfunction v3_translate_text_with_glossary($text,$targetLanguage,$sourceLanguage,$projectId,$glossaryId)\n{\n$translationServiceClient = new TranslationServiceClient();\n\n$request[\"content\"] = $text;\n\ntry {\n\u00a0 \u00a0 $formattedParent = $translationServiceClient->locationName($projectId, 'global');\n\u00a0 \u00a0\n\u00a0 \u00a0 $response = $translationServiceClient->detectLanguage($formattedParent, $request);\n\u00a0 \u00a0 \/\/echo \"<pre>\";print_r($response->getLanguageCode());die;\n\u00a0 \u00a0 foreach ($response->getLanguages() as $language) {\n\u00a0 \u00a0 \u00a0 \u00a0 printf('getDetectedLanguageCode text: %s' . PHP_EOL, $language->getLanguageCode());\n\u00a0 \u00a0 }\n\u00a0 \u00a0\n}\nfinally {\n\u00a0 \u00a0 $translationServiceClient->close();\n}\n\n$glossaryPath = $translationServiceClient->glossaryName(\n\u00a0 \u00a0 $projectId,\n\u00a0 \u00a0 'us-central1',\n\u00a0 \u00a0 $glossaryId\n);\n$contents = [$text];\n$formattedParent = $translationServiceClient->locationName(\n\u00a0 \u00a0 $projectId,\n\u00a0 \u00a0 'us-central1'\n);\n$glossaryConfig = new TranslateTextGlossaryConfig();\n$glossaryConfig->setGlossary($glossaryPath);\n\n\/\/ Optional. Can be \"text\/plain\" or \"text\/html\".\n$mimeType = 'text\/plain';\n\ntry {\n\u00a0 \u00a0 $response = $translationServiceClient->translateText(\n\u00a0 \u00a0 \u00a0 \u00a0 $contents,\n\u00a0 \u00a0 \u00a0 \u00a0 $targetLanguage,\n\u00a0 \u00a0 \u00a0 \u00a0 $formattedParent,\n\u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'sourceLanguageCode' => $sourceLanguage,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'glossaryConfig' => $glossaryConfig,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'mimeType' => $mimeType\n\u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 );\n\u00a0 \u00a0 \/\/ Display the translation for each input text provided\n\u00a0 \u00a0 foreach ($response->getGlossaryTranslations() as $translation) {\n\u00a0 \u00a0 \u00a0 \u00a0 printf('Translated text: %s' . PHP_EOL, $translation->getTranslatedText());\n\u00a0 \u00a0 }\n} finally {\n\u00a0 \u00a0 $translationServiceClient->close();\n}\n}\n\nv3_translate_text_with_glossary($text, $tl, $sl, $project_id, $glossary_id);\n\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ ERROR RESPONCE \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nError: Call to a member function getTag() on null: { \"message\": \"Target language can't be equal to source language.\", \"code\": 3, \"status\": \"INVALID_ARGUMENT\", \"details\": [ { \"@type\": \"type.googleapis.com\/google.rpc.BadRequest\", \"fieldViolations\": [ { \"field\": \"source_language_code\", \"description\": \"Source language: iw\" }, { \"field\": \"target_language_code\", \"description\": \"Target language: he\" } ] } ] }\n\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ ERROR END \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nHow can I translate text in \"en\" to \"en\" OR \"he\" to \"he\" OR \"he\" to \"iw\"\n\nThanks\nAnkit",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What's the order for the labels in txt file after I have exported my tflite model from Vertex AI",
        "Question_tag_count":4,
        "Question_created_time":"2022-10-21T12:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-order-for-the-labels-in-txt-file-after-I-have\/m-p\/480804#M674",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":474,
        "Question_body":"I have exported my trained tflite model. But I noticed the order of the labels in the txt file matters. I'm using image classification models. The ones with only two labels, it's an easy fix. I just switch the two. But when I have more than two labels, I notice the predictions are way off. Does it say in Vertex AI or is there a general rule to what label should go first, second, third..etc in the txt file that we create on our own?",
        "Question_closed_time":"10-25-2022 03:27 PM",
        "Answer_score_count":0.0,
        "Answer_body":"After reviewing more about Export AutoML Edge models, you can see the following TensorFlow documentation to learn more about extracting this information.\n\nTensorFlow Lite inference with metadata\nGenerate model interfaces with TensorFlow Lite code generator\nAdding metadata to TensorFlow Lite models\n\nThe documentation that might help more for your question is the last one \u201cAdding metadata to TensorFlow Lite Models\u201d.\n\nBut what I can suggest to you is to send an email to tensorflow-enterprise-support@google.com with your question, and hopefully they can give you a direct solution to your concerns.\n\nAdditionally, I found this Stack Overflow question to create labels.txt manually.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Translate service error - Unsupported language pair",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-25T04:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translate-service-error-Unsupported-language-pair\/m-p\/459774#M531",
        "Question_answer_count":14,
        "Question_score_count":3,
        "Question_view_count":764,
        "Question_body":"Our application started to have some strange error from 25th of August which was working properly until today. Some very basic translation requests get the \"Status(StatusCode=\"InvalidArgument\", Detail=\"Unsupported language pair.\" error. For example the words \"loan\", \"excellent\", \"wonderful\" get the errors from service. I checked the release notes of the service but found nothing. Could you please help about the issue?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI deploy custom model error - Model server terminated: model server container terminated:",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-18T05:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-deploy-custom-model-error-Model-server-terminated\/m-p\/490796#M811",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":550,
        "Question_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to deploy custom model to vertex-ai endpoint.\n\nCommand:\n\n\u00a0\n\ngcloud ai endpoints deploy-model {ENDPOINT_ID}\\\n  --region={REGION} \\\n  --model={MODEL_ID} \\\n  --display-name={DEPLOYED_MODEL_NAME} \\\n  --machine-type=n1-standard-2 \\\n  --enable-access-logging \\\n  --enable-container-logging \\\n  --min-replica-count=1 \\\n  --max-replica-count=100 \\\n  --traffic-split=0=100\n\n\u00a0\n\nError:\nUsing endpoint [https:\/\/europe-west3-aiplatform.googleapis.com\/]\nERROR: (gcloud.beta.ai.endpoints.deploy-model) Model server terminated: model server container terminated: exit_code: 0\nreason: \"Completed\"\nstarted_at { seconds: 1668599448 }\nfinished_at { seconds: 1668599448 } .\n\nThe same error appears if I try to deploy from UI.\nNote: I was able to import my custom model, and also I can create a new endpoint, but if I try to deploy model or make batch prediction this error appears.\u00a0Where am I wrong?\n\nPlease help me",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Matching engine not accepting Batch Prediction embeddings and generating error",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-09T05:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Matching-engine-not-accepting-Batch-Prediction-embeddings-and\/m-p\/610409#M2325",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"I am attempting to build a question answering bot using a model from TensorFlow Hub, specifically the model located at https:\/\/tfhub.dev\/google\/universal-sentence-encoder-qa\/3. I have successfully uploaded the model to the model registry and performed batch predictions, resulting in two text\/plain files as output. I am now working on creating a matching index using the DOT_PRODUCT method. Below is the code I am using for this task.\n\nBut getting errorr as\u00a0\n\nShow More\n\nHow do I resolve this error?, Do I need to convert the predictions to json format?.\n\nIts very urgent for me, if anyone can help please let us know",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Translation language support for bcp-47",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-20T20:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translation-language-support-for-bcp-47\/m-p\/445359#M439",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":139,
        "Question_body":"Google Speech to Text supports languages using bcp 47 codes like es-MX for mexican spanish and pt-BR for Brazilian Portugese.\n\nI am using transcription and translation in a pipeline.\n\nIs there any support for bcp 47 languages in Google Cloud Translation.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Mount gcsfuse in gcloud ai custom-jobs local-run",
        "Question_tag_count":2,
        "Question_created_time":"2022-09-28T00:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Mount-gcsfuse-in-gcloud-ai-custom-jobs-local-run\/m-p\/471834#M609",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":282,
        "Question_body":"When locally testing my custom-job through \"gcloud ai custom-jobs local-run\" command, I would like to have access to a bucket mounted though gcsFuse as it happens when I launch the same containerized job from GCloud console. Is there the option to have the same access locally?\n\nThank you for helping",
        "Question_closed_time":"09-30-2022 08:43 AM",
        "Answer_score_count":1.0,
        "Answer_body":"What you could do is use cloud storage as a file system within ai training, since while using fuse your training jobs on both of the platforms can access your data that is stored on Cloud Storage as files on your local file system, also the documentation I shared provides you useful information as the problems you might encounter, permissions, a brief description of cloud storage fuse, performance related information, the restrictions this method has and also how you can make use of the logs.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vision API quota\/budget limit and API key help",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-01T09:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-quota-budget-limit-and-API-key-help\/m-p\/409566#M253",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":123,
        "Question_body":"Hello, I have never used Vision API before but I recently found it very powerful for a project of mine. However I have two concerns regarding its budget limiting, in order to not get an unexpected bill:\n\nIs it possible to set a monthly cost limit? I have been used to Compute Engine which gives me an almost exact cost of the month but this seems not possible here. Since I will be using the API for labelling I have set the label detections requests per minute and per user to a specific amount, also to be sure I have set the global request per minute and per user to the same amount, all the other quotas to 0. If I have understood correctly, setting the max calls quota per minute to 4, for example, should provide a maximum of 178560 calls per month, right?\u00a0Should this limit my budget? Am I safe?\n\nThe API will be used as an API key in a mobile app. I have followed the code examples for iOS & Android and I have seen the key is written in the code. Is this safe? For a better security I have restricted the key to iOS\/Android apps bundle and to Cloud Vision API only. Would it be a safe enough option?\n\nThanks everyone for any help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How does it work underhood: Predictions of multiple instances (Batch) to Vertex AI online serving",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-31T21:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-does-it-work-underhood-Predictions-of-multiple-instances\/m-p\/462022#M554",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"Hello,\n\nVertex AI online serving:\n\nWhen multiple instances are passed for prediction to an endpoint, Does prepackaged container serve the inferences in the same manner as TFX Serving does with enable_batching.\u00a0 If so how do we optimize batching parameters with multiple instances sent to Vertex AI online.\n\nIf multi_instances prediction is different from TFX serving batching, how do we gain GPU resources efficient usage optimization with prepackaged serving container.\n\nOn a general note, how to handle efficient GPU usage for both prepackaged container and custom container using a custom trained model.\n\nPlease guide.\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI demo error",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-12T13:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-demo-error\/m-p\/602355#M2137",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"trying to run a simple classification tutorial using Google supplied dataset and getting this error\u00a0\n\nThe DAG failed because some tasks failed. The failed tasks are: [exit-handler-1].; Job (project_id = grand-pact-389317, job_id = 7431764568637964288) is failed due to the above error.; Failed to handle the job: {project_number = 829431017637, job_id = 7431764568637964288}\u00a0\n\nTried 3 times, thoughts?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Translation issue",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-28T08:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translation-issue\/m-p\/483154#M700",
        "Question_answer_count":6,
        "Question_score_count":1,
        "Question_view_count":760,
        "Question_body":"Hello Team,\n\nI am trying to document translation using Google cloud translate V3.\n\nI found some issue in below-\n\n1. Text Overlapping from German to English\n\n2.Some text position was not correct\n\n3.table column name show in bottom of pages.\n\n4.Some pages were not being Translate.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Translate API - Your client does not have permission to get URL \/language\/translate\/v2",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-17T03:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API-Your-client-does-not-have-permission\/m-p\/533662#M1436",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":200,
        "Question_body":"I'm trying to make a request to google cloud translate:\n\n\u00a0\n\n$res = $this->request(\n            'POST',\n            'https:\/\/www.googleapis.com\/language\/translate\/v2?'.$queryUrl,\n            $params,\n            array(\"X-HTTP-Method-Override: GET\")\n        );\n\n\u00a0\n\nHowever, I'm getting a response that:\n\n\n\n\u00a0\n\nThat\u2019s an error. \nYour client does not have permission to get URL \/language\/translate\/v2 from this server.\n That\u2019s all we know.\n\n\u00a0\n\n\nThe same code on a different server works without problems. I didn't setup any restrictions for api key:\n\n\nWhat could be the possible causes for this ? Can the request be blocked by the server and how should I check it ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issue with using system functions in parameter presets",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-25T06:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-with-using-system-functions-in-parameter-presets\/m-p\/513851#M1135",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":208,
        "Question_body":"Hi there,\n\nI'm trying to dynamically append values to a list parameter using the 'Parameter Preset' box when certain conditions are met for a transitional route.\u00a0\n\nThe list parameter is defined as follows:\u00a0\n\nThen, during the route I use the following system function:\n\n\u00a0\n\n$sys.func.APPEND($session.params.negative_products, credit score)\n\n\u00a0\n\n\u2003However, when the condition is met during the conversation, the updated value in $session.params.negative_products is not: [\" \", credit score], but prints out the whole text in the parameter preset box, e.g.,\u00a0$sys.func.APPEND($session.params.negative_products, credit score).\u00a0\n\nWhy is this the case? I thought system functions were able to be used to dynamically change values in parameter presets and I have no idea why it just keeps on printing out the function as a string!\u00a0\n\nAny help would be much appreciated,\n\nVicky",
        "Question_closed_time":"01-27-2023 02:03 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hey!\n\nWhen referencing the parameter in my fulfilment I use:\u00a0\n\n$session.params.negative_products\n\n\u00a0I did figure a workaround yesterday. I realised when I define $session.params.negative_products within the 'Entry Fulfilment', 'Parameter Presets' section of the page the appending works:\u00a0\n\n1. Define empty list parameter in presets of the Entry Fulfilment, i.e., before I want to append to the list:\u00a0\n\n\u20032. During a conditional route, append to the empty list using system functions in the parameter presets with the following command:\u00a0\n\n$sys.func.APPEND($session.params.negative_products, \"credit score\")\n\n3. This then prints out the correct output when referencing $session.params.negative_products:\u00a0\n\n\" , credit score\"\n\nI think my original issue was that when trying to define $session.params.negative_products as a 'isList' parameter, it wasn't actually generating a list per se. So, when I was trying to call it in $sys.func.APPEND(), the function wasn't reading the $session.params.negative_products as a list, and as a result, just printed out the system command.\u00a0\n\nHope this helps other users who get faced with a similar issue!\n\nVicky\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Tenserflow model not detecting plants correctly",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-27T05:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Tenserflow-model-not-detecting-plants-correctly\/m-p\/447276#M449",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":96,
        "Question_body":"Hi there,\n\nWe are using AutoML skd with Tenserflow model (https:\/\/tfhub.dev\/google\/lite-model\/aiy\/vision\/classifier\/plants_V1\/3) for detecting plants. The model return results, but they are not accurate. I wanted to see if there are any pre-trained TenserFlow models for detecting plant type? Similar to plant.id.\u00a0\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"best practice for document AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-02T04:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/best-practice-for-document-AI\/m-p\/539911#M1561",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":105,
        "Question_body":"while defining label locations,\u00a0\n\nis it better to strictly select the desired data, or is it more beneficial to the AI if you select the desired data, as well as its surrounding data, and then delete the surrounding data in the selection text box?\n\nto be clear i only want the name field,\n\nbut wondering whether selecting the entire address and prior headline text and then removing it from in the left text box field is better for learning?\n\nsee pics for clarification",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vision API Product search",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-02T10:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-Product-search\/m-p\/485081#M718",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":343,
        "Question_body":"Following is vision API product search request json\n\n{\n \"requests\": [\n{\n  \"image\": {\n    \"content\": base64-encoded-image\n  },\n  \"features\": [\n    {\n      \"type\": \"PRODUCT_SEARCH\",\n      \"maxResults\": 5\n    }\n  ],\n  \"imageContext\": {\n    \"productSearchParams\": {\n      \"productSet\": \"projects\/project-id\/locations\/location-id\/productSets\/product-set-id\",\n      *\"productCategories\": [\n           \"apparel\"\n      ]*,\n      \"filter\": \"style = womens\"\n    }\n  }\n}\n]\n}\n\nFor ImageContext, ProductCategories(apparel in this request) is mandatory in API. My concern is if I want product search from all the available\u00a0ProductCategories, do I need to set multiple requests?\n\n ImageContext imageContext =\n    ImageContext.newBuilder()\n        .setProductSearchParams(\n            ProductSearchParams.newBuilder()\n                .setProductSet(productSetPath)\n                .addProductCategories(\"apparel-v2\")\n                .setFilter(filter))\n        .build();\n\nFor example, addProductCategories(\"apparel\") can have only one productcategory at a time. But I want product search from all the category something like addProductCategories(\"apparel-v2\").addProductCategories(\"toys-v2\").addProductCategories(\"general-v1\") etc.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Generative AI Read File",
        "Question_tag_count":4,
        "Question_created_time":"2023-07-07T02:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Generative-AI-Read-File\/m-p\/609978#M2318",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":77,
        "Question_body":"Hi Team,\n\nI am looking for steps to create a generative AI mode that reads the file and prompt Q&A example.\nBasically, I am trying for chatbot that answers based on the input file contents.\n\nKindly help.\n\nThanks,\n\nRajavelu",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"The kernel for MyTest.ipynb appears to have died. It will restart automatically.",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-24T11:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-kernel-for-MyTest-ipynb-appears-to-have-died-It-will-restart\/m-p\/492715#M860",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":273,
        "Question_body":"Hello,\n\nI'm trying to run a test jupyter notebook of a LSTM model running tensorflow. I have tried setting the GPU memory limit like suggested here. But still the I get the error mentioned above. I can not find anything realted to GC vertex AI and everyone suggest setting the gpu memory in case of such errors.\u00a0\n\nFor reference I have tried to run this\u00a0as well on my Vertex AI jupyter lab and it crashes as well. The only thing I added was this:\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\ntf.config.set_logical_device_configuration(\ngpus[0],\n[tf.config.LogicalDeviceConfiguration(memory_limit=12288)]\n)\n\nlogical_gpus = tf.config.list_logical_devices('GPU')\nprint(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n\nOn my personal computer it runs just fine, but it would take 13 hours to train which is not a option for me at the moment.\n\nAny help would be appriciated.\u00a0\n\nBarnabas.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Scheduling Vertex AI Pipeline - Error 503",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-10T11:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Scheduling-Vertex-AI-Pipeline-Error-503\/m-p\/552124#M1849",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":222,
        "Question_body":"Hi,\n\nI successfully trained and deployed a pipeline in Vertex AI using Kubeflow for a retrieval model, Two Towers.\n\nNow I want to schedule this pipeline run every 8 minutes. Here's my code:\n\n\u00a0\n\nfrom kfp.v2.google.client import AIPlatformClient\napi_client = AIPlatformClient(project_id='my-project', region='us-central1')\n\napi_client.create_schedule_from_job_spec(\n    job_spec_path='vacantes_pipeline.json',\n    schedule=\"\/8 * * * *\", # every 8 minutes\n    time_zone='America\/Sao_Paulo',\n    parameter_values={\n        \"epochs_\": 5,\n    \"embed_length\":768,  \n        \"maxsplit_\" : 130\n    }\n)\n\n\u00a0\n\nThe JSON is successfuly created, but the Scheduler Job fails immediately.\n\nLogging tells me the httpRequest has an error 503 plus:\n\n\u00a0\n\njsonPayload: {\n@type: \"type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished\"\njobName: \"projects\/my-project\/locations\/us-central1\/jobs\/pipeline_vacantes-pipeline-with-deployment_c7e98a8f_59-14-a-a-a\"\nstatus: \"UNAVAILABLE\"\ntargetType: \"HTTP\"\nurl: \"https:\/\/us-central1-bogotatrabaja.cloudfunctions.net\/templated_http_request-v1\"\n}\n\n\u00a0\n\nAny ideas on how to solve this issue ?",
        "Question_closed_time":"05-13-2023 09:52 AM",
        "Answer_score_count":0.0,
        "Answer_body":"I solved with Compute Engine and cron jobs.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Vertex AI training pricing",
        "Question_tag_count":2,
        "Question_created_time":"2022-03-09T22:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-training-pricing\/m-p\/402031#M229",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":779,
        "Question_body":"I recently tried out Vertex AI and used AutoML to train my image classification model. I did train 3 sets and noticed from the billing that i was billed for 24 node hours. Is it so that i will be billed for the 8 node hours per model regardless if the training takes only one hour?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Use GCP Endpoints as reverse proxy for Vertex Ai Endpoint",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-24T08:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Use-GCP-Endpoints-as-reverse-proxy-for-Vertex-Ai-Endpoint\/m-p\/396912#M209",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":728,
        "Question_body":"I am using GCP Endpoints to work as a reverse proxy to a Vertex Ai Endpoint. I can authenticate to GCP Endpoints with api keys, service account... but I get the following error code. Yet, am able to get a successful response from Vertex Ai Endpoint directly.\n\n\u00a0\n\n# Error code when requesting to GCP Endpoints (API is authenticated)\n\n\u00a0\n\n{\n  \"error\": {\n    \"code\": 401,\n    \"message\": \"Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https:\/\/developers.google.com\/identity\/sign-in\/web\/devconsole-project.\",\n    \"status\": \"UNAUTHENTICATED\"\n  }\n}\n\n\u00a0\n\n\u00a0\n\nEven using the flag \"--allow-unauthenticated\" when setting up ESPv2 still fails.\n\n\u00a0\n\nThe request\n\n\u00a0\n\ncurl -X POST -H \"content-type:application\/json\" \"${HOST}?key=${API_KEY}\" -vL -d \"@${DATA_FILE}\"\n\n\u00a0\n\n\u00a0\n\nopenapi.json (host and address removed for privacy)\n\n\u00a0\n\nswagger: '2.0'\ninfo:\n  title: Cloud Endpoints + Cloud Run\n  description: Sample API on Cloud Endpoints with a Cloud Run backend\n  version: 1.0.0\nhost: HOST\nschemes:\n  - https\nconsumes:\n  - application\/json\n  - multipart\/form-data\nproduces:\n  - application\/json\nx-google-backend:\n  address: VERTEX_AI_ENDPOINT\n  protocol: h2\nsecurityDefinitions:\n  # This section configures basic authentication with an API key.\n  api_key:\n    type: \"apiKey\"\n    name: \"key\"\n    in: \"query\"\npaths:\n  \/:\n    post:\n      summary: Greet a user\n      security:\n        - api_key: []\n      operationId: hello\n      responses:\n        '200':\n          description: A successful response\n          schema:\n            type: string\n\n\u00a0\n\n\u00a0\n\nAny help would be greatly appreciated",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"text-bison@001 IAM_PERMISSION_DENIED",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-12T04:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/text-bison-001-IAM-PERMISSION-DENIED\/m-p\/602198#M2133",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":300,
        "Question_body":"Hello,\n\nI am currently facing an issue with calling the text-bison@001 API in my App Engine deployment. While the API works perfectly fine on my local machine, I encounter the following error message when calling the API from the deployed app:\n\n```\n403 Permission 'aiplatform.endpoints.predict' denied on resource '\/\/aiplatform.googleapis.com\/projects\/tr-media-analysis\/locations\/us-central1\/publishers\/google\/models\/text-bison@001' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"]\n```\n\nI have already checked the Service Account associated with the App Engine, and it has been granted the necessary AI Platform Developer rights.\n\nI would greatly appreciate any insights or suggestions on how to resolve this problem. Are there any additional permissions or configurations that I might be missing?\n\nThank you in advance for your assistance!\n\nBest regards,\nLudovico",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Join us on August 4! Machine Learning Day on Google Open Source Live",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-28T06:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Join-us-on-August-4-Machine-Learning-Day-on-Google-Open-Source\/m-p\/447714#M454",
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":106,
        "Question_body":"This month\u2019s Google Open Source Live will showcase\u00a0Tools for Responsible Machine Learning\u00a0along with multiple sessions presented by Machine Learning team members and the community. Join us to learn\u00a0TensorFlow Hub: Machine Learning Models for Everyone,\u00a0Introduction to On-device Machine Learning, as well as\u00a0Using Job Queueing to Optimize for Efficiency of Your Compute Resources.\n\n\u00a0\nRegister Now\n\u00a0\nReasons why you should attend this virtual event LIVE: \u00a0\nSelected questions will be answered by our speakers in real time! The Live Q&A Forum will be open during the event from 9:00 am to 10:07 am PT.\u00a0\nJoin in on the after party fun, where you can participate in an exciting quiz, and hear from our speakers and emcees immediately following the event!\u00a0\nWe look forward to seeing you all at Machine Learning Day on Google Open Source Live!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Need help for compute engine pricing",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-06T05:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Need-help-for-compute-engine-pricing\/m-p\/463295#M562",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"GPU:\u00a0nvidia-a100-80gb has no pricing but\u00a0\u00a0nvidia-tesla-a100 has",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI video action recognition - can it return action timeframes instead of a timestamps?",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-20T13:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-video-action-recognition-can-it-return-action\/m-p\/491200#M824",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":213,
        "Question_body":"My problem is that my usecase requires the AI engine I use to provide predictions with the entire duration of the action. It seems to me that vertex AI picks a random frame in the span of the action and return it as the same start\/end values. Here's an excerpt from an actual response\n\n\u00a0\n\nCan I make it work the way I need it to? Maybe I'm annotating in a wrong manner?\n\nHere's a mockup of what I need. Notice how timeSegmentStart and timeSegmentEnd represent a duration now:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Batch Predictions: Bigquery format must be used as input and output simultaneously",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-01T15:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Batch-Predictions-Bigquery-format-must-be-used-as\/m-p\/484734#M713",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":245,
        "Question_body":"I'm encountering an error when I try to create a batch prediction job with a bigquery table as my input, and a JSONL output in a GCS bucket. The documentation\u00a0for batch predictions seems to indicate that I can do so, but I still see an error.\n\nI'm trying to create a batch prediction job on the Vertex AI console, and I see this error.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Custom container image not found by Vertex AI for model upload",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-16T07:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-container-image-not-found-by-Vertex-AI-for-model-upload\/m-p\/467487#M582",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":183,
        "Question_body":"Hi,\n\n1. I have pushed a custom container to gcr.io\/<project id>\/reponame\/imagename:latest\nfrom gcloud cli on local WSL + podman.\n\n2. Then from google console\\ vertex ai\\ model registry, i tried to import the custom container as new model and new version. I'm able to browse to the container image and select the image URI.\n\n3. Then once I click finish, i get error container Image not found.\nSame is the error with gcloud\u00a0 ai models upload command executed from the notebook.\n\nPlease suggest, how to debug the issue and identify root cause.\n\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Feature Store MLOps Pipeline",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-04T02:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-MLOps-Pipeline\/m-p\/528764#M1375",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":112,
        "Question_body":"Hey,\n\ni'm trying to understand the MLOps Pipeline with the CI\/CD-Automation (Stage 2 Maturity Level) and struggle with the Feature Store as the component feeding the Automated Pipeline with data. What i found out in the internet was, that Feature Stores extract data from different sources, transform them and create training data which can be used to train the model (retraining with new data). But in the pipeline the steps like Data preperation and Data extraction come after the Feature Store.\n\nCan somebody explain to me, whats the output of the Feature Store and how it is used to serve the data for the Automated Pipeline and the Prediction Service?\n\nThanks in advance",
        "Question_closed_time":"03-06-2023 02:08 PM",
        "Answer_score_count":0.0,
        "Answer_body":"The Feature Store is just a centralized repository of features. By that, its output is just a set of features typically used to train an ML model. Depending on your specific needs, you can serve the ingested data in the Feature Store to the model right away (in what is called feature serving) or export feature values and do further preparation of data.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Does vertex ai support simultaneous ingestion (batch) from multiple sources?",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-04T21:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-vertex-ai-support-simultaneous-ingestion-batch-from\/m-p\/518165#M1206",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":85,
        "Question_body":"I read here that it supports ingestion from BigQuery and GCS, however, I was wondering if it supports simultaneous ingestions from various sources",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX CLI",
        "Question_tag_count":3,
        "Question_created_time":"2023-02-19T11:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-CLI\/m-p\/523858#M1297",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":124,
        "Question_body":"hello!! I\u2019ve been building an open-source project for several months and I think is ready to go! It is a CLI to interact with Dialogflow CX, its NLU and has quite useful testing capabilities within the conversational AI field. It also has possibilities of Speech-to-text and Text-to-speech\u00a0\u00a0https:\/\/cxcli.xavidop.me\/\nI have written it in Go (quite a cool experience, really) Tell me what you think of it please, if you can give the repo a star, it would also be great:\u00a0https:\/\/github.com\/xavidop\/dialogflow-cx-cli\n\nToday I just released the biggest version since I created the Dialogflow CX CLI!\nthis version includes:\n1. Create and update an agent\n2. Export and Restore an agent with the `json-package` format\n3. Create, update and delete webhooks\n4. Create, update and delete versions (under the versioning command)\n5. Create, update and delete environments\n6. Update intents. Create and update the description of an entity\n7. Update entity types. Create and update redacted entity types using `--redacted` parameter\n\n8. Create, Update, Delete and train flows!\n\nPlease read the usage here: https:\/\/cxcli.xavidop.me\/cmd\/cxcli\/\n\nYou can support the project by starring or sponsoring the project: https:\/\/github.com\/xavidop\/dialogflow-cx-cli",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI\/ML",
        "Question_tag_count":14,
        "Question_created_time":"2021-06-18T13:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-ML\/m-p\/31#M1",
        "Question_answer_count":0,
        "Question_score_count":10,
        "Question_view_count":572,
        "Question_body":"This is the discussion space to talk about all things AI\/ML related.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"OCR Detect Text .Net does not return score",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-18T08:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/OCR-Detect-Text-Net-does-not-return-score\/m-p\/544923#M1702",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":95,
        "Question_body":"Hello All,\n\nI am trying to use the c# .Net nuget package to run OCR on images. I can get it to work fine and process images and returns data by following the examples in here:\u00a0https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.Vision.V1\/latest#detect-text-in-a-single...\u00a0but I don't seem to be able to retrieve information on the \"Score\" of the text read. Both confidence and score are always 0 for all text returned. I am using the \"DetectText\" method specifically and also tried to set the parameter \"EnableTextDetectionConfidenceScore\" to true and pass it via ImageContext, but still no success. Would anybody be able to help or advice what can be investigated further please?\n\nThis is my first experience with Google cloud in general so apologies if I am posting in the wrong channel, please advice otherwise if this is the case.\n\nKind Regards,\n\nSilvio",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Time to train a new model in Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-12T14:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Time-to-train-a-new-model-in-Vertex-AI\/m-p\/611730#M2348",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":64,
        "Question_body":"Hello community,\u00a0\n\nI'm trying to train my first model to classify images with Vertex AI. I set 8 hours as the budget for maximum node hours but the process run for 18 hours and I wasn't sure if I was going to be billed for the 18 hours so I canceled the training. The \"Enable early stopping\" was enabled.\u00a0\nIn addition, I used 13 images by label but I read that the recommended quantity is 100. I guess that the quantity of images might impact the training time.\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Detailed Vision API usage report",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-10T02:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Detailed-Vision-API-usage-report\/m-p\/520704#M1262",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":58,
        "Question_body":"Hello,\n\nWe would need help to get list of Vision API hits with source IPs as there are no standard reports available for the same GCP console\n\nIf some one can share details or ways to extract those data, it will be great help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI endpoint deployment",
        "Question_tag_count":4,
        "Question_created_time":"2022-11-04T04:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-endpoint-deployment\/m-p\/485783#M734",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"How can I utilize the mega GPU during endpoint deployment\u00a0 for vertex ai work? Are there any model for examples or other resources that I can use to better grasp this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX incorrect parsing of currency in millions e.g. 1million",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-02T08:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-incorrect-parsing-of-currency-in-millions-e-g\/m-p\/549494#M1775",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":82,
        "Question_body":"Hi all\n\nAfter some extensive testing and trying workarounds, thought I'd post here to mention this issue and see if there's a solution.\n\nWhen capturing a currency amount in the millions, the wrong values are picked up when the user says something like \"1million\" or \"5million\"\n\ne.g.\n\nI want to pay \"2million\" results in this:\n\n\"original\": \"million\",\n\"currency\": \"USD\",\n\"amount\": 1000000\n\nor when I have the currency set to list, it ends up with this:\n\n{\n\"currency\": \"USD\",\n\"amount\": 2,\n\"original\": \"2\"\n},\n{\n\"currency\": \"USD\",\n\"amount\": 1000000,\n\"original\": \"million\"\n}\n\nHowever, adding a space e.g. \"1 million\" gets the right value captured.\n\nI've tried\u00a0 a few ways to workaround with composite entities, and annotating training phrases but couldn't get it work without custom code.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deep Learning VM Config to connect to Google Colab",
        "Question_tag_count":2,
        "Question_created_time":"2022-05-19T10:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Learning-VM-Config-to-connect-to-Google-Colab\/m-p\/424603#M354",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":645,
        "Question_body":"Morning to all\n\nIm trying to connect a google colab file to a Google Deep Learning VM with any results. My guess is that I need to configure something inside the VM or the google console but not sure how to do so.\n\nI get the error that you will find in image 1 that says:\n\n\"The VM requested does not exist. Check out our guide to set up GCE VMs in Colab\"\n\nand has a the next link in which theres not much info on how to solve the situation:\u00a0 https:\/\/research.google.com\/colaboratory\/marketplace.html\n\nOn image 2 and 3 you will find the info that I add to the colab file that is the same as the VM configuration that you will fins on image 3.\n\nWhat I\u00b4m doing wrong? Do I need to asing special permits to the VM?\u00a0\n\nAny comments or advice is more than appreciated\n\nImage 1\n\nImage 2\nImage 3",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Translate API & Referer Restriction Issue",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-31T17:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API-amp-Referer-Restriction-Issue\/m-p\/181705#M148",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hello Dear Community !\n\nI have a frustrating issue with the Google Cloud Translate API.\n\nI set up correctly the restriction of the API key to some domains including *.example.com\/*\u00a0\n\nI launch the script on the URL\u00a0https:\/\/www.example.com\/translate\u00a0and i have the following message :\n\n\u00a0\n\n\u00a0\n\nMessage: {\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Requests from referer \\u003cempty\\u003e are blocked.\",\n    \"errors\": [\n      {\n        \"message\": \"Requests from referer \\u003cempty\\u003e are blocked.\",\n        \"domain\": \"global\",\n        \"reason\": \"forbidden\"\n      }\n    ],\n    \"status\": \"PERMISSION_DENIED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com\/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_HTTP_REFERRER_BLOCKED\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"consumer\": \"projects\/464292577200\",\n          \"service\": \"translate.googleapis.com\"\n        }\n      }\n    ]\n  }\n}\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nWhen i remove the restriction, everything works, but i need the restriction to avoid misuse\/abuse.\nI tried to change the restriction to *.example.com, www.example.com, example.com\/*, even with the dedicated URL, but nothing works, always the same error message.\n\nDo you have any ideas or any ways to investigate better this issue ? How i can know the referrer Google get when i launch my request ?\u00a0\n\nIt's driving me crazy !\n\nThanks a lot and happy new year to everybody !!!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Submit Job But Never Ran",
        "Question_tag_count":3,
        "Question_created_time":"2023-05-19T10:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Submit-Job-But-Never-Ran\/m-p\/554999#M1963",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":82,
        "Question_body":"I am trying to test a bash script on the cloud, but when I submit it, it repeatedly switch between SCHEDULE and QUEUE and never ran. Here is the bash script that I am trying to run:\u00a0\n\n\u00a0\n\n#!\/bin\/bash\n\n\nsource \/opt\/conda\/etc\/profile.d\/conda.sh\nconda activate WaifuDiffusion\n\nexport FOLDER=\/home\/kannachan\/SeaSalt-Downloader\n\npython $FOLDER\/main.py -u \"https:\/\/danbooru.donmai.us\/posts?tags=mika_pikazo\" \\\n       --scraper danbooru \\\n       --filter tag_filter animated video \\\n       --saver folder mika_pikazo       \n\n\u00a0\n\n\nI config my job as follows:\n\n\u00a0\n\n{\n    \"taskGroups\": [\n        {\n            \"taskSpec\": {\n                \"runnables\": [\n                    {\n                       \"script\": {\n\t\t\t      \"text\": \"#!\/bin.bash\\n bash ..\/run.sh\" \n\t\t\t}\n                    }\n                ],\n                \"computeResource\": {\n                    \"cpuMilli\": 2000,\n                    \"memoryMib\": 16\n                },\n                \"maxRetryCount\": 2,\n                \"maxRunDuration\": \"3600s\"\n            },\n            \"taskCount\": 4,\n            \"parallelism\": 2\n        }\n    ],\n    \"allocationPolicy\": {\n        \"instances\": [\n            {\n                \"policy\": { \"machineType\": \"g2-standard-8\" }\n            }\n        ]\n    },\n    \"labels\": {\n        \"department\": \"ai\",\n        \"env\": \"web-scrapper\"\n    },\n    \"logsPolicy\": {\n        \"destination\": \"CLOUD_LOGGING\"\n    }\n}\n\n\u00a0\n\n\u00a0\n\nBoth run.sh and config.json is in the same directory. I ran:\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json\n\n\u00a0\n\ngcloud batch jobs submit batch-job-1 --location us-central1 --config config.json",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Building what's next with Generative AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-03T15:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Building-what-s-next-with-Generative-AI\/m-p\/550086#M1780",
        "Question_answer_count":5,
        "Question_score_count":3,
        "Question_view_count":283,
        "Question_body":"Join our Innovator Champions that are included in our Gen AI Trusted Tester program to discuss their excitement of Gen AI, predictions, and what they\u2019ve been building in pre-release Gen AI in Vertex AI.\n\nEvent information:\n\nAdd to your Google Calendar\nJoin live at 11AM PT",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Month-to-date total cost (VertexA\u0131) keeps increasing even though I delete all my projects and billin",
        "Question_tag_count":3,
        "Question_created_time":"2022-11-11T01:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Month-to-date-total-cost-VertexA%C4%B1-keeps-increasing-even-though-I\/m-p\/488385#M764",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":105,
        "Question_body":"Hello. I am a student and only use GCP for learning. But even though there is no project in my profile and\nI have closed my billing account, the Month-to-date total cost (VertexA\u0131) is constantly increasing. \nI tried everything but couldn't find a solution. I would be very happy if you could help me what to do.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Order of entries in BatchAnnotateImagesResponse",
        "Question_tag_count":1,
        "Question_created_time":"2022-01-11T01:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Order-of-entries-in-BatchAnnotateImagesResponse\/m-p\/182949#M169",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":128,
        "Question_body":"Given a list of image urls I want to annotate each image, i.e. extract text from each image. For that, I want to use Google Cloud Vision API client library in Java. Here is my pseudocode:\n\nList<String> imageUrls = ...;\nList<AnnotateImageRequest> requests = imageUrls.stream()\n    .map(convertToRequest)\n    .collect(Collectors::toList);\nBatchAnnotateImagesResponse batchResponse = imageAnnotatorClient.batchAnnotateImages(requests);\n\n\n\nNow from batchResponse I can get a list of AnnotateImageResponse. The questions are, does the number of AnnotateImageResponse correspond to the number of requests? Does the order of responses correspond to the order of requests? Can I safely assume that by doing so\n\nfor (int i = 0 ; i < imageUrls.size(); i++) {\n    var url = imageUrls.get(i);\n    var annotations = batchResponse.getResponses(i).getTextAnnotationsList();\n}\n\nI will get annotations for the right image on each iteration of the for loop? This is something that is not clear to me from the documentation.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google translator is free or has any kind of pricing?",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-05T23:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-translator-is-free-or-has-any-kind-of-pricing\/m-p\/463225#M561",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"I'm using\u00a0this code\u00a0for translating my website in my angular project. I'm not using translate API provided by google cloud. So, I just need to confirm that the source I'm using is paid for publicly available (free)?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Imbalance DataSet for Tabular AutoML",
        "Question_tag_count":3,
        "Question_created_time":"2022-04-18T10:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Imbalance-DataSet-for-Tabular-AutoML\/m-p\/414630#M278",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":379,
        "Question_body":"Hi, I would like to know if in case of having a tabular database,\u00a0 with binary data (class 0 and Class 1), that has an imbalance between class 0 and class 1, as it occurs in scenarios of fraud in financial transactions.\n\nDoes AutoML solves automatically the imbalance situation? Or is it possible to add SMOTE or ADASYN to the AutoML model?\u00a0 Any comments to advice more than appreciated",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"The new languages are missing",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-17T01:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-new-languages-are-missing\/m-p\/423648#M344",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":134,
        "Question_body":"Google cloud translation have added new languages. About\u00a024 new languages has been added to Google Translate. Very good job, well done. But they are not listed on this link.\nhttps:\/\/cloud.google.com\/translate\/docs\/languages\n\nI tried to access it using basic v2 API code, but no response came to my translation request. When will this new languages be available to be accessed by v2 APIs?",
        "Question_closed_time":"05-18-2022 04:00 PM",
        "Answer_score_count":1.0,
        "Answer_body":"These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https:\/\/blog.google\/products\/translate\/24-new-languages\/\u00a0\n\n[2]https:\/\/arxiv.org\/pdf\/2205.03983.pdf\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Custom Tune of LLM in Generative AI Studio - training time\/parameters - Costs",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-21T19:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-Tune-of-LLM-in-Generative-AI-Studio-training-time\/m-p\/614610#M2428",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":27,
        "Question_body":"Hi,\n\nI want to tune a LLM in Vertex AI Generative Studio (text-bison001\u00a0) and I know it has 137B parameters. I investigated the costs and they are:\n\nTuning jobs in us-central1 use eight A100 80GB GPUs. Tuning jobs in europe-west4 use 64 cores of the TPU v3 pod custom model training resource, only available upon request. Using a fast calculation, eight A100 80GB will cost 40.22 USD\/hour and the TPU V3 64 cores, supposing is the double of 32 cores, will cost 64 USD\/hour.\n\nI have a proper JSONL dataset with 1,000 to 52,000 examples and I want to train for 300 epochs.\n\nThe issue here is that I need to know how much time usually the tune of text-bison001 takes (1 hour, 10 hours), or at least how many parameters will be tuned, to have an idea about costs involved.\n\nThis information is not provided in Vertex AI Pricing, it is not provided in Generative AI Studio Language documentation. Should I consider the regular Vertex AI pricing in calculator ? Maybe this is not the case, as 8 A100 GPUs will be used.\n\nThanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"cloud vision API",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-23T08:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cloud-vision-API\/m-p\/425445#M358",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":189,
        "Question_body":"Hello everyone,\nMy question is really hypothetical.\u00a0 I am right now using Cloud vision api with feature web detection. Desc:\u00a0Detect topical entities such as news, events, or celebrities within the image, and find similar images on the web using the power of Google Image Search.\n\nDoes profile login effects the results that web detection will return. For example if I type in google search python, I will get python programming language, while my mum will snake. Does the same logic works for cloud vision API web detection ?\u00a0\n\nWhy this question even raised, because same image executed from 2 different profiles ( I have 2 accounts on cloud Vision API) the model return different probabilities.",
        "Question_closed_time":"05-30-2022 10:25 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Yes, it is indeed an intended behavior getting different results even if you use the same image.\n\nThe API returns matching results with their respective scores. The scenario you are experiencing would entail that the scores are relatively close to one another and the API cannot properly select which is a better representation of the image being provided. For more accurate results, we would advise using product reference images with bounding poly coordinates[1].\n\n[1]https:\/\/cloud.google.com\/vision\/product-search\/docs\/tutorial#5_create_a_products_reference_image\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Datasets best practices",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-24T12:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Datasets-best-practices\/m-p\/596635#M2015",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":56,
        "Question_body":"I am new to ML and VertexAI. I have some questions about an app I am building that requires image classification labels. The closest example I can think of is that mobile app which identifies plants, like PlantNet. You take a photo, and it returns the type of plant, ideally with a relationship from parent species.\n\nI chose Vertex because it includes the Google Bucket storage, allows for custom labels, and having more than 1 label per image. I plan to have a single endpoint to query against, across all my data.\n\nIn terms of what should go inside of a dataset, are there best practices for setting up the datasets in Vertex?\u00a0 Should I put all images (categories) under the same\u00a0dataset\u00a0OR can I add multiple datasets that query across a single endpoint?\n\nShould there be 1 dataset, or multiple?\n\nIE: a separate dataset for trees and a dataset for flowers?\n\nin this case trees would include photos with labels \"oak\", \"pine\", \"maple\", and would include `none_of_these` label associated to things like \"roses\" and \"poison ivy\" and \"grass\"\n\nor a single large database that would include all the labels for all the things?\n\nWhat about model deployment? How can I set a budget on that? It's darn pricey at 1.375 USD per hour\n\nWhat about training hours? Is that a bit more ambiguous because it's based on the training output ratings?\n\nit's also pricey at 3.465 USD per hour",
        "Question_closed_time":"05-29-2023 04:37 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@lucksp,\n\nWelcome to Google Cloud Community!\n\nThis will be depending on your use case, here are some suggestions for your questions:\n\n1. You can use a single dataset with all the categories if the categories you're working with are closely related and you want your model to distinguish between them. In your example, you should use a single dataset with all of these labels if you want the model to differentiate the photos. This will enable the model to understand the variations among all the categories and produce more accurate predictions. If you wish to create two different datasets, I would suggest that if the tasks are not related to one another, then you can create two datasets with two different models.\u00a0If you wish to sustain modularity and have various models for various tasks, this method may be helpful.\u00a0\n\n2. You can limit the number of compute nodes in your model settings during deployment but this will also limit its performance. You can check this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/image-recognition-automl\/deploy-predict\nYou can also track this ongoing feature request of auto scaling to zero:\u00a0https:\/\/issuetracker.google.com\/206042974\nYou can check this link also if you want to learn more about the considerations when deploying a model:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment\n\n3.\u00a0 You can try configuring the training budget of the training but please note that the pricing is based on the node hour, and you can also enable the early stopping. Disabling early stopping will train the model until your training budget is exhausted. It is also important to know that this will still depend:\n\nModel training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one.\u00a0You can use this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-\ndata\/forecasting\/train-model\n\nYou can use this link for more information about the training pricing:\u00a0https:\/\/cloud.google.com\/vertex-ai\/pricing#automl_models\u00a0\n\nFor best practices on creating training data, this will be helpful to increase the quality of the model. you can check this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tabular-data\/bp-tabular\n\nHope this helps!\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Being told to contact Vertex AI support but we don't have a support contract?!",
        "Question_tag_count":1,
        "Question_created_time":"2022-03-08T11:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Being-told-to-contact-Vertex-AI-support-but-we-don-t-have-a\/m-p\/401449#M227",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":103,
        "Question_body":"Getting an internal error when training a model on Vertex AI.\n\nI have gotten repeated emails from Google telling me to contact Vertex AI support about this.\n\nWe don't pay for a support contract.\n\nIt seems odd that there is no way to report issues like this to Vertex AI without a support contract.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Run Colab with Mulitple GPUs using Drive Files OR workaround",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-18T15:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Run-Colab-with-Mulitple-GPUs-using-Drive-Files-OR-workaround\/m-p\/444346#M430",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":870,
        "Question_body":"Hi,\n\nI am trying to run my ML model in Colab utilizing a custom VM with multiple GPUs. I can successfully spin up a 2 GPU DeepLearning VM and connect to a Colab notebook via port-forwarding to a locally-hosted connection (Jupyter Notebook), as shown here.\n\nAlthough I can connect to custom runtimes directly WITHOUT port-forwarding to a locally-hosted connection, I can only access 1 of the 2 GPUs this way (i.e.\u00a0\n\nlen(tf.config.list_physical_devices('GPU')) always returns 1); hence, I'm tied to port-forwarding, unless there's an alternative option.\n\nI can successfully connect to a locally-hosted, port-forwarded runtime and verify that the notebook can access the 2 GPUs; however I am running into issues when trying to mount my Google Drive.\n\nI know that ocamlfuse was offered as a suggestion to this Drive issue,\u00a0 however, none of the download options work. Specifically, it seems like a locally-hosted port-forwarded runtime doesn't allow terminal inputs, so I can't \"Press [ENTER]\" to allow the download, as shown below:\n\nUser import cursor shows up for a direct connection to a custom or hosted runtime:\n\nUser import cursor fails to show up\/accept inputs in a locally-hosted, port-forwarded custom VM.\n\nIn general, it seems like terminal commands don't work in Colab in a locally-hosted runtime.\n\n\u00a0\n\n\u00a0\n\nAnother option is\u00a0PyDrive, which I've used in the past. However, since\u00a0PyDrive\u00a0relies on authentication through a local port, I can't get it to work on my locally-hosted custom VM.\n\nIn short I'm looking for tips\/suggestions for any of the following issues:\n\n1) An alternative workflow to run my ML model using multiple GPUs (i.e. that's not through port-forwarding to a locally-hosted connection)\n\n2) How to get that user cursor to show up (enabling me to download\u00a0ocamlfuse)\n\n3) How to authenticate in PyDrive, given I'm already using a local port connection to host my runtime.\n\n4) Alternatives to accessing my Drive\/Drive files.\u00a0\n\n\u00a0\n\nThank you so much!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI fine tuning LLM model",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-13T08:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-fine-tuning-LLM-model\/m-p\/611982#M2359",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":130,
        "Question_body":"Hi,\n\n\u00a0\nI'm new to GCP and I was trying to run a LLM tuning process in Vertex AI.\u00a0\nI upload my data in the jsonl format in a bucket and selected it to start the tuning process.\u00a0During the pipeline, I got this error:\n\u00a0\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\u00a0aiplatform.googleapis.com\/restricted_image_training_tpu_v3_pod, cause=null; Failed to create custom job.Project number: 162269030045, Job id: 8911904581961646080, Task id: -5724177380969283584, Task name: large-language-model-tuner, Task state: DRIVER_SUCCEEDED, Execution name: projects\/162269030045\/locations\/europe-west4\/metadataStores\/default\/executions\/9317118956493817351; Failed to create external task or refresh its state. Task:Project number: 162269030045, Job id: 8911904581961646080, Task id: -5724177380969283584, Task name: large-language-model-tuner, Task state: DRIVER_SUCCEEDED, Execution name: projects\/162269030045\/locations\/europe-west4\/metadataStores\/default\/executions\/9317118956493817351; Failed to handle the pipeline task. Task: Project number: 162269030045, Job id: 8911904581961646080, Task id: -5724177380969283584, Task name: large-language-model-tuner, Task state: DRIVER_SUCCEEDED, Execution name: projects\/162269030045\/locations\/europe-west4\/metadataStores\/default\/executions\/9317118956493817351\n\u00a0\nSo I looked online for a solution\/work around to this problem. I found that some users were resolving it by updating their quotas. Then in the error message I've seen that my limit are reached for the\u00a0europe-west4\u00a0(if I've understood correctly). That's what I'm trying to do right now. Do some of you guys got the same error and can give me some advice to fix it?\n\u00a0\nI look forward to hearing back from you!\nThank you so much!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"question about IAM Dialogflow API Client role",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-19T03:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/question-about-IAM-Dialogflow-API-Client-role\/m-p\/501058#M974",
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":395,
        "Question_body":"Hello, Can anyone help me please?\u00a0I received the following message:\u00a0\n\nIn Dialogflow, the IAM Viewer role currently allows users of the\u00a0console simulator\u00a0to test agent conversations and to call the API's detectIntent\/streamingDetectIntent methods. Starting\u00a0January 31, 2023, Google will remove this permission from this role. Therefore these actions will require the \u2018dialogflow.sessions.detectIntent' or \u2018dialogflow.sessions.streamingDetectIntent' permission, which is included in roles such as the IAM Dialogflow API Client role.\u00a0\n\nWho can tell what to do, where, and how?\u00a0\n\nThanks in advance, Anton",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vocal emojis in Speech-to-Text",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-03T07:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vocal-emojis-in-Speech-to-Text\/m-p\/485418#M725",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":60,
        "Question_body":"Hello! I am majoring in Theoretical Linguistics this year and I would like to write my dissertation on Google Cloud API and the vocal emojis supported, delving into the neural network to find out how they are translated. I have seen that my native language is missing and could build a dataset of spoken forms. Following the tutorial for using the Speech-to-Text API with Phyton I found out that very little information on this project are public.\n\nShould I contact some specific person\/service via my institutional account to receive material for a study case?\n\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to know the total words count of the character per month that I have translated?",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-24T00:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-know-the-total-words-count-of-the-character-per-month\/m-p\/546588#M1730",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":223,
        "Question_body":"I use the Cloud Translation - Basic API and I know the pricing for this basic API. But I do not know how many words have been translated by this API this month. Where to check this information?\u00a0\n\nI searched Google Cloud > API\/Service Details > Could Translation API > METRICS and QUOTAS and still did not find the information.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About Object Localization in Vision API",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-25T02:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Object-Localization-in-Vision-API\/m-p\/176494#M92",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":199,
        "Question_body":"Hi,\n\nWe are considering using the Object Localization feature in the Vision API, however, we cannot find any information about supported object classes. Is the information open to the public? If Yes, where can we find the information?\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DTMF working in dialogflow cx telephony gateway",
        "Question_tag_count":2,
        "Question_created_time":"2022-01-05T22:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DTMF-working-in-dialogflow-cx-telephony-gateway\/m-p\/182110#M159",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":310,
        "Question_body":"Hi,\nI want to know that whether we can integrate\u00a0 bot to Dialogflow CX Phone Gateway and then we can make something like press 1 for this and press 2 for this? and then that bot should work on numbers entered by user? Actually i have done a research regarding this and found that we have dtmf option which let us take input from the user but that is not working , so can you please let me know if something like this is supported?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to package custom prediction code and serve it using an Endpoint in Vertex AI ?",
        "Question_tag_count":2,
        "Question_created_time":"2021-10-25T11:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-package-custom-prediction-code-and-serve-it-using-an\/m-p\/173876#M67",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":258,
        "Question_body":"Goal: serve prediction request from a Vertex AI Endpoint by executing custom prediction logic.\n\nExpected Workflow:\n\n1. Upload a pretrained image_quality.pb model (developed in a non vertex-ai pythonic environment) in a gcs bucket\n\n2. Port existing image inference logic into a container and serve the prediction functionality through a vertex AI endpoint.\u00a0\n\n3. Use Vertex AI api for logging and capturing metrics inside the \u00a0custom inference logic.\n\n4. Finally we want to pass a list of images (stored in another gcs bucket) to that endpoint.\n\n5. We also want to see the logs and metrics in tensorboard.\n\nExisting Vertex AI code samples provide examples for custom training , invoking model.batch_predict \/ endpoint.predict , but don't mention how to execute custom prediction code.\n\nIt would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"how often do I need to call gcloud auth login?",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-30T08:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/how-often-do-I-need-to-call-gcloud-auth-login\/m-p\/598233#M2049",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"I am a FE engineer working on a new company from the ground up. The \"backend\" world is completely foreign to me. It was suggested to use SupaBase for user authorization.\n\u00a0\nWe are using Google VertexAI as the core of the app feature. There is a \"public\" REST endpoint which would look like this:\n\u00a0\n\u00a0\n\n\u00a0\n\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application\/json; charset=utf-8\" \\\n-d u\/request.json \\\n\"https:\/\/LOCATION-aiplatform.googleapis.com\/v1\/projects\/PROJECT\/locations\/LOCATION\/endpoints\/ENDPOINT_ID:explain\"\n\n\u00a0\n\n\u00a0\nThe issue I run into trying to convert this to a `fetch` type API is that the `gcloud` commands & the oAuth expire very frequently, and storing them as a ReactNative \"constant\" via `process.env` isn't ideal.\n\u00a0\n\u00a0\n\n\u00a0\n\nconst path = `https:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${Constants.expoConfig.extra.VERTEX_PROJECT_ID}\/locations\/us-central1\/endpoints\/${Constants.expoConfig.extra.VERTEX_ENDPOINT_ID}:predict`;\n\nconst init = {\n    method: 'POST',\n    body: JSON.stringify(requestBody),\n    headers: {\n      Authorization: `Bearer ${Constants.expoConfig.extra.VERTEX_GCLOUD_ACCESS_TOKEN}`,\n      'Content-Type': 'application\/json',\n    },\n  };\n\nconst response = await fetch(path, init);\n\n\u00a0\n\n\u00a0\nThe Google documentation says:\nNote: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login, or by using Cloud Shell, which automatically logs you into the gcloud CLI. You can check the currently active account by running gcloud auth list.\n\nIs it correct that I want to call\u00a0gcloud auth login\u00a0basically once, when the server is initialized so that any of my users can use the feature on the mobile app, or does this need to be called each time a request is made to the gcloud API? Then the\u00a0gcloud auth print-access-token\u00a0gets called for each API request.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI create pipeline file content error",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-23T22:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-pipeline-file-content-error\/m-p\/596343#M2009",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":144,
        "Question_body":"Have been trying to build a pipeline using several generic examples. Python created the following yaml file, but when I upload it into vertex ai pipeline page, I always get the following error \"Invalid File Content\".\n\n\u00a0\n\n\u00a0\n\napiVersion: argoproj.io\/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: tfrecord-processing-pipeline-\n  annotations: {pipelines.kubeflow.org\/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org\/pipeline_compilation_time: '2023-05-23T21:47:19.096627',\n    pipelines.kubeflow.org\/pipeline_spec: '{\"inputs\": [{\"default\": \"gs:\/\/aa.bb.cc.dd\/data\",\n      \"name\": \"tfrecord_dir\", \"optional\": true, \"type\": \"String\"}], \"name\": \"TFRecord\n      Processing Pipeline\"}'}\n  labels: {pipelines.kubeflow.org\/kfp_sdk_version: 1.8.22}\nspec:\n  entrypoint: tfrecord-processing-pipeline\n  templates:\n  - name: load-tfrecord-dataset\n    container:\n      args:\n      - \"\\n            import tensorflow as tf\\n            file_pattern = \\\"{{inputs.parameters.tfrecord_dir}}\\\"\\\n        \\ + '\/*.block'\\n            files = tf.io.gfile.glob(file_pattern)\\n     \\\n        \\       for file in files:\\n                dataset = tf.data.TFRecordDataset(file)\\n\\\n        \\            \"\n      command: [python, -c]\n      image: tensorflow\/tensorflow:2.6.0\n    inputs:\n      parameters:\n      - {name: tfrecord_dir}\n    metadata:\n      labels:\n        pipelines.kubeflow.org\/kfp_sdk_version: 1.8.22\n        pipelines.kubeflow.org\/pipeline-sdk-type: kfp\n        pipelines.kubeflow.org\/enable_caching: \"true\"\n  - name: tfrecord-processing-pipeline\n    inputs:\n      parameters:\n      - {name: tfrecord_dir}\n    dag:\n      tasks:\n      - name: load-tfrecord-dataset\n        template: load-tfrecord-dataset\n        arguments:\n          parameters:\n          - {name: tfrecord_dir, value: '{{inputs.parameters.tfrecord_dir}}'}\n  arguments:\n    parameters:\n    - {name: tfrecord_dir, value: 'gs:\/\/aa.bb.cc.dd\/data'}\n  serviceAccountName: pipeline-runner\n\n\u00a0\n\n\u00a0Any thoughts?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Access public saved query in \"Using BigQuery ML to predict basketball outcomes\" tutorial",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-08T11:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Access-public-saved-query-in-quot-Using-BigQuery-ML-to-predict\/m-p\/497608#M923",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":40,
        "Question_body":"I am trying to complete the tutorial at this link:\u00a0https:\/\/cloud.google.com\/bigquery-ml\/docs\/bigqueryml-ncaa\u00a0but cannot access the public saved queries for the feature input,\u00a0 training, or evaluation data.\u00a0\n\nHere is the link to the feature input query:\u00a0\n\nhttps:\/\/bigquery.cloud.google.com\/savedquery\/1057666841514:77711b21274b4c6485c907483ef2f6fe\n\nIt just redirects me to the BigQuery console. Please let me know if you have any tips or how to access the query. Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Translate API - Laotian Translation Failures",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-13T12:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-Laotian-Translation-Failures\/m-p\/431021#M375",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":139,
        "Question_body":"Description:\n\nCalling method\u00a0\n\n\u00a0\n\n\u00a0\n\ngoogle.cloud.translate.v2.TranslateService.TranslateText\n\n\u00a0\n\n\u00a0\n\nLaotian language is not consistently translated using Google translate.\nApproximately 40% of paragraphs in our sample data failed to translate text to English\nLonger paragraphs and those with mixed languages (Lao English) and dates appear to be more susceptible to failures.\nRemoving special characters and zero-length characters did not improve\n\nInquiries:\n\nIs this a known bug in the API?\nCould there be something else we try on our own end?\nWhat is the process for resolving this issue?\n\n\u00a0\n\nCode Example:\n\nfrom google.cloud import translate_v2\n\noriginal_text = [\n    \"\u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9\",\n    \"\u0e81\u0eb4\u0e94\u0e88\u0eb0\u0e81\u0eb3\u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e9b\u0eb0\u0e88\u0eb3\u0e9b\u0eb5 2022 \u0ec4\u0e94\u0ec9\u0e88\u0eb1\u0e94\u0e82\u0eb6\u0ec9\u0e99\u0ec3\u0e99 \u0ea7\u0eb1\u0e99 \u0e97\u0eb5 22 \u0ec0\u0ea1\u0eaa\u0eb2 \u0e99\u0eb5\u0ec9 \u0e97\u0eb5\u0ec8\u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0ec2\u0e94\u0e8d\u0e81\u0eb2\u0e99\u0ec0\u0e82\u0ebb\u0ec9\u0eb2\u0eae\u0ec8\u0ea7\u0ea1\u0e82\u0ead\u0e87 \u0e97\u0ec8\u0eb2\u0e99 \u0e97\u0ead\u0e87\u0e88\u0eb1\u0e99 \u0e9e\u0eb9\u0ea1\u0eb5\u0e9e\u0eb1\u0e99 \u0eae\u0ead\u0e87\u0ec0\u0ea5\u0e82\u0eb2\u0e84\u0eb0\u0e99\u0eb0\u0e9a\u0ecd\u0ea5\u0eb4\u0eab\u0eb2\u0e99\u0e87\u0eb2\u0e99\u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0e9c\u0eb9\u0ec9\u0e8a\u0eb5\u0ec9\u0e99\u0eb3\u0ea7\u0ebd\u0e81\u0e87\u0eb2\u0e99\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1, \u0ea1\u0eb5\u0e9a\u0eb1\u0e99\u0e94\u0eb2\u0eab\u0ebb\u0ea7\u0edc\u0ec9\u0eb2\u0e81\u0ebb\u0ea1, \u0eae\u0ead\u0e87\u0e81\u0ebb\u0ea1 \u0ec1\u0ea5\u0eb0 \u0e9e\u0eb2\u0e81\u0eaa\u0ec8\u0ea7\u0e99\u0e81\u0ec8\u0ebd\u0ea7\u0e82\u0ec9\u0ead\u0e87\u0ec0\u0e82\u0ebb\u0ec9\u0eb2\u0eae\u0ec8\u0ea7\u0ea1.\",\n    \"\u0e81\u0eb2\u0e99\u0e88\u0eb1\u0e94\u0e81\u0eb4\u0e94\u0e88\u0eb0\u0e81\u0eb3\u0e84\u0eb1\u0ec9\u0e87\u0e99\u0eb5\u0ec9 \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ec0\u0e9b\u0eb1\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ec9\u0eb2\u0e87\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0ebb\u0ea1\u0ec0\u0e8a\u0eb5\u0e8d\u0ea7\u0eb1\u0e99\u0e8a\u0eb2\u0e94\u0e97\u0eb5 2 \u0e97\u0eb1\u0e99\u0ea7\u0eb2 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 47 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0ec0\u0e81\u0eb5\u0e94\u0e9b\u0eb0\u0e97\u0eb2\u0e99 \u0ec4\u0e81\u0eaa\u0ead\u0e99 \u0e9e\u0ebb\u0ea1\u0ea7\u0eb4\u0eab\u0eb2\u0e99 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 102 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0ec0\u0e81\u0eb5\u0e94\u0e9b\u0eb0\u0e97\u0eb2\u0e99 \u0eaa\u0eb8\u0e9e\u0eb2\u0e99\u0eb8\u0ea7\u0ebb\u0e87 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 113 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0eaa\u0ec9\u0eb2\u0e87\u0e95\u0eb1\u0ec9\u0e87 \u0e9e\u0eb1\u0e81\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 67 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0eaa\u0ec9\u0eb2\u0e87\u0e95\u0eb1\u0ec9\u0e87 \u0e84\u0eb0\u0e99\u0eb0\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 67 \u0e9b\u0eb5 \u0ec1\u0ea5\u0eb0 \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ec3\u0eab\u0ec9\u0ec0\u0e9b\u0eb1\u0e99\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0ead\u0eb0\u0e99\u0eb8\u0ea5\u0eb1\u0e81\u0eae\u0eb1\u0e81\u0eaa\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1\u0ec3\u0eab\u0ec9\u0e8d\u0eb7\u0e99\u0e8d\u0ebb\u0e87\u0ec0\u0eab\u0eb1\u0e99\u0ec4\u0e94\u0ec9\u0ea7\u0ec8\u0eb2\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0ea1\u0eb5\u0e9a\u0ebb\u0e94\u0e9a\u0eb2\u0e94\u0eaa\u0eb3\u0e84\u0eb1\u0e99\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0e82\u0eb1\u0e9a\u0ec0\u0e84\u0eb7\u0ec8\u0ead\u0e99\u0ea7\u0ebd\u0e81\u0e87\u0eb2\u0e99\u0e94\u0eb1\u0ec8\u0e87\u0e81\u0ec8\u0eb2\u0ea7. \u0e94\u0eb1\u0ec8\u0e87\u0e99\u0eb1\u0ec9\u0e99 \u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0ec3\u0e99\u0e96\u0eb2\u0e99\u0eb0\u0eae\u0eb1\u0e9a\u0e9c\u0eb4\u0e94\u0e8a\u0ead\u0e9a\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0e99\u0eb3\u0e9e\u0eb2\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0ec0\u0e84\u0eb7\u0ec8\u0ead\u0e99\u0ec4\u0eab\u0ea7\u0e95\u0ec8\u0eb2\u0e87\u0ec6\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edd\u0eb8\u0ec8\u0ea1, \u0ec2\u0e94\u0e8d\u0eaa\u0eb0\u0ec0\u0e9e\u0eb2\u0eb0\u0e9e\u0eb0\u0ec1\u0e99\u0e81\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e81\u0ebb\u0ea1\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e8a\u0eb6\u0ec8\u0e87\u0ea1\u0eb5\u0e9e\u0eb2\u0ea5\u0eb0\u0e9a\u0ebb\u0e94\u0e9a\u0eb2\u0e94\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0ec0\u0eaa\u0eb5\u0ea1\u0e9e\u0ead\u0e99\u0eaa\u0eb0\u0eab\u0ea7\u0eb1\u0e99, \u0eab\u0ebb\u0ea7\u0e84\u0eb4\u0e94\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0eaa\u0ec9\u0eb2\u0e87\u0e94\u0ec9\u0eb2\u0e99\u0e95\u0ec8\u0eb2\u0e87\u0ec6\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e88\u0eb6\u0ec8\u0e87\u0ec4\u0e94\u0ec9\u0e88\u0eb1\u0e94\u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e9e\u0eb2\u0e8d\u0ec3\u0e99\u0e99\u0eb0\u0e84\u0ead\u0e99\u0eab\u0ebc\u0ea7\u0e87\u0ea7\u0ebd\u0e87\u0e88\u0eb1\u0e99 \u0e82\u0eb6\u0ec9\u0e99\u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0e9b\u0eb9\u0e81\u0e88\u0eb4\u0e94\u0eaa\u0eb3\u0e99\u0eb6\u0e81\u0eae\u0eb1\u0e81\u0eaa\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1 \u0ec1\u0ea5\u0eb0 \u0e97\u0eb1\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0ec0\u0eaa\u0eb5\u0ea1\u0eab\u0ebb\u0ea7\u0e84\u0eb4\u0e94\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0eaa\u0ec9\u0eb2\u0e87\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e95\u0eb7\u0ec8\u0ea1\u0ead\u0eb5\u0e81, \u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e84\u0eb0\u0e99\u0eb0\u0e81\u0eb3\u0ea1\u0eb0\u0e81\u0eb2\u0e99\u0ec4\u0e94\u0ec9\u0e84\u0eb1\u0e94\u0ec0\u0ea5\u0eb7\u0ead\u0e81\u0ec0\u0ead\u0ebb\u0eb2 8 \u0e97\u0eb5\u0ea1 \u0e84\u0eb7: \u0e97\u0eb5\u0ea1 Passion, \u0e97\u0eb5\u0ea1\u0e82\u0eb1\u0e99\u0ec4\u0e8a \u0eaa\u0ead\u0e99\u0ea1\u0eb2\u0ea5\u0eb2, \u0e97\u0eb5\u0ea1\u0eab\u0ebc\u0eb8\u0e94\u0e9c\u0ec8\u0ead\u0e99\u0e82\u0eb5\u0ec9\u0ec0\u0eab\u0e8d\u0eb7\u0ec9\u0ead\u0ec3\u0e99\u0eaa\u0eb1\u0e87\u0e84\u0ebb\u0ea1, \u0e97\u0eb5\u0ea1\u0e9b\u0eb0\u0e8d\u0eb8\u0e81\u0ec3\u0e8a\u0ec9, \u0e97\u0eb5\u0ea1 NRD, \u0e97\u0eb5\u0ea1\u0eaa\u0eb0\u0eab\u0ebc\u0eb2\u0e94\u0e84\u0eb4\u0e94, \u0e97\u0eb5\u0ea1\u0eaa\u0eb0\u0eab\u0ea7\u0ec8\u0eb2\u0e87\u0eaa\u0eb0\u0ec4\u0eab\u0ea7, \u0ec1\u0ea5\u0eb0 \u0e97\u0eb5\u0ea1 VL \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ea1\u0eb2\u0e8a\u0eb5\u0e87\u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 1,2 \u0ec1\u0ea5\u0eb0 3.\",\n    \"\u0e9c\u0ebb\u0e99\u0e82\u0ead\u0e87\u0e81\u0eb2\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0e97\u0eb5\u0ea1\u0e97\u0eb5\u0ec8\u0ec4\u0e94\u0ec9 \u00a0\u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 1 \u0ec1\u0ea1\u0ec8\u0e99\u0e97\u0eb5\u0ea1 \u0e9b\u0eb0\u0e8d\u0eb8\u0e81\u0ec3\u0e8a\u0ec9 \u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0ec0\u0e87\u0eb4\u0e99\u0ea5\u0eb2\u0e87\u0ea7\u0eb1\u0e992.000.000 \u0e81\u0eb5\u0e9a, \u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 2 \u0ec1\u0ea1\u0ec8\u0e99\u0e97\u0eb5\u0ea1 Passion \u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0ec0\u0e87\u0eb4\u0e99\u0ea5\u0eb2\u0e87\u0ea7\u0eb1\u0e99 1.500.000 \u0e81\u0eb5\u0e9a \u0ec1\u0ea5\u0eb0 \u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 3 \u0ec1\u0ea1\u0ec8\u0e99\u0e97\u0eb5\u0ea1 \u0eaa\u0eb0\u0eab\u0ea7\u0ec8\u0eb2\u0e87\u0eaa\u0eb0\u0ec4\u0eab\u0ea7 \u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0ec0\u0e87\u0eb4\u0e99\u0ea5\u0eb2\u0e87\u0ea7\u0eb1\u0e99 1.000.000 \u0e81\u0eb5\u0e9a.\",\n    \"\u0e94\u0eb1\u0ec8\u0e87\u0e97\u0eb5\u0ec8\u0eae\u0eb9\u0ec9\u0e99\u0ecd\u0eb2\u0e81\u0eb1\u0e99\u0ec1\u0ea5\u0ec9\u0ea7\u0ea7\u0ec8\u0eb2\u0ec4\u0ea5\u0e8d\u0eb0\u0e9c\u0ec8\u0eb2\u0e99\u0ea1\u0eb2\u0ec0\u0eaa\u0e94\u0e96\u0eb0\u0e81\u0eb4\u0e94-\u0eaa\u0eb1\u0e87\u0e84\u0ebb\u0ea1 \u0e82\u0ead\u0e87 \u0eaa\u0e9b\u0e9b \u0ea5\u0eb2\u0ea7 \u0ec4\u0e94\u0ec9\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7\u0ea2\u0ec8\u0eb2\u0e87\u0e95\u0ecd\u0ec8\u0ec0\u0e99\u0eb7\u0ec8\u0ead\u0e87 \u0ec1\u0ea5\u0eb0 \u0e81\u0eb3\u0ea5\u0eb1\u0e87\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2 \u0ec2\u0e94\u0e8d\u0eaa\u0eb0\u0ec0\u0e9e\u0eb2\u0eb0\u0ec1\u0ea1\u0ec8\u0e99\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7\u0e94\u0ec9\u0eb2\u0e99\u0ead\u0eb8\u0e94\u0eaa\u0eb2\u0eab\u0eb0\u0e81\u0eb3, \u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2\u0e94\u0ec9\u0eb2\u0e99\u0ec2\u0e84\u0e87\u0ea5\u0ec8\u0eb2\u0e87\u0e9e\u0eb7\u0ec9\u0e99\u0e96\u0eb2\u0e99\u0e82\u0ead\u0e87\u0ec0\u0eaa\u0e94\u0e96\u0eb0\u0e81\u0eb4\u0e94\u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0e81\u0eb2\u0e99\u0e9b\u0eb1\u0e9a\u0e9b\u0eb8\u0e87, \u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2\u0e95\u0ebb\u0ea7\u0ec0\u0ea1\u0eb7\u0ead\u0e87, \u0e81\u0eb2\u0e99\u0e82\u0ebb\u0e99\u0eaa\u0ebb\u0ec8\u0e87, \u0e9e\u0eb0\u0ea5\u0eb1\u0e87\u0e87\u0eb2\u0e99\u0ec4\u0e9f\u0e9f\u0ec9\u0eb2\u0ec4\u0e94\u0ec9\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7 \u0ec0\u0eae\u0eb1\u0e94\u0ec3\u0eab\u0ec9\u0e8a\u0eb5\u0ea7\u0eb4\u0e94\u0e81\u0eb2\u0e99\u0ec0\u0e9b\u0eb1\u0e99\u0ea2\u0eb9\u0ec8\u0e82\u0ead\u0e87\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99\u0ea5\u0eb2\u0ea7\u0e9a\u0eb1\u0e99\u0e94\u0eb2\u0ec0\u0e9c\u0ebb\u0ec8\u0eb2\u0e99\u0eb1\u0e9a\u0ea1\u0eb7\u0ec9\u0e94\u0eb5\u0e82\u0eb6\u0ec9\u0e99\u0ec0\u0e97\u0eb7\u0ec8\u0ead\u0ea5\u0eb0\u0e81\u0ec9\u0eb2\u0ea7, \u0e84\u0ebd\u0e87\u0e84\u0eb9\u0ec8\u0e81\u0eb1\u0e9a\u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2 \u0eaa\u0e9b\u0e9b \u0ea5\u0eb2\u0ea7 \u0e81\u0ecd\u0ec4\u0e94\u0ec9\u0e9b\u0eb0\u0ec0\u0e8a\u0eb5\u0e99\u0e81\u0eb1\u0e9a\u0e9a\u0eb1\u0e99\u0eab\u0eb2\u0e97\u0ec9\u0eb2\u0e97\u0eb2\u0e8d\u0eab\u0ebc\u0eb2\u0e8d\u0ea2\u0ec8\u0eb2\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e95\u0ebb\u0ec9\u0e99\u0e9a\u0eb1\u0e99\u0eab\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e88\u0eb2\u0e81\u0e81\u0eb2\u0e99\u0e99\u0eb3\u0ec3\u0e8a\u0ec9\u0e82\u0ead\u0e87\u0e84\u0ebb\u0e99\u0ec0\u0eae\u0ebb\u0eb2\u0e99\u0eb1\u0e9a\u0ea1\u0eb7\u0ec9\u0e99\u0eb1\u0e9a\u0ec0\u0e9e\u0eb5\u0ec8\u0ea1\u0e82\u0eb6\u0ec9\u0e99\u0e88\u0ebb\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0e9c\u0ebb\u0e99\u0e81\u0eb0\u0e97\u0ebb\u0e9a\u0e95\u0ecd\u0ec8\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1\u0ec3\u0e99\u0eab\u0ebc\u0eb2\u0e8d\u0e82\u0ebb\u0e87\u0ec0\u0e82\u0e94.\",\n]\n\nclient = translate_v2.Client()\n\nfor text in original_text:\n    result = client.translate(text, format_='text')\n\nResulting Translation:\n\nORIGINAL \u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9\n\nTRANSLATED Waste artifact competition\n****************************************\nORIGINAL \u0e81\u0eb4\u0e94\u0e88\u0eb0\u0e81\u0eb3\u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e9b\u0eb0\u0e88\u0eb3\u0e9b\u0eb5 2022 \u0ec4\u0e94\u0ec9\u0e88\u0eb1\u0e94\u0e82\u0eb6\u0ec9\u0e99\u0ec3\u0e99 \u0ea7\u0eb1\u0e99 \u0e97\u0eb5 22 \u0ec0\u0ea1\u0eaa\u0eb2 \u0e99\u0eb5\u0ec9 \u0e97\u0eb5\u0ec8\u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0ec2\u0e94\u0e8d\u0e81\u0eb2\u0e99\u0ec0\u0e82\u0ebb\u0ec9\u0eb2\u0eae\u0ec8\u0ea7\u0ea1\u0e82\u0ead\u0e87 \u0e97\u0ec8\u0eb2\u0e99 \u0e97\u0ead\u0e87\u0e88\u0eb1\u0e99 \u0e9e\u0eb9\u0ea1\u0eb5\u0e9e\u0eb1\u0e99 \u0eae\u0ead\u0e87\u0ec0\u0ea5\u0e82\u0eb2\u0e84\u0eb0\u0e99\u0eb0\u0e9a\u0ecd\u0ea5\u0eb4\u0eab\u0eb2\u0e99\u0e87\u0eb2\u0e99\u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0e9c\u0eb9\u0ec9\u0e8a\u0eb5\u0ec9\u0e99\u0eb3\u0ea7\u0ebd\u0e81\u0e87\u0eb2\u0e99\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1, \u0ea1\u0eb5\u0e9a\u0eb1\u0e99\u0e94\u0eb2\u0eab\u0ebb\u0ea7\u0edc\u0ec9\u0eb2\u0e81\u0ebb\u0ea1, \u0eae\u0ead\u0e87\u0e81\u0ebb\u0ea1 \u0ec1\u0ea5\u0eb0 \u0e9e\u0eb2\u0e81\u0eaa\u0ec8\u0ea7\u0e99\u0e81\u0ec8\u0ebd\u0ea7\u0e82\u0ec9\u0ead\u0e87\u0ec0\u0e82\u0ebb\u0ec9\u0eb2\u0eae\u0ec8\u0ea7\u0ea1.\n\nTRANSLATED The 2022 Inventory Competition was held on 22 April at the Lao People's Revolutionary Youth Center with the participation of Mr. Thongchanh Phoumiphan, Deputy Secretary of the Central Executive Committee of the Lao People's Revolutionary Youth Center, who led the work.\n****************************************\nORIGINAL \u0e81\u0eb2\u0e99\u0e88\u0eb1\u0e94\u0e81\u0eb4\u0e94\u0e88\u0eb0\u0e81\u0eb3\u0e84\u0eb1\u0ec9\u0e87\u0e99\u0eb5\u0ec9 \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ec0\u0e9b\u0eb1\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ec9\u0eb2\u0e87\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0ebb\u0ea1\u0ec0\u0e8a\u0eb5\u0e8d\u0ea7\u0eb1\u0e99\u0e8a\u0eb2\u0e94\u0e97\u0eb5 2 \u0e97\u0eb1\u0e99\u0ea7\u0eb2 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 47 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0ec0\u0e81\u0eb5\u0e94\u0e9b\u0eb0\u0e97\u0eb2\u0e99 \u0ec4\u0e81\u0eaa\u0ead\u0e99 \u0e9e\u0ebb\u0ea1\u0ea7\u0eb4\u0eab\u0eb2\u0e99 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 102 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0ec0\u0e81\u0eb5\u0e94\u0e9b\u0eb0\u0e97\u0eb2\u0e99 \u0eaa\u0eb8\u0e9e\u0eb2\u0e99\u0eb8\u0ea7\u0ebb\u0e87 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 113 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0eaa\u0ec9\u0eb2\u0e87\u0e95\u0eb1\u0ec9\u0e87 \u0e9e\u0eb1\u0e81\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 67 \u0e9b\u0eb5, \u0ea7\u0eb1\u0e99\u0eaa\u0ec9\u0eb2\u0e87\u0e95\u0eb1\u0ec9\u0e87 \u0e84\u0eb0\u0e99\u0eb0\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0e84\u0ebb\u0e9a\u0eae\u0ead\u0e9a 67 \u0e9b\u0eb5 \u0ec1\u0ea5\u0eb0 \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ec3\u0eab\u0ec9\u0ec0\u0e9b\u0eb1\u0e99\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0ead\u0eb0\u0e99\u0eb8\u0ea5\u0eb1\u0e81\u0eae\u0eb1\u0e81\u0eaa\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1\u0ec3\u0eab\u0ec9\u0e8d\u0eb7\u0e99\u0e8d\u0ebb\u0e87\u0ec0\u0eab\u0eb1\u0e99\u0ec4\u0e94\u0ec9\u0ea7\u0ec8\u0eb2\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0ea1\u0eb5\u0e9a\u0ebb\u0e94\u0e9a\u0eb2\u0e94\u0eaa\u0eb3\u0e84\u0eb1\u0e99\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0e82\u0eb1\u0e9a\u0ec0\u0e84\u0eb7\u0ec8\u0ead\u0e99\u0ea7\u0ebd\u0e81\u0e87\u0eb2\u0e99\u0e94\u0eb1\u0ec8\u0e87\u0e81\u0ec8\u0eb2\u0ea7. \u0e94\u0eb1\u0ec8\u0e87\u0e99\u0eb1\u0ec9\u0e99 \u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0ec3\u0e99\u0e96\u0eb2\u0e99\u0eb0\u0eae\u0eb1\u0e9a\u0e9c\u0eb4\u0e94\u0e8a\u0ead\u0e9a\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0e99\u0eb3\u0e9e\u0eb2\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0ec0\u0e84\u0eb7\u0ec8\u0ead\u0e99\u0ec4\u0eab\u0ea7\u0e95\u0ec8\u0eb2\u0e87\u0ec6\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edd\u0eb8\u0ec8\u0ea1, \u0ec2\u0e94\u0e8d\u0eaa\u0eb0\u0ec0\u0e9e\u0eb2\u0eb0\u0e9e\u0eb0\u0ec1\u0e99\u0e81\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e81\u0ebb\u0ea1\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e8a\u0eb6\u0ec8\u0e87\u0ea1\u0eb5\u0e9e\u0eb2\u0ea5\u0eb0\u0e9a\u0ebb\u0e94\u0e9a\u0eb2\u0e94\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0ec0\u0eaa\u0eb5\u0ea1\u0e9e\u0ead\u0e99\u0eaa\u0eb0\u0eab\u0ea7\u0eb1\u0e99, \u0eab\u0ebb\u0ea7\u0e84\u0eb4\u0e94\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0eaa\u0ec9\u0eb2\u0e87\u0e94\u0ec9\u0eb2\u0e99\u0e95\u0ec8\u0eb2\u0e87\u0ec6\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e88\u0eb6\u0ec8\u0e87\u0ec4\u0e94\u0ec9\u0e88\u0eb1\u0e94\u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e9e\u0eb2\u0e8d\u0ec3\u0e99\u0e99\u0eb0\u0e84\u0ead\u0e99\u0eab\u0ebc\u0ea7\u0e87\u0ea7\u0ebd\u0e87\u0e88\u0eb1\u0e99 \u0e82\u0eb6\u0ec9\u0e99\u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0e9b\u0eb9\u0e81\u0e88\u0eb4\u0e94\u0eaa\u0eb3\u0e99\u0eb6\u0e81\u0eae\u0eb1\u0e81\u0eaa\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1 \u0ec1\u0ea5\u0eb0 \u0e97\u0eb1\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0ec0\u0eaa\u0eb5\u0ea1\u0eab\u0ebb\u0ea7\u0e84\u0eb4\u0e94\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0eaa\u0ec9\u0eb2\u0e87\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e95\u0eb7\u0ec8\u0ea1\u0ead\u0eb5\u0e81, \u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e84\u0eb0\u0e99\u0eb0\u0e81\u0eb3\u0ea1\u0eb0\u0e81\u0eb2\u0e99\u0ec4\u0e94\u0ec9\u0e84\u0eb1\u0e94\u0ec0\u0ea5\u0eb7\u0ead\u0e81\u0ec0\u0ead\u0ebb\u0eb2 8 \u0e97\u0eb5\u0ea1 \u0e84\u0eb7: \u0e97\u0eb5\u0ea1 Passion, \u0e97\u0eb5\u0ea1\u0e82\u0eb1\u0e99\u0ec4\u0e8a \u0eaa\u0ead\u0e99\u0ea1\u0eb2\u0ea5\u0eb2, \u0e97\u0eb5\u0ea1\u0eab\u0ebc\u0eb8\u0e94\u0e9c\u0ec8\u0ead\u0e99\u0e82\u0eb5\u0ec9\u0ec0\u0eab\u0e8d\u0eb7\u0ec9\u0ead\u0ec3\u0e99\u0eaa\u0eb1\u0e87\u0e84\u0ebb\u0ea1, \u0e97\u0eb5\u0ea1\u0e9b\u0eb0\u0e8d\u0eb8\u0e81\u0ec3\u0e8a\u0ec9, \u0e97\u0eb5\u0ea1 NRD, \u0e97\u0eb5\u0ea1\u0eaa\u0eb0\u0eab\u0ebc\u0eb2\u0e94\u0e84\u0eb4\u0e94, \u0e97\u0eb5\u0ea1\u0eaa\u0eb0\u0eab\u0ea7\u0ec8\u0eb2\u0e87\u0eaa\u0eb0\u0ec4\u0eab\u0ea7, \u0ec1\u0ea5\u0eb0 \u0e97\u0eb5\u0ea1 VL \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ea1\u0eb2\u0e8a\u0eb5\u0e87\u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 1,2 \u0ec1\u0ea5\u0eb0 3.\n\nTRANSLATED This event was organized to celebrate the 47th anniversary of the 2nd National Day, the 102nd birthday of President Kaysone Phomvihane, the 113rd birthday of President Suphanuvong, the 67th anniversary of the founding of the Lao People's Revolutionary Party, the 67th anniversary of the establishment of the Lao People's Revolutionary Party. \u0e94\u0eb1\u0ec8\u0e87\u0e99\u0eb1\u0ec9\u0e99 \u0eaa\u0eb9\u0e99\u0e81\u0eb2\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99 \u0e9b\u0eb0\u0e95\u0eb4\u0ea7\u0eb1\u0e94\u0ea5\u0eb2\u0ea7 \u0ec3\u0e99\u0e96\u0eb2\u0e99\u0eb0\u0eae\u0eb1\u0e9a\u0e9c\u0eb4\u0e94\u0e8a\u0ead\u0e9a\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0e99\u0eb3\u0e9e\u0eb2\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0ec0\u0e84\u0eb7\u0ec8\u0ead\u0e99\u0ec4\u0eab\u0ea7\u0e95\u0ec8\u0eb2\u0e87\u0ec6\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edd\u0eb8\u0ec8\u0ea1, \u0ec2\u0e94\u0e8d\u0eaa\u0eb0\u0ec0\u0e9e\u0eb2\u0eb0\u0e9e\u0eb0\u0ec1\u0e99\u0e81\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e81\u0ebb\u0ea1\u0e82\u0eb0\u0e9a\u0ea7\u0e99\u0e81\u0eb2\u0e99\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e8a\u0eb6\u0ec8\u0e87\u0ea1\u0eb5\u0e9e\u0eb2\u0ea5\u0eb0\u0e9a\u0ebb\u0e94\u0e9a\u0eb2\u0e94\u0ec3\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0ec0\u0eaa\u0eb5\u0ea1\u0e9e\u0ead\u0e99\u0eaa\u0eb0\u0eab\u0ea7\u0eb1\u0e99, \u0eab\u0ebb\u0ea7\u0e84\u0eb4\u0e94\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0eaa\u0ec9\u0eb2\u0e87\u0e94\u0ec9\u0eb2\u0e99\u0e95\u0ec8\u0eb2\u0e87\u0ec6\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1 \u0e88\u0eb6\u0ec8\u0e87\u0ec4\u0e94\u0ec9\u0e88\u0eb1\u0e94\u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0eaa\u0eb4\u0ec8\u0e87\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0e88\u0eb2\u0e81\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e9e\u0eb2\u0e8d\u0ec3\u0e99\u0e99\u0eb0\u0e84\u0ead\u0e99\u0eab\u0ebc\u0ea7\u0e87\u0ea7\u0ebd\u0e87\u0e88\u0eb1\u0e99 \u0e82\u0eb6\u0ec9\u0e99\u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0e9b\u0eb9\u0e81\u0e88\u0eb4\u0e94\u0eaa\u0eb3\u0e99\u0eb6\u0e81\u0eae\u0eb1\u0e81\u0eaa\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1 \u0ec1\u0ea5\u0eb0 \u0e97\u0eb1\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e81\u0eb2\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0ec0\u0eaa\u0eb5\u0ea1\u0eab\u0ebb\u0ea7\u0e84\u0eb4\u0e94\u0e9b\u0eb0\u0e94\u0eb4\u0e94\u0eaa\u0ec9\u0eb2\u0e87\u0e82\u0ead\u0e87\u0e8a\u0eb2\u0ea7\u0edc\u0eb8\u0ec8\u0ea1\u0e95\u0eb7\u0ec8\u0ea1\u0ead\u0eb5\u0e81, \u0e81\u0eb2\u0e99\u0ec1\u0e82\u0ec8\u0e87\u0e82\u0eb1\u0e99\u0e84\u0eb0\u0e99\u0eb0\u0e81\u0eb3\u0ea1\u0eb0\u0e81\u0eb2\u0e99\u0ec4\u0e94\u0ec9\u0e84\u0eb1\u0e94\u0ec0\u0ea5\u0eb7\u0ead\u0e81\u0ec0\u0ead\u0ebb\u0eb2 8 \u0e97\u0eb5\u0ea1 \u0e84\u0eb7: \u0e97\u0eb5\u0ea1 Passion, \u0e97\u0eb5\u0ea1\u0e82\u0eb1\u0e99\u0ec4\u0e8a \u0eaa\u0ead\u0e99\u0ea1\u0eb2\u0ea5\u0eb2, \u0e97\u0eb5\u0ea1\u0eab\u0ebc\u0eb8\u0e94\u0e9c\u0ec8\u0ead\u0e99\u0e82\u0eb5\u0ec9\u0ec0\u0eab\u0e8d\u0eb7\u0ec9\u0ead\u0ec3\u0e99\u0eaa\u0eb1\u0e87\u0e84\u0ebb\u0ea1, \u0e97\u0eb5\u0ea1\u0e9b\u0eb0\u0e8d\u0eb8\u0e81\u0ec3\u0e8a\u0ec9, \u0e97\u0eb5\u0ea1 NRD, \u0e97\u0eb5\u0ea1\u0eaa\u0eb0\u0eab\u0ebc\u0eb2\u0e94\u0e84\u0eb4\u0e94, \u0e97\u0eb5\u0ea1\u0eaa\u0eb0\u0eab\u0ea7\u0ec8\u0eb2\u0e87\u0eaa\u0eb0\u0ec4\u0eab\u0ea7, \u0ec1\u0ea5\u0eb0 \u0e97\u0eb5\u0ea1 VL \u0ec0\u0e9e\u0eb7\u0ec8\u0ead\u0ea1\u0eb2\u0e8a\u0eb5\u0e87\u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 1,2 \u0ec1\u0ea5\u0eb0 3.\n****************************************\nORIGINAL \u0e9c\u0ebb\u0e99\u0e82\u0ead\u0e87\u0e81\u0eb2\u0e99\u0e9b\u0eb0\u0e81\u0ea7\u0e94\u0e97\u0eb5\u0ea1\u0e97\u0eb5\u0ec8\u0ec4\u0e94\u0ec9 \u00a0\u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 1 \u0ec1\u0ea1\u0ec8\u0e99\u0e97\u0eb5\u0ea1 \u0e9b\u0eb0\u0e8d\u0eb8\u0e81\u0ec3\u0e8a\u0ec9 \u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0ec0\u0e87\u0eb4\u0e99\u0ea5\u0eb2\u0e87\u0ea7\u0eb1\u0e992.000.000 \u0e81\u0eb5\u0e9a, \u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 2 \u0ec1\u0ea1\u0ec8\u0e99\u0e97\u0eb5\u0ea1 Passion \u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0ec0\u0e87\u0eb4\u0e99\u0ea5\u0eb2\u0e87\u0ea7\u0eb1\u0e99 1.500.000 \u0e81\u0eb5\u0e9a \u0ec1\u0ea5\u0eb0 \u0ead\u0eb1\u0e99\u0e94\u0eb1\u0e9a 3 \u0ec1\u0ea1\u0ec8\u0e99\u0e97\u0eb5\u0ea1 \u0eaa\u0eb0\u0eab\u0ea7\u0ec8\u0eb2\u0e87\u0eaa\u0eb0\u0ec4\u0eab\u0ea7 \u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0ec0\u0e87\u0eb4\u0e99\u0ea5\u0eb2\u0e87\u0ea7\u0eb1\u0e99 1.000.000 \u0e81\u0eb5\u0e9a.\n\nTRANSLATED As a result of the competition, the first place team was the application team with a prize money of 2,000,000 kip, the second place team was Passion team with a prize money of 1,500,000 kip and the third place team was Sawangsavai with a prize money of 1,000,000 kip.\n****************************************\nORIGINAL \u0e94\u0eb1\u0ec8\u0e87\u0e97\u0eb5\u0ec8\u0eae\u0eb9\u0ec9\u0e99\u0ecd\u0eb2\u0e81\u0eb1\u0e99\u0ec1\u0ea5\u0ec9\u0ea7\u0ea7\u0ec8\u0eb2\u0ec4\u0ea5\u0e8d\u0eb0\u0e9c\u0ec8\u0eb2\u0e99\u0ea1\u0eb2\u0ec0\u0eaa\u0e94\u0e96\u0eb0\u0e81\u0eb4\u0e94-\u0eaa\u0eb1\u0e87\u0e84\u0ebb\u0ea1 \u0e82\u0ead\u0e87 \u0eaa\u0e9b\u0e9b \u0ea5\u0eb2\u0ea7 \u0ec4\u0e94\u0ec9\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7\u0ea2\u0ec8\u0eb2\u0e87\u0e95\u0ecd\u0ec8\u0ec0\u0e99\u0eb7\u0ec8\u0ead\u0e87 \u0ec1\u0ea5\u0eb0 \u0e81\u0eb3\u0ea5\u0eb1\u0e87\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2 \u0ec2\u0e94\u0e8d\u0eaa\u0eb0\u0ec0\u0e9e\u0eb2\u0eb0\u0ec1\u0ea1\u0ec8\u0e99\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7\u0e94\u0ec9\u0eb2\u0e99\u0ead\u0eb8\u0e94\u0eaa\u0eb2\u0eab\u0eb0\u0e81\u0eb3, \u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2\u0e94\u0ec9\u0eb2\u0e99\u0ec2\u0e84\u0e87\u0ea5\u0ec8\u0eb2\u0e87\u0e9e\u0eb7\u0ec9\u0e99\u0e96\u0eb2\u0e99\u0e82\u0ead\u0e87\u0ec0\u0eaa\u0e94\u0e96\u0eb0\u0e81\u0eb4\u0e94\u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0e81\u0eb2\u0e99\u0e9b\u0eb1\u0e9a\u0e9b\u0eb8\u0e87, \u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2\u0e95\u0ebb\u0ea7\u0ec0\u0ea1\u0eb7\u0ead\u0e87, \u0e81\u0eb2\u0e99\u0e82\u0ebb\u0e99\u0eaa\u0ebb\u0ec8\u0e87, \u0e9e\u0eb0\u0ea5\u0eb1\u0e87\u0e87\u0eb2\u0e99\u0ec4\u0e9f\u0e9f\u0ec9\u0eb2\u0ec4\u0e94\u0ec9\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7 \u0ec0\u0eae\u0eb1\u0e94\u0ec3\u0eab\u0ec9\u0e8a\u0eb5\u0ea7\u0eb4\u0e94\u0e81\u0eb2\u0e99\u0ec0\u0e9b\u0eb1\u0e99\u0ea2\u0eb9\u0ec8\u0e82\u0ead\u0e87\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99\u0ea5\u0eb2\u0ea7\u0e9a\u0eb1\u0e99\u0e94\u0eb2\u0ec0\u0e9c\u0ebb\u0ec8\u0eb2\u0e99\u0eb1\u0e9a\u0ea1\u0eb7\u0ec9\u0e94\u0eb5\u0e82\u0eb6\u0ec9\u0e99\u0ec0\u0e97\u0eb7\u0ec8\u0ead\u0ea5\u0eb0\u0e81\u0ec9\u0eb2\u0ea7, \u0e84\u0ebd\u0e87\u0e84\u0eb9\u0ec8\u0e81\u0eb1\u0e9a\u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2 \u0eaa\u0e9b\u0e9b \u0ea5\u0eb2\u0ea7 \u0e81\u0ecd\u0ec4\u0e94\u0ec9\u0e9b\u0eb0\u0ec0\u0e8a\u0eb5\u0e99\u0e81\u0eb1\u0e9a\u0e9a\u0eb1\u0e99\u0eab\u0eb2\u0e97\u0ec9\u0eb2\u0e97\u0eb2\u0e8d\u0eab\u0ebc\u0eb2\u0e8d\u0ea2\u0ec8\u0eb2\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e95\u0ebb\u0ec9\u0e99\u0e9a\u0eb1\u0e99\u0eab\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e88\u0eb2\u0e81\u0e81\u0eb2\u0e99\u0e99\u0eb3\u0ec3\u0e8a\u0ec9\u0e82\u0ead\u0e87\u0e84\u0ebb\u0e99\u0ec0\u0eae\u0ebb\u0eb2\u0e99\u0eb1\u0e9a\u0ea1\u0eb7\u0ec9\u0e99\u0eb1\u0e9a\u0ec0\u0e9e\u0eb5\u0ec8\u0ea1\u0e82\u0eb6\u0ec9\u0e99\u0e88\u0ebb\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0e9c\u0ebb\u0e99\u0e81\u0eb0\u0e97\u0ebb\u0e9a\u0e95\u0ecd\u0ec8\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1\u0ec3\u0e99\u0eab\u0ebc\u0eb2\u0e8d\u0e82\u0ebb\u0e87\u0ec0\u0e82\u0e94.\n\nTRANSLATED \u0e94\u0eb1\u0ec8\u0e87\u0e97\u0eb5\u0ec8\u0eae\u0eb9\u0ec9\u0e99\u0ecd\u0eb2\u0e81\u0eb1\u0e99\u0ec1\u0ea5\u0ec9\u0ea7\u0ea7\u0ec8\u0eb2\u0ec4\u0ea5\u0e8d\u0eb0\u0e9c\u0ec8\u0eb2\u0e99\u0ea1\u0eb2\u0ec0\u0eaa\u0e94\u0e96\u0eb0\u0e81\u0eb4\u0e94-\u0eaa\u0eb1\u0e87\u0e84\u0ebb\u0ea1 \u0e82\u0ead\u0e87 \u0eaa\u0e9b\u0e9b \u0ea5\u0eb2\u0ea7 \u0ec4\u0e94\u0ec9\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7\u0ea2\u0ec8\u0eb2\u0e87\u0e95\u0ecd\u0ec8\u0ec0\u0e99\u0eb7\u0ec8\u0ead\u0e87 \u0ec1\u0ea5\u0eb0 \u0e81\u0eb3\u0ea5\u0eb1\u0e87\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2 \u0ec2\u0e94\u0e8d\u0eaa\u0eb0\u0ec0\u0e9e\u0eb2\u0eb0\u0ec1\u0ea1\u0ec8\u0e99\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7\u0e94\u0ec9\u0eb2\u0e99\u0ead\u0eb8\u0e94\u0eaa\u0eb2\u0eab\u0eb0\u0e81\u0eb3, \u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2\u0e94\u0ec9\u0eb2\u0e99\u0ec2\u0e84\u0e87\u0ea5\u0ec8\u0eb2\u0e87\u0e9e\u0eb7\u0ec9\u0e99\u0e96\u0eb2\u0e99\u0e82\u0ead\u0e87\u0ec0\u0eaa\u0e94\u0e96\u0eb0\u0e81\u0eb4\u0e94\u0ec4\u0e94\u0ec9\u0eae\u0eb1\u0e9a\u0e81\u0eb2\u0e99\u0e9b\u0eb1\u0e9a\u0e9b\u0eb8\u0e87, \u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2\u0e95\u0ebb\u0ea7\u0ec0\u0ea1\u0eb7\u0ead\u0e87, \u0e81\u0eb2\u0e99\u0e82\u0ebb\u0e99\u0eaa\u0ebb\u0ec8\u0e87, \u0e9e\u0eb0\u0ea5\u0eb1\u0e87\u0e87\u0eb2\u0e99\u0ec4\u0e9f\u0e9f\u0ec9\u0eb2\u0ec4\u0e94\u0ec9\u0ea1\u0eb5\u0e81\u0eb2\u0e99\u0e82\u0eb0\u0eab\u0e8d\u0eb2\u0e8d\u0e95\u0ebb\u0ea7 \u0ec0\u0eae\u0eb1\u0e94\u0ec3\u0eab\u0ec9\u0e8a\u0eb5\u0ea7\u0eb4\u0e94\u0e81\u0eb2\u0e99\u0ec0\u0e9b\u0eb1\u0e99\u0ea2\u0eb9\u0ec8\u0e82\u0ead\u0e87\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99\u0ea5\u0eb2\u0ea7\u0e9a\u0eb1\u0e99\u0e94\u0eb2\u0ec0\u0e9c\u0ebb\u0ec8\u0eb2\u0e99\u0eb1\u0e9a\u0ea1\u0eb7\u0ec9\u0e94\u0eb5\u0e82\u0eb6\u0ec9\u0e99\u0ec0\u0e97\u0eb7\u0ec8\u0ead\u0ea5\u0eb0\u0e81\u0ec9\u0eb2\u0ea7, \u0e84\u0ebd\u0e87\u0e84\u0eb9\u0ec8\u0e81\u0eb1\u0e9a\u0e81\u0eb2\u0e99\u0e9e\u0eb1\u0e94\u0e97\u0eb0\u0e99\u0eb2 \u0eaa\u0e9b\u0e9b \u0ea5\u0eb2\u0ea7 \u0e81\u0ecd\u0ec4\u0e94\u0ec9\u0e9b\u0eb0\u0ec0\u0e8a\u0eb5\u0e99\u0e81\u0eb1\u0e9a\u0e9a\u0eb1\u0e99\u0eab\u0eb2\u0e97\u0ec9\u0eb2\u0e97\u0eb2\u0e8d\u0eab\u0ebc\u0eb2\u0e8d\u0ea2\u0ec8\u0eb2\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e95\u0ebb\u0ec9\u0e99\u0e9a\u0eb1\u0e99\u0eab\u0eb2\u0eaa\u0eb4\u0ec8\u0e87\u0ec0\u0eaa\u0e94\u0ec0\u0eab\u0ebc\u0eb7\u0ead\u0ec3\u0e8a\u0ec9 \u0e88\u0eb2\u0e81\u0e81\u0eb2\u0e99\u0e99\u0eb3\u0ec3\u0e8a\u0ec9\u0e82\u0ead\u0e87\u0e84\u0ebb\u0e99\u0ec0\u0eae\u0ebb\u0eb2\u0e99\u0eb1\u0e9a\u0ea1\u0eb7\u0ec9\u0e99\u0eb1\u0e9a\u0ec0\u0e9e\u0eb5\u0ec8\u0ea1\u0e82\u0eb6\u0ec9\u0e99\u0e88\u0ebb\u0e99\u0eaa\u0ebb\u0ec8\u0e87\u0e9c\u0ebb\u0e99\u0e81\u0eb0\u0e97\u0ebb\u0e9a\u0e95\u0ecd\u0ec8\u0eaa\u0eb4\u0ec8\u0e87\u0ec1\u0ea7\u0e94\u0ea5\u0ec9\u0ead\u0ea1\u0ec3\u0e99\u0eab\u0ebc\u0eb2\u0e8d\u0e82\u0ebb\u0e87\u0ec0\u0e82\u0e94.\n\n-\u00a0text 0, 1, and 3 are correct\n- text 2 is partially translated\n- text 4 are not translated at all",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Asynch Events",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-12T13:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Asynch-Events\/m-p\/611714#M2347",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":38,
        "Question_body":"Has anyone had to solve for the lack of Asynch Events in Dialogflow CX? These events existed in ES, but were not migrated to CX.\n\nThe Use Case is a request from the virtual agent to the contact (Voice or Chat) that may take the user longer than the webhook timeout, maybe as long as several minutes. The virtual agent should just wait until triggered by the user action that the request is completed.\n\nWe've tried several ways of accomplishing this, but have not come up with a solution.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI Warehouse UI administration unreachable",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-29T10:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Warehouse-UI-administration-unreachable\/m-p\/538454#M1525",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":111,
        "Question_body":"Hi,\n\nI have completed setup of Document AI Warehouse and I now want to go to the Admin page.\n\nIn console I have a link like\u00a0https:\/\/documentwarehouse.cloud.google.com\/projects\/xxxxxx but when I click it, it either opens a new browser tab that is logged into GCP with another account or it opens the docs page.\n\nIn any event, I am unable to login.\n\nPlease help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX can't connect to integrations",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-28T20:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-can-t-connect-to-integrations\/m-p\/538170#M1519",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":103,
        "Question_body":"my\u00a0 text messenger integrations (LINE, Facebook) does not receive a response message from Dialogflow CX but has no problem on Dialogflow CX test agent simulator and Dialogflow ES",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Batch prediction on custom model",
        "Question_tag_count":3,
        "Question_created_time":"2022-07-13T11:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Batch-prediction-on-custom-model\/m-p\/442147#M414",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi,\n\nI used custom containers for training and prediction to create a model on Vertex AI. Now I want to run batch prediction against it but get error message that says \"Unable to start batch prediction job due to the following error: A model using a third-party image must specify PredictRoute and HealthRoute in ContainerSpec.\"\n\nI checked documentation,\u00a0AIP_HEALTH_ROUTE =\u00a0\/v1\/endpoints\/ENDPOINT\/deployedModels\/DEPLOYED_MODEL\n\nDoes this mean that the model has to be deployed to an endpoint in order to generate\u00a0the value of the AIP_ENDPOINT_ID variable?\n\nHowever, the documentation \u201cGet batch predictions\u201d says \u201cRequesting a batch prediction is an asynchronous request (as opposed to online prediction, which is a synchronous request). You request batch predictions directly from the model resource; you don't need to deploy the model to an endpoint.\n\nI am confused whether in my situation, the model has to be deployed first. Also, is there any resources regarding hosting custom models for batch predictions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard: The Next Generation of AI",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-06T11:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-The-Next-Generation-of-AI\/m-p\/600583#M2088",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":29,
        "Question_body":"Hey guys,\n\nI published an article on Bard with everything you should know about Google's collaborative AI service that will help boost your productivity, and bring your ideas to life.\n\nYou can check it out here.\n\nGoogle Bard: The Next Generation of AI (hashnode.dev)\n\nhttps:\/\/emmanuelogebe.hashnode.dev\/google-bard-the-next-generation-of-ai",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Internal error (code 13) when tuning a model in Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-25T09:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Internal-error-code-13-when-tuning-a-model-in-Vertex-AI\/m-p\/615702#M2457",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":10,
        "Question_body":"When processing a model tuning in Vertex AI, during the pipeline creation, i recieved an Internal error message.:\n\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects\/xxxxxxxxx\/locations\/xxxxx-xxxxx\/pipelineJobs\/tune-large-model-xxxxxxxxxxx current state:\nPipelineState.PIPELINE_STATE_PENDING\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-591a9a561b04> in <cell line: 4>()\n      2 vertexai.init(project=\"xxxxxxxxx\", location=\"us-central1\")\n      3 model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n----> 4 model.tune_model(\n      5     training_data=training_data,\n      6     train_steps=100,\n\n3 frames\n\/usr\/local\/lib\/python3.10\/dist-packages\/google\/cloud\/aiplatform\/pipeline_jobs.py in _block_until_complete(self)\n    500         # JOB_STATE_FAILED or JOB_STATE_CANCELLED.\n    501         if self._gca_resource.state in _PIPELINE_ERROR_STATES:\n--> 502             raise RuntimeError(\"Job failed with:\\n%s\" % self._gca_resource.error)\n    503         else:\n    504             _LOGGER.log_action_completed_against_resource(\"run\", \"completed\", self)\n\nRuntimeError: Job failed with:\ncode: 13\nmessage: \"Internal error encountered. Please try again\"\n\nThe error doesn't show more details, so i can't figure out what's wrong, any ideas about what is wrong?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text recognition language (and numbers only)",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-01T00:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-recognition-language-and-numbers-only\/m-p\/494929#M891",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":148,
        "Question_body":"Dear Fellow Developers,\n\nI have 2 issues with the Google Vision API for handwriting recognition.\n\nFirst of all, I don't see an option to specify the recognition language. Sometimes it detects numbers as Russian characters. And second: sometimes I only want to recognize numbers. Is there a setting for this?\n\nI use a PHP library (Google\\Cloud\\Vision\\V1\\ImageAnnotatorClient) for handwriting recognition in an image. I get my result after calling the\u00a0documentTextDetection() function, like this:\n$response->getFullTextAnnotation()->getText()",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Retail - Tag Manager Setup",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-04T15:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Retail-Tag-Manager-Setup\/m-p\/540701#M1583",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":112,
        "Question_body":"We've been trying to setup tag manager to get events flowing into our cloud retail project. It feels like I'm stuck on step 0, which is setting up the Tag correctly. There's a step that asks for the project number but when I hover over it, it actually says it's the project ID. I'm not sure what to put in here, the Project ID of our cloud project or the Project Number of the cloud project which is available on the project dashboard.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AUTHORIZATION RELATED ERROR using TTS API",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-20T05:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AUTHORIZATION-RELATED-ERROR-using-TTS-API\/m-p\/545629#M1716",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"Hi,\n\nI am using the following code to see if i can use the text to speech api. I am getting a consistent 401 authorization error. I plan to use google cloud text to speech in my existing web app, which was originally using web speech api. I am unable to get the google api to work. Please assist. Thanks. Here is the code I am using:\n\n<!DOCTYPE html>\n<html>\n<head>\n<title>Text-to-Speech Example<\/title>\n<script src=\"https:\/\/code.jquery.com\/jquery-3.6.0.min.js\"><\/script>\n<\/head>\n<body>\n<h1>Text-to-Speech Example<\/h1>\n<textarea id=\"inputText\" rows=\"4\" cols=\"50\"><\/textarea>\n<button onclick=\"synthesize()\">Synthesize<\/button>\n<audio id=\"audio\" controls><\/audio>\n<script>\nfunction synthesize() {\nvar text = document.getElementById('inputText').value;\n$.ajax({\nurl: 'https:\/\/texttospeech.googleapis.com\/v1\/text:synthesize',\ntype: 'POST',\ndata: JSON.stringify({\ninput: { text: text },\nvoice: { languageCode: 'en-US', ssmlGender: 'FEMALE' },\naudioConfig: { audioEncoding: 'MP3' }\n}),\nheaders: {\n'Content-Type': 'application\/json',\n'Authorization': 'Bearer THIS IS WHERE MY API KEY GOES'\n},\nsuccess: function (data) {\nvar audioUrl = 'data:audio\/mp3;base64,' + data.audioContent;\ndocument.getElementById('audio').src=audioUrl;\ndocument.getElementById('audio').play();\n},\nerror: function (error) {\nconsole.error('Error:', error);\n}\n});\n}\n<\/script>\n<\/body>\n<\/html>",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Hello NON-engineer here...... How to view previous VM instance history",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-29T06:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Hello-NON-engineer-here-How-to-view-previous-VM-instance-history\/m-p\/538307#M1521",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":34,
        "Question_body":"Hello as the title suggest I am not an engineer. But I hired a freelancer a while ago to create a computer vision model for me and then after a couple of weeks he just stopped responding to me. The last thing he told me was the computer vision model was working great as he was training it in google cloud. But as I mentioned he then just abruptly stopped answering all my forms of communication. So I see that he created a VM instance in google cloud is there any way to retrieve the model\/dataset or anything that he was working on in my google cloud account?\n\nFYI: I granted him administrator access so he logged into my google cloud so that I would pay for the usage fees but like I said I listed his email under administrator so that he can log in and out freely.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model adaptation - Speech-to-Text - GA?",
        "Question_tag_count":1,
        "Question_created_time":"2021-10-19T08:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-adaptation-Speech-to-Text-GA\/m-p\/173368#M62",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":439,
        "Question_body":"Hello,\n\nI'd like to know if Model adaptation feature is ready to use in production (custom classes, phrase sets, etc). Official web documentation (https:\/\/cloud.google.com\/speech-to-text\/docs\/model-adaptation) says it is a preview feature (Pre-GA). Also, REST resources are inside namespace v1p1beta1 (https:\/\/cloud.google.com\/speech-to-text\/docs\/reference\/rest).\n\nOn the other hand, release notes web page (https:\/\/cloud.google.com\/speech-to-text\/docs\/release-notes#May_07_2021) says \"The Speech-to-Text model adaptation\u00a0 feature is now a GA feature\".\n\nThank you very much,\n\nPablo Gomez",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Short polish inputs recognized but not returned by ASR.",
        "Question_tag_count":2,
        "Question_created_time":"2022-09-22T01:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Short-polish-inputs-recognized-but-not-returned-by-ASR\/m-p\/469439#M590",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"Hi,\n\n\u00a0\nWe're currently working with dialogflow CX and everything's running smoothly, except for the fact that whenever we answer with short polish words, e.g. \"tak\", \"nie\", the ASR system or STT recognizes it, but doesn't return it. Instead, it waits as if the client was still talking, once you repeat the phrase for some time or input a longer phrase, it returns it almost instantly.\u00a0\n\u00a0\nAny ideas of the fix? Has anybody experienced anything similar?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Calling vertex API for embeddings in asia region does not work",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-14T12:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Calling-vertex-API-for-embeddings-in-asia-region-does-not-work\/m-p\/612417#M2374",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"I'm trying to use Vertex AI to generate embeddings.\n\nMaking a request to\u00a0 POST https:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/PROJECT_ID\/locations\/us-central1\/publishers\/google\/models\/textembedding-gecko:predict works as per this documentation.\n\nHowever, I can't seem to retrieve embeddings from a model running in a region closer to asia.\n\nI have tried the following API call, POST https:\/\/asia-east1-aiplatform.googleapis.com\/v1\/projects\/PROJECT_ID\/locations\/asia-east1\/publishers\/google\/models\/textembedding-gecko:predict\u00a0but I\u00a0get a 404 error saying\u00a0Publisher Mode not found\n\nDo I need to do something to enable Vertex AI API in other regions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Make deep learning VM JupyterLab publicly available?",
        "Question_tag_count":1,
        "Question_created_time":"2022-01-27T11:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/m-p\/386576#M191",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":149,
        "Question_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Question_closed_time":"01-31-2022 01:58 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible\/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Dialogflow ES Bad update mask fields",
        "Question_tag_count":2,
        "Question_created_time":"2023-03-01T05:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-Bad-update-mask-fields\/m-p\/527470#M1362",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hello everyone,\n\nI am encountering an error while trying to update the text_to_speech_settings.enable_text_to_speech parameter using the Dialogflow API. The error message I'm getting is \"Bad update mask fields: [text_to_speech_settings.enable_text_to_speech]\".\n\nI'm not sure what is causing this error, and I've already checked my API request and made sure that the resource name and update mask are correct. Can someone please help me troubleshoot this issue?\n\nHere are some additional details about my setup:\n\n\u00a0\n\nThank you in advance for your help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Tabular Forecasting Model in Vertex AI - Cannot deploy model to endpoint",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-08T03:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Tabular-Forecasting-Model-in-Vertex-AI-Cannot-deploy-model-to\/m-p\/475893#M631",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":193,
        "Question_body":"I'm getting started with Vertex AI, and the model I'd like to use is a Tabular Forecasting Model. After spending hours tweaking the model that I wanted to deploy, I came across this error message. \"The default version cannot be deployed\". I can deploy a normal Tabular model to an endpoint, but not the Tabular Forecasting model.\n\n\u00a0\n\nDoes anyone know if there is a way to deploy a Tabular Forecasting Model? If not, is Google planning on adding this functionality anytime soon?\n\n\u00a0\n\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"cloud vision text coordinates format",
        "Question_tag_count":2,
        "Question_created_time":"2021-09-15T23:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cloud-vision-text-coordinates-format\/m-p\/170059#M49",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":381,
        "Question_body":"Hi Team,\n\u00a0\nI was going through the vision api document https:\/\/cloud.google.com\/vision\/docs\/pdf to understand the format of the coordinates of the text.\u00a0 I was a bit confused regarding the normalized values.\n\u00a0\nCould you please clarify on how to find the original coordinates with respect the dimension of the document.\n\u00a0\nThanks,\nArun",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"NLP is hard",
        "Question_tag_count":2,
        "Question_created_time":"2021-08-15T22:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NLP-is-hard\/m-p\/167242#M38",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":358,
        "Question_body":"Who else thinks NLP is the hardest subset of AI to build?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Total Novice",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-10T19:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Total-Novice\/m-p\/421957#M323",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":70,
        "Question_body":"Friends,\n\nCan I submit a file for conversion from speech to text without having to learn computer coding - even if it is at a very simple level? Can I just submit the file somewhere for transcription?\n\nThank You,\n\nJust, simply, a consumer",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create a Tuned model - No response",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-14T17:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-a-Tuned-model-No-response\/m-p\/612495#M2375",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":97,
        "Question_body":"Hi\u00a0\n\nI am trying to follow the youtube video for parameter based tuning - I created a .jsonl file with the input_text and output_text data and uploaded the same as in the Created tuned model Step - 1 - uploaded this file to the Bucket and selected the dataset location. This file was processed without any errors and the flow went to the #2 step - Model details - i provided a model name, adjusted the train steps and learning rate with the base model pre-populated as text-bison and finally selected the destination work directory.\n\nThe Start Tuning button becomes active as soon as i complete all the steps - but after i click on that one - there is a blip and then nothing happens - i do not get a success or failure dialog as shown in the youtube video.\n\nNot sure what i am missing here - any help will be highly appreciated.\n\nThis is the generated code\n\n\u00a0\n\nfrom google.cloud import storage\nimport vertexai\nfrom vertexai.preview.language_models import TextGenerationModel\n\ndataset = \"\"\"my .jsonl text data goes here with the correct format of input_text and output_text\"\"\"\n\n#Uploads dataset to cloud storage\nstorage_client = storage.Client()\nbucket = storage_client.bucket(\"cloud-ai-platform-MY_BUCKETID\")\nblob = bucket.blob(\"input.jsonl\")\nblob.upload_from_string(dataset)\ntraining_data = \"gs:\/\/cloud-ai-platform-MY_ID\/input.jsonl\"\n\nvertexai.init(project=\"PROJECT_ID\", location=\"us-central1\")\nmodel = TextGenerationModel.from_pretrained(\"text-bison@001\")\nmodel.tune_model(\n    training_data=training_data,\n    train_steps=100,\n    tuning_job_location=\"europe-west4\",\n    tuned_model_location=\"us-central1\",\n)\n\n\u00a0\n\nsomewhere in the documentation they have mentioned that Preview happens only in europe-west4 and if i get a 500 error i need to run a piece of code - in my case i do not get 500 or internal server error and the generated code also shows the tuning job location is europe-west4.\n\nany pointers around why this is not working and what am i missing or the youtube video is missing as i am trying to follow it step by step.\n\nThanks in advance\n@kvandres\u00a0- tagging you on this post as you helped someone on a related issue .\n\nSai",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text to speech data residency",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-05T12:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-speech-data-residency\/m-p\/474727#M621",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":121,
        "Question_body":"Compliance team is asking for Data residency for text to speech data.\u00a0\n\n\n\nOn Google website, it says,\n\n\u201cText-to-Speech is both stateless and resourceless. This means Data Access and System Event data don't apply. As a result, Text-to-Speech is out of the scope of Client Access Licenses (CAL). Google does not log any customer Text-to-Speech text or audio data.\u201d\n\nDoes logging here refer to data storage. We do not want to store any data in the cloud.\n\n\u00a0\n\nRegards,",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to set multiple series identifier columns on tabular forecast?",
        "Question_tag_count":1,
        "Question_created_time":"2021-08-11T22:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-set-multiple-series-identifier-columns-on-tabular\/m-p\/166959#M37",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":237,
        "Question_body":"Hello,\n\nI tried BigQuery ML's ARIMA+ to predict sales data, but the results were not particularly good.\n\nSo, I wanted to try adding weather as a feature to the dataset. This requires the use of Vertex AI Tabular forecast (AutoML).\n\nThe dataset looks like this.\n\ndate, store, product, total_amount_sold, temperature, is_rainy\n\nWhen using ARIMA+, multiple columns can be specified by using the following statement.\n\nTIME_SERIES_ID_COL = ['store', 'product']\n\n\u00a0How to set multiple series identifier columns on AutoML?\u00a0Should I consider merging the store and product columns into one column(eg: tokyo_pixel6)?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error with \"import vertexai\"",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-11T22:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-with-quot-import-vertexai-quot\/m-p\/552708#M1877",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi,\n\nI'm trying to use GCP: Vertex Ai. I already installed the Vertex AI SDK for Python package successfully.\n\nI can \"from google.cloud import aiplatform\" successfully. However, \"import vertexai\" caused error as follows: No module named 'vertexai'\n\nI also already installed the module vertexai in the linux machine successfully.\n\nWould you help to show me what caused the error? Thanks so much in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Introducing Bard: An experimental conversational AI service",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-06T13:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Introducing-Bard-An-experimental-conversational-AI-service\/m-p\/518901#M1224",
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":0,
        "Question_body":"Have you heard about\u00a0Bard?\u00a0Announced today, Bard is an experimental conversational AI service powered by Google's latest version of LaMDA (Language Model for Dialogue Applications), which draws from the web to provide fresh, high-quality responses.\u00a0\n\nLearn more here and let us know your thoughts in the comments. We'd love to hear from you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML : Unable to get the Details of Native Models",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-04T02:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Unable-to-get-the-Details-of-Native-Models\/m-p\/540509#M1578",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":165,
        "Question_body":"We are using Auto ML Client libraries(C# and PHP) for listing the AutoML custom Translation models.\nBut with the API's\n\nAutoMlClient.ListModels(parent);\n$autoMlClient->listModels($formattedParent);\n\nwe are only able to list the Legacy models, Native models details are missing from the response. Is there any other API's that can be used to list Native models?\n\nIn console GCP suggests to use Cloud Translation API for managing the native models, but in that library, there are no API's to list the models",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Working with Context for different intent",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-19T10:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Working-with-Context-for-different-intent\/m-p\/468253#M585",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":156,
        "Question_body":"Hello,\ni'm really new in DialogFlow and would like to dig deep my knowledge about this topic.\nMy question right now is about context. So for my case, i build a Conversational Bot for a Fitness Center and trying to create intent related to an individual's program goal\n\nSo for the training phrases would be\n\n\"i want to get ideal weight\"\n\"i want to build muscle\"\n\"i want to be more healthy\"\n\"i want to lose weight\"\n\"i want to gain weight\"\n\nI Called the entity \"Individual-goal\"\nTha Output Context for this is \"Fitness-Goal\"\n\nNow for my question:\ni would like to segment OR create the \"Sub-Context\" for this Fitness Goal, in following category:\nHealth - Gain Weight\nHealth - Lose Weight\nHealth - General\nFitness - Muscle Building\nFitness - General\n\nFor this case:\n1. Is it better for me to create multiple Intent ?\n2. Is there a way to put a context based on the response, like \"IF Individual goal contain 'gain weight' then Output Context set to \"Health - Gain Weight\"\n\nWhats the best scenario for this ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Request-Response Logging Not Working on Vertex AI Endpoints",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-31T08:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Request-Response-Logging-Not-Working-on-Vertex-AI-Endpoints\/m-p\/598682#M2055",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":116,
        "Question_body":"I'm having trouble with request-response logging on Vertex AI endpoints. Previously, with AI Platform endpoints, I was able to enable request-response logging via gcloud, which would then store the request-response data in a chosen BigQuery table. However, with the newer Vertex AI endpoints, the process requires using the REST API to enable request-response logging.\n\nI followed the official documentation and made a PATCH request with the structure provided. Despite receiving a confirmation that the PATCH request was successful (as it displayed the updated configuration for the specified endpoint), the request-response logs are not appearing in BigQuery.\n\nWith the AI Platform endpoints, enabling logging resulted in a corresponding BigQuery insert job for every prediction request. However, with Vertex AI endpoints, no such job is created even after enabling the logging.\n\nHere's the PATCH request I used:\n\n\n\ncurl -X PATCH \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application\/json\" \\\n\"https:\/\/$REGION-aiplatform.googleapis.com\/v1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID\"\n  -d '{\n      \"predict_request_response_logging_config\": {\n      \"enabled\": true,\n        \"sampling_rate\": 1,\n        \"bigquery_destination\": {\n          \"output_uri\": \"bq:\/\/PROJECT_ID.DATASET_NAME.TABLE_NAME\"\n        }\n      }\n  }'\n\n\u00a0\n\nI've also tried using gcloud ai and gcloud beta ai-platform versions update commands, but these do not recognize\/find the endpoint. Furthermore, I have also created the endpoint with the above config using a POST request, which also did not work.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"only getting error_validation and not results when using vertex for forecasting",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-04T02:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/only-getting-error-validation-and-not-results-when-using-vertex\/m-p\/609090#M2283",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":60,
        "Question_body":"i have been trying to get batch predicitions to a dataset i am only getting error-validation but no predictions can someone help me out asap?\n\nThe date format is according to the format specified in the document",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Retail User events from Google Tag Manager",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-18T07:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Retail-User-events-from-Google-Tag-Manager\/m-p\/554585#M1953",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":97,
        "Question_body":"I am reaching out to seek your assistance and guidance regarding an issue I have been experiencing while trying to send user events to Cloud Retail using Google Tag Manager.\n\nI have invested a significant amount of time searching for information in the community and other online forums, as well as exploring the provided documentation and videos. However, despite my efforts and multiple attempts, I have been unable to make this functionality work properly. I have come across responses that do not provide exact details about the underlying cause of the problem, making it even more challenging to find a solution.\n\nFurthermore, I have noticed that the documentation and videos do not adequately explain or provide a clear visualization of how the sent events are ultimately reflected in Cloud Retail. This lack of precise information hinders my ability to identify potential errors and follow the appropriate recommendations.\n\nI have conducted numerous thorough tests and tried every possible configuration, but the events simply do not get registered in Cloud Retail. Without being able to access this information, it is extremely difficult for me to gather data and obtain valuable recommendations to proceed with my implementation.\n\nTherefore, I would like to request your valuable assistance and guidance in this matter. Could you please provide additional support to address this specific issue? I would greatly appreciate any assistance you can offer to help me overcome these challenges and achieve a successful implementation of user events in Cloud Retail.\n\nThank you in advance for your kind consideration and help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX - DTMF on intents",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-24T07:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-DTMF-on-intents\/m-p\/615107#M2441",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":33,
        "Question_body":"I have Google Dialogflow CX integrated with the Cisco contact center. I like to use utterances and also DTMF on the menus. Doing it on Dialogflow ES was easy by entering the right digit under the training phrase. But I am unable to achieve the same result in CX by entering the desired digit under the training phrase. It looks like I can enable DTMF only under parameters, not able to use it with intents. Anyone has done this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Empty Search result from previously Indexed Product set",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-29T21:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Empty-Search-result-from-previously-Indexed-Product-set\/m-p\/494580#M878",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"Hi everyone,\nI have a problem with vision product search. Will the index become invalid after being idle for a period of time, and will there be charges for searching under the invalid index? And how long will the index become invalid after being idle?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text-to-Speech seems to ignore SSML input",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-17T08:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-Speech-seems-to-ignore-SSML-input\/m-p\/179629#M117",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":268,
        "Question_body":"Greetings, all! Since getting started with the TTS service, I have had good success with submitting JSON files that specify simple text input. I am using the instructions for PowerShell as described here:\n\nhttps:\/\/cloud.google.com\/text-to-speech\/docs\/quickstart-protocol#windows\n\nWhen submitting JSON files that specify SSML input, however, it seems that some of the SSML elements are being ignored by the speech synthesizer. I'd like to use the\u00a0<prosody> and <emphasis> elements, but the output isn't reflecting the values I specified. Here's an example:\n\n{\n  \"input\":{\n    \"ssml\":\"<speak><prosody rate=\\\"fast\\\" pitch=\\\"low\\\"><emphasis level=\\\"strong\\\">Guten Tag!<\/emphasis> Sie sind mit dem Anrufbeantworter verbunden. Gerne k\u00f6nnen Sie uns nach dem Signal-Ton eine Nachricht hinterlassen. <emphasis level=\\\"strong\\\">Vielen Dank f\u00fcr Ihren Anruf!<\/emphasis><\/prosody><\/speak>\"\n  },\n  \"voice\":{\n    \"languageCode\":\"de-DE\",\n    \"name\":\"de-DE-Wavenet-A\",\n    \"ssmlGender\":\"FEMALE\"\n  },\n  \"audioConfig\":{\n    \"audioEncoding\":\"MP3\"\n  }\n}\n\nIt doesn't seem to matter how I specify the\u00a0rate and\u00a0pitch attributes\u2014the output comes back with no alteration.\n\nThank you for taking the time to read this. If you have information or suggestions, please reply with your ideas!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Vision model change",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-13T05:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-model-change\/m-p\/178017#M114",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":127,
        "Question_body":"Hello,\n\n\u00a0\n\nThe changes for the Vision API's model from latest -> stable and stable -> legacy are scheduled around Dec 30th\/Jan 1st. Is there a more concrete date and time for this planned change?\n\nI'd like to use the currently stable model for the time being, which would involve switching from \"stable\" to \"legacy\". Since this could have an impact on some environments, I will need to make this swap shortly after the model references are being changed.\n\n\u00a0\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to assign specialist for specialist",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-27T09:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-assign-specialist-for-specialist\/m-p\/447380#M450",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"Hello,\n\nI configured Document AI processor with HITL, No filter(self-validate). Using a python code, I am sending specific document to hitl queue to be processed by a specialist.\n\nI can clearly see that there are documents to be reviewed as \"Queued for review\" column with 2 documents.\n\nI also configured the specialist assignment assigning to all tasks (P0, audit, P1) to all the available specialists as it is shown in the image:\n\nHowevere, accesing to specialist platform, I cannot see any of the documents in the queue.\n\nWhat am I missing here?\n\nThanks for your help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Out of the box solution for regression problem to get daily prediction",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-29T19:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Out-of-the-box-solution-for-regression-problem-to-get-daily\/m-p\/548817#M1766",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":30,
        "Question_body":"hi team , im new to GCP.\n\nwant to know is there readily available service \/product to work on regression problem to get prediction at day level. Or any combination that can be used\u00a0\n\nsituation is Historical data\u00a0 do not have consistency at a day level , but it does have consistent pattern at month level. how to leverage such data to ensure day level prediction accuracy above 80",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Datasets & batch predictions",
        "Question_tag_count":2,
        "Question_created_time":"2022-07-24T13:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Datasets-amp-batch-predictions\/m-p\/446346#M443",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":441,
        "Question_body":"Hi all,\n\nI just started to play with Vartex AI. I am working with \"Tabular\" - \"Forecasting\" and currently struggling with few things and i hope you can help me in order i can continue. I tried to organize my questions to three categories:\n\n1) Datasets for training:\u00a0\n\n\u00a0 \u00a0 \u00a0a) \"series identifier\" define to which time series data are belonging ... lets assume that i have two series identifier - one is called \"A\" and one is called \"B\". Does this means that AI treats them as completely separate and noncorrelated - this means any data that belongs to series A don't have any correlation to B, right? This give me possibility to train different dataset with one shot right? Otherwise i would need to make (in my case) two trainings - one for A and one for B.\n\n2) Training new model --> Model details\n\na) Is possible to predict more then one target column?\u00a0\n\nb) lets assume that my dataset data granularity is 1 day. Can i use data granularity of \"5min\" for Forecast configuration or can this setup decrease quality of my forecast? Should it be more correct to use already at beginning lets say dataset granularity of 5 minute and afterwards it could be more flexible when setting data granularity for forecast configuration without influencing forecast quality?\n\nc) If I set Forecast horizon of 7 and context window 30, does this means that this setting limit my forecast to maximum 7 time steps and requesting always exactly 30 time steps of historical data as input when forecasting on existing trained model?\n\n3) Batch predictions\n\na) Batch Source file\n\nLets assume that i have data with 15 columns from which one is \"serial identifier\" , one is \"time step\" - actually date and one of those columns is target column. Rest of 12 columns are used as influencer and used to train my model. I know that i need to have same structure for batch source file - i read that i can use same file as i used for training, but i just need to add in my case a 2x7 new rows with adding 7 dates and serial identification (in my case 7x A and 7xB) and target column need to be empty. But what should i do with data of rest 12 columns? Do i need manually to enter data for those new rows (2x7) of those 12 columns which values should be for future? But what if i don't have those data? Does this means i cannot do prediction?\n\nI tried to make prediction without those future data and i got following message:\n\n\"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\"\n\nI hope you can help me with above questions.\n\n\u00a0\n\nTnx in advance!\n\n\u00a0\n\nRegards,\n\n\u00a0\n\nArny",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"OCR text recognition for changing images.",
        "Question_tag_count":2,
        "Question_created_time":"2023-01-17T16:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/OCR-text-recognition-for-changing-images\/m-p\/511081#M1077",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":235,
        "Question_body":"Hi,\u00a0\n\nI'm new to Cloud vision and am looking into how I can Scan an image and detect a specific Job Number E.g. 28379 that has a unique colour. We have images of steel which have lots of text and we figured if we write the Job number in a different colour, we could extract only that information to then rename the images to this job number and if there is multiple of the same job number append the data with a (1),(2),(3) etc on the end of each image. My goal is to automate renaming a full folder of photos based of the Job number detected in each image.\u00a0\n\nWe are using Power Automate for most of our automation process, so ideally we would like to access the Cloud via Power Automate (Text detection via API Gateway key to access Google Cloud SDK Shell).\u00a0\n\nDoes anyone know how to do this or any advice\/tips on how to get this working.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to determin which GCP VM do I need for ML",
        "Question_tag_count":2,
        "Question_created_time":"2022-07-26T13:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/m-p\/447075#M448",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":112,
        "Question_body":"Hi to all\n\nIm trying to run a procedure looking to reduce the number of features for a model.\n\nThe first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM\u00a0n1-highmem-8 that has:\u00a0\n\nGPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nand still not getting the process done.\n\nThe question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Question_closed_time":"08-01-2022 10:31 AM",
        "Answer_score_count":2.0,
        "Answer_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Version 2 model in natural language API",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-01T11:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Version-2-model-in-natural-language-API\/m-p\/484641#M711",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":258,
        "Question_body":"Hi, could anyone share the python code on how to get\u00a0 natural language API to use version 2 classify text\u00a0 categories?\n\nI can get it working well with the default (version 1) categories but can't figure out where to adapt the standard code (as here: https:\/\/cloud.google.com\/natural-language\/docs\/samples\/language-classify-text-tutorial-classify?hl=e...)\u00a0 to\u00a0 use model version 2.\n\nMany thanks",
        "Question_closed_time":"11-02-2022 04:27 PM",
        "Answer_score_count":0.0,
        "Answer_body":"From the Classifying Content guide, you can include classification_model_options\u00a0within the request\u00a0dictionary argument to the classify_text()\u00a0function. In these options, you can define the model and version to use for content categories.\n\n\/\/ ...\ncontent_categories_version = (\n        language_v1.ClassificationModelOptions.V2Model.ContentCategoriesVersion.V2) \/\/ Assigning the v2 model type\n    response = client.classify_text(request = {\n        \"document\": document,\n        \"classification_model_options\": {\n            \"v2_model\": {\n                \"content_categories_version\": content_categories_version\n            }\n        }\n    })\n\/\/ ...\n\n\nYou can also check ClassificationModelOptions\u00a0reference for available options.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"AutoML - pre-trained models?",
        "Question_tag_count":1,
        "Question_created_time":"2021-07-01T12:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-pre-trained-models\/m-p\/162864#M7",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":593,
        "Question_body":"I know that for AutoML, the user has to train the model. But are there existing \"pre-trained\" models that you can leverage to identify sentiments or classifications like profanity, irony, and bullying?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocAI Warehouse UI provision problem",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-04T14:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Warehouse-UI-provision-problem\/m-p\/518134#M1205",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":183,
        "Question_body":"hi. have the problem with\u00a0Provision DocAI Warehouse. I can complete all 4 steps but on the last step when all is don and I see\n\nDocAI Warehouse UI\nVisit the link below to the DocAI Warehouse UI (Preview) to finalize setup of your schema, ACLs, AI processor mappings, and notifications.\n\nhttps:\/\/documentwarehouse.cloud.google.com\/provision\/***********\n\nWhen I enter to this link to complete my provision. On the first step I see the project number and the following message \"Request had insufficient authentication scopes. Logout and re-login will fix this problem.\" - it somehow log out from my Google account and provision page somehow on the unlogged page and I am stuck. On the provision page all other steps 2-4 are grey. according the docs all have to be easy -\u00a0https:\/\/cloud.google.com\/document-warehouse\/docs\/administer-warehouse\u00a0\n\non the first step the author on the top right corner of the screen have Search, project name, admin, and his Google account. In my case I have only Search and project name. Admin and my Google account from no reason does not exist. Can you please help me? I have the project owner right.\u00a0\n\n\"\"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"whats the proper syntax to print out a specific entity in python",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-02T17:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/whats-the-proper-syntax-to-print-out-a-specific-entity-in-python\/m-p\/540001#M1566",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":56,
        "Question_body":"Ive gotten all the way to getting this working\u00a0\n\ndocument_object = result.document\nprint(\"Document processing complete.\")\nprint(f\"Text: {document_object.text}\"\n\nunfortunately that just prints out the entire page.\n\nwhats the syntax to print out just a specific field \"entity\"?\n\ncant figure out the proper syntax",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Uploading images to Object detection with bounding boxes",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-16T21:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Uploading-images-to-Object-detection-with-bounding-boxes\/m-p\/490325#M801",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":135,
        "Question_body":"I am trying to upload images with bounding boxes and want to know\u00a0is there an online tool that can calculate bounding box co-ordinates from an image specifically for Automl?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech to Text for Tamil audio file",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-23T22:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-for-Tamil-audio-file\/m-p\/181208#M130",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":248,
        "Question_body":"Hi,\n\nI am trying to convert an audio file to text.\n\nLanguage: Tamil\n\nAudio type: Phone call\n\nBut when\u00a0 I look at the support documentation for this API the Model=\"phone call\" is not available for this language. Because of this I am getting very low accuracy after the conversion. please let me know this model would be available in the future. And is there any alternate way to achieve this.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow quota reset",
        "Question_tag_count":1,
        "Question_created_time":"2022-03-02T01:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-quota-reset\/m-p\/398513#M220",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hello,\n\nI have a Dialogflow ES agent, and I'm sending a few thousand (~5k) DetectIntent requests asynchronously. Our quota is 9k requests per minute, so it shouldn't be a problem. However what I'm seeing is that even after waiting for 10 minutes or more, when I run another batch (also ~5k), I get a resource exhausted error. If the quota is 9k per minute, why is the resource still exhausted after 10 minutes? And is there a way to know by what time I should try again?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI workbench and Google cloud storage problems accesing files",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-07T05:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-workbench-and-Google-cloud-storage-problems-accesing\/m-p\/390712#M196",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":599,
        "Question_body":"I am trying to create a ML project in which the job is a classification task of videos, so I upload those videos in Google cloud storage, and then I create a notebook on the workbench of vertex AI, for making data balancing, and then train my respective ML algorithm. But I have this problem:\n\n1. I want to use the video files from GCS without the need of downloading them again(that was the purpose of uploading them in GCS), but I don't know how can i do this?.\n\nI also try uploading de videos into the dataset space of the vertex AI workbench but still don't know how to acces to this files without downloading them again.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Too many pages",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-17T05:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Too-many-pages\/m-p\/478806#M657",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"I sent a 13 page pdf thru a document ai parser.\u00a0\u00a0\n\nGCP, instead of populating the errors collection of the result with an error indicating too many pages, instead throws a runtime error causing a crash.\n\nIs try...except... really the best solution for this as I've not seen use of try...except in any of Google parser examples.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What qualifies as a \"request\" in Google Video Intelligence API?",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-19T00:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-qualifies-as-a-quot-request-quot-in-Google-Video\/m-p\/511593#M1086",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":126,
        "Question_body":"I have a Requests per Minute quota limit of 120 and Backend time of 600 seconds per minute.\nEven when I annotate 20 videos together (distributed using multiple cores on Databricks ), it shows the following error. However, since 20 is far less than 120, I can't seem to identify why it exceeds 120 requests per minute. So I'm wondering if any other thing other than \".annotate()\" function could be classified as a request.\n\n\u00a0\n\n\u00a0\n\n429 Quota exceeded for quota metric 'Requests' and limit 'Requests per minute' of service 'videointelligence.googleapis.com' for consumer 'project_number:__________'. [reason: \"RATE_LIMIT_EXCEEDED\"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I am getting a permissions error when trying to create a new Agent in DF",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-03T10:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-am-getting-a-permissions-error-when-trying-to-create-a-new\/m-p\/505956#M1016",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":41,
        "Question_body":"I want to create a bot for my team that will answer incoming chats.\u00a0 When I go to create the Agent in DF ES I get a caller does not have permission error.\u00a0 How can I gain access to do this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow cx v3 DetectIntentRequest returning no-match",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-26T13:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-v3-DetectIntentRequest-returning-no-match\/m-p\/482545#M692",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"I am trying to create and manage agents using exclusively the api.\u00a0 I have created an agent with one intent only:\n\nname: \"projects\/???\/locations\/global\/agents\/1828e34b-78bc-48f5-9212-6dd83497d409\/intents\/b9e91883-f358-46ea-9661-a8a39c7d2557\"\ndisplay_name: \"test-age\"\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 23 \"\nparameter_id: \"p0\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" my age is \"\n}\nparts {\ntext: \" 68 \"\nparameter_id: \"p1\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 44 \"\nparameter_id: \"p2\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age \"\n}\nparts {\ntext: \" 81 \"\nparameter_id: \"p3\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age is \"\n}\nparts {\ntext: \" 35 \"\nparameter_id: \"p4\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" the age is \"\n}\nparts {\ntext: \" 29 \"\nparameter_id: \"p5\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 37 \"\nparameter_id: \"p6\"\n}\nparts {\ntext: \" years of age \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 45 \"\nparameter_id: \"p7\"\n}\nparts {\ntext: \" years \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 52 \"\nparameter_id: \"p8\"\n}\nparts {\ntext: \" years old \"\n}\nrepeat_count: 1\n}\nparameters {\nid: \"p0\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p1\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p2\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p3\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p4\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p5\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p6\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p7\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p8\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\npriority: 500000\n\nHowever, when I try to detect that intent using the DetectIntentRequest as shown in the github samples I keep getting no-match results:\n\n====================\nQuery Text: ' I am 55 '\nDetected Intent: text: \" I am 55 \"\nlanguage_code: \"en\"\nresponse_messages {\ntext {\ntext: \"Sorry, could you say that again?\"\n}\n}\ncurrent_page {\nname: \"projects\/???\/locations\/global\/agents\/1828e34b-78bc-48f5-9212-6dd83497d409\/flows\/00000000-0000-0000-0000-000000000000\/pages\/START_PAGE\"\ndisplay_name: \"Start Page\"\n}\nintent_detection_confidence: 0.3\ndiagnostic_info {\nfields {\nkey: \"Alternative Matched Intents\"\nvalue {\nlist_value {\n}\n}\n}\n\nI did train the agent in advance.\u00a0\n\nAppreciate any help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using Service Account Keys on different machines",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-25T00:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-Service-Account-Keys-on-different-machines\/m-p\/459716#M530",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":69,
        "Question_body":"It is a little unclear to me on how to use the AutoML translation model I created on different machines. I generated a service account key and reference it when calling the API and it works fine on my machine. However, when another user uses the same service account key on their machine, occasionally the following error occurs. The odd part is it doesn't happen all the time.\n\nStatus(StatusCode=Unavailable, Detail=\"Permission Denied: automl.models.predict\"\n\nI had two questions:\n1. When using the service account key on another machine, is there anything else I need to do to authenticate the user?\n2. I know Google advises against sharing keys, however I am using a third-party translation software that requires you set a JSON file key to use the API. Any advice on other methods for allowing other users to use the AutoML translation model would be appreciated.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"speech-to-text demo",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-25T10:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/speech-to-text-demo\/m-p\/176523#M93",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":398,
        "Question_body":"https:\/\/cloud.google.com\/speech-to-text\n\nspeech to text demo doesn't work for a while now ! annoying .. I'd like to play with it. Can you fix, please.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Vision API pricing",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-10T09:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vision-API-pricing\/m-p\/430454#M371",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":382,
        "Question_body":"Hello,\n\nI'm currently using the service of the Google Cloud Vision API. On the website it says that the first 1000 Request are for free every month.\u00a0\n\nBut for that I need a Billing account which is not for free if I understand correctly.\u00a0 So basically you can't use the Cloud Vision API completely for free.\u00a0\n\nAm I right or can you use the service without any costs?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"3D object detection using mobile camera or 3D scanner using cloud vision",
        "Question_tag_count":4,
        "Question_created_time":"2021-11-23T00:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/3D-object-detection-using-mobile-camera-or-3D-scanner-using\/m-p\/176320#M90",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":85,
        "Question_body":"The use case is, I want detect the 3d model through mobile camera or 3d scanner with dimensions to verify the scanned model is available or not in cloud storage. If model is not available it should list the model with approx model with percentage.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I deploy my custom model I have trained on workbench GCP?",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-09T01:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-deploy-my-custom-model-I-have-trained-on-workbench-GCP\/m-p\/551556#M1834",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"I have trained a detectron2 model on vertex ai workbench. i have NOT used tensorflow, xgboost or scikit-learn.\u00a0\n\ni have a model.pth file and a metrics.json file stored in my bucket when i run the model.\u00a0\n\nHow do i deploy this model on GCP and further evaluate it? Is it possible for me to create an endpoint and directly deploy my model on this or will i have to use something like cloud build?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI - Custom Job (with GPU) froze without errors",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-05T10:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Custom-Job-with-GPU-froze-without-errors\/m-p\/474701#M620",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hi all,\n\nI ran into an issue yesterday when submitting a custom job in Vertex AI. The job successfully started (as evident by the logs reported) but at some point, just before the script starts using the GPU on the machine, we stopped receving any logs. I let the job run for 20 minutes, but it did not procide any more logs - as well as there was no indication of the machine having any issues.\u00a0 I then stopped the job manually,\u00a0 re-created the exact same job by running the same script (using the google-cloud-aiplatform package in Python) with the exact same parameters, and the job ran successfully.\n\nIs there any way I can figure out what went wrong in the first job? I am looking for a stable solution to manage custom jobs, but the fact that this happened within my first 5 runs seems very concerning to me, especially since there was no indication that the job was frozen as it could have ran until it hit the max time which would have costed a lot of money.\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can't use medical model in STT version 2?",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-16T23:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-t-use-medical-model-in-STT-version-2\/m-p\/533595#M1443",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":82,
        "Question_body":"An error occurs if auto dictation is required.\nAre you still not supported?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI Form Processor Parse Table Structure Incorrectly",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-01T20:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Form-Processor-Parse-Table-Structure-Incorrectly\/m-p\/449252#M464",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":195,
        "Question_body":"Hi All,\n\nWe are currently using Document AI for form parsing some PDF document and half of times the default former processor either missing a col or messed up some col structure.\n\nLet's say the expected file header\n\nSales | Dollar Volume | Average Price\u00a0\n\nFor example, I saw cases like\n\n1. Missing Header\n\nSales|Average Price\n\n2. Wrong structure\n\nSalesDollar|Volume|Average Price\n\nThe content of first two cols are messed up as well. The cell could be missing value or incomplete value.\n\nAny recommendation to improve this? If no easy way, any guidance with examples to train or deploy one's own form processor? PS: the document has the same structure.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Optimization Variables \/ Inputs",
        "Question_tag_count":3,
        "Question_created_time":"2022-04-12T07:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Optimization-Variables-Inputs\/m-p\/412877#M268",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"Hi\n\n\u00a0\n\nis it possible to implement Optimization Problems using AutoML or other Google Cloud application different than regular google Colab \/ Jupiter Notebooks?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google translate API gave a mixed language translation result?",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-11T00:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-translate-API-gave-a-mixed-language-translation-result\/m-p\/440841#M412",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":383,
        "Question_body":"I am using Google Translate API for translating a Japanese sentence to Portuguese.\n\nOn July 6 to 8, the translation result was a sentence of mixed English and Portuguese words, but on July 9 the result seems to be a correct Portuguese sentence.\n\nWas there any event on July 6 to 8 such that Google Translate API gave a mixed language translation result?\n\nThank you for your time.",
        "Question_closed_time":"07-13-2022 03:33 PM",
        "Answer_score_count":1.0,
        "Answer_body":"The issue was generated due our service update and multiple service languages including Portuguese were affected. Our Translate API engineer team detected the root cause of the issue, and it was mitigated by a roll back recently performed.\n\nOur engineers confirmed that the issue is officially mitigated and you should not be experiencing any service misbehavior at this point.\n\nWe apologize for any inconvenience this may have caused on your operation.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Retail API predict call saves the userEvent. It should NOT!",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-17T17:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Retail-API-predict-call-saves-the-userEvent-It-should-NOT\/m-p\/443911#M428",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":102,
        "Question_body":"According to document, the userEvent sent as part of the predict body is not recorded.\u00a0\u00a0https:\/\/cloud.google.com\/retail\/docs\/predict#recommend\n\nHowever, I noticed this was not TRUE.\u00a0 Here is how to reproduce this\n\nI used \"FAKE_SESSION_ID_1\" (literally) for making ten recommendation calls for ten skus.\nI used\u00a0\"FAKE_SESSION_ID_1\" to make a recommendation call for another sku ( first time) and writes down the results\nRepeat step 2 but use \"FAKE_SESSION_ID_2\". The recommendation result is different from the one that uses\u00a0\"FAKE_SESSION_ID_1\"\n\nBecause both\u00a0\"FAKE_SESSION_ID_1\" and\u00a0\"FAKE_SESSION_ID_2\" are never used before this experient.\u00a0 \u00a0The recommendation result for the same sku should be same or very similar.\u00a0 But they diff a lot.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Adding price as a search facet in Google retail API",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-14T07:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Adding-price-as-a-search-facet-in-Google-retail-API\/m-p\/543862#M1666",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hi Community,\n\nI was doing a POC for implementing google retail api for search in my project. I used the default produts json that they provide and i tried to search the products, but it seems like i cannot add price as a facet or provide in the search query.\u00a0\n\nDoes anyone has any suggestions how to add the price of the product as a search facet?\n\nAny help is appreciated.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Python Code example to transcribe 2 audio inputs into speech at the same time",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-21T00:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-Code-example-to-transcribe-2-audio-inputs-into-speech-at\/m-p\/180446#M120",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":175,
        "Question_body":"I'm trying to create a piece of python code that can take 2 audio inputs,\u00a0\n\n1. from my microphone\n\n2. virtual input from zoom\n\nat the same time\n\nhowever, i am not sure how to transcribe them simultaneously.\n\nany help would be appreciated, thank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud AutoML Vision annotation stopped working",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-18T13:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-AutoML-Vision-annotation-stopped-working\/m-p\/490920#M816",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":110,
        "Question_body":"Has anyone encountered the issue where the AutoML Vision annotations for datasets stopped working. This includes not being able to change labels anymore, not being able delete created labels or not save create labels. The annotations were working as expected last week, but for some reason they stopped working this week.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Vision OCR On-Prem",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-21T06:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-OCR-On-Prem\/m-p\/614378#M2416",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":26,
        "Question_body":"I wanted to use Google Cloud Vision OCR on prem due to some security concern.\n\nCan anybosy assist",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Got Welcome email, but doesn't have permission Makersuite.google.com!",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-17T08:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Got-Welcome-email-but-doesn-t-have-permission-Makersuite-google\/m-p\/554306#M1942",
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":235,
        "Question_body":"I got an email with the following content.\n\n\u00a0\n\nWelcome to the\nPaLM API and MakerSuite\nThanks for putting your name on the waitlist for access to the PaLM API and MakerSuite. We\u2019ve been opening up to small groups of people gradually and now it\u2019s your turn. Open MakerSuite to enable API access or start prototyping today.\n\nBut when I click on the \"Visit MakerSuite\", I get a message saying that my gmail account\u00a0doesn't have permission to see this page.\n\nTried adding my email back into the waiting list, but got \"You're all set!\" page with the \"Get Started\" button. Clicking on \"Get Started\" results in the same error message\n\nAny idea how to resolve this issue?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google LLM API - User Payment Methods & Cost of Use Metrics",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-23T04:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-LLM-API-User-Payment-Methods-amp-Cost-of-Use-Metrics\/m-p\/614815#M2436",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":59,
        "Question_body":"The LLM is still in beta and efforts are concentrated on what the LLM can do and be used for. ChatGPT API on the other hand is available to developers and apps are appearing for use by individuals once a Key has been obtained. OpenAI parses AI input and output to consume purchased Tokens however, a fixed block of Tokens cannot be purchased as their business model requires unattended access to a credit card for either for debits or credits or to place a transaction lock on a purchase. This prevents cashcards from being used.\n\nIndividual use of the LLM and the purchase of session time needs consideration but its a bit of a chicken and egg situation with the egg arriving first. How much did it cost to feed the chicken?\n\nAn issue that OpenAI has yet to address in terms of consumable tokens is that consumer use of the their API will need a meter on the app to indicate the level of unit consumption and monetary cost something required by statutes globally. In other words consumption metrics.\n\nAs part of the beta program will Google offer an explanation to both developer and consumer of the LLM API apps as to how the purchase cost of LLM time and data will operate and be displayed to the user?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Duplication of intents and entities while importing an updated version of a pre-existing flow of sam",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-22T04:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Duplication-of-intents-and-entities-while-importing-an-updated\/m-p\/524979#M1311",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"With Feb\u00a0 06 2023\u00a0 release dialogflow cx introduced the functionality ; in which while importing a flow a person was given a choice if he\/she wants to keep the original resources or replace the existing resources ,\u00a0\u00a0\nthe link to release note -\u00a0\n\nhttps:\/\/cloud.google.com\/dialogflow\/docs\/release-notes#February_06_2023\n\u00a0\nand the image of the console -\n\u00a0\n\u00a0\nfeb release functionality\n\n\n\nAll I want is to be able to do this(thing that is shown in image ) from client library\n\nMy specific problem is I want to automate the import flow process; if a change is made in a flow of dev bot , the new version of the same flow should be imported in the prod bot ,\nbut on doing so from client library all the resources(intent entities) and the flow itself is duplicating\n\n\nPlease , quick responses and any suggestions can help me , I am in a bit of urgency\nplease help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Feature Engineering Vertex AI\/AutoML",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-28T09:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Engineering-Vertex-AI-AutoML\/m-p\/447814#M455",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":158,
        "Question_body":"Hey There,\n\nI am writing my Master Thesis at the moment. I am comparing AutoML products for image classification. There I compare the product Vertex AI with Azure from Microsoft. However, I can't find the concrete methods of feature engineering and model selection from the documentation. Does anybody know these methodes used for Google AutoML for image classification?\n\nThanks a lot!\n\nArndt",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX: intent detection issue when we delete a page",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-19T08:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-intent-detection-issue-when-we-delete-a-page\/m-p\/511745#M1093",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hi,\nWhen we remove a page, the users who have a session get an error:\n\"Could not find Page with ID...\"\nwhat should we do in this situation? Is there any way to disable all active sessions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech-to-Text for many langages",
        "Question_tag_count":1,
        "Question_created_time":"2022-03-28T06:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-for-many-langages\/m-p\/407723#M251",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":77,
        "Question_body":"Hello,\n\nI'm testing bunch of variety of the Speech-to-Text API to transcribe audio from microphone but I'm going through two issues:\n\nIt doesn't reconize others langages like arabic or chinese or at least the transcription shows unknow characters like in this picture below(which is for arabic)\nwhen transcribing audio sentences can be repeated many times\n\nNote: I didn't see anywhere how to use utf-8 for transcription\n\nHow can I fix it I used the code found here https:\/\/github.com\/googleapis\/python-speech\/blob\/main\/samples\/microphone\/transcribe_streaming_infini...",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Just curious, can I use cloud bigtable as a feature store instead of using vertex AI feature store?",
        "Question_tag_count":2,
        "Question_created_time":"2022-06-28T03:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Just-curious-can-I-use-cloud-bigtable-as-a-feature-store-instead\/m-p\/435633#M395",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":603,
        "Question_body":"I am trying to migrate my features table stored in bigquery to a feature store with lower latency. I'm choosing whether I should make use of vertex AI feature store or just cloud bigtable.\n\nMy features tables are <10MB, and it is used for real time prediction hence a database with low latency is sufficient.\n\nIm just wondering aside from pricing, and ease of exporting data (bigtable requires more steps than vertex ai feature store), what is the difference between the 2 options?\n\nAlso, what type of database (eg: bigtable or redis?) is vertex AI feature store behind the scenes, when I am creating the feature store using the web UI?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Removed voices from German standard text to speech (tts)",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-17T06:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Removed-voices-from-German-standard-text-to-speech-tts\/m-p\/394397#M200",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":185,
        "Question_body":"We have a problem related to the Cloud text to speech API.\nWe develop an AI based chatbot system, and we have lot of different chatbot which speak in English and German also.\nWe are using two different voices 'de-DE-Standard-B' (male) and 'de-DE-Standard-C' (female) in the case of German bots, but both bots speak in same vois at now.\nWe detected the problem at 2022-02-16.\n\nCould you give me some information about this problem?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Get Cloud Vision API as good as Google Lens",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-24T13:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Get-Cloud-Vision-API-as-good-as-Google-Lens\/m-p\/434624#M391",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":652,
        "Question_body":"As part of a student team, I am building a\u00a0system to classify used shoes.\n\nI know that\u00a0Google Lens\u00a0is doing a really good job here.\n\nI came across\u00a0Google Cloud Vision API\u00a0(which should be a similar thing) and implemented this in python.\n\nFor clean, well-angled images like this Air Force One:\u00a0\n\n\u00a0\n\n\u00a0\n\nI am getting really promising results:\n\n10 Web entities found: \n\n    Score      : 0.9957345128059387\n    Description: Nike Air Force 1 07 LV8 EMB Raiders Mens\n\n    Score      : 0.7279999852180481\n    Description: Nike\n\n    Score      : 0.7279999852180481\n    Description: \n\n    Score      : 0.7130167484283447\n    Description: Nike Mens Air Force 1 '07 LV8 'Metallic Swoosh Pack\n\n    Score      : 0.7052000164985657\n    Description: Sneakers\n\n    Score      : 0.7049999833106995\n    Description: Shoe\n\n    Score      : 0.6831490993499756\n    Description: Nike Mens Air Force 1 Low\n\n    Score      : 0.6559000015258789\n    Description: Nike\n\n    Score      : 0.6399800181388855\n    Description: Nike Air Max\n\n    Score      : 0.6158000230789185\n    Description: Men's Shoe\n\nIf however, i input real-world images like this old used Nike Tanjun:\u00a0\n\n\u00a0\n\nThings fall apart:\n\n8 Web entities found: \n\n    Score      : 0.5776046514511108\n    Description: Shoe\n\n    Score      : 0.4444863796234131\n    Description: Product design\n\n    Score      : 0.42980000376701355\n    Description: Design\n\n    Score      : 0.4197726845741272\n    Description: Product\n\n    Score      : 0.39287227392196655\n    Description: Activewear\n\n    Score      : 0.384799987077713\n    Description: Walking\n\n    Score      : 0.35569998621940613\n    Description: Walking Shoe\n\n    Score      : 0.3215000033378601\n    Description: outdoor\n\nBut if I upload the image to google lens, I could still figure out the right label:\u00a0\n\n\u00a0\n\n\u00a0\n\nLogo detection (Nike) almost always works. And using this, I could for example search after the most often occurring word after the Logo (Tanjun) to figure out the model.\n\nIt must be mentioned that the data of our system will be better than that, there will be multiple images taken from different angles and very good lighting conditions.\n\nNow i am trying to figure out how to\n\nEITHER: Get\u00a0Vision API\u00a0working in the same way as\u00a0Google Lens\n\nOR: Acces Google Lens data in a somehow convenient way (should in the best case run from a raspberry pi)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create an instance of TextToSpeechClient() and ApplicationDefaultCredentials ...",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-01T16:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-an-instance-of-TextToSpeechClient-and\/m-p\/418964#M298",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":470,
        "Question_body":"Hi Folks,\n\nMy first post here. This was posted on stackoverflow without much feedback - it is a little specific to the TextToSpeechClient and using\u00a0ApplicationDefaultCredentials.\u00a0 The link to the stackoverflow article is below just for reference.\n\nhttps:\/\/stackoverflow.com\/questions\/72074724\/trying-to-create-an-instance-of-googles-class-texttospe...\n\nI'm attempting to Create an instance of TextToSpeechClient() and an getting an exception - Could not construct ApplicationDefaultCredentials.\u00a0\n\nI was able to get the php sample code provided on your github site running from the command line. I'm now executing in a browser session on an apache server. I have added the putenv() function to set the GOOGLE_APPLICATION_CREDENTIALS value.\n\nputenv('GOOGLE_APPLICATION_CREDENTIALS=\/Users\/macgowan\/google_cloud\/service-account-text-to-speech-test-00.json'); \n\nBelow is the code sample\n\n\u00a0\n\n<?php\nheader(\"Content-Type: application\/json; charset=UTF-8\");\nheader(\"Access-Control-Allow-Methods: POST\");\nheader(\"Access-Control-Max-Age: 3600\");\nheader(\"Access-Control-Allow-Headers: Content-Type, Access-Control-Allow- Headers, Authorization, X-Requested-With\");\n\nrequire_once '\/home\/macgowan\/vendor\/autoload.php';\n\n\/\/ [START tts_synthesize_text]\nuse Google\\Cloud\\TextToSpeech\\V1\\AudioConfig;\nuse Google\\Cloud\\TextToSpeech\\V1\\AudioEncoding;\nuse Google\\Cloud\\TextToSpeech\\V1\\SsmlVoiceGender;\nuse Google\\Cloud\\TextToSpeech\\V1\\SynthesisInput;\nuse Google\\Cloud\\TextToSpeech\\V1\\TextToSpeechClient;\nuse Google\\Cloud\\TextToSpeech\\V1\\VoiceSelectionParams;\n\nputenv('GOOGLE_APPLICATION_CREDENTIALS=\/Users\/macgowan\/google_cloud\/service-account-text-to-speech-test-00.json');\n\ntry\n{\n\nputenv('GOOGLE_APPLICATION_CREDENTIALS=\/Users\/macgowan\/google_cloud\/service-account-text-to-speech-test-00.json');\n\/\/ $client->useApplicationDefaultCredentials();\n\n$ip = getenv('GOOGLE_APPLICATION_CREDENTIALS');\nprintf(\"Get env var - GOOGLE_APPLICATION_CREDENTIALS: %s<br \/>\", $ip);\n\n$ip = getenv('APACHE_RUN_USER');\nprintf(\"Get env var - APACHE_RUN_USER: %s<br \/>\", $ip);\n\n\/\/ *** FAILS HERE ***\n$client = new TextToSpeechClient();\n\n$text = \"Hello Joe\";\n\nprint('Set input text using the SynthesisInput() object' . PHP_EOL);\n$input_text = (new SynthesisInput())\n->setText($text);\n\n$voice = (new VoiceSelectionParams())\n->setLanguageCode('en-US')\n->setSsmlGender(SsmlVoiceGender::FEMALE);\n\n$audioConfig = (new AudioConfig())\n->setAudioEncoding(AudioEncoding::MP3);\n\n$response = $client->synthesizeSpeech($input_text, $voice, $audioConfig);\n$audioContent = $response->getAudioContent();\n\nfile_put_contents('\/home\/macgowan\/output.mp3', $audioContent);\n$client->close();\n}\ncatch (Exception $e)\n{\nprintf(\"Caught exception: %s<br \/>\", $e->getMessage());\n}\n?>\n\nThanks for your help - Chris",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to build a data to AI solution with BigQuery and Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-22T12:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-build-a-data-to-AI-solution-with-BigQuery-and-Vertex-AI\/m-p\/595708#M1991",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":147,
        "Question_body":"In this article, learn how to build an end-to-end data to AI solution on Google Cloud, including a practical example of a real-time fraud detection system and the architecture behind it. You'll also discover how to train, deploy, and monitor machine learning models in production.\n\nThis article is based on a recent Cloud OnBoard session.\u00a0Register here to watch on demand.\u00a0\u00a0\u00a0\n\nIf you have any questions, please leave a comment on the blog (or below) and someone from the Community or Google Cloud team will be happy to help.\n\nRead the blog",
        "Question_closed_time":"05-23-2023 09:25 AM",
        "Answer_score_count":2.0,
        "Answer_body":"Great read!\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Google Translate API and Serbian Latin script",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-11T02:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-and-Serbian-Latin-script\/m-p\/476723#M638",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi there,\n\nIn Serbia we are using 2 scripts side by side - Cyrillic and Latin script.\n\nI am heaving an issue with translation to Serbian Latin.\nBy default Google translate offer translation to Serbian Cyrillic , but bellow that default translation there is a translation to Serbian Latin.\nTake a look at this example:\nhttps:\/\/translate.google.com\/?sl=en&tl=sr&text=Hello%20world!&op=translate\n\nI have found this post from early 2019.\nhttps:\/\/support.google.com\/translate\/thread\/1836538?hl=en\n\nLike in that post my question is the same:\nI need it to support Serbian Latin, for some projects I don`t use the Cyrillic script. Also there is a problem with translating pages or similar plugins, e.g.: Google Language Translator for WordPress and some others CMS system like Kopage you can translate only to Serbian Cyrillic script.\n\nAs I found this post on stackoverflow:\nhttps:\/\/stackoverflow.com\/questions\/73699065\/google-cloud-translate-serbian-latin-not-working\n\nIt seems, according to the poster of that article, that there was a workaround.\nInstead of \"sr\" ISO-639 code you can put \"sr_Latn\" - and you will get translation into Serbian Latin script.\nBut that workaround stop working several weeks ago - according to the poster.\n\nIs there a workaround to translate into Serbian Latin characters rather into Serbian Cyrillic characters?\n\nRegards,\nBranislav",
        "Question_closed_time":"10-12-2022 08:12 AM",
        "Answer_score_count":0.0,
        "Answer_body":"It appears that translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API, as discussed in this recent issue. Therefore it\u2019s not assured that any possible workaround will be functional or reliable. You can see the list of supported language codes for translation here.\n\nYou can, however, submit a Feature Request to the public Google issue tracker for Cloud Translation API. The higher the number of users who bring attention to this request, the more likely it is for it to be eventually built into the API.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Does Vertex AI support multiple model instances in Same Endpoint Node.",
        "Question_tag_count":2,
        "Question_created_time":"2021-09-13T04:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-multiple-model-instances-in-Same-Endpoint\/m-p\/169614#M47",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":137,
        "Question_body":"We are trying to deploy the model in Vertex Endpoint with GPU support.\u00a0\nHere we are facing two problems, GPU memory is fully reserved by a single model but GPU power\n\nis underutilize.\u00a0\n\nSo can we deploy multiple Workers in the Same Node and also how to allow the worker to reserve VRAM only up to it required?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to create an endpoint using the model trained by Vertex AI?",
        "Question_tag_count":4,
        "Question_created_time":"2022-12-29T23:37:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-create-an-endpoint-using-the-model-trained-by-Vertex-AI\/m-p\/504610#M1003",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":321,
        "Question_body":"I trained a model using Google Cloud Vertex AI. I have a dataset containing different videos and model does the video classification of these videos. In the Google Cloud Platform, I can find accuracy of the given videos after model has been trained. However, I could not find a way to create an Endpoint which is required to implement my model to the project.\n\nIn other words, I have a model trained by Google Cloud\u2019s Vertex AI and I want to create an Endpoint which accepts a request with video and returns the accuracy according to the trained model. Google Cloud provides a way to do it using Endpoints but as shown below, following\u00a0error\u00a0occurs.\u00a0This model cannot be deployed to an Endpoint\n\nI could not find any documents on why this error is given to me. On the other hand, when I download the given model, code requires input type as below with the video;\n\n\u201cRESNETISH_V3: float32 Tensor, shape=(None, 128)\u201d.\n\nI do not know what is this variable type and this is basically where I am stuck. I need help deploying my trained model to any serverless environment, create a code handling posted video and return the accuracy and class of the given video to the request.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GenerativeAI Studio is flagging this as inappropriate due to policy...",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-15T23:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GenerativeAI-Studio-is-flagging-this-as-inappropriate-due-to\/m-p\/553719#M1922",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":37,
        "Question_body":"\"View this repo at https:\/\/github.com\/aaronn\/slack-gpt\n\nWhat is it about?\"\n\nThis points to a Github repo that has code for a Slackbot.\n\nOn bard.google.com, it runs fine.\n\nWhat's causing this though?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Custom Training Job Container not finding my module: Error while finding module for '...'",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-25T02:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Custom-Training-Job-Container-not-finding-my-module\/m-p\/416620#M293",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":519,
        "Question_body":"Hello,\n\nI have a PyTorch training job that I am packaging in a Python software distribution (.tar.gz file). I upload the sdist to a GCS bucket and run it in a container using the gcloud ai custom-jobs create CLI.\n\nUp until a couple of weeks ago this worked fine but in recent days my jobs consistently fail with messages like these appearing in their logs:\n\nRunning command: python3 -m MyPackage.MyModule --job-dir=gs:\/\/my-bucket\/my-job\/model --model-name=my-model\n\n\/opt\/conda\/bin\/python3: Error while finding module specification for 'MyPackage.MyModule' (ModuleNotFoundError: No module named 'MyPackage.MyModule')\n\n\u00a0\n\nMyPackage.MyModule is my module where my training code runs, naturally.\n\nAs I've mentioned above the same procedure worked until recently. There have not been any changes to it and I can clearly see that MyModule.py is located under MyPackage in my .tar.gz file.\n\nThe container image that I am using is us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-9:latest and from what I can tell it has not changed since the time I successfully used it before.\n\nWhy is the Vertex AI container not finding my training module? How can I further debug and fix this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"need help in figuring out how to make the Vertex AI DAG wait for the task to complete?",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-10T11:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/need-help-in-figuring-out-how-to-make-the-Vertex-AI-DAG-wait-for\/m-p\/610794#M2336",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":66,
        "Question_body":"Hi team,\n\n\u00a0\n\ni am trying to create the big query table in Vertex AI but the current implementation makes it difficult for the DAG to wait for the Big query table to be created and hence the DAG fails ultimately.\n\n\u00a0\n\ncan you tell me the way on how to wait for the resource creation before moving forward with the next step in DAG?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech to text hangs (infintie load)",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-22T07:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-hangs-infintie-load\/m-p\/512571#M1114",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":274,
        "Question_body":"I uploaded a mp3 file (1.07 min) to transcribe using the video model, and the transcription is stuck, no errors, just infinite loading\n\nThis is the file https:\/\/drive.google.com\/file\/d\/1QLucfAwJZXxSAOKSIWoPtIbljSM9pvsl\/view?usp=sharing\n\nUPDATE: It failed with \"Error running recognize request. Too many retries, giving up.\"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Bard API",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-28T12:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Bard-API\/m-p\/597656#M2036",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hello, I am student currently working on a chatbot project and I would love to use the Bard API, please can someone assist me in how I can",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create User Managed Workbook with A100 40gb",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-03T06:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-User-Managed-Workbook-with-A100-40gb\/m-p\/540188#M1570",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"I'm trying to create a user managed workbook with an a100 40gb GPU, but I am always running into this same error:\n\nProject-Name:\u00a0The zone 'projects\/project-name\/zones\/us-east1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later.: Something went wrong. Sorry about that.\n\nI am pretty sure I have the appropriate quotas, and I don't have any other workbooks so my global gpu quota shouldn't be the chokepoint. (Do I need the Preemptible Nvidia A100 GPUs quota? All of those are set to 0.) I have the \"Managed Notebooks NVIDIA A100 GPUs per region\" set to 1, but I guess those only apply to, well, managed notebooks. Any tips?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About Speech-to-Text support area",
        "Question_tag_count":1,
        "Question_created_time":"2021-09-26T04:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Speech-to-Text-support-area\/m-p\/171226#M53",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":355,
        "Question_body":"Does Speech-to-Text have any nodes in Hong Kong?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow cx v3 detect intent returning event handler not defined",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-17T07:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-v3-detect-intent-returning-event-handler-not\/m-p\/510845#M1072",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":263,
        "Question_body":"Hi, I am getting this error in some detectIntent requests:\u00a0\u00a0No handler is defined for the event.\n\nExample:\nIn this page, trying to call the register_finished\u00a0event:\n\n\nIn the conversation history, we can see that the user was in this page when the request was made:\n\nConversation Id: \"070077b0-9673-11ed-990e-bf1f225bb3df\"\n\nps: in other pages this error also happens sometimes",
        "Question_closed_time":"01-18-2023 12:00 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi, It appears that there are various factors for triggering this error. (Naming mismatch, Having another session open while using the same flow... etc) I would suggest to contact Google Support [1] to further investigate this concern as it is not recommended to share some of the project details in public for investigation.\n\n[1]https:\/\/cloud.google.com\/contact\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How to create a private instance of Bard and feed it private data to help answer questions",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-04T23:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-create-a-private-instance-of-Bard-and-feed-it-private\/m-p\/599980#M2078",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":164,
        "Question_body":"Hi Google support,\n\n\u00a0 \u00a0 I'm trying to figure out how I can create a private Bard instance and feed it private data to train it so that it can help me answer questions in an informative way. There is a lot of content out there, but I feel it's quite convoluted. Please let me know if you can guide me in any way.\u00a0\n\nThank you.\u00a0\nRegards,\n\nTraolly Xiong",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow Retrieving Start Page, End Session Page via API call",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-12T07:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-Retrieving-Start-Page-End-Session-Page-via-API-call\/m-p\/552820#M1885",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":152,
        "Question_body":"Hi I tried to get Start Page\/End Session pages provided from Dialogflow CX as default pages.\n\nHowever I was not able to get those pages in API level which leads me not being able to implement test cases along with those pages.\n\nThe image above shows one of my flows\n\nwhen I trigger list_page API, it just returns the custom pages in blue box but it does not return default pages such as End Session or Start Page in API level.\n\n\n\n# Initialize request argument(s)\nrequest = dialogflowcx_v3.ListPagesRequest(\n    parent=parent,\n)\n\n# Make the request\npage_result = client.list_pages(request=request)\n\n\u00a0\n\n\u00a0\n\nCould you please help?",
        "Question_closed_time":"05-14-2023 06:22 PM",
        "Answer_score_count":0.0,
        "Answer_body":"I\u00a0 resolved this issue by specifying End Session (default) page on current_page of test case\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"How to prevent google translator, change the format of dates?",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-16T09:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-prevent-google-translator-change-the-format-of-dates\/m-p\/603931#M2177",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":23,
        "Question_body":"Hi All,\n\u00a0\nI have a problem related to Google Translate.\nI am using\u00a0Google.Cloud.Translation.V2 in c# application\n\u00a0following code can re produce the problem I am facing\n-- Code Start\n\u00a0var client = TranslationClient.Create(Google.Apis.Auth.OAuth2.GoogleCredential.FromFile(@\"keyFile.json\"));\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 string textToTranslate = @\"\u0646\u0648\u062f \u0625\u0641\u0627\u062f\u062a\u0643\u0645 \u0628\u0623\u0646\u0646\u0627 \u062d\u0636\u0631\u0646\u0627 \u062c\u0644\u0633\u0629 \u0627\u0644\u064a\u0648\u0645 \u0648\u062d\u0636\u0631 \u0627\u0644\u0645\u062f\u0639\u0649 \u0639\u0644\u064a\u0647 \u0648\u0637\u0644\u0628\u062a \u0645\u0646 \u0627\u0644\u0645\u062d\u0643\u0645\u0629 \u0623\u062c\u0644 \u0623\u062e\u064a\u0631 \u0644\u0644\u062a\u0633\u0648\u064a\u0629 \u0648\u0648\u0627\u0641\u0642\u062a \u0627\u0644\u0645\u062d\u0643\u0645\u0629 \u0639\u0644\u0649 \u062a\u0623\u062c\u064a\u0644 \u0627\u0644\u062f\u0639\u0648\u0649 \u0643\u0623\u062c\u0644 \u0623\u062e\u064a\u0631 \u0644\u062a\u0627\u0631\u064a\u062e\u00a012\/10\/2023\u0645\u00a0\u0644\u0644\u062a\u0633\u0648\u064a\u0629 \u0623\u0648 \u0627\u0644\u0631\u062f\";\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 string textToLang = @\"en\";\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 string textFromLang = @\"ar\";\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 string translatedTextReturn = string.Empty;\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TranslationResult translationResult = client.TranslateText(textToTranslate, textToLang, textFromLang, TranslationModel.ServiceDefault);\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 translatedTextReturn = translationResult.TranslatedText;\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Console.WriteLine(translatedTextReturn);\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Console.ReadLine();\n-- Code End\ntranslatedTextReturn\u00a0= \"We would like to inform you that we attended today's session and the defendant attended, and I asked the court for a deadline for settlement, and the court agreed to postpone the case as a deadline for\u00a010\/12\/2023 AD\u00a0for settlement or response\"\n\u00a0\nas you see I have the\u00a0dd\/MM\/yyyy format for dates but translated text automatically converts to\u00a0MM\/dd\/yyyy\n\u00a0\nHow could I prevent this?\nThanks!!!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"External url retrieved for locust web interface for ai load testing is not accessible in browser",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-13T00:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/External-url-retrieved-for-locust-web-interface-for-ai-load\/m-p\/611811#M2350",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":77,
        "Question_body":"Hello Folks,\n\nI am trying to implement \"Load testing and monitoring AI Platform models\" using Vertex AI Workbench. I have deployed the ML model and validated it I have also created the locust image as per docs i followed the steps now the challenge I am facing is that when the external load balancer url is retrieved and I am trying to access that in browser to simulate my load test on the ML API I am getting site cant be reached error .\nthe below url is the external load balancer url where locust is installed and hosted I am trying to access that by providing the load balancer server name as given in docs : -https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/load-testing-and-monitoring-aiplatform-models#:~:text=Locust%20is%20used%20to%20implement,testing%20the%20model%20prediction%20service. .Does anyone have any idea how to resolve this or have anyone faced a similar kind of an issue while trying the steps ? Is there a possibility of some firewall settings that can be tweaked ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Fast Start GPU for AI training",
        "Question_tag_count":1,
        "Question_created_time":"2021-09-02T02:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Fast-Start-GPU-for-AI-training\/m-p\/168768#M44",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":378,
        "Question_body":"Is there a way to fast start-up GPU (like Cloud RUN) if there is training request come-in?\n\nDue to GPU cost is high, turn-on 24 hours\/day does not make sense.\n\nPre-empted GPU cloud be another option but offer only 1st minute free.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using RegEx Entity with other entitiy in Traning Phrase gives empty value for the parameter",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-19T02:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-RegEx-Entity-with-other-entitiy-in-Traning-Phrase-gives\/m-p\/479671#M666",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":357,
        "Question_body":"n Dialogflow ES I am using a Regular Expression for the date format (DD\/MM\/YYYY) validation. If I have the training phrase as 22\/05\/2021 it works perfectly.\n\nBut I need to have the training phrase as 22\/05\/2021 sample@sample.com. When I use like that, for the date it gives empty value.\n\nNot even for this mydate validation, this is not working if I use any RegEx entity along with other entity in the same training phrase.\n\nmyDigit RegEx is ^[0-9]$\n\nSo can't it be used Regex entity along with other entity in the same training phrase in Dialogflow?",
        "Question_closed_time":"10-26-2022 10:18 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Sorry for the delay in replying, I see that your regular expression is matching the end of the line with \u201c$\u201d after the digit (^[0-9]$). This would explain why you are able to capture the data when the training phrase contains only the digit, but not when additional data is located after (for example, \u201c2 sample@sample.com\u201d does not contain the EOL metacharacter after \u201c2\u201d, but having only \u201c2\u201d as the training phrase does).\n\nIf your date regular expression matches the end of the line as well, you\u2019d see the same behavior. I made a quick intent using this expression to match \u201cDD\/MM\/YYYY\u201d: ^[0-9]{2}\\\/[0-9]{2}\\\/[0-9]{4}. As shown below, it captures both the date and Email:\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI explain with a custom trained scikit-learn classification model",
        "Question_tag_count":2,
        "Question_created_time":"2022-06-30T09:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-explain-with-a-custom-trained-scikit-learn\/m-p\/436711#M397",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":480,
        "Question_body":"Hi Google Community,\n\nI was wondering, has anyone been able to successfully train and deploy a custom trained scikit-learn classification model and deploy it to a vertex endpoint with the feature attribution through the explain endpoint working?\n\nEvery time i define my instances, predictions and explanation_spec while uploading my model, i get errors on the endpoint for the :explain method. Specifically, i get '400 bad request' with no information on why it was a bad request.\n\nI am using the v1beta1 ai platform python SDK and also am using a custom basic serving container. The custom container works for :predict but :explain does not work. Is there some example code out there? Is\u00a0scikit-learn not supported for feature attribution?\u00a0\n\nThanks! Ryan",
        "Question_closed_time":"07-14-2022 04:51 PM",
        "Answer_score_count":1.0,
        "Answer_body":"For anyone looking back on this, i was able to use the following notebook to solve my problem. It seems we need to use encoding BAG_OF_FEATURES. I am not to sure why this is required, but it seems to have done the trick for me.\n\nhttps:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage4...\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Google Translate javascript API",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-06T01:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-javascript-API\/m-p\/451250#M491",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":429,
        "Question_body":"I am trying to incorporate Google Translate javascript API into my application.\nI understand that a usage fee is charged for Google Cloud Translation.\nI was just wondering if I could use the following javascript free of charge:\n<script type=\"text\/javascript\"> src=\"\/\/translate.google.com\/translate_a\/element.js?cb=googleTranslateElementInit\"><\/script>\nIs it OK to apply this script to commercial use free of charge?\nOr is it also covered under the Google Cloud Translation policy?\n\u00a0\nKInd regards,\n\u00a0\nMinoru Kume",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX - DTMF Barge in not working",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-03T06:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-DTMF-Barge-in-not-working\/m-p\/495732#M897",
        "Question_answer_count":1,
        "Question_score_count":6,
        "Question_view_count":250,
        "Question_body":"Dialogflow CX\u00a0Barge in does not work for DTMF input.\n\n-- Steps to reproduce the issue:\n1. Enable Barge in at Agent level settings\n2. Create a page with a parameter with DTMF enabled\n3. Call agent via phone connector like Twilio and use phone keypad to provide parameter value\n\n-- Expected behavior \u00a0--\nThe agent prompt should stop on DTMF input, similar to how it stops on voice input.\n\n-- Actual behavior \u00a0--\nThe agent prompt does not stop on DTMF input.https:\/\/issuetracker.google.com\/issues\/259816857\n\nIssue report url:\u00a0https:\/\/issuetracker.google.com\/issues\/259816857\u00a0 Please upvote.\u00a0\nCX phone experience is highly degraded without DTMF barge in.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Short glossary terms not respected in the Translation API",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-20T11:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Short-glossary-terms-not-respected-in-the-Translation-API\/m-p\/480322#M669",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":133,
        "Question_body":"We are seeing that some glossary terms are not respected in the translation API. For example, we have \"IT\" defined in our glossary for both English and Spanish. However,\u00a0 it is being translated to \"TI\" when translating from English to Spanish. Other glossary terms are behaving as expected. Is anyone else seeing this? It's causing a lot of issues with our translations.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Billing & Cloud Vision API issue with \"Recognize Text\" on Android system",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-21T23:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Billing-amp-Cloud-Vision-API-issue-with-quot-Recognize-Text-quot\/m-p\/469422#M589",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":56,
        "Question_body":"Hi there,\n\nHere I got a billing problem with Cloud Vision API.\n\nFirst, I follow this link to setup my Firebase project to enable the feature of \"Recognize Text\".\n\nhttps:\/\/firebase.google.com\/docs\/ml\/android\/recognize-text\n\nThen all the functions used are normal. I call the function of \"annotateImage\" in Cloud Functions to invoke the Cloud Vision API, then can also used successful.\n\nAbsolutely, I have trace the flow and requests on Cloud Vision API, it is just looks reasonable.But the issue I encountered is, \"it still charges when I'm not using it\", also when it has no any flow and requests!\u00a0\n\nBilling, September 1-22, 2022 (the project has billing alerts set up now) :\n\nCloud Vision API, 30 days to 9\/22\/2022 :\n\n\u00a0\n\nIt would be so gratefull if there any good suggestions !",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Need help related create intent in dialogflow using nodejs",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-29T20:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Need-help-related-create-intent-in-dialogflow-using-nodejs\/m-p\/461237#M2426",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":44,
        "Question_body":"I have created some agent in dialogflow using nodejs API, now I need to create intent inside those agent using node js API,\n\nBut I am getting error related create intent permission denied on agent.\n\nHow we can fix that ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Vision API - Wrong line order",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-05T01:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-API-Wrong-line-order\/m-p\/506806#M1022",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":170,
        "Question_body":"Hi there,\n\nI'm curious how are you handling the too common case where DOCUMENT_TEXT_DETECTION returns the text lines (or specific words) at incorrect order, despite the input being a simple paragraph. For example, line n would jump to line n+2, then backtrack to line n+1.\n\nUsing \"legacy_layout\" does not usually help.\n\nthanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Insufficient data error in Others You May Like",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-29T04:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Insufficient-data-error-in-Others-You-May-Like\/m-p\/494198#M873",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":153,
        "Question_body":"Hi,\n\nI'm getting the below error while training Others You May Like Model even though all the data requirements are being satisfied.\n\nINSUFFICIENT_TRAINING_DATA Recommendation model others-you_others-you_1669706866636 cannot be trained because of insufficient data.\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nCould anyone please help with the same?\n\nThanks,\n\nAnusha",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to get auto speech adaption to work",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-07T12:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-get-auto-speech-adaption-to-work\/m-p\/519495#M1241",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"(this is actually Dialogflow ES, wouldn't let me label it that way)\n\n\u00a0\n\nTrying to get dialogflow auto speech adaption to take a 9 digit number consistently when being spoken.\u00a0 But no matter what I do it keeps trying to convert what I say to a 10 digit phone number (adding in extra numbers).\u00a0 I am speaking as slow as I possibly can clearly delineating the numbers, yet it'll add an extra number randomly to make it a phone number.\u00a0 Any assistance would be helpful.\n\nI have a relatively empty dialogflow agent (enabled for ES\/speech adaption\/all the right settings turned on). with a single intent\u00a0\n\nThe single intent is set to highest priority, no input contexts, The NineDigitNumber entity is set up as a parameter that is marked as required, no matter what I do a number like 200733201 is getting turned into 2007-332-201.\u00a0 I've tried using input contexts, spot filling, tried using synonyms to define (2007) as a prefix entity and 0-9 as \"number\" entities and do \"@prefix\u00a0@number\u00a0@number\" etc, nothing seems to work.\u00a0 I have another entity where it starts R12345678 and that one works fine, i just can't get dialogflow to just stop turning 9 digit numbers into phone numbers.\u00a0 Any ideas would be appreciated.\u00a0 Even if there was a way to use CSTT rather than Dialogflow to do the transcription is acceptable, but needing to get the engine to preference a 9 digit response.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deploying AutoML tabular model changes feature column types to text",
        "Question_tag_count":3,
        "Question_created_time":"2022-03-15T03:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deploying-AutoML-tabular-model-changes-feature-column-types-to\/m-p\/403658#M236",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":72,
        "Question_body":"I\u2019ve trained an AutoML tabular model using a pretty simple CSV file of numeric data. When I ran the training I ensured each feature column was set as numeric. When viewing the column meta data of the trained model, all columns show as numeric. However, when I deploy the model they all show as text and will only accept strings. What am I doing wrong?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"text-bison@001 tuned model serving",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-22T08:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/text-bison-001-tuned-model-serving\/m-p\/605737#M2212",
        "Question_answer_count":11,
        "Question_score_count":0,
        "Question_view_count":452,
        "Question_body":"Hi! I would like to tune a model based on text-bison@001\u00a0and have it run online inferences. The documentation about how to tune is very clear. However, I can't figure out how Vertex serves my model for inference.\n\nDo I need to deploy the tuned model to an endpoint and pay hourly? If so, what instance type is necessary to support the tuned model?\n\nAlternatively, is the tuned model hosted \"serverlessly\" and I pay the same (or different) per-character rate as for regular requests to the base text-bison@001\u00a0model?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Question_tag_count":3,
        "Question_created_time":"2022-11-15T07:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/m-p\/489639#M786",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":189,
        "Question_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000.\n\n\u00a0\n\nThanks!",
        "Question_closed_time":"11-15-2022 11:57 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Tensorflow Distributed ParameterServer setup",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-19T09:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Tensorflow-Distributed-ParameterServer-setup\/m-p\/613712#M2400",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":21,
        "Question_body":"Hi,\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0Earlier I asked the Tensorflow forum but didn't get a practical answer. I read the Tensorflow documentation and set up a simple\u00a0\n\ndistribution = tf.distribute.MultiWorkerMirroredStrategy()\n\nThe cluster spec. is\u00a0\n\ncluster_spec = { \"worker\":[\"127.0.0.1:9901\",\n                           \"127.0.0.1:9902\"]\n               }\n\nIt produced an appropriated result when I trained using the MNIST dataset. I documented what worked in https:\/\/branetheory.org\/2022\/05\/25\/distributed-training-using-tensorflow-federated\/\n\nBut I never understood how to use a truly distributed ParameterServer. It isn't documented because it involves set up of compute VMs, GPUs etc. I think.\n\nWhen I read the paper \"Monolith: Real Time Recommendation System With\nCollisionless Embedding Table\" this came up again. This is the diagram.\n\nCan anyone point out instructions to set up this and execute a simple training task ? I am mainly\n\ninterested in the TensorFlow distributed set up.\n\nI may set up Kafka and Flink as described in the paper for learning later.\n\nThanks,\n\nMohan",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Tables for model where comparison is required?",
        "Question_tag_count":3,
        "Question_created_time":"2021-09-29T05:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Tables-for-model-where-comparison-is-required\/m-p\/171520#M55",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":397,
        "Question_body":"Hi there,\n\nI have used GCP for a while now, and have trained quite a few models using AutoML Tables - all of these have been fairly simple datasets with probably a maximum of 20 columns.\n\n\u00a0\n\nI now have a problem that I would like to solve, but the dataset is a lot more complicated. I want to be able to predict the results of Greyhound Racing, or at least the % chances of each Greyhound winning a given race, compared to the other greyhounds running in that same race.\n\n\u00a0\n\nTo be able to do this I need to feed multiple pieces of data for each Greyhound in each given race, to be able to predict the winning chance of that greyhound in that day's race.\n\nHowever, I am very stuck on how to structure my data. Using AutoML Tables - would I need to structure the data in a tabular form with many columns? Or is there a better way to tackle this problem.\n\nHere is an example of the data I would be using:\n\nRace:\n\nExample data for each Greyhound in the race:\n\n\u00a0\n\nDoes anyone please have any advice of how to tackle this kind of problem, and how best to structure the data to attempt to predict the winning chance of each Greyhound for that day's race, based on that greyhound's previous data, compared to the other greyhounds in that day's race?\n\n\u00a0\n\nThanks,\nRob",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google cloud transcription API",
        "Question_tag_count":2,
        "Question_created_time":"2022-05-26T21:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-transcription-API\/m-p\/426546#M362",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":134,
        "Question_body":"I would like to calculate the time duration for every speaker in a two way conversation call with speaker tag, transcription, time stamp of speaker duration and confidence of it.\n\nFor example: I have mp3 file of a customer care support with 2 speaker count. I would like to know the time duration of the speaker with speaker tag, transcription and confidence of the transcription.\n\nI am facing issues with end time and confidence of the transcription. I'm getting confidence as 0 in transcription and end time is not appropriate with actual end time.\n\naudio link:\u00a0https:\/\/drive.google.com\/file\/d\/1OhwQ-xI7Rd-iKNj_dKP2unNxQzMIYlNW\/view?usp=sharing",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Trying to do multiple voice files with speech-to-text",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-11T18:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Trying-to-do-multiple-voice-files-with-speech-to-text\/m-p\/422295#M325",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":176,
        "Question_body":"Hello.\n\nI'm someone who's trying to make speech-to-text work without being a coder in any way whatsoever. I have let's say hundreds of individual audio files and they go from 30 seconds to a minute and a half. The problem is that uploading them to the bucket makes it so there's hundreds of individual ones. And I need to create a transcriptions individually. what do I do? can I not just transcribe everything in one folder?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Language was support with machine translation engine and not supported any more or got error 400",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-18T23:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Language-was-support-with-machine-translation-engine-and-not\/m-p\/545148#M1694",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":91,
        "Question_body":"- Last time we work well when choose Arabic Iraq to deal with Kurdish Sorani on google machine engine, but now this is not run well and give us Arabic or kurdish Kurmanji translation instead.\nis this a reason for that? how to fix?\u00a0how we supposed to deal with Kurdish Sorani now?\n- also for Dari language I got the following error while I read that it is supported from google machine translation engine:\nPretranslation from the machine translation did not complete. Detail: Google Translate API returned error 400: Bad language pair: en|prs (additional info: language pair en-prs is not supported)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Real-time feature engineering",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-12T10:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Real-time-feature-engineering\/m-p\/509639#M1059",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"I was wondering if there's a way to do real-time feature engineering with Vertex AI featurestore? An example use-case is if you needed to calculate\u00a0Z-score for a transaction\u00a0in real time on a huge number of transactions per second, while also keeping this feature up to date for a quick serving.\u00a0If this possible, how would it be done?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How can I use this specific voice? (English (en-gb-x-gbg-network)",
        "Question_tag_count":2,
        "Question_created_time":"2022-01-20T22:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-use-this-specific-voice-English-en-gb-x-gbg-network\/m-p\/184984#M186",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":430,
        "Question_body":"Goal: Given text, generate mp3 files using Google Cloud tts services\n\nProblem: Unable to find specific voice I am used to hearing English (en-gb-x-gbg-network).\u00a0\n\nOther info: I've been using this tts app on android in which I can select the aforementioned Voice Type from the Google TTS engine on android. I have since created a Google Cloud account, and followed the tutorial to setup a project to which I can use their selection of voices. However, when I went through the list of voice that I can use, the\u00a0en-gb-x-gbg-network was not available to use. AFAIK,\u00a0en-gb-x-gbg-network is not a premium WaveNet voice type.\u00a0\n\nI suspect it has something to do with android but I can't not see why I can't use this voice on the Google Cloud Platform.\n\n\u00a0\n\nMany thanks for any helpful info or any nudge that can point me to the right direction\n\nCheers,\u00a0\n\nWelp",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Streaming Ingestion into Vertex AI Feature Store",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-12T09:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Streaming-Ingestion-into-Vertex-AI-Feature-Store\/m-p\/441577#M411",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":383,
        "Question_body":"I'm just wondering if Vertex AI Feature Store supports streaming ingestions rather than just batch ingestion as seen here (https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/ingesting-batch). I figured that the presence of an online store (https:\/\/cloud.google.com\/vertex-ai\/pricing) means that there is a way to store the most up-to-date data and serve them.\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How can I deploy a pretrained fasttext model?",
        "Question_tag_count":2,
        "Question_created_time":"2022-03-13T01:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-deploy-a-pretrained-fasttext-model\/m-p\/403114#M235",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":391,
        "Question_body":"Hi, I have this code :\u00a0\n\n\"\n\nimport fasttext\nft = fasttext.load_model('pretrained model location')\n\"\nIs there any way I could deploy this model using the google cloud platform? I've been looking around and there doesn't seem to be a way to do it. When I want to create a model it wants me to choose scikit or tensorflow but nothing allows me to deploy this model. Can anyone help? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"RESOURCE_EXHAUSTED at custom-training-job.",
        "Question_tag_count":3,
        "Question_created_time":"2023-01-21T18:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/RESOURCE-EXHAUSTED-at-custom-training-job\/m-p\/512495#M1111",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":150,
        "Question_body":"I have trouble to execute this below vertex pipeline example from google in vertexAI workbench notebook.\n\nhttps:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/goog...\n\nI have below error code which mention\u00a0RESOURCE_EXHAUSTED at custom-training-job.\n\n\"com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\"\n\nI change kind of cpu or memory . however it is meaningless for me.\n\nCould you please give me some idea?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to connect Elasticsearch to Jupyter Notebook on GCP",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-08T02:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-connect-Elasticsearch-to-Jupyter-Notebook-on-GCP\/m-p\/475882#M630",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":300,
        "Question_body":"Hi,\n\nI am working on an ML project which requires to use Transformers and Elasticsearch.\n\nFor Transformers, I have created a Jupyter Notebook instance on GCP.\n\nFor Elasticsearch I will create another instance on CGP.\n\nAs part of project requirement, I need to access Elasticsearch in Jupyter Notebook through port 9200. I need to ingest data in Elasticsearch and run search queries which can fetch relevant information from Elasticsearch DB and give it in Notebook.\n\nMy question is that if I create two separate instances of Notebook and Elasticsearch and if i try to connect Elasticsearch through Jupyter Notebook via port 9200, will i be able to connect and perform the above mentioned operations ?\n\nIf NO, then what is the procedure to do so ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"chat-bison model not able to answer in closed context.",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-17T02:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/chat-bison-model-not-able-to-answer-in-closed-context\/m-p\/554175#M1940",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":623,
        "Question_body":"I am using `chat-bison` model for chat-bot to answer question based on retrieved documents.\u00a0 However, it answers questions from its own knowledge without caring about the context.\n\nHere is a small experiment, I tried.\n\nExample 2\nd=[{'context': 'Please answer the user question based on only the CONTEXT provided. Do not answer of your own. CONTEXT: Sachin Tendulkar is a cricketer. He was born is 1980.', 'messages': [{'author': 'user', 'content': 'When was Sachin Tendulkar born?'}]}]\n\nAnswer by chat-bison model\nSachin Ramesh Tendulkar was born on April 24, 1973 in Mumbai, India. He is a former international cricketer who served as captain of the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket.\n\nAlso, what are the values of `author` which depicts if its `user`, `bot`, `system`?\n\n@mchrestkha1",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error- Warning message: Annotation label is deduped",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-13T00:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-Warning-message-Annotation-label-is-deduped\/m-p\/521402#M1274",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":313,
        "Question_body":"I have uploaded a csv for image classification. I have checked the csv for any errors or duplicate values. But when I import the csv, i get the error that annotation is deduped and only a few images out of all the images are getting imported.\n\nWhat could be the possible cause for this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"The model monitoring job is not executed.",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-01T21:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-model-monitoring-job-is-not-executed\/m-p\/599304#M2065",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"I followed the URL below to deploy the model to the endpoint.\n\nhttps:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/goog...\n\nManually I enabled monitoring for the model, but the monitoring job did not run and I got the message \"The first run of the monitoring job is pending.\"\u00a0\n\nI tried both Training-serving skew detection and Prediction drift detection, but they did not run.\n\nHow do I run a monitoring job?\n\nI requested 20 predictions after deploying.\u00a0Need more requests?\n\n\u25cbSettings\n\nMonitoring window length: 1hour\n\nSampling rate: 100%\n\nPrediction input schema: not specified",
        "Question_closed_time":"06-08-2023 09:54 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@YoshinaoMori,\n\nWelcome to Google Cloud Community!\n\nYou need to provide the schema of your table dataset in order for it to correctly parse the input payload. To provide your own input schema, you must create a YAML file. you can refer to this link for an example:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/schemas#custom-input-schemas-online\nIf you are trying to use automatic schema parsing where the model monitoring automatically parses the input schema, you might need 1,000 input requests in order for the model monitoring to determine its schema. It will work best if the input requests are in key-value pairs. Here is an example:\n\n{\"Name\":\"Mike\", \"age\":\"30\", \"gender\":\"M\", \"ethnicity\":\"latin american\"}\n\nYou can check this link for more information:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/model-monitoring\/schemas#automatic-schema-parsing-online\n\nYou can also check this guide on how to turn on monitoring models:\u00a0https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/monitor-models-training-serving-skew-v...\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Download \/ Print Evaluation report",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-17T06:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Download-Print-Evaluation-report\/m-p\/612912#M2384",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":48,
        "Question_body":"How can I print or download all of my evaluation data, and precision curves for publication?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Custom AI Model with product documents",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-19T23:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-AI-Model-with-product-documents\/m-p\/604674#M2194",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"We intend to create an AI model that possesses contextual knowledge about any given product. The model will be trained using various documents associated with the product like requirement documents, Functional Specifications, Technical Design, etc. The trained AI model should maintain the context of the product and provide solutions to product-specific queries or problems. Currently, we are not able to train AI models with huge product docs without losing the context. The LLMs which we used are splitting the docs into multiple portions and the responses are losing context due to that. Also, we are facing token and character limitations when we wanted to use Vertex AI.\n\nIs this possible with vertex AI? If possible how to implement this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":".",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-22T14:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/unreadable-title\/m-p\/469789#M592",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":92,
        "Question_body":".",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How can I effectively use Google Cloud for machine learning and AI?",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-29T07:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-effectively-use-Google-Cloud-for-machine-learning-and\/m-p\/515192#M1149",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"Using Google Cloud's suite of tools and services for machine learning and artificial intelligence. The person asking the question is looking for steps they can follow to make the most of Google Cloud's capabilities in this area, including how to determine their requirements, preprocess and clean data, train and evaluate models, deploy models, and monitor and maintain them. The goal is to build high-quality, reliable models that deliver real-world impact.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GCP Forecast horizon without literal value",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-03T07:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-Forecast-horizon-without-literal-value\/m-p\/517704#M1196",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":126,
        "Question_body":"Is there a way to set horizon value with ML.FORECAST without using literal value\n\nselect value_type as movement_type , date(forecast_timestamp) as month_date, cast(forecast_value as INT64)   as value_count from \nML.FORECAST(MODEL `model_name`, STRUCT( (select days_left from month_table) as horizon, .90 as confidence_level) )\n\nI get the following error:\n\n\u00a0Query error: Invalid table-valued function ML.FORECAST Table Valued Function expects the settings struct to have literal constant",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"StreamingRecognize api very slow this morning (3\/24)?",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-24T09:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/StreamingRecognize-api-very-slow-this-morning-3-24\/m-p\/536621#M1475",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":74,
        "Question_body":"Today March 24 between 9-10 AM EST I found that for some reason the streamingrecognize api became very slow.\u00a0 For instance, after starting a streaming transcribe and saying \"no\" I got no transcripts back for 14 seconds.\u00a0 I then called WritesDone() on my\u00a0\n\ngrpc::ClientReaderWriterInterface<StreamingRecognizeRequest, StreamingRecognizeResponse> object and it took a full 40 seconds to complete getting a final transcript which did have the \"no\".\n\u00a0\nWas there an issue with the service at this time does anyone know?",
        "Question_closed_time":"03-28-2023 06:54 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Upon checking the service health dashboard, there has been no recent issue. If you're experiencing the same, I would recommend reaching out to Google Cloud support for further assistance.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Does Vertex AI support multi model endpoints",
        "Question_tag_count":2,
        "Question_created_time":"2021-07-07T03:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-multi-model-endpoints\/m-p\/163169#M11",
        "Question_answer_count":8,
        "Question_score_count":2,
        "Question_view_count":0,
        "Question_body":"We have 100's of models and deploying each one to its independent endpoint is very expensive.We are looking for a way to deploy multiple models to a single endpoint.Our docker image will have all the models and we will be having custom logic to invoke the models based on the request from the endpoint.\n\nSimilar functionality is available in AWS SageMaker.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Usage of spot machines while training in Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2023-01-19T11:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Usage-of-spot-machines-while-training-in-Vertex-AI\/m-p\/511862#M1094",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":548,
        "Question_body":"Hello GCP community, I have the following question, I am training in Vertex using a custom container, I am porting pipelines that were in Kubeflow to vertex and using this to train:\n\n\n\nfrom google.cloud import aiplatform\n\njob = aiplatform.CustomContainerTrainingJob(display_name=\"training-job\", container_uri=container_uri)\n# define training code arguments\ntraining_args = [\"--num-epochs\", \"2\", ]\nmodel = job.run(\nreplica_count=1,\nmachine_type=\"n1-standard-8\",\naccelerator_type=\"NVIDIA_TESLA_V100\",\naccelerator_count=1,\nargs=training_args,\nsync=False,\n)\n\nIt looks ok, but here is my question is there anyway in which I can do the training but in a SPOT machine to try to reduce my training costs.\n\nThanks!",
        "Question_closed_time":"01-19-2023 12:09 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi David\n\nNo unfortunately there is no support for spot \/ preemptible instances with Vertex AI.\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Image-to-Image Translation Possible w\/ Google Tranlsate API ?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-04T01:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Image-to-Image-Translation-Possible-w-Google-Tranlsate-API\/m-p\/599844#M2077",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":112,
        "Question_body":"Hi. I plan to utilize a multimodal vision-language transformer that only (reasonably) supports English. The image translation feature in the web version of Google almost perfectly suits my needs. Is this available for use via an API? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Enconding $session.params.[parameter] on Dialog Flow CX ERROR: ByteString is not valid UTF8",
        "Question_tag_count":3,
        "Question_created_time":"2023-03-15T17:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Enconding-session-params-parameter-on-Dialog-Flow-CX-ERROR\/m-p\/533056#M1429",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":248,
        "Question_body":"I am having a problem to display a message of type $session.params.[parameter] on DialogFlow CX\n\nI am reading a json file from Google Bucket and when the Json is with no special characters it works fine example\u00a0Convencao.\n\nBut when I put some special character example\u00a0Conven\u00e7\u00e3o\u00a0it not working giving me the error below:\n\n   \"FunctionExecution\": {\n    \"Webhook\": {\n      \"URL\": \"example.com\",\n      \"Status\": {\n        \"ErrorCode\": \"INTERNAL\",\n        \"ErrorMessage\": \"Response body [<ByteString@40a8d64c size=182 contents=\\\"{\\\\n  \\\\\\\"session_info\\\\\\\": {\\\\n    \\\\\\\"parameters\\\\\\\": {\\\\n     ...\\\">] is not valid UTF8.\"\n      },\n      \"DisplayName\": \"example\",\n      \"Latency\": \"170 ms\",\n      \"ID\": \"a8660475-f28b-440e-ad4c-4e8506cb6795\"\n    },\n    \"Responses\": [\n      {\n        \"responseType\": \"ENTRY_PROMPT\",\n        \"text\": {\n          \"text\": [\n            \"Escolha o evento:\\n1. $session.params.evento1\\n2. $session.params.evento2\\n3. $session.params.evento3\\n4. Outros Eventos\"\n          ],\n          \"redactedText\": [\n            \"Escolha o evento:\\n1. $session.params.evento1\\n2. $session.params.evento2\\n3. $session.params.evento3\\n4. Outros Eventos\"\n          ]\n        },\n        \"source\": \"VIRTUAL_AGENT\"\n      }\n    ]\n  }\n}\n\n},\n\nThis is the .json that work\n\n{\n   \"eventname1\":{\n      \"eventname\":\"Test Diamond Summit\",\n      \"eventdate\":\"07 at\u00e9 12 de Fevereiro\",\n      \"eventlocation\":\"Kenya\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname2\":{\n      \"eventname\":\"Test de Incentivo 2023\",\n      \"eventdate\":\"08 at\u00e9 12 de Mar\u00e7o\",\n      \"eventlocation\":\"Ilha de Comandatuba\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname3\":{\n      \"eventname\":\"Convencao Florescer 2023\",\n      \"eventdate\":\"Sess\u00f5es complementares: 17 de maio de 2023 - Conven\u00e7\u00e3o: 18, 19 e 20 de maio 2023\",\n      \"eventlocation\":\"Rio de Janeiro \/ RJ\",\n      \"eventsite\":\"example.com\"\n   },\n   \"edicao\":{\n      \"edicaoname\":\"Motiva\u00e7\u00e3o\",\n      \"duracao\":\"1\u00b0 de Dezembro \u00e0 28 de Fevereiro\",\n      \"link\":\"example.com\"\n   },\n   \"businesshourPT\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   },\n   \"businesshourES\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   }\n}\n\nThis is the .json that not work\n\n{\n   \"eventname1\":{\n      \"eventname\":\"Test Diamond Summit\",\n      \"eventdate\":\"07 at\u00e9 12 de Fevereiro\",\n      \"eventlocation\":\"Kenya\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname2\":{\n      \"eventname\":\"Test de Incentivo 2023\",\n      \"eventdate\":\"08 at\u00e9 12 de Mar\u00e7o\",\n      \"eventlocation\":\"Ilha de Comandatuba\",\n      \"eventsite\":\"example.com\"\n   },\n   \"eventname3\":{\n      \"eventname\":\"Conven\u00e7\u00e3o Florescer 2023\",\n      \"eventdate\":\"Sess\u00f5es complementares: 17 de maio de 2023 - Conven\u00e7\u00e3o: 18, 19 e 20 de maio 2023\",\n      \"eventlocation\":\"Rio de Janeiro \/ RJ\",\n      \"eventsite\":\"example.com\"\n   },\n   \"edicao\":{\n      \"edicaoname\":\"Motiva\u00e7\u00e3o\",\n      \"duracao\":\"1\u00b0 de Dezembro \u00e0 28 de Fevereiro\",\n      \"link\":\"example.com\"\n   },\n   \"businesshourPT\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   },\n   \"businesshourES\":{\n      \"intervalName\":\"MONDAY-FRIDAY-08:59-20:01\"\n   }\n}\n\nThis is my java code\n\n\/\/Used to collecting bucket data (all parameterized data)\npublic JSONObject readBucket() throws IOException {\n    Storage storage = StorageOptions.getDefaultInstance().getService(); \n    BlobId blobId = BlobId.of(\"test\", \"test_properties.json\");\n    byte[] content = storage.readAllBytes(blobId);\n    String contentString = new String(content,  StandardCharsets.UTF_8);\n\n    JSONObject jsonObj = new JSONObject(contentString.toString());\n    return jsonObj;\n}\n\n\n\/\/Used to build webhookResponse for Dialog Flow CX\npublic JsonObject setSessionParameter(JsonObject parameterObject){\n    \/\/ Constructs the webhook response object\n    JsonObject parameterSessionObject = new JsonObject();\n    parameterSessionObject.add(\"parameters\", parameterObject); \n    JsonObject webhookResponse = new JsonObject();\n    webhookResponse.add(\"session_info\", parameterSessionObject);   \n    return webhookResponse;\n}\n\n\n\n\n@Override\n  public void service(HttpRequest request, HttpResponse response) throws Exception {    \n    \/\/Create DialogFlow Objects\n    Gson gson = new GsonBuilder().setPrettyPrinting().create();\n    Map<String, Object> requestBody = gson.fromJson(request.getReader(), Map.class);\n    Map<String, Object> sessionInfo = (Map<String, Object>) requestBody.get(\"sessionInfo\");\n    Map<String, Object> parameters = (Map<String, Object>) sessionInfo.get(\"parameters\");\n    Map<String, Object> fulfillmentInfo = (Map<String, Object>) requestBody.get(\"fulfillmentInfo\");\n    \n    System.out.println(\"parameters: \"+parameters);\n\n    \/\/Extracting Values of DialogFlow\n    String ani = (String) parameters.get(\"ani\");\n    System.out.println(\"ani: \"+ani);\n    String tag = (String) fulfillmentInfo.get(\"tag\");\n    System.out.println(\"tag: \"+tag);\n    \n    JSONObject con;\n    String status;\n    JsonObject parameterObject;\n    String jsonResponseObject;\n    BufferedWriter writer;\n    String intervalName;\n    JSONObject properties;\n    String eventname;\n    String eventname1value;\n    String eventname2value;\n    String eventname3value;\n    String eventdate;\n    String eventlocation;\n    String eventsite;\n    JSONObject edicao;\n    String edicaoduracao;\n    String edicaonome;\n    String edicaolink;\n    JSONObject eventname1;\n    JSONObject eventname2;\n    JSONObject eventname3;\n    JSONObject eventnamedtmf;\n    JSONObject bhour;\n    String iaeventmenudtmf = null;\n    String intervalNameFormat;\n    \n\n    \/\/Checking Routes\n    try {\n        switch(tag) {\n        case \"11040\":\n            properties = readBucket();\n\n            \/\/Writing values on Objects that will became Parameters\n        eventname1 = properties.getJSONObject(\"eventname1\");\n        eventname1value = eventname1.getString(\"eventname\");\n        eventname2 = properties.getJSONObject(\"eventname2\");\n        eventname2value = eventname2.getString(\"eventname\");\n        eventname3 = properties.getJSONObject(\"eventname3\");\n        eventname3value = eventname3.getString(\"eventname\");\n    \n            \/\/Creating Response Objects\n            parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"evento1\", eventname1value.toString());    \n            parameterObject .addProperty(\"evento2\", eventname2value.toString());    \n            parameterObject .addProperty(\"evento3\", eventname3value.toString());\n            jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n            \n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObject.toString());\n            break;\n    \n        case \"11100\":\n            \/\/Get user option to concat file bucket file\n            iaeventmenudtmf = (String) parameters.get(\"iaeventmenu\");\n            System.out.println(\"iaeventmenudtmf: \"+iaeventmenudtmf);\n              \n            properties = readBucket();\n\n            \/\/Writing values on Objects that will became Parameters\n            eventnamedtmf = properties.getJSONObject(\"eventname\"+iaeventmenudtmf);\n        eventname = eventnamedtmf.getString(\"eventname\");\n        eventdate = eventnamedtmf.getString(\"eventdate\");\n        eventlocation = eventnamedtmf.getString(\"eventlocation\");\n        eventsite = eventnamedtmf.getString(\"eventsite\");\n            \n        \/\/Creating Response Objects\n            parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"evento\", eventname.toString());   \n            parameterObject .addProperty(\"eventoData\", eventdate.toString());   \n            parameterObject .addProperty(\"eventoLocal\", eventlocation.toString());      \n            parameterObject .addProperty(\"eventoSite\", eventsite.toString());\n            jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n            \n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObject.toString());\n            break;\n              \n        case \"12040\":\n            properties = readBucket(); \n    \n            \/\/Writing values on Objects that will became Parameters\n            edicao = properties.getJSONObject(\"edicao\");\n            edicaonome = edicao.getString(\"edicaoname\");\n            edicaoduracao = edicao.getString(\"duracao\");\n            edicaolink = edicao.getString(\"link\");\n             \n            \/\/Creating Response Objects\n            parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"edicao\", edicaonome.toString());      \n            parameterObject .addProperty(\"duracao\", edicaoduracao.toString());      \n            parameterObject .addProperty(\"link\", edicaolink.toString());\n            jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n            \n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObject.toString());\n            break;\n                \n        default:\n            break;\n            \n        }\n    }catch(Exception e) {\n        parameterObject  = new JsonObject();\n        parameterObject .addProperty(\"error\", \"999 Error on request - Class:  \" + getClass().getName() + \": Message\" + e.getMessage()+ \" Cause: \"+e.getCause() + \" Stack: \" +Arrays.toString(e.getStackTrace()));       \n        jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n        System.out.println(\"jsonResponseObject: \"+jsonResponseObject.toString());\n          \n        \/\/Sends the responseObject\n        writer = response.getWriter();\n        writer.write(jsonResponseObject.toString());\n    }\n  }\n}\n\nThis is my return on Google Cloud Function Log\n\njsonResponseObject: {   \"session_info\": {\n    \"parameters\": {\n      \"evento1\": \"Blue Diamond Summit\",\n      \"evento2\": \"Viagem de Incentivo 2023\",\n      \"evento3\": \"Conven\u00e7\u00e3o Florescer 2023\"\n    }   } }\n\nWhen it passes\u00a0Conven\u00e7\u00e3o\u00a0I receive the error above when it is\u00a0Convencao\u00a0it displays the message with success.\n\nI tried some enconding UTF-8 ways without suceess\n\nI tried some solutions below\n\n            byte[] jsonResponseObjectb = jsonResponseObject.getBytes(StandardCharsets.UTF_8);\n        String jsonResponseObjects = new String(jsonResponseObjectb, StandardCharsets.UTF_8);\n\n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObjects);\n\n\n        \/\/handle encoding error on Dialog Flow CX\n        CharsetDecoder decoder = StandardCharsets.UTF_8.newDecoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);\n\n        byte[] eventname1valueb = eventname1value.getBytes(StandardCharsets.UTF_8);\n        String eventname1values = decoder.decode(ByteBuffer.wrap(eventname1valueb)).toString();\n            \n        byte[] eventname2valueb = eventname2value.getBytes(StandardCharsets.UTF_8);\n        String eventname2values = decoder.decode(ByteBuffer.wrap(eventname2valueb)).toString();\n\n        byte[] eventname3valueb = eventname3value.getBytes(StandardCharsets.UTF_8);\n        String eventname3values = decoder.decode(ByteBuffer.wrap(eventname3valueb)).toString(); \n            \n            \n                \/\/Creating Response Objects\n        JsonObject parameterObject  = new JsonObject();\n            parameterObject .addProperty(\"evento1\", eventname1value.toString());    \n            parameterObject .addProperty(\"evento2\", eventname2value.toString());    \n            parameterObject .addProperty(\"evento3\", eventname3value.toString());\n            String jsonResponseObject = gson.toJson(setSessionParameter(parameterObject));\n            \n                \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObjects);\n\n\n            byte[] jsonResponseObjectb = jsonResponseObject.getBytes(StandardCharsets.UTF_8);\n        String jsonResponseObjects = new String(jsonResponseObjectb, StandardCharsets.UTF_8);\n            String encodedString = URLEncoder.encode(jsonResponseObjects, \"UTF-8\");\n\n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(encodedString);\n\n\n            CharsetDecoder decoder = StandardCharsets.UTF_8.newDecoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);\nbyte[] \n                jsonResponseObjectb = jsonResponseObject.getBytes(StandardCharsets.UTF_8);\n        String jsonResponseObjects = decoder.decode(ByteBuffer.wrap(eventname3valueb)).toString();\n\n            \/\/Sends the responseObject\n            writer = response.getWriter();\n            writer.write(jsonResponseObjects);\n\nMy return must be\n\nChoose the event:\n\nTest Diamond Summit\nTest de Incentivo 2023\nConven\u00e7\u00e3o\u00a0Test 2023\nOther Events\n\nbut in it is\u00a0Conven\u00e7\u00e3o\u00a0I received\n\nChoose the event:\n\n$session.params.evento1\n$session.params.evento2\n$session.params.evento3\nOther Events\n\nbut in it is\u00a0Convencao\u00a0I received\n\nChoose the event:\n\nTest Diamond Summit\nTest de Incentivo 2023\nConvencao\u00a0Test 2023\nOther Events\n\nNo one of them worked\n\nI do not know more what I could try to do to solve this error\n\nCould you help me please?\n\nTried to use some enconding methods that I know",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"PaLM APIs web access",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-19T12:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/PaLM-APIs-web-access\/m-p\/604556#M2190",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":90,
        "Question_body":"Hi, Is it possible to recreate a Bard-like output using the PaLM API? Neither the chat-bison nor text-bison models seem to have web access. Both return hallucinations to questions like \"What is the address of McKinsey's New York office\" whereas Bard via the web UI can handle this consistently correctly.\u00a0\n\nIs there any way to add web access to the PaLM API?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Pipeline: The replica workerpool0-0 exited with a non-zero status of 13",
        "Question_tag_count":3,
        "Question_created_time":"2023-03-08T03:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Pipeline-The-replica-workerpool0-0-exited-with-a-non\/m-p\/530067#M1391",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":411,
        "Question_body":"I'm trying to run a vertex AI pipeline (AutoML) using default setting and required service account permission but every time i run that pipeline, i will get failed with above error.\u00a0\n\n\n\u00a0\n\n\nsame i had run with custom will success\u00a0but when running with AutoML for Tabular Classification \/ Regression it will get failed after long process time.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow Messenger: Setting up different environments",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-01T09:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-Messenger-Setting-up-different-environments\/m-p\/599125#M2060",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":103,
        "Question_body":"Hello, I'm using Dialogflow ES and integrating it onto our site using DIaglogflow Messenger.\u00a0 I'm now looking into creating environments and I can follow the steps fine at\u00a0https:\/\/cloud.google.com\/dialogflow\/es\/docs\/agents-versions.\u00a0 However, what I want to do is have a development environment integrated into our development site and have a production environment integrated into our production site.\u00a0 When I go to Dialogflow's Integration settings, I can only activate one environment at a time (draft, development, OR production).\u00a0 I also see the embed code is the same between the environments.\u00a0 Is it possible to have the draft, development, AND production environments active at the same time?",
        "Question_closed_time":"06-06-2023 04:41 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@fogpuddle,\n\nWelcome to Google Cloud Community!\n\nUnfortunately,\u00a0Dialogflow messenger in Dialogflow CX is still in preview, you can only test one environment at a time using Dialogflow messenger. Although, as a workaround you can try creating another agent by exporting your current agent in a different version (development), then you can embed the code in your production site and development site. Also, if you want this feature you can submit a feature request using this link:\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"VertexAI- Auto ML training model failed without giving the reason",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-21T00:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-Auto-ML-training-model-failed-without-giving-the-reason\/m-p\/445439#M435",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":226,
        "Question_body":"After an hour of training Auto ML with Vertex AI, it failed without mentioning the reason. I have received the following email;\n\"Due to an error, Vertex AI was unable to train model \"some_model\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name:\u00a0\nprojects\/xxxxxxxxxxxxxxx\/locations\/region\/trainingPipelines\/xxxxxxxxxxxxxxxxxxxxxxxx\nError Messages: Internal error occurred. Please retry in a few minutes. If\u00a0\nyou still experience errors, contact Vertex AI.\"\n\n\nWould you please help me with it?\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI -- text to image",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-29T15:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-text-to-image\/m-p\/607902#M2264",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":121,
        "Question_body":"I saw a YouTube video showing the incredible capability to use Vertex AI as a tool for prompt to image (similar to Leonardo AI).\u00a0 But I cannot seem to find that capability when going to my Vertex AI landing page.\u00a0 All I see under my Generative AI heading is Language and Speech.\u00a0 How to I get to the Vertex AI Image Generation page?\u00a0 Thx.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Agent struggles to get a car registration number\/postcode over voice",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-23T11:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Agent-struggles-to-get-a-car-registration-number-postcode-over\/m-p\/596157#M2007",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"I'm struggling to get my Dialogflow CX agent to understand user's car registration number (e.g. GV15YGH + lot of other formats).\n\nI'm using VoxImplant integration and I have created Regexp entity to match UK vehicle registration numbers, including spaces after each letter\/number to mimic user making a small break after each letter (like spelling). On chat everything works fine, so I can confirm the regex is correct, but over the phone, most of the time, the agent will missunderstand the letters, for example mixing a 'B' with a 'D', 'F' with 'S' and so on.\u00a0\n\nSimilar issue with understanding postcodes, which are quite similar format (e.g. SO55 7LD).\n\nI have implemented a solution for spelling foreign names and there I'm using a list of a custom letter entity I have created (this includes english alphabet letters and nato phonetic alphabet), and in this case, the agent understands lot better those letters, so tend to believe this might be a problem with regexp entities over voice conversations.\n\nI'm a bit stuck at the moment, so any suggestions are very welcome. Thank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Time taking to take A Job in a Cluster on GCP DataProc",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-21T13:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Time-taking-to-take-A-Job-in-a-Cluster-on-GCP-DataProc\/m-p\/524763#M1305",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":42,
        "Question_body":"Dear Googlians,\n\nThere was a problem relevant to the time taken to create a cluster on GCP Dataproc. What may cause such a problem?\n\nRegards.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"English to Chinese glossary is ignored",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-20T03:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/English-to-Chinese-glossary-is-ignored\/m-p\/524065#M1299",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"I am trying to translate domain-specific English (en-US) text to Chinese simplified (zh). I have created a glossary, however, it looks like CloudTranslate gives precedence to other translations over the glossary.\n\nFor example, the word \"fabric\" has many translations to Chinese, so the glossary has an entry \"the fabric,\u4ea4\u6362\u7f51\", but the API returns \"\u9762\u6599\", which is the same translation as I get from Google Translate website.\n\nQuestion 1: Since I didn't get any error, is it true to assume that the glossary was found and used?\n\nQuestion 2: I expected that the glossary will get strict precedence over other translations. Is there a way to force it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What's the most efficient way to load data for training?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-01T07:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-most-efficient-way-to-load-data-for-training\/m-p\/599080#M2059",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":81,
        "Question_body":"I currently have a training task that loads sharded CSV files from GCS using TorchData library (training code in Pytorch).\n\n\u00a0\n\nHowever I notice that my GPU usage has like ~ 2-3 minutes of 0% utilisation after each epochs, which I presume is due to I\/O issues of streaming data from GCS and starving my GPU.\n\nWhat's the most efficient way of getting around this? Would it be to download all my files from GCS to my compute instance, then loading the data directly?",
        "Question_closed_time":"06-05-2023 05:22 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@shengy90,\n\nWelcome to Google Cloud Community!\n\nYou can validate the following suggestions:\n\n1.\u00a0You try implementing WebDatasets. It shards and compiles multiple data files into POSIX tar archive files, it doesn't do any format conversion and the data format is the same in the tar file as it is on the disk, and it can be created with the tar command. WebDataset is a great way to achieve Sequential I\/O since it will read the individual files in the tar file. This will be helpful since the data is collected in GCS which is in a remote setting, it will provide faster I\/O of objects over the network and will reduce potential bottlenecks.\u00a0You can check this blog post for more information regarding WebDatasets:\u00a0https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/scaling-deep-learning-workloads-pytorc...\n\n2. Google Cloud Storage Fuse is used for accessing data on Cloud Storage for Vertex AI training. This will allow you to access Google Cloud Storage as a local file system which provides high throughput, by simply using the code:\u00a0\n\nfile = open('\/gcs\/bucket-name\/object-path', 'r')\n\nYou can use this link to learn more:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/code-requirements#fuse\n\u00a0\n3. Also for best performance, your bucket must reside in the region where you are performing the custom training. You can use this link to learn more:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/code-requirements#loading-data\n\n4. You can also use this blog as a guide on how to efficiently train with vertex AI, you can check the demonstration using this link:\u00a0https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/efficient-pytorch-training-with-vertex-ai\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"403 error when trying to access PalmAPI",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-15T19:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/403-error-when-trying-to-access-PalmAPI\/m-p\/553667#M1917",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"Dear community,\n\nI got an invite to try out PalmAPI through makesuite, but unfortunately it is showing me the following error: The account <my-email>@gmail.com doesn't have permission to see this page.\nContact your admin for access or use a different account.\n\nis there something I can do to fix this? note that I am trying to log in with the account I got the invite for.\n\n\u00a0\n\nappreciate all the insights",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Palm2 not able to read urls?",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-15T10:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Palm2-not-able-to-read-urls\/m-p\/553579#M1915",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":56,
        "Question_body":"I've tried this in GenerativeAI studio and via API, but it's either giving me a blank message, or not showing me via policy (in the studio).\n\n\"View this repo at <https:\/\/github.com\/aaronn\/slack-gpt>, what is it?\"\n\nHowever, this works perfectly on bard.google.com\n\nAny way to fix this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech-To-Text High Latency for audio files",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-11T02:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-To-Text-High-Latency-for-audio-files\/m-p\/552344#M1855",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":58,
        "Question_body":"I am testing the Speech-To-Text API with audio files and have observed high latency (>25s) from time to time (it happens rarely). The high latency leads to a timeout on my application.\n\nI have already followed the best practices as described in here (https:\/\/cloud.google.com\/speech-to-text\/docs\/best-practices-provide-speech-data). My audio files are around 3 - 6 seconds long each. What other Best Practices are advisable to handle the long latency cases? Retry with back-off?\n\nAny hints are greatly appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow should have its own official facebook app for integration",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-13T21:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-should-have-its-own-official-facebook-app-for\/m-p\/477998#M649",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":49,
        "Question_body":"Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflow\n\nI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the\u00a0settings for\u00a0Webhooks channels it need\n\nI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experience\n\nps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cookie cutter options for newbie?",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-20T14:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cookie-cutter-options-for-newbie\/m-p\/614194#M2410",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":49,
        "Question_body":"Hi,\u00a0\n\nI'm trying to create a straightforward tool that will draw comparisons between 2 small groups of content (10 pages worth in each group). The output would be a set of bullet points, written in an informal\/professional tone. The content in group #1 will be periodically updated with additional content. Group #2's content will be completely replaced each time the tool is used.\n\nI'm still very much finding my way around the Gen AI studio -- can anyone recommend a pre-fab item that can be easily adapted? The prompt examples I see aren't quite on the mark, AFAIK.\n\nThanks in advance for any advice!\n\n[Separately -- I really wish there was a Vertex support chatbot I could use for basic questions. I appreciate there are lots of non-trivial considerations, but even something designed for newbies like me would very helpful]",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Expense Parser issue importing labels",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-29T05:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Expense-Parser-issue-importing-labels\/m-p\/607714#M2259",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":56,
        "Question_body":"We're trying to import labels for the Expense Parser, and this is the example JSON request we're using:\n\n{\n\"uri\": \"\",\n\"mimeType\": \"application\/pdf\",\n\"text\": \"11\/10\/2021, 16:42\\nOffice DEPOT\\nOfficeMax\\nLOS GATOS (408) 356-3757\\n05\/30\/2020 9:42 AM\\nV2VT5X3P5XY56YX66\\nSALE\\n5379432 PRNTER, ET-4760\\nSubtotal:\\n***\\nSales Tax:\\nTotal:\\nVisa\\n950-1-1844-473229-20.5.2\\noffice-depot-redacted.png\\nAUTH CODE 083396\\nTDS Chip Read\\nAID A0000000031010 CITI VISA\\nTVR 0800008000\\nCVS PIN Verified\\n499.99 SS\\n90499.99\\n19 45.00\\n544.99\\n544.99\\nhttps:\/\/mail.google.com\/chat\/u\/0\/#chat\/dm\/qimyvgAAAAE\\n5730827812\\nPlease create your online rewards\\naccount at officedepot.com\/rewards.\\nYou must complete your account to\\nclaim your rewards and view your\\nstatus.\\nShop online at www.officedepot.com\\nWE WANT TO HEAR FROM YOU!\\nVisit survey.officedepot.com\\nand enter the survey code below:\\n1508 QP9G OY41\\n****\\n**\\n****\\n1\/1\",\n\"page\": [],\n\"entities\": [\n{\n\"mentionText\": \"receipt_number\",\n\"type\": \"receipt_number\"\n},\n{\n\"mentionText\": \"amount\",\n\"type\": \"total_amount\"\n},\n{\n\"mentionText\": \"currency\",\n\"type\": \"currency\"\n},\n{\n\"mentionText\": \"date\",\n\"type\": \"receipt_date\"\n}\n]\n}\n\nThis errors out in the following way:\n\n\u201cinputGcsSource\u201d: \u201cgs:\/\/receipts321\/jsonimport\/output\u00a02\/2301149.json\u201d,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u201cstatus\u201d: {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u201ccode\u201d: 3,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u201cmessage\u201d: \u201cRequest contains an invalid argument.\u201d\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\nIt doesn't give any details what about what is wrong in the request. Any help appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model Garden - Tag Recognizer",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-13T04:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-Garden-Tag-Recognizer\/m-p\/602560#M2144",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":72,
        "Question_body":"I would like to test out and play around with a model I found in the Goocle Cloud model garden, namely the Tag Recognizer Model. Im trying to figure out how I could actually add this model to my model registry and train it using my images\/labelling. When I select the model in the model garden, it does not give me any options to make use for it. Am I missing something?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can I export a my model for internal usage ?",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-16T01:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-I-export-a-my-model-for-internal-usage\/m-p\/500236#M951",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":693,
        "Question_body":"Hi,\n\nI created my models with Auto ML (image classification or object detection).\n\nNow, I would like to use these in my application, on local (disconnected).\n\nIs it possible to extract a model file from Auto ML that I can use (.pb for instance) ?\n\nAfter some researches, it seems to me that it is not possible but I would like to be sure.\n\nElse, how?\n\nRegards.",
        "Question_closed_time":"12-20-2022 01:56 AM",
        "Answer_score_count":1.0,
        "Answer_body":"I just didn't understand that I need to select the \"edge\" option to have the \"export model\" available.\n\nThank you for the useful documentation .\n\nAs a additionnal question : can I know the version of Tensorflow used for these export model? (I have some incompatibiliy to use these in my software).\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Language Tuning Model - RESOURCE EXHAUSTED message",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-28T10:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Language-Tuning-Model-RESOURCE-EXHAUSTED-message\/m-p\/607459#M2255",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"While tuning language model, the process stops every time at llm model tuning step with the below error\u00a0 code=resource_exhausted, message=the following quota metrics exceed quota limits:\"\n\nWhen the Quota status is checked, the assigned quota limit for\u00a0restricted_image_training_tpu_v3_pod\u00a0\u00a0is 0. How to increase this value?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Form Parsing in Document AI",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-05T02:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Form-Parsing-in-Document-AI\/m-p\/420076#M302",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":340,
        "Question_body":"Hi All,\n\nWe are currently using Document AI for form parsing scanned documents and we are now required to capture the checkboxes data from the form.\n\nWas able to capture the Handwritten tick mark using the Form Parser.\u00a0 Have one doubt, do we have any limitation on the number of max checkboxes on a page?\n\u00a0\nWhen I was trying out the Form Parser, I found that when the checkbox number exceeded 50, it was not able to capture all the checkboxes.\n\u00a0\nCan we do anything about this? Is there any option to adjust the max count in Document AI Form Processor settings?\n\u00a0\nThank you for the help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Simple but frustrating error: Google.cloud module not found",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-28T21:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Simple-but-frustrating-error-Google-cloud-module-not-found\/m-p\/504285#M998",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":332,
        "Question_body":"Have installed Google Cloud via pip and CLI installer, yet programs cannot seem to see import statements from Google.cloud, returning the following error:\n\nline 9, in <module>\nfrom google.cloud import vision\nModuleNotFoundError: No module named 'google.cloud'\n\nPlease advise and thank you for your time.",
        "Question_closed_time":"12-30-2022 09:41 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Have you tried installing google cloud vision? You can also check what python version you are using, this package is only supported in python versions 3.7 and up.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Looking for (paid) help to help me extract text from Instagram images",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-30T05:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Looking-for-paid-help-to-help-me-extract-text-from-Instagram\/m-p\/538848#M1532",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":56,
        "Question_body":"Hello all,\n\nMy skills in data science are too low to allow me to apply Google's tutorials about Google Cloud Vision so I am looking for some (paid) help to show me step by step what I have to do to extract text from (Instagram) images. I can do it for one image but I need to be able to do it for bunches of images.\n\nMany many many thanks! \n\nCatherine",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error in Vertex AI - Language",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-21T06:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-in-Vertex-AI-Language\/m-p\/614381#M2417",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":30,
        "Question_body":"Hello everyone! I\u00a0have an issue with using the Vertex AI Language on my project. When using Science Chatbot, we need to ask questions in Portuguese and receive answers in Portuguese, but we are receiving the following error when performing the procedure: \"\nI'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback\". Can you help me with this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BigQuery ARIMA plus changes TIME_SERIES_DATA_COL data",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-18T00:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQuery-ARIMA-plus-changes-TIME-SERIES-DATA-COL-data\/m-p\/613188#M2389",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"I am using ARIMA plus model to predict anomalies. My TIME_SERIES_DATA_COL parameter is number of incidents received per day and is integer value in input. But the ARIMA plus model in output converts\u00a0 this to fraction value in some rows which is incorrect as number of incident received cannot be fractional. How to correct this behavior. My Query is as follows:\n\n\u00a0\n\nCREATE OR REPLACE MODEL `test_inputs_arima.ARIMA_15mins_model`\nOPTIONS\n(MODEL_TYPE = 'ARIMA_PLUS',\nTIME_SERIES_TIMESTAMP_COL = 'opened_at',\nTIME_SERIES_DATA_COL = 'number',\nTIME_SERIES_ID_COL = 'Issue_Category',\nHOLIDAY_REGION = ['GLOBAL'])\nAS\nSELECT\nopened_at,\nnumber,\nIssue_Category\nFROM\ntest_inputs_arima.15_mins_7days;\n\nCREATE OR REPLACE TABLE test_inputs_arima.ARIMA_15mins_out\nAS\n(SELECT\nIssue_Category,opened_at,number,is_anomaly,lower_bound,upper_bound,anomaly_probability\nFROM\nML.DETECT_ANOMALIES(\n  MODEL `test_inputs_arima.ARIMA_15mins_model`,\n  STRUCT(0.9 AS anomaly_prob_threshold)\n));\n\nUpdate test_inputs_arima.ARIMA_15mins_out\n set is_anomaly = false\nwhere number<lower_bound;\n\n\nSELECT * FROM test_inputs_arima.ARIMA_15mins_out;",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-21T03:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-GPUS-ALL-REGIONS-exceeded-Limit-0-0-globally\/m-p\/595266#M1987",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":114,
        "Question_body":"Hi,\n\nI have notebook like this\n\nWhen trying to start is I get error \"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.\"\n\nI upgraded from free trial, but it didn't help. Next I went to quota page and found no limit:\n\nWhat's the issue?\n\nThanks to all.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Method for Photo Analysis (AI\/Machine Learning\/OCR\/Others)",
        "Question_tag_count":3,
        "Question_created_time":"2022-12-01T06:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Method-for-Photo-Analysis-AI-Machine-Learning-OCR-Others\/m-p\/495040#M887",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":59,
        "Question_body":"Hi,\n\nI am very new to Google Cloud Platform, here I attached a photo of a cable being cut and measured on measuring tape. From the photo, the dimension of the cable is 5500mm. Because I am not sure which method I can use, either AI? Machine Learning? OCR engine? or others?.\n\nThis photo was captured and stored in Google Drive (using Appsheet), is there any solution in GCP able to read from this photo and\u00a0\u00a0analyze it, finally tell this is 5500.\n\nAnyone from community that took part in such project or maybe a similar project able to enlighten me in this?\n\nThanks in advance for any valuable opinion!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech-to-Text billing optimization",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-11T20:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-billing-optimization\/m-p\/477135#M641",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"Hi All\n\nAnyone knows if with the Speech-to-Text API \u00bfCan we do a committed consume contract similar with other GCP services to get fees or billing optimization by month?\n\nThanks in advance for your response",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI - Text entity extraction response format",
        "Question_tag_count":3,
        "Question_created_time":"2022-09-18T11:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Text-entity-extraction-response-format\/m-p\/467926#M584",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":281,
        "Question_body":"Hello!\n\nI would like to ask if it's possible to get the same output format from the Vertex AI entity extraction REST API as from the Google Natural Language API?\n\nBecause now in the response of Vertex AI we get only a list with the confidence scores, displayNames and the start\/end offsets of the textSegments, but the entities itself are not in the json (in the NLP response there is also \"content\" with the entity). So this is how I would like to get the response from the Vertex AI for each entity:\n\n{\n\"annotationSpecId\": \" \",\n\"displayName\": \"date\",\n\"textExtraction\": {\n\"score\": 0.69745916,\n\"textSegment\": {\n\"startOffset\": \"382\",\n\"endOffset\": \"392\",\n\"content\": \"12.07.2022\"\n}\n\nCan you help please how I could achive this?\n\nThank you for your help in advance!\n\nZsolt",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How Should I Cite the AutoML Model for Tabular Data?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-26T18:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-Should-I-Cite-the-AutoML-Model-for-Tabular-Data\/m-p\/606699#M2233",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":36,
        "Question_body":"Hello Community,\n\nI've been utilizing Google AutoML's Tabular-form dataset classifier for my study and am trying to find an official citation for the model or method. I've come across papers that cite Google's AI blog (Lu, Y. An end-to-end AutoML solution for tabular data at KaggleDays. Google AI Blog, 2019. URL: (\u00a0http:\/\/ai.googleblog.com\/2019\/05\/an-end-to-end-automl-solution-for.html ) or the website URL ( https:\/\/cloud.google.com\/vertex-ai\/docs\/training-overview#tabular ), but I'm uncertain as to which is the correct approach. It would be much appreciated if anyone could provide insight on this matter.\n\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model changes",
        "Question_tag_count":1,
        "Question_created_time":"2021-10-19T12:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-changes\/m-p\/173396#M63",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":356,
        "Question_body":"It appears that the default model for GCP Cloud Vision API has changed. Specifically, the current API results lack localized objects that were available previously, say around June 2021. How does the team communicate, if at all, what entities the model supports?\n\n\u00a0\n\nThanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Exported edge model always returns the same values",
        "Question_tag_count":2,
        "Question_created_time":"2023-01-19T06:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Exported-edge-model-always-returns-the-same-values\/m-p\/511682#M1091",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"Hi,\n\nI trained a binary classification model with AutoML, and I have exported it for use in a container.\n\nI have followed this article to the letter https:\/\/cloud.google.com\/automl-tables\/docs\/model-export and the prediction server is running OK. However, it doesn't matter what values I submit to it as the inputs, the output is always absolutely the same value.\n\nI should specify that when deploying the same model to a VertexAI endpoint, it works great. I thought the exported model and container was supposed to work the same as using an endpoint.\n\nAny ideas would be great.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"When will Generative AI Studio have access to the internet?",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-03T11:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Generative-AI-Studio-have-access-to-the-internet\/m-p\/608924#M2275",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":86,
        "Question_body":"Hey everyone, does anyone know\u00a0when Generative AI Studio will have access to the internet?\n\nWhen I prompted Generative AI Studio with \"When did the Queen died?\", its giving me silly answers, so this tells me Generative AI Studio doesn't currently have access to the internet (unlike Google Bard which has access to the internet).\n\nIf\u00a0 Generative AI Studio isn't having this feature anytime soon, what's the best way for me to add this feature into my Python code?\n\nWould really appreciate if anyone can give any pointers. Thanks!",
        "Question_closed_time":"07-04-2023 01:04 AM",
        "Answer_score_count":2.0,
        "Answer_body":"There is no public information about when Generative AI Studio will have access to the internet. However, you can add this feature to your Python code by using a library like Google Search API or DuckDuckGo API. These libraries allow you to search the internet and get the results back in JSON format. You can then use this JSON data to provide more accurate and up-to-date information to Generative AI Studio.\n\nHere is an example of how you can use Google Search API to add internet access to Generative AI Studio:\n\nPython\n\u00a0\nimport googlesearch def get_internet_info(query): \"\"\"Gets information from the internet about the given query.\"\"\" results = googlesearch.search(query, num_results=1) return results[0].url def generate_text(prompt, internet_info): \"\"\"Generates text using Generative AI Studio with internet access.\"\"\" return gca.generate_text(prompt, internet_info) if __name__ == \"__main__\": query = \"When did the Queen died?\" internet_info = get_internet_info(query) text = generate_text(query, internet_info) print(text)\n\nThis code will first use Google Search API to get the URL of the first search result for the given query. It will then use this URL to provide internet access to Generative AI Studio. Finally, it will generate text using Generative AI Studio with internet access.\n\nYou can also use other libraries like DuckDuckGo API to add internet access to Generative AI Studio. The process is similar to using Google Search API.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Google Vision release notes",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-14T00:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vision-release-notes\/m-p\/431130#M376",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":128,
        "Question_body":"From here\u00a0https:\/\/cloud.google.com\/vision\/docs\/release-notes\u00a0it says that there was an upgrade on\u00a0OCR model for\u00a0TEXT_DETECTION and DOCUMENT_TEXT_DETECTION. What is the recent model improvement compare to legacy model (is there some metrics used)?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX - continue conversation of abandoned user",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-02T01:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-continue-conversation-of-abandoned-user\/m-p\/599353#M2066",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":43,
        "Question_body":"I'm testing dialogflow cx to service chatbot.\n\nIs it possible to remember and continue previous conversation of abandoned user?\n\n- workflow\nuser abandon conversation -> restart service -> remember and continue previous conversation\n\nThanks for any help or idea!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Over fitting during RL or DRL when using tabular data",
        "Question_tag_count":5,
        "Question_created_time":"2022-04-18T10:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Over-fitting-during-RL-or-DRL-when-using-tabular-data\/m-p\/414640#M280",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":96,
        "Question_body":"Hi\n\nI will like to know how the Vertex Api handles or warns about models with over fitting conditions when using Reinforcement Learning or Deep Reinforcement Learning ? If so can you help me with the documents where you explain this situations when using the Vertex Api for tabular dataframes?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I list all the image URIs from a bucket to create a csv for classification automl model?",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-08T21:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-list-all-the-image-URIs-from-a-bucket-to-create-a-csv\/m-p\/520177#M1255",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"I have a bucket which has multiple folders with images in them. I want to make a csv for image classification with\u00a0[ML_USE],GCS_FILE_PATH,[LABEL] format. how do i get the gcs file path for it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"google.cloud.vision.v1.ImageAnnotator exceeded 600000 milliseconds",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-09T01:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/google-cloud-vision-v1-ImageAnnotator-exceeded-600000\/m-p\/551551#M1833",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"It was working well for a year and we have not changed anything. I think something has changed recently in cloud vision. I am getting this error on PRODUCTION server no way to know what is wrong. On Production Server I am able to do telnet on vision\n\ntelnet vision.googleapis.com 443 gives the expected output.\n\nVersion:\n\"@google-cloud\/vision\": \"^2.1.1\",\n\n```\nMay 09 13:46:43 fi-services npm[20346]: GoogleError: Total timeout of API google.cloud.vision.v1.ImageAnnotator exceeded 600000 milliseconds before any response was received.\nMay 09 13:46:43 fi-services npm[20346]: at repeat (\/opt\/******\/node_modules\/google-gax\/build\/src\/normalCalls\/retries.js:66:31)\nMay 09 13:46:43 fi-services npm[20346]: at Timeout._onTimeout (\/opt\/*****\/node_modules\/google-gax\/build\/src\/normalCalls\/retries.js:101:25)\nMay 09 13:46:43 fi-services npm[20346]: at listOnTimeout (internal\/timers.js:549:17)\nMay 09 13:46:43 fi-services npm[20346]: at processTimers (internal\/timers.js:492:7) {\nMay 09 13:46:43 fi-services npm[20346]: code: 4\nMay 09 13:46:43 fi-services npm[20346]: } error\n\n```\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"upload images\/photos - Dialogflow",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-02T12:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/upload-images-photos-Dialogflow\/m-p\/505401#M1012",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":161,
        "Question_body":"How can an \"Action and parameters\" load an image\/photo as input data. It is having the possibility of fearing an image field.\nThank you\n\nHow can an \"Action and parameters\" load an image\/photo as input data. It is having the possibility of fearing an image field.\nThank you\n\nHow can an \"Action and parameters\" load an image\/photo as input data. It is having the possibility of fearing an image field.\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Vertex AI Automl Model ID is invalid. It should start with 3 letters Error",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-20T02:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vertex-AI-Automl-Model-ID-is-invalid-It-should-start-with\/m-p\/491126#M822",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":194,
        "Question_body":"I have trained and deployed a model for image classification (multi-label). When I try to use the following code, I get an error =>\u00a0rpc error: code = InvalidArgument desc = Model ID is invalid. It should start with 3 letters.\"\u00a0But in the dashboard If i go to the console.cloud.google.com > vertex ai > model registry and then select version details Tab, The\u00a0Model_ID\u00a0does not have any 3 letter prefix in it. Its a plain 18 digit number. Dont know what to do here. Help required.\nclient, err := automl.NewPredictionClient(ctx)\n\n....\n\nreq := &automlpb.PredictRequest{\n        Name: fmt.Sprintf(\"projects\/%s\/locations\/%s\/models\/%s\", \"MY_PROJECT_ID\", \"LOCATION\", \"MODEL_ID\"), \/\/<= where can I get this Model's Prefix\n        Payload: &automlpb.ExamplePayload{\n            Payload: &automlpb.ExamplePayload_Image{\n                Image: &automlpb.Image{\n                    Data: &automlpb.Image_ImageBytes{\n                        ImageBytes: bytes,\n                    },\n                },\n            },\n        },\n        \/\/ Params is additional domain-specific parameters.\n        Params: map[string]string{\n            \/\/ score_threshold is used to filter the result.\n            \"score_threshold\": \"0.1\",\n        },\n    }\n\n    resp, err := client.Predict(ctx, req)\n    if err != nil {\n         return fmt.Errorf(\"Predict: %v\", err)\n    }",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to retrive transcript of virtual agent and customer in dialogflowcx?",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-01T03:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-retrive-transcript-of-virtual-agent-and-customer-in\/m-p\/494977#M884",
        "Question_answer_count":9,
        "Question_score_count":0,
        "Question_view_count":465,
        "Question_body":"I have two questions?\n\n1.\u00a0How to retrieve transcript of virtual agent and customer in dialogflowcx? After the virtual agent has triggered the \"live agent handoff\" the call reaches our internal contact centre. I want to fetch the transcript of the conversation so far to show to the real agent. Using API\/Client library\/anything.\n\n2. When live agent hand off happens, then there are certain parameters passed on. There is also an option to app a JSON in the dialogue option. Is there a way to retrieve this JSON\/Parameters using an API call?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ContexualVersionConflict When Importing Aiplatform Package in Executor of Vertex AI Workbench",
        "Question_tag_count":3,
        "Question_created_time":"2023-01-08T23:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ContexualVersionConflict-When-Importing-Aiplatform-Package-in\/m-p\/508016#M1030",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":599,
        "Question_body":"Hi There,\n\nI had the following problem when importing aiplatform\u00a0package in executor of Vertex AI workbench. This issue did not occur when I manually run the code in the Vertex AI workbench. However, the error came when I set a executor to run my code on schedule.\n\n---------------------------------------------------------------------------\n\nHere is the script\n\nfrom google.cloud import aiplatform\n\n---------------------------------------------------------------------------\n\nHere is the error message:\n\nContextualVersionConflict                 Traceback (most recent call last)\n\/tmp\/ipykernel_295\/605285511.py in <module>\n     63 \n     64 # Google cloud\n---> 65 from google.cloud import aiplatform\n     66 from datetime import datetime\n     67 from google.cloud import bigquery\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     29 from google.auth.exceptions import GoogleAuthError\n     30 \n---> 31 from google.cloud.aiplatform import compat\n     32 from google.cloud.aiplatform.constants import base as constants\n     33 from google.cloud.aiplatform import utils\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/compat\/__init__.py in <module>\n     16 #\n     17 \n---> 18 from google.cloud.aiplatform.compat import services\n     19 from google.cloud.aiplatform.compat import types\n     20 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/compat\/services\/__init__.py in <module>\n     16 #\n     17 \n---> 18 from google.cloud.aiplatform_v1beta1.services.dataset_service import (     19     client as dataset_service_client_v1beta1,\n     20 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/__init__.py in <module>\n     15 #\n     16 \n---> 17 from .services.dataset_service import DatasetServiceClient\n     18 from .services.dataset_service import DatasetServiceAsyncClient\n     19 from .services.deployment_resource_pool_service import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/__init__.py in <module>\n     14 # limitations under the License.\n     15 #\n---> 16 from .client import DatasetServiceClient\n     17 from .async_client import DatasetServiceAsyncClient\n     18 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/client.py in <module>\n     55 from google.protobuf import struct_pb2  # type: ignore\n     56 from google.protobuf import timestamp_pb2  # type: ignore\n---> 57 from .transports.base import DatasetServiceTransport, DEFAULT_CLIENT_INFO\n     58 from .transports.grpc import DatasetServiceGrpcTransport\n     59 from .transports.grpc_asyncio import DatasetServiceGrpcAsyncIOTransport\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/transports\/__init__.py in <module>\n     17 from typing import Dict, Type\n     18 \n---> 19 from .base import DatasetServiceTransport\n     20 from .grpc import DatasetServiceGrpcTransport\n     21 from .grpc_asyncio import DatasetServiceGrpcAsyncIOTransport\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1beta1\/services\/dataset_service\/transports\/base.py in <module>\n     40     DEFAULT_CLIENT_INFO = gapic_v1.client_info.ClientInfo(\n     41         gapic_version=pkg_resources.get_distribution(\n---> 42             \"google-cloud-aiplatform\",\n     43         ).version,\n     44     )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in get_distribution(dist)\n    469         dist = Requirement.parse(dist)\n    470     if isinstance(dist, Requirement):\n--> 471         dist = get_provider(dist)\n    472     if not isinstance(dist, Distribution):\n    473         raise TypeError(\"Expected string, Requirement, or Distribution\", dist)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in get_provider(moduleOrReq)\n    345     \"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\n    346     if isinstance(moduleOrReq, Requirement):\n--> 347         return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    348     try:\n    349         module = sys.modules[moduleOrReq]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in require(self, *requirements)\n    889         included, even if they were already activated in this working set.\n    890         \"\"\"\n--> 891         needed = self.resolve(parse_requirements(requirements))\n    892 \n    893         for dist in needed:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pkg_resources\/__init__.py in resolve(self, requirements, env, installer, replace_conflicting, extras)\n    780                 # Oops, the \"best\" so far conflicts with a dependency\n    781                 dependent_req = required_by[req]\n--> 782                 raise VersionConflict(dist, req).with_context(dependent_req)\n    783 \n    784             # push the new requirements onto the stack\n\nContextualVersionConflict: (google-cloud-bigquery 3.3.6 (\/opt\/conda\/lib\/python3.7\/site-packages), Requirement.parse('google-cloud-bigquery<3.0.0dev,>=1.15.0'), {'google-cloud-aiplatform'})\n\n-----------------------------------------------------------------------------------------------------\n\nSeeking help from everyone. Thank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX | How to increase flows limit?",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-20T13:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-How-to-increase-flows-limit\/m-p\/614188#M2409",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"According to this\u00a0page\u00a0 the limit is 50 flows per agent. Is the flow limits per agent is 50 globally?\u00a0 How can I increase the limit if that's the case?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I would like to receive notification of the start of the ML pipeline",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-06T18:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-would-like-to-receive-notification-of-the-start-of-the-ML\/m-p\/600721#M2091",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"I have constructed Email notification of ML pipeline according to the URL below.\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/email-notifications?hl=ja\n\nAll I receive is notification of success or failure.\u00a0I would like to receive notification of the start of the ML pipeline.\u00a0Is that possible?\n\nIt looks like there are only email notifications, can I switch to Slack notifications?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"nearest neighbors predict pipeline",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-16T00:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/nearest-neighbors-predict-pipeline\/m-p\/553729#M1923",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":48,
        "Question_body":"Hi All,\n\nI am trying to build pipeline for nearestneighbors from scikit learn. As there is no \"predict\" method in this, i need to write a custom predictor class.\u00a0\n\ncan any1 help me on how to call this predict class on model is deployed on endpoint?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Translation: again one of 22 requests finishes with timeout",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-06T04:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Translation-again-one-of-22-requests-finishes-with\/m-p\/541243#M1599",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":115,
        "Question_body":"Again we meet some problems with AutoML Translation API \u00a0\n\nTrained 22 custom models for translation from EN to target language. Try to translate with Google Cloud API and every minute minimum one of\u00a022 custoim models can't execute translation with 504 deadline error.\n\nWe can not manage or configure our models on your infrustructure. How can we fix\u00a0504 deadline error while translating with custom model with API call? Can we increase any quotes? Or pay additional for something? Please, help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Selecting a region for Cloud Talent Solution",
        "Question_tag_count":2,
        "Question_created_time":"2023-03-02T04:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Selecting-a-region-for-Cloud-Talent-Solution\/m-p\/527976#M1366",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":88,
        "Question_body":"Starting using Cloud Talent Solution service but i am not able to see an option to configure a default region for this service. I would like to choose the region us-west-1 (oregon), is there a way i can do that?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Time to deploy model",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-06T07:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Time-to-deploy-model\/m-p\/463368#M563",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"I asked to deploy my AutoML model over an hour ago but it is processing the request.... \u00a0How long should this take?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Checking uptime of an agent created in Dialogflow CX",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-01T20:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Checking-uptime-of-an-agent-created-in-Dialogflow-CX\/m-p\/599296#M2064",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":73,
        "Question_body":"Hi, is it possible to check the uptime of a virtual agent I created in Dialogflow CX?\u00a0\n\nI want to know the percentage\u00a0of time that the system is fully functional and see if there's any system latency or defects during a period of time. Wondering if there's a console for viewing these data.\n\nThanks for any help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Quota not listed in Vertex AI pipelines?",
        "Question_tag_count":3,
        "Question_created_time":"2023-02-05T22:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-not-listed-in-Vertex-AI-pipelines\/m-p\/518466#M1211",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":334,
        "Question_body":"Hi everyone I have a step in vertex ai pipelines that looks like this:\n\ntranscribe_task = transcribe_audios(audio_files=download_task.output)\ntranscribe_task.set_cpu_limit(\"2\").set_memory_limit(\n\"8G\"\n).add_node_selector_constraint(\"NVIDIA_TESLA_T4\").set_gpu_limit(\"1\")\n\nyet that task is not executed due to:\n\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits:\u00a0aiplatform.googleapis.com\/custom_model_training_nvidia_t4_gpus, cause=null; Failed to create custom job for the task.\n\nBut that quota is not listed anywhere in the quota manager, how can I enable GPU in Vertex AI pipelines?",
        "Question_closed_time":"02-06-2023 07:09 AM",
        "Answer_score_count":1.0,
        "Answer_body":"I solved it, it is quite not easy to find:\n\nSo for anyone with the same problem, go to Quotas and use the following filters:\n\n\n\nHope it can help anyone\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Virtual Machine Configuration Error for User-managed Notebook on Vertex AI Workbench",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-04T10:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Virtual-Machine-Configuration-Error-for-User-managed-Notebook-on\/m-p\/609199#M2286",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":75,
        "Question_body":"I am creating a user-managed notebook on the Vertex AI Workbench, and when choosing the Machine Type Configuration, I put\u00a0g2-standard-8, which comes with a Graphics Optimized NVIDIA L4 GPU, 8 vCPUs, and 32GB of RAM. However, when I created the notebook, I got an error saying \"Instances with guest accelerators do not support live migration.: Something went wrong. Sorry about that.\" I would greatly appreciate it if someone could help to solve this issue so I am able to use the g2-standard-8 configuration. Thank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"VertexAI autoML pipeline template error (tabular-regression)",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-08T08:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-autoML-pipeline-template-error-tabular-regression\/m-p\/551367#M1828",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"I have been using the the autoML regression pipeline templates for a while successfully.\u00a0 Three days ago overnight they broke.\u00a0 The same job that worked the night before hangs with the error message: \"Unable to create pipeline run due to the following error: Input parameter type mismatch. PipelineSpec.root.input_definitions.parameters['dataflow_use_public_ips'] is defined as BOOLEAN that parses BOOL_VALUE type, but the default value is provided as NUMBER_VALUE type.\"\n\nIf I want to re-run a clone of a previously successful training the same happens.\u00a0 I have tried to set public IP setting to different values -- no success.\u00a0 I have downloaded and edited the yaml according to the error-- no luck either!\n\nAnybody encountered the same?\u00a0 Is there a workaround?",
        "Question_closed_time":"05-10-2023 05:20 AM",
        "Answer_score_count":0.0,
        "Answer_body":"OK!\u00a0 The original Google provided template is now fixed and running!\n\nThanks for the help!\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"GCP STT (language-pt-BR) getting concatenated utterances",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-01T00:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-STT-language-pt-BR-getting-concatenated-utterances\/m-p\/516347#M1161",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":157,
        "Question_body":"For voice channel (telephony), pt-BR language, We are facing an issue where the two utterances are getting concatenated from GCP side (STT)\n\nAs we can see in the above image of the logs,\n\nThe first utterance is \"fotos\" and then the second utterance is \"sim\" but we get the concatenated output \"foto sim\" as second utterance. This happens because the \"final\" value remains \"false\", \u00a0because of which the intent is not getting recognized. We would like to get support for this scenario for this particular language.\n\nNote: This scenario is only for pt-BR language and is not replicable in other languages.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Auto ML Edge failing if highest accuracy option is selected",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-31T17:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Auto-ML-Edge-failing-if-highest-accuracy-option-is-selected\/m-p\/484251#M708",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"Hello all,\n\nI am doing an automl training for edge ad if I pick the higher accuracy option training fails after about 3 hours with the following error. Training completes with no issues if I pick the best trade-off option. I have opened a ticket but has received zero support from Google so posting here. Has anyone seen this issue and know how to fix it?\u2003I have tried multiple times with the same error.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BigQueryML Explainability Apparently Not Working",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-22T09:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQueryML-Explainability-Apparently-Not-Working\/m-p\/181036#M126",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":513,
        "Question_body":"I'm using BigQueryML to train an XGBoost model on some of my data. When I create the model, I set the ENABLE_GLOBAL_EXPLAIN flag to TRUE, the model then trains properly and I can evaluate it. However there is no Interpretability tab on the model's page, and when I try to query the model with the ML.GLOBAL_EXPLAIN command, I get an error that says:\n\n\u00a0\n\nInvalid table-valued function ML.GLOBAL_EXPLAIN In function GLOBAL_EXPLAIN, the input model was not explained when it was created. at [4:3]\n\nIs this a bug or am I doing something wrong?\n\nHere's my create model code:\n\n\u00a0\n\nCREATE OR REPLACE MODEL `apteo-gcp.2539775073233929517.gb_prod_recos5`\nOPTIONS(\n    INPUT_LABEL_COLS=['next_product_id'], -- label of future products purchased\n    MODEL_TYPE='BOOSTED_TREE_CLASSIFIER', -- gradient boosting using xgboost\n    CLASS_WEIGHTS=[STRUCT('gid:\/\/shopify\/Product\/6995522453704', 265),STRUCT('gid:\/\/shopify\/Product\/6995522355400', 265),STRUCT('gid:\/\/shopify\/Product\/6995522715848', 265),STRUCT('gid:\/\/shopify\/Product\/4448707969120', 100),STRUCT('gid:\/\/shopify\/Product\/6970658717896', 100),STRUCT('gid:\/\/shopify\/Product\/5688760467623', 100),STRUCT('gid:\/\/shopify\/Product\/4448711213152', 100),STRUCT('gid:\/\/shopify\/Product\/5430667804839', 100),STRUCT('gid:\/\/shopify\/Product\/5651893158055', 100),STRUCT('gid:\/\/shopify\/Product\/6995568918728', 125),STRUCT('gid:\/\/shopify\/Product\/4405707243616', 100),STRUCT('NO ORDER', 0.05)], \n    LEARN_RATE=0.15,\n    L2_REG=2.0,\n    L1_REG=1.5,\n    EARLY_STOP=TRUE,\n    MAX_ITERATIONS=75,\n    MIN_REL_PROGRESS=0.0001,\n    ENABLE_GLOBAL_EXPLAIN=TRUE,\n    DATA_SPLIT_METHOD=\"RANDOM\"\n)\n\nAS\n\n...",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Having Issues with authentication using REST API",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-16T15:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Having-Issues-with-authentication-using-REST-API\/m-p\/533517#M1433",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":249,
        "Question_body":"Hello,\n\nI'm building a rust application that needs to use cloud vision ocr. I use\u00a0service account key JSON file\u00a0to authenticate, and I had no problems with it using the Python and Go libraries. I wasn't able to understand how to use this authentication method in the rust library, so I decided to use the POST request method. To do such thing I need to somehow include my key in the request header, and I have not been able to do it. My code and the response I get are below.\n\n```\u00a0\n\npub\u00a0async\u00a0fn\u00a0req(url:\u00a0&str)\u00a0->\u00a0Result<String,\u00a0Box<dyn\u00a0std::error::Error>> {\nlet\u00a0auth_token\u00a0=\u00a0fs::read_to_string(String::from(\"auth_sin.json\"))\n.expect(\"Should have been able to read the file\");\nlet\u00a0client\u00a0=\u00a0reqwest::Client::builder().build()?;\nlet\u00a0reqtext\u00a0=\u00a0format!(\"Bearer -d\u00a0{}\",&auth_token[..]);\nlet\u00a0request\u00a0=\u00a0client\n.post(\"https:\/\/vision.googleapis.com\/v1\/images:annotate\")\n.header(String::from(\"Authorization\"),\u00a0reqtext.replace(\"\\n\",\"\"))\n.header(\"Content-Type\",\u00a0\"application\/json; charset=utf-8\")\n.json(&build_request(url));\nprintln!(\"{:?}\",\u00a0request);\nlet\u00a0response\u00a0=request\n.send()\n.await?;\nlet\u00a0t\u00a0=\u00a0response\n.text()\n.await?;\nOk(t)\n}\n```\n\u00a0\nReponse\u00a0\n```\n{\n\"error\": {\n\"code\": 401,\n\"message\": \"Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See\u00a0https:\/\/developers.google.com\/identity\/sign-in\/web\/devconsole-project.\",\n\"status\": \"UNAUTHENTICATED\",\n\"details\": [\n{\n\"@type\": \"type.googleapis.com\/google.rpc.ErrorInfo\",\n\"reason\": \"ACCESS_TOKEN_TYPE_UNSUPPORTED\",\n\"metadata\": {\n\"method\": \"google.cloud.vision.v1.ImageAnnotator.BatchAnnotateImages\",\n\"service\": \"vision.googleapis.com\"\n}\n}\n]\n}\n}\n```",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Commercial usage of Google Cloud TTS",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-21T18:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Commercial-usage-of-Google-Cloud-TTS\/m-p\/458420#M518",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":495,
        "Question_body":"Hi,\n\nI wish to use Google cloud's Wavenet TTS (TextToSpeech) for commercial use for my company. Can anyone please confirm whether it is allowed or not?\n\nRegards\n\nUtkarsh Dubey",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Model Deployment Error",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-01T05:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Model-Deployment-Error\/m-p\/495020#M885",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":420,
        "Question_body":"Hi, I just got started using vertex ai with google cloud console. I am trying to deploy this mode to an endpoint. https:\/\/tfhub.dev\/tensorflow\/efficientnet\/lite0\/feature-vector\/2 I successfully imported it into a google storage bucket and uploaded it to the model registry. However, when I attempt to deploy the model to an endpoint, I receive the following error.\n\nHello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to create endpoint \"Feature Vectors\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \n**path to project**\nError Messages: Model server terminated: model server container terminated: \nexit_code:       255\nreason: \"Error\"\nstarted_at {\n   seconds: 1669817118\n}\nfinished_at {\n   seconds: 1669817421\n}\n. Model server logs can be found at \n**some link**\n\nI have attempted to change the TensorFlow version and the folder that i import (I attempted to import the containing folder instead of the model folder) however nothing seems to help. Any suggestions would be greatly appreciated. Thank you!",
        "Question_closed_time":"02-21-2023 03:48 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \".\/efficientnet_lite0_feature-vector_2\/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to \/tmp\/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='.\/efficientnet_lite0_feature-vector_2\/info.meta')\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Bigquery ML billiing support",
        "Question_tag_count":3,
        "Question_created_time":"2022-08-19T01:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Bigquery-ML-billiing-support\/m-p\/455616#M516",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"I have been working on Demand Forecasting in Bigquery ML and i have been creating model with different datasets but recently for a particular dataset price spiked up for the CREATE MODEL query while working on the dataset can anyone help me regarding training model it is because of dataset or any other matter associated with it",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DialogFlow CX triggering no-match when adding words like \"issue\" or \"working\".",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-29T15:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DialogFlow-CX-triggering-no-match-when-adding-words-like-quot\/m-p\/494494#M877",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":149,
        "Question_body":"I am having an issue with DialogFlow CX not recognizing certain words. For example, if I test \"Internet\" it will trigger the proper intent. \"Internet not\" also triggers the proper intent. But, when I said \"Internet not working\" the system triggers a no-match. There are no other intents with the word \"working\" listed as a training phrase.\n\n\u00a0\n\nHelp?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"exporting a google autoML translate model",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-27T12:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/m-p\/471646#M607",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":162,
        "Question_body":"Hi,\n\u00a0\nIs there a way to download my Google AutoML Transation model and use it offline once it's trained?\n\u00a0\nAnd in what format can the model be exported?\u00a0\n\u00a0\nThank you",
        "Question_closed_time":"09-30-2022 03:57 PM",
        "Answer_score_count":0.0,
        "Answer_body":"1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request\/Issues.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Error 524 on jupyterlab",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-05T02:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-524-on-jupyterlab\/m-p\/486115#M738",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":553,
        "Question_body":"Hi,\n\nI cannot access Jupyterlab by web interface. It still works by ssh. I've followed the support documentations, but nothing works.\n\nMy best guess is that the main issue is with the opened ports of docker.\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\ncurl http:\/\/127.0.0.1:8080\/api\/kernelspecs\ncurl: (7) Failed to connect to 127.0.0.1 port 8080: Connection refused\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nI have the full result of the following command available, but i'm not sure if it's good idea to post it in public.\n\nsudo -i\ncd \/opt\/deeplearning\/bin\/\n.\/diagnostic_tool.sh\n\nI think the main error is from:\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n{\"log\":\"2022\/11\/05 08:47:34 Health Check request failed: Get \\\"http:\/\/localhost:8080\/api\/kernelspecs\\\": dial tcp [::1]:8080: connect: connection refused\\n\",\"stream\":\"stderr\",\"time\":\"2022-11-05T08:47:34.588994439Z\"}\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard API",
        "Question_tag_count":2,
        "Question_created_time":"2023-03-29T12:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-API\/m-p\/538517#M1526",
        "Question_answer_count":36,
        "Question_score_count":16,
        "Question_view_count":0,
        "Question_body":"My request to join the Bard preview was recently accepted and I've begun evaluating it. So far it looks amazing and I'm anxious to begin working with it as a developer. I asked Bard if there was an API that I can start to use and it responded with the information below. I cannot seem to find the API waitlist sign-up page in the Bard website, nor can I enable the API from my Cloud Account (https:\/\/cloud.google.com\/),\n\nIs there any additional action I should take to request to join the wait list for access to the Bard API? Perhaps I am too anxious and just need to wait.\u00a0\n\nThanks in advance for any advice.\n\nResponse from Bard itself:\n\nYes, there is an API for Google Bard. It is currently in beta and is available to a limited number of users. You can sign up for the waitlist on the Google AI website.\n\nThe Google Bard API allows you to access the power of Google's large language models to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n\nYou can use the Google Bard API to create a variety of applications, including:\n\nChatbots that can hold conversations with users\nGenerators that can create content for websites and social media\nTranslators that can translate text from one language to another\nAnswerers that can answer questions about a variety of topics\n\nTo use the Google Bard API, you will need to create a Google Cloud Platform project and enable the Google Bard API. You will then need to create a service account and download the JSON key file.\n\nOnce you have done this, you can start using the Google Bard API in your applications. You can find more information about the Google Bard API on the Google AI website.",
        "Question_closed_time":"04-19-2023 01:11 PM",
        "Answer_score_count":14.0,
        "Answer_body":"Hi\u00a0@DeanS - thank you for your patience! We are excited to hear that you are ready to begin working with Google LLM API's as a developer.\u00a0 Check out the information below:\u00a0\n\nEnterprise customers can sign up to Vertex AI LLM Trusted Testers program here:\u00a0https:\/\/cloud.google.com\/ai\/earlyaccess\/join?hl=en\nIndependent developers can sign up the MakerSuite and PalmAPI waitlist here:\u00a0https:\/\/makersuite.google.com\/waitlist.\n\u00a0\nRegister for the Google Cloud Innovators Program to stay informed on the latest updates and roadmap sessions.\n\u00a0\nI hope this helps. Thanks!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"I want to bulk the CRD part of the CRUD function of the intent.",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-07T20:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-want-to-bulk-the-CRD-part-of-the-CRUD-function-of-the-intent\/m-p\/551160#M1816",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"For the ES version, we are using the batchUpdate method in the googleapis module to perform bulk updates. However, for the CX version, we are using the @Google-cloud\/dialogflow-cx module and there doesn't seem to be a method for bulk adding, modifying, or deleting intents.\n\n1. Is it not possible to perform bulk operations in the CX version?\n2. And if not, does CX not have the ability to import and export intents as files like the ES version does?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DialogFlow error. Messages not updating.",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-21T01:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DialogFlow-error-Messages-not-updating\/m-p\/614303#M2414",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"Starting this week the messages from our Dialogflow project do not push to the machine where they should reproduce. When accessing to the Dialogflow panel I get this error:\n\n\u00a0\n\ncom.google.apps.framework.request.CanonicalCodeException: AoG Sunset - Read apis are disabled for deprecated cloud project 0 Code: FAILED_PRECONDITION\ncom.google.net.rpc3.client.RpcClientException: <eye3 title='\/AgentBuilder.ListIntentDefinitions, FAILED_PRECONDITION'\/> APPLICATION_ERROR;google.actions.v2\/AgentBuilder.ListIntentDefinitions;com.google.apps.framework.request.CanonicalCodeException: AoG Sunset - Read apis are disabled for deprecated cloud project 0 Code: FAILED_PRECONDITION;AppErrorCode=9;StartTimeMs=1689927421628;unknown;ResFormat=uncompressed;ServerTimeSec=0.018021919;LogBytes=256;FailFast;EffSecLevel=none;ReqFormat=uncompressed;ReqID=b6b37edb52f61dd2;GlobalID=0;Server=[2002:a05:620a:2401::]:4469\n\nAny clues of what could be happening?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"cloud vision logging",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-31T17:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cloud-vision-logging\/m-p\/516239#M1159",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"Hi Everyone,\n\nI've been using Google Vision for quite sometime. However, through Logs Explorer, I cannot find any log related to that service. I've tried to query using this filter:\n\nprotoPayload.serviceName=\"vision.googleapis.com\"\nAnd return empty result. I've put wider timeline for the last 30 days, in which the Vision API service was hit thousands time.\nIs there something that I missed in configuration?\n\u00a0\nThank you for all the help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-04T06:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-DAGScheduler-Broadcasting-large-task-binary-with-size-2-2\/m-p\/450466#M487",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi there.\nThere were quite a number of such warnings as the model was getting trained.\n\n22\/08\/04 07:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1139.5 KiB\n22\/08\/04 07:08:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n\nMay I know if we are safe to ignore them?\nWhat does it mean actually?\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Should I custom split my image data?",
        "Question_tag_count":2,
        "Question_created_time":"2021-07-05T04:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Should-I-custom-split-my-image-data\/m-p\/163031#M10",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":497,
        "Question_body":"Even with auto Ml, should carefully custom split my data to my satisfaction or just leave it to AutoML?\n\n\u00a0\n\nAnd what difference does it make?",
        "Question_closed_time":"07-14-2021 06:16 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi Ayoola\n\nIf your data is large enough and have wide representation of each category, you may go with the automated split in AutoML. That would save time and perform well.\n\nIf you have some specific needs, such as the representation of certain observations in a specific category is important and limited within the data, you may want to make sure that it is well distributed for validation and test. And custom split would help for that. Another reason of using custom split could be for comparison of your model performance with external models so you use exactly the same training\/test datasets and make an apples to apples comparison.\n\nHere are some tips I find useful in this doc:\n\nhttps:\/\/cloud.google.com\/vision\/automl\/docs\/beginners-guide#distribute_examples_equally_across_categ...\n\nCheers\n\nTuba.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Dialogflow CX - Voximplant liveAgentHandoff issues",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-13T08:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-Voximplant-liveAgentHandoff-issues\/m-p\/531855#M1419",
        "Question_answer_count":15,
        "Question_score_count":0,
        "Question_view_count":380,
        "Question_body":"Hi,\n\nI am currently experimenting with the liveAgentHandoff Feature and Voximplant and noticed that when the liveAgentHandoff gets triggered, my inputs still get recognized by Voximplant but Dialogflow gets stuck and wont react or continue the flow. Also sending an Event to Dialogflow doesn't have any impact.\u00a0 How can I resolve that issue, does Dialogflow just stop by default when the liveAgentHandoff gets triggered or is the Voximplant connection the problem?\n\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can we Download an AutoML Model after training?",
        "Question_tag_count":0,
        "Question_created_time":"2021-08-05T11:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-we-Download-an-AutoML-Model-after-training\/m-p\/166258#M36",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":294,
        "Question_body":"Is there a way to Download the AutoML Model after training?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Workbench JSONL format",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-22T14:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Workbench-JSONL-format\/m-p\/605833#M2218",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":69,
        "Question_body":"I am trying to use Generative AI studio to tune a model. Can someone send me the example of JSONL file that I should use. I build following file and I am getting an error:\n\n\u00a0\n\n{\n'input_text':'This is input1','output_text':'This is output1'\n'input_text':'This is input2','output_text':'This is output2'\n'input_text':'This is input3','output_text':'This is output3'\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflox cx conflict: \"intents matching\" and \"parameters form\" at the same page.",
        "Question_tag_count":3,
        "Question_created_time":"2022-11-15T05:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflox-cx-conflict-quot-intents-matching-quot-and-quot\/m-p\/489582#M783",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":301,
        "Question_body":"Hello everyone,\n\nI found an unexpected behavior with the following page-level configuration.\n\nParameters:\n- Only 1 required parameter - intent_param\n- Custom entity type: Map entity - @intent\n-\u00a0Initial prompt fulfillment: \"\u00a0What do you want?\u00a0\"\u00a0\n\n\n@intent Entity type:\n- Disabled \"Entities only\"\n- Disabled \"Regexp entities\"\n- One entity: \"Entity1\"\u00a0- Synonyms: \"Value1\"\n\n\nRoutes:\n- Various Intent routes that if matching with the input will transfer to other page.\n\n\nEvent handler:\n- sys.no-match-1\u00a0\n-\u00a0This event handler should show a Fulfillment message and Route to another page\u00a0when none of the intents is matched.\n\nCONFLICT: After the question\u00a0\"What do you want?\u00a0\", if the user input is not clear there will not be any intent matching, but if the user include in the sentence the word \"Value1\" which is synonym of \"Entity1\", then the parameter \"intent_param\" (entity type \"@intent\") will be collected with value \"Entity1\".\u00a0When this happens I was expecting \"sys.no-match-1\"\u00a0to be activated, but this did not occurred and the page state status is \"PROCESSING_FORM\" (FormFilled: false).\n\nDoes anyone knows why this happens and if is there a way to avoid this behavior? In this situation I would like to continue the workflow with the\u00a0 parameter collected and no intents matched.\n\nThank you,\nMiguel.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-27T00:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/m-p\/460565#M543",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":371,
        "Question_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.\u00a0 I have enabled\u00a0aiplatform.googleapis.com.\n\nCommand:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifier\n\nUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.\n\nPlease suggest what am I missing.",
        "Question_closed_time":"08-27-2022 04:38 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Neural2 voices sometimes appear to sound like it's drunk or having a stroke",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-23T09:37:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Neural2-voices-sometimes-appear-to-sound-like-it-s-drunk-or\/m-p\/525604#M1331",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":249,
        "Question_body":"For some reasons, Neural2 voices sometimes appear to sound like it's drunk or having a stroke. For instance, synthesize the following text with en-US-Neural2-J using the demo prompt on this page:\n\nFirst came a stout puffy gentleman with a carpet bag; he wanted to go to the Bishopsgate station; then we were called by a party who wished to be taken to the Regent's Park; and next we were wanted in a side street where a timid, anxious old lady was waiting to be taken to the bank; there we had to stop to take her back again, and just as we had set her down a red-faced gentleman, with a handful of papers, came running up out of breath, and before Jerry could get down he had opened the door, popped himself in, and called out, \u201cBow Street Police Station, quick!\u201d so off we went with him, and when after another turn or two we came back, there was no other cab on the stand.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About Vertex AI Billing",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-26T23:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Vertex-AI-Billing\/m-p\/606790#M2238",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":120,
        "Question_body":"There are four things I would like to know about Vertex AI billing.\n\n\u00a0\n\n1. Is the 300$\/month fee for Vertex AI Tensorboard a one-time charge?\n2. How can I get a breakdown of Vertex AI fees?\u00a0I would like to know the fee per pipeline and endpoint.\n3. Is there any lag in billing? I started up Tensorboard instance on the 26th, but it looks like I was billed on the 25th.\n4.\u00a0If I activate Tensorboard on 6\/26, am I correct that I will not be billed until 7\/26?\nPlease tell me additionally, is Tensorboard instance pay-as-you-go billing? Is the cost per month 300$ no matter how much you use?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vonage Smart Number",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-25T00:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vonage-Smart-Number\/m-p\/481819#M680",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":145,
        "Question_body":"Trying to find a way to link a Vonage Smart Number (Vonage Communications, not API) to Dialogflow",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cannot remove user events in Retail API",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-08T22:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-remove-user-events-in-Retail-API\/m-p\/530421#M1402",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":171,
        "Question_body":"I've imported user events for testing but I want to clear all user events and re-import new user events for another testing. I've followed the instruction to remove user events (Link). After waiting for 24hrs, operation status is completed.\n\nThis is a POST request to delete user events between 2023-03-01 to 2023-03-07\n\nRESPONSE=$(curl -X POST \\\n  -H \"Authorization: Bearer \"${ACCESS_TOKEN}\"\" \\\n  -H \"Content-Type: application\/json; charset=utf-8\" \\\n  --data '{\n  \t\"filter\":\"eventTime > \\\"2023-03-01T00:00:01.511Z\\\" eventTime < \\\"2023-03-07T00:00:01.511Z\\\"\",\n  }' \\\n  \"https:\/\/retail.googleapis.com\/v2\/projects\/${PROJECT_ID}\/locations\/global\/catalogs\/default_catalog\/userEvents:purge\"\n)\necho $RESPONSE\n\nThis is a response of operation status\n\n{\n  \"name\": \"projects\/{PROJECT_ID}\/locations\/global\/catalogs\/default_catalog\/operations\/purge-user-events-XXXXXXXXXX\",\n  \"done\": true,\n  \"response\": {\n    \"@type\": \"type.googleapis.com\/google.cloud.retail.v2.PurgeUserEventsResponse\",\n    \"purgedEventsCount\": \"6\"\n  }\n}\n\nI expect to see 0 events from Retail console in GCP but the number of events ingested still existed (nothing changed)\n\nHow can I remove user events under same project without deleting the entire project?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Need \"no code\" help with buckets and datasets.",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-05T18:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Need-quot-no-code-quot-help-with-buckets-and-datasets\/m-p\/529084#M1378",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":52,
        "Question_body":"I have uploaded thousands of images (individual jpgs) to a Bucket. I now want to access those and use them in a new dataset. I can see all my uploaded images in the \"cloud storage\" \"bucket\" interface, but I cannot, for the life of me, work out how to attach any of those to a new dataset, except one at a time. Is there a way to make these images available to a new dataset in batches? Also, and I can't stress this enough, if that involves any form of coding, then that's a show-stopper for me.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Where are Dialogflow ES\/CX audio files stored?",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-25T08:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Where-are-Dialogflow-ES-CX-audio-files-stored\/m-p\/596981#M2025",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":101,
        "Question_body":"This page indicates that Dialogflow ES audio recording are stored somewhere once enabled, but I can't find where they're stored.\n\nhttps:\/\/cloud.google.com\/dialogflow\/docs\/support\/faq?agent=any&category=any",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX - ES-US Spanish Currency Collection",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-26T12:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-ES-US-Spanish-Currency-Collection\/m-p\/537201#M1496",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"I'm running into issues where the unit-currency Entity for Spanish is not collecting cents (centavos) when text is fed into it.\u00a0 Ex: $5 y 5 centavos is only being recognized as $5 instead of $5.05 by the entity.\n\nHas anyone else run into this?\u00a0 Also due the the size of the agent we are using Standard NLU.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Memory issue",
        "Question_tag_count":8,
        "Question_created_time":"2022-06-21T02:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Memory-issue\/m-p\/433242#M383",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":566,
        "Question_body":"Hi friends,\n\n\u00a0\n\nIam facing this error recently -\u00a0\n\nThe replica workerpool0-0 ran out-of-memory and exited with a non-zero status of 137(SIGKILL).\u00a0\n\nKindly help me , i am using 800GB , still getting this error",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI predictions fail without details about the error",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-23T05:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-predictions-fail-without-details-about-the-error\/m-p\/605995#M2219",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":109,
        "Question_body":"I am a very new Vertex AI user, and I started using it with very basic AutoML regression model. I performed evey action using web UI, but something goes wrong.\n\nBasically I uploaded my dataset, trained a new model and then deployed to an endpoint to make first predictions. For doing I used the web interface available in Deploy and use -> Model registry -> Deploy and test -> Test your model... but every time I try to make some prediction I get the following error \"The prediction did not succeed due to the following error:\", and no further detail is available... what can I do to debug the error and hopefully make some prediction?\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to implement custom evaluation jobs (with custom metrics for custom learning tasks) in Vertex AI",
        "Question_tag_count":3,
        "Question_created_time":"2023-05-18T08:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-implement-custom-evaluation-jobs-with-custom-metrics-for\/m-p\/554593#M1954",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"From what I can see in GCP's official docs:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/evaluation\/introduction\n\nEvaluation job comes with predefined metrics for predefined tasks. If I have a training task that's slightly more bespoke with metrics\/ objective outside the scope of the available tasks\/ metrics, how would I implement them?\n\nExamples of custom metrics are MMD or KL Divergence loss etc.\n\nMy current thinking is:\n\nCreate a custom evaluation app in a docker container\nCreate a CustomJob that runs my evaluation app\nOutput the evaluation artefacts (e.g. results, plots) to GCS etc?\n\nMany thanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Translate API",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-30T10:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API\/m-p\/181620#M143",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":390,
        "Question_body":"Hi,\n\nI would like to use Google Translate API in plain javascript.\n\nAs far as I understand from this guide, the supported languages are :\u00a0\n\nGo\nJava\nNode.js\nPython\n\n... and some additional languages :\n\nC#\nPhp\nRuby\n\nDoes translate API is supported for Javascript as well? If so, where is the guide?\n\nThanks in advance.",
        "Question_closed_time":"01-10-2022 02:09 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Here is a third-party solution you may find useful [1].\u00a0You may also report it to the Public Issue Tracker (PIT) [2]\u00a0 as well as feature request.\n\n[1]\u00a0 https:\/\/github.com\/topics\/javascript-translate\n\n[2]\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Google TTS Spring Boot Program - Not getting any response",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-16T23:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-TTS-Spring-Boot-Program-Not-getting-any-response\/m-p\/533592#M1434",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":251,
        "Question_body":"public class TextToSpeechServiceV2\n{\n@Value(\"${audioOutput.fileDirectory}\")\nprivate String audioFileDirectory;\n\n@Value(\"${audioOutput.audioFormat}\")\nprivate String audioFileFormat;\n\n@Value(\"classpath:careful-ensign-380704-cdd798eba5dd.json\")\nprivate Resource googleJsonResource;\n\nString audioFileName = \"output\";\n\nprivate static final Logger logger = LogManager.getLogger(TextToSpeechServiceV2.class);\n\npublic void generateAudioFile(String receivedtext) throws Exception\n{\nlogger.info(\"V1 Service -> Received Text : \" + receivedtext);\n\n\/\/ Load the service account key file as a GoogleCredentials object\nGoogleCredentials credentials = GoogleCredentials.fromStream(googleJsonResource.getInputStream());\n\nlogger.info(\"Initialised Google Credentials\");\n\n\/\/ Build the TextToSpeechSettings using the builder pattern and set the service\n\/\/ account credentials\nTextToSpeechSettings settings = TextToSpeechSettings.newBuilder().setCredentialsProvider(() -> credentials)\n.build();\n\nlogger.info(\"Build TTS Settings\");\n\n\/\/ Build the TextToSpeechClient using the GoogleCredentials object\ntry (TextToSpeechClient textToSpeechClient = TextToSpeechClient.create(settings))\n{\nSystem.out.println(\"In Try - Create TTS client\");\nlogger.info(\"In Try - Create TTS client\");\n\/\/ Set the text input to be synthesized\nSynthesisInput input = SynthesisInput.newBuilder().setText(receivedtext).build();\n\n\/\/ Build the voice request, select the language code (\"en-US\") and the ssml\nVoiceSelectionParams voice = VoiceSelectionParams.newBuilder().setLanguageCode(\"en-US\")\n.setSsmlGender(SsmlVoiceGender.FEMALE).build();\nlogger.info(\"Set Voice Params\");\n\n\/\/ Select the type of audio file you want returned\nAudioConfig audioConfig = AudioConfig.newBuilder().setAudioEncoding(AudioEncoding.MP3).build();\nlogger.info(\"Audio Configured\");\n\n\/\/ Perform the text-to-speech request\nSynthesizeSpeechResponse response = textToSpeechClient.synthesizeSpeech(input, voice, audioConfig);\nlogger.info(\"Audio Synthesised\");\n\n\/\/ Get the audio contents from the response\nByteString audioContents = response.getAudioContent();\n\n\/\/ Write the response to the output file\ntry (OutputStream out = new FileOutputStream(audioFileDirectory + audioFileName + audioFileFormat))\n{\nout.write(audioContents.toByteArray());\nlogger.info(\"File Opened and Written\");\n} catch (Exception e)\n{\nlogger.error(\"Exception in Opening File : \" + e.getMessage());\nlogger.error(e.getMessage(), e);\nthrow e;\n}\n}catch (Exception e) {\nlogger.error(\"Exception in TTS Client Create : \" + e.getMessage());\nlogger.error(e.getMessage(), e);\nthrow e;\n}\n}\n}\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThe program stops at the line\n\n\u00a0\n\nSynthesizeSpeechResponse response = textToSpeechClient.synthesizeSpeech(input, voice, audioConfig);\n\n\u00a0\n\nDoes not get any response from there. When I am testing with Postman, I get a Gateway Timeout Error and if I check for logs, anything after this line is not coming. No exceptions. Nothing.\u00a0\n\nLike the program just stops.\n\n\u00a0\n\nPlease help. Thanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issue with Load to draft in version of dialogflow Cx agent",
        "Question_tag_count":2,
        "Question_created_time":"2023-01-16T01:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-with-Load-to-draft-in-version-of-dialogflow-Cx-agent\/m-p\/510456#M1069",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":118,
        "Question_body":"-- Issue description (short summary) \u00a0--\nIn dialogflow agent, after creating the version, to restore back the selected version there is an option called LOAD TO DRAFT. After using this everything was not restoring. It only restores pages and flows but not intends, entities, webhooks etc...\n\nThere is also a special checkbox which indicates , this will override the current resources in the flow( Intends, Entities, webhooks..etc).Even after check-in it doesn't worked.\n\n-- Steps to reproduce the issue (detailed description) \u00a0--\n\n1.Create dialogflow agent with some pages, flows, intends and webhooks\n\n2. Create a version in manage section\n\n3. Make some changes in agent like adding new pages, intends and deleting some intends.\n\n4. Now navigate to manage > version\n\n5. Select the version previously created, in option click Load to Draft.\n\n6. After loading, refresh the pages\n\n\n-- Expected behavior \u00a0--\n7. Agent should completely restore with pages, intends (same as before changes done )\n\n-- Actual behavior \u00a0--\n\n8 . But the actual output was, only pages are updated but no changes in intends , webhooks, entities... etc.\n\nReference :\u00a0\n\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/concept\/version\n\u00a0\n\n\n\ncorrect me if I did anything wrong.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech-to-Text Transcription File",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-18T21:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-Transcription-File\/m-p\/604363#M2183",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"I uploaded an mp3 and apparently converted it to text but the resulting file is a json and I want a text file. How do I get this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex Workbench Managed Notebook vs Quotas",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-03T08:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-Workbench-Managed-Notebook-vs-Quotas\/m-p\/450055#M477",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":240,
        "Question_body":"Hi to all\n\nA little help need it... Im trying to create a Managed Notebook with the next configuration\u00a0for what I request a quota increase as you can see in the following images\n\na2-highgpu-1g (Accelerator Optimized: 1 NVIDIA Tesla A100 GPU, 12 vCPUs, 85GB Ram)\u00a0\n\nThe problem is that although I got the quota increase I have not been able to create the notebook in any us regions as:\u00a0 US Central (Iowa1), Us West (Oregon), always get the same error:\u00a0\n\nCould not create instance: Quota limit 'GPUsA100PerProjectPerRegion' has been exceeded. Limit: 0 in region us-central1.\n\nWhat Im doing wrong? How much quota do I need to get so I can use this type of machine?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Translate Cloud API",
        "Question_tag_count":2,
        "Question_created_time":"2021-12-06T05:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-Cloud-API\/m-p\/177308#M109",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":345,
        "Question_body":"Hi All,\n\nI have recently started using Google cloud translate API with python. M having trouble converting this word in the Telugu language which is written in English \"parishkaram chesamu\".\u00a0 In general internet or mobile the application google translate which we use is giving correctly. But API is returning the same word again.\n\nGoogle Cloud translate API:\n\nInput text:\u00a0parishkaram chesamu\n\nOutput text:\u00a0parishkaram chesamu\n\nparameters :\u00a0\n\ntext ='''parishkaram chesamu'''\ntarget = \"en\"\noutput = translate_client.translate(text)\n\nprint(output) -->\u00a0\n\n{'translatedText': 'parishkaram chesamu', 'detectedSourceLanguage': 'te', 'input': 'parishkaram chesamu'}\n\n================================\n\nMobile or Internet google translate:\n\nInput text : parishkaram chesamu\n\nOutput text: We have solved",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"It's possible create a dataset in document ai processor and import documents via python API?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-23T10:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/It-s-possible-create-a-dataset-in-document-ai-processor-and\/m-p\/606091#M2223",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"It's possible, to do the steps in the below guide, using the Python API? I saw some documentation about creating processors and training processors via Python API. But, none about creating the dataset and importing documents in the processor.\n\nhttps:\/\/cloud.google.com\/document-ai\/docs\/workbench\/create-dataset\n\nDocumentations that I already know:\n\n\u00a0- Train processors:\u00a0https:\/\/cloud.google.com\/document-ai\/docs\/workbench\/train-processor\n\n\u00a0- Manipulation of processors:\u00a0https:\/\/cloud.google.com\/document-ai\/docs\/samples\/documentai-create-processor",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocAI - Response in a single json file",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-17T23:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Response-in-a-single-json-file\/m-p\/490702#M810",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":421,
        "Question_body":"Hello Experts,\nI'm doing BatchProcessDocument. I have 18 pages of a PDF file and tried to process this using DocumentProcessorServiceClient API. After the process, Im getting response in json file. This is perfect.\nBut the json output file is created only for the 5 pages of the source PDF file. Each 5 pages of the content are converted into a separate json file.\n\nMy question here is, is it possible to have a single output json file for a PDF source file?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Translation API",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-30T14:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translation-API\/m-p\/548941#M1768",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hello,\n\nI am using google translation API with a defined glossary config.\n\nWhen using a glossary with languages like es, pt etc... everything works fine.\n\nbut when creating a glossary that support hebrew language I always get an empty string back\n\nI tried to check the file encoding and its UTF-8, and from what I understand this is the right format.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"how to update model version on model registry",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-06T01:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/how-to-update-model-version-on-model-registry\/m-p\/600399#M2086",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hi, currently I'm working on vertex ai pipelines. I'm confused on how to update my model version on model registry.\n\nVersions:\n\n\u00a0\n\nkfp                                    2.0.0rc1\nkfp-pipeline-spec                      0.2.2\nkfp-server-api                         2.0.0rc1\ngoogle_cloud_pipeline_components       2.0.0b\n\n\u00a0\n\nHere's the code snippet:\n\n\u00a0\n\nmodels = aiplatform.Model.list(filter=(\u201cdisplay_name={}\u201c).format(MODEL_DISPLAY_NAME))\nparent_model = models[0].resource_name\nmodel_upload_op = ModelUploadOp(\nproject=PROJECT_ID,\ndisplay_name=MODEL_DISPLAY_NAME,\nparent_model=,\nunmanaged_container_model=unmanaged_model_importer.outputs[\u201cartifact\u201d],\n).after(unmanaged_model_importer)\n\n\u00a0\n\nError message:\n\n\n\u00a0\n\nInconsistentTypeException: Incompatible argument passed to the input 'parent_model' of component 'model-upload':\nArgument type 'STRING' is incompatible with the input type 'google.VertexModel@0.0.1'\n\n\u00a0\n\nIs there any way to resolve this?\u00a0\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issues uploading entities via a CSV file in Dialogflow CX",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-11T08:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-uploading-entities-via-a-CSV-file-in-Dialogflow-CX\/m-p\/552438#M1858",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"Tengo dificultades para cargar un archivo CSV en Dialogflow CX para definir entidades y sus sin\u00f3nimos. A pesar de seguir las recomendaciones sobre el formato de los archivos CSV, encuentro un mensaje de error que indica \"Formato CSV incorrecto: no requiere l\u00ednea de valor 1\" cada vez que intenta cargar el archivo.Tengo dificultades para cargar un archivo CSV en Dialogflow CX para definir entidades y sus sin\u00f3nimos. A pesar de seguir las recomendaciones sobre el formato de los archivos CSV, encuentro un mensaje de error que dice \"Formato CSV incorrecto: no se requiere l\u00ednea de valor 1\" cada vez que intente cargar el archivo.\n\nHe intentado varias cosas para solucionar este problema. Primero, verifiqu\u00e9 el formato de mi archivo CSV para asegurarme de que se ajusta a las especificaciones de Dialogflow. Tambi\u00e9n me asegur\u00e9 de que mi archivo CSV est\u00e9 codificado en UTF-8 y que los delimitadores de campo y cadena est\u00e9n configurados correctamente. Adem\u00e1s, intent\u00e9 cargar el archivo CSV sin realizar ninguna modificaci\u00f3n despu\u00e9s de descargarlo de Dialogflow. En todos estos casos, esperaba que el archivo CSV se cargara sin problemas. Sin embargo, a pesar de recibir un mensaje de que la carga se realiz\u00f3 correctamente (OK verde), las entidades no se reflejaron en mi agente de Dialogflow.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Return a list in Dialogflow CX",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-15T04:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Return-a-list-in-Dialogflow-CX\/m-p\/522398#M1286",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":111,
        "Question_body":"Hi guys how do you return a list in fulfillment in Dialogflow CX",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI: Getting a GRPC Exception when sending a prediction request in Java",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-07T03:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Getting-a-GRPC-Exception-when-sending-a-prediction\/m-p\/600835#M2093",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":181,
        "Question_body":"Hi,\n\nI have deployed a custom model (from a Docker image) in a Vertex AI endpoint.\n\nWhen I try to get a prediction in Java with the following code:\n\n\u00a0\n\nprivate PredictResponse predict(String endpointId, String query, String project, String location) throws IOException {\n    try (PredictionServiceClient serviceClient = getPredictionServiceClient()) {\n        EndpointName endpointName = EndpointName.of(project, location, endpointId);\n        ListValue.Builder listValue = ListValue.newBuilder();\n        JsonFormat.parser().merge(query, listValue);\n        List<Value> instanceList = listValue.getValuesList();\n        PredictRequest request = PredictRequest.newBuilder()\n                .setEndpoint(endpointName.toString())\n                .addAllInstances(instanceList)\n                .build();\n        return serviceClient.predict(request);\n    }\n}\n\n\u00a0\n\nI got a\u00a0io.grpc.StatusRuntimeException: INTERNAL: RST_STREAM closed stream. HTTP\/2 error code: INTERNAL_ERROR.\n\nI see no error when I check the logs in the Google Cloud Console. It looks like the request has been processed without error.\n\nThe strange part is that everything is working if:\n\nI call another Vertex model endpoint (with a simpler model inside a Docker image)\nI use a HTTP REST request (with curl) to get the prediction from the initial model (instead of Java code)\n\nThe request in CLI works so it seems that the model+endpoint is working.\n\nThe request in Java to another endpoint is working, so the code seems correct.\n\nAny idea how to debug\/fix this?",
        "Question_closed_time":"06-20-2023 09:49 AM",
        "Answer_score_count":0.0,
        "Answer_body":"I have found an open issue on this topic:\u00a0https:\/\/issuetracker.google.com\/issues\/234474507\n\nI have posted a workaround that also solves the bug in the Java SDK.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"AutoML Vision - Error type - No valid preprocessed examples",
        "Question_tag_count":2,
        "Question_created_time":"2022-01-11T13:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Vision-Error-type-No-valid-preprocessed-examples\/m-p\/183067#M170",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":108,
        "Question_body":"I have set up an Image classification (Single-label).\n\nThe model trained for 18 min 25 sec before I recieved the following error:\n\nDue to one or more errors, this training job was canceled on Jan 11, 2022 at 07:34AM Batch prediction job GAF-prediction-test encountered the following errors: No valid preprocessed examples.\n\nThere is no documentation that I could find that explains this error type. Anyone with any ideas what this means?",
        "Question_closed_time":"01-12-2022 01:24 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi, Thank you for reporting this behavior.\u00a0 \u00a0\n\nPlease note that\u00a0Groups is reserved for general product discussions. If you require further technical support it is recommended to post your detailed question on Stack Overflow which i can see that you have correctly did. [1].\n\n\u00a0\n\n[1]:https:\/\/stackoverflow.com\/questions\/70673890\/automl-vision-error-no-valid-preprocessed-examples\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"speech-to-text improvements",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-01T09:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/speech-to-text-improvements\/m-p\/174445#M71",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":360,
        "Question_body":"Good afternoon!\n\nI am a user of Speech-to-Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually.\n\nSo, in Russian it works fine, however, 30-40 percents of the words are incorrect. Moreover, there are no Russian punctuation at all. \u00a0So I get the speech-to-text transcript, then I create the perfect transcript out of this with correct words and punctuations.\n\nAll I want to know is how I can improve Speech-to-Text by using the perfect transcript I have already corrected? Where I can send that data to?\n\nP.S. Sorry for my English. I hope You can understand me",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Recover deleted Vertex AI resources",
        "Question_tag_count":2,
        "Question_created_time":"2022-03-22T22:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Recover-deleted-Vertex-AI-resources\/m-p\/405934#M243",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":517,
        "Question_body":"Hi,\n\nIn order to save on billing, I deleted most of the resources in the data sources, workbench, pipelines in Vertex AI.\n\nIs there a way I can recover them??",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard API",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-21T11:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-API\/m-p\/595340#M1988",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":150,
        "Question_body":"Hello, I know that there's the PaLM-2 API. My question is if there will ever be a BARD API. So a model with the ability to surf the internet.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Retail Facet Search: How to get all facets where result count is not empty in search response",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-03T19:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Retail-Facet-Search-How-to-get-all-facets-where-result\/m-p\/540416#M1576",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":184,
        "Question_body":"Dear Developers,\n\nI am currently working on implementing Google Retail Search in my application. The challenge I am facing is that I have over 200 custom product attributes, making it impractical to add all of them in the FacetSpecs during searching requests. However, I need to retrieve facets value and their count to build a user-friendly filter.\n\nI attempted to use dynamic Facets, but unfortunately, I could not find any specific documentation on how to retrieve dynamic facet data when using Search API. Hence, I am requesting suggestions from experienced developers within the community. Any help provided will be greatly appreciated.\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud VM Instances",
        "Question_tag_count":7,
        "Question_created_time":"2023-06-09T22:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-VM-Instances\/m-p\/601789#M2121",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":52,
        "Question_body":"hello everyone, I just sign up on google cloud yesterday and got a 300$ free trial also for 90 days but now when I am creating my very first new VM Instance I am getting an error \"The C2-CPUS-per-project-region quota maximum in region europe-west4 has been exceeded\" almost tries all regions, please anyone help me guide me.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"best DS solution for the use case below",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-25T03:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/best-DS-solution-for-the-use-case-below\/m-p\/615452#M2451",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":6,
        "Question_body":"we are looking to set a pipeline that will contain two layers , we are thinking a creating a dag that combines the two below layers, but we are still not sure what tools to use for the data science layer\n\ndata generation layer: Generation of a Table in bigquery containing around hundreds of thousands of rows (product) and around 26 fields based on a sql logic (bigquery)\u00a0 \u00a0, baring in mind that the number of rows might increase to millions in the near future\nData science layer :prediction of possible outcomes of each product\u00a0 \u00a0by a data science model in\u00a0 written in python.\u00a0 (for each product we need to predict what are the next possible stages\u00a0 for it),\u00a0there is a lot of\u00a0 computation done by the model which requires 20 types of gaussian mixture fittings, also the performance\u00a0 will depend on the amount of input products\/ output outcomes\u00a0\n\nThe solution would be running hourly , every day. Priorities of criterias whilst looking for a solution for the data science layer of the pipeline\u00a0 are as follows:\n\nInference :\u00a0possibility of making\u00a0 the model scale horizontally \u00a0( increase number of samples x ) or vertically (number of producst n ) \u00a0in order to \u00a0produce x possible outcomes for each single product , the scaling will be in the hand of the data scientist\nCosts\nPossibility of having a model registry ( similar to images registry , which will\u00a0 keep a history of artifacts of the models that can be deployed) \u00a0\nTraining whilst doing inference\nPossibility of giving the end user the choice of \u00a0input output (one or more specific product id as input\/and to choose \u00a0\u00a0the number of samples \u00a0as output for those input products)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Transliteration API has stopped working",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-11T02:37:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Transliteration-API-has-stopped-working\/m-p\/531290#M1414",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"Google Transliteration API was deprecated long time ago. Yet, the api link for the same worked for me till recent time. This API now generates an error response and I'm not able to find any solution to it.\u00a0\n\nMuch grateful to the community for shedding some light on this problem.\u00a0\n\nRegards.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Steps to get Real life data into the features section",
        "Question_tag_count":2,
        "Question_created_time":"2022-04-29T15:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Steps-to-get-Real-life-data-into-the-features-section\/m-p\/418695#M297",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"Hi,\u00a0\n\nI already create a tabular classification model using AutoML. All ready have the features created in a Goolge colab and now I need to get the real life data from a Public API to feed the features, then pass the features into the Model and finally get the classifications.\n\nThe question is are the steps and which tools should I use in order to connect to the API in order to receive the real time data?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Running Yolov8 on Google Colab leveraging GCP GPUs",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-09T13:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Running-Yolov8-on-Google-Colab-leveraging-GCP-GPUs\/m-p\/551745#M1844",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":141,
        "Question_body":"Hello!\n\nI'm sure that many of you might find my question\/request a trivial problem, and that I am just inexperienced, which I am. This will be my first time using GCP so I would like to be careful so that I don't get billed hundreds of dollars.\u00a0\n\nWith that out of the way, I could use some help. I have this colab file (https:\/\/colab.research.google.com\/drive\/13tLrfRPPc7m3V7X7Pdwjrv1w7RXC8CFR#scrollTo=JjooafMMr6zo\u00a0) and I would like to run the training portion of it. The problem is, the 100k dataset is far to big for colab's gpus. This led me to find GCP which provides essentially free GPU's with its $300 credit free trial. This leads me to the true problem, I have no idea how I would go about running this colab file (or the code in general) with GCP's GPU's. I've looked at a couple tutorials already, and many of them talk about port-forwarding jupyter things and whatnot which I have no idea how to use. Does anyone have\/know about good tutorials which go really in depth (like for complete and total dummies (also preferably a video)) of how to essentially run my colab file, or is anyone willing to step by step work with me to get this done?\n\nThanks,\n\nFrank",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow ES API returns payload not found in Intents",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-18T03:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-API-returns-payload-not-found-in-Intents\/m-p\/554537#M1951",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":69,
        "Question_body":"Hello,\n\nWhen calling the DialogFlowES agent via API for a query, it returns a payload that is not found in any of the intents. Any ideas on why this is happening?\u00a0\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow ES - Chat Window Content Customization to fix scroll issue",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-15T14:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-Chat-Window-Content-Customization-to-fix-scroll\/m-p\/533030#M1428",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":96,
        "Question_body":"Hello,\n\nI'm working on a Dialogflow ES agent where the bot has long responses in my use cases to where the beginning of most responses get hidden under the titlebar in the header of the chot window.\u00a0 The user would have to scroll up to see the start of the message which is not an ideal UX.\u00a0\n\nA thought to fix this issue is show one paragraph at a time in the response by using a delay and maybe showing three dots like someone is typing so a user would have time to read the response and has some kind of signal that there's more to the response vs dialogflow showing all the response at once.\u00a0 How would I go about setting that up either through configurations and\/or scripts? The HTML is something like:\n\n<div id=\"messageList\">\n\u00a0 \u00a0 <div class=\"message\">Paragraph 1<\/div>\n\u00a0 \u00a0\u00a0<div class=\"message\">Paragraph 2<\/div>\n\u00a0 \u00a0<div class=\"message\">Paragraph 3<\/div>\n\u00a0 \u00a0<div class=\"message\">Paragraph 4<\/div>\n<\/div>\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Rate Limiting on Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2023-03-09T15:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Rate-Limiting-on-Vertex-AI\/m-p\/530748#M1408",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":169,
        "Question_body":"I'm preparing to launch a prod endpoint using vertex ai automl text solution. I want to understand more prelaunch how many requests\/second my endpoint will handle.\u00a0\n\n\u00a0\n\nI see that\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/quotas\u00a0this implies I can run 30,000\/minute. Is that the limit or will things break down before then? Looking for some best practices.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Static HTML site translation",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-05T13:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Static-HTML-site-translation\/m-p\/541034#M1595",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"My site has no php (yet), only HTML and js. The following 3 extra lines on a page insert the functionality for the visitor (with a flag) to convert the page to other languages.\n\n\u00a0\n\n<div id=\"google_translate_element\"><\/div> <script type=\"text\/javascript\"> function googleTranslateElementInit() {\n  new google.translate.TranslateElement({pageLanguage: 'hu'}, 'google_translate_element'); } <\/script>\n<script type=\"text\/javascript\" src=\"\/\/translate.google.com\/translate_a\/element.js?cb=googleTranslateElementInit\"><\/script>\n\n\u00a0\n\nThis free service works quite well except with some names. I have collected a table of proper translations and would like to use that as a glossary. Please help me to modify the code. A GCloud project and the Translate API is already created, but I did not find how to use them. TIA",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Urgent access to private marked Bank Statement Parser & Procurement Doc Splitter Processor",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-22T01:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Urgent-access-to-private-marked-Bank-Statement-Parser-amp\/m-p\/512529#M1112",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":60,
        "Question_body":"I wanted to access Bank Statement Parser & Procurement Doc Splitter Processor in Document AI. It is currently marked as private. I have already Requested Access for it. But I have not heard back from Google. I need it urgently for POC to show it my client. Please help asap. We really want to use this in our project.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Observation related to Reponses",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-24T00:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Observation-related-to-Reponses\/m-p\/536378#M1471",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"We have observed that there is a mismatch in request and response.\n\nIn Detect language, Unable to detect multiple language still its response\u00a0type Location is of array\n\n\u00a0\"languages\":\u00a0[\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"languageCode\":\u00a0\"hi\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"confidence\":\u00a00.576136\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0]\n\u00a0\nWhy\u00a0languages is type of array?\u00a0\n\u00a0\nsame thing observe in Translate Document request we can pass single content whereas in response we get\u00a0byteStreamOutputs in array format\n\u00a0\nWhy we are keep keeping O\/P is in array format?\n\nThese were some observation we found.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"English to German translation issue in html for Chrome",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-25T08:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/English-to-German-translation-issue-in-html-for-Chrome\/m-p\/596972#M2024",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":55,
        "Question_body":"Hello,\n\nThis is the very first message for me here, sorry if some rules are not followed precisely. So our users in Germany are very confused by Google translation and the result of the translation is 100% opposite to what supposed to be and in the end they click on the wrong button. Details. If to save on disk and open in Chrome the simple HTML:\n\n\u00a0\n\n\u00a0\n\n<html>\n<body >\n    <input type=\"submit\" name=\"Allow\" value=\"Allow\" id=\"Allow\" >\n    <input type=\"submit\" name=\"Cancel\" value=\"Cancel\" id=\"Cancel\">\n<\/body>\n<\/html>\n\n\u00a0\n\n\u00a0\n\nand the to translate to German the translation will be:\n\n\u00a0\n\n\u00a0\n\n<html>\n<body >\n    <input type=\"submit\" name=\"Allow\" value=\"Abbrechen \" id=\"Allow\" >\n    <input type=\"submit\" name=\"Cancel\" value=\"zulassen\" id=\"Cancel\">\n<\/body>\n<\/html>\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nand this is the 100% opposite result for both buttons. If in English the word \"Allow\" to replace by the word \"Approve\" then the translation works fine - including the second button even though we do not change it!\n\n\u00a0\n\nOK, this is not super critical and urgent but might be interesting enough bug report for the API developers.\n\n\u00a0\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BigQuery ARIMA Model - What changed?",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-09T12:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQuery-ARIMA-Model-What-changed\/m-p\/421507#M318",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"I have published an open source project using the\u00a0ARIMA model\u00a0available in Big Query to predict the price of BTC using a\u00a0free API.\n\nThe results are displayed in a\u00a0Datastudio dashboard.\n\nSomething changed on the 28th of March of 2022. I suggest selecting the date range: from 2022-03-26 to 2022-03-31.\n\nI failed to find the answer in any of\u00a0Google's documentation.\n\nDoes anyone know what changed?\n\nRegards,",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Translate API Accuracy",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-04T08:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API-Accuracy\/m-p\/550287#M1788",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":66,
        "Question_body":"Hello,\n\nWe use Google Cloud Translate API to translate law and policy documents for our web app. As these will be used for research, we need to give users a disclaimer about accuracy in our Terms of Use. I can't see any information about how accurate Google Cloud Translate is, only Google Translate (which I understand have different accuracies). Can someone please help?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI - Any trainings ?",
        "Question_tag_count":2,
        "Question_created_time":"2021-06-23T16:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Any-trainings\/m-p\/161842#M4",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":580,
        "Question_body":"Hi Community\n\nFirstly I would like to congratulate all for the announcement of VertexAI.\n\nI am interested to know that with the advent of VertexAI, is the AI-Platform planned to be deprecated completely ? How can existing AIP ML engineers easily transit to VertexAI as there's a huge list of topics and things which VertexAI has brought in and has changed the way GCP is doing ML going forward.\n\nDo we have any free Qwiklabs notebooks specifically for VertexAI as now I find myself a lot unaware about new GCP ML( VertexAI ) stack even though I am a GCP ML certified Engineer. This is I am sure an issue with a lot of customers and engineers who were building on AIP since a long time.\n\nHope someone can understand and provide their advice. @rseshadri\u00a0@Former Community Member\u00a0what are your thoughts ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Exploring General-AI-Like Functionality in Dialogflow and Google Cloud",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-03T13:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Exploring-General-AI-Like-Functionality-in-Dialogflow-and-Google\/m-p\/517876#M1203",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":167,
        "Question_body":"Hello everyone,\n\nI hope this message finds you well. I'm new to this forum and have recently been exploring Dialogflow as a potential solution for my research on general-AI-like dialogue bots. I have set up an agent and connected it to Telegram via a webhook using repl.it and OpenAI's GPT-3 davinci model.\n\nHowever, I was curious if Google Cloud offers any similar functionality to GPT-3. I would greatly appreciate any advice or guidance on this matter.\n\nThank you and best regards, Tom",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"cannot access documentai api from compute",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-30T15:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cannot-access-documentai-api-from-compute\/m-p\/515739#M1154",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":147,
        "Question_body":"Hi all,\n\nI've got a compute instance that cannot access the documentai processor.\u00a0 The compute engine is in the same project as the processor, and I've given the service account the roles\n\n\"Document AI API User\" and\n\"Document AI Viewer\"\u00a0\n\nThe error I receive is\n\n\"7 PERMISSION_DENIED: Request had insufficient authentication scopes.\"\u00a0\n\nwhich feels like an Oauth issue, but my reading leads me to believe that documentAI uses Application Default Credentials, and that my compute instance should use the service account for the request.\u00a0\u00a0\n\nthanks in advance for any insight.",
        "Question_closed_time":"01-31-2023 06:46 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi,\n\nthanks, I had tried adding scopes, but the solution was to add a key to the service account\u00a0 add the json config to the file system and add the environment variable\u00a0\n\nGOOGLE_APPLICATION_CREDENTIALS\n\nas per\u00a0\n\nhttps:\/\/stackoverflow.com\/questions\/65703339\/fixedcredentialsprovider-gives-unauthorized-exception-w...\n\n\u00a0\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Webhooks",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-12T08:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Webhooks\/m-p\/509590#M1056",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":147,
        "Question_body":"I just want to clarify my understanding. I can use my own servers for calling webhooks correct (as long as they return the json structure required). The webhooks will essentially reach out another API service and return data for fulfillment. Thanks in advance for your time.",
        "Question_closed_time":"01-12-2023 08:47 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Exactly correct.\u00a0 During the processing of a conversation, if you have a Web Hook enabled, the Dialogflow engine will call-out to the target URL passing in a JSON payload and expecting a correctly formatted JSON response.\n\nSee the following for details:\n\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/concept\/webhook\n\nTake care to notice that the target service MUST be callable through HTTPS which means that it has a valid SSL certificate.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Multi-Webhook Challenges in Dialogflow CX Conversations",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-25T03:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Multi-Webhook-Challenges-in-Dialogflow-CX-Conversations\/m-p\/615446#M2450",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":12,
        "Question_body":"I have a Dialogflow CX conversation with the objective of filling out a form with parameters. For this purpose, I have created two webhooks: one that allows for disambiguation of the location and provides additional context information, and another that sends the data to be saved in the database. Both webhooks are implemented as Google Cloud functions. I have tested them separately, and they work fine. However, I am encountering an issue where it seems that only one webhook can be triggered per session in Dialogflow CX. Nevertheless, I couldn't find any references in the documentation that explicitly state this limitation of having only one webhook per session.\n\nI have tested the same webhook with another agent, and it works as expected. However, it seems that within the same session, the system does not execute it if there has been a prior request to a different webhook.\n\nThis is the only reasonable explanation for why another webhook is not triggered in the same conversation. I wanted to know if this is a problem that others have encountered or if there's something I might be doing wrong and haven't found the correct solution yet. Assuming there are restrictions for only one webhook per session, the right approach would be to create a webhook that contains both functions I need, and each function can be called separately through function tags. However, before proceeding with this approach, I wanted to confirm with the community if this is a recurring issue.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Triton on Vertex AI does not support multiple models?",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-25T07:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Triton-on-Vertex-AI-does-not-support-multiple-models\/m-p\/459822#M533",
        "Question_answer_count":5,
        "Question_score_count":2,
        "Question_view_count":427,
        "Question_body":"Currently, I want to deploy a Triton server to Vertex AI endpoint. However I received this error message.\n\n\"failed to start Vertex AI service: Invalid argument - Expect the model repository contains only a single model if default model is not specified\"\n\nIs this mean that the Triton server deploy only support one model? It is different from what I have read in this document about concurrent model execution\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-nvidia-triton",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Product Specifications or Product Recommendation in the Support Section",
        "Question_tag_count":3,
        "Question_created_time":"2022-04-21T08:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Product-Specifications-or-Product-Recommendation-in-the-Support\/m-p\/415770#M288",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":52,
        "Question_body":"Hi\n\nAny ones knows if I could get\u00a0Product Specifications or Product Recommendation in the Support Section from Google Cloud:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Artificial Intelligence - list of APIs",
        "Question_tag_count":4,
        "Question_created_time":"2021-07-23T18:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Artificial-Intelligence-list-of-APIs\/m-p\/164618#M22",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":673,
        "Question_body":"Good day.\n\nIs it possible to list the Google APIs in the\u00a0Artificial Intelligence, especially in the Voice recognitions,\u00a0comparison, analytics etc?\n\n\u00a0\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Importing to Vertex dataset does not import labels.",
        "Question_tag_count":3,
        "Question_created_time":"2022-11-02T04:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Importing-to-Vertex-dataset-does-not-import-labels\/m-p\/484912#M716",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":536,
        "Question_body":"In Vertex AI I am updating an image dataset, thus:\n\nfrom google.cloud import aiplatform\nimport_schema_uri = aiplatform.schema.dataset.ioformat.image.single_label_classification\ndataset_id = \"my_ds_id\"\n\nds = aiplatform.ImageDataset(dataset_id)))\nds.import_data(gcs_source=DATASET_PATH, import_schema_uri=import_schema_uri)\n\nthe images are uploaded to the dataset but their labels are ignored and they are classed as\u00a0Unlabeled. What am I doing wrong? TIA!\n\nPS they are in a csv, like:\n\ngs:\/\/path\/to\/file\/barnacles.jpg,label1\n\nwhich worked fine for the dataset creation.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error 403 in Vertex Ai Workbench",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-08T02:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-403-in-Vertex-Ai-Workbench\/m-p\/530061#M1390",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":340,
        "Question_body":"I am facing an issue where if I click on the \"OPEN JUPITER LAB\" link on my instance it directs me to a Error 403 page with the text: \"403.\u00a0That\u2019s an error.\u00a0That\u2019s all we know.\"\n\nWhat access am I missing?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI don't recognize parent label area correctly, and does it only on per line basis",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-11T10:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-don-t-recognize-parent-label-area-correctly-and-does\/m-p\/521091#M1268",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":104,
        "Question_body":"Good Day, I have an issue with Document AI. When I try to create Parent label, with child labels in it, it does not recognize whole area of parent label correctly, and only recongizes it on per line basis, with separate label for each. Can it be done somehow that it recognize both lines in one parent label insteand of dividing it?\u00a0\n\nHere is example how AI labeled it.\n\n\nBelow I have provided example on how I labeled the \"Training\/Testing\" datasets",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud translation doesn't use glossary in glossary config",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-13T22:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-translation-doesn-t-use-glossary-in-glossary-config\/m-p\/543726#M1663",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hello everyone,\n\nOur team has been using the Cloud Translation service to translate strings from Chinese into other languages, with the help of glossaries. However, we have recently encountered an issue where we are unable to use newly uploaded glossaries to perform translations.\n\nProblem Details\nThe issue began on December 22nd, 2022.\nWe have attempted to upload the last available glossary again to prove that the issue is not with the format or the validity of the glossary.\nWe have also tried updating the glossary with a new entry, but this did not resolve the problem.\nHowever, we have tested with an older glossary, which worked fine. Therefore, we suspect that the issue is not with the API itself.\nRequest for Assistance\n\nDespite searching through the documentation, we have not been able to find any helpful information to resolve this issue. We are seeking advice and guidance from the community to determine what could be causing this problem and how we can address it.\n\nThank you for your help in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"LogisticRegression model to TFLite?",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-09T19:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/LogisticRegression-model-to-TFLite\/m-p\/508457#M1037",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hi All,\n\nI need your guidance!\n\nI have trained a logistic regression model from the scikit-learn library and saved the model as .cpickle file.\n\nBut the goal is to use this model in an Android app so my android team needs the model in supportive format. I am not sure if .cpickle is supportive for them or not.\n\nSo my query is in which format I can save the model so that mobile team can easily integrate it with the Android app. I know about the TFLite format but how can I convert the model to this format or is there any other way?\n\nPlease share your thoughts on this.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Translation of MySQL data in 6 different language",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-17T02:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translation-of-MySQL-data-in-6-different-language\/m-p\/454758#M512",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":169,
        "Question_body":"I have 20K record in 1 Table of MySQL DB\n\nThis table is having a Description column and 6 different columns as TLang1, TLang2, Tlang3....\n\nI have to translate the data in Description column in 6 Different Languages and insert them in\u00a0TLang1, TLang2, Tlang3.... columns in the same row.\n\nWhat approach I can use to do this since the current approach is taking too long.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error uploading csv file to Vertex DataSets",
        "Question_tag_count":2,
        "Question_created_time":"2022-06-17T09:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-uploading-csv-file-to-Vertex-DataSets\/m-p\/432499#M380",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":302,
        "Question_body":"Hi to all\n\nTrying to upload a .csv file to AutoMl for training.\n\nNot sure what Im doing wrong, I save the file as\u00a0csv encode utf 8 and\u00a0values separated by comma and with both cases getting the error that you will find in the next image.\n\nDo I need to upload the files to Cloud Storage or Google BigQuery before using them for training?\u00a0\n\nWhen trying to create and train the model got the warning from the next image:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Why does Jupyter Notebooks not recognize changes in my .py files?",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-09T11:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Why-does-Jupyter-Notebooks-not-recognize-changes-in-my-py-files\/m-p\/421489#M316",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":38,
        "Question_body":"Hello,\u00a0\n\nWhen working locally, I usually put routine tasks inside functions held in a .py file and import those.\u00a0 When I need to make a change, I change the function, reimport and moving on with the main script.\u00a0\u00a0\n\n\u00a0\n\nGCP's jupyter instance does not recognize when I make the change an re-import the function.\u00a0 I have to restart the kernel each time.\u00a0 Is there a way around this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Problem with diarization using Portuguese Brazil (pt-BR)",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-13T06:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Problem-with-diarization-using-Portuguese-Brazil-pt-BR\/m-p\/602584#M2145",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"I am using Google Speech-to-Text to transcribe my call center's phone calls, all of them in Portuguese and using recorded in WAV format (Encoding type: LINEAR16, Sample rate: 8000Hz.\n\nI'm using API V2, spoken language (pt-BR), model Telephony, Recognizer (below):\n\nBut the transcription recognized only one speaker.\u00a0 Where am I doing wrong?\n\n\u00a0\n\nThank you.\n\nRicardo Barcellos",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"WARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-04T06:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-BlockManager-Block-rdd-6-0-already-exists-on-this-machine\/m-p\/450462#M486",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":163,
        "Question_body":"Hi there.\nI am working with Vertex AI Jupyterlab Notebook.\nThere were a few such warnings\n\nWARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it\nWARN BlockManager: Block rdd_817_0 already exists on this machine; not re-adding it\n\non this as the model was getting trained.\nMay I know if we are safe to ignore this?\nWhat do they mean actually?\nThanks in advance.",
        "Question_closed_time":"08-23-2022 10:20 AM",
        "Answer_score_count":0.0,
        "Answer_body":"No, you should not worry since this is just a warning that tells you that those two blocks will not be re added to your notebook.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Effect of location on NLU model performance in Dialogflow CX",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-09T20:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Effect-of-location-on-NLU-model-performance-in-Dialogflow-CX\/m-p\/610505#M2328",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":56,
        "Question_body":"Hi all,\u00a0\nI have 2 queries related to the issue of NLU model's performance:\n1. we have a CX agent whose user base is mainly USA, but since the client is from Asia, the location for the agent is set as Asia Northeast1(Tokyo). Is there any relation between location and the NLU model's intent recognition performance?\n2. Does the performance of NLU model change overtime due to interaction logging? In the footnote of this setting, it is mentioned that this setting needs to be enabled for \"NLU Model Improvement\". What does this mean?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Python: Google Vision doesn't (cannot) read and convert photocopied books, from PDF into TXT",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-18T12:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-Google-Vision-doesn-t-cannot-read-and-convert-photocopied\/m-p\/604310#M2182",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"THIS CODE HAVE NO ERROR. The OUPTUT done.\n\nThe problem is that the converted files (.txt) are 0 bytes. Seems that google cloud vision cannot\u00a0read and convert photocopied books, from PDF into TXT\n\n\u00a0\n\nimport\u00a0os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"d:\/doc\/doc\/MY-KEY.json\"\n\nfrom\u00a0google.cloud\u00a0import\u00a0vision\n\nfrom\u00a0google.cloud.vision_v1\u00a0import\u00a0types\n\nfrom\u00a0google.oauth2.service_account\u00a0import\u00a0Credentials\n\n#\u00a0from\u00a0google.cloud\u00a0import\u00a0storage\n#\u00a0client\u00a0library\n#\u00a0storage_client\u00a0=\u00a0storage.Client()\n\n#\u00a0Set\u00a0up\u00a0the\u00a0Google\u00a0Cloud\u00a0Vision\u00a0client\u00a0with\u00a0service\u00a0account\u00a0credentials\n#\u00a0credentials\u00a0=\u00a0Credentials.from_service_account_file('d:\/doc\/doc\/bebe-1084-992b240528be.json')\n#\u00a0client\u00a0=\u00a0vision.ImageAnnotatorClient(credentials=credentials)\n\n#pip\u00a0install\u00a0google-cloud-vision\n\n#\u00a0Set\u00a0up\u00a0the\u00a0Google\u00a0Cloud\u00a0Vision\u00a0client\n\nclient\u00a0=\u00a0vision.ImageAnnotatorClient()\n\n\n#\u00a0Directory\u00a0containing\u00a0the\u00a0PDF\u00a0files\n\npdf_directory\u00a0=\u00a0\"d:\/doc\/doc\"\n\n\n#\u00a0Output\u00a0directory\u00a0for\u00a0the\u00a0TXT\u00a0files\n\noutput_directory\u00a0=\u00a0\"d:\/doc\/doc\"\n\n\n#\u00a0Get\u00a0a\u00a0list\u00a0of\u00a0PDF\u00a0files\u00a0in\u00a0the\u00a0directory\n\npdf_files\u00a0=\u00a0[file\u00a0for\u00a0file\u00a0in\u00a0os.listdir(pdf_directory)\u00a0if\u00a0file.endswith(\".pdf\")]\n\n\n#\u00a0Process\u00a0each\u00a0PDF\u00a0file\n\nfor\u00a0pdf_file\u00a0in\u00a0pdf_files:\n\n\u00a0\u00a0\u00a0\u00a0pdf_path\u00a0=\u00a0os.path.join(pdf_directory,\u00a0pdf_file)\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Create\u00a0the\u00a0output\u00a0TXT\u00a0file\u00a0path\n\n\u00a0\u00a0\u00a0\u00a0txt_file\u00a0=\u00a0os.path.splitext(pdf_file)[0]\u00a0+\u00a0\".txt\"\n\n\u00a0\u00a0\u00a0\u00a0txt_path\u00a0=\u00a0os.path.join(output_directory,\u00a0txt_file)\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Read\u00a0the\u00a0PDF\u00a0file\u00a0as\u00a0bytes\n\n\u00a0\u00a0\u00a0\u00a0with\u00a0open(pdf_path,\u00a0'rb')\u00a0as\u00a0file:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0content\u00a0=\u00a0file.read()\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Convert\u00a0PDF\u00a0to\u00a0image\u00a0using\u00a0Google\u00a0Cloud\u00a0Vision\u00a0API\n\n\u00a0\u00a0\u00a0\u00a0input_image\u00a0=\u00a0types.Image(content=content)\n\n\u00a0\u00a0\u00a0\u00a0response\u00a0=\u00a0client.document_text_detection(image=input_image)\n\n\n\u00a0\u00a0\u00a0\u00a0#\u00a0Extract\u00a0text\u00a0from\u00a0the\u00a0response\u00a0and\u00a0save\u00a0it\u00a0as\u00a0TXT\n\n\u00a0\u00a0\u00a0\u00a0text\u00a0=\u00a0response.full_text_annotation.text\n\n\u00a0\u00a0\u00a0\u00a0with\u00a0open(txt_path,\u00a0'w',\u00a0encoding='utf-8')\u00a0as\u00a0file:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0file.write(text)\n\n\n\u00a0\u00a0\u00a0\u00a0print(f\"Converted\u00a0{pdf_file}\u00a0to\u00a0{txt_file}\")",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Document AI - The Caller Does Not Have Permission Error",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-27T13:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Document-AI-The-Caller-Does-Not-Have-Permission-Error\/m-p\/526760#M1351",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":196,
        "Question_body":"Hi , i'm trying to use Google Document API to parse invoices using invoice parser. Followed the doc and imported 10 invoices to train the parser. Used Auto Labeling option. While trying to mark the imported invoices as \"Mark as Labeled\" after few corrections, Im getting an error saying \"The caller does not have permission\". The user trying to mark is the owner of the project. The owner of the project has all permissions to the parser and storage. Can some one please help.",
        "Question_closed_time":"02-27-2023 04:25 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Howdy\u00a0ganeshsarathi,\nWhat an awesome post.\u00a0 THANK YOU for providing all these details.\u00a0 I used the post content as search arguments inside Google to see if other customers have had similar problems.\u00a0 I immediately got lucky and found a report (for my records\u00a0b\/266677102).\u00a0 This report had exactly the same symptoms and was reported on 2023-01-25 which means that it is recent.\u00a0 The problem (for that client) was that the GCS bucket that was being used to host the documents had a \"retention policy\" defined on it.\u00a0 As I understand retention policies, they effectively mean that once an object has been written into the bucket, it may not be deleted or updated.\u00a0 Retention policies are used to prevent accidental deletion and mean that the content becomes immutable for the period of time that the documents are being retained.\u00a0 We can read more about buckets with retention policies here.\n\nWhat I'd like you to do is examine your environment as it relates to Document AI.\u00a0 Let's see which Google Cloud Storage buckets may be in play (i.e. where the documents may be living) and whether or not they have retention policies applied.\u00a0 If the answer is yes, then we have an exact match.\u00a0 If no, then we will have to look for other reasons ... but we'll start here.\n\nThe other customer switched to using a bucket that did not have a retention policy.\u00a0 However, the ticket remains open while Google considers what this means in terms of retaining documents while at the same time allowing HITL processing.\u00a0 Looking forward to hearing back.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Google Translate API - Basic Version",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-21T01:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-Basic-Version\/m-p\/614310#M2415",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":33,
        "Question_body":"Below is one of the python scripts recommended by google to translate text. I am using cloud translation basic version. I want to convert all the text strings to English but if it encounters an English text string it shouldn't translate and just return the initial text string as output. As per my understanding, first it detects the language and then it passes the text for translation. When this entire loop is completed I get charged per character. But when it detects English, does it translates it back to English or does it just detects and passes the string as it is. And do I get charged in the 2nd case?\n\ndef translate_text(target: str, text: str) -> dict:\n    \"\"\"Translates text into the target language.\n\n    Target must be an ISO 639-1 language code.\n    See https:\/\/g.co\/cloud\/translate\/v2\/translate-reference#supported_languages\n    \"\"\"\n    from google.cloud import translate_v2 as translate\n\n    translate_client = translate.Client()\n\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\")\n\n    # Text can also be a sequence of strings, in which case this method\n    # will return a sequence of results for each text.\n    result = translate_client.translate(text, target_language=target)\n\n    print(\"Text: {}\".format(result[\"input\"]))\n    print(\"Translation: {}\".format(result[\"translatedText\"]))\n    print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n\n    return result\n\noutput = translate_text('en', 'text')\n\n\u00a0\n\n\u00a0\n\ndef translate_text(target: str, text: str) -> dict:\n    \"\"\"Translates text into the target language.\n\n    Target must be an ISO 639-1 language code.\n    See https:\/\/g.co\/cloud\/translate\/v2\/translate-reference#supported_languages\n    \"\"\"\n    from google.cloud import translate_v2 as translate\n\n    translate_client = translate.Client()\n\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\")\n\n    # Text can also be a sequence of strings, in which case this method\n    # will return a sequence of results for each text.\n    result = translate_client.translate(text, target_language=target)\n\n    print(\"Text: {}\".format(result[\"input\"]))\n    print(\"Translation: {}\".format(result[\"translatedText\"]))\n    print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n\n    return result\n\n\u00a0\n\nthe script works properly but I how do I ensure that I am not wasting any money to translate English to English or for that matter any language. Like German to German, French to French etc.\n\nThank You!\n\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Audio export bucket - only save incoming audio",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-08T06:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Audio-export-bucket-only-save-incoming-audio\/m-p\/497482#M922",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":135,
        "Question_body":"Hello everyone\n\nWe're using dialogflow cx for a call bot. For later use we load the exported audio from the dialogflow bucket and display it in a seperate frontend.\n\nAs it is written in the tooltip help \"Configure the incoming audio to be automatically exported to the specific Google Cloud Storage destination by Dialogflow.\"\n\nI expect to only get the incoming audio files (what the user says on the phone).\u00a0But I also get the responses from the agent defined in \"Agent says\":\n\nIs there a way to prevent this from happening? Thanks",
        "Question_closed_time":"12-09-2022 01:01 PM",
        "Answer_score_count":0.0,
        "Answer_body":"As this is specific to your project, you may file a 1:1 Support. A dedicated resource will work with you on this scenario.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Custom container in vertex workbench",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-10T10:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-container-in-vertex-workbench\/m-p\/430464#M372",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":660,
        "Question_body":"I noticed that you can use a custom container from the container registry when creating user-managed notebook, but I couldn't find any documentation on the required configuration\/dockerfile specs for it to work with jupyterlab in a similar fashion to launcing a regular workbench environment (e.g. python 3). Should I open default jupyter lab port? anything else?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI does not import line_item child labels",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-16T06:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-does-not-import-line-item-child-labels\/m-p\/500346#M957",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":184,
        "Question_body":"Hi,\nAfter exporting a dataset from one processor, we're seeing labels under line_item parent group fail to be imported for another processor, even though both processors have child labels in common.\nHow can we avoid this problem?\nThanks in advance,\nRasmus",
        "Question_closed_time":"12-16-2022 07:08 AM",
        "Answer_score_count":1.0,
        "Answer_body":"OK, I think I found the issue. I believe there's a bug in the Document AI dataset exporter. Steps to repro:\n\nCreate a new invoice processor\nLabel a document using a child label, ie create parent label \"line_item\" and a child label \"unit\"\nExport document and inspect json. Notice unit type \"line_item\/unit\" within the \"line_item\" section.\nCreate a new custom processor, setup parent label \"line_item\", child label \"unit\".\nImport document. Notice unit fails to identify.\u00a0\nLabel unit again.\nExport document and inspect json.\u00a0Notice unit type \"unit\" within the \"line_item\" section. This is the reason import didn't work.\n\nSo in order to successfully import documents from dataset of one processor to another, drop the \"line_item\/\" prefix for all \"type\" fields using a text editor.\n\nDocument AI team, any chance for a fix for this?\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Can Google Cloud Vision work as fast as Google lens for OCR?",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-29T12:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-Google-Cloud-Vision-work-as-fast-as-Google-lens-for-OCR\/m-p\/181569#M139",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":745,
        "Question_body":"Hello, I am using Google Cloud Vision for text recognition, but the processing speed is quite slow (5 to 15 seconds). I would like to know how does Google lens work so fast and if there's any way to make Google Vision as fast.\n\nEdit: My photos that go through Vision are stored in Firebase Storage. (As I've read in some posts this is the quickest way to process them).\n\n\u00a0\n\nThanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Kubeflow pipeline fails on GKE cluster",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-03T08:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Kubeflow-pipeline-fails-on-GKE-cluster\/m-p\/517717#M1197",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":157,
        "Question_body":"Need to create a Kubeflow pipeline for ML use-cases on GKE cluster, currently working on recommendation.\u00a0Have made the Vertex AI pipeline for the same but vertex being serverless takes time to make the containers up and then run the pipeline.\u00a0We needed a platform where the kubeflow pipelines can be executed faster - especially during development the results are needed instantly so that we can change inputs or tune the model.\n\nAfter uploading the yaml file, creating a run for the pipeline. After the first component the file read fails to write on the given bucket with below error.\n\nFileNotFoundError: [Errno 2] No such file or directory: '\/gcs\/bucket\/reco_v2\/637705bf-381e-40fa-8597-91089e700aaf\/pipeline\/reco_v2\/637705bf-381e-40fa-8597-91089e700aaf\/get-dataframe\/df_path.csv'39F0203 13:57:24.664574\u00a0 \u00a0 \u00a0 18 main.go:50] Failed to execute component: exit status 140time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact \/tmp\/outputs\/test_df_path\/data\" argo=true error=\"stat \/tmp\/outputs\/test_df_path\/data: no such file or directory\"41time=\"2023-02-03T13:57:24.672Z\" level=error msg=\"cannot save artifact \/tmp\/outputs\/train_df_path\/data\" argo=true error=\"stat \/tmp\/outputs\/train_df_path\/data: no such file or directory\"42Error: exit status 1\n\nTried running the sample tutorial kubeflow pipelines, they are running fine and does not throw any such file write error.\n\nWe have created a cluster and integrated Kubeflow pipelines from the GCP marketplace -> https:\/\/console.cloud.google.com\/marketplace\/details\/google-cloud-ai-platform\/kubeflow-pipelines\n\nNeed to know if there is a version issue or code is not proper or anything else. Have tried running a kfp v2 code too(from tutorials online) that worked well too.\n\nThe kubeflow pipeline should have worked and saved outputs to the bucket given.\n\nPlease help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"NLP",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-27T02:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NLP\/m-p\/181365#M133",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"https:\/\/colab.research.google.com\/drive\/1U26EA63hocAzyGFLsNP3u-nQUCFYVbpz",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"[Vertex AI] Bug - Failed to create endpoint due to the error: INTERNAL",
        "Question_tag_count":1,
        "Question_created_time":"2022-03-15T06:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Bug-Failed-to-create-endpoint-due-to-the-error\/m-p\/403711#M237",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":514,
        "Question_body":"When attempting to create a new Vertex AI endpoint in us-central1 using a healthy model, I keep getting the error: \"Failed to create endpoint \"NAME\" due to the error: INTERNAL\"\n\nI expected the endpoint to get deployed successfully. \u00a0In fact, up to about 7 days ago, this operation worked perfectly.\n\nSteps to reproduce:\nAttempt to deploy a health Vertex AI model to a new endpoint in us-central1\n\nI'm currently trying to figure out if this INTERNAL error is specific to a region (or not), but it will take me hours before I can determine if the region is a factor. \u00a0I suspect there's some other global issue that's the problem.\n\nHas anyone else encountered this problem?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Confirmation page \/ custom text",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-22T03:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Confirmation-page-custom-text\/m-p\/480915#M675",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"Hi All,\n\nI'm just migrating over from AWS to CX, and so far I think its great. However - the tutorial section I'm working through kind of hit a 'draw the rest of the owl' meme - if you don't know it, look it up.\n\nThe difficult bit is where it gets to 'confirmation page' in the quick start - here:\n\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/quick\/build-agent#create_the_confirmation_page\n\u00a0\nBefore this it all makes sense and is easy to follow, but this is bit like 'now we've shown how to do the simple stuff we're just going to give you the title of the next bit even tho its by far the most complex and important.\n\u00a0\nDoes anyone have an example of how to actually step through this confirmation page? I've tried all the ways I know and failed. Basically the agent response is just reporting the $param stuff not treating it as populated field. I've had the same issues with trying to return any custom text.\u00a0\n\u00a0\nI see there are a few staffers on here - it would be great if this Quickstart had something to follow from confirmation page on, as currently it really doesn't - not to someone moving over from AWS anyway.\n\u00a0\nThanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using Vison ML via REST API",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-06T02:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-Vison-ML-via-REST-API\/m-p\/438619#M404",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":226,
        "Question_body":"I'm taking my first steps with Vision ML and using the REST interface (https:\/\/vision.googleapis.com\/v1\/files:annotate). As API key I provide the key from the Firebase project settings. In the Authorization Bearer, I supply the token from Firebase-Auth after sign-in.\n\nWhen accessing Annotate I get a 403 (Permission_Denied) error message back:\nError opening file: gs:\/\/######.appspot.com\/MyFile.tiff.\n\nThe object is available in the corresponding bucket and it is not blocked due to the Firebase Storage rules.\n\nCan I pass a Firebase token in this REST interface at all?\n\nHow do I make sure that the service account can access the storage?\n\nThank you for any hint\n\nAuthor of FB4D GitHub Project (A Delphi Library for access Firebase Services via REST).",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model Selection \/ Feature Engineering",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-02T05:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-Selection-Feature-Engineering\/m-p\/449387#M466",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"Hey There,\n\nI am writing my Master Thesis at the moment. I am comparing AutoML products for image classification. There I compare the product Vertex AI with to Azure. However, I can't find the concrete methods of feature engineering and model selection from the documentation.\n\nThanks a lot!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google cloud speech to text : Dealing with high amount of input data in realtime",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-28T07:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-speech-to-text-Dealing-with-high-amount-of-input\/m-p\/471924#M610",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":147,
        "Question_body":"Scenario: We are receiving huge amount of voice data streams which could be upto couple of minutes long. There will huge set of data streams as an input every second. This a real time application where I get live data from multiple users and return response from api. I am using google' AsyncStreamingRecognize to create stream. This stream open, read and write functions are blocking as I cannot proceed with other operation until success is returned by get() functions. Due to this my threads responsible for dealing with stream data are getting blocked. Is there a better way to where I can write high amount of input data without my thread being blocked for previous write function ? The code\/setup works fine for a single call in real time. But I doubt how to design it well that could handle huge realtime incoming traffic",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can't enable generativelanguage API even on Admin account for GCP",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-15T23:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-t-enable-generativelanguage-API-even-on-Admin-account-for\/m-p\/553710#M1921",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":34,
        "Question_body":"Is there a way to resolve this? As I need it enable for use with Langchain.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX masking customer data entered in a wrong place",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-13T07:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-masking-customer-data-entered-in-a-wrong-place\/m-p\/611970#M2358",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":36,
        "Question_body":"Team,\n\nwe have very successful chatbot that collect some of customer's private information. If entered in the right part of the dialog we are able to mask and redact the information in the logs. However some users are inputing information in a wrong part of the dialog.\n\nExample: Dialog expect user to say \"Continue\" or push a continue button, but instead of that, user enters an username.\n\nAny ideas how we can capture it and redact it in the logs?\n\nM",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google TTS problem with large numbers",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-18T17:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-TTS-problem-with-large-numbers\/m-p\/545075#M1687",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":178,
        "Question_body":"I am using Google TTS and running into this error:\n\n\u00a0\n\nGoogle has returned an error: 400 This request contains sentences that are too long.\n\n\u00a0\n\nwith this text which is only 155 bytes:\n\nThe Assessment Review Board of Canada has determined that the assessment of 185 Corkstown Road in Ottawa should be reduced from 35447892 to 31537798 in the Large Industrial Class and 66985690 total assessed value.\n\nWhy am I receiving this error?\u00a0 Could it have something to do with the large numbers?",
        "Question_closed_time":"04-18-2023 08:00 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Sadly, I am not familiar with the \"make.com\" system.\u00a0 Google Cloud provides a rich array of services that can be invoked over the Internet.\u00a0 The text to speech is an example of one such service.\u00a0 Google makes APIs available that can be called to request the service function and, when a request is received, Google satisfies that request.\u00a0 The API call for a text to speech request\u00a0 can be found here.\u00a0 As we look at the API request, we see that it has expected parameters which includes the text to verbalize.\u00a0 In the demo\/web site, we pasted in what we think is the text we want verbalized and it comes back correctly.\u00a0 However, all this does is exercise the Google Cloud API ... it doesn't tell us anything about how the \"Make.com\" module either works or the parameters that are\u00a0actually being passed when you invoke your \"make.com\" application.\u00a0 \u00a0About the best I can suggest is to examine very carefully your configuration of the make.com app (scenario?).\u00a0 Be as sure as possible that what you are sending to Google Cloud to verbalize is the actual text you think you are sending.\u00a0 Look also at other parameters and see if you can map those to what are expected by the API.\u00a0 You might also want to reach out to help to the good folks at make.com.\u00a0 Maybe they have a recipe to add debugging to your application to see what is actually being passed in to the Google Text To Speech call.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"AIML",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-20T04:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AIML\/m-p\/501497#M973",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":97,
        "Question_body":"Show More",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issues with Handover Protocols - Facebook & Dialogflow",
        "Question_tag_count":1,
        "Question_created_time":"2021-09-29T18:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-Handover-Protocols-Facebook-amp-Dialogflow\/m-p\/171609#M58",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":380,
        "Question_body":"We're currently experiencing an ongoing issue where Bot conversations in our Facebook inbox are moving from \u2018Main\u2019 to \u2018Done\u2019 without any manual agent involvement. This means that conversations being escalated to the Main folder from the Bot will revert to the Done folder and the Bot will answer again.\n\u00a0\nOnce the conversation with the bot has been escalated to a human, there should be no further Bot involvement.\n\u00a0\nThis issue originally started after we noticed a failed payment on our Dialogflow account, where we went in, updated payment information and successfully charged the card to resume services. However, once we initiated a few test conversations, we noticed the above.. any advice or suggestions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"location variable setting for the Google Cloud Translation API (Advanced)",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-12T22:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/location-variable-setting-for-the-Google-Cloud-Translation-API\/m-p\/543332#M1650",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":124,
        "Question_body":"Does anyone know which locations can be used to set the variable \"location\" in the following code snippet from a .js program designed to use Google Cloud Translation (Advanced) to translate asynchronously a .docx file stored in the subdirectory of a bucket into another supported language?\n\n\u00a0\n\n\/\/ Set your project ID, location and bucket name here\nconst projectId = ....\nconst location = ....\nconst bucketName\u00a0 = ....\n\nMy VM instance is located in \"europe-north1-a\", but is it possible to also use this as a valid region\/zone to set the \"const location\" variable to? I don't\u00a0 see anything about this in the documentation.",
        "Question_closed_time":"04-13-2023 02:57 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi\u00a0@legrandtimonier,\u00a0\n\nWelcome back to Google Cloud Support,\n\nThe area \"europe-north1-a\" is a valid location, You may able to generate resources like buckets and VM instances there, you may use it as the location value for the location variable in the code snippet you gave.\n\nThe location parameter of the Google Cloud Translation API indicates the region in which the Translation API service is hosted. The following locations are listed as being accessible for the API service in the official documentation:\n\nasia-east1\neurope-west2\nus-central1\n\nTo reduce latency and increase speed while using services, You may pick the area that is either closest to your users or where your resources are located.\n\nHere are some references that might help you:\nhttps:\/\/cloud.google.com\/compute\/docs\/regions-zones?_ga=2.183424358.-1392753435.1676655686\n\nhttps:\/\/cloud.google.com\/translate\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Cannot use isTranslateNativePdfOnly field in Cloud Translate API",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-17T14:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-use-isTranslateNativePdfOnly-field-in-Cloud-Translate-API\/m-p\/613078#M2386",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":43,
        "Question_body":"I'm using native pdfs, so I'd like to enable the\u00a0isTranslateNativePdfOnly field to raise my page limit to 300.\u00a0\n\n    document_input_config = {\n        \"content\": document_content,\n        \"mime_type\": mine_type,\n    }\n\n    response = client.translate_document(\n        request={\n            \"parent\": parent,\n            \"target_language_code\": \"en\",\n            \"document_input_config\": document_input_config,\n            \"isTranslateNativePdfOnly\": True,\n        }\n    )\n\nThis field doesn't seem to be recognized...\n\n(baseenv) ubuntu@ip-172-31-73-230:~\/revrec-dev$ python base_image\/services\/translation_service\/google\/doc_to_doc_translation.py ..\/docs\/test_larger.pdf\n2023-07-17 21:13:30 DEBUG    Checking None for explicit credentials as part of auth process...\n2023-07-17 21:13:30 DEBUG    Checking Cloud SDK credentials as part of auth process...\nTraceback (most recent call last):\n  File \"base_image\/services\/translation_service\/google\/doc_to_doc_translation.py\", line 78, in <module>\n    translate_document(file_path)\n  File \"base_image\/services\/translation_service\/google\/doc_to_doc_translation.py\", line 50, in translate_document\n    \"isTranslateNativePdfOnly\": True,\n  File \"\/home\/ubuntu\/anaconda3\/envs\/baseenv\/lib\/python3.6\/site-packages\/google\/cloud\/translate_v3\/services\/translation_service\/client.py\", line 974, in translate_document\n    request = translation_service.TranslateDocumentRequest(request)\n  File \"\/home\/ubuntu\/anaconda3\/envs\/baseenv\/lib\/python3.6\/site-packages\/proto\/message.py\", line 566, in __init__\n    \"Unknown field for {}: {}\".format(self.__class__.__name__, key)\nValueError: Unknown field for TranslateDocumentRequest: isTranslateNativePdfOnly\n\nAny advice? Same issue occurs on version 3.11.2 on Python 3.11.",
        "Question_closed_time":"07-21-2023 02:07 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@chromecast56,\n\nWelcome to Google Cloud Community!\n\nTo limit the page of online native pdf translation to 300 in Python, you need to set the attribute\n\n\"is_translate_native_pdf_only\": True\n\nand not the\u00a0\"isTranslateNativePdfOnly\", You are getting the error unknown field since the attribute\u00a0\"isTranslateNativePdfOnly\"\u00a0does not exist in the class\u00a0TranslateDocumentRequest(version 3.11.2), however the attribute that you are trying to refer to is \"is_translate_native_pdf_only\".\u00a0\n\nYou can check this link for more information:\u00a0\nhttps:\/\/cloud.google.com\/python\/docs\/reference\/translate\/latest\/google.cloud.translate_v3.types.Tran...\n\nHope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Google Speech-to-text for live audio",
        "Question_tag_count":1,
        "Question_created_time":"2022-01-10T04:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Speech-to-text-for-live-audio\/m-p\/182518#M162",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":205,
        "Question_body":"I was working with google speech to text for transcribing live audio. I was able to use the auto detect feature to detect user's language he\/she is speaking in. it worked perfectly when transcribing an audio file but i was not able to achieve the same result when doing the same with live audio. I followed every sample and documentation made available by google but still no luck.\nPlatform: Python 3.9\nHere is my snippet:\n\nconfig = speech.RecognitionConfig(\n    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n    sample_rate_hertz=RATE,\n    language_code=language_code,\n    model=\"command_and_search\",\n    alternative_language_codes=['mr-IN', 'en-IN']\n)\nstreaming_config = speech.StreamingRecognitionConfig(\n    config=config, interim_results=True\n)\n\nwith MicrophoneStream(RATE, CHUNK) as stream:\n    audio_generator = stream.generator()\n    requests = (\n        speech.StreamingRecognizeRequest(audio_content=content)\n        for content in audio_generator\n    )\n\n    responses = client.streaming_recognize(streaming_config, requests)\n\nAny help will be appreciated.\u00a0\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow ES: Not receiving response.query_result information in from streaming_detect_intent()",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-21T12:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-Not-receiving-response-query-result-information-in\/m-p\/614525#M2421",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":39,
        "Question_body":"Hi:\n\nI wrote a Python script to take audio from a microphone and process it using streaming_detect_intent(). I am using Dialogflow ES.\u00a0 Since I am writing a custom integration, I disabled the web hook (not that this should matter). I tested the agent with the \"try it now.\"\n\nThe problem is I don't get a query_result. The streaming seems to work. At some point recognition_result.is_final is True. However I am not getting query results. I do not see anything in the log.\u00a0 My code is a modified version of the Python example. Any suggestions would be appreciated!\n\n\u00a0\n\ndef sample_streaming_detect_intent(audio_queue, project_id, session_id, sample_rate):\n\n    from google.cloud import dialogflow\n\n    language_code = \"en\"\n    session_client = dialogflow.SessionsClient()\n    audio_encoding = dialogflow.AudioEncoding.AUDIO_ENCODING_LINEAR_16\n    sample_rate_hertz = sample_rate\n    session_path = session_client.session_path(project_id, session_id)\n    print(f\"Session path: {session_path}\")\n\n    def request_generator(audio_config):\n        query_input = dialogflow.QueryInput(audio_config=audio_config)\n\n        # The first request contains the configuration.\n        yield dialogflow.StreamingDetectIntentRequest(\n            session=session_path, query_input=query_input\n        )\n\n        while True:\n            chunk = audio_queue.get()\n            if not chunk:\n                break\n            # The later requests contains audio data.\n            yield dialogflow.StreamingDetectIntentRequest(input_audio=chunk)\n\n    audio_config = dialogflow.InputAudioConfig(\n        audio_encoding=audio_encoding,\n        language_code=language_code,\n        sample_rate_hertz=sample_rate_hertz,\n        single_utterance=False,\n    )\n\n    requests = request_generator(audio_config)\n    responses = session_client.streaming_detect_intent(requests=requests)\n\n    for response in responses:\n        print(\n            f\"{response.recognition_result.is_final} intermediate transcript: {response.recognition_result.transcript}\"\n        )\n        if response.recognition_result.is_final:\n            print(\"=\" * 20)\n            print(f\"Query text: {response.query_result.query_text}\")\n            print(f\"Detected intent: {response.query_result.intent.action} confidence: {response.query_result.intent_detection_confidence}\")\n            print(f\"Fulfillment text: {response.query_result.fulfillment_text}\")",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"EntityAnalysis, Version 2 model in natural language API",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-17T12:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/EntityAnalysis-Version-2-model-in-natural-language-API\/m-p\/490557#M806",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":267,
        "Question_body":"Hello,\n\ncould anyone share the python code on how to get natural language API to use version 2 for Entity\u00a0 Sentiment Analysis?\nThe Demo can be run for that, but it seems like in the docs this part is missing:\nhttps:\/\/cloud.google.com\/natural-language\/docs\/reference\/rest\/v1\/documents\/analyzeEntitySentiment\n\nHowever, for the classification, it is possible, as it is described here:\u00a0\nhttps:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Version-2-model-in-natural-language-API\/m-p\/484641\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud translations with glossary drops words after glossary item.",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-12T05:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-translations-with-glossary-drops-words-after-glossary-item\/m-p\/453337#M501",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":358,
        "Question_body":"We use the cloud translation API, supplemented with a glossary, to translate English to Dutch. On some occasions the glossary translation drops part of the translation if it is a two part ducht construct, where part 2 follows immediately after the glossary item.\n\nFor example \"add\" in English becomes \"voeg toe\" in Dutch.\u00a0 In a sentence with a glossary item for \"tomato passata -> \"tomaten passata\" this becomes:\n\nEnglish:\u00a0Then add the tomato passata and simmer for 10 minutes.\n\nDutch with glossary:\u00a0Voeg vervolgens de tomaten passata en laat 10 minuten sudderen.\n\nDutch without glossary: Voeg vervolgens de tomatenpassata toe en laat 10 minuten sudderen.\n\nIn this case the \"toe\" is required but somehow missing in the glossary translation. Any idea what goes wrong here?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Features",
        "Question_tag_count":3,
        "Question_created_time":"2022-05-12T12:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Features\/m-p\/422551#M326",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"Hi\n\nAssume that I create a model using AutoML with 50 features from the Vertex AI Feature Store and after training I found that from the 50 original features, 10 has a very low incidence over the model.\n\nLooking to increase the accuracy, reduce the consumption of resources and increase the speed of the model:\n\nDo I need to remove the 10 features from the Feature Store and deploy the model to the endpoint?\n\nShould I retrain the model with the 40 features and deploy it to the end point?\n\nAny comments more than appreciated",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Being charge for Translate API usage",
        "Question_tag_count":2,
        "Question_created_time":"2023-01-02T05:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Being-charge-for-Translate-API-usage\/m-p\/505295#M1011",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":98,
        "Question_body":"I have some doubts about translate API usage.\n\nI thought it would be unlimited according to this image:\n\nBut they have charged me 180 reais in one day because of translating texts using the API.\n\nI have two questions: How can i check how much money will i pay for X characters on each translation?\n\nEven if i was about to be charged, wheres the 300$ i had for testing it? I didnt know i was going to be charged.\n\nCan anyone helps me, please?\n\nThanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"OAuth Rejection Notice",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-11T12:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/OAuth-Rejection-Notice\/m-p\/542724#M1634",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"After getting back an error message on running my javascript program in node.js with my VM instance, I was informed by a member of the Google Cloud Community:\u00a0\"If you need to set up your instance with Identity-Aware Proxy (IAP), you may refer to this document Setting up IAP for Compute Engine. But if you already set up your instance, you just need to enable IAP Enabling IAP for Compute Engine.\" Given that my VM instance had already been set up, I jumped (at the suggestion of the person advising me) to the part of the document dealing with \"Enabling IAP for Compute Engine\" without carrying out the other steps for persons who did not already have an up and running VM instance (i.e., load balancing, etc.) While following the steps listed in the documentation, I discovered I must activate the \"Configure consent screen\" button and create an OAuth consent screen. None of the things I saw after this point seemed to have anything to with my sort of project, that involves no one but myself to give consent to.\u00a0 But I decided this time not to \"think too hard\" and to simply follow the instructions. After several long, puzzling minutes trying to do what I thought I was supposed to do, I submitted my OAuth request. And after 48 hours or so of waiting for an answer from the Trust & Safety Security & Privacy Team, I just received a Request Denied from them, with no explanation of what was wrong with my submission (though I'm sure several things were), along with a warning that I might need to submit a video. But of what? Perhaps it's worth mentioning at this point that what I wish to do is \"simply\" use Google Cloud Translation (Advanced) with a glossary in .tmx format to translate a .docx file from one Google-supported human language to another Google-supported human language, outputting the result also in .docx format and storing both in different subdirectories of my bucket. (One directory for the .docx input file and another for the .docx output file, the result of the translation.) My main question is this: is it even necessary to create an OAuth consent screen in such a scenario,\u00a0 with only person (i.e. me) accessing my VM via the GC console? And if a consent screen turns out to be a necessity, can someone please suggest what I might do to get things right this time and not receive a second Request Denied from the members of the Trust & Safety Security & Privacy Team?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Voice\/language options during conversion of long text files to speech",
        "Question_tag_count":1,
        "Question_created_time":"2021-08-02T19:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Voice-language-options-during-conversion-of-long-text-files-to\/m-p\/165947#M34",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":401,
        "Question_body":"The voice\/language options during conversion of long text files to speech. Can anyone help with the doc\/sample for the same.",
        "Question_closed_time":"08-03-2021 03:53 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How can I avoid being charged for Tensorboard?",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-21T12:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-avoid-being-charged-for-Tensorboard\/m-p\/180658#M121",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":592,
        "Question_body":"Today I received an email from GCP saying that my account will be charged for using Vertex AI Tensorboard from February. It is quite expensive and I want to stop using the service and avoid being charged.\n\nHow can I do that? There is no option for Tensorboard in the API dashboard (just one for Vertex AI generally). I only have \"basic support\" so I cannot contact technical support, and I am not the billing administrator so I cannot contact billing support. Is there any way I can disable Tensorboard?\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cannot open jupyter notebook in vertexAI",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-07T08:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-open-jupyter-notebook-in-vertexAI\/m-p\/610067#M2319",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":110,
        "Question_body":"I recently started to use GCP with the free trial. The interface and the functionalities are messed up. Often the texts collide with each other, and it also doesn't open Jupyter Notebook in Vertex AI. Even the customer support page looks like in the picture below. How can I fix that? Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX environment specific webhook timeout is not considered",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-07T01:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-environment-specific-webhook-timeout-is-not\/m-p\/529553#M1383",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":238,
        "Question_body":"Hello,\n\nI am running into an issue with timeout value for environment specific webhooks in Dialogflow CX.\n\nWe have extended the timeout value to 15 seconds for the webhook and we have multiple environment specific webhook URLs set as well.\u00a0 You can see precise setup as part of this issue posted by another user facing the exact same issue 3 months back :\u00a0https:\/\/issuetracker.google.com\/issues\/261683010\n\nIt seems like the webhook timeout value is not being considered for environment specific URLs.\u00a0 I see it does work for the agent level URL which is not what I would like to use as we have for obvious reasons different URL for non-prod and production apps.\n\nIs there any workaround or possible solution to this problem?\n\nCheers, jags",
        "Question_closed_time":"03-09-2023 07:32 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Found a suitable workaround:\n\nUpdate the default webhook URL to production with timeout 15 seconds.\u00a0 Remove the environment specific webhook URL.\nRelease a version of the flow and use it with the production environment.\nUpdate the default webhook URL back to test endpoint with timeout 15 seconds.\nRepeat the steps before releasing the new version for production usage.\n\nWhen you release the version of a flow, Dialogflow CX would create the snapshot of webhook configuration as well along with flow.\u00a0 And when you update the default webhook URL again, it would only update for unpublished\/draft version and it does not affect the webhook URL in the released version of the flow.\u00a0 It's verified and working approach!\n\nCheers, jags\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Will Google provide MTQP in Cloud Translation API?",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-25T12:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Will-Google-provide-MTQP-in-Cloud-Translation-API\/m-p\/482115#M682",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":91,
        "Question_body":"Hi,\u00a0\n\nI discovered with interest that your Google Translation Hub advanced tier offers document post-editing features, and, as part of that, includes an MTQP quality prediction score on a segment by segment basis.\u00a0\n\nThis would be a very interesting feature to include in Cloud Translation API, particularly for TMS and CAT tools like Trados\/MemoQ\/Memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional Translation Memory technology.\n\nIt could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.\n\nSo my question is whether Google is looking at making this available in the API, or whether Google is treating that as proprietary information that you guys do not want to make available outside of your Google Translation Hub?\u00a0 I really hope the answer is the former, not the latter...\n\nThank you.\n\nMichel Farhi\nPrincipal Localization Engineer\nNI (formerly National Instruments)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Normalised Text from TTS",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-13T01:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Normalised-Text-from-TTS\/m-p\/441871#M413",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":92,
        "Question_body":"Is there a way for the Google TTS service to return the normalised text along with the generated speech file? For example, if \"This is 1993\" is sent to the service, can it return the verbalised for \"This is nineteen ninety three\"?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Production readiness POC: Our Client wants to use GCP and want to make sure about few features.",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-09T04:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Production-readiness-POC-Our-Client-wants-to-use-GCP-and-want-to\/m-p\/508106#M1032",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":80,
        "Question_body":"There is a requirement of our client in GCP\u00a0that Dialog flow CX agent should support the chat history maintenance and also if it provides chatroom option to maintain different chats as per user id in webchat integration option. We have studied few features on Azure Chatbot those are supporting these requirements and client needs to see if GCP dialog flow can be customize like this or not. We are planning to do some POC for production readiness with GCP, it will be good if we can get answers to these questions ASAP",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to use trained vision model in App Inventor",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-05T04:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-trained-vision-model-in-App-Inventor\/m-p\/177222#M108",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":222,
        "Question_body":"I have trained a cloud image model with a set of images, I want to use this model to make a mobile app through MIT App Inventor 2. Please suggest how to do this.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"cloud retail: real time user events are not appear on retail data page",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-11T07:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/cloud-retail-real-time-user-events-are-not-appear-on-retail-data\/m-p\/542590#M1630",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":158,
        "Question_body":"hi,\u00a0\n\nWe tried many things, but somehow we could not transfer the events to the retail. I followed these steps:\n\n1: Get an API key from\u00a0APIs & Services >\u00a0Credentials > Create\u00a0Credentials > API Key\n\n2: Get project id: There are\u00a0Project number (like 111112222333) and\u00a0Project ID\u00a0(like \"kite-first-11111\").\u00a0I don't know which one to use. But i tried both.\n\nScreenshots of tag manager sections are below. Also we create a trigger. And we use ga4\u00a0datalayer without any problem on site side like this:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Moving (or copying) doc processors under a single project",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-16T07:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Moving-or-copying-doc-processors-under-a-single-project\/m-p\/553866#M1925",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":102,
        "Question_body":"Hello, we started exploring Doc AI by trying multiple processors under several projects. We are now consolidating them into dev, test and production projects, each with a version of the processors.\n\nOur goal is to first train the processors in the test project and then copy\/move them into the production project.\n\nHow can we copy over processors from one project to another? We are unable to find any documentation that would help us accomplish this.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deployed model does not support explanation.",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-24T08:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deployed-model-does-not-support-explanation\/m-p\/525932#M1337",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":322,
        "Question_body":"I have deployed my model and tried to test it, but I received the error message:\n\n\"Deployed model XXX does not support explanation.\"\u00a0\n\nCan anyone help? Thanks very much",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Multiple Neural2 Speakers with text-to-speech error.",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-16T10:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Multiple-Neural2-Speakers-with-text-to-speech-error\/m-p\/533408#M1432",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":179,
        "Question_body":"Hello,\n\nI'm trying to use the text-to-speech API to generate a multi speaker audio file. When I use the older wavenet voices it works just fine. But when I replace the speakers with the newer Neural2 models I get a 400 error saying:\u00a0\n\nInvalidArgument: 400 Request contains an invalid argument.\n\nHow can I get this to work for multiple speakers using the newer models?\n\nHere is a sample:\n<speak>\n<voice name=\"en-US-Neural2-J\">\n<p>Hello, everyone! Welcome to today's podcast. I'm your host A, and joining me is my co-host, B.<\/p>\n<\/voice>\n<voice name=\"en-US-Neural2-I\">\n<p>Hi, A! It's great to be here. Today, we're going to discuss an interesting topic that's been making headlines recently.<\/p>\n<\/voice>\n<voice name=\"en-US-Neural2-J\">\n<p>That's right, B. We're talking about the collapse of Silicon Valley Bank, which was triggered by a massive online bank run.<\/p>\n<\/voice>\n<voice name=\"en-US-Neural2-I\">\n<p>Indeed, A. This bank run was unlike any other we've seen before, as it was primarily fueled by social media platforms and private chat groups.<\/p>\n<\/voice>\n<\/speak>\n\n\u00a0\n\nThis sample works when I replace\u00a0en-US-Neural2 speakers with\u00a0en-US-Wavenet.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Output filename for Translate Document API",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-26T08:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Output-filename-for-Translate-Document-API\/m-p\/460318#M541",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":81,
        "Question_body":"Hello,\n\nI have been developing an iOS mobile app to translate PDF documents and currently using Google Translate documents API without any issue.\n\nIs there any way, we can update or change the output filename coming from the API? Currently, the output file name includes all the attributes starting from the project name, bucket name and finally the file name. This won't be appropriate for the users to showcase these things.\n\nIs there any configuration to change this filename to more user friendly format?\n\ne.g. Input file name -\u00a0Test_Translation_En_Fr_Sp.pdf\n\nOutput filename from Translate API -scantranslatorapp.appspot.com_uploaded_documents_FuOrb0L4tudAZvhZ99IwsFYg83M2_Test_Translation_En_Fr_Sp_fr_translations.pdf\n\nIt would be appropriate to showcase only -\u00a0Test_Translation_En_Fr_Sp_fr_translations.pdf",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GOOGLE VISION DETECT PAGE NUMBER PDF",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-06T01:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GOOGLE-VISION-DETECT-PAGE-NUMBER-PDF\/m-p\/518515#M1212",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":154,
        "Question_body":"Hi All\n\nThe filename will be output-x-to-y, where x and y represent the PDF\/TIFF page numbers included in that output file.\nHow to get page numbers or detect it?\u00a0I don't know how to recognize the number of pages in a PDF when it is used config in\u00a0Output Config.\u00a0\n\nI've tried but I can't search it.\n\nKind Regards\nTuong",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vision API on video files?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-16T04:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-on-video-files\/m-p\/603801#M2173",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":37,
        "Question_body":"I'm primarily working with video-intelligence on a dataset of videos for some academic research. I'm a complete novice, but I've started to get the hang of things, and I'm getting some really useful outputs from speech to text, text recognition, and labelling.\u00a0\n\nHowever, in the vision API, there is a 'detect image properties' function that looks really promising. If I could detect things like dominant colour for shots within my videos, and then get an an average output for each video as a whole, this could really help with my analysis.\n\nThe problem is, I don't know how much flexibility there is in this kind of thing, and I don't have enough fluency with this kind of work to even know how to ask (I only just about understand a for loop lol).\u00a0\n\nMy current thinking is run the analysis on each shot of a video, attribute weights to outputs, and then take a weighted average of some sort? This is only pie in the sky thinking though. Any thoughts welcome, I'm just exploring right now.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"empty train model (Train) of machine translation GOOGLE ADVANCED",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-28T07:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/empty-train-model-Train-of-machine-translation-GOOGLE-ADVANCED\/m-p\/548351#M1755",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hello community,\u00a0\n\nSince the migration from Google AutoML to Google Advanced I can upload a dataset like before but then the \"Train\" button does not respond: there seems there is nothing to be trained.\u00a0\n\nHas anyone faced this problem with Google Advanced when training machine translation models? I leave a screenshot below:\u00a0\n\nThanks in advance and kind regards,\u00a0\n\nClara",
        "Question_closed_time":"04-28-2023 06:58 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@claraginovart,\n\nWelcome to Google Cloud Community!\n\nThere are several reasons why you are encountering this issue, please note that in order to access your upgraded resources you will be using the Cloud Translation Advanced API not the AutoML API. Listed below are some possible solutions to the problem:\u00a0\n\n1. If you are using an API to import data into datasets unlike in legacy (AutoML) where you are using CSV file to specify the location of the source file in CGS, in native (Cloud Translation) you need to specify the locations of TSV and TMX files in CGS. You can learn more about the difference of legacy and native resources using this link:\nhttps:\/\/cloud.google.com\/translate\/docs\/advanced\/automl-upgrade#differences_between_legacy_and_nativ...\n\n2. You need to update your code to use the Cloud Translation API and call the native resource IDs not the legacy resource IDs, although legacy and native are identical they don't have the same resource IDs, same goes for newly created resources and translation predictions where you must use Cloud Translation API rather than\u00a0AutoML API. You can check these documentations to learn more:\n\u00a0https:\/\/cloud.google.com\/translate\/docs\/advanced\/automl-upgrade#cloud-translation-api\nhttps:\/\/cloud.google.com\/translate\/docs\/advanced\/translating-text-v3\nhttps:\/\/cloud.google.com\/translate\/docs\/advanced\/automl-models\n\n\n3. You need to attach the necessary roles in order for the service account to work correctly. To learn more about the necessary roles, you can check this link.\u00a0https:\/\/cloud.google.com\/translate\/docs\/intro-to-v3#iam\nIf you are also using the features of GCS or AutoML models, you need to include the necessary roles to the service account.\u00a0\n\n4. Verify your dataset to ensure it is in the proper format and has sufficient data to train the model. To see if the training procedure begins, you may also try uploading a smaller dataset.\n\nHope it helps!\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"RegEX to identify a custom name in the middle of sentence in Dialogflow ES.",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-12T07:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/RegEX-to-identify-a-custom-name-in-the-middle-of-sentence-in\/m-p\/611595#M2345",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":51,
        "Question_body":"I'm trying to identify the project name from the given input using a regular expression in Dialogflow ES. Name can be in anywhere in the message. It only consists alphanumeric and hyphen(-) and It should not exceed 255 characters.\n\nI tested adding several regex combinations. But everything was failed to identify the project name correctly. Please help me to find the suitable regex pattern for my use case.\n\nAdditionally, I want to recognize the project name when it is given as a separate single input following the regex ^[a-zA-Z0-9\\-]{1,255}$ . But this regex does not supported in Dialogflow. Can you suggest a alternative regex which can be used in Dialogflow ES.\u00a0\n\n\u00a0 Thank you.",
        "Question_closed_time":"07-20-2023 08:46 AM",
        "Answer_score_count":2.0,
        "Answer_body":"Good day\u00a0@piyumi-uththara,\n\nWelcome to Google Cloud Community!\n\nThis issue is happening since the expression is too broad, based on the project name, you are trying to capture Xyxp-7, Assuming that this is the naming convention you are trying to follow letters-digits try using this regex instead and see if it works:\n\n\u00a0\n\n\\w{1,127}\\-\\d{1,128}\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"ML Pipeline parameter_values does not work well.",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-13T22:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ML-Pipeline-parameter-values-does-not-work-well\/m-p\/612195#M2367",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":108,
        "Question_body":"I am currently running an ML pipeline on Vertex AI and it is not working as expected, so I am asking this question.\n\n\nI have the following pipeline defined right now To explain, when I run the pipeline, I want to use timestamp via paramter_values (e.g. 20230714100000) when executing the pipeline. I want the custom training job to output the models in the timestamp directory, and I want the importer to import the models in that directory. However, when I run it, GCS creates a directory named bucket_name\/{{channel:task=;name=timestamp;type=String;}}. How can this be avoided?\n\n\n\n```\nimport google.cloud.aiplatform as aip\nimport kfp\n\n\n@kfp.dsl.pipeline(name=\"xxx\")\ndef pipeline(\n\u00a0 \u00a0 timestamp: str = \"notimestamp\"\n\u00a0 \u00a0 from google_cloud_pipeline_components.types import artifact_types\n\u00a0 \u00a0 from google_cloud_pipeline_components.v1.custom_job import \\\n\u00a0 \u00a0 \u00a0 \u00a0 CustomTrainingJobOp\n\u00a0 \u00a0 from kfp.components import importer_node\n\n\n\u00a0 \u00a0 custom_job_task = CustomTrainingJobOp(\n\u00a0 \u00a0 \u00a0 \u00a0 project=\"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 display_name=f\"xxx-{timestamp}\",\n\u00a0 \u00a0 \u00a0 \u00a0 worker_pool_specs=[\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"containerSpec\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"env\": [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"name\": \"BUCKET_NAME\", \"value\": f\"bucket_name\/{timestamp}\"},\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"imageUri\": \"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"replicaCount\": \"1\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machineSpec\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machineType\": \"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_type\": xxx,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_count\": 1,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 \u00a0 \u00a0 ],\n\u00a0 \u00a0 )\n\n\n\u00a0 \u00a0 importer_node.importer(\n\u00a0 \u00a0 \u00a0 \u00a0 artifact_uri=f\"bucket_name\/{timestamp}\",\n\u00a0 \u00a0 \u00a0 \u00a0 artifact_class=artifact_types.UnmanagedContainerModel,\n\u00a0 \u00a0 \u00a0 \u00a0 metadata={\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"containerSpec\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"imageUri\": \"xxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 ).after(custom_job_task)\n\n\n```",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX - Voice bot takes too long to response",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-28T06:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-Voice-bot-takes-too-long-to-response\/m-p\/548348#M1754",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":129,
        "Question_body":"I have a voice Bot integrated with the Cisco Webex contact center. This BOT is used for collecting customers' names when they call in. But after collecting the name, the BOT is taking almost 15 seconds of silence before moving to the next prompt. Is there any setting to reduce the speech timeout option so that immediately after the agent enters a full name, the BOT will go to the next option?\n\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Deleting a failed dataset",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-28T07:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deleting-a-failed-dataset\/m-p\/527077#M1360",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":120,
        "Question_body":"Under the Vertex AI - a dataset failed to create due to a constraint applied to the organization. It does not allow for the deletion of the dataset, I attempted using python (Delete a dataset \u00a0|\u00a0 Vertex AI \u00a0|\u00a0 Google Cloud) and the response was -\u00a0\"...is in failure state and cannot be deleted. It will be deleted automatically after a few days.\"\u00a0\u00a0but it didn't delete. There is not a gcloud command to correct. Short of a support request..how can the dataset be removed as I foresee this occuring as others attempt experiments. I have addressed the issue with the constraint.",
        "Question_closed_time":"03-01-2023 02:35 PM",
        "Answer_score_count":1.0,
        "Answer_body":"I tried running the same\u00a0code you used and I was able to delete a dataset that was successfully created. I suspect in your case, the failure state of the dataset is the problem. Also, there is indeed no gcloud command to manually delete it. I would still suggest you file a\u00a0ticket here so\u00a0Google Cloud's engineering team can further investigate.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Speech to Text using Microphone",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-21T22:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-using-Microphone\/m-p\/176201#M88",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":137,
        "Question_body":"Hi there,\u00a0\n\nWas trying to use convert speech to text using a microphone, getting a pop-up security error saying \"failed to construct 'worker': script at ..........\"\n\nAny idea why?\n\nAlso, how can I use the speech to text service with microphone input, I tested uploading a video file and worked perfectly, but no idea how to use the service with microphone input?\n\nPlease let me know\u00a0\n\nAJ",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML forecasting: understanding the rolling forecast window during model evaluation",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-31T13:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-forecasting-understanding-the-rolling-forecast-window\/m-p\/516170#M1158",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":180,
        "Question_body":"I trained a time series forecasting model with AutoML. During training, I checked the option to \"Export test dataset to BigQuery.\" I have a question about how to understand the data that appears in the exported table.\n\nMy understanding is that a \"predicted_on\" timestamp is essentially the first date of the forecast horizon of a rolling forecast window. I see that for each \"predicted_on\" timestamp, there are 6 timestamps from the test data split of my training data. This suggests that the forecast horizon is 6 weeks long; i.e., for each \"predicted_on\" date (and starting on that date) it predicts 6 weeks of data.\n\nMy question is, where does the number 6 come from? (When I trained the model, I specified that the forecast horizon is 26 weeks, not 6...)",
        "Question_closed_time":"02-24-2023 08:22 AM",
        "Answer_score_count":0.0,
        "Answer_body":"To answer my question:\n\nI'm not sure where the number 6 came from, but I've since discovered the following.\n\nThe timestamp format I was using is not among the timestamp formats supported by Google according to this documentation. I changed the format of my timestamps. I also ensured that every number in my target column has a decimal (it was previously a mix of integers and decimal numbers).\n\nAfter making these changes, I trained a new model and examined the data exported to BigQuery.\n\nNow I see that there are 26 weeks of timestamps from the test data split associated with the first\u00a0`predicted_on` timestamp. This would suggest that the forecast horizon of the rolling forecast window is 26 weeks long. This is what I would expect, given that I set the forecast horizon to 26 when I trained the model.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"What you think about CHATGPT",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-05T07:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-you-think-about-CHATGPT\/m-p\/506958#M1023",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":130,
        "Question_body":"What do you all think about \"ChatGPT\" by open AI? Does it take the jobs of developers or it's just a kind of personal assistant who will make your job easier",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Emotional mobiles",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-26T23:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Emotional-mobiles\/m-p\/471342#M605",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":38,
        "Question_body":"My idea is to create emotional mobiles. Were we cannot buy mobiles with only money,mobile must choose us for buy and unique emotional between specific person and his new mobile . An intimacy between mobile and human. Like a puppy or understanding couples mobile and human sinking using AI.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"SSML mark timepointing (v1beta1) suddenly only returns the timepoints until first period",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-06T11:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/SSML-mark-timepointing-v1beta1-suddenly-only-returns-the\/m-p\/529358#M1381",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":141,
        "Question_body":"I'm loving GCP's Text to Speech API! I have a live product (used by thousands of users every day) that relies on the TTS API v1beta1 (Method: text.synthesize \u00a0|\u00a0 Cloud Text-to-Speech Documentation \u00a0|\u00a0 Google Cloud).\u00a0 I've gotten a lot of bug reports over the past few days and traced it to a change in behavior in the `text.synthesize` method (v1beta1), when `enableTimePointing: [\"SSML_MARK\"]`.\n\nBefore: `text.synthesize` with\u00a0`enableTimePointing: [\"SSML_MARK\"]`\u00a0would return an object in `timepoints` with a `markName=i` and `timeSeconds` for each `<mark name = i>` in the input SSML.\n\nNow:\u00a0`text.synthesize` with\u00a0`enableTimePointing: [\"SSML_MARK\"]`'s response\u00a0`timepoints` object only contains timepoints for a\u00a0fraction of\u00a0\u00a0all the\u00a0`<mark name = i>` in the input SSML.\n\n\u00a0\n\nFor example:\n\n1) Use the API explorer at\u00a0Method: text.synthesize \u00a0|\u00a0 Cloud Text-to-Speech Documentation \u00a0|\u00a0 Google Cloud\n\n2) Set the request body to:\n\n{\n\"enableTimePointing\": [\n\"SSML_MARK\"\n],\n\"input\": {\n\"ssml\": \"<speak><prosody><mark name=\\\"0\\\"\/>I <mark name=\\\"1\\\"\/>am <mark name=\\\"2\\\"\/>my <mark name=\\\"3\\\"\/>aunt's <mark name=\\\"4\\\"\/>sister's <mark name=\\\"5\\\"\/>daughter. <mark name=\\\"6\\\"\/>He <mark name=\\\"7\\\"\/>was <mark name=\\\"8\\\"\/>sure <mark name=\\\"9\\\"\/>the <mark name=\\\"10\\\"\/>Devil <mark name=\\\"11\\\"\/>created <mark name=\\\"12\\\"\/>red <mark name=\\\"13\\\"\/>sparkly <mark name=\\\"14\\\"\/>glitter.<\/prosody><\/speak>\"\n},\n\"voice\": {\n\"name\": \"en-US-Standard-A\",\n\"languageCode\": \"en-US\"\n},\n\"audioConfig\": {\n\"audioEncoding\": \"MP3\"\n}\n}\n\n3) See how the `timepoints` object in the response only has timepoints for mark names \"0\" through \"5\". The actual number of mark tags in the input SSML is fifteen (largest mark tag being \"14\").\u00a0\n\n\u00a0\n\n4. In the input SSML, remove the period at the end of \"<mark name=\\\"5\\\"\/>daughter.\", execute, and notice how the `timepoints` object in the response now has timepoints for all the mark names \"0\" through \"14\".\n\nCan someone confirm that this is unexpected behavior? And if it's being worked on, roughly when can we expect a fix? Thank you!",
        "Question_closed_time":"03-07-2023 10:49 AM",
        "Answer_score_count":1.0,
        "Answer_body":"I used your request body and it was able to get to 14. Are you still getting \"fractions\" of timepoints now?\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"terraform_vertex_ai_notebooks_instance",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-05T07:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/terraform-vertex-ai-notebooks-instance\/m-p\/609400#M2299",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":57,
        "Question_body":"How to set static IP for vertex_ai_notebooks_instance?\n\nTerraform Resources\nhttps:\/\/github.com\/terraform-google-modules\/terraform-docs-samples\/tree\/main\/vertex_ai\/user_managed_...\n\nI checked all documentation about how to create notebooks_instance via Terraform. But I came across the fact that Vertex Ai Notebooks have Dynamic IPs that complicate the process of distributing access over IP. As a workaround, you can manually create a static IP and bind it in the Notebook VM itself. But I need to be able to do this through Terraform. Since the implementation works in a manual format so it's possible via terraform format. Maybe you have a work sample to set static IP?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Point pip on a artifact registry",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-22T08:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Point-pip-on-a-artifact-registry\/m-p\/605726#M2210",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":120,
        "Question_body":"Hello to evryone. I'm new to Vertex AI and I'm learning step by step.\u00a0\n\nI'm wondering, it is possible to use PIP on Vertex AI workbench but instead of pointing at PyPI by default, pointing to a python artifact registry containing pre-installed python packages? This is due to the fact that for project constraints, it is not possible to direcly reach outside internet.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Translation API Request - Detailed info",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-17T09:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translation-API-Request-Detailed-info\/m-p\/510906#M1074",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"Hi all\n\n\u00bfCan we get the detailed information in monitoring, about all API requests? for instance users id which has make the request, volume (in MB) of the requests, grouped by user id. \u00bfIs it possible to get for Translation API?\n\nThanks in advance for your guidance",
        "Question_closed_time":"01-17-2023 10:05 AM",
        "Answer_score_count":1.0,
        "Answer_body":"The first thing I would suggest we look at is the audit logging from the Cloud Translation service that is documented here:\n\nhttps:\/\/cloud.google.com\/translate\/docs\/audit-logging\n\nWhen Google cloud API services (such as translation) are executed, they can generate audit logs.\u00a0 Some are enabled by default but others have to be explicitly turned on.\u00a0 I would say that you are likely going to want to look at the \"Data Access audit logs\" which are off by default.\u00a0 I'd suggest switching those on and running some of your tests and examine the resulting audit records found in Cloud Logging.\u00a0 Let's see if those contain the records\/data you are looking for.\u00a0 Realize that switching on extra auditing records results in more consumption of Cloud Logging so make sure that you monitor your costs if you exceed the free allocation of Cloud Logging records.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Issue Link between Vertex AI and Appsheet",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-19T20:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-Link-between-Vertex-AI-and-Appsheet\/m-p\/534403#M1442",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":151,
        "Question_body":"Hello,\n\nI am working on quantity counting by combining appsheet and vertex AI.\nI have a json file link at GCP, deploy model of vertex AI, but when I go to the appsheet, it can't count the number and can't be identified.\n\n\u00a0\n\nWhere am I doing wrong. Please help me.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI Online Succeeds but Batch Fails with mysterious Code 3",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-12T12:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Online-Succeeds-but-Batch-Fails-with-mysterious-Code\/m-p\/602353#M2136",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":23,
        "Question_body":"I am testing out prediction requests to the Google Invoice Processor with the Node.js SDK.\u00a0 When I make prediction requests as an \"online request\", the process succeeds and I get back a result object to parse.\u00a0 However, when I submit the exact same document to the same processor as a \"batch\", the long-running operation fails after approx. 20 seconds with a very generic message:\n\nGoogleError: Failed to process all documents.\n    at Operation._unpackResponse ([path]\/google-gax\/src\/longRunningCalls\/longrunning.ts:220:23)\n    at [path]\/google-gax\/src\/longRunningCalls\/longrunning.ts:195:14 {\n  code: 3\n\nI have tested with multiple different sample documents, all with the same result: online prediction works but batch prediction fails with this error.\n\n  const [operation] = await client.batchProcessDocuments({\n    name: FQ_INV_PROCESSOR_NAME,\n    inputDocuments: {\n      gcsDocuments: {\n        documents: DOCS_FROM_GCS,\n      },\n    },\n    documentOutputConfig: {\n      gcsOutputConfig: {\n        gcsUri: \"gs:\/\/<my-bucket>\/<prefix>\/\",\n      },\n    },\n  });\n\n  try {\n    const res = await operation.promise();\n    console.log(\"Success result\", res); \/\/ Never reached\n  } catch (err) {\n    console.error(\"Error class:\", err); \/\/ Prints out the Code 3 \"Failed to process all documents\" error.\n  }\n\n\u00a0I've also investigated different API endpoints to query the long-running operation (LRO) details to see if they include any additional debuggable information, but all the ones I tried gave me the same information; nothing additional.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to deploy model to endpoint",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-17T01:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-deploy-model-to-endpoint\/m-p\/554155#M1939",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":232,
        "Question_body":"Hi,\n\nI am trying to deploy a simple model on an endpoint in order to start making predictions.\n\nI followed these steps:\n\n1. Create the model, create a docker image, push it into Artifact Registry\n\n2. Upload the model to Vertex AI:\n\n\u00a0\n\n! gcloud ai models upload --container-image-uri=<REGION>-docker.pkg.dev\/<PROJECT>\/<REPOS>\/<MODEL>:latest --region=<REGION> --display-name=my-model\n\n\u00a0\n\n3. Create an endpoint on Vertex AI:\n\n\u00a0\n\n! gcloud ai endpoints create --display-name=my-model-endpoint --region=<REGION>\n\n\u00a0\n\n4. Deploy the model to the endpoint:\n\n\u00a0\n\n! gcloud ai endpoints deploy-model <ENDPOINT_ID> --model <MODEL_ID> -display-name my-model --traffic-split=0=100 --region <REGION> --machine-type=n1-standard-8 --enable-access-logging \n\n\u00a0\n\nHere's the message:\n\nModel server terminated: model server container terminated: exit_code: 0 reason: \"Completed\" started_at { seconds: 1684309447 } finished_at { seconds: 1684309449 } . Model server logs can be found at xxxxx\n\nWhen i check the logs:\u00a0\n\nAs you can see i have no errors.\n\nWhat i tried:\n\n- upsized the machine-type\u00a0\n\n- deployed with a python script\u00a0\n\n- deployed manually using the portal\u00a0\u00a0\n\n- changed my model to a simple linear regression to test",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Regex Expression Not Working in Dialogflow ES",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-10T11:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Regex-Expression-Not-Working-in-Dialogflow-ES\/m-p\/601872#M2123",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":25,
        "Question_body":"I'm using Dialogflow to build a bot that will capture a book title.\n\nFo starters I wrote a regex that will capture if someone writes \"my title is harry potter\" or \"the title is Hamlet\".\n\nThe expected output is harry potter, or, Hamlet\n\nThe regex entity that I've written is:\u00a0^(?:([a-zA-z0-9 ]*\\s{0,3}title\\s{0,3}is\\s{0,3})+)([a-zA-Z0-9 -)(@&'$#]*)?$\n\nIt's not getting recognized in the intent at all. Just blank.\n\nPlease help. It's urgent.\n\nRegards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Missing `us-central2-b` when creating TPU VM",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-06T11:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Missing-us-central2-b-when-creating-TPU-VM\/m-p\/550953#M1811",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":110,
        "Question_body":"Based on this excerpt:\n\nNote:\u00a0Cloud TPU v4 capacity is located in us-central2 region. Currently, v4 is only available in zone\u00a0us-central2-b. See\u00a0Types and Topologies\u00a0for information about supported v4 TPU types and topologies.\n\nfrom https:\/\/cloud.google.com\/tpu\/docs\/regions-zones\n\n\u00a0\n\nI'm expecting to find the region however it's not present in the dropdown. CLI is also saying it's either forbidden or mistyped.",
        "Question_closed_time":"05-09-2023 07:25 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@lukas0,\n\nWelcome to Google Cloud Community!\n\nAs of now, you need to reach out to Google Cloud Support in order to enable this feature in your Project. You can check the key note in this documentation:\u00a0https:\/\/cloud.google.com\/tpu\/docs\/system-architecture-tpu-vm#tpu-v4-config\n\nYou can use this link to reach out to Google Cloud Support:\u00a0https:\/\/cloud.google.com\/support\n\nHope this will help!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Dialogflow CX bot does not display the fulfilment text when called by api",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-24T10:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-bot-does-not-display-the-fulfilment-text-when\/m-p\/525966#M1339",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":132,
        "Question_body":"Hi guys, I'm struggling with a problem related to calling the Dialogflow API. I'm using an API to call a specific page using the following payload:\n\n\u00a0\n\n\u00a0\n\n{\n\tsession: \"projects\/xxxx\/locations\/us-east1\/agents\/xxxx\/session\/xxx\",\n\tqueryParams: {\n\t  currentPage: \"projects\/xxxx\/locations\/us-east1\/agents\/xxxx\/flows\/xxxx\/pages\/xxxx\",\n\t  parameters: {\n\t\tfields: {},\n\t  },\n\t},\n\tqueryInput: {\n\t  text: {\n\t\ttext: \"\",\n\t  },\n\t  \"en\",\n\t},\n}\n\n\u00a0\n\nand to call it, I use:\n\n\u00a0\n\nconst assistant = new dialogflow.SessionsClient(config);\nconst [response] = await assistant.detectIntent(request);\n\n\u00a0\n\nI'm trying to call this specific page:\n\nBut I don't get any response. No text is sent back.\n\nNow, if I use a parameter, I'll get the response:\n\nBut here's the thing, I won't use parameters everywhere. I need to get the fulfillment message.\n\nDoes anyone have any idea what I can do? Thank you all!!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Adding Dialogflow es library to python virtualenv",
        "Question_tag_count":3,
        "Question_created_time":"2023-02-09T14:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Adding-Dialogflow-es-library-to-python-virtualenv\/m-p\/520568#M1260",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":91,
        "Question_body":"First, thank you to those that contribute to this forum.\n\nI am having a hard time trying to get the library imported into my python environment. I've tried all different methods and nothing seems to work.\n\nI've upgraded my environment. My python version is 3.11. I have isolated the environment in its own virtualenv. I've tried uninstalling\/reinstalling. I attempted the different imports.\n\nI would greatly appreciate any direction, links, articles, kind words. Thank you.\n\nimport dialogflow_v2 as dialogflow\n# import google_cloud_dialogflow as dialogflow",
        "Question_closed_time":"02-12-2023 12:25 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Thank you for your reply. I had a few things off. I needed to install the SDK and write the following into my code:\u00a0 I was using the first two (commented out with #) and they didn't work perhaps because they were deprecated. Once I did this, I was able to connect to the agent.\n\n# import dialogflow_v2 as dialogflow\n# import google_cloud_dialogflow as dialogflow\nfrom google.cloud import dialogflow_v2beta1 as dialogflow\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Unable to use audio to text transcribe",
        "Question_tag_count":2,
        "Question_created_time":"2022-03-20T00:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-use-audio-to-text-transcribe\/m-p\/405132#M242",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":193,
        "Question_body":"I am new to this Google Audio transcription and I have set up the whole Google Free Trial thing and I have tried to use the function of Google's Audio to Speech transcript and well so far my customer experience has been so hard.\u00a0 I have two files and *.mpa and a *.mp4 file and no matter what i do i keep getting an error that it cannot transcribe.\n\nCan someone\u00a0 please help me with this.\u00a0\u00a0\n\nHere are the errors I am getting.\n\nNotifications\nRunning recognize for transcription \"2022.03.20_16-30 Paradigm Shift Review Meeting David & Steve Zoom GMT20220320-050943_Recording_640x360-1dddcbcacc9d52be-9cc61\"\n6 minutes ago\nMy First Project\nUnknown error.\nRunning recognize for transcription \"2022.03.20_16-30 Paradigm Shift Review Meeting David & Steve Zoom GMT20220320-050943_Recording-c51097d6e01f1270-6f202\"\n13 minutes ago\nMy First Project\nUnknown error.\nRunning recognize for transcription \"2022.03.20_16-30 Paradigm Shift Review Meeting David & Steve Zoom GMT20220320-050943_Recording-85fc910221e53949-6752c\"\n15 minutes ago\nMy First Project\nUnknown error.\nRunning recognize for transcription \"2022.03.20_16-30 Paradigm Shift Review Meeting David & Steve Zoom GMT20220320-050943_Recording-fed8777481bb94c4-33e9f\"\n16 minutes ago\nMy First Project\nUnknown error.\n\u00a0\nAccording to Handbrake the m4a file has the following ...\n\nFormat : MPEG-4\nFormat profile : Base Media \/ Version 2\nCodec ID : mp42 (isom\/mp42)\nFile size : 26.0 MiB\nDuration : 28 min 30 s\nOverall bit rate mode : Variable\nOverall bit rate : 127 kb\/s\nEncoded date : UTC 2022-03-20 05:09:43\nTagged date : UTC 2022-03-20 05:09:43\n\nAudio\nID : 1\nFormat : AAC LC\nFormat\/Info : Advanced Audio Codec Low Complexity\nCodec ID : mp4a-40-2\nDuration : 28 min 30 s\nBit rate mode : Variable\nBit rate : 126 kb\/s\nMaximum bit rate : 166 kb\/s\nChannel(s) : 1 channel\nChannel layout : C\nSampling rate : 32.0 kHz\nFrame rate : 31.250 FPS (1024 SPF)\nCompression mode : Lossy\nStream size : 25.7 MiB (99%)\nTitle : AAC audio\n\n\u00a0\n\nAnd the mp4 file has\u00a0\n\nFormat : MPEG-4\nFormat profile : Base Media \/ Version 2\nCodec ID : mp42 (isom\/mp42)\nFile size : 30.2 MiB\nDuration : 28 min 30 s\nOverall bit rate mode : Variable\nOverall bit rate : 148 kb\/s\nEncoded date : UTC 2022-03-20 05:09:43\nTagged date : UTC 2022-03-20 05:09:43\n\nVideo\nID : 2\nFormat : AVC\nFormat\/Info : Advanced Video Codec\nFormat profile : High@L3.1\nFormat settings : CABAC \/ 11 Ref Frames\nFormat settings, CABAC : Yes\nFormat settings, Reference frames : 11 frames\nCodec ID : avc1\nCodec ID\/Info : Advanced Video Coding\nDuration : 28 min 30 s\nBit rate : 19.8 kb\/s\nWidth : 640 pixels\nHeight : 360 pixels\nDisplay aspect ratio : 16:9\nFrame rate mode : Constant\nFrame rate : 25.000 FPS\nColor space : YUV\nChroma subsampling : 4:2:0\nBit depth : 8 bits\nScan type : Progressive\nBits\/(Pixel*Frame) : 0.003\nStream size : 4.03 MiB (13%)\nTitle : H.264\/AVC video\nEncoded date : UTC 2022-03-20 05:09:43\nTagged date : UTC 2022-03-20 05:09:43\nCodec configuration box : avcC\n\n\u00a0\n\nWhat codec options do i choose and how do I use this feature.\n\nGoogle",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI dataset permissions",
        "Question_tag_count":1,
        "Question_created_time":"2021-09-20T07:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-dataset-permissions\/m-p\/170536#M51",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":496,
        "Question_body":"Is there a way to assign IAM roles to datasets in Vertex AI so only certain people have access to certain datasets?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"503 on translations",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-20T05:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/503-on-translations\/m-p\/445069#M433",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":251,
        "Question_body":"I started to see this error on multiple clusters in America. But there is nothing in the status page. I don't think we had any updates to our code.\n\ngoogle.api_core.exceptions.ServiceUnavailable: 503 POST https:\/\/translation.googleapis.com\/language\/translate\/v2?prettyPrint=false: The service is unavailable at this time.\n\nI guess I need to wait, but posting here just to raise it",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Experiment tracking - Metadata store",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-03T02:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Experiment-tracking-Metadata-store\/m-p\/540132#M1567",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"Hey,\n\ni got a question regarding the MLOps-Principles. Is the metadatastore one component of experiment tracking or are those different aspects of MLOps. What i thought, experiment tracking tracks the metadata for the experiments in creating a training pipeline and the metadata store tracks the information about the pipeline runs in production. Or is everything of that included in experiment tracking?\n\nCan somebody help me with that? Thanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to invoke a rich response button in Diagflow CX through API",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-24T08:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-invoke-a-rich-response-button-in-Diagflow-CX-through-API\/m-p\/513390#M1126",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"I have a diagflow agent that it does return a custom Payload (buttons which transition the session to another flow). Also I have a standalone client (Java client) that queries the Diagflow agent and return response back to the End User.\n\nUsers will interact with the Diagflow agent through Java client developed using google Java SDK. I need help in how can I invoke action button that is retuned by diagflow agent in my Java Client, so the User is transitioned to another flow?\n\nBelow is the custom payload response after calling detectIntent function on SessionClient.\u00a0\n\n\u00a0\n\nType: payload\npayload {\n  fields {\n    key: \"richContent\"\n    value {\n      list_value {\n        values {\n          list_value {\n            values {\n              struct_value {\n                fields {\n                  key: \"event\"\n                  value {\n                    struct_value {\n                      fields {\n                        key: \"name\"\n                        value {\n                          string_value: \"EN_TXT\"\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: \"icon\"\n                  value {\n                    struct_value {\n                      fields {\n                        key: \"color\"\n                        value {\n                          string_value: \"#FF9800\"\n                        }\n                      }\n                      fields {\n                        key: \"type\"\n                        value {\n                          string_value: \"chevron_right\"\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: \"link\"\n                  value {\n                    string_value: \"\"\n                  }\n                }\n                fields {\n                  key: \"text\"\n                  value {\n                    string_value: \"English\"\n                  }\n                }\n                fields {\n                  key: \"type\"\n                  value {\n                    string_value: \"button\"\n                  }\n                }\n              }\n            }\n            values {\n              struct_value {\n                fields {\n                  key: \"event\"\n                  value {\n                    struct_value {\n                      fields {\n                        key: \"name\"\n                        value {\n                          string_value: \"\\331\\220AR_TXT\"\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: \"icon\"\n                  value {\n                    struct_value {\n                      fields {\n                        key: \"color\"\n                        value {\n                          string_value: \"#FF9800\"\n                        }\n                      }\n                      fields {\n                        key: \"type\"\n                        value {\n                          string_value: \"chevron_right\"\n                        }\n                      }\n                    }\n                  }\n                }\n                fields {\n                  key: \"link\"\n                  value {\n                    string_value: \"\"\n                  }\n                }\n                fields {\n                  key: \"text\"\n                  value {\n                    string_value: \"\\330\\271\\330\\261\\330\\250\\331\\212\"\n                  }\n                }\n                fields {\n                  key: \"type\"\n                  value {\n                    string_value: \"button\"\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Eliminacion de fondos personalizados",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-12T05:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Eliminacion-de-fondos-personalizados\/m-p\/488739#M768",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":51,
        "Question_body":"Hay documentos que tienen un fondo personalizado ya sea con logos o texto referencial a la empresa o al proceso que se lleva a cabo, es o ser\u00e1 posible eliminar estas caracter\u00edsticas y as\u00ed poder hacer m\u00e1s eficiente el nivel de eficiencia del OCR",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"API for HITL UI and Operations",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-24T15:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/API-for-HITL-UI-and-Operations\/m-p\/546853#M1736",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":274,
        "Question_body":"Hello, is there a set of APIs for HITL operations - labeling, correcting values, adding new entities and training using a custom UI we are building? It appears that all HITL operations have to be performed using the Google Labelers workbench. Any alternative path you can recommend?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"App building assistance",
        "Question_tag_count":4,
        "Question_created_time":"2023-03-27T09:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/App-building-assistance\/m-p\/537508#M1511",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"Hi there,\n\nI am completely new to this and have no programming language or coding experience. I am looking to desging an AI assistant app using Dialog flow. I am trying to navigate my way and find out where to start. Anyone that can help?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"dialogflow no response if input too frequency",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-28T18:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/dialogflow-no-response-if-input-too-frequency\/m-p\/504274#M997",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"If user input the wording too frequency.\n\nDialogflow agent will hang and no intent message reply.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Invoice parser \"invents\" non-existing string for supplier_name normalized value",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-18T06:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Invoice-parser-quot-invents-quot-non-existing-string-for\/m-p\/554570#M1952",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"While processing a dummy invoice taken from the internet, I have found that invoice parser returns normalized value for supplier_name with a wrong string that is not found anywhere on the image.\n\nExpected normalized value:\u00a0Stanford Plumbing & Heating\n\nProposed value:\u00a0Stanford University\n\n\u00a0\n\n\"type\": \"supplier_name\",\n\"mentionText\": \"Stanford Plumbing & Heating\"\n[...]\n\"normalizedValue\": {\"text\": \"Stanford University\"}\n\n\u00a0\n\nInvoice header looks like below\n\nI wonder if I should consider this a bug or a feature?",
        "Question_closed_time":"05-19-2023 12:57 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi, Yes you are correct. Although the value is incorrect, this is a feature called Enrichment and Normalization (this is with the intention to reduce post processing for the API)\u00a0 for more details about this, please visit the link about this feature[1].Also I would recommend contacting Google Support for them to look in to this and improve the quality of capturing for this object[2].\n\n[1]https:\/\/cloud.google.com\/document-ai\/docs\/ekg-enrichment\n\n[2]https:\/\/cloud.google.com\/contact\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Cloud Vision issue: Incorrect end time for MP4 videos",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-24T00:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-issue-Incorrect-end-time-for-MP4-videos\/m-p\/525828#M1335",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"Hi!\nI encountered a weird issue with Cloud Vision.\nWhen processing MP4 videos that were recorded on iPhone (Safari) with 5 fps, the\u00a0endTimeOffset is incorrectly computed.\n\nFor example, processing a 5 seconds MP4 with a frame-rate of 5fps would yield the following start\/end time:\n\n\u00a0\n\n \"segment\": {\n        \"endTimeOffset\": \"0.834166s\",\n        \"startTimeOffset\": \"0s\"\n      },\n\n\u00a0\n\nThis is clearly incorrect.\nIs this a known issue?\nIs there a way to resolve this?\n\nThanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GCP function deploy",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-26T04:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-function-deploy\/m-p\/597192#M2029",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":44,
        "Question_body":"The error message you received indicates that there was a bad syntax in the request you sent to the server. This error typically occurs when the server cannot understand or process the request due to incorrect formatting or missing information. i got this error while testing with post man i deploy my function and write code for model download i checked the code no any syntax but when ever i test with post man i got that error",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"CUstom Container in Vertex AI pipeline",
        "Question_tag_count":1,
        "Question_created_time":"2022-01-07T06:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/CUstom-Container-in-Vertex-AI-pipeline\/m-p\/182237#M160",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":210,
        "Question_body":"Hi,\n\nI wanted to check that is it possible to create a custom container in a Vertex AI pipeline and Push it to Artifact registery?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I run my custom model training from vscode",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-30T02:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-run-my-custom-model-training-from-vscode\/m-p\/538798#M1531",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":221,
        "Question_body":"I have run a custom model training on workbench. I want to execute this training from VSCode. Is that possible and if yes, how do i do it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"JupyterLab classic UX not Vertex Ai",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-06T14:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/JupyterLab-classic-UX-not-Vertex-Ai\/m-p\/609860#M2314",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":48,
        "Question_body":"I created Notebook\n\nvm_image_family = \"common-cpu-debian-11-py310\"\nand got Classic interface JupyterLab.\n\n\u00a0\n\nI saw a lot of guides and official docs with the Vertex Ai interface JupyterLab.\nI rebuilt a lot of time Notebook aka VM and still a classic interface without GCS, memory usage and etc.\n\u00a0\nHow I can set\u00a0Vertex Ai interface JupyterLab?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Relation Google node hour to Azure computing hour",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-05T06:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Relation-Google-node-hour-to-Azure-computing-hour\/m-p\/450911#M490",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":44,
        "Question_body":"Hey there,\n\nI am writing my masters thesis at the moment.\u00a0In my master thesis I compare the machine learning services of Google and Microsoft for image classification. This also includes the costs. Google uses node hours and Microsoft computing hours for the calculation. Is it possible to compare these units? This would be a crucial part of the comparison.\nThanks a lot!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create dataset and labeling in Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-06T04:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-dataset-and-labeling-in-Vertex-AI\/m-p\/609712#M2309",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":84,
        "Question_body":"Hi, I'm new to GCP and I tried to create an AutoML model in Vertex AI for that\u00a0I created a dataset by taking 20 .txt files and by adding 4 labels but training failed, showing insufficient validation data labeled error. I went through the documents as well but the issue is not resolved yet. Please help me out with this.\n\nThanks,\n\nJohn",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"capturing Parameter advanced speech settings from pages through pagesClient",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-22T01:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/capturing-Parameter-advanced-speech-settings-from-pages-through\/m-p\/524933#M1308",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":98,
        "Question_body":"hello everyone,\n\nI was trying to capture parameter DTMF settings in pages from backend through pagesClient , but that client has limitation , it is unable to capture the DTMF settings. Is there any other way to capture DTMF settings from backend?\n\nreference :\u00a0@https:\/\/cloud.google.com\/dialogflow\/cx\/docs\/reference\/rest\/v3\/projects.locations.agents.flows.pages#...",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Latency estimation of Product Search API",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-05T11:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Latency-estimation-of-Product-Search-API\/m-p\/609502#M2302",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":66,
        "Question_body":"Hi everyone,\n\nI'm considering using the Product Search service from the Cloud Vision API, but I need to have an estimation of how the latency increases with the catalog size. Testing with a catalog of around 10 products, the latency is neglegible for the user experience. However, if I have catalog of around 100,000 products, how can I estimate the time of response?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Which service is suitable for speaker verification?",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-27T06:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Which-service-is-suitable-for-speaker-verification\/m-p\/547855#M1751",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":133,
        "Question_body":"Hello I am new in google cloud. Which service I can use to build a speaker verification api and use it in the app can you guide me I found speaker-id but I think I can't use it with python.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I enable generativelanguage.googleapis.com?",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-13T09:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-enable-generativelanguage-googleapis-com\/m-p\/553160#M1894",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":233,
        "Question_body":"I already have API Access to Vertex AI, and the Python code works with the google AI SDK.\n\nHowever, I'm using this with Langchain, and it requires\u00a0\n\ngoogle.generativeai\u00a0module along with a Google API Key. However I'm unable to enable\n\u00a0\ngenerativelanguage.googleapis.com, due to a permission denied issue. Is there any way to resolve this? Since I already have Enterprise access to Vertex AI.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Feature Store Calculations",
        "Question_tag_count":3,
        "Question_created_time":"2022-05-12T13:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Feature-Store-Calculations\/m-p\/422580#M329",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":111,
        "Question_body":"Hi,\n\nI\u00a0have a Google Colab notebook with some functions (Python) that been used to calculate the features for a model.\n\nThe functions use as inputs data from an API.\n\nThe question is if I can or should calculate the features inside a Features Store and feed the results to the Model?\n\nOr in which Instance do I need to make the calculations and then feed the results into the model?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Appsheet with Google Cloud Vision",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-26T05:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Appsheet-with-Google-Cloud-Vision\/m-p\/417105#M294",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":160,
        "Question_body":"Hello,\n\nHow can I integrate Google Cloud Vision in Appsheet to create\u00a0 a Facial recognition CHECK IN system where the staff can just take photos of themselves on a device and the image captured is compared with the stored image in the Google Drive. If the Face is detected or matched with the stored image, it should automatically CHECK IN OR CHECK OUT the staff.\n\nPlease kindly help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Is Cloud Vision API supported at all?",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-19T08:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Is-Cloud-Vision-API-supported-at-all\/m-p\/613693#M2399",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":16,
        "Question_body":"The LANDMARK_DETECTION feature has been broken since beginning of April:\n\nhttps:\/\/issuetracker.google.com\/issues\/276683178\n\nIt fails on Google's demo Try It! page. Has Google destaffed the development team?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"vertex AI Workbench is hanging with error \"Opening notebook with JupyterLab\" for more than a day",
        "Question_tag_count":2,
        "Question_created_time":"2022-09-08T08:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/vertex-AI-Workbench-is-hanging-with-error-quot-Opening-notebook\/m-p\/464300#M570",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":387,
        "Question_body":"I am trying to follow instructions in\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/jupyter-notebooks\u00a0(vertex AI Jupyter Notebooks tutorials). Steps done\n\n1. For the first notebook \"Text Classification model\" I have clicked on \"Vertex AI Workbench\". It takes me to GCP console & workbench.\n\n2. I am supposed to click on the \"Create\" button, which I did.\n\n3. THen the message \"Opening notebook with JupyterLab\" will come. But it is there for past 1 day, and still it hasn't finished creating. So I canceled the same. I tried once more the same thing happens. Not sure why?\n\nI have screen shots, but can't see anywhere to attach.\n\nHave anyone tried this tutorial, especially in workbench?\u00a0\n\nThanks,",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI OCR problem and IssueTracker no reply",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-03T20:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-OCR-problem-and-IssueTracker-no-reply\/m-p\/550130#M1782",
        "Question_answer_count":8,
        "Question_score_count":0,
        "Question_view_count":220,
        "Question_body":"Hi,\n\nI have submitted reports on IssueTracker about\u00a0Document AI OCR recognition errors, but the report has not been assigned yet.\n\nAll the reports I've submitted are about OCR recognition errors, which are affecting my use. In addition, I've also submitted a suggestion report that can improve the accuracy of OCR.\n\nPlease\u00a0assigned my report and check my update error info.\n\nhttps:\/\/issuetracker.google.com\/issues\/280659979\n\nhttps:\/\/issuetracker.google.com\/issues\/277497068\n\nhttps:\/\/issuetracker.google.com\/issues\/280231466\n\nhttps:\/\/issuetracker.google.com\/issues\/280480137",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I can't add a follow up Intent using Swahili Language (Dialogflow ES)",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-03T06:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-can-t-add-a-follow-up-Intent-using-Swahili-Language-Dialogflow\/m-p\/549845#M1778",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":67,
        "Question_body":"Hello Guys, Am kind stuck because Dialogflow will not allow me to add follow-up intent. Am developing my project using swahili language. However if\u00a0 I switch the language to English then Dialogflow allows me to add follow-up intents.\u00a0 What could be the solution besides switching the language?",
        "Question_closed_time":"05-04-2023 03:01 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Good day\u00a0@Nyanda_Jr\u00a0,\n\nWelcome to Google Cloud Community!\n\nOne of the possible reasons of this issue is due to the language limitations, as of now, the only feature available to Swahili language and other languages is text-only chat, which means speech-to-text, text-to-speech, phone, etc. are not yet available to Swahili language. I highly suggest that you switch languages since intents in the English language will still be copied to other languages including follow-up intents. You can check this documentation for more information regarding language limitations:\u00a0https:\/\/cloud.google.com\/dialogflow\/es\/docs\/reference\/language\n\nHope this will help!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Authenticating to Vertex AI deployed endpoints",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-10T13:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authenticating-to-Vertex-AI-deployed-endpoints\/m-p\/488229#M762",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":398,
        "Question_body":"Hello, I am a new user of Vertex AI.\u00a0 I have trained and deployed a tabular data categorization model to an Vertex AI hosted endpoint.\u00a0 I have successfully called it from a program running on my laptop where the \"gcloud\" cli is installed.\u00a0 If I want to run this not from my desktop but have it called from another service, how do I authenticate ?\u00a0 I have created a service account but I am not sure 1) what roles would need to be attached to that account and 2) how I would provide the service account credentials given that I don't have much control over how the service that will call my model is started (i.e. I can't control its environment vars).\u00a0 Any help would be appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Impossible to create vertex Ai model version with gcloud ai models",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-08T07:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Impossible-to-create-vertex-Ai-model-version-with-gcloud-ai\/m-p\/601294#M2105",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"I use\u00a0\n\n\u00a0\n\ngcloud ai models upload\n\n\u00a0\n\nto create a model from a custom docker image. But I do not see a way to create a new version of that model in command line? Only with UI console.\n\nIs the feature missing or am I missing something?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Next Step from Google Colab +Pro",
        "Question_tag_count":3,
        "Question_created_time":"2022-05-10T08:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Next-Step-from-Google-Colab-Pro\/m-p\/421797#M322",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":452,
        "Question_body":"Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some models\n\nWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?\n\nShould I get a Google Compute Engine and try to connect the google colab files to it?\n\nShould I up load the model to vertex AI?\n\nWhat characteristics should I need to take into consideration before I select any of the different tools?",
        "Question_closed_time":"05-13-2022 11:42 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"enhanced speech feature",
        "Question_tag_count":3,
        "Question_created_time":"2021-08-19T01:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/enhanced-speech-feature\/m-p\/167747#M39",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":476,
        "Question_body":"Hi I have a query\n\nIn Dailogflow if we enable enhanced speech feature, specifically, credit card info (i.e. number), if that is spoken by user, is that stored by Google.\n\n\u00a0\n\nPlease help",
        "Question_closed_time":"08-20-2021 06:59 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Moving from GPT-4 API to Palm2 API",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-25T13:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Moving-from-GPT-4-API-to-Palm2-API\/m-p\/615776#M2460",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":12,
        "Question_body":"Out of frustration with OpenAI, I signed up for Google Palm2 API and was accepted today.\u00a0 Looking at the generateMessage curl request.\u00a0\u00a0\n\nBeen working with GPT-4 API for months.\u00a0 Palm2 API seems similar, but before I go monkeying around, I was wondering if anyone with any experience moving from GPT to Palm conversational chat APIs\u00a0 had any pearls of wisdom to share?\n\nAlso, is there some support forum specific to Palm2, or is this AI\/ML group it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"SSML to read Date in German Language not working",
        "Question_tag_count":3,
        "Question_created_time":"2022-04-21T14:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/SSML-to-read-Date-in-German-Language-not-working\/m-p\/415908#M292",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":152,
        "Question_body":"Hi - I am working on converting Text to Speech using SSML via Google Speech API. Below is the request to the API. This perfectly works when language Code is En-US , however for code de-DE and to hear in German voice, the output is totally random. Please help me in checking this issue,\u00a0\n\n\u00a0\n\nTTS Request JSON :: {\"voice\":{\"ssmlGender\":\"MALE\",\"name\":\"de-DE-Wavenet-E\",\"languageCode\":\"de-DE\"},\"input\":{\"ssml\":\"<speak><say-as interpret-as=\\\"date\\\" format=\\\"yyyymmdd\\\"> 20220506<\\\/say-as><\\\/speak>\"},\"audioConfig\":{\"sampleRateHertz\":8000,\"volumeGainDb\":0,\"speakingRate\":1,\"audioEncoding\":\"LINEAR16\",\"pitch\":0,\"effectsProfileId\":[\"telephony-class-application\"]}}\n\n\u00a0\n\nThe same request when changeing the name and Language code works perfectly.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
        "Question_tag_count":2,
        "Question_created_time":"2021-11-28T16:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI\/m-p\/176629#M96",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
        "Question_closed_time":"11-29-2021 12:59 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2022-07-26T01:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-while-trying-to-get-explanation-from-custom-container\/m-p\/446817#M447",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":382,
        "Question_body":"Hi,\n\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get\u00a0explainable\u00a0predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\n\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\n\nEnvironment details\nGoogle Cloud Notebook\nPython version: 3.7.12\npip version: 21.3.1\ngoogle-cloud-aiplatform\u00a0version: 1.15.0\nReference\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\n\nexplanation-metadata.json\n\n(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)\n\n{\n    \"inputs\": {\n        \"A\": {},\n        \"B\": {}\n    },\n    \"outputs\": {\n        \"Y\": {}\n    }\n}\nModel upload with explanation parameters and metadata\n! gcloud ai models upload \\\n  --region=$REGION \\\n  --display-name=$MODEL_NAME \\\n  --container-image-uri=$PRED_IMAGE_URI \\\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\n  --explanation-method=sampled-shapley \\\n  --explanation-path-count=10 \\\n  --explanation-metadata-file=explanation-metadata.json\nPrediction\/Explanation Input\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\n# Prediction (works fine):\nendpoint.predict(instances=instances)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\n\n# Another example\ninstances_2 = [[1.1,20], [2.2,21]]\n# Prediction (works fine):\nendpoint.predict(instances=instances_2)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances_2) # Returns error\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\nPrediction Server (Flask)\n# Custom Flask server to serve online predictions\n# Input for prediction\nraw_input = request.get_json()\ninput = raw_input['instances']\ndf = pd.DataFrame(input, columns = ['A', 'B'])\n# Prediction from model (loaded from GCP bucket)\npredictions = model.predict(df).tolist() # [0, 1]\nresponse = jsonify({\"predictions\": predictions})\nreturn response\nStack trace of error (1)\n---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     49         try:\n---> 50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--> 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--> 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\n\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\n   1563             parameters=parameters,\n   1564             deployed_model_id=deployed_model_id,\n-> 1565             timeout=timeout,\n   1566         )\n   1567 \n\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\n    917             retry=retry,\n    918             timeout=timeout,\n--> 919             metadata=metadata,\n    920         )\n    921 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[\"metadata\"] = metadata\n    153 \n--> 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n---> 52             raise exceptions.from_grpc_error(exc) from exc\n     53 \n     54     return error_remapped_callable\n\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\n---------------------------------------------------------------------------\n# https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"NotFound: 404 Publisher Model `publishers\/google\/models\/chat-bison@001` is not found",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-09T13:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NotFound-404-Publisher-Model-publishers-google-models-chat-bison\/m-p\/601725#M2115",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":552,
        "Question_body":"Hi - I'm using on-prem jupyter notebook to run the following code and am getting the error \"NotFound: 404 Publisher Model `publishers\/google\/models\/chat-bison@001` is not found\". Has anyone run into this issue? Thank you.\u00a0\n\ndef predict_large_language_model_sample(\nproject_id: str,\nmodel_name: str,\ntemperature: float,\nmax_output_tokens: int,\ntop_p: float,\ntop_k: int,\nlocation: str = \"us-central1\",\n) :\n\"\"\"Predict using a Large Language Model.\"\"\"\nvertexai.init(project=project_id, location=location)\n\nchat_model = ChatModel.from_pretrained(model_name)\nparameters = {\n\"temperature\": temperature,\n\"max_output_tokens\": max_output_tokens,\n\"top_p\": top_p,\n\"top_k\": top_k,\n}\n\nchat = chat_model.start_chat(\nexamples=[]\n)\nresponse=chat.send_message('''What version of PaLM are you?''',**parameters)\nprint(response.text)\n\npredict_large_language_model_sample(PROJECT_ID, \"chat-bison@001\", 0, 256, 0.8, 40, LOCATION)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Receiving quota error when trying to use the Embedding for Image model in Model Garden",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-12T21:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-the-Embedding-for-Image\/m-p\/602429#M2139",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":34,
        "Question_body":"Hi, I am working on building an application around image search. I am using the Embeddings for Image model\u00a0https:\/\/console.cloud.google.com\/vertex-ai\/publishers\/google\/model-garden\/5?project=applied-ai-labs.\n\n\u00a0\n\nBut, when I am trying to run it from the colab notebook attached,\u00a0 I am getting this error.\u00a0\n\n429 Quota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: multimodalembedding. Please submit a quota increase request.\n\nI checked the quota list, but can't find\u00a0multimodalembedding.\u00a0\n\nPlease help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to integrate ML model with excel sheet containing 400k image links",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-11T06:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-integrate-ML-model-with-excel-sheet-containing-400k-image\/m-p\/542553#M1628",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"I created a model to identify good or bad profile images for a company, Data is in excel sheet with 400k+ links, I don't want to download all the images. I'm looking for another method in which one can run ML model on excel sheet. Is there anyone who has done this before or know the method? I created the model on Jupyter notebook, I don't know where to deploy the model to get the API key. Any reference will be apricated.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Node hours vs actual time",
        "Question_tag_count":2,
        "Question_created_time":"2022-06-16T01:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Node-hours-vs-actual-time\/m-p\/431897#M387",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":375,
        "Question_body":"What is meant by node hours in VertexAI?\n\nI set the budget in VertexAI AUtoML to a 1 node hour but my model has been training for 1 hr and 30+ minutes.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Translation API male female translation issue with ukranian language.",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-08T01:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translation-API-male-female-translation-issue-with\/m-p\/610241#M2321",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":42,
        "Question_body":"When translating names from Ukranian(also seen in Polish, Hungarian), there is a problem where the returning translated value is a name of the opposite gender. The\u00a0incorrect translation will be related to all the names that in Ukraine can have a male and female \u201cversion\u201d (e.g. Yaroslav \u2013 Yaroslava, Oleksandr \u2013 Oleksandra etc.).\n\nExample :\u00a0\n\n\"\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u0430 \u0424\u0435\u0434\u043e\u0440\u0435\u043d\u043a\u043e\" is translated as \"Yaroslav Fedorenko\" by Google Translation API. The correct expected output is \"Yaroslava Fedorenko\".\n\n\"Bing\" \/ \"Deep L\" \/ \"www.reverso.net\" \/ all provided the correct translation ie\u00a0\"Yaroslava Fedorenko\".\n\nEven in Google translation done from web, the text shown under ukranian text(which is I think is meant for pronunciation) is correct. But the translation on right side is wrong.\u00a0\u00a0\n\n\u00a0In Ukranian language \"\u042f\u0440\u043e\u0441\u043b\u0430\u0432\" is male(without a in end) and \"\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u0430\" is female version. Below wikipedia page explains this.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Ukrainian_name\n\nIs there a way to get correct translation of\u00a0\"\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u0430 \u0424\u0435\u0434\u043e\u0440\u0435\u043d\u043a\u043e\" as\u00a0\"Yaroslava Fedorenko\" from Google API. If this is a bug in API, where should we report this to get it fixed.\u00a0\n\nHave attached Screenshots below.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX: optional parameter capture",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-11T03:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-optional-parameter-capture\/m-p\/508944#M1047",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":115,
        "Question_body":"Sometimes user input is\n\n1: \u00a0\u00a0 I want to order a new project\n\nand sometimes it is\n\n\u00a02: \u00a0 I want to order a new project called SalesPitch\n\nand sometimes it is\n\n\u00a03:\u00a0 I want to order a new project called SalesPitch for delivery by tomorrow\n\nInput 1 above is easy to recognise. The chatbot has to go on to ask what the project name should be, and when delivery is wanted.\n\nInput 2 can detect the intent and pick out the projectName in one go. But in the Dialogflow CX console, I cannot mix Input 2 phrases and input 1 phrases, so that projectName is optionally extracted from input. Once I start with an annotated training phrase, the intent editor seems to demand that every subsequent training phrase includes the same entity to be extracted. It automatically annotates every phrase with the entity, and I cannot delete the annotation from the training phrase.\n\nIt seems I cannot mix annotated and unannotated training phrases in one intent. Is that true? Anyway, I can try and work around this by creating two intents, one that requires the parameter to be defined in the input, and one that does not.\n\nBut then we come to input 3. The user can specify delivery date too. So now I have two optional parameters, either of which might be present in the input. That means I now have to have four different intents with training phrases for the four options of zero, one or two parameters in the input. And then if I have another optional parameter, I need eight different intents...\n\nIs that the way to do it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DialogFlow CX integration with Genesys or CX One",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-13T07:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DialogFlow-CX-integration-with-Genesys-or-CX-One\/m-p\/499133#M935",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"Hi everyone\n\nI'm trying to connect DialogFlow CX chat with some CX tools like Nice or Genesys. And the chat is working.\u00a0\nBut my problem is to receive parameters from the platform and the way to use it in DialogFlow.\n\nHere an example of the input i receive on log activity.\nHow can i use the parameter firstName on DialogFlow ?\u00a0\n\n\u00a0\n\n{\n  \"insertId\": \"-cggsxvqhcamq\",\n  \"jsonPayload\": {\n    \"queryParams\": {\n      \"payload\": {\n        \"context\": \"{\\n  \\\"id\\\": \\\"information\\\",\\n  \\\"lifespan\\\": \\\"2\\\",\\n  \\\"parameters\\\": {\\n    \\\"MSDynaContactId\\\": \\\"07cf2edb-ab48-ed11-bba2\\\",\\n    \\\"firstName\\\": \\\"FN\\\",\\n    \\\"lastName\\\": \\\"Kimura\\\",\\n    \\\"segment\\\": \\\"SILVER\\\",\\n    \\\"OvaloContactId\\\": \\\"OV0002\\\",\\n    \\\"mobilePhone\\\": \\\"6346373\\\",\\n    \\\"emailAddress1\\\": \\\"takashikimura.customer@gmail.com\\\",\\n    \\\"facebook\\\": \\\"\\\"\\n  }\\n}\"\n      },\n      \"parameters\": {}\n    },\n    \"queryInput\": {\n      \"languageCode\": \"fr\",\n      \"text\": {\n        \"text\": \"OV0002\"\n      }\n    },\n    \"session\": \"projects\/profile-bot-dohn\/locations\/europe-west3\/agents\/3136149e-42f2-4ac5-ab47-\/environments\/draft\/sessions\/BusNo--\"\n  },\n  \"resource\": {\n    \"type\": \"global\",\n    \"labels\": {\n      \"project_id\": \"profile-bot-dohn\"\n    }\n  },\n  \"timestamp\": \"2022-12-13T13:23:25.801205Z\",\n  \"severity\": \"INFO\",\n  \"labels\": {\n    \"session_id\": \"BusNo--\",\n    \"agent_id\": \"3136149e-42f2-4ac5-ab47-\",\n    \"location_id\": \"europe-west3\"\n  },\n  \"logName\": \"projects\/profile-bot-dohn\/logs\/dialogflow-runtime.googleapis.com%2Frequests\",\n  \"receiveTimestamp\": \"2022-12-13T13:23:26.476085542Z\"\n}\n\n\u00a0\n\n\u00a0\n\nthank you for your help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Api TTS",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-07T17:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Api-TTS\/m-p\/497285#M920",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":88,
        "Question_body":"I am using the TTS and STT services of Google Cloud API together. I am inquiring because an error has occurred only in the TTS function since last week without code modification or redistribution. When I checked the data from the backend log, I found no abnormality and the Google Cloud server is sending 400 errors. There is no problem with STT, but only TTS is experiencing problems. Please check.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BatchPredict could not start due to empty input CSV file",
        "Question_tag_count":2,
        "Question_created_time":"2021-12-31T03:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BatchPredict-could-not-start-due-to-empty-input-CSV-file\/m-p\/181685#M146",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":73,
        "Question_body":"Hello,\nI have a problem with batch prediction for text classification.\nAccording to the documentation I have created a csv file in which every single line refers to a PDF in my bucket. However I get the error message \"InvalidArgument: 400 BatchPredict could not start due to empty input CSV file\".\n\nI would be infinitely grateful for help in this case....",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX is there a way to iterate a parameter?",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-22T07:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-is-there-a-way-to-iterate-a-parameter\/m-p\/605694#M2207",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"Hello,\n\nIs there a way to iterate an array that you have in paremeter. For example I have an array =\u00a0[ { \"rate_float\": 29735.3307, \"symbol\": \"&#36;\", \"description\": \"United States Dollar\", \"rate\": \"29,735.3307\", \"code\": \"USD\" }, { \"rate_float\": 29735.3307, \"symbol\": \"&#36;\", \"description\": \"United States Dollar\", \"rate\": \"29,735.3307\", \"code\": \"USD\" } ] in a parameter called test.\u00a0\n\nIn the agent response I would put $session.params.test[0] to get the first one, but for example I want to iterate and make a response like a list, how should I approach\u00a0this? With a custom payload?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Authentication for the Document AI",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-03T10:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authentication-for-the-Document-AI\/m-p\/428530#M369",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":227,
        "Question_body":"First off, please be kind, as I'm not a developer and may struggle with some basics concepts.\n\nI'm trying to build a AI Invoice reader to collect invoice data in a spreadsheet, using Integromat \/ make.com (no-code platform) and Google Cloud Services.\nUsually, there are integrations for what I need in Integromat or I use simple REST calls.\u00a0\n\nWith the Document AI, afaik I have to use OAuth. I have my \"processor\" and the\u00a0\n\nClient ID and\u00a0Client secret for Integromat. What I'm now missing are:\nAuthorize URI\nToken URI\n\nI've been searching the Google Cloud documentation for a while, but for a non-dev it's quite confusing. Where can I find the two URLs needed?\n\nThank you very much for your help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Which NLU Model is used for Dialogflow CX?",
        "Question_tag_count":2,
        "Question_created_time":"2023-01-13T06:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Which-NLU-Model-is-used-for-Dialogflow-CX\/m-p\/509932#M1063",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":128,
        "Question_body":"We assume that Sentence Embeddings are used for the intent analysis. Is this correct? If so, is it a publicly available model? We would like to display the embeddings of the training sentences graphically (e.g. with t-SNE) for the optimization of our training.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech to text work",
        "Question_tag_count":9,
        "Question_created_time":"2021-11-08T00:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-work\/m-p\/175139#M79",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":372,
        "Question_body":"I am a user of Speech to Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually.\n\n\u00a0\n\nSo, in Russian it works fine, however, 20-30 percents of the words are incorrect. Moreover, there are no Russian punctuation at all. \u00a0So I get the speech to text transcript, then I create the perfect transcript out of this with correct words and punctuations.\n\n\u00a0\n\nAll I want to know is how I can improve Speech toText by using the perfect transcript I have already corrected? Where I can send that data to?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"PipelineJob using a dedicated service account",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-10T00:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/PipelineJob-using-a-dedicated-service-account\/m-p\/530893#M1412",
        "Question_answer_count":4,
        "Question_score_count":2,
        "Question_view_count":318,
        "Question_body":"When I submit my pipelinejob (submit)\n\njob = PipelineJob(...)\njob.submit(\n    service_account='<PII removed by staff>'\n)\n\nI get the following error message:\n\ngrpc_message:\"You do not have permission to act as service_account: <PII removed by staff>. (or it may not exist).\"\n\nI am the owner of the project and when I go through the console to create a pipeline job, there is no issue in specifying another service account to run as. It all works if I remove the service_account parameter, so the default account is used. Any idea what the issue can be?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"torch.cuda.is_available() returns False on Vertex AI",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-07T14:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/torch-cuda-is-available-returns-False-on-Vertex-AI\/m-p\/601055#M2097",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":407,
        "Question_body":"I run a custom model on vertex AI. Model is a simple FastAPI app that loads a whisper model. The beginning\u00a0of the app looks like this.\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nif torch.cuda.is_available():\nprint(\"GPU is available =)\")\nmodel = whisper.load_model(model_name).cuda()\nelse:\nprint(\"GPU is not available =(\")\nmodel = whisper.load_model(model_name)\n\n\u00a0\n\n\u00a0\n\nWhen running on vertex AI\u00a0\n\n\u00a0\n\n\u00a0\n\ngcloud ai endpoints deploy-model [ENDPOINT_NAME] \\\n     --region=europe-west4 \\\n     --model=[MODEL_NAME] \\\n     --machine-type=n1-standard-2 \\\n     --accelerator=type=nvidia-tesla-t4,count=1 \\\n\n\u00a0\n\n\u00a0\n\n\ntorch.cuda.is_available() always returns false.\n\nThere is also a log message prio to that\n\n\n\u00a0\n\n\u00a0\n\n\/app\/.venv\/lib\/python3.10\/site-packages\/torch\/cuda\/__init__.py:88: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at ..\/c10\/hip\/HIPFunctions.cpp:110.)\n\n\u00a0\n\n\u00a0\n\nCan you advice me a direction to look into. I'm running out of ideas how to set the app for the GPU support.\u00a0\n\nThis very same docker image works on Compute Engine Vm and can find nvidia drivers. Why can it not do it on Vertex AI.\n\nDocker base image is this btw\n\n\u00a0\n\n\u00a0\n\nFROM nvidia\/cuda:11.7.0-base-ubuntu22.04\n\nENV PYTHON_VERSION=3.10\nENV POETRY_VENV=\/app\/.venv\n\nRUN export DEBIAN_FRONTEND=noninteractive \\\n  && apt-get -qq update \\\n  && apt-get -qq install --no-install-recommends \\\n  python${PYTHON_VERSION} \\\n  python${PYTHON_VERSION}-venv \\\n  python3-pip \\\n  ffmpeg \\\n  && rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN ln -s -f \/usr\/bin\/python${PYTHON_VERSION} \/usr\/bin\/python3 && \\\n  ln -s -f \/usr\/bin\/python${PYTHON_VERSION} \/usr\/bin\/python && \\\n  ln -s -f \/usr\/bin\/pip3 \/usr\/bin\/pip\n\nRUN python3 -m venv $POETRY_VENV \\\n  && $POETRY_VENV\/bin\/pip install -U pip setuptools \\\n  && $POETRY_VENV\/bin\/pip install poetry\n\nENV PATH=\"${PATH}:${POETRY_VENV}\/bin\"\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN poetry config virtualenvs.in-project true\nRUN poetry install\n\nRUN $POETRY_VENV\/bin\/pip install torch==1.13.0 -f https:\/\/download.pytorch.org\/whl\/torch\n\nEXPOSE 8080\nENV PORT 8080\n\nCMD exec gunicorn --bind :${PORT} --workers 1 --threads 8 --timeout 0 app.webservice:app -k uvicorn.workers.UvicornWorker",
        "Question_closed_time":"06-08-2023 04:22 AM",
        "Answer_score_count":1.0,
        "Answer_body":"I think it's best to use the official pytorch gpu image e.g. this:\nhttps:\/\/hub.docker.com\/layers\/pytorch\/pytorch\/1.13.1-cuda11.6-cudnn8-runtime\/images\/sha256-1e26efd42...\n\nJust make sure that you're not doing pip install pytorch again as that image already comes with Pytorch pre-installed with GPU set up, or you'll be overriding it and potentially disable GPUs.\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Changing the service account for an endpoint",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-21T01:37:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Changing-the-service-account-for-an-endpoint\/m-p\/491278#M826",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":447,
        "Question_body":"Hi, I have deployed a Vertex AI model that was created using a custom image. However, when I tried deploying to an endpoint, it fails when it tries to run this line:\n\nbucket = client.get_bucket(\"my-project-id\")\n\nIn the logs, there was an error message that reads\n\ngoogle.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/{my project ID}?projection=noAcl&prettyPrint=false: custom-online-prediction@{some random numbers}-tp.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\n\nIt appears that the endpoint has been assigned a service account that is not associated with my account. From this documentation (https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/custom-service-account), it appears that a\u00a0service account managed by AI Platform Prediction is being used when a custom container is being used. However, my account does not have the permissions\u00a0to create another custom service account, as it is being managed by my client.\u00a0\n\nI came across this solution (https:\/\/stackoverflow.com\/questions\/68456262\/gcp-vertex-ai-model-access-gcs-failed) where the user had the exact same problem and solved it by changing the service account used at the endpoint. As such, I would like to ask how it will be possible for me to change the service account used by the endpoint without having to create a new service account?\n\nThank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Vision broken for English?",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-12T13:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Vision-broken-for-English\/m-p\/488836#M769",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":73,
        "Question_body":"Has anyone else noticed that the Google Cloud Vision OCR that processes the text in images operates starting top to bottom, then left to right for English?\u00a0 And that it didn't used to?\n\nThe problem with this is generally:\n\nWe\u00a0write in English like this.\nSo we want to read the lines from left to right, top to bottom.\n\nWe\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 write\ndo\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0english\nnot\u00a0 \u00a0 \u00a0 \u00a0 \u00a0like\nreally\u00a0 \u00a0 \u00a0 this\n\nTop to bottom, left to right.\u00a0 Which is how you're reading it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Action Needed | OAuth Google Cloud platform | multiple unique domains",
        "Question_tag_count":2,
        "Question_created_time":"2022-04-04T02:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Action-Needed-OAuth-Google-Cloud-platform-multiple-unique\/m-p\/410024#M254",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":135,
        "Question_body":"Hi,\n\nI am carrying out the OAuth verification in Google Cloud Platform, I received an email that said:\n\"Thanks for your patience while we reviewed your project.\n\nYour project pc-api-XXXXXXXXXXXXXXX-XX has multiple unique domains in the redirect URI and origin URLs, many of which have\u00a0unrelated applications. This is in direct violation of the\u00a0Google API Services: User Data Policy, which requires that projects accurately represent their identity and intent to Google and to our users when they request access to Google user data.\n\nPlease follow the instructions on the\u00a0Google API Console\u00a0to:\n\nCreate new projects\nMigrate your redirect URIs with distinct brands to different projects, and\/or\nEnsure that these projects accurately represent their true identity to Google users\n\nYou can find more information in the\u00a0OAuth Application Verification FAQ. \u00a0To make sure we don't miss your messages, respond directly to this email to continue with the verification process.\"\n\n\n\nI have a web server, which checks the validity (domain-1.com) in-app purchases, and I also have a site with a different domain containing: privacy-policy and terms-of-service (domain-2.com).\n\nMy settings are as follows:\n\nOAuth consent screen:\n- Home page application:\u00a0https:\/\/www.domain-2.com\/\n- Privacy Policy:\u00a0https:\/\/www.domain-2.com\/privacy-policy\/\n- Terms of Service:\u00a0https:\/\/www.domain-2.com\/terms-of-service\/\n\nAuthorized domains:\n- domain-2.com\n- domain-1.com\n\nID client OAuth 2.0 -> Authorized Redirect URIs:\n-\u00a0https:\/\/game.domain-1.com:8443\n\nI have a working service account.\nI have successfully verified all 2 domains.\n\n\nWhere is the mistake?",
        "Question_closed_time":"04-04-2022 09:08 PM",
        "Answer_score_count":3.0,
        "Answer_body":"I have found the solution:\n\nI had 2 Google Cloud Platform projects for the same application.\n\nI deleted a Google Cloud Platform.\nI implemented all the configurations of the deleted project in the other project, and it worked!\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"How can I explicitly authenticate to the ai-platform using the java PredictionServiceClient",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-21T14:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-can-I-explicitly-authenticate-to-the-ai-platform-using-the\/m-p\/491537#M833",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":405,
        "Question_body":"I have a model hosted on a Google Cloud endpoint and I would like to access it via the Java client.\u00a0 I've created a service account and a key for that service account with the , when I run my client code with the GOOGLE_APPLICATION_CREDENTIALS env var pointed to the key, I am able to call the service.\u00a0 When I try to authenticate explicitly using FixedCredentialProvider, it fails with an \"unauthenticated\" message.\u00a0\u00a0\n\nThe code is as follows\n\n```\n\nPredictionServiceSettings predictionServiceSettings =\n        PredictionServiceSettings.newBuilder().setEndpoint(location + \"-aiplatform.googleapis.com:443\")\n                .setCredentialsProvider(FixedCredentialsProvider.create(ServiceAccountCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\"))))\n                .build();\npredictionServiceClient = PredictionServiceClient.create(predictionServiceSettings);\nendpointName = EndpointName.of(project, location, endpointId);\nValue featureVal = Value.newBuilder().setStructValue(features).build();\nPredictResponse response =  predictionServiceClient.predict(\n        endpointName,\n        Collections.singletonList(featureVal),\n        Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build());\n\n\n\n```",
        "Question_closed_time":"11-22-2022 09:27 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi,\n\nUpon checking your code, FixedCredentialsProvider.create()\u00a0accepts\u00a0com.google.auth.Credentials\u00a0as a parameter. Can you try a Credentials object to\u00a0FixedCredentialsProvider.create()? See code below:\n\nGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(\"\/Users\/ME\/Downloads\/XYZ.json\")).createScoped(Lists.newArrayList(\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"));\n\n\u00a0If code above did not work, can you provide the stack trace of the error? Also what roles did you assign on your service account?\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Converting TIFF files to base64 using Java",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-24T08:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Converting-TIFF-files-to-base64-using-Java\/m-p\/481476#M679",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":259,
        "Question_body":"Hi\u00a0\n\nI have two instances where I'm converting a TIFF file before sending the data to google vision AI. Using CLI, the base64 string works fine. I get the expected response\n\nWhen I use Java I'm getting a different out, which causes my API call to fail. The base64 output is different.\n\n\u00a0\n\nimageBytes = Files.readAllBytes(new File(fileLocation).toPath());\nString imageString = Base64.getEncoder().withoutPadding().encodeToString(imageBytes);\n\/\/ API response \"message\": \"Unsupported input file format.\"\n\n\u00a0\n\nI'm looking for suggestions on how to convert a file (tiff, pdf, png) to base64 using plain Java",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI OCR error",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-04T20:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-OCR-error\/m-p\/550519#M1795",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":39,
        "Question_body":"Hi,\n\nMy Document AI OCR error, like image show.\n\nWhen \"UPLOAD TEST DOCUMENT\" finish, will recevie error report: \"Failed to preview the document\".\n\nNo matter whether I upload a PDF or a JPG\/BMP file, it always shows this error.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What's the route after conversion action's sunset",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-22T09:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-route-after-conversion-action-s-sunset\/m-p\/469678#M591",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"Hello,\n\ni learn dialogflow with one of the goal to connect with Google Assistant. But i just found out the conversion actions will be sunset.\n\nCan anyone route me on the path to take for the next step of dialogflow integration?\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Export Vertex model error 400",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-17T08:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Export-Vertex-model-error-400\/m-p\/604143#M2180",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":54,
        "Question_body":"Hi,\n\nhi try to export my vertex model. in the 3 case if i choose TF lite, container or Tensorflow, one time i choose the file in my bucket and i click export i have this error message:\u00a0Exporting artifact for model ` ` in format `tf-js` is not supported.\u00a0\n\nAny solution please ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How should the input JSONL look for a batch prediction job?",
        "Question_tag_count":2,
        "Question_created_time":"2022-04-04T10:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-should-the-input-JSONL-look-for-a-batch-prediction-job\/m-p\/410193#M256",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":297,
        "Question_body":"I can't find any examples online of how an input jsonl is supposed to look for a batch training job. When I tried with this:\n\n\u00a0\n\n{'instances': {'mimeType': 'text\/plain', 'content': '0'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '1'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '1'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '2'}}\n{'instances': {'mimeType': 'text\/plain', 'content': '8'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'a'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'a'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'h'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'q'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 's'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'y'}}\n{'instances': {'mimeType': 'text\/plain', 'content': 'y'}}\n\n\u00a0\n\nI got an error email saying\u00a0\n\n\u00a0\n\nError Messages: BatchPrediction could not start because no valid instances\u00a0\nwere found in the input file.\n\n\u00a0\n\nIs there some other way this should look for it to work? Maybe like\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n{\n\n'instances': [\n\n{'mimeType': 'text\/plain', 'content': 'a'}\n\n{'mimeType': 'text\/plain', 'content': 'bc'}\n\n{'mimeType': 'text\/plain', 'content': 'de'}]\n\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I want to monitor user input text with Vertex AI Model Monitoring.",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-14T01:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-want-to-monitor-user-input-text-with-Vertex-AI-Model\/m-p\/602854#M2152",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":69,
        "Question_body":"I am using the ML model to predict toxicity or not for a user entered text. If I enable Vertex AI Model Monitoring for that user-entered text, can I successfully detect skew and drift?\nI am thinking that the string type data is treated as a categorical feature and the user entered text is not the same as the training data, which would trigger an alert for skew or drift.\n\nIf not, is there a way to detect skew and drift for different user-entered text each time?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI: Extracting table data from custom processor.",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-19T00:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Extracting-table-data-from-custom-processor\/m-p\/604389#M2184",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"Hello Support\n\nI am trying to extract data from a PDF file using document-ai. This PDF contains, form like values and line items table information. So, to simply fields, I used Custom Document Extractor\u00a0and trained a version with form labels.\n\nAfter processing, I got these labels with respective values under entities.\nBut\u00a0formFields and tables\u00a0are empty in processDocument response.\u00a0\n\nSo, how can I get line items table data along with marked labels?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Pipeline Template Schema Issue\/Question",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-19T09:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Pipeline-Template-Schema-Issue-Question\/m-p\/545368#M1704",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":167,
        "Question_body":"Hi all,\n\nWhen compiling our artifacts to yaml file, we are expecting the following schema format in our runtime parameter value :\u00a0https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/api\/v2alpha1\/pipeline_spec.proto#L677\u00a0\n\nWhen I first tried to upload my yaml file with the following schema above and try to run the pipeline template, the runtime value treats everything as a string instead of converting it to it's appropriate data type.\u00a0\n\nHere is the yaml file I tried to upload and run:\n```\n\ncomponents:\n\u00a0 input_data:\n\u00a0 \u00a0 executorLabel: input_data_executor\n\u00a0 \u00a0 inputDefinitions:\n\u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 outputDefinitions:\n\u00a0 \u00a0 \u00a0 artifacts:\n\u00a0 \u00a0 \u00a0 \u00a0 output_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifactType:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instanceSchema: 'title: tfx.String\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: object\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '\ndeploymentSpec:\n\u00a0 executors:\n\u00a0 \u00a0 input_data_executor:\n\u00a0 \u00a0 \u00a0 container:\n\u00a0 \u00a0 \u00a0 \u00a0 args:\n\u00a0 \u00a0 \u00a0 \u00a0 - --executor_class_path\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx_common.components.bigquery.executor.BigQueryComponentExecutor\n\u00a0 \u00a0 \u00a0 \u00a0 - --json_serialized_invocation_args\n\u00a0 \u00a0 \u00a0 \u00a0 - '{{$}}'\n\u00a0 \u00a0 \u00a0 \u00a0 - --project=projectid\n\u00a0 \u00a0 \u00a0 \u00a0 - --region=us-central1\n\u00a0 \u00a0 \u00a0 \u00a0 - --temp_location=gs:\/\/tmp\n\u00a0 \u00a0 \u00a0 \u00a0 - --runner=DataflowRunner\n\u00a0 \u00a0 \u00a0 \u00a0 - --experiments=use_runner_v2\n\u00a0 \u00a0 \u00a0 \u00a0 - --sdk_container_image=image:latest\n\u00a0 \u00a0 \u00a0 \u00a0 command:\n\u00a0 \u00a0 \u00a0 \u00a0 - python\n\u00a0 \u00a0 \u00a0 \u00a0 - -m\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor\n\u00a0 \u00a0 \u00a0 \u00a0 image: image:latest\npipelineInfo:\n\u00a0 name: hello-world\nroot:\n\u00a0 dag:\n\u00a0 \u00a0 tasks:\n\u00a0 \u00a0 \u00a0 input_data:\n\u00a0 \u00a0 \u00a0 \u00a0 cachingOptions:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enableCache: true\n\u00a0 \u00a0 \u00a0 \u00a0 componentRef:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 \u00a0 \u00a0 \u00a0 inputs:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 intValue: 1\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 intValue: 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringValue: CREATE_IF_NEEDED\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: custom-config\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringValue: 'null'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: input-table\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 intValue: 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringValue: WRITE_EMPTY\n\u00a0 \u00a0 \u00a0 \u00a0 taskInfo:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 inputDefinitions:\n\u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 custom-config:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 input-table:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\nschemaVersion: 2.1.0\nsdkVersion: tfx-1.12.0\n```\n\u00a0\nResult:\n\nBut what I'm expecting is something like this (this run is submitted through the ai platform python library):\n\nAs an alternative solution to get the above screenshot (the expected one) - I changed the yaml file as a POC to the following:\n\n```\n\ncomponents:\n\u00a0 input_data:\n\u00a0 \u00a0 executorLabel: input_data_executor\n\u00a0 \u00a0 inputDefinitions:\n\u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: INT\n\u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 outputDefinitions:\n\u00a0 \u00a0 \u00a0 artifacts:\n\u00a0 \u00a0 \u00a0 \u00a0 output_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifactType:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instanceSchema: 'title: tfx.String\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: object\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '\ndeploymentSpec:\n\u00a0 executors:\n\u00a0 \u00a0 input_data_executor:\n\u00a0 \u00a0 \u00a0 container:\n\u00a0 \u00a0 \u00a0 \u00a0 args:\n\u00a0 \u00a0 \u00a0 \u00a0 - --executor_class_path\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx_common.components.bigquery.executor.BigQueryComponentExecutor\n\u00a0 \u00a0 \u00a0 \u00a0 - --json_serialized_invocation_args\n\u00a0 \u00a0 \u00a0 \u00a0 - '{{$}}'\n\u00a0 \u00a0 \u00a0 \u00a0 - --project=projectid\n\u00a0 \u00a0 \u00a0 \u00a0 - --region=us-central1\n\u00a0 \u00a0 \u00a0 \u00a0 - --temp_location=gs:\/\/tmp\n\u00a0 \u00a0 \u00a0 \u00a0 - --runner=DataflowRunner\n\u00a0 \u00a0 \u00a0 \u00a0 - --experiments=use_runner_v2\n\u00a0 \u00a0 \u00a0 \u00a0 - --sdk_container_image=image:latest\n\u00a0 \u00a0 \u00a0 \u00a0 command:\n\u00a0 \u00a0 \u00a0 \u00a0 - python\n\u00a0 \u00a0 \u00a0 \u00a0 - -m\n\u00a0 \u00a0 \u00a0 \u00a0 - tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor\n\u00a0 \u00a0 \u00a0 \u00a0 image: image:latest\npipelineInfo:\n\u00a0 name: hello-world\nroot:\n\u00a0 dag:\n\u00a0 \u00a0 tasks:\n\u00a0 \u00a0 \u00a0 input_data:\n\u00a0 \u00a0 \u00a0 \u00a0 cachingOptions:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enableCache: true\n\u00a0 \u00a0 \u00a0 \u00a0 componentRef:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 \u00a0 \u00a0 \u00a0 inputs:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_large_results_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\u00a01\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_pre_computation_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\u00a00\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 create_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue: CREATE_IF_NEEDED\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 custom_config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: custom-config\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue: 'null'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 non_artifact_input_table:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 componentInputParameter: input-table\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 union_bq_shards_flag:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue: 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 write_disposition:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 runtimeValue:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 constantValue:\u00a0WRITE_EMPTY\n\u00a0 \u00a0 \u00a0 \u00a0 taskInfo:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: input_data\n\u00a0 inputDefinitions:\n\u00a0 \u00a0 parameters:\n\u00a0 \u00a0 \u00a0 custom-config:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\n\u00a0 \u00a0 \u00a0 input-table:\n\u00a0 \u00a0 \u00a0 \u00a0 type: STRING\nschemaVersion: 2.1.0\nsdkVersion: tfx-1.12.0\n```\n\u00a0\nand reran it however, I got this instead:\n\nIt ran successfully in this use case, but for other components we are expecting an `int` as `int` not `double`. For example we are running the chicago taxi pipeline as a template but we got the following error:\n\n```\n\nThe replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs:\u00a0https:\/\/console.cloud.google.com\/logs\/viewer?project=636071587074&resource=ml_job%2Fjob_id%2F1886953...\n\n```\n\nMy question here, is there a way in vertex when uploading our yaml file to pipeline template, we can specify the parameter type as well? if so how? if not, what are our alternate approach? (Note here, we are also using TFX to generate these artifacts)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"computational instances at the tool Vertex AI",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-18T04:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/computational-instances-at-the-tool-Vertex-AI\/m-p\/455213#M514",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"Hello,\nI have a question about the computational instances at the tool Vertex AI in the field of image classification. What are the underlying instances of the process or where can I find out? I'm looking for Information comparable to these syntax for example: Virtual Machine (CPU: Intel Xeon E5-2690 v3, 6 vCPUs, GPU: NVIDIA Tesla K80, 56 GB RAM, 380 GB SSD)\nThanks\nArndt",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Automl training process",
        "Question_tag_count":4,
        "Question_created_time":"2022-12-07T08:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Automl-training-process\/m-p\/497090#M918",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":83,
        "Question_body":"Hey folk,\nI'm using automl approach to train some classification text models. I had trained two models with the same data set and got a big difference in the performance metrics between the models. e.g. recall model 1: 0.80, recall model 2: 0.65.\nSo, I have a question about how the training process works, and if the model depends on a stochastic process (seed).",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Viewing model architecture",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-23T14:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Viewing-model-architecture\/m-p\/396608#M207",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hello! How can I interpret the feedforward NN model architecture described in AutoML logs (after training a model using AutoML in VertexAI)?\n\nI understand the base structure described in\u00a0https:\/\/cloud.google.com\/automl-tables\/docs\/logging\n\nBut I am not sure this describes the full architecture. For example, if num_neurons = 256, and num_layers = 2, how do I know how many neurons on each layer? Or for dropout = 0.5, in which layer is the dropout happening?\n\nAny sources your recommend that might explain this a bit better?\n\nThank you very much in advance for your help! I have been researching this and have found no clear explanation",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Event \"home-page-view\" not working",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-10T12:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Event-quot-home-page-view-quot-not-working\/m-p\/498253#M933",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"We set up the home-page-view event to create a recommendation system, and implemented the code on the site fitformula.ru via dataLayer according to the article https:\/\/cloud.google.com\/retail\/docs\/record-events We also added the Cloud Retail tag in Google Tag Manager \nand selected the dataLayer source. But it does not collect the main page visit. Why?\n\n<script>\nwindow.dataLayer = window.dataLayer || [];\nwindow.dataLayer.push({\n'cloud_retail': {\n'eventType': 'home-page-view',\n'visitorId': window.gaGlobal.vid\n}\n});\n<\/script>",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"error message when using vertex AI with jsonl file",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-28T10:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/error-message-when-using-vertex-AI-with-jsonl-file\/m-p\/607470#M2256",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"Hi,\u00a0\n\n\u00a0 I am using Generative AI studio to tune model, my jsonL file is like this:\n\n{\n\"input_text\": \"Create a description for Plantation Palms\",\n\"output_text\": \"Enjoy some fun in the sun at Gulf Shores.\"\n}\n\nwhen I submit the task, it said \"invalid data, missing one or more required fields.\"\n\nWonder what could be wrong?\n\nthanks, Helen",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Notebook deleting cells",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-30T08:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Notebook-deleting-cells\/m-p\/461480#M550",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":92,
        "Question_body":"Hello everyone, I have been facing an issue for the past few months where on occasion my vertex AI notebooks will completely wipe and delete all the cells in a .ipynb file. This happens at what appears to be random times.\u00a0\n\nContext to reproduce:\u00a0\n\nN96 High Memory instance 624 gb of ram\n\nIdle time: 1440\n\nsingle user only notebook\u00a0\n\nWhat happens: notebook with shutdown in the midst of running. Once the notebook is back up and running all the cells in the ipynb file are gone. There is no error message\u00a0\n\nIf anyone has faced this issue in the past and knows how to resolve I would really appreciate and information!\u00a0\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"reCAPTCHA IA",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-07T12:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/reCAPTCHA-IA\/m-p\/519498#M1242",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"Hello,\u00a0Captcha has an artificial intelligence model that allows it to learn to improve security scores and thus know when it asks for validation or not? my question is related to a user who always accesses from the same pc and the same ip to a site that has reCAPTCHA validation and always asks for validation, is it possible to avoid this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Advanced NLU v\/s Standard NLU",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-06T08:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Advanced-NLU-v-s-Standard-NLU\/m-p\/475233#M625",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":376,
        "Question_body":"Can someone help me understand when should we use Advanced NLU in Dialogflow CX?\n\nFrom my understanding, standard NLU is suitable during development as it has the auto-train ability and once we deploy the agent to Prod (let's assume its a different agent and not an environment), we can enable Advanced NLU as we don't expect any changes directly made to the prod agent.\u00a0\n\u00a0\nBut if we do this can we guarantee that the testing done for the dev agent with standard NLU is sufficient and there won't be any unwanted behavior?\n\u00a0\nIs Advanced NLU recommended for all the agents built in Dialogflow\u00a0CX or is there a specific criterion for choosing the NLU option?\nWhat are actual ML algorithms used to train in Standard v\/s Advanced NLU configuration?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"is there any list of brands\/logos supported by google vision api?",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-10T21:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/is-there-any-list-of-brands-logos-supported-by-google-vision-api\/m-p\/175425#M82",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":361,
        "Question_body":"is there anyplace i can see the list of brands\/logos that are currently supported by the google vision api's logo recognition service?",
        "Question_closed_time":"11-23-2021 11:41 AM",
        "Answer_score_count":0.0,
        "Answer_body":"The product team does not currently publish such a list. I recommend for you to submit a Feature Request to the Vision API product team.\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Make Google Translator Widget Language Labels in English rather than user's browser language",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-23T20:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-Google-Translator-Widget-Language-Labels-in-English-rather\/m-p\/525800#M1334",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":63,
        "Question_body":"I am using a google translator widget on my website. Script below.\n\nfunction googleTranslateElementInit() { new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE, autoDisplay: false}, 'google_translate_element'); }\n\nThe translation widget comes up and language switching works fine.\nBut the problem is that the text in google widget drop-down is getting loaded in the browser language.\nFor example, if I open the website from Netherlands\/Dutch, it will show\u00a0Engels\u00a0instead of\u00a0English\nWhat I need is to display the widget text always in English.\nI did try adding the\u00a0Pagelaguage\u00a0as 'en' but didn't work.\nNeed help to solve this.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Idle shutdown for user-managed notebook (vertex-AI)",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-02T07:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Idle-shutdown-for-user-managed-notebook-vertex-AI\/m-p\/428171#M366",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":954,
        "Question_body":"There are two types of notebooks in Vertex-AI\n\n1) managed notebook:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/managed\/introduction\n\n2) user-managed notebook:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/introduction\n\nI see that the former has a useful function called \"idle shutdown\" that\u00a0help manage costs:\u00a0managed notebooks instances shut down after being idle for a specific time period by default.\n\nWhy we didn't make it available for user-managed notebook as well? Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocAI - Form Processor table issue",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-23T22:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Form-Processor-table-issue\/m-p\/492493#M856",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":148,
        "Question_body":"Hello,\nI used Document AI form processor to convert pdf file table data into table object.\nSome table data are not converted properly.\n\nIn the sample file, the 3rd table is not detected columns properly.\nCould you please throw some light on this ?\n\nSource PDF file\n\nAfter conversion using form processor. The third one is having issue on column detection.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML tables - sample size of an average",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-25T06:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-tables-sample-size-of-an-average\/m-p\/397276#M210",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"Hi everyones\n\nI'm new to google automl tables and have a basic question about which data is worthwhile including in the training of my model.\n\nI have a dataset of golfers and will be looking at the averages of scores over different periods. For example, average over the past 3 months, 6 months, 1 year etc.\n\nMy question is, is it worthwhile also including the sample size for each date range for each player. For example, over the past 3 months, some players will have a sample size of 28 while some will only have 2. Those players that have 28 rounds will have more accurate averages than those with 2. However, I didn't know whether google automl tables would pick up this link automatically, whether I could create a different weighting\/reliability variable, or whether there's a way to specify a link between columns? Or if this automated type of automl isn't really suitable or just leave out that sample size variable?\n\nThanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"\u062a\u062d\u0633\u064a\u0646 \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0628\u062d\u062b",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-01T21:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/%D8%AA%D8%AD%D8%B3%D9%8A%D9%86-%D8%B9%D9%85%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A8%D8%AD%D8%AB\/m-p\/176885#M104",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"\u0627\u0633\u0639\u062f \u0627\u0644\u0644\u0647 \u0635\u0628\u0627\u062d\u0643\u0645\n\n\u0641\u064a \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u0628\u062d\u062b \u0641\u064a \u0627\u0644\u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u0634\u0627\u0626\u0639\u0629 \u0648 \u0627\u0644\u0623\u0633\u0626\u0644\u0629 \u0627\u0644\u062a\u064a \u062a\u062c\u0639\u0644 \u0627\u0644\u0634\u062e\u0635 \u0644\u0627 \u064a\u0639\u0628\u0631 \u0639\u0646 \u0633\u0624\u0627\u0644\u0647 \u0623\u0648 \u0645\u0648\u0636\u0648\u0639\u0647 \u0647\u0648 \u0639\u062f\u0645 \u0627\u0644\u0648\u0635\u0648\u0644 \u0625\u0644\u0649 \u0627\u0644\u0643\u062a\u0627\u0628\u0647 \u0627\u0644\u062e\u0637\u064a\u0629 \u0628\u0634\u0643\u0644 \u0635\u062d\u064a\u062d \u0623\u0648 \u0639\u0646\u062f\u0645\u0627 \u064a\u0633\u0623\u0644 \u0633\u0624\u0627\u0644 \u0644\u0627 \u064a\u0633\u062a\u0637\u064a\u0639 \u0634\u0631\u062d\u0647\u0627 \u0639\u0646 \u0637\u0631\u064a\u0642 \u0627\u0644\u0643\u0644\u0627\u0645)\u0627\u0642\u062a\u0631\u062d \u0639\u0646\u062f\u0645\u0627 \u064a\u062a\u0643\u0644\u0645 \u0627\u0644\u0628\u0627\u062d\u062b \u0639\u0646 \u0645\u0639\u0644\u0648\u0645\u0629 \u0623\u0648 \u0633\u0624\u0627\u0644 \u064a\u062a\u0643\u0644\u0645\u0647\u0627 \u0627\u0644\u0628\u0627\u062d\u062b \u0628\u0627\u0644\u0635\u0648\u062a \u0648\u0639\u0644\u0649 \u0637\u0631\u064a\u0642\u062a\u0647 \u0627\u0644\u0639\u0627\u0645\u064a\u0629 \u0648\u0627\u0644\u0643\u0644\u0627\u0645 \u0627\u0644\u0645\u062a\u062f\u0627\u0648\u0644 \u0639\u0644\u064a\u0647 \u0641\u064a \u0645\u0646\u0637\u0642\u062a\u0647 \u0648\u064a\u0643\u0648\u0646 \u0647\u0646\u0627\u0644\u0643 \u0627\u0634\u062e\u0627\u0635\u00a0 \u0645\u0646 \u0646\u0641\u0633 \u0627\u0644\u0645\u0646\u0637\u0642\u0629 \u064a\u0641\u0647\u0645 \u0644\u063a\u0629 \u0627\u0644\u0645\u062a\u0643\u0644\u0645 \u0648\u064a\u062c\u064a\u0628\u0647 \u0639\u0644\u0649 \u0627\u0633\u0627\u0633\u0647\u0627 \u0648\u064a\u0648\u062c\u062f \u0627\u0634\u062e\u0627\u0635 \u0643\u062b\u0631 \u0645\u062a\u0637\u0648\u0639\u064a\u0646 \u0641\u064a \u0646\u0634\u0631 \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0629 \u0648\u0627\u0644\u062e\u064a\u0631 \u0628\u0627\u0644\u0645\u062c\u0627\u0646 \u0633\u0648\u0627\u0621 \u0643\u0627\u0646\u062a \u0627\u0644\u0637\u0628 \u0627\u0648 \u0627\u0644\u0647\u0646\u062f\u0633\u0629 \u0627\u0648 \u0639\u0644\u0645 \u0645\u0639\u064a\u0646 \u0627\u0648 \u0627\u064a \u0639\u0644\u0645 \u0648\u0645\u0639\u0644\u0648\u0645\u0629\n\n\u0627\u0644\u0645\u062e\u062a\u0635\u0631 \u0639\u0646\u062f\u0645\u0627 \u0627\u062a\u0643\u0644\u0645 \u0645\u0646 \u0627\u0644\u062e\u0627\u062f\u0645 \u062d\u0648\u062c\u0644 \u0635\u0648\u062a \u0644\u0627 \u064a\u062a\u0643\u0644\u0645 \u0628\u0627\u0644\u0644\u063a\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0641\u0635\u062d\u0649 \u0648\u0644\u0643\u0646 \u064a\u0648\u062c\u062f \u0627\u0634\u062e\u0627\u0635 \u0643\u0627\u062f\u0645 \u062d\u0648\u062c\u0644 \u064a\u062d\u0644\u0644\u0648\u0646 \u0627\u0644\u0643\u0644\u0627\u0645 \u0628\u0644\u063a\u0629 \u0627\u0644\u0634\u062e\u0635 \u0627\u0644\u0645\u062a\u0643\u0644\u0645 \u0643\u0644\u064b \u062d\u0633\u0628 \u0645\u0646\u0637\u0642\u062a\u0647\n\n..\n\n\u0627\u062a\u0645\u0646\u0627 \u0648\u0635\u0644\u062a \u0627\u0644\u0645\u0639\u0644\u0648\u0645\u0629 \u0648\u0627\u0630\u0627 \u0628\u062f\u0643\u0645 \u0627\u062d\u0643\u064a\u0647\u0627 \u0635\u0648\u062a \u0648\u0627\u0634\u0631\u062d\u0647\u0627 \u0627\u0641\u0636\u0644 \u064a\u0627\u0631\u064a\u062a \u062a\u062e\u0628\u0631\u0648\u0646\u064a \u0648\u0627\u062a\u0648\u0627\u0635\u0644 \u0645\u0639\u0643\u0645",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google vertex AI support is terrible",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-22T20:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-vertex-AI-support-is-terrible\/m-p\/492001#M846",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":126,
        "Question_body":"I usually don't post things like this but I have been trying to work with Google support for Vertex AI for a while. It has been a month on a P2 ticket and no help or support so far. I guess I don't pay the big bucks to get Google's attention. So just wanted to post a warning that is helpful hopefully. Use Vertex AI at your own risk. If something fails you are on your own.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"When will Studio Voices move out of Preview?",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-11T08:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Studio-Voices-move-out-of-Preview\/m-p\/611180#M2338",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":73,
        "Question_body":"I'm very excited to to use Studio Voices in the text-to-speech API. The voice sounds a lot more realistic than wavenet. If the input is too large, I get this error:\n\nINVALID_ARGUMENT: Input size limit exceeded for Studio Voice. This is a temporary constraint while in Preview. Please try again with a shorter input or different voice type.\n\nWhen will this \"temporary constraint\" be lifted?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Whenever I access Gen App Builder, it redirects me to the welcome page.",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-24T09:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Whenever-I-access-Gen-App-Builder-it-redirects-me-to-the-welcome\/m-p\/596568#M2013",
        "Question_answer_count":1,
        "Question_score_count":3,
        "Question_view_count":105,
        "Question_body":"Whenever I access Gen App Builder, it redirects me to the welcome page.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DTMF updation through client sdk",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-22T23:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DTMF-updation-through-client-sdk\/m-p\/525382#M1318",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":72,
        "Question_body":"can we update DTMF values through client SDK from backend?. if yes how?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX ignores boolean values in WH responses",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-26T16:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-ignores-boolean-values-in-WH-responses\/m-p\/606675#M2231",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":38,
        "Question_body":"No sure why, but our webhook responses in dialogflow CX is not reading any boolean values. We know it is not our API since we have another system that is able to read the information. Is there anyone who knows why or how this can be fixed?\n\nCheers,",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"PyTorch is using the GPU on a container on my local machine, but is unable to use the GPU on Vertex",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-23T07:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/PyTorch-is-using-the-GPU-on-a-container-on-my-local-machine-but\/m-p\/458980#M523",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":227,
        "Question_body":"Expected Behavior\nI want to use a GPU on a component of Vertex AI.\n\nActual Behavior\nUnfortunately, `torch.cuda.is_available()` is returning `False`. Also, `nvidia-smi` is not working if ran in the container of Vertex AI.\nNote: both commands also don't work locally in the container if I'm not specifying the `--gpus all` flag in the command `docker run --rm -it --gpus all ee97db5bbd98 \/bin\/bash`. However, I can't find any option to add the `--gpus all` flag for Vertex AI. Would this be required?\n\nSteps to Reproduce the Problem\n\nMy YAML file:\n\nname: Processing\ndescription: Process all the found HTML\n\ninputs:\n- name: friendly_name\ntype: String\ndescription: The name of the company\n- name: language\ntype: String\ndescription: The language to process\n- name: models\ntype: String\ndescription: The models that will be used (all, genre, or standard)\nimplementation:\ncontainer:\nimage: eu.gcr.io\/uman-interns\/backend:v1.7\ncommand:\n[\npython,\nbackend\/pages\/III_Process_website_data\/process_website_data.py,\n--friendly_name,\n{ inputValue: friendly_name },\n--language,\n{ inputValue: language },\n--models,\n{ inputValue: models }\n]\n\n\u00a0\n\nMy pipeline:\n\nrom kfp.v2 import compiler, dsl\nimport kfp.components as comp\n\nfrom config import GCS_ARTIFACT_BUCKET, VARS\n\nprocessing = comp.load_component_from_file(\"scraping\/components\/processing.yaml\")\nembeddings = comp.load_component_from_file(\"scraping\/components\/embeddings.yaml\")\n\n\ndef compile_pipeline(file_name: str, tag: str):\n@dsl.pipeline(\nname=\"scraping\",\ndescription=\"Scrape a site and extract meaningful topics\",\npipeline_root=f\"gs:\/\/{GCS_ARTIFACT_BUCKET}\/scraping\/{tag}\",\n)\ndef pipeline(\nfriendly_name: str, url: str, language: str, google: bool, models: str) :\nPROJECT_ID = VARS[\"PROJECT_ID\"]\n\nprocess = (\nprocessing(friendly_name, language, models)\n.set_display_name(\"URL processing\")\n.set_env_variable(\"PROJECT_ID\", PROJECT_ID)\n.set_caching_options(enable_caching=False)\n.set_cpu_limit(\"4\")\n.set_memory_limit(\"16G\")\n.add_node_selector_constraint(\n\"cloud.google.com\/gke-accelerator\", \"NVIDIA_TESLA_T4\"\n)\n.set_gpu_limit(1)\n).after(crawling)\nembed = (\nembeddings(friendly_name, language, models)\n.set_display_name(\"Create embeddings\")\n.set_env_variable(\"PROJECT_ID\", PROJECT_ID)\n.set_caching_options(enable_caching=False)\n.set_cpu_limit(\"4\")\n.set_memory_limit(\"16G\")\n.add_node_selector_constraint(\n\"cloud.google.com\/gke-accelerator\", \"NVIDIA_TESLA_T4\"\n)\n.set_gpu_limit(1)\n).after(process)\n\ncompiler.Compiler().compile(pipeline, file_name)\n\n\nVisualized in the browser:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"load a .h5 trained model directly from GCS ?",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-21T03:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/load-a-h5-trained-model-directly-from-GCS\/m-p\/395442#M205",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":414,
        "Question_body":"Hello, it's the fist time I actually try to put in a production environment a locally trained .h5 model. I have a website hosted on a cloud run container and I'm trying to run an Image processing pipeline every-time a file is uploaded to GCS via the website (that's why I want to use a cloud function that triggers when a new file is created).\n\nmy issue:\n\nI have found a way to load my .h5 model from GCS but It's taking way too mush time and I'm sure there's surely a better way to do what i'm trying to do:\n\nalmost 1 minute to load on my local machine. Do you have any recommendation on how to trigger the prediction of my trained model + (pre\/post processing) easily upon file upload from my website (in a serverless context) ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Infinte long process",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-08T07:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Infinte-long-process\/m-p\/601274#M2103",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":61,
        "Question_body":"Hi,\n\nI sent an audio for transcription, and didn't get the result yet (should be in a bucket)\n\nthis is the process id: 7346104001135815711\n\nany ideas how I can debug it and find why it fails?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I would like to assign a custom domain to Vertex AI Endpoints.",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-05T22:16:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-would-like-to-assign-a-custom-domain-to-Vertex-AI-Endpoints\/m-p\/609623#M2308",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":64,
        "Question_body":"When I deploy a model to Vertex AI Endpoints, it is assigned a default domain ({region}-aiplatform.googleapis.com). How do I switch from the default domain to the custom domain?",
        "Question_closed_time":"07-13-2023 07:28 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@YoshinaoMori,\n\nWelcome back to Google Cloud Community!\n\nUnfortunately, this is not yet possible when you are creating an endpoint in vertex ai. However if you want this feature you can submit a request using this link:\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\nIf you are really looking for a way to assign a custom domain to the endpoint URL, you can try applying a reverse proxy that will accept the request from the client then it will forward the request to the endpoint URL.\n\nBut if you want to secure a connection for Vertex AI prediction service, It is possible to create a private endpoint for prediction, the prediction URI will look a little bit different from the Vertex AI public endpoint. Here is the format:\n\n\u00a0\n\nhttp:\/\/ENDPOINT_ID.aiplatform.googleapis.com\/v1\/models\/DEPLOYED_MODEL_ID:predict\n\n\u00a0\n\nYou can use this link as a guide on how to create a private endpoint:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints\n\nHope this is useful!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Endpoint in GCP",
        "Question_tag_count":3,
        "Question_created_time":"2022-10-14T12:37:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Endpoint-in-GCP\/m-p\/478360#M653",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":130,
        "Question_body":"In GCP I was deploying a model which obtain from training a dataset and after success full Vertex AI Model Registry. It takes too much time around 10 min to create endpoint for model. How can I reduce creation time when creating endpoints on GCP? What factors affected endpoint creation ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to advise Safe Search",
        "Question_tag_count":1,
        "Question_created_time":"2021-08-20T14:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-advise-Safe-Search\/m-p\/167893#M41",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":439,
        "Question_body":"I'm using Safe Search API in Cloud Vision to detect adult or harmful pictures and don't let them pass to be published in my project. But sometimes pictures are recognized wrong - not adult picture tagged so. Is it possible to \"teach\" Vision API, or mark a file as safe for my project?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Long audio files with Speech API - notify about errors",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-27T04:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Long-audio-files-with-Speech-API-notify-about-errors\/m-p\/606904#M2244",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":63,
        "Question_body":"I have long audio files, and as suggested here -\u00a0\n\nhttps:\/\/cloud.google.com\/speech-to-text\/docs\/async-recognize#upload_your_transcription_results_to_a_...\n\nit writes transcription results to a bucket. Then I use Storage Event triggers to invoke a Cloud Function when the operation is successful.\n\nBut I also want to invoke a different function when the transcription operation fails.\nWhat are my options?\nAre there events from Eventarc that can notify about failure of a long-running operation in Speech API?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex Batch prediction with BigQuery table as input. Predict API receives input without column name",
        "Question_tag_count":3,
        "Question_created_time":"2022-12-29T05:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-Batch-prediction-with-BigQuery-table-as-input-Predict-API\/m-p\/504368#M1001",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":252,
        "Question_body":"Hello All!\n\n\nWhen we use BigQuery as input for batch prediction, then my custom container predict API receives requests in the below format\u00a0\n\n\n{\n\u00a0 \u00a0\"instances\": [[1.23, \"xxx\", 379],[2.43, \"yyy\", 184]]\n}\n\n\nThere is no column name in the above example and that's why the request failed with the error \"missing column names\".\nBut it should be in the below format based on documentation,\n\n\n{\n\u00a0\"instances\": [\n\u00a0\u00a0\u00a0 {\n\u00a0\u00a0\u00a0\u00a0 \"feat1\": 1.23,\n\u00a0\u00a0\u00a0\u00a0 \"feat2\": \"xxx\",\n\u00a0\u00a0\u00a0\u00a0 \"feat3\": 379,\n},\n{\n\u00a0\u00a0\u00a0\u00a0 \"feat1\": 2.43,\n\u00a0\u00a0\u00a0\u00a0 \"feat2\": \"yyy\",\n\u00a0\u00a0\u00a0\u00a0 \"feat3\": 184,\n},\n]\n}\n\n\nHow can I fix it as I can't control the code which picks the data from the big query, converts it to the JSON and then sends it to my predict API?\nThanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Retail API: how can i retrieve\/export user events ?",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-10T06:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Retail-API-how-can-i-retrieve-export-user-events\/m-p\/610700#M2334",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":121,
        "Question_body":"I would like to export my all user events. Is there a way to retrieve all user events data with API ? I tried:\nimport_user_events\u00a0 but i get:\n\n\u00a0\n\nInvalidArgument: 400 Field \"inputConfig\" is a required field, but no value is found.",
        "Question_closed_time":"07-19-2023 10:49 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hello\u00a0@lsolatorio\u00a0,\u00a0I think my question was misunderstood. I would like to retrieve user events as csv, json or dataframe. And finally I was able to get the events using the recommendationengine API. Here is the code and reference page\u00a0:\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nfrom google.cloud import recommendationengine_v1beta1\nimport proto\nfrom google.oauth2 import service_account\nimport json\n# Create a client\n\ngbq_credentials = service_account.Credentials.from_service_account_file(\"credentials.json\")\nclient = recommendationengine_v1beta1.UserEventServiceClient(credentials=gbq_credentials)\n\nrequest = recommendationengine_v1beta1.ListUserEventsRequest(\n    parent=\"projects\/your_project_id\/locations\/global\/catalogs\/default_catalog\/eventStores\/default_event_store\",\n)\n# Make the request\npage_result = client.list_user_events(request=request)\n\n# Handle the response\nevents=[]\nfor i,response in enumerate(page_result):\n    print(i)\n    events.append(json.loads(proto.Message.to_json(response)))\n    \n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Cloud Video Intelligence API Still Relevant?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-26T07:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Video-Intelligence-API-Still-Relevant\/m-p\/606541#M2230",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"Hi, it seems like there's a big push towards Vertex as the future of Google's AI platform. This said, the cloud video intelligence API provides the functionality that I need for my project. Is it a bad idea to start a new project building on this platform? Are there any plans to sunset the service in the foreseeable future? I'm building a business around the app I'm creating, so it's important to know that Google will continue to invest in this service.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"A100 ram limitations",
        "Question_tag_count":2,
        "Question_created_time":"2022-07-07T10:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/A100-ram-limitations\/m-p\/439296#M407",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":121,
        "Question_body":"Can someone please help me understand why the best GPUs google offer (A100) have a fixed CPU RAM of 85GB (only 2x that of the GRAM) and all the other poorer GPU options can go over 300GB. It's terribly frustrating for large dataset training pipelines. Especially when mmdetection libraries don't work well on mutiple GPUs and would rather just use the 1",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Pipeline failed to deploy model: \"service_account cannot be specified for deploying AutoML models\"",
        "Question_tag_count":3,
        "Question_created_time":"2022-04-11T12:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Pipeline-failed-to-deploy-model-quot-service-account-cannot-be\/m-p\/412645#M265",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":185,
        "Question_body":"I made a pipeline that almost mirrors step 6 of\u00a0Intro to Vertex Pipelines\u00a0which has managed to get past every step up until the model deployment side of things. The code snippet for my model deploy op is here:\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nAnd the associated error message in the logs for the deployment part of the pipeline was:\n\ngcc_aip.ModelDeployOp(\n    model=training_op.outputs[\"model\"],\n    endpoint=endpoint_op.outputs[\"endpoint\"],\n    dedicated_resources_min_replica_count=1,\n    dedicated_resources_max_replica_count=1,\n    dedicated_resources_machine_type=\"n1-standard-4\",\n    service_account = \"vertex-notebooks@[redacted].iam.gserviceaccount.com\"\n    )\n\nRuntimeError: Failed to create the resource. Error: {'code': 400, 'message': 'service_account cannot be specified for deploying AutoML Models.', 'status': 'FAILED_PRECONDITION'}\n\n\u00a0\n\nDoes it have to do with a specific permission I need to give my service account? I don't know how to interpret this error.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do I enable Speaker Id on a free account",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-06T09:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-enable-Speaker-Id-on-a-free-account\/m-p\/609806#M2310",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":62,
        "Question_body":"I've created a trial GCP account and enabled DialogFlow CX.\u00a0 I am trying to access Speaker ID but this seems impossible.\u00a0 Tech support sends me to Sales who then sends me back to tech support.\u00a0 Has anyone successfully completed this process with a free account?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Bard isn't supported for this account",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-03T06:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Bard-isn-t-supported-for-this-account\/m-p\/599751#M2074",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":268,
        "Question_body":"I'm continually encountering an issue where I'm told \"Bard isn't supported for this account\", even though I was able to access it before with the aid of a VPN. I'm not utilizing Google Workspace, and my account has my credit cards and identification attached to it, verifying that I'm above 18 years of age. Using incognito mode hasn't resolved the issue, and the problem persists even after I set up a Workspace account correctly (Core Data Access Permissions included).\n\nDoes anyone have any insights into how I might regain access to Google Bard?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Checkbox in custom Form parser not working",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-10T00:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Checkbox-in-custom-Form-parser-not-working\/m-p\/542100#M1616",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":237,
        "Question_body":"hi, I am using custom Document Ai Processor to label and train on my document, but facing issue that every time it missed checkboxes. I have labelled 40 documents for train and 10 for test. Even after training the\u00a0 f1 score of check boxes are 0. Can anyone please help e out in this. Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What is the difference between vertexai.preview.language_models and google-generativeai?",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-24T15:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-is-the-difference-between-vertexai-preview-language-models\/m-p\/596702#M2016",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":397,
        "Question_body":"as asked in the title what is the difference between these two python packages? how can I get my hands on PaLM 2?",
        "Question_closed_time":"05-29-2023 02:54 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Good day\u00a0@morpheus,\n\nWelcome to Google Cloud Community!\n\n1.\u00a0The google.generativeai is more general and can be used to perform a variety of generative AI tasks such as generation of text, embedding and chat services, whereas the vertexai.preview.language_models is used to access the preview language models of PALM API in vertex AI. (E.g. migration from Azure OpenAi to Palm API).\nFor more information you can check this link:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/generative-ai\/migrate-from-azure\nhttps:\/\/developers.generativ...\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/generative-ai\/learn\/overview\n\n2. Please note that it is still in public preview and currently it is only available in the United States. If you want to have access to this public preview, you can use this link to register in the waitlist:\u00a0https:\/\/makersuite.google.com\/waitlist\nFor more information about the public preivew, you can check this link:\nhttps:\/\/developers.generativeai.google\/guide\/preview_faq\n\nHope this helps!\n\n\n\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"CX Phone gateway caller_id lost when forwarding to US-number",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-06T04:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/CX-Phone-gateway-caller-id-lost-when-forwarding-to-US-number\/m-p\/475112#M624",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":147,
        "Question_body":"Hello everyone\n\nWe use dialogflow cx with the cx phone gateway. But because we aren't based in the US we used a local phone number which we forward to the US-number from the gateway.\n\nThis worked great for some days, but now we are experiencing the following issue:\nThe caller_id from the original caller does not get sent in the webhook response anymore. It only sends the number which forwards the call.\n\nDoes someone know how to prevent this from happening?\n\nThanks for your help!\n\n- Federico",
        "Question_closed_time":"12-08-2022 06:14 AM",
        "Answer_score_count":0.0,
        "Answer_body":"looks like this is a problem with the local network\/phone provider, which does not transfer the caller_id correctly.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Python 3.10 Wheels for Cloud TPU VM",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-16T05:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Python-3-10-Wheels-for-Cloud-TPU-VM\/m-p\/500329#M956",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":198,
        "Question_body":"For running Cloud TPU VM workflows inside a docker container, I have the following two lines in the\u00a0Dockerfile:\n\nRUN pip install https:\/\/storage.googleapis.com\/cloud-tpu-tpuvm-artifacts\/tensorflow\/tf-2.11.0\/tensorflow-2.11.0-cp38-cp38-linux_x86_64.whl\nRUN curl -L https:\/\/storage.googleapis.com\/cloud-tpu-tpuvm-artifacts\/libtpu\/1.5.0\/libtpu.so -o \/lib\/libtpu.so\n\nI would like to get wheels for python:3.10. Ideally don\u2019t want to be building from source as that increases difficulty level significantly. Can developers maintaining the cloud-tpu-tpuvm-artifacts wheels please update the python versions. For reference here is a linked issue asked on the tensorflow forum",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Save audio file from speech to text stream",
        "Question_tag_count":2,
        "Question_created_time":"2022-03-03T07:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Save-audio-file-from-speech-to-text-stream\/m-p\/398993#M222",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":113,
        "Question_body":"I am using @Google-cloud\/speech for streaming audio from the browser to my nodejs backend.\nI would like to save the recorded audio.\nI see no option to do so. Any suggestions? Thanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Getting Error Deadline Exceeded when deploying model from Cloud Firestoer functions",
        "Question_tag_count":2,
        "Question_created_time":"2021-12-03T11:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-Error-Deadline-Exceeded-when-deploying-model-from-Cloud\/m-p\/177128#M105",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":0,
        "Question_body":"Hello,\n\nI currently have a firebase function that is set to deploy my AutoML tables model everyday at 5am. This has been working fine for the past month, up until the last week. I have been getting the following error below when the function attempts to deploy the model.\n\nError: 4 DEADLINE_EXCEEDED: Deadline exceeded\n\nI watched a google tutorial and it recommend to return a promise from my cloud function. That seemed to work for 1 day, but I received the error again this morning.\n\nI am going to try to implement a retry function, but I figured I would ask on here as well. Also, I am thinking that moving from autoML to VertexAI might help alleviate my issues. Any guidance here is helpful.\n\nSee below for my deploy model code:\n\n\u00a0\n\nconst deploy_model = () => {\n\n\nconst client = new automl.v1beta1.AutoMlClient(config);\n\n\n\/\/ Get the full path of the model.\nconst modelFullId = client.modelPath(projectId, computeRegion, modelId);\n\n\n\/\/ Deploy a model with the deploy model request.\nreturn client.deployModel({name: modelFullId})\n\n\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML forecast model batch predictions: \"rows with non-empty target values\" error",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-31T13:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-forecast-model-batch-predictions-quot-rows-with-non-empty\/m-p\/516165#M1157",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":292,
        "Question_body":"I'm trying to get a batch prediction from a time series forecasting model trained with AutoML. I'm seeing the following error in the BigQuery \"errors_validation\" table:\n\n\"There are rows with non-empty target values after this row. The time series has been excluded from predictions.\"\n(There are 7 such error messages, one for each time series. Each error message indicates \"01\/01\/2023\" as the timestamp.\u2020)\n\nI just can't see how what the error message is saying could possibly be true.\n\n- The granularity is weekly, and the forecast horizon is 26 weeks.\n- For the batch prediction, I'm using a CSV file which consists of all the data used in training, plus an additional 26 weeks of future timestamps appended to it. (To be precise, each of the 26 future timestamps appears 7 times - once for each time series.)\n- For each row with a future timestamp, the target column is empty (i.e., in the CSV there is nothing in the column whatsoever).\n- I also tried a batch prediction sourced from a BigQuery table. I created the table from the CSV file described above. I examined the table and confirmed that there are null values in the target column wherever there is a future timestamp. I got the same errors.\n\nI'm at a loss. Any help would be greatly appreciated.\n\n\u2020 I'm not sure if this is significant in any way, but the forecast horizon actually starts on 12\/04\/2022 (earlier than the timestamp indicated by the errors). In any case, there is no data in the forecast horizon with a non-empty\/non-null target value.",
        "Question_closed_time":"02-24-2023 07:12 AM",
        "Answer_score_count":0.0,
        "Answer_body":"I was able to get batch predictions after making the following changes to the data and training a new model:\n\nChanged the format of the timestamps from \"mm\/dd\/yyyy\" to \"yyyy-mm-dd\".\nEnsured that all numbers in the target column have decimals (\"0.0\" instead of \"0\", etc.). (Previously the numbers in this column were a mix of integers and decimal numbers.)\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"GCP Idle Model Charging",
        "Question_tag_count":5,
        "Question_created_time":"2022-11-07T04:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-Idle-Model-Charging\/m-p\/486604#M740",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":164,
        "Question_body":"Hello!\nI would like to deploy the ML Model into GCP.\n\nMost of the time the model will be sleeping. Sometimes I should use it through Endpoint for some seconds.\nI don't want to pay for full-time GPU instance and I need fast responses at the same time, without deployment from scratch everytime I need it.\n\nIs this possible in GCP ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI Labeling is Not Working (and there are numerous other glitches)",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-23T10:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Labeling-is-Not-Working-and-there-are-numerous-other\/m-p\/606083#M2222",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":24,
        "Question_body":"My primary problem is that when I attempt to do \"manual\" labelling of my Document AI documents, they are displayed in an incredibly blurry resolution that makes it impossible to label them. These same documents are actually high resolution and have all already had Document AI OCR conducted on them successfully separately.\u00a0\n\nIt makes no difference if I attempt to zoom. I have a paranoid suspicion that Google is doing this on purpose to try to force us to pay them for their labelling service.\u00a0\n\nI have tried refreshing, Chrome, Firefox, etc. Again, the documents themselves are good quality, plain image formats, etc. There are numerous other bugs with Document AI, several of which seem gearing towards forcing the user to incur higher unwanted billing:\n\n\u00a0 \u00a0 - Only the full bucket could be imported (850 images) not just the desired subset (50 images)\n\u00a0 \u00a0 - Once import started it could not be stopped\n\u00a0 \u00a0 - Manual and automatic labelling cannot be conducted but paid labelling is readily available\n\u00a0 \u00a0 - My task is to remove personal information from documents (de-identify), which is an incredibly common use case, yet it seems I have to re-invent the wheel here and no such pre-existing processing task can be found\n\u00a0 \u00a0 - I cannot search the files by filename to delete large subsets within the UI\n\u00a0 \u00a0 - There's no deduplication of identical images (which accounts for 50% of my images)\n\nI suppose it may just be a mountain of glitches and technical debt from the Google team, not all all-out conspiracy to incur higher bills, but they certainly choose to solve broken money-saving features last.\n\n======\n\nUPDATE\n\nIt seems the unwarranted blurriness is specific to the Train tab of the primary Document AI interface. Strange that they even have the labelling tool there given that it is thoroughly unusable. However, when I created the labelling task and assigned it to a pool consisting of only myself, it eventually emailed me a link to another LabelBox-like labeling view after some confusing configuration steps through the Manager interface. So finally, I can label.\n\nI think the aforementioned problems, bugs \/ glitches and missing features are still valid though, and I would add to them:\n\n\u00a0 \u00a0 - Sluggish updates within the Document AI interface to results from labelling tasks\n\u00a0 \u00a0 - Inability to balance testing and training samples within an assigned task",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"TPU initialization error while running training script",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-13T09:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/TPU-initialization-error-while-running-training-script\/m-p\/553165#M1895",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":84,
        "Question_body":"I am trying to train my pytorch model on a TPU pod v3-32 but it shows me the following error while running my training script:\n\n2023-05-13 16:35:49.648194: F tensorflow\/tsl\/platform\/statusor.cc:33] Attempting to fetch value instead of handling error UNKNOWN: TPU initialization failed: open(\/dev\/accel0): Operation not permitted: Operation not permitted; Couldn't open device: \/dev\/accel0; Unable to create Node RegisterInterface for node 0, config: device_path: \"\/dev\/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f7acf09500b,7f7acf09508f,7f79a4342bff,7f79a4646a26,7f79a...\n*** SIGABRT received by PID 317591 (TID 317591) on cpu 25 from PID 317591; stack trace: ***\nPC: @ 0x7f7acf09500b (unknown) raise\n@ 0x7f79a07c2a1a 1152 (unknown)\n@ 0x7f7acf095090 468830624 (unknown)\n@ 0x7f79a4342c00 400 tsl::internal_statusor::Helper::Crash()\n@ 0x7f79a4646a27 768 xla::PjRtComputationClient::PjRtComputationClient()\n@ 0x7f79a4629a72 1440 xla::ComputationClient::Create()\n@ 0x7f79a462bfe3 32 std::call_once<>()::{lambda()#2}::_FUN()\n@ 0x7f7acf0404df (unknown) __pthread_once_slow\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f7acf09500b,7f79a07c2a19,7f7acf09508f,7f79a4342bff,7f79a...\nE0513 16:35:49.741066 317591 coredump_hook.cc:414] RAW: Remote crash data gathering hook invoked.\nE0513 16:35:49.741084 317591 coredump_hook.cc:453] RAW: Skipping coredump since rlimit was 0 at process start.\nE0513 16:35:49.741108 317591 client.cc:278] RAW: Coroner client retries enabled (b\/136286901), will retry for up to 30 sec.\nE0513 16:35:49.741111 317591 coredump_hook.cc:512] RAW: Sending fingerprint to remote end.\nE0513 16:35:49.741116 317591 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket \/var\/google\/services\/logmanagerd\/remote_coredump.socket\nE0513 16:35:49.741129 317591 coredump_hook.cc:518] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\nE0513 16:35:49.741133 317591 coredump_hook.cc:580] RAW: Dumping core locally.\nE0513 16:35:49.817174 317591 process_state.cc:784] RAW: Raising signal 6 with default behavior\nAborted (core dumped)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using text recognition while also using object detection",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-16T21:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-text-recognition-while-also-using-object-detection\/m-p\/490329#M802",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":82,
        "Question_body":"Can you use visionAI text recognition while using object detection, or will they have to be two separate calls the Vertex AI?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI\/ML Research Paper Publish",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-31T08:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-ML-Research-Paper-Publish\/m-p\/484051#M705",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":39,
        "Question_body":"How should get into research about AI\/ML to get some international research intern offer?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Connect API to AutoML Model",
        "Question_tag_count":2,
        "Question_created_time":"2022-05-02T06:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Connect-API-to-AutoML-Model\/m-p\/419081#M299",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":137,
        "Question_body":"Hi, I already have the classification model created using AutoML, I have a Google Colab file in which I calculate the different features based from the information received from a public API\n\nWhat Google tool should I use to be able to connect Google Vertex to the API?\n\nWhat module \/ section of Vertex AI should I connect the API to in order to receive live data?\n\nTo which instance \/ section should I upload the Google Colab notebook with the calculations of the features?\n\nWhat tool should I use to connect the model created in AutoML to the feature data?\n\nFinally, if I wanted to export the predictions to an API, which tool should I use?\n\nThanks alot for any help you may provide",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How would you model a list of an unknown number of items in DialogFlow CX?",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-21T19:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-would-you-model-a-list-of-an-unknown-number-of-items-in\/m-p\/491605#M834",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":60,
        "Question_body":"Hi,\n\nTaking from the example at Dialogflow CX: Build a retail virtual agent\u00a0, if you were to build a shopping cart where users could add unlimited items to purchase. How would you model a solution for this?\n\nThat is, instead of having:\n\n\n\n$session.params.merch = \"T-shirt\"\n$session.params.size = \"M\"\n$session.params.price = 5\n\nCan we have something equivalent to:\n\n\n\n$session.params.order_items = [\n\u00a0 {\"merch\": \"T-shirt\", \"size\": \"M\", \"price\": 5},\n\u00a0 {\"merch\": \"T-shirt\", \"size\": \"XL\", \"price\": 10}\n]\n\n\nHow?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech-to-text in web application giving unexpected results",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-27T07:41:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-in-web-application-giving-unexpected-results\/m-p\/526612#M1347",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":223,
        "Question_body":"I am trying to implement Google Cloud's Speech-to-Text API in a web application, so users can speak into a microphone and see what they say in real time. I am using React.js on the frontend and Express.js in the backend. I am using the `microphone-stream` npm package to capture and stream user audio and the `websocket-stream` npm package to stream the audio through a web socket. Here is my source code on the frontend:\n\n\u00a0\n\nimport MicrophoneStream from \"microphone-stream\";\nimport webSocketStream from \"@httptoolkit\/websocket-stream\";\n\nfunction Test() => {\n  const mediaStream = useRef(null);\n  const micStream = useRef(null);\n  const webSocket = useRef(null);\n\n  const listen = async () => {\n    if (isRecording) {\n      micStream.current.stop();\n      setIsRecording(false);\n      return;\n    }\n\n    const sampleRate = 16000;\n\n    \/\/ get media stream\n    mediaStream.current = await navigator.mediaDevices.getUserMedia({\n      audio: {\n        deviceId: \"default\",\n        sampleRate: sampleRate,\n        sampleSize: 16,\n        channelCount: 1,\n      },\n      video: false,\n    });\n    setIsRecording(true);\n    micStream.current = new MicrophoneStream();\n    micStream.current.setStream(mediaStream.current);\n    micStream.current.on(\"data\", (chunk) => {\n      console.log(\"data received from mic stream\");\n    });\n    micStream.current.on(\"error\", (error) => {\n      console.error(error);\n    });\n    micStream.current.on(\"close\", () => {\n      console.log(\"mic stream closed\");\n      mediaStream.current.getAudioTracks()[0].stop();\n      setIsRecording(false);\n    });\n\n    webSocket.current = webSocketStream(\"ws:\/\/localhost:8000\/ws\/stt\", {\n      perMessageDeflate: false,\n    });\n    webSocket.current.on(\"data\", (data) => {\n      console.log(\"Data received:\", data);\n    });\n    webSocket.current.on(\"error\", (error) => {\n      console.log(error);\n    });\n    webSocket.current.on(\"close\", (error) => {\n      console.log(\"web socket stream closed\");\n    });\n\n    micStream.current.pipe(webSocket.current);\n\n    setTimeout(() => {\n      \/\/ micStream.current.unpipe(webSocket.current);\n      micStream.current.stop();\n    }, 3000);\n}\n\n\u00a0\n\nAnd here is how I handle it on the backend:\n\n\u00a0\n\nimport express from \"express\";\nimport { SpeechClient } from \"@google-cloud\/speech\";\n\nimport websocketStream from \"@httptoolkit\/websocket-stream\";\n\nconst router = express.Router();\n\nconst sttClient = new SpeechClient();\n\nrouter.ws(\"\/ws\/stt\", (ws, req) => {\n  console.log(\"Client connected\");\n\n  const recognizeStream = sttClient\n    .streamingRecognize({\n      config: {\n        encoding: \"LINEAR16\",\n        sampleRateHertz: 16000,\n        languageCode: \"en-GB\",\n        enableAutomaticPunctuation: true,\n      },\n      interimResults: true,\n    })\n    .on(\"error\", (error) => {\n      console.log(\"Error:\", error);\n    })\n    .on(\"data\", (data) => {\n      console.log(\"Received data:\", data);\n      console.log(\"transcript:\", data.results[0].alternatives[0].transcript);\n      ws.send(data.results[0].alternatives[0].transcript);\n    });\n\n  const wss = websocketStream(ws, { perMessageDeflate: false });\n  wss.pipe(recognizeStream);\n\n  ws.on(\"close\", () => {\n    console.log(\"Client disconnected\");\n    wss.end();\n  });\n\n  ws.on(\"message\", async (message) => {\n    console.log(\"Received message:\", message);\n  });\n});\n\n\u00a0\n\nThe data is sent through correctly, but I am getting very unexpected results. I keep getting a transcript that reads \"play\" or \"play radio\", even if I say nothing. This is an example response:\n\n\u00a0\n\nReceived data: {\n[0] results: [\n[0] {\n[0] alternatives: [Array],\n[0] isFinal: true,\n[0] stability: 0,\n[0] resultEndTime: [Object],\n[0] channelTag: 0,\n[0] languageCode: 'en-gb'\n[0] }\n[0] ],\n[0] error: null,\n[0] speechEventType: 'SPEECH_EVENT_UNSPECIFIED',\n[0] totalBilledTime: { seconds: '18', nanos: 0 },\n[0] speechAdaptationInfo: null,\n[0] requestId: '4181042299479530578'\n[0] }\n[0] transcript: Play radio.\n\n\u00a0\n\n\u00a0Am I approaching this correctly? Any help or advice would be greatly appreciated.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"NLP Classification Categories",
        "Question_tag_count":3,
        "Question_created_time":"2022-06-07T11:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NLP-Classification-Categories\/m-p\/429420#M370",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":159,
        "Question_body":"When using the NLP API and in particular the documents.classifyText, it will obviously be classified under one of the categories listed here. My question is, do we know what was used to create these categories? Were they created from different datasets\/corpora like Wikipedia, Gigaword, and Freebase? Does the Word2Vec term embedding relate to category embeddings at all? Any information, references or resources would be greatly appreciated.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI & MLOps Garage (Demos + Hands-on w\/ Prizes) - March 9th",
        "Question_tag_count":3,
        "Question_created_time":"2022-02-27T10:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-amp-MLOps-Garage-Demos-Hands-on-w-Prizes-March-9th\/m-p\/397728#M215",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":117,
        "Question_body":"Join Google Cloud Industry experts for a half-day dedicated to the possibilities of MLOps & AI. Dig deeper into how you can mature your current Machine Learning & AI practices. Not a Data Scientist or a ML Engineer? No worries! Let us help you get started with modern ML technologies like AutoML and ML w\/ SQL. End the day\u2019s learning by building an MLOps pipeline to automate data engineering, model training & model deployment.\n\n\u00a0\n\n\u00a0\n\n\u00a0Registration Link : https:\/\/inthecloud.withgoogle.com\/machine-learning-ai-garage-series\/register.html\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThrough conversations and hands-on workshops, you\u2019ll explore:\n\n\u00a0\n\n\u00a0\n\n\u00a0The newest AI and ML innovations, use cases, and best practices\n\n\u00a0How to build high-quality ML models with minimal effort\n\n\u00a0How to use automation to your advantage\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0Running AI and ML solutions both in the cloud and on-premises\n\n\u00a0\n\n\u00a0LinkedIn Event\u00a0\n\n\u00a0Registration Page",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Wrong OCR text detection.",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-19T01:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Wrong-OCR-text-detection\/m-p\/511605#M1087",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":197,
        "Question_body":"Hello All,\n\nI use this official playground site (https:\/\/cloud.google.com\/vision\/docs\/drag-and-drop) to try out the text detection, but the most important word is wrong.\n\nBALLANTINE\u00a0 to KALLANTINE\n\nHere are request details :\n\nurl :\u00a0https:\/\/vision.googleapis.com\/v1\/images:annotate\n\nrequest & response\u00a0\nhttps:\/\/gist.github.com\/chris-lee-lb\/1106a5c2af2a7118d694056d665b2bab",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"webhook timeouts debugging issue",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-02T06:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/webhook-timeouts-debugging-issue\/m-p\/516923#M1176",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hi there,\n\nWhen users are interacting with our agent it will randomly timeout during the conversation. There seems to be no immediate pattern to the timeouts, and it tends to occur at different points in the conversation flow, making it hard to debug.\u00a0\n\nCould anyone outline to me common reasons for agent\/webhook timeouts? And any steps I can take to resolve\/pinpoint the issue?\u00a0\n\nThank you in advance,\n\nVicky",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Documentai batch failure",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-26T22:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Documentai-batch-failure\/m-p\/537268#M1501",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":133,
        "Question_body":"Hi,\n\nI am new to Google cloud platform, but I am testing out the Documentai API in python. I created an invoice parser and had successfully processed a few dozen invoices sequentially, before trying the batch API following the documentation. However, the code sample provided doen't work as a batch. Here is the sample code provided.\n\nhttps:\/\/cloud.google.com\/document-ai\/docs\/samples\/documentai-batch-process-documents-processor-versi...\n\nI used the alternative approach, to specify entire directory prefix. The batch started successfully, but terminated after 3 invoices we processed.\u00a0\n\nIt then threw below error.\u00a0\n\nValueError: ('Iterator has already started', <google.api_core.page_iterator.HTTPIterator object at 0x000001D0E8F089A0>)\n\n\n\nHas anyone encountered this error while using batch processing with Documentai?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Token",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-21T11:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Token\/m-p\/614502#M2420",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":12,
        "Question_body":"El problema que tengo es el siguiente: En M\u00e9xico no se hizo cambio de hora por un tiempo, tengo un problema con la hora. Contamos con el servicio de token de acceso OAuth 2.0 para solucionar el problema es alargar el tiempo de caducidad del token, me podr\u00edan brindar alguna gu\u00eda para poder realizar este proceso por favor quedo atento a sus comentarios Saludos",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vision AI labels",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-12T13:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-AI-labels\/m-p\/422564#M327",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":79,
        "Question_body":"Where can I find list of all labels what could be detected in\u00a0Vision AI ?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Bard API access in Czech republic via Google Cloud Console",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-18T01:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Bard-API-access-in-Czech-republic-via-Google-Cloud\/m-p\/613198#M2390",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":26,
        "Question_body":"Hi, we are trying to enable Google Bard API in our Google Cloud Console but in the API library there is no Google Bard API available. Google Bard itself is available in the Czech republic since last week but in the API library we cannot find it. Is it because it is not available in the Czech republic yet?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI User Managed Notebooks all broken?",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-13T09:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-User-Managed-Notebooks-all-broken\/m-p\/612020#M2362",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":51,
        "Question_body":"We are unable to run our existing Jupyter notebooks, and get an error \"Restarting notebook does not have enough resources available to fulfill the request. Retry later or try another zone in your configurations\". We have tried in multiple different regions and zones.\u00a0\n\nWe just tried spinning up a new Jupyter notebook, and that failed as well. Are other people seeing these errors?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to control the segmentation and extraction of structured data using Document AI API OCR",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-10T11:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-control-the-segmentation-and-extraction-of-structured\/m-p\/610796#M2339",
        "Question_answer_count":2,
        "Question_score_count":3,
        "Question_view_count":245,
        "Question_body":"I am using Document AI API OCR, what I am trying to do is extract the text of the document in a formatted manner so that using the output I can use regex to get the result.\n\nFor example if a document has the fields like Registration Number: 12345, Name: XYZ both on seperate lines, I wanted to get the output in two lines.\n\nBut when I ask for the API to return the text \"Registration Number\" is on 1st line Name: on 2nd line then 12345 on 3rd and XYZ on 4th. Even if I can get 12345 on the 2nd line it will work out for me. How can I fix this segmentation on v1 of documentai.\n\nPlease help me out on how do I fix the segmentation of the output.\n\n@ErnestoC\u00a0\u00a0@kvandres",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Translate API",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-14T02:52:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API\/m-p\/489124#M772",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"Hi,\n\ni am use free trails of Google Cloud Translate send request on this URL\u00a0 https:\/\/translation.googleapis.com\/language\/translate\/v2 with API key with body row\n\nq:\u00a0 Discovering Supported Languages,\nsource: en,\ntarget:ar,\nformat:text\n\nget error response kindly see below\n\n\n\"code\": 403,\n\"message\": \"The request is missing a valid API key.\",\n\"errors\":\n\n\"message\": \"The request is missing a valid API key.\",\n\"domain\": \"global\",\n\"reason\": \"forbidden\"\n\"status\": \"PERMISSION_DENIED\"",
        "Question_closed_time":"11-15-2022 09:16 AM",
        "Answer_score_count":2.0,
        "Answer_body":"Hi, mjunaid,\n\nThe key you're using might not have the permission to use Translate APIs.\n\nTo fix this:\n\nGo to the Google Cloud Platform console\nChoose your project from the drop-down menu in the top bar\nGo to API & Services > Library\nSearch for Cloud Translation API and click on it\nEnable it\n\nGo to API & Services > Credentials\nSelect the key you are using in your Android App\nFrom the menu called Restrict key, choose Cloud Translation API\nSave your edit.\n\nNow the APIs should work properly.\n\nAdditionally, please note that the documentation\u00a0mentions that the structure of the HTTP method should be something like:\n\nhttps:\/\/translation.googleapis.com\/language\/translate\/v2?key=[yourAPIkey]&target=language\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"The DAG failed because some tasks failed. The failed tasks are: [exit-handler-1];....",
        "Question_tag_count":3,
        "Question_created_time":"2023-05-30T00:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-DAG-failed-because-some-tasks-failed-The-failed-tasks-are\/m-p\/598103#M2045",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":326,
        "Question_body":"Vertex ai text classification was interrupted with this error. What does it mean and how can I resolve it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"translate api may give different translation variations in response to the same request",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-27T05:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/translate-api-may-give-different-translation-variations-in\/m-p\/417588#M295",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":81,
        "Question_body":"I am using Google cloud translate api v3 NMT to translate sentences from English to Hebrew. I noted that for the same source sentence I may get slightly different results on subsequent calls. I see these going back and forth in a short time frame, so it is NOT a result of the model being updated. For my application I would like to either get all possible variations or at least get reproducible results. Is there an option to get all variations or to set the random seed? Or must I resort to multiple polling and caching on my side?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Troubleshooting POSTing to text-to-speech API from command line",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-03T13:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Troubleshooting-POSTing-to-text-to-speech-API-from-command-line\/m-p\/599786#M2081",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":58,
        "Question_body":"I'm looking to troubleshoot using the text-to-speech API. This is my first time using google cloud, so I had to signup, and I enabled billing, and I am able to get the speech synthesize service working through my browser with the cloud console.\n\nMy goal is to synthesize batches of text files with a python script. I'm on linux and have installed Google Cloud CLI, and I have been able to authorize using gcloud auth aplication-default login.\n\nI've been following these instructions:\n\nhttps:\/\/cloud.google.com\/text-to-speech\/docs\/create-audio-text-command-line\n\nWhen I execute the REST request, I get the prompt, \"Please enter content (application-www-form-urlencoded) to be POSTed:\" I have my request body saved as request.json, but when I execute the command to POST, I don't get any return. Am I missing a step, or can anyone help me troubleshoot? Thank you in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google cloud transcription Spanish not useful at all",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-09T02:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-transcription-Spanish-not-useful-at-all\/m-p\/541965#M1608",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":51,
        "Question_body":"The transcription I'm receiving with the\u00a0Google cloud transcription service in Spanish (MX and CO) languages are not useful at all. More than 40% of the text transcribed is wrong. I have used the\u00a0https:\/\/console.cloud.google.com\/speech\/transcriptions\u00a0url with the UI of\u00a0Speech-to-Text. Not useful at all, tbh. Is there any way of improve the result of this languages?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI - Slow Batch Predictions",
        "Question_tag_count":2,
        "Question_created_time":"2022-01-20T06:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Slow-Batch-Predictions\/m-p\/184803#M184",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":926,
        "Question_body":"Hi,\n\n\u00a0\n\nI've been running a Vertex AI Tabular batch prediction job for about 500k rows (50MB BQ table) for nearly 5 hours now, and I can't see any reference to how it's performing anywhere. Is there an estimate for how long this should take? Or where I should look for progress?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex pipeline model training component stuck running forever because of metadata issue",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-09T02:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-pipeline-model-training-component-stuck-running-forever\/m-p\/464631#M575",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":219,
        "Question_body":"'m attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.\n\nThe error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:\n\nRetryable error reported. System is retrying.\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=ABORTED, message=Specified Execution `etag`: `1662555654045` does not match server `etag`: `1662555533339`, cause=null System is retrying.\n\nI've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:\n\nprotoPayload.methodName=\"google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph\"\n\nprotoPayload.resourceName=\"projects\/724306335858\/locations\/europe-west4\/metadataStores\/default\n\nLeading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.\n\nI've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Translating streaming audio into text",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-11T14:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translating-streaming-audio-into-text\/m-p\/412679#M267",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"Hi, I'm using\u00a0@Google-cloud\/media-translation in node with express js server. I want to translate media file (\".wav\" format) with media-translation. At first, i got an error because of authentication and I fixed it with env variable as specified in documentation, I followed each and every step exactly told in the documentation but I'm getting no response from server. When i looked into APIs & Services tab it only recorded my failed auth attempts no other API calls are recorded. Please help because there is no help available online about this product and it doesn't even send error responses so i can debug.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"deploying model on vertex ai deploymentResourcePool to an endpoint located in another project.",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-26T00:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/deploying-model-on-vertex-ai-deploymentResourcePool-to-an\/m-p\/482309#M687",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":177,
        "Question_body":"I'am trying to deploy a custom trained model to a deployment resource pool that is located in project-1 to an endpoint located in project-2 , I have granted the editor role for project-1 to user account (u1) which also has editor role in project-2. when I try to deploy the model from user account (u1) ,I get the following error:\n\ngrpc_message:\"DeploymentResourcePool 'projects\/{project-1}\/locations\/us-central1\/deploymentResourcePools\/drlpool' does not exist.\n\n*the deployment resource pool (drlpool) exists and also deploys successfully if the endpoint and the deployment Resource Pool are in the same project.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google ML kit",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-10T08:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-ML-kit\/m-p\/452579#M498",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":153,
        "Question_body":"I know Google provides an ML kit supported by android that we can integrate into an app. The ML Kit provides many Vision and NLP APIs that can help us make our own Google-like Lens.\n\nAnyone can give me more information on how to get the ML kit?\n\nI am the CEO and I am looking for a CTO to my company, must be good in Python, A.I., Machine Learning, IoT and Robotics.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What's the training corpus of models behind GCP Natural Language APIs?",
        "Question_tag_count":3,
        "Question_created_time":"2022-11-17T14:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-training-corpus-of-models-behind-GCP-Natural-Language\/m-p\/490614#M807",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"Hi, where can I find some information about which datasets are used for training models that power the natural language APIs for sentiment analysis, entity extraction, etc.? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Calling speech-to-text suddenly giving me bad transcripts ( starting 2022-Dec-1)",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-01T18:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Calling-speech-to-text-suddenly-giving-me-bad-transcripts\/m-p\/495250#M890",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":122,
        "Question_body":"For several months I've been using S2T to transcribe mp3 audio files (1 - 40 minutes long). It's given great results and since I'm using the gcloud CLI I can script batches of submissions.\n\nToday I submitted 10 jobs totalling 40 minutes and the results are all junk. The JSON transcript files which are normally 50-300K in size are a few hundred bytes long and just consist of a handful of individual random words. One of the files I had run on Nov-11 and it gave a good result (230K JSON file of basically correct transcriptions.)\u00a0\n\nTo test this, I ran the same file through the \"Create Transcription\" GUI and it gave exactly the same correct result.\n\nI modified my gcloud call (which was \"gcloud beta ml speech ....\" to remove the \"beta\" option, and the submission failed on encoding=mp3. I then added back in the \"alpha\" option after gcloud, this accepted the mp3 encoding but again returned the defective JSON transcrption file.\n\nIt would really be a massive inconvenience to have to use the GUI to submit jobs one at a time.\u00a0\n\nI went to the S2T \"What's new\" page and it didn't make any reference that seemed to explain this. (Incidentally there is a bug there where if you click on the \"Speech-To-Text v1\" drop down and choose \"Speech-to-Text\" under Public Features, you actually end up at page titled \"Speed-to-Text V2\" with \"Speech-To-Text On-Prem\" above it, and no information on either one. )\n\nAny suggestions will be greatly appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Vision API reports (x,) in boundingBox when (x,y) is expected",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-19T00:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Vision-API-reports-x-in-boundingBox-when-x-y-is\/m-p\/554811#M1961",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":62,
        "Question_body":"I have this piece of response where (x,y) information for 4-points of the bounding box is expected but you'll see a point where only (x,) is reported.\u00a0\n\n\u00a0\n\n\"boundingBox\": {\n\"vertices\": [\n{\n\"x\": 81,\n\"y\": 1\n},\n{\n\"x\": 144\n},\n{\n\"x\": 144,\n\"y\": 9\n},\n{\n\"x\": 81,\n\"y\": 10\n}\n]\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Poor OCR results from PDF files compared to TIFFs",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-14T03:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Poor-OCR-results-from-PDF-files-compared-to-TIFFs\/m-p\/478060#M650",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":119,
        "Question_body":"Hi,\n\nWe're using DOCUMENT_TEXT_DETECTION in production to perform OCR on documents. We've found\u00a0 the quality of OCR of PDF documents compared to the exact same TIFF to be very poor (with missing characters, extra whitespace etc).\n\nI've attached an example test image in both PDF and TIFF formats. You can see the text is very legible and the OCR from the TIFF is 100% correct. The OCR from the PDF has multiple missing characters.\n\nThis leads me to believe that the internal rendering of PDFs performed by the cloud vision API is buggy.\n\nCan anyone shed any light?\n\nCorrect OCR results from TIFF:\n\nSTANDING ORDER PAYMENT\nTHANK YOU\nRETAIL PURCHASE\nINTEREST - PURCHASES\nSTANDING ORDER PAYMENT\nTHANK YOU\nRETAIL PURCHASE\nSTANDING ORDER PAYMENT\nTHANK YOU\n############\n############\n############\n############\n############\n############\nSantander UK plc. Registered Office: 2 Triton Square, Regent's Place, London NW1 3AN, United Kingdom. Registered Number 2294747. Registered in England. www.santander.co.uk. Telephone 0800\n389 7000. Calls may be recorded or monitored. Authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Our Financial\nServices Register number is 106054. Santander UK plc is also licensed by the Financial Supervision Commission of the Isle of Man for its branch in the Isle of Man. Deposits held with the Isle of Man\nbranch are covered by the Isle of Man Depositors' Compensation Scheme as set out in the Isle of Man Depositors' Compensation Scheme Regulations 2010. In the Isle of Man, Santander UK plc's\nprincipal place of business is at 19\/21 Prospect Hill, Douglas, Isle of Man, IM1 1ET. Santander and flame logo are registered trademarks.\nPage 14 of 19\n\nPoor read from PDF:\n\nSTANDING ORDER PAYMENT\nTHANK YOU\nRETAIL PURCHASE\nINTEREST PURCHASES\nSTANDING ORDER PAYMENT\nTHANK YOU\nRETAIL PURCHASE\nSTANDING ORDER PAY NT\nTHANK YOU\n############\n####\n###\n###\n###\n####\n####\n############\nSantander UK plc. Registered Office: 2 Triton Square, Regent's Place, London NW1 3AN, United Kingdom. Registered Number 2294747. Registered in England. www.santander.co.uk. Telephone 0800\n389 7000. Calls may be recorded or monitored. Authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Our Financial\nServices Register number is 106054. Santander UK plc is also licensed by the Financial Supervision Commission of the Isle of Man for its branch in the Isle of Man. Deposits held with the Isle of Man\nbranch are covered by the Isle of Man Depositors' Compensation Scheme as set out in the Isle of Man Depositors' Compensation Scheme Regulations 2010. In the Isle of Man, Santander UK plc's\nprincipal place of business is at 19\/21 Prospect Hill, Douglas, Isle of Man, IM1 1ET. Santander and flame logo are registered trademarks.\nPage 14 of 19\n\nSee missing hyphen, missing 'ME' from 'PAYMENT', and various lost hash\/pound characters with extra newlines.\n\nThe pdf and tiff can be found in this shared gdrive: https:\/\/drive.google.com\/drive\/folders\/1M4VZ3cT3YDoEn5o565fdWP6_47Y_KISL?usp=sharing\n\nHere's a screenshot of the PDF for ease:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Exempt few words within paragraph while translating through Google Translation clod API",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-13T08:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Exempt-few-words-within-paragraph-while-translating-through\/m-p\/477744#M647",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":84,
        "Question_body":"I have a use case where I need to translate biographies of some Doctors. I am using cloud API for translation but I want to exempt few words like degree or school of degree and some other specific terms and some html tags. How can we restrict those not to be translated. Glossary works only for exact match. How can I exempt words within paragraph?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can I see the models vertex AI AutoML has used to train my model?",
        "Question_tag_count":3,
        "Question_created_time":"2023-02-22T02:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-I-see-the-models-vertex-AI-AutoML-has-used-to-train-my-model\/m-p\/524954#M1310",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":144,
        "Question_body":"I have trained a few object detection models on vertex ai automl. is there a way for me to see what ML model has been used by vertex ai to train my data?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Empty pages array in Google Document AI API OCR response",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-28T05:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Empty-pages-array-in-Google-Document-AI-API-OCR-response\/m-p\/548320#M1753",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":110,
        "Question_body":"I'm currently using the Google Document AI API to extract text from PDFs using OCR. However, I've noticed that the pages array in the OCR response is always empty, even though the OCR operation completes successfully and I'm able to retrieve text from the document.\n\nHere's a simplified version of the code I'm using:\n\nfrom google.cloud import documentai_v1beta3 as documentai\n\n@classmethod\ndef extract_text(cls, book_link: str):\n    \"\"\"Extract text from book using OCR\"\"\"\n\n    # Upload the book to GCS\n    filename = cls._upload_file_to_gcs(book_link=book_link)\n\n    # Create the Batch Process Request\n    gcs_input_uri = f\"gs:\/\/{BUCKET}\/input\/{filename}\"\n    operation = cls._create_batch_process_request(gcs_input_uri=gcs_input_uri)\n\n    # Wait for the operation to finish\n    try:\n        operation.result(timeout=300)\n    # Catch exception when operation doesn't finish before timeout\n    except (RetryError, InternalServerError) as e:\n        raise exceptions.APIException(\n            detail={e.message}\n        )\n\n    metadata = documentai.BatchProcessMetadata(operation.metadata)\n\n    if metadata.state != documentai.BatchProcessMetadata.State.SUCCEEDED:\n        raise exceptions.APIException(\n            detail={metadata.state_message}\n        )\n\n    output_documents = cls._get_output_documents(metadata=metadata)\n\n    # Delete the input file from GCS\n    cls.gcs_bookmapping_bucket.delete_blob(blob_name=f\"input\/{filename}\")\n\n    # Extract text from the output documents\n    book_text = []\n    for document in output_documents:\n        for page in document.pages: # **here document.pages is always empty**\n            book_text.append(\n                cls._layout_to_text(layout=page.layout, text=document.text)\n            )\n\n\n    return book_text\n\n\u00a0\n\nThe document.text attribute contains the text of the entire document, but the pages array is always empty. This is preventing me from extracting text on a per-page basis, which is something I need for my application.\n\nI've double-checked the input PDF files to ensure that they have multiple pages, so I'm confident that the issue is not with the input data.\n\nI'm using documentai_v1beta3, I've also tried documentai_v1 but still it didn't work.\n\nHas anyone else experienced this issue with the Google Document AI API? Any suggestions for how I can retrieve text on a per-page basis?\n\nThanks in advance for your help.",
        "Question_closed_time":"05-04-2023 12:19 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Thanks. I noticed that my field mask was \"text\". I changed it to \"text,pages.layout\" and it worked.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"User level Gen AI restriction",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-18T04:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/User-level-Gen-AI-restriction\/m-p\/613219#M2391",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":25,
        "Question_body":"Hi All,\n\nI created a chatbot using generative AI model chat-bison with some business pdf data and created a customized UI.\n\nNow I want to restrict the user access. Like only few users can get response to the Project A\u00a0 Q&A in chat and other group can see project B document content in Q&A. How to bring user authorization concept into this?\n\nKindly help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Translate API - Does en_IN not support en yet?",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-26T23:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Translate-API-Does-en-IN-not-support-en-yet\/m-p\/537271#M1502",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"https:\/\/translation.googleapis.com\/language\/translate\/v2\n-d '{\"target\":\"en_IN\",\"source\":\"en\",\"format\":\"html\",\"model\":\"base\",\"q\":\"Free Tournament Ticket!\",\"q\":\"Congratulations!\"}'\n\nUpper request return error like below.\n{\n\u00a0 \"error\": {\n\u00a0 \u00a0 \"code\": 400,\n\u00a0 \u00a0 \"message\": \"Bad language pair: {0}\",\n\u00a0 \u00a0 \"errors\": [\n\u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \"message\": \"Bad language pair: {0}\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"domain\": \"global\",\n\u00a0 \u00a0 \u00a0 \u00a0 \"reason\": \"badRequest\"\n\u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 ]\n\u00a0 }\n}\n\nBut if source is \"ko\" or \"zh_CN\", it doesn't return error.\nex)\u00a0https:\/\/translation.googleapis.com\/language\/translate\/v2\n-d '{\"target\":\"en_IN\",\"source\":\"ko\",\"format\":\"html\",\"model\":\"base\",\"q\":\"\uc548\ub155!\",\"q\":\"\ucd95\ud558!\"}'\n\nFor now, en_IN is not on the support language list, (https:\/\/cloud.google.com\/translate\/docs\/languages?hl=ko).\nShould we wait until en_IN is on the supported language list,\nto be used in \"target\" with other supported language?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI data lost on VM stop",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-22T08:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-data-lost-on-VM-stop\/m-p\/445990#M442",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":348,
        "Question_body":"I am new to Vertex AI and wanted to try it out for a Kaggle competition. I was able to get a GPU machine up and running, as well as download the data to the machine. The download script was automatically generated when uploading my notebook to Vertex AI. I ran the script and 5 hours later all of the data was there successfully (to the boot disk -\u00a0 standard persistent disk with 1000 GB). I then ran a first iteration of my model and everything worked great. When I was done, I went back to GCP and stopped my VM, assuming all of my data would be saved. It was not!\n\nI then started over and once the data was on the machine I took a snapshot so I wouldn't have to redownload the data a third time. I then made some edits to my model and ran it again. After I was done, I again stopped my VM to not leave it running. All of the data was lost again, but less surprisingly this time.\u00a0\n\nI thought a snapshot could be used as a backup to the original machine, but the documentation makes it seem like it is only for creating a new VM from the boot disk. I then made a new machine but cannot figure out how to use it. I also tried looking for a way to make a new notebook on Vertex with the disk snapshot, but it did not look possible.\u00a0\n\nQuestions:\n\nWhy was my data deleted on stopping the VM? (not resetting or deleting the VM)\nHow can you back up data for Vertex\/how can you use that backup?\nHow can I use the VM created by my snapshot?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"gcloud ml speech recognize, option for setting phrase\/hint boost",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-21T03:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/gcloud-ml-speech-recognize-option-for-setting-phrase-hint-boost\/m-p\/545966#M1727",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"Hi,\n\nI am using the the gcloud ml speech recognize command line tool to test speech to text transcriptions. I am looking for a way to set the boost level for phrases\/hints that are supplied in the --hints argument - similar to what can be set in RecognitionConfig of the REST API.\u00a0\n\nhttps:\/\/cloud.google.com\/speech-to-text\/docs\/reference\/rest\/v1p1beta1\/RecognitionConfig#speechcontex...\n\u00a0\nI don't see anything in the gcloud documentation for this but wondering if there is a way to do it.\nhttps:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ml\/speech\/recognize\n\u00a0\nThanks,\nEoghan",
        "Question_closed_time":"04-24-2023 09:23 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi @eoghanoh,\n\nWelcome to Google Cloud Community.\n\nThe `gcloud ml speech recognize` command-line tool may not provide an option to set the boost level for hints or phrases. This is because the `gcloud ml speech recognize` tool uses the older `v1` version of the Speech-to-Text API, which doesn't support setting boost levels for hints or phrases.\n\nHowever, you can use the REST API directly to set boost levels for hints and phrases. You can do this by creating a RecognitionConfig JSON object with the appropriate boost levels, and passing it to the Speech-to-Text API via a POST request. You can use a program like `curl` or `httpie` to send a `POST` request to the Speech-to-Text API with this `RecognitionConfig`JSON object.To use the boost capability, you must activate the Speech-to-Text API's `v1p1beta1` version.\n\nHere are some documentations you may use as a reference:\nhttps:\/\/cloud.google.com\/speech-to-text\/docs\/quickstart-client-libraries#before-you-begin\nhttps:\/\/cloud.google.com\/speech-to-text\/docs\/adaptation-model?_ga=2.149903862.-1392753435.1676655686\nhttps:\/\/cloud.google.com\/sdk\/gcloud\/reference\/alpha\/ml\/speech\/recognize?_ga=2.149903862.-1392753435....\nhttps:\/\/cloud.google.com\/speech-to-text\/docs\/reference\/rest\/v1p1beta1\/projects.locations.phraseSets?...\nhttps:\/\/cloud.google.com\/speech-to-text\/docs\/best-practices-provide-speech-data?_ga=2.149903862.-139...\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Due to an error, Vertex AI was unable to train model \"some_model",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-09T07:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Due-to-an-error-Vertex-AI-was-unable-to-train-model-quot-some\/m-p\/476128#M632",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":145,
        "Question_body":"Hi Team\nWe are trying to train the model, but we are getting the below error after running 2 hrs.\n\nRegion\u00a0 \u00a0 \u00a0 \u00a0: us-centerl1(IOWA)\u00a0\n\nAlgorithm : AutoML\nObjective\u00a0 : Image classification (Single-label)\nData split:\u00a0 \u00a0Randomly assigned (80\/10\/10)\n\n\nDue to an error, Vertex AI was unable to train model \"some_model\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name:\u00a0\nprojects\/1096088445304\/locations\/us-central1\/trainingPipelines\/8154185764406558720\nError Messages: INTERNAL\n\nKindly help us to resolve the issue.\u00a0\nThanks & Regards\nJambu",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"import file (.csv) too big to import to vertex ai dataset",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-20T18:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/import-file-csv-too-big-to-import-to-vertex-ai-dataset\/m-p\/614227#M2413",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":42,
        "Question_body":"I have image dataset of receipts of about 12gb along with their label and bounding boxes (.json) of about 800 mb. I have uploaded the images dataset successfully to gcp bucket and created the cloud storage path in CSV. I then combined the path with the label and bounding boxes to create the input file in CSV. However, when I Uploaded the file (CSV) to gcp, I got an error that said my input file was too big to import, the limitation is\u00a0209715200 byte. My input file (CSV) is about 780 mb. How can I upload large input file to gcp?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI job run failure",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-21T06:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-job-run-failure\/m-p\/535079#M1448",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"We are streamlining our models workflow and as part of that I am building a pipeline for the ease of usability, I am using a custom container image for my component but when the job runs the component fails and returns an error thats its worked has exited, im unable to find the error since the logs dont specify what error is there exactly, I have verified running the component running it on a virtual machine however vertex ai does not work. Can",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Incremental ingestion into Feature Store",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-24T11:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Incremental-ingestion-into-Feature-Store\/m-p\/513484#M1129",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":54,
        "Question_body":"Hello evryone,\n\nHow to set up a incremental ingestion for online serving using Feature Store?\n\nI would like to use this flow:\u00a0\n\nQuery into The BigQuery with delta time\n\nIngestion into FeatureStore\u00a0 witho only this incremental values\n\nUsing the online serving with Incremental + Previous data into FeatureStore\n\nHow Can I?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About the extended model \"phone_call\" of Speech-to-Text",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-11T18:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-the-extended-model-quot-phone-call-quot-of-Speech-to-Text\/m-p\/441346#M409",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"I am using Speech-to-Text from a server application developed with Node.js using gRPC.\n\nWhen using the extended model phone_call in Japanese and recognizing it for a long time,\nThe voice recognition result of the intermediate result may be rewound, or if you think that it does not return for about 1 to 2 seconds, it may return at once in one sentence.\n\nI've incorporated the Speech-to-Text API into my C # app before, and I used the extended model phone_call as well, but I didn't see anything like this in my C # app.\nDo you know what is causing it?\n\nThe API setting value (Recognition Config) is the same for both Node.js and C #.\n\nThank you.\n\n(I am sorry if it is rude or inappropriate to you, since I use the online translation site.)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to use Recommendations AI(Retail API) for multiple stores?",
        "Question_tag_count":4,
        "Question_created_time":"2022-10-12T04:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-Recommendations-AI-Retail-API-for-multiple-stores\/m-p\/477225#M643",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"Hi Guys,\n\nWe are trying to build an e-commerce personalized recommendation system. we want to make it worthwhile for our clients.\u00a0 We are trying to use the recommendations API(retail API) for multiple e-commerce stores. But in the retail API, it seems we can use retail API under one project per store. Importing catalogs, creating models, and getting recommendations are only applicable to a single store under one project.\n\nOne solution is to create separate projects per store only to use retail API, which is not the right way for numerous customers.\n\nSo, is there any way to do this or any other GCP service that we can go for? Please suggest.\n\n\u00a0\n\nThanks in advance.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Bad Gateway error when using speech_v1 in a Django application that is running on nginx and waitress",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-08T11:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Bad-Gateway-error-when-using-speech-v1-in-a-Django-application\/m-p\/530239#M1400",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":52,
        "Question_body":"Hi everyone,\n\nI've been working on a Django project using Google's speech-to-text api. It works fine on my local as well as a standalone application. However, when I deploy it to the production that has nginx working with waitress, it doesn't work and gives Bad Gateway error and the nginx server stops running.\n\nThis is where it happens.\n\nfrom google.cloud import speech_v1\n....\n....\nclient = speech_v1.SpeechClient()\n\nHere is my nginx-waitress configuration:\n\n    #Finally, send all non-media requests to the Django server.\n    location \/ { \n    proxy_pass http:\/\/localhost:8080;\n    #proxy_set_header Host $host;\n    #proxy_set_header X-Forwarded-For $remote_addr;\n    }\n\nI tried with that $host and $remote_addr lines, still no luck.\n\nHere is the credentials json file that is working fine in both local django server and standalone application.\n\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"my_project_id\",\n  \"private_key_id\": \"my_private_key_id\",\n  \"private_key\": \"my_private_key\",\n  \"client_email\": \"my_client_email\",\n  \"client_id\": \"my_client_id\",\n  \"auth_uri\": \"https:\/\/accounts.google.com\/o\/oauth2\/auth\",\n  \"token_uri\": \"https:\/\/oauth2.googleapis.com\/token\",\n  \"auth_provider_x509_cert_url\": \"https:\/\/www.googleapis.com\/oauth2\/v1\/certs\",\n  \"client_x509_cert_url\": \"https:\/\/www.googleapis.com\/robot\/v1\/metadata\/x509\/speechtoblog-service%40yt2blog-377413.iam.gserviceaccount.com\"\n}\n\nIs there any other configurations that I need to do in the nginx server side? Or anything else? I would highly appreciate your help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cannot use listed and ready TPU from VM for pretraining",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-29T15:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-use-listed-and-ready-TPU-from-VM-for-pretraining\/m-p\/504543#M1002",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"I signed up for the TRC program for the third time in two years. Now I barely created a preemptible v3-8 TPU. Before that, I could efficiently allocate five non-preemptible v3-8 TPUs. Even with that allocation, TPU is listed as\u00a0READY\u00a0and\u00a0HEALTHY. However, when I want to access it from the pretraining script, I run into this error that I have never encountered before:\nFailed to connect to the Tensorflow master. The TPU worker may not be ready (still scheduling), or the Tensorflow master address is incorrect\nThe TPU is accessible, ready, and healthy, and the master URL is correct (it is automatically retrieved from the TPU_NAME, which I also double-checked).\n\nI also get this:\n\nAttempting refresh to obtain initial access_token\nRefreshing access_token",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech to text API not returning long running operation results - is service impaired?",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-02T12:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-API-not-returning-long-running-operation-results\/m-p\/528170#M1367",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":134,
        "Question_body":"The Speech to text API stopped returning results today. It was working properly yesterday evening (16 hours ago.\n\nI have tried it from python and node, both of which worked fine yesterday.\n\nI am using the same code and short audio (2 minutes) that I have been using.\n\nI am using the python speech_v1p1beta1 lib and have tried speech_v1 also.\n\nconfig = {\n    'encoding': 'FLAC',\n    'language_code': 'en-US',\n    'audio_channel_count': channel_count,\n    'sample_rate_hertz': sample_rate,\n    'enable_word_time_offsets': True,\n    'enable_separate_recognition_per_channel': False,\n    'enable_automatic_punctuation': True,\n    'use_enhanced': True,\n    'model': 'video'\n}\naudio = {\n    'uri': audio_uri\n}\nrequest = {\n    'config': config,\n    'audio': audio\n}\noperation = cls.client.long_running_recognize(request)\nresponse = operation.result(timeout=10000)\ntranscript = cls.build_transcript(response)\n\nI get the operation object but the results just timeout no matter what timeout I set\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"No more Wavenet for fr-FR lang",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-09T00:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/No-more-Wavenet-for-fr-FR-lang\/m-p\/391415#M198",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":108,
        "Question_body":"Hi,\n\nI have been using Google cloud API for text-to-speech to generate audio based on text for some days using the Wavenet voices and it worked great. The vast majority of my text is French and I have been using the fr-FR-Wavenet-C voice for it. I can't find it anymore. Even the page https:\/\/cloud.google.com\/text-to-speech\/ doesn't show up in the demo section. That's seems to be the case for all fr-FR-Wavenet voices. Have they been deleted?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Where is Visual Inspection AI?",
        "Question_tag_count":3,
        "Question_created_time":"2022-01-04T06:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Where-is-Visual-Inspection-AI\/m-p\/181914#M154",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":241,
        "Question_body":"Does anybody know where to get started with the Visual Inspection AI?\u00a0 It has been advertised for more than 6 months, but I cannot find where it is available.\u00a0\u00a0\n\n\u00a0\n\nThe landing page is here:\u00a0https:\/\/cloud.google.com\/solutions\/visual-inspection-ai\u00a0\n\n\u00a0\n\nHowever, it has never shown up in my Google Cloud Platform Console.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"PHP, google-api-vision and CURL",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-14T08:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/PHP-google-api-vision-and-CURL\/m-p\/543895#M1668",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":252,
        "Question_body":"HI,\n\nMy script for API VISION no longer works for me. Everything worked until last week.\nI use PHP and CURL, and I get error Status Code 400.\n\nThis is my code:\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n$url=\"https:\/\/vision.googleapis.com\/v1\/images:annotate?key=$apikey\";\n                        $detection_type=\"WEB_DETECTION\";\n                        \n                             \/\/ base64 encode image\n                             \/\/   $image=file_get_contents($url_immagine);\n                             \/\/   $image_base64=base64_encode($image);\n                        \n                                $json_request='{\n                                          \"requests\": [\n                                            {\n                                            \"image\": {\n                                                \"content\":\"'.$url_jpg.'\"\n                                              },\n                                              \"features\": [\n                                                  {\n                                                      \"type\": \"'.$detection_type.'\",\n                                                      \"maxResults\": 400\n                                                  }\n                                              ]\n                                            }\n                                        ]\n                                    }';\n                        \n                                $curl=curl_init();\n                                curl_setopt($curl, CURLOPT_URL, $url);\n                                curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);\n                                curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, false);\n                                curl_setopt($curl, CURLOPT_HTTPHEADER, array(\"Content-type: application\/json\"));\n                                curl_setopt($curl, CURLOPT_POST, true);\n                                curl_setopt($curl, CURLOPT_POSTFIELDS, $json_request);\n                                $json_response=curl_exec($curl);\n                                $status=curl_getinfo($curl, CURLINFO_HTTP_CODE);\n                                $curl_errno= curl_errno($curl);\n                                curl_close($curl);\n                        \n                                if ( $status!=200 ) {\n                                    die(\"Status code: $status <br> $curl_errno\" );\n                                }\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nIs the API url still valid? What has changed?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"dialogflow cx, how to obtain multiple answer from users",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-01T08:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/dialogflow-cx-how-to-obtain-multiple-answer-from-users\/m-p\/505122#M1010",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":90,
        "Question_body":"hello, I am trying to build a chatbot with dialogflow cx that allows user to enter multiple answers, for example:\n\nwhat is your favorite color ?\u00a0\n\nanswer to select\u00a0Black, Red, Green\n\ncurrently I have 3 suggestion chips built that allow user to click on each color,the problem I am running into is\n\n1. I tried looping back the user to the questions via route and condition like session.params.color !== null , but dialogflow wont let me do this cause the it thinks this will result in an infinite loop, what am I doing wrong?\n\n2. if the apporach above is not right, what is the best way to this problem?\u00a0\n\n\u00a0\n\nThanks in advance",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Authentication errors running vaictl in container",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-07T15:09:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Authentication-errors-running-vaictl-in-container\/m-p\/486888#M744",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":105,
        "Question_body":"I'm trying to run\u00a0vaictl on OSX inside a docker container based on these Vertex AI Vision instructions, but hitting the following auth error:\n\n\u00a0\n\nroot@dc89ced3ac47:\/# vaictl -p backyard-camera -l us-central1 -c application-cluster-0 -v list streams\nE1107 23:01:37.006251735     858 google_default_credentials.cc:434] Could not create google default credentials: {\"created\":\"@1667862097.001779783\",\"description\":\"Failed to load file\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/load_file.cc\",\"file_line\":72,\"filename\":\"\/root\/.config\/gcloud\/application_default_credentials.json\",\"referenced_errors\":[{\"created\":\"@1667862097.001778260\",\"description\":\"No such file or directory\",\"errno\":2,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/load_file.cc\",\"file_line\":45,\"os_error\":\"No such file or directory\",\"syscall\":\"fopen\"}]}\nE20221107 23:01:37.007882   858 resource_manager_app.cc:312] INVALID_ARGUMENT: Invalid credentials.; while listing streams; while listing streams\n\n\u00a0\n\nI've run gcloud auth login\u00a0in the container and saved the authorization code.\n\nAre there any extra steps needed to make this work?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to create model",
        "Question_tag_count":3,
        "Question_created_time":"2022-08-04T02:08:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-create-model\/m-p\/450348#M483",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":136,
        "Question_body":"I am working on demand forecasting where my timestamp duration is 15 minutes and i have attached sample output to below documents.The issue i am facing is despite setting DATA_FREQUENCY = [AUTO_FREQUENCY].ii am getting the error \"Invalid time series: the finest data frequency supported is PER_MINUTE. All input time intervals must be at least one minute\" and the query for create model is given below\n\n\u00a0\n\n\n\n\nCREATE\u00a0OR\u00a0REPLACE\u00a0MODEL\u00a0forecasting.cmc_model\nOPTIONS\n\u00a0\u00a0(model_type\u00a0=\u00a0'ARIMA_PLUS',\n\u00a0\u00a0\u00a0time_series_timestamp_col\u00a0=\u00a0'date',\n\u00a0\u00a0\u00a0time_series_data_col\u00a0=\u00a0'temperature',\n\u00a0\u00a0\u00a0DATA_FREQUENCY\u00a0=\u00a0'AUTO_FREQUENCY'\u00a0\n\u00a0\u00a0)\u00a0AS\nSELECT\u00a0EXTRACT(DATETIME\u00a0FROM\u00a0dateheure)\u00a0AS\u00a0date,\nvaleur\u00a0as\u00a0temperature\nFROM\u00a0`stunning-cell-345904.cmc_thermo.releves`\nWHERE\u00a0dateheure\u00a0>=\u00a0'2020-12-01'\u00a0and\u00a0dateheure\u00a0<=\u00a0'2021-02-28'\nand\u00a0idcapteur\u00a0=\u00a0323\nORDER\u00a0BY\u00a0dateheure;",
        "Question_closed_time":"08-08-2022 03:48 PM",
        "Answer_score_count":0.0,
        "Answer_body":"What is happening is that using \u201cAUTO_FREQUENCY\u201d is trying to send the Information as \u201cPER_MINUTE\u201d because of your data, and this needs to have an interval value per minute in each HOUR. You could try with \u201cHOURLY\u201d instead of \u201cAUTO_FREQUENCY\u201d, and it should work.\n\nInstead of:\nDATA_FREQUENCY = 'AUTO_FREQUENCY'\n\nUse \u201cHOURLY\u201d or any other DATA_FREQUENCY:\u00a0\n\nDATA_FREQUENCY = 'HOURLY'\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Text-to-Speech",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-25T02:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-Speech\/m-p\/425961#M359",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":80,
        "Question_body":"Hi every one, Google Text-to-Speech seems not to be working again",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error: Input sets do not have the same labels.",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-14T15:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-Input-sets-do-not-have-the-same-labels\/m-p\/499698#M942",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":174,
        "Question_body":"I have tried to train a model with over 50,000 images and 227 labels (with bounding boxes) I loaded the images using a csv file with each image left as UNASSIGNED for ML use.\u00a0\n\nEach time I try to train it, it gets to the 28 minute mark and errors out with\u00a0Training pipeline failed with error message:\u00a0Input sets do not have the same labels.\n\nI am at my wits end trying to work out what is the issue. I cannot find anyone else with the same issue (or I have found one, but there was no solution) and no matter what I do, it errors out at the same point (I have removed some labels etc)\u00a0\n\nDoes anyone know what the issue is (with more detail) so I can try to fix it? I assume it is talking about the\u00a0\n\nDataset and the Annotation set, but I cannot work out what is causing the issue.\n\u00a0\nI have followed all bounding box details, but to no avail. Any help would be appreciated. I am on the trial at the moment, so do not want to pay for support until I have",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document Ai to csv document",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-18T13:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-Ai-to-csv-document\/m-p\/554699#M1958",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":95,
        "Question_body":"Hi,\n\nI am using Document AI to extract data from a pdf file. However, the result is a json file. What can I do to get a csv file?\n\nThank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Max results in batched vision.ImageAnnotatorClient",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-18T07:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Max-results-in-batched-vision-ImageAnnotatorClient\/m-p\/544884#M1684",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":59,
        "Question_body":"Hi,\u00a0\n\nI am trying to set maxResults in batched vision.ImageAnnotatorClient as follows but such a parameter is not recognized.\u00a0\n\n\u00a0\n\n\u00a0\n\ncontents = loadImages(img_chunk)\n        client = vision.ImageAnnotatorClient()\n        requests = []\n        for content in contents:\n            image = {\"content\": content}\n            features = [\n                {\n                    \"type_\": vision.Feature.Type.LABEL_DETECTION,\n                    \"maxResults\": 50\n                },\n            ]\n            requests.append({\"image\": image, \"features\": features})\n\n        response = client.batch_annotate_images(requests=requests,)\n\n\u00a0\n\nWhat am I doing wrong?",
        "Question_closed_time":"04-18-2023 07:15 AM",
        "Answer_score_count":1.0,
        "Answer_body":"try `max_results` instead\n\n@magenti\u00a0wrote:\n\n\nmaxResults\n\n\u00a0\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex Matching Engine deny list tokens",
        "Question_tag_count":2,
        "Question_created_time":"2022-08-24T12:56:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-Matching-Engine-deny-list-tokens\/m-p\/459559#M528",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":277,
        "Question_body":"How does the Vertex matching engine deny list work?\n\nLet's say I have a class fruit which will ONLY have deny list tokens (no allow) such as \"apple\", \"mango\", etc. How do I filter out \"mango\" in the query (search all fruits except mango)? I have tried the following method but it does not work as expected:\n\njson\n{\"id\": \"1\", \"embedding\":[0.002792,0.000492], \"restricts\": [{\"namespace\": \"fruit\", \"deny\": [\"mango\"]}]}\n\nquery\ndeny_namespace = match_service_pb2.Namespace()\ndeny_namespace.name = \"fruit\"\ndeny_namespace.deny_tokens.append(\"mango\")\nrequest.restricts.append(deny_namespace)\n\nI have coded this similar to allow list which has worked for me but with deny tokens it does not seem to skip deny tokens even after completely overwriting the index.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"I want to build bard tools",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-13T05:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-want-to-build-bard-tools\/m-p\/553119#M1893",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":199,
        "Question_body":"Hello Google \n\nI'm one of the first 40 approved plugin developers at gpt and am currently in talks with 5 Fortune 200 companies to bring their site online as a plugin, having made functioning demos for all use cases I am a firm believer that this is the future of the web.\n\nAfter Google IO, I'm eager to build for Bard tools.\n\nPlease point me in the right direction.\u00a0\n\nMuch appreciated \ud83e\udd70\n\nAlexander",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI image classification models lose accuracy when being placed in a python dictionary",
        "Question_tag_count":5,
        "Question_created_time":"2022-10-26T10:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-image-classification-models-lose-accuracy-when-being\/m-p\/482475#M690",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":211,
        "Question_body":"I have made a\u00a0 model using vertex AI's image classification. Exported as EdgeTPU tflite model to my Raspberry pi 4 with Coral USB accelerator. When I used the Pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py\u00a0 to run my model, I get a perfect prediction result. But when I passed them to a python dictionary in my script, the prediction accuracy is way off.\u00a0https:\/\/github.com\/hillyuyichu\/Pycoral-python-API\/blob\/main\/pycoral_classification.py\u00a0\n\n\u00a0\n\n\u00a0\n\nHere is a screenshot of the prediction results on my python classification.py:\n\nThe label in row 1 is always the most active. The one in the last rows are the least active and most inaccurate.\n\nex: In picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to you implement Retail recommendation model?",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-09T08:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-you-implement-Retail-recommendation-model\/m-p\/508252#M1033",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"We are trying to implement a Retail recommendation model, in particular, the\u00a0Recommended for you model.\n\nI share here the docs for reference:\u00a0https:\/\/cloud.google.com\/retail\/docs\/reference\/rest\/v2beta\/projects.locations.catalogs.placements\/pr...\n\nAll the catalog is uploaded correctly, the model is trained and inside GCP in google Retail we get a prediction if we provide a cookie.\n\nOur problem is that we are trying to get the list of predictions\u00a0 of users with the new url:\n\n\"https:\/\/retail.googleapis.com\/v2beta\/YOUR_MODEL_LOCATION\"\n\nBut we have to provide the API Key and also the user has to be authenticated (OAuth).\n\nThis is an issue because we users can't authentica themselfs in order to get the predictions.\n\nA prior version that worked had a different url:\n\nhttps:\/\/recommendationengine.googleapis.com\/v1beta1\/\n\nThis one worked and no OAuth was needed, it returned a list\/array with the predictions for each user correctly. Sadly this website is going to be deprecated and we have to use the new one.\n\nHas somebody encountered the same issue or implemented the Retail models correctly in a website?\n\nThank you in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Custom Training Job with custom container failed with error 'Cant find specification for module...'",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-01T22:00:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Custom-Training-Job-with-custom-container-failed-with-error-Cant\/m-p\/516766#M1172",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":150,
        "Question_body":"Hi everyone,\n\nFor some reason, my custom training Jobs with custom container keep failing on vertex AI but the local run is working fine (I verified by running with local run as indicated in the docs here. I also built the image and run it manually and it works fine).\u00a0\u00a0\n\nError log when running custom job:\u00a0\n\n<code>\n\n{\n\"insertId\": \"2s7rqvfjzoq4v\",\n\"jsonPayload\": {\n\"attrs\": {\n\"tag\": \"workerpool0-0\"\n},\n\"message\": \"\/opt\/conda\/bin\/python: Error while finding module specification for 'trainer.train' (ModuleNotFoundError: No module named 'trainer')\\n\",\n\"levelname\": \"ERROR\"\n},\n\n<\/code>",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"A brief introduction to Convolutional Neural Network with Tensorflow",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-18T12:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/A-brief-introduction-to-Convolutional-Neural-Network-with\/m-p\/534073#M1439",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":34,
        "Question_body":"Dear Learners' Community,\n\nI hope you're having a good day. I've created a notebook on Kaggle to simplify CNN with tf. You can review it here:\n\nhttps:\/\/www.kaggle.com\/code\/eslamfouad\/introduction-to-convolutions-with-tensorflow?scriptVersionId=...\n\nRegards.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GCP Speech to Text Nodejs with GCP Cloud Storage Example",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-25T07:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GCP-Speech-to-Text-Nodejs-with-GCP-Cloud-Storage-Example\/m-p\/536941#M1486",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":62,
        "Question_body":"Just wanted to share this simple code to convert an audio file to text using just GCP Speech to text API and Google Cloud Storage.\u00a0 It takes in a .flac file and outputs a text file.\u00a0 You need to upload the .flac file to the Google Cloud first.\u00a0 Here is the GitHub Repo.\u00a0 Feel free to try it.\u00a0 You will need to create the\u00a0service account key with access to the Speech-to-Text API and Google Cloud Storage (in this case the files on GCP are set to public\n\nhttps:\/\/github.com\/jameslangdon1\/GCP-Speech-to-Text-with-Storage-Bucket-public",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to train Vertex AI: Generative AI studio on custom dataset?",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-25T23:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-train-Vertex-AI-Generative-AI-studio-on-custom-dataset\/m-p\/606423#M2227",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"How to input own dataset for generative ai?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Translate text with a glossary (unsupported languages)",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-03T05:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translate-text-with-a-glossary-unsupported-languages\/m-p\/528448#M1369",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":120,
        "Question_body":"Previously, in the list of supported languages (https:\/\/cloud.google.com\/translate\/docs\/languages), some languages were marked with an asterisk (*) and there was a note at the end of the list stating this: \u201c* You cannot use a glossary when translating to or from these languages. Glossaries aren't supported for these languages.\u201d\n\nNow that information has disappeared from the list. Does this mean all listed languages now support the glossary feature?\n\nThese were the languages listed as unsupported for this feature: \"Akan, Assamese, Aymara, Bhojpuri, Bambara, Kurdish (Sorani), Dogri, Divehi, Ewe, Guarani, Goan Konkani, Iloko, Krio, Ganda, Lingala, Mizo, Maithili, Manipuri (Meitei Mayek), Northern Sotho, Oromo, Quechua, Sanskrit, Tigrinya, Tsonga\"\n\nBR,\n\nJulian",
        "Question_closed_time":"03-03-2023 12:21 PM",
        "Answer_score_count":0.0,
        "Answer_body":"According to the latest release note, there are 24 languages added to the glossary. All the languages provided here are all supported languages, that is why there is no asterisk(*) indication anymore. Also, there are still languages that were not added. I suggest to just check the release notes every now and then for any new announcements.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Auto ML edge training failure",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-30T16:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Auto-ML-edge-training-failure\/m-p\/483746#M704",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"I am training an edge model in vertex AI. It is failing after a few hours. Details in the screenshot below. Tried 4 times and failed all 4 times. I cannot see to see any detail at all on the error. Can someone from Vertex AI please help? Training fails after about 3 hours if I pick the highest accuracy option but seems to process if I pick the 'best trade-off' options.\u00a0 Screenshot of the of the failed jobs below.\n\nScreenshot upload fails just like getting any support from Google.\u00a0 The training pipeline id is\u00a02116799302125748224",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Document AI fails for one particular image, else works great",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-02T13:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-fails-for-one-particular-image-else-works-great\/m-p\/419233#M301",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":163,
        "Question_body":"We are delivering a platform to a customer based on Document AI. The use case it to send a lottery ticket via API and return the structure information using Document AI. We tried for several hundred images and the Document AI OCR worked great (95%+ times captured right string, only errors were line feeds and Q turning into O etc. that we could resolve using a post-processor). But for one set of images (from DC), the OCR fails miserably.\u00a0 This is a corner case that seems to throw the Document AI engine off the mark.I will appreciate greatly if anyone can help explain it.\n\nSee one particular image which is the most problematic.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"When will Hebrew language be available in Text-To-Speech API?",
        "Question_tag_count":1,
        "Question_created_time":"2022-05-18T06:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Hebrew-language-be-available-in-Text-To-Speech-API\/m-p\/424088#M350",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":191,
        "Question_body":"Hi everyone,\nWhen will Hebrew language be available in Text-To-Speech API?\nin this list\nhttps:\/\/cloud.google.com\/text-to-speech\/docs\/voices\n\u00a0\nthanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Im implementing logistic regression ,but somehow this code not seems to be working..please help me.",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-03T00:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Im-implementing-logistic-regression-but-somehow-this-code-not\/m-p\/517491#M1185",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":104,
        "Question_body":"This is the python code for logisticRegression using Preceptron Trick but not working.. im having doubt in finding coefficients..please check and correct the code.\n\n\u00a0\n\n\u00a0\n\nclass LogisticRegression:\n    def __init__(self,learning_rate=0.01,epochs=1000):\n        self.epochs=epochs\n        self.learning_rate=learning_rate\n        self.coef_=0\n        self.intercept_=0\n    \n    def fit(self,x,y):\n        x=x.copy()\n        y=y.copy()\n        import numpy as np\n        x.insert(loc=0,column='ones',value=np.ones(x.shape[0]))\n        w = np.ones(x.shape[1])\n        for i in range(self.epochs):\n            j=np.random.randint(0,x.shape[0])\n            y_hat = self.step(np.dot(x.iloc[j,:], w))\n            w=w+(y.iloc[j,]-y_hat)*x.iloc[j,:]\n        \n        self.coef_=-w[1:x.shape[1]]\/w[x.shape[1]-1]\n        self.intercept_ = -w[0]\/w[x.shape[1]-1]\n        return w[0],w[1:]\n    def step(self,v):\n        return 1 if v>0 else 0\n\n    def predict(self, X_test):\n        import numpy as np\n        y_pred = np.dot(X_test,self.coef_) + self.intercept_\n        return y_pred",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Retail product catalog not sync with Merchant Center after initial import",
        "Question_tag_count":1,
        "Question_created_time":"2022-07-17T15:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Retail-product-catalog-not-sync-with-Merchant-Center-after\/m-p\/443901#M426",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"I followed the documents to import product catalog from Merchant Center Sync. It seems to work at the beginning - but I recently notice it has not synced for last seven days. There is no error I can see that explains why.\n\n- There is a warnig on the Product Catalog integration page saying Last import is more than 1 week old.\n\n- If I click Import button, select merchant account and branch 0 . It says The branch already has a data source. But it is not syncing at all.\n\nI have daily new products added to merchant center so this is serious issue for me as many events become unjoined and recommendations on those new products are all messed up.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Capture a parameter from an annotated training phrase",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-10T01:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Capture-a-parameter-from-an-annotated-training-phrase\/m-p\/508526#M1040",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":86,
        "Question_body":"When the user input is\n\n\u00a0\u00a0\u00a0 \"I want to order a new project called SalesPitch\"\n\nI want Dialogflow CX to determine that the intent is to order a new project, and also to capture the text \"SalesPitch\" in a parameter called projectName. I created a custom entity type for the project name, a regex, called projectNameText,\u00a0 and created a training phrase:\n\n\u00a0\u00a0\u00a0 \"I want to order a new project called Annabel\"\n\nand I annotated the training phrase so that Annabel was labelled as a projectNameText entity, all in the DialogFlow-CX console, as described in the documentation.\u00a0But that instantly creates a parameter with Parameter id projectNameText. And I cannot edit that Parameter id. I can't require that when Dialogflow matches that training phrase and extracts an entity of type projectNameText, it puts it into the parameter projectName. Dialogflow demands that it goes into a parameter called projectNameText, or projectNameText1 if there is more than one. When I run the simulator and type input that matches that training phrase, Dialogflow does indeed correctly extract the entity, but will only create a parameter named projectNameText - I can see the name and value in the simulator.\n\nThis StackOverflow answer implies that I can send the matched entity into any parameter I want. That would be sensible. But how do I do it? I can't find any way to edit the parameter name in the Intent editor. All it gives me is this:\n\nand I cannot change the Parameter Id.\n\nI must be missing something really basic. Hints, please?\n\nI asked this same question on StackOverflow but very few people read it so I am trying again here.",
        "Question_closed_time":"01-17-2023 07:58 AM",
        "Answer_score_count":0.0,
        "Answer_body":"It is indeed not possible to edit the Parameter Id in the Intent Editor directly.\n\nInstead, in the Intent Editor from the `Build` tab, accept the default Parameter Id, and `Save` the modified intent.\n\nThen go to the tabs on the left of the Dialogflow CX console and choose the `Manage` tab. Choose `Intents` from the menu and find the intent you have just edited from the menu. Click the intent name to be given a different version of the Intent Editor. Same fields, same data, different functionality. In this different Intent Editor, click the Parameter Id you want to edit. It is now editable. Do not forget to hit `Save` after editing it.\n\nFrom start to end, that took thirteen days to find. I posted here and received no answer. I finally subscribed to Google Cloud paid support and raised a support case and was given the answer in a video call with Google India. Perhaps it should be in the Google Documentation.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Video Intellence API - Calculation Error - Request task has error from parents.",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-04T04:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Video-Intellence-API-Calculation-Error-Request-task-has-error\/m-p\/550210#M1787",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":199,
        "Question_body":"Hello All,\n\nIn our team we have been using video intelligence API for speech transcription.\u00a0\n\nThe API was running fine for about a year now. However lately we have been getting below error -\u00a0\n\nannotation_results {\n  input_uri: \"\/prd_gcpvideo\/video.mp4\"\n  error {\n    code: 13\n    message: \"Calculator error.\"\n    details {\n      type_url: \"type.googleapis.com\/google.rpc.Status\"\n      value: \"\\010\\t\\022$Request task has error from parents.\"\n    }\n    details {\n      type_url: \"type.googleapis.com\/google.rpc.Status\"\n      value: \"\\010\\t\\022&Operation task has error from parents.\"\n    }\n  }\n}\n\nThe error is intermittent that means for about 20 tries to video intelligence API about 15 fails and another 5 are successful in giving transcripts.\u00a0\n\nThe videos that we process are of around 1.4 Gb.\u00a0\n\nDoes anyone know where might be the problem ?\u00a0\n\n\nThis are the things that I have already tried without success\u00a0 -\n\n\n\nupdating video intelligence to latest version",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI\/ML",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-29T00:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-ML\/m-p\/504317#M999",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"hello. could you please tell me which language is best for AI\/ML?\u00a0 and how could I start learning?",
        "Question_closed_time":"12-30-2022 12:14 PM",
        "Answer_score_count":1.0,
        "Answer_body":"You may start on the lists of AI and Machine Learning Products that Google Cloud has to offer. Languages\/Client libraries available for these products are Go, Node.js, Python and many more. Quickstarts are available on the documentation of each product and may serve as a guide as you navigate your way on AI and Machine Learning.\n\n\u00a0\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Thanks",
        "Question_tag_count":0,
        "Question_created_time":"2021-07-25T10:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Thanks\/m-p\/164694#M23",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":16,
        "Question_body":"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google TTS can't convert text longer than ~500-600 characters when using Neural2 voices",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-03T03:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-TTS-can-t-convert-text-longer-than-500-600-characters\/m-p\/517555#M1189",
        "Question_answer_count":8,
        "Question_score_count":1,
        "Question_view_count":0,
        "Question_body":"According to Google TTS documentation, the Speech Synthesis Limit is 5000 bytes per request -\u00a0https:\/\/cloud.google.com\/text-to-speech\/quotas.\n\nHowever, when I use Neural2 voice (for example \"es-US-Neural2-A\"), I can't even convert 600 bytes per request. Instead, I receive an error message:\u00a0google.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument.\u00a0\n\nConverting text < 500 bytes works fine, so that's definitely not an \"invalid argument\" issue.\n\nIt looks like an issue on Google's side, but please let me know if you have any ideas on how to fix it.\n\nThank you",
        "Question_closed_time":"02-03-2023 09:06 AM",
        "Answer_score_count":1.0,
        "Answer_body":"There is already an ongoing internal bug for this. For the meantime, possible workarounds would be, converting the text into <500 bytes as you mentioned or send the request into smaller pieces.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Regarding GCP Cloud Vision and Functions",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-30T04:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Regarding-GCP-Cloud-Vision-and-Functions\/m-p\/598143#M2048",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":44,
        "Question_body":"I have a simple setup where a GCP Function gets triggered when a pdf is uploaded to a bucket. Once the file is uploaded the Function calls the GCP Vision API AsyncBatchAnnotate for every 5 pages in the PDF. I had followed the\u00a0https:\/\/cloud.google.com\/vision\/docs\/pdf\u00a0tutorial. Even though only 22 pdfs have been uploaded with close to 4200 pages. According to the pricing I should be only charged for about a $1.5 * 4.2 = 6.3$ ( 1.5$ for every\u00a0 1000 units). The charges have come much more than expected for GCP Vision API. Can anyone explain why my GCP Vision Charges are much more than the above mentioned value. Close to 200X the 6.3$ value.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI endpoint updating model version",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-09T19:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-endpoint-updating-model-version\/m-p\/542061#M1611",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":102,
        "Question_body":"Hi, currently I'm working on vertex ai pipelines using kfp. I'm confused on how to update my model version on the same endpoint(not updating model version on model registry) using python script, is there a way to do it? Thanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Comment avoir l\u2019api Bard",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-18T15:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Comment-avoir-l-api-Bard\/m-p\/613457#M2394",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":32,
        "Question_body":"Je voudrais avoir l\u2019api bard pour une petite application mobile",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX - Telephony Transfer does not work with Voicemail",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-05T11:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-Telephony-Transfer-does-not-work-with-Voicemail\/m-p\/507030#M1024",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"Whenever I transfer to a voicemail number, it does not work. It could be an infinite loop where I transfer to the number and the VOIP system transfers back to the dialogflow cx number but I am not sure.\u00a0\n\nThis severely limits our customers who already have a voicemail system set up.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cannot increase machine type on AI Platform Prediction",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-07T08:05:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cannot-increase-machine-type-on-AI-Platform-Prediction\/m-p\/600914#M2094",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":148,
        "Question_body":"We are running GPU-based online predictions on our AI Platform.\nWe are applying auto-scaling, but wanted to improve the machine type because the CPU load is higher than the GPU.\n\nThe original machine type we were using was n1-standard-4 and we tried to raise it from there to n1-standard-8 or higher, but we got an error on versions create.\nThe details of the error are below.\n```\n\"error\": {\n\"code\": 429,\n\"message\": \"The requested number of n1-standard-8 exceeds the quota limit. Current usage\/limit: 0\/20, Requested: 24.\",\n\"status\": \"RESOURCE_EXHAUSTED\"\n}\n```\nI thought I could apply to increase the quota, but I don't know which quota it corresponds to.\n\nOtherwise, I tried the following\n1. if minNodes=1 in n1-standard-8, I was able to deploy, but could not find the resource that was being used more. 2.\n2. we were able to create a new n1-standard-16 (with GPU) in Compute Engine.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Talent Solution - Orphaned companies",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-24T16:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Talent-Solution-Orphaned-companies\/m-p\/526050#M1341",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":67,
        "Question_body":"I have been experimenting\/testing with Talent Solution, but hit a (not so interesting) problem.\n\nIt seems to be possible to delete a tenant even while it contains active companies. Unfortunately, once you do that, it seems to be impossible to get a handle back to the company. If I try to get it directly (GetCompanyRequest), I get an error that the tenant does not exist. I know the company is not cleaned up after running an export on the \"Jobs and companies\" summary page which is showing a bunch of companies I am not expecting...\n\nIs there a way to clean up the companies? \n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Translation Permission",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-15T07:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Translation-Permission\/m-p\/489632#M785",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":0,
        "Question_body":"So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:\n\n [error]  {\n    \"message\": \"Cloud IAM permission 'cloudtranslate.generalModels.predict' denied. \",\n    \"code\": 7,\n    \"status\": \"PERMISSION_DENIED\",\n    \"details\": []\n} \n\nI have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org.\u00a0\n\nI did verify that the service account has the permission from the\u00a0https:\/\/console.cloud.google.com\/iam-admin\/troubleshooter\u00a0so and that my organization's admin sees that the service account is granted access through ancestor policies.\u00a0\u00a0\n\nSo what else can we check?",
        "Question_closed_time":"11-15-2022 08:30 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"VERTEX PIPELINE",
        "Question_tag_count":3,
        "Question_created_time":"2022-12-17T12:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VERTEX-PIPELINE\/m-p\/500836#M966",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":63,
        "Question_body":"I have this problem:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to use Vertex AI training pipelines",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-23T00:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-use-Vertex-AI-training-pipelines\/m-p\/595873#M2005",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":57,
        "Question_body":"Hi!I have been trying to use Vertex AI for training custom ML models. I requested for a quota increase and did all the procedures as requested(Made\u00a0 a $10 transaction). I got an email that my quota has increased. However, I'm still getting the following errors:\n\nHow can I resolve this?\n\n2. I also wanted to upgrade to a standard support package so as to get prompt responses to my queries on email. However, that also tells me I dont have sufficient permissions.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Agent Assist - Simulator view - feature FAQ, article suggestion CHECKBOX disabled",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-26T03:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Agent-Assist-Simulator-view-feature-FAQ-article\/m-p\/514223#M1143",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":122,
        "Question_body":"While working on simulator of Google Agent assist, the checkbox to enable FAQ or Article Suggestion or smart reply option is DISABLED.\n\nThe chat on simulator is working with virtual agent I have configured via Dialog flow with intents that I have built, but can't use agent assist feature since I am not able to enable it.\n\nPlease note I have configured smart reply, article suggestion and Knowledge Base option in the agent assist in my google project.\n\nLooking for help to fix the issue.",
        "Question_closed_time":"02-01-2023 04:44 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Smart reply, FAQ and Articlet Suggestion of google agent assist became visible after I disabled the \"Choose to use Dialogflow\" option in my selected \"Conversation Profile\".\n\nHowever there is another issue. The \"smart reply\"\/\"FAQ\"\/\"Article Suggestion\" though enabled and visible in UI but they are not working. I am not getting any suggestion or smart reply for the customer chat. Any idea what could be wrong ?\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Error while deploying hugging pytorch model (ROBERTA) to Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-22T02:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-while-deploying-hugging-pytorch-model-ROBERTA-to-Vertex-AI\/m-p\/595498#M1989",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":127,
        "Question_body":"Hello\n\nI am new to the vertex AI. I have created a binary classfication model using hugging face Pytorch model (ROBERTA).\u00a0 Now I am following the google vertex AI documentation to deploy but I am facing somer errors while deploying.\n\nERROR 2023-05-22T08:08:18.241378784Z usage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\nERROR 2023-05-22T08:08:18.241437196Z [--model-store MODEL_STORE]\nERROR 2023-05-22T08:08:18.241444587Z [--workflow-store WORKFLOW_STORE]\nERROR 2023-05-22T08:08:18.241451025Z [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\nERROR 2023-05-22T08:08:18.241456985Z [--log-config LOG_CONFIG] [--foreground]\nERROR 2023-05-22T08:08:18.241462945Z [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\nERROR 2023-05-22T08:08:18.241468906Z torchserve: error: unrecognized arguments: --handler \/home\/jupyter\/Container\/hugging_face_handler.py\nERROR 2023-05-22T08:08:22.568594694Z usage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\nERROR 2023-05-22T08:08:22.568649768Z [--model-store MODEL_STORE]\nERROR 2023-05-22T08:08:22.568657159Z [--workflow-store WORKFLOW_STORE]\nERROR 2023-05-22T08:08:22.568664073Z [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\nERROR 2023-05-22T08:08:22.568670034Z [--log-config LOG_CONFIG] [--foreground]\nERROR 2023-05-22T08:08:22.568676233Z [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\nERROR 2023-05-22T08:08:22.568681955Z torchserve: error: unrecognized arguments: --handler \/home\/jupyter\/Container\/hugging_face_handler.py\n\n\u00a0\n\nDocker Image code\n\nFROM pytorch\/torchserve:latest\n\n# Install additional dependencies if required\n# RUN pip install transformers\n\n# Copy the requirements file to the container\nCOPY requirements.txt \/home\/jupyter\/Container\/requirements.txt\n\n# Install the requirements\nRUN pip install -r \/home\/jupyter\/Container\/requirements.txt\n# Copy your model and inference code to the container\nCOPY cls \/home\/jupyter\/Container\/cls\nCOPY hugging_face_handler.py \/home\/jupyter\/Container\/hugging_face_handler.py\n\n# Set the working directory\nWORKDIR \/home\/jupyter\/Container\/\n\n# Expose the port used by TorchServe (default: 8080)\nEXPOSE 8080\n\n# Start TorchServe with your custom model and handler\nCMD [\"torchserve\", \"--start\", \"--model-store\", \".\", \"--models\", \"my_model=\/home\/jupyter\/Container\/cls\/pytorch_model.bin\", \"--handler\", \"\/home\/jupyter\/Container\/hugging_face_handler.py\"]\n\n\u00a0\n\n\u00a0\n\nHandler file code:\n\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\nimport torch\nfrom sklearn import preprocessing\nimport numpy as np\n\nclass TransformersClassifierHandler(BaseHandler):\n\"\"\"\nThe handler takes an input string and returns the classification text\nbased on the serialized transformers checkpoint.\n\"\"\"\ndef __init__(self):\nsuper(TransformersClassifierHandler, self).__init__()\nself.initialized = False\nself.model = None\nself.tokenizer = None\nself.device = None\n\ndef initialize(self, ctx):\n# self.manifest = ctx.manifest\n# properties = ctx.system_properties\nmodel_dir = \"\/home\/jupyter\/Container\/cls\" # Set the model directory path\nself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load model\nself.model = RobertaForSequenceClassification.from_pretrained(model_dir)\nself.model.to(self.device)\nself.model.eval()\nlogger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n\n# Ensure to use the same tokenizer used during training\nself.tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n\nself.initialized = True\n\ndef preprocess(self, data):\ntext = data.get(\"data\", [\"\"])[0]\nsentences = text.decode('utf-8')\nlogger.info(\"Received text: '%s'\", sentences)\n\n# Tokenize the texts\ntokenizer_args = ((sentences,))\ninputs = self.tokenizer(*tokenizer_args,\npadding='max_length',\nmax_length=512,\ntruncation=True,\nreturn_tensors=\"pt\")\nreturn inputs\n\ndef inference(self, inputs):\nwith torch.no_grad():\noutputs = self.model(**inputs)\nlogits = outputs.logits\nprobabilities = torch.softmax(logits, dim=1)\npredicted_labels = torch.argmax(probabilities, dim=1).item()\nreturn ['Y' if predicted_labels == 1 else 'N']\n\n\ndef postprocess(self, inference_output):\nreturn inference_output\n\n\u00a0\n\n\u00a0\n\nI am not sure where I am going wrong over here and getting the above mentioned error. So any help in this topic would be of great help\n\nThanks\n\nJagdish\n\n(PII Removed by Staff)\n\n(PII Removed by Staff)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Looking for a Tool to Refine Object Recognition AI Model on Vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-23T05:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Looking-for-a-Tool-to-Refine-Object-Recognition-AI-Model-on\/m-p\/546397#M1728",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"I would like to know if there is a tool that would allow me to refine the image object recognition artificial intelligence model developed on vertex AI that I made. I recognize several objects in an image.\nI would like to send thousands of images to this model and depending on the model's response, I can validate the response, or modify it by adding or removing the label frames.\nI would also like to be able to display the answers according to their scores.\nI don't know if Google offers such a tool or if I have to go through a specialized company.\n\nThank you for your help.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocumentAI \u2013 Internal Server Error when training on pre-labeled documents",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-04T15:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocumentAI-Internal-Server-Error-when-training-on-pre-labeled\/m-p\/550466#M1791",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":125,
        "Question_body":"We want to automate training of our custom extraction processor by sending PDFs to the OCR processor, adding entities to the document for the values that we know of, uploading it to GCS as .json, and training the processor on our document.\u00a0\n\nOn our initial tries\u00a0we get the following error:\n\n\u00a0\n\n\u00a0\n\n{\n  \"name\": \"projects\/818666290880\/locations\/us\/operations\/8089109272130466059\",\n  \"done\": true,\n  \"result\": \"error\",\n  \"response\": {},\n  \"metadata\": {\n    \"@type\": \"type.googleapis.com\/google.cloud.documentai.uiv1beta3.ImportDocumentsMetadata\",\n    \"commonMetadata\": {\n      \"state\": \"FAILED\",\n      \"createTime\": \"2023-05-04T21:36:28.268708Z\",\n      \"updateTime\": \"2023-05-04T21:36:29.172396Z\",\n      \"resource\": \"projects\/818666290880\/locations\/us\/processors\/a57d00c6e1c2727\/dataset\"\n    },\n    \"individualImportStatuses\": [\n      {\n        \"inputGcsSource\": \"gs:\/\/mybucket.appspot.com\/path\/to\/00d45d9f-c6e3-4937-aa5c-59f395cfb5f0.json\",\n        \"status\": {\n          \"code\": 13,\n          \"message\": \"Internal error encountered.\"\n        }\n      }\n    ],\n    \"totalDocumentCount\": 1\n  },\n  \"error\": {\n    \"code\": 13,\n    \"message\": \"Internal error encountered.\",\n    \"details\": []\n  }\n}\n\n\u00a0\n\nThere is no log in Cloud Logging related to Document AI. The operation is\u00a0\n\nprojects\/818666290880\/locations\/us\/operations\/8089109272130466059.\n\u00a0\nIs there a way for us to determine what it causing the error? Thank you",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Turn off text normalization in Speech to Text API",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-06T02:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Turn-off-text-normalization-in-Speech-to-Text-API\/m-p\/541225#M1598",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":78,
        "Question_body":"I am using Speech to Text API to transcribe audio files. I see that the output contains a lot of characters which might be possibly occurring due to inverse text normalization somewhere. Symbols like $ for dollars and other currency symbols and also numbers written in numeric format rather than words. Is there some option in RecognitionConfig which gives me verbatim output in words instead of numbers and symbols?\u00a0\n\nI see there's a \"transcriptNormalization\" option in the config but then I have to provide my own rules.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"hello custom training tutorial failed on cloud function deploy",
        "Question_tag_count":1,
        "Question_created_time":"2022-03-10T15:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/hello-custom-training-tutorial-failed-on-cloud-function-deploy\/m-p\/402689#M230",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":673,
        "Question_body":"Hi guys\n\n\u00a0I'm following this tutorial to get my had around Vertex AI\n\n\u00a0-\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/image-recognition-custom\/\n\n\u00a0On step\u00a0\n\n\u00a0-\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/image-recognition-custom\/serving#2_deploy_a\n\n\u00a0when I lunch\u00a0\n\u00a0\ngcloud functions deploy classify_flower \\\n\u00a0 --region=us-central1 \\\n\u00a0 --source=function \\\n\u00a0 --runtime=python37 \\\n\u00a0 --memory=2048MB \\\n\u00a0 --trigger-http \\\n\u00a0 --allow-unauthenticated \\\n\u00a0 --set-env-vars=ENDPOINT_ID=${ENDPOINT_ID}\n\n\u00a0I get this error\n\n\u00a0\n\nOperationError: code=3, message=Function failed on loading user code. This is likely due to a bug in the user code. Error message: Error: please examine your function logs to see the error cause: https:\/\/cloud.google.com\/functions\/docs\/monitoring\/logging#viewing_logs. Additional troubleshooting documentation can be found at https:\/\/cloud.google.com\/functions\/docs\/troubleshooting#logging. Please visit https:\/\/cloud.google.com\/functions\/docs\/troubleshooting for in-depth troubleshooting documentation.\n\n\u00a0\n\n\u00a0since I'm new on GCP anyone tried this tutorial and have the same error?\n\n\u00a0any tips on how to fix this?\n\nthank you very much guys",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Problem integrating Python asyncio, sound device, Dialogflow streaming_detect_intent",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-06T06:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Problem-integrating-Python-asyncio-sound-device-Dialogflow\/m-p\/529250#M1379",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":239,
        "Question_body":"Hi Folks:\n\nI am trying to write a custom Dialogflow integration. I am writing a small program that streams audio to Dialogflow's streaming_detect_intent(). I have modified existing examples. For the audio, I am using sounddevice.\u00a0\n\nI have two Python tasks. One runs the audio, the other Dialogflow. The tasks communicate through a shared queue. I can successfully stream audio into a file. I can successfully stream a file into Dialogflow. My code fails when I stream audio into Dialogflow. The immediate culprit is an asyncio.CancelledError(). The trace is\n\n\u00a0\n\n\u00a0\n\nFile \"\/home\/andrew\/experiments\/messaging\/a_recording.py\", line 95, in sample_streaming_detect_intent\nasync for response in stream:\nFile \"\/home\/andrew\/venv\/lib\/python3.11\/site-packages\/google\/api_core\/grpc_helpers_async.py\", line 102, in _wrapped_aiter\nasync for response in self._call: # pragma: no branch\nFile \"\/home\/andrew\/venv\/lib\/python3.11\/site-packages\/grpc\/aio\/_call.py\", line 327, in _fetch_stream_responses\nawait self._raise_for_status()\nFile \"\/home\/andrew\/venv\/lib\/python3.11\/site-packages\/grpc\/aio\/_call.py\", line 233, in _raise_for_status\nraise asyncio.CancelledError()\nasyncio.exceptions.CancelledError\n\n\u00a0\n\n\u00a0\n\nThe code fragment is\n\n\u00a0\n\n\u00a0\n\nasync def sample_streaming_detect_intent(\n    loop, audio_queue, project_id, session_id, sample_rate\n):\n    # Create a client\n\n    client = dialogflow.SessionsAsyncClient()\n\n    audio_config = dialogflow.InputAudioConfig(\n        audio_encoding=dialogflow.AudioEncoding.AUDIO_ENCODING_LINEAR_16,\n        language_code=\"en\",\n        sample_rate_hertz=sample_rate,\n    )\n\n    async def request_generator(loop, project_id, session_id, audio_config, audio_queue):\n\n        query_input = dialogflow.QueryInput(audio_config=audio_config)\n\n        # Initialize request argument(s)\n        yield dialogflow.StreamingDetectIntentRequest(\n            session=client.session_path(project_id, session_id), query_input=query_input\n        )\n\n        while True:\n            chunk = await audio_queue.get()\n            if not chunk:\n                break\n            # The later requests contains audio data.\n            yield dialogflow.StreamingDetectIntentRequest(input_audio=chunk)\n\n    # Make the request\n    client_task = asyncio.create_task(\n        client.streaming_detect_intent(\n            requests=request_generator(\n                loop, project_id, session_id, audio_config, audio_queue\n            )\n        )\n    )\n\n    try:\n        stream = await client_task\n    except Exception as e:\n        print(f\"failed with {e.__cause__}\")\n\n    try:\n        async for response in stream:\n            print(response)\n    except Exception as e:\n        print(f\"failed with {e.__cause__}\")\n\n    query_result = response.query_result\n\n    print(\"=\" * 20)\n    print(\"Query text: {}\".format(query_result.query_text))\n    print(\n        \"Detected intent: {} (confidence: {})\\n\".format(\n            query_result.intent.display_name, query_result.intent_detection_confidence\n        )\n    )\n    print(\"Fulfillment text: {}\\n\".format(query_result.fulfillment_text))\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n    audio_queue = asyncio.Queue()\n\n    # to assert that we are using the same event loop\n    loop = asyncio.get_event_loop()\n\n    await asyncio.gather(\n        record_audio(fp, loop, audio_queue, sample_rate, device, channels),\n        sample_streaming_detect_intent(\n            loop, audio_queue, project_id, session_id, sample_rate\n        ),\n    )\n\n\u00a0\n\n\u00a0\n\n\u00a0Any insights would be appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can we create chatrooms with Dialogflow cx",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-22T22:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-we-create-chatrooms-with-Dialogflow-cx\/m-p\/535972#M1465",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":103,
        "Question_body":"can we create chatrooms with Dialogflow?\n\nDifferent chat window for different user (we want this feature to address issues) user is just example here, we have specific id to the issues(situations) and for that individual situation, client want specific chat window.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ML",
        "Question_tag_count":7,
        "Question_created_time":"2021-11-12T04:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ML\/m-p\/175533#M84",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":358,
        "Question_body":"How to start my journey for being an ML\/AI or data science engineer?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"MERCHANT CENTER CATALOG",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-03T16:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/MERCHANT-CENTER-CATALOG\/m-p\/608963#M2277",
        "Question_answer_count":6,
        "Question_score_count":2,
        "Question_view_count":59,
        "Question_body":"Hi! I am importing my feed from MC that has 4959 products, the process seem pretty easy. But somehow the quantity of imported product to google cloud retail catalog is 9900.\n\n\u00a0\n\nAm i doing something wrong? i don't know what the issue might be. Thanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech changes to a more robotic feel if I use certain phonemes",
        "Question_tag_count":1,
        "Question_created_time":"2022-01-17T00:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-changes-to-a-more-robotic-feel-if-I-use-certain-phonemes\/m-p\/184068#M178",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":71,
        "Question_body":"I am using phonemes for some speaks, to make them sound more natural, but some phonemes change the rest of the sentence feel. I have made a demo here.\n\nUsing these settings:\n\nEnglish\nEN-GB-Wavenet-F\nspeed 0.95\npitch 0\n\nIn the speak below I have two identical speaks, where one word is replaced with a phoneme. Notice how the end of the sentence changes to a more robotic feel in the first.\n\n\u00a0\n\nHow do I avoid this?\n\n<speak>\nIn this training, you will learn more about how you sell <phoneme alphabet=\"ipa\" ph=\"k\u0251\u02d0d\">placeholder<\/phoneme> as a solution for companies that want to minimize out-of-pocket spending and have better control of company spending by employees.\n\nIn this training, you will learn more about how you sell card as a solution for companies that want to minimize out-of-pocket spending and have better control of company spending by employees.\n<\/speak>",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Title: Dialogflow ES not matching the correct intent even with the exact training phrase",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-15T03:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Title-Dialogflow-ES-not-matching-the-correct-intent-even-with\/m-p\/532741#M1424",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":186,
        "Question_body":"I'm facing an issue with Dialogflow ES where it's not matching the correct intent even when I use the exact training phrase that I have entered as a user expression. Instead, it keeps matching the fallback intent or the __system_counters__ context.\n\nI have tried the following troubleshooting steps:\n\nCreated a new agent and tested with only one intent.\nRemoved all contexts from the intents.\nRetrained the agent multiple times.\nTested the agent in different browsers and environments.\n\nDespite trying these steps, the problem persists. When using the \"Try it now\" feature in the Dialogflow console and inputting the exact training phrase (even with the same entity reference), the correct intent is still not being matched.\n\nAny help or guidance on how to resolve this issue would be greatly appreciated.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using tensorflow in python, can I use vertex?",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-19T04:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-tensorflow-in-python-can-I-use-vertex\/m-p\/604448#M2186",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":63,
        "Question_body":"Hello,\u00a0\n\nI have a script:\u00a0https:\/\/github.com\/Louvivien\/ProductWatcher\/blob\/master\/estimatepriceforgivendays_anyproduct.py\n\nThat i have deployed on Google Cloud Run. I like that it is link to my github and deployed when I push. But when I run it it is very slow. Is there a way to use GPU or to use Vertex for this?\u00a0\n\nWhat is the best way to use python script with Google Cloud Run?\u00a0\n\nRegards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"[Vertex AI] Bug - Failed to download file",
        "Question_tag_count":2,
        "Question_created_time":"2022-07-07T07:35:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Bug-Failed-to-download-file\/m-p\/439222#M406",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":376,
        "Question_body":"Vertex AI recently fails to download any file greater than 30M. Any downloaded file will be trimmed at 30M. The download speed is also way slower recently (200k\/s). It was working a few days ago. (downloads files of 100+M at 5M\/s) Any ideas?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Experiment not collecting data",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-10T09:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Experiment-not-collecting-data\/m-p\/542251#M1618",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"Hi, we've been trying to use the experiments feature for a long time now but to no avail. We've set up everything according to documentation: Environments, versions, traffic allocation... Everything seems to work except for the data collection, we can see the agent diverting traffic from the\u00a0 control version to the non-control one but the experiment remains in the collecting data stage and doesn't show any kind of results no matter for how long we keep it going or how many interactions we have with it (both from the test agent, or through the API). We haven't been able to find any other information regarding the experiments feature besides the documentation, so any help would be greatly appreciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Managed notebooks OSError: undefined symbol: cublasLtGetStatusString",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-30T12:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Managed-notebooks-OSError-undefined-symbol\/m-p\/598352#M2051",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":131,
        "Question_body":"When I install Pytorch on a notebook, sometimes import torch works other times I get this error:\u00a0\n\nOSError: \/opt\/conda\/lib\/python3.7\/site-packages\/torch\/lib\/..\/..\/nvidia\/cublas\/lib\/libcublas.so.11: undefined symbol: cublasLtGetStatusString, version libcublasLt.so.11\n\nIt resolves only by deleting the notebook and starting again. How can I resolve this without having to start over?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-27T02:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/m-p\/397685#M214",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":698,
        "Question_body":"Hello,\n\nI am trying to run a Custom Training Job in the Vertex AI Training service.\n\nThe job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).\n\nWhen I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:\n\n\n$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"\n\n\n\n... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.\n\nGranted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?\n\nPlease note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image.\n\n\u00a0\n\nThanks,\nurig",
        "Question_closed_time":"04-17-2022 08:03 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Text to speech Google Cloud Python",
        "Question_tag_count":1,
        "Question_created_time":"2022-03-22T18:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-speech-Google-Cloud-Python\/m-p\/405883#M244",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":351,
        "Question_body":"I would like to calculate the time duration for sentences when I convert text to speech in Google Cloud in Python. For example, if I have three sentences converted to audio, I would like to know when the first sentence starts in the audio, the second one, etc.\n\nExample:\n\ntext= 'Hello, World. I can speak any language. I would like to help you.'\n\nHello, World: starts 00:00 ends 00:03\n\nI can speak any language: starts 00:04 ends 00:09\n\nI would like to help you: starts 00:10 ends 00:13\n\nIs there something for that in python? here is the main code:\n\n\u00a0\n\n\"\"\"Synthesizes speech from the input string of text or ssml.\n\nNote: ssml must be well-formed according to:\n    https:\/\/www.w3.org\/TR\/speech-synthesis\/\n\"\"\"\nfrom google.cloud import texttospeech\n\n# Instantiates a client\nclient = texttospeech.TextToSpeechClient()\n\n# Set the text input to be synthesized\nsynthesis_input = texttospeech.types.SynthesisInput(text=\"Hello, World. I can speak any language. I would like to help you.\")\n\n# Build the voice request, select the language code (\"en-US\") and the ssml\n# voice gender (\"neutral\")\nvoice = texttospeech.types.VoiceSelectionParams(\n    language_code=\"en-US\", ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL\n)\n\ntexttospeech_v1beta1.types.cloud_tts_pb2\n\n# Select the type of audio file you want returned\naudio_config = texttospeech.types.AudioConfig(\n    audio_encoding=texttospeech.enums.AudioEncoding.MP3\n)\n\n# Perform the text-to-speech request on the text input with the selected\n# voice parameters and audio file type\nresponse = client.synthesize_speech(\n    input_=synthesis_input, voice=voice, audio_config=audio_config\n)\n\n# The response's audio_content is binary.\nwith open(\".\/output.mp3\", \"wb\") as out:\n    # Write the response to the output file.\n    out.write(response.audio_content)\n    print('Audio content written to file \"output.mp3\"')",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Getting charged for VertexAI but can't find resources to turn off.",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-13T06:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-charged-for-VertexAI-but-can-t-find-resources-to-turn\/m-p\/611940#M2373",
        "Question_answer_count":1,
        "Question_score_count":2,
        "Question_view_count":43,
        "Question_body":"I'm being charged about $500 a month for VertexAI services but I can't for the life of me locate which services are turned on.\n\nI used Google Colab to run through some notebooks, and I think I upgraded to a certain more dedicated system there, but I can't locate where to downgrade this, or even know which notebook it was.\n\nCan someone help?!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Correct JSONL format for Vertex AI training.",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-15T07:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Correct-JSONL-format-for-Vertex-AI-training\/m-p\/603463#M2163",
        "Question_answer_count":7,
        "Question_score_count":1,
        "Question_view_count":329,
        "Question_body":"Tried to upload data in JSONL format in Vertex AI, based on the example given. It still gives an error -\u00a0Invalid data. Missing one or more required fields.\nWhat can be the issue here.\u00a0\n\nData format pushed -\u00a0\n{\"input_text\": \"Please provide fresh website content for upto <5-page> website. Each page should not exceed more than 200 words of content.\", \"output_text\": \"Page 1 Home - Title (30 to 50 Characters) RCA & Asociados, Professional Services, Chula Vista, California. \\n Description (280 to 300 Characters) RCA is a regional Foodservice Disposables and packaging broker servicing company in Mexico and Southern California. Our sales representatives are all bilingual and travel in and out of Mexico and the US. We are associated with both large and small independent distributors, chain accounts, and C-Stores. \\n Title Foodservice and Packaging Broker with 10+ Years of Experience in CA \\n Hero Text We Focus On Manufacturers Looking To Increase Sales & Help Them Grow Customers In CA & Mexico! Contact Us (Link to Contact Page Button) \\n (H1) (20 to 70 Characters) RCA \u2013 Profile, Mission & Vision RCA is a regional foodservice disposable and packaging broker servicing company in Mexico and Southern California. We have been in business since 2006 and are associated with both large and small independent distributors, chain accounts, and C-Stores. \\n We have offices, both in Mexico and the US, and our sales representatives are bilingual and weekly travel in and out of Mexico and the US. \\n (H2) Our Strategic Mission. Our goal is to represent and focus only on a handful of manufacturers who are looking to expand sales in Mexico and Southern California. We help them to grow their market share and increase their selective list of valued customers in Mexico, who are strategic and smart. \\n (H2) Our Business Vision \\n We establish long term relationships with our suppliers and distributors, together, building a successful partnership benefiting our mutual interests.\\n Page 2 Services - Title (30 to 50 Characters) RCA & Asociados, Services, Chula Vista, California \\n Description (280 to 300 Characters) RCA is an established, professional, and reputable company having good relationships with a wide range of distributors in Mexico and the US. We cater to the Hispanic and Asian Markets in the US and are exploring the opportunity to represent and work with your company to help you improve your market range. Contact us. \\n (H1) (20 to 70 Characters) Well Organized, Professional & Reputed Organization \\n RCA is an established, professional, and reputable company having good relationships with a wide range of distributors in Mexico and the US. We cater to the Hispanic and Asian Markets in the US and are exploring the opportunity to represent and work with your company to help you improve your market range. \\n (H2) Your Local Salesforce \\n We provide our clients who are manufacturers and who have never been to Mexico with transparency and authenticity. We bring in experts to help create relationships with their consumers and grow their client base. \\n (H2) Our Valued Relationships \\n We have established relationships with many markets in Mexico and can help you connect with foodservice distributors, retail, grocery, and chain accounts, depending on your needs. \\n View our Clients (Links to Our Clients) \\n Page 3 Our Clients \\n Title (30 to 50 Characters) RCA & Asociados, Our Clients, Chula Vista, California \\n Description (280 to 300 Characters) RCA & Asociados has been active in foodservice disposables and packaging brokerage since 2006 and is headquartered in Chula Vista, CA. Our primary focus is to help companies grow market share and, with a joint effort, successfully penetrate the market. Know our customers. \\n (H1) (20 to 70 Characters) RCA Clientele \\n Page 4 Contact Us \\n Title (30 to 50 Characters) RCA & Asociados , Contact Us , Chula Vista, California \\n Description (280 to 300 Characters) RCA & Asociados believes in its vision of becoming the premier brokerage company in Mexico & the US. We offer experienced and professional sales representation. We specialize in selling and promoting US and Mexican products within the foodservice and packaging industry. \\n (H1) (20 to 70 Characters) Get in Touch With Us.\"}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text-to-Speech Usage (number of synthesized characters)",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-23T01:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-Speech-Usage-number-of-synthesized-characters\/m-p\/614796#M2434",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":31,
        "Question_body":"Hi, I'm using both Google Cloud TTS and Microsoft Azure TTS. Both offer a free quota of characters (or bytes, for Google) per month. I use these TTS services both via API and on their web interfaces.\n\nWhile I can easily see the number of synthesized characters in my Azure TTS service (see picture below), I can't find anything in my Google TTS service. Is there such a thing?\n\nApologies if this might just be a newbie question, but I wasted way too much time trying to find a simple graph or something that would tell me 'used 34,457 characters of 1M'. Can't be that hard...\n\nThanks for your time.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text to SQL : How to get access",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-22T08:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-SQL-How-to-get-access\/m-p\/614692#M2430",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":24,
        "Question_body":"Hi, we need to convert queries from natural languages to the SQL. Backend is MariaDB\/MySQL with relational database. Please advise:\n\n1) How and where to get access?\n\n2) The documentation and links.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Speech-to-text in Dialogflow Messenger integration?",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-05T06:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-in-Dialogflow-Messenger-integration\/m-p\/600119#M2079",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":33,
        "Question_body":"Hi everyone,\n\nis it possible to have audio input in Dialogflow Messenger? Something similar to WhatsApp\/Telegram's audio messages, but in Dialogflow CX's Messenger integration. I did find\u00a0this answer on StackOverflow\u00a0which states that it's not possible, but it's more than one year old so I'm wondering if it's still the case.\n\nIf it's still not an option today, can it be accomplished with a custom integration? Are there better alternatives?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Function of Labels",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-05T11:49:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Function-of-Labels\/m-p\/609506#M2304",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":50,
        "Question_body":"With Document AI, is the algorithm learning the location of the labels or the structure? I have a few documents and the content\/fields are the same in the document, but sometimes the order is changed so would a Custom Document Extractor be applicable to this situation? Is the model being taught to look for a label in a specific location\/position, thus making it bad at handling situations where the order or positioning of some field is changed? Also is there any Documentation for how the model actually works?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google TTS keeps having a high-pitched, robotic voice crack intermittently",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-08T00:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-TTS-keeps-having-a-high-pitched-robotic-voice-crack\/m-p\/551205#M1818",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":63,
        "Question_body":"Google TTS keeps having a high-pitched, robotic voice crack intermittently, regardless of the voice model I use. Why is that happening?\n\nIs there no way to resolve this issue?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Glossary not found.; Failed to initialize a glossary.",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-17T20:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Glossary-not-found-Failed-to-initialize-a-glossary\/m-p\/490677#M809",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":139,
        "Question_body":"I have created glossary to translate text using Cloud Translation API. it shows me status as running-\n\"name\": \"projects\/xxx\/locations\/us-central1\/operations\/xxx\",\n\"metadata\": {\n\"@type\": \"type.googleapis.com\/google.cloud.translation.v3.CreateGlossaryMetadata\",\n\"name\": \"projects\/xxx\/locations\/us-central1\/glossaries\/xxx\",\n\"state\": \"RUNNING\",\n\"submitTime\": \"2022-11-18T03:59:51.876209069Z\"\n}\n}\n\nbut when I am trying to use this Glossary for translation api, it shows me error as-\n\"Glossary not found.; Failed to initialize a glossary\".\nEven when I tried listing my Glossary, it doesn't show.\n\nNot sure what is the issue. Console activity dashboard shows activity as created Glossary.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"hi-Latn Language Detect Error",
        "Question_tag_count":4,
        "Question_created_time":"2022-09-05T01:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/hi-Latn-Language-Detect-Error\/m-p\/463002#M560",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":225,
        "Question_body":"I keep getting this language as detected language code but i can't seem to find this code in official docs of google translate api\n\n\u00a0\n\nPlease Help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"REQUEST : Google ML Kit for web",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-20T22:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/REQUEST-Google-ML-Kit-for-web\/m-p\/524375#M1301",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":180,
        "Question_body":"Hi\n\nAm wondering can we have Google ML kit Vision APIs like Barcode Scanning for web. currently its only for mobile phones android and ios platform,\u00a0 it will be good if we can use it on web like through JS upload image having barcode and we get scan result\n\nhttps:\/\/developers.google.com\/ml-kit\/vision\/barcode-scanning\n\nthanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ssml text to speech audio tag python",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-16T22:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ssml-text-to-speech-audio-tag-python\/m-p\/544312#M1680",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Cannot get the audio tag to work. Have tried it with both a local and remote mp3 files.\u00a0\u00a0\n\nHere is the text:\n\n<speak version=\"1.1\" xmlns=\"http:\/\/www.w3.org\/2001\/10\/synthesis\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.w3.org\/2001\/10\/synthesis http:\/\/www.w3.org\/TR\/speech-synthesis11\/synthesis.xsd\" xml:lang=\"en-GB\">\n\nThe rain in Spain stays mainly in the plain.\n<audio src=\"uhm_male.mp3\" \/> {If I take this out everything works)\nHow kind of you to let me come.\n\n<\/speak>\n\nAnybody got any suggestions?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX on webhook disconnect",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-16T06:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-on-webhook-disconnect\/m-p\/603852#M2174",
        "Question_answer_count":6,
        "Question_score_count":0,
        "Question_view_count":124,
        "Question_body":"Hello,\u00a0\n\nI am working with dialogflow and botcopy integration. Whenever a intent use a webhook, it works fine, and return the response, but my issue is in case the webhook is down, botcopy freezes and stays with 3 dots waiting for the response, how could I solve this when the webhook is down, and no response go through.\n\n\u00a0\n\nThanks!\n\nSomething like: \"Sorry could you try again later\" in case the timeout is out.",
        "Question_closed_time":"06-16-2023 08:53 AM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi!\n\nThere is an event to match webhook failures! Check the docs!\u00a0https:\/\/cloud.google.com\/dialogflow\/cx\/docs\/concept\/handler#event-built-in\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Glossary file format(s) for use with Google Cloud Translation",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-10T21:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Glossary-file-format-s-for-use-with-Google-Cloud-Translation\/m-p\/542421#M1621",
        "Question_answer_count":5,
        "Question_score_count":0,
        "Question_view_count":82,
        "Question_body":"Hello,\n\nI'm interested to know whether it is possible to use a .tmx file as a\u00a0 source\/target-language bilingual glossary with Google Cloud Translation (Advanced) instead of a .csv file. The reason I ask is that when running my program in its early stage of development I got back an error message stating that only a .csv file is the accepted. However,\u00a0 in a video I happened upon recently, produced by the Google Cloud team, I definitely heard at one point that .tmx was also an accepted format. Does anyone know the definitive answer to this question? Thanks in advance. Regards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"created a project on Google Doc AI ,exported its dataset and facing problem.",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-13T23:37:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/created-a-project-on-Google-Doc-AI-exported-its-dataset-and\/m-p\/532118#M1420",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":77,
        "Question_body":"i created a doc processing project on Google Doc AI, there i labelled docs and ran pipeline,then i exported the dataset to use it in another processor ,i imported the dataset,i right hand side of GUI its showing all labelled doc counts, number of entities we labelled in all docs but it is not showing anything in right of gui where we get option to run the pipeline ,i will attach image for reference.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"\"Internal error encountered\" \"code: 13\"",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-26T12:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/quot-Internal-error-encountered-quot-quot-code-13-quot\/m-p\/503519#M988",
        "Question_answer_count":15,
        "Question_score_count":1,
        "Question_view_count":0,
        "Question_body":"Hello friends, I would like to ask you a question.\nI am working with DocumentIA, and after uploading the documents and tagging them, when training the model, I got the following error:\n\nerror\ncode: 13\nmessage: \"Internal error encountered,\ndetails: []\n\nAs you can see there are no details. I didn't find anything in the documentation either.\n\nCan anyone tell me what that means and how to fix it.\n\nThank you very much!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"stream.Recv() gets stuck in Media Translation API even after sending audio buffer",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-16T10:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/stream-Recv-gets-stuck-in-Media-Translation-API-even-after\/m-p\/603965#M2178",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":65,
        "Question_body":"Below is a code snippet, I am using to send audio buffer to media translation api. The audio codec is ogg_opus with sample rate 48000 hz. But the server is not responding with anything. The python example in official documentation of Media Translation API with linear16 codec works fine. I replaced Media Translation API with Cloud Speech To Text API, that is working fine too. Read the documentation too, not sure what I am doing wrong.\n\n\u00a0\nimport (\n\"context\"\n\"errors\"\n\"io\"\n\"strings\"\n\"sync\"\n\n\nmediatranslation \"cloud.google.com\/go\/mediatranslation\/apiv1beta1\"\n\"cloud.google.com\/go\/mediatranslation\/apiv1beta1\/mediatranslationpb\"\n\"github.com\/livekit\/protocol\/logger\"\n\"github.com\/pion\/rtp\"\n\"github.com\/pion\/webrtc\/v3\"\n\"github.com\/pion\/webrtc\/v3\/pkg\/media\/oggwriter\"\n\"google.golang.org\/grpc\/codes\"\n\"google.golang.org\/grpc\/status\"\n)\n\n\ntype MediaTranslator struct {\nctx context.Context\ncancel context.CancelFunc\n\n\nspeechClient *mediatranslation.SpeechTranslationClient\nlanguage *Language\n\n\nrtpCodec webrtc.RTPCodecParameters\n\u00a0\nlock sync.Mutex\noggWriter *io.PipeWriter\noggReader *io.PipeReader\noggSerializer *oggwriter.OggWriter\n\n\nresults chan RecognizeResult\ncloseCh chan struct{}\n}\n\n\ntype RecognizeResult struct {\nError error\nText string\nIsFinal bool\n}\n\n\nfunc NewMediaTranslator(rtpCodec webrtc.RTPCodecParameters, speechClient *mediatranslation.SpeechTranslationClient, language *Language) (*MediaTranslator, error) {\nif !strings.EqualFold(rtpCodec.MimeType, \"audio\/opus\") {\nreturn nil, errors.New(\"only opus is supported\")\n}\n\n\noggReader, oggWriter := io.Pipe()\nctx, cancel := context.WithCancel(context.Background())\nt := &MediaTranslator{\nctx: ctx,\ncancel: cancel,\nrtpCodec: rtpCodec,\n\/\/sb: samplebuilder.New(200, &codecs.OpusPacket{}, rtpCodec.ClockRate),\noggReader: oggReader,\noggWriter: oggWriter,\nlanguage: language,\nspeechClient: speechClient,\nresults: make(chan RecognizeResult),\ncloseCh: make(chan struct{}),\n}\ngo t.start()\nreturn t, nil\n}\n\n\nfunc (t *MediaTranslator) Language() *Language {\nreturn t.language\n}\n\n\nfunc (t *MediaTranslator) WriteRTP(pkt *rtp.Packet) error {\nt.lock.Lock()\ndefer t.lock.Unlock()\n\n\nif t.oggSerializer == nil {\noggSerializer, err := oggwriter.NewWith(t.oggWriter, t.rtpCodec.ClockRate, t.rtpCodec.Channels)\nif err != nil {\nlogger.Errorw(\"failed to create ogg serializer\", err)\nreturn err\n}\nt.oggSerializer = oggSerializer\n}\n\n\n\u00a0\nif err := t.oggSerializer.WriteRTP(pkt); err != nil {\nreturn err\n}\n\u00a0\n\n\nreturn nil\n}\n\n\nfunc (t *MediaTranslator) start() error {\ndefer func() {\nclose(t.closeCh)\n}()\n\n\nfor {\nstream, err := t.newStream()\nif err != nil {\nif status, ok := status.FromError(err); ok && status.Code() == codes.Canceled {\nreturn nil\n}\n\n\nlogger.Errorw(\"failed to create a new speech stream\", err)\nt.results <- RecognizeResult{\nError: err,\n}\nreturn err\n}\n\n\nendStreamCh := make(chan struct{})\nnextCh := make(chan struct{})\n\n\n\/\/ Forward oggreader to the speech stream\ngo func() {\ndefer close(nextCh)\nbuf := make([]byte, 1024)\nfor {\nselect {\ncase <-endStreamCh:\nreturn\ndefault:\nn, err := t.oggReader.Read(buf)\nif err != nil {\nif err != io.EOF {\nlogger.Errorw(\"failed to read from ogg reader\", err)\n}\nreturn\n}\n\n\nif n <= 0 {\ncontinue \/\/ No data\n}\n\n\nif err := stream.Send(&mediatranslationpb.StreamingTranslateSpeechRequest{\nStreamingRequest: &mediatranslationpb.StreamingTranslateSpeechRequest_AudioContent{\nAudioContent: buf[:n],\n},\n}); err != nil {\nif err != io.EOF {\nlogger.Errorw(\"failed to forward audio data to speech stream\", err)\nt.results <- RecognizeResult{\nError: err,\n}\n}\nlogger.Infow(\"Going to return from stream send..\")\nreturn\n}\n}\n}\n\n\n}()\n\n\n\/\/ Read translation results\nfor {\nresp, err := stream.Recv()\nlogger.Infow(\"Stream recv\", \"resp\", resp, \"error\", err)\nif err != nil {\nif status, ok := status.FromError(err); ok {\nif status.Code() == codes.OutOfRange {\nlogger.Infow(\"Media translation failed due to\", \"StatusCode\", status.Code())\nbreak \/\/ Create a new speech stream (maximum speech length exceeded)\n} else if status.Code() == codes.Canceled {\nlogger.Infow(\"Media translation failed due to\", \"StatusCode\", status.Code())\nreturn nil \/\/ Context canceled (Stop)\n}\n}\n\n\nlogger.Errorw(\"failed to receive response from speech stream\", err)\nt.results <- RecognizeResult{\nError: err,\n}\n\n\nreturn err\n}\n\n\nif resp.Error != nil {\nbreak\n}\n\n\nresult := resp.GetResult().GetTextTranslationResult()\nlogger.Infow(\"Getting result from media translation api\", \"result\", result)\nif result.GetIsFinal() {\nt.results <- RecognizeResult{\nText: result.GetTranslation(),\nIsFinal: result.GetIsFinal(),\n}\nlogger.Infow(\"Translated text: \", result.GetTranslation())\n}\n}\n\n\nclose(endStreamCh)\n\n\n<-nextCh\n\n\nt.lock.Lock()\nt.oggSerializer = nil\nt.lock.Unlock()\n}\n}\n\n\nfunc (t *MediaTranslator) Close() {\nt.cancel()\nt.oggReader.Close()\nt.oggWriter.Close()\n<-t.closeCh\nclose(t.results)\n}\n\n\nfunc (t *MediaTranslator) Results() <-chan RecognizeResult {\nreturn t.results\n}\n\n\nfunc (t *MediaTranslator) newStream() (mediatranslationpb.SpeechTranslationService_StreamingTranslateSpeechClient, error) {\nstream, err := t.speechClient.StreamingTranslateSpeech(t.ctx)\nif err != nil {\nlogger.Infow(\"Error creating streaming translate speech\")\nreturn nil, err\n}\n\n\naudioConfig := &mediatranslationpb.TranslateSpeechConfig{\nSourceLanguageCode: t.language.Code,\nTargetLanguageCode: t.language.TranslateToCode,\nSampleRateHertz: int32(t.rtpCodec.ClockRate),\nAudioEncoding: \"ogg_opus\",\nModel: \"google-provided-model\/video\",\n}\n\n\nif err := stream.Send(&mediatranslationpb.StreamingTranslateSpeechRequest{\nStreamingRequest: &mediatranslationpb.StreamingTranslateSpeechRequest_StreamingConfig{\nStreamingConfig: &mediatranslationpb.StreamingTranslateSpeechConfig{\nSingleUtterance: true,\nAudioConfig: audioConfig,\n},\n},\n}); err != nil {\nlogger.Infow(\"Error sending mt config request\")\nreturn nil, err\n}\nreturn stream, nil\n}",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI scientist",
        "Question_tag_count":3,
        "Question_created_time":"2022-06-19T15:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-scientist\/m-p\/432867#M381",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"I'm from Ukraine. Therefore, I write with the help of a translator. I immediately apologize for any mistakes.\n\nI am a doctor. I am interested in many areas of science that are related to medicine. But because of their volume and complexity, it is impossible to learn by one person.\n\nI propose to create an AI that will analyze information on the Internet (video lectures, articles, books, audio books, images ...) and find relationships. For example, the electrophysical properties of DNA are analyzed through all known theories of physics. And a concrete example: Academician P. Garyaev's \"Linguistic Wave Genome\" through V. Atsyukovsky's \"Ether Theory\".\n\nThis tool needs to be made multifunctional and accessible to all users. This will revolutionize science by combining all knowledge.\nIt is important that there is a convenient voice interface and a personal account where studies are saved.\n\nThank you for attention. Sincerely, Sukhachov Denis.\n\n\u042f \u0441 \u0423\u043a\u0440\u0430\u0438\u043d\u044b. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u043f\u0438\u0448\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0447\u0438\u043a\u0430. \u0421\u0440\u0430\u0437\u0443 \u0438\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.\n\n\u042f \u0434\u043e\u043a\u0442\u043e\u0440. \u041c\u0435\u043d\u044f \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u043c\u043d\u043e\u0433\u0438\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0443\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u043e\u0439. \u041d\u043e \u0438\u0437-\u0437\u0430 \u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u0430 \u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438\u0445 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043e\u0441\u0432\u043e\u0438\u0442\u044c \u043e\u0434\u043d\u043e\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0443.\n\n\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0418\u0418, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435 (\u0432\u0438\u0434\u0435\u043e\u043b\u0435\u043a\u0446\u0438\u0438, \u0441\u0442\u0430\u0442\u044c\u0438, \u043a\u043d\u0438\u0433\u0438, \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f...) \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0414\u041d\u041a \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0435\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u043e\u0440\u0438\u0439. \u0418 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440: \u00ab\u041b\u0438\u043d\u0433\u0432\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u043e\u043b\u043d\u043e\u0432\u043e\u0439 \u0433\u0435\u043d\u043e\u043c\u00bb \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u043a\u0430 \u041f. \u0413\u0430\u0440\u044f\u0435\u0432\u0430 \u0447\u0435\u0440\u0435\u0437 \u00ab\u0422\u0435\u043e\u0440\u0438\u044e \u044d\u0444\u0438\u0440\u0430\u00bb \u0412. \u0410\u0446\u044e\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e.\n\n\u042d\u0442\u043e\u0442 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043c\u043d\u043e\u0433\u043e\u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u042d\u0442\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u0442 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u044e \u0432 \u043d\u0430\u0443\u043a\u0435, \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0432 \u0432\u0441\u0435 \u0437\u043d\u0430\u043d\u0438\u044f.\n\u0412\u0430\u0436\u043d\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u0430 \u0438 \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430, \u0433\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0437\u0430\u043d\u044f\u0442\u0438\u044f.\n\n\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435. \u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0421\u0443\u0445\u0430\u0447\u0435\u0432 \u0414\u0435\u043d\u0438\u0441.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Form parameter 'parameterName' is defined with an unknown entity type '@sys.unit-currency'.",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-11T04:15:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Form-parameter-parameterName-is-defined-with-an-unknown-entity\/m-p\/542504#M1627",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"Hi\n\nFor a while I've been getting strange validation errors in Dialogflow CX about parameters. Now I have this error in the Validations page:\n\nForm parameter 'parameterName' is defined with an unknown entity type '@sys.unit-currency'.\n\nThe parameter is captured and set just fine when testing, and also works fine in the Intent training phrases. Any idea what the cause of this is anyone?\n\nThanks",
        "Question_closed_time":"04-13-2023 04:01 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi\u00a0@FlashMaddison,\n\nWelcome back to Google Cloud Community.\n\nThe entity type you're using in your form parameter might be causing the issue. Even if it is utilized in your description of the form parameter, the error notice implicates that '@sys.unit-currency' is an unknown entity type.\n\nHere are some possible solutions that might involve your issue:\n\nVerify '@sys.unit-currency's spelling and formatting once again. Ensure that it is a perfect match to the entity type utilized by the system entities.\nVerify that your Dialogflow CX agent has the '@sys.unit-currency' entity type enabled. You may accomplish this by visiting the Entities page and making sure the switch is turned on next to \"@sys.unit-currency.\"\n\n\nTry redeploying your agent if you recently added or modified the '@sys.unit-currency' entity type, by doing this, you may help verify that your entity type is being used as of late.\n\n\nTry generating the form parameter again using a different entity type and checking to see if the issue still occurs.\n\nHere are some documentation that might help you:\nhttps:\/\/cloud.google.com\/dialogflow\/es\/docs\/reference\/system-entities?_ga=2.213766740.-1392753435.16...\nhttps:\/\/cloud.google.com\/dialogflow\/es\/docs\/entities-options?_ga=2.213766740.-1392753435.1676655686\nhttps:\/\/cloud.google.com\/dialogflow\/cx\/docs\/reference\/system-functions?_ga=2.104734656.-1392753435.1...\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Exceeded limit 'QUOTA_FOR_INSTANCES' on resource 'dataflow-tabular-stats-and-e .... Limit: 24",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-16T11:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Exceeded-limit-QUOTA-FOR-INSTANCES-on-resource-dataflow-tabular\/m-p\/553942#M1927",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":152,
        "Question_body":"I'm trying to run Vertex AI using the Multiclass3_mod.csv example and I get the error bellow:\n\nThe same happens with bank-marketing.csv example\n\nAny help?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI custom training job never finished",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-16T23:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-custom-training-job-never-finished\/m-p\/554143#M1938",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":131,
        "Question_body":"Hi,\u00a0\n\nI am trying to implement custom model training on Vertex AI.\n\nI pushed custom training job to Vertex AI and met the freezing issue at the\u00a0trainer.train()\u00a0 process.\u00a0\n\n\u00a0Command:\n```\n\ngcloud ai custom-jobs create \\\n--region=us-west1 \\\n--display-name=test_model \\\n--worker-pool-spec=machine-type=n1-highmem-2,accelerator-type=NVIDIA_TESLA_T4,accelerator-count=1,replica-count=1,executor-image-uri=asia-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-13:latest, local-package-path=src, script=task.py\n\n```\n\nVersion\n\n```\n\nimage:\u00a0asia-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-13:latest\n\ntransformers v4.28.0\u00a0 (changed to other versions but got the same freeze issue)\n\n```\n\nLog in Logging is below without error messages.\n\n```\n\nMap: 0%| | 0\/72 [00:00<?, ? examples\/s]\n\n```\n\nThe process succeeded two month ago with vertex-ai\/training\/pytorch-gpu.1-11:latest and the same script task.py.\n\u00a0\nIs there anyone who've been facing a same issue or have solved before?\nThanks,",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX Intergration with Messenger from Facebook",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-15T20:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-Intergration-with-Messenger-from-Facebook\/m-p\/489873#M793",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":128,
        "Question_body":"Hi, I have some issue when I integrate dialogflow cx with messenger.\u00a0\nThe issue is I didn't get response from my bot, I have a suspect that dialogflow failed to send message to user on messenger.\nSo, my problem now is how to check sending message process in dialogflow to get the detail of error?\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI quota policy exceed when training custom model",
        "Question_tag_count":3,
        "Question_created_time":"2022-09-26T02:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-quota-policy-exceed-when-training-custom-model\/m-p\/470907#M601",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":582,
        "Question_body":"Hello team,\n\nCan anyone please help me with this,\n\nI have been trying to run the custom model training in vertex ai and gives an error saying\n\n\"Training pipeline failed with error message:\u00a0The following quota metrics exceed quota limits: aiplatform.googleapis.com\/custom_model_training_cpus\"\n\nFollowed the below steps to solve it but didn't help me at all,\n\n1. Changed the region (As it mentioned in one comment of Stack Overflow for this error)\n\n2. Increased CPU instances in the work pool as well as notebooks but didn't help me at all.\n\nI have gone through the IAM & API Services, and then when I checked the quotas for the Vertex AI API for all resources in it, none of them had exceeded the quota limit. I'm still confused as to why it was showing a quota exceed error when I was training the custom model.\n\nPlease help me on this issue, how to solve it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"About Vertex AI Model Monitoring billing",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-28T22:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Vertex-AI-Model-Monitoring-billing\/m-p\/607607#M2258",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":108,
        "Question_body":"BigQuery's pricing page says the analysis fee is $5.00 per TB, and Model Montoring's pricing page says it is $3.50 per GB. Does this only cost $3.50 per GB when Model Monitoring is enabled? Or does both apply?",
        "Question_closed_time":"07-06-2023 10:42 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hello YoshinaoMori,\n\n\nGoogle Cloud follows a pay-as-you-go pricing structure, meaning, you only pay for the services you use. If both Analysis and Model Monitoring features are active then both will apply.\n\nThe Analysis fee covers the processing of queries, user-defined functions, scripts, and certain data manipulation language (DML) and data definition language (DDL) statements while the Model Monitoring fee cover the processing of data used to monitor your models.\n\nIn the event that you disabled the Model Monitoring feature, then you will only be charged for the Analysis fee.\n\nI hope I was able to answer your question.\n\n\nUsable resource:\n- Pricing | Vertex AI Model Monitoring\n- Pricing | BigQuery\n- Google Cloud Pricing Calculator\n\n\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How to use multiple data files in vertex AI",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-10T05:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-multiple-data-files-in-vertex-AI\/m-p\/610696#M2333",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":46,
        "Question_body":"Hi\n\nI am looking to implement Vertex AI on product. The use case is very simple. The end user uploads some files (mostly CSV or xls containing 50 to 1000 entries.). Then user can request info using prompt.\n\nHere is an example:\n\nUser 1 -> uploads a csv having pricelist -> tshirt- $10, book - $15, bag - $20\nUser 2 -> uploads a csv having pricelist -> tshirt- $25, cap - $12, shorts - $15\n\nNow if user 1 prompts and asks for value of tshirt. He should see $10. Also if he requests price of cap he should get an error.\n\nSimilarly if\u00a0 user 2 prompts and asks for value of tshirt. He should see $25. Also if he requests price of book he should get an error.\n\nI was wondering how can I implement this on vertex AI in a way that data\/files uploaded by one user is not accessible to other user. I am not looking create a fine tuned model for each client as it is not cost effective and also the client data does not require any special learning.\n\nPlease help.\n\nThanks in advance.\n\nRegards\nYogendra",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What is the limit of ProductSet per location",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-03T13:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-is-the-limit-of-ProductSet-per-location\/m-p\/485590#M726",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":84,
        "Question_body":"When using Google Vision's Product Search API, does anyone have an idea the maximum number of ProductSets allowed in a location(region)?\n\nThis documentation shares limits on reference images per ProductSet, but it says nothing about ProductSets per location",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"C# handwritten text detection using google.cloud.Vision.v1 Api. why can't detect language?",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-02T23:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/C-handwritten-text-detection-using-google-cloud-Vision-v1-Api\/m-p\/473437#M616",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":199,
        "Question_body":"hello mam\/sir,\n\nI was used google.cloud.Visiion.v1 for handwritten text recognition for Indian languages .\u00a0 this code is work but only for Marathi, Hindi languages. but other language like Malayalam, Tamil, Kaneda Telegu its not return a 100 percent result.\n\nFor Example- i have a 12 months handwritten name(Malayalam, Kaneda etc.) but its recognize only 7 to 8 correct word detection .can you please help me to 100 percent\u00a0 accurate word detection.\n\nImageAnnotatorClient client = ImageAnnotatorClient. Create();\nIReadOnlyList<Entity Annotation> text Annotations = client.DetectText(image);\nfor each (Entity Annotation text in text Annotations)\n{\n\u00a0 \u00a0 Console. WriteLine($\"Description: {text. Description}\");\n}\n\nplease give me a solution .\n\nthanks and regards,\n\nBhagyashri",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can i show alias instead of voice name?",
        "Question_tag_count":1,
        "Question_created_time":"2022-06-22T23:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-i-show-alias-instead-of-voice-name\/m-p\/434016#M389",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":151,
        "Question_body":"[ ko-KR-Wavenet-A ] This name is so awkward for me.\n\nCan i show alias instead of that voice name?\n\nlike this. [ ko-KR-Wavenet-A ] -> [ Jinsung ]\n\n------------------------------------------------------\n\nI'm developing a web service that can edit videos on the web.\n\nI will provide Google TTS on that web service.\n\nI show the Google (source of the voice) on the side, just wanna alias.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error from Vertex AI Getting predictions from custom trained models",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-12T19:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-from-Vertex-AI-Getting-predictions-from-custom-trained\/m-p\/543292#M1647",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":526,
        "Question_body":"I am trying to call an API to inference from a model I have uploaded to vertex AI.\n\nI have tried three methods, and none worked so far.\n\nAt first, I was following a youtube from standford university,\u00a0https:\/\/www.youtube.com\/watch?v=fw6NMQrYc6w&t=3876s\u00a0which uses ai platform.\n\n1. I also tried that, but I think google is trying to get rid of AI platform, and although I succesfully uploaded the model, it doesn't allow me to make a new version, basically allows me nothing.\n\n2. I tried to work this tutorial,\u00a0https:\/\/codelabs.developers.google.com\/vertex-p2p-predictions#5\u00a0\n\nand it keeps complaining that my payload is above 1.5MB limit, but my image is only 49KB, so it's ridiculous. maybe something happened in this code, but it's from the tutorial, so the tutorial must be wrong then.\n\n\u00a0\n\nIMAGE_PATH = \"test-image.jpg\"\nim = Image.open(IMAGE_PATH)\nx_test = np.asarray(im).astype(np.float32).tolist()\nendpoint.predict(instances=x_test).predictions\n\n\u00a0\n\n3. Last, I've been trying to call the API from the sample code,\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\n\nbut it gives me a json format error.\n\nI have referenced from this website to get the json format.\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\u00a0\n\nThe error I am getting is as is :\u00a0\n\n400 { \"error\": \"Failed to process element: 0 key: instances of 'instances' list. Error: Invalid argument: JSON object: does not have named input: instances\" }\n\u00a0\nThe code that I have used is :\n\u00a0\n\n\u00a0\n\n    encoded_content = base64.b64encode(image).decode(\"utf-8\")\n    instances = {\"instances\": {\"image\": {\"b64\": encoded_content}}, \"key\": \"0\"}\n\n\u00a0\n\nand the 400 error comes from API and the log from vertex AI is not very useful when debugging.\n\nHonestly I have been struggling with this issue for days and in my opinion, this should not be this difficult. My experience with GCP and vertex AI is very disappointing and I'm considering to explore other options. Please let me know if any of you have any advices. Thanks",
        "Question_closed_time":"04-20-2023 11:05 PM",
        "Answer_score_count":0.0,
        "Answer_body":"I actually solved this error by applying this document.\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-predict-image-classification-sample.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Cloud Vision API is super slow today",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-08T08:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-API-is-super-slow-today\/m-p\/519864#M1248",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":144,
        "Question_body":"Facing slowness with google vision api. We have only exhausted 50% of the quota, and dont see irregular usage.\nBut we see that api thoughput have reduced from regular 40rps to 9rps and the latency of 99% percentile have increased from 1s to 17s.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Meetup on machine translation for low-resource languages this Friday!",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-19T12:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Meetup-on-machine-translation-for-low-resource-languages-this\/m-p\/479955#M667",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":42,
        "Question_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.\n\nThe next machine translation meetup is all about low-resource machine translation\u00a0and it'll be online.\n\n\u00a0\nmachinetranslate.org\/meetup\n\u00a0\nThe 25-minute panel features guests from Meta AI,\u00a0NeuralSpace, LoResMT, and Masakhane!\n\nRegister to join us\u00a0this Friday at 8am PST",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Data from Azure Storage Gen2 and Synapse dedicate pool to train models in VertexAI",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-27T23:36:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Data-from-Azure-Storage-Gen2-and-Synapse-dedicate-pool-to-train\/m-p\/607275#M2251",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":31,
        "Question_body":"How can we create the models in Vertex AI when data is Azure Storage Gen2 and\u00a0Synapse dedicate pool WITHOUT moving the data to the Google cloud (BQ, Cloud storage etc.) and consume the model outputs in Azure. Can someone point me to the reference architecture?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"ERROR log for health checks, liveness checks, prediction requests for Vertex AI endpoints?",
        "Question_tag_count":3,
        "Question_created_time":"2023-06-29T14:07:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/ERROR-log-for-health-checks-liveness-checks-prediction-requests\/m-p\/607886#M2263",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"They seem to log as ERROR, which makes filtering for actual errors in a container really difficult. Is there some way I can change the logging for these so I can actually find errors in my container?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"dtml dialogflow cx twilio",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-21T08:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/dtml-dialogflow-cx-twilio\/m-p\/535148#M1455",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":77,
        "Question_body":"https:\/\/groups.google.com\/g\/dialogflow-cx-edition-users\/c\/y3EWGi9DQJ0\n\nGood morning,\n\nI'm trying to set up a scenario with a dtml on a phone number provided by Twilio.\nThe chat bot works but when I try on the phone it does not take into account the keys on the other hand if I say verbally it works.\nI followed the procedure as the link above but it still does not work\nTwilio tells me it's from Diallogflow cx but I can't see what's wrong.\nPlease tell me if you have some time to tell me what's wrong\n\nBest.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Image size for Google AutoML Training",
        "Question_tag_count":3,
        "Question_created_time":"2023-01-09T18:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Image-size-for-Google-AutoML-Training\/m-p\/508426#M1035",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":132,
        "Question_body":"Hi, I have some images which are lager than\u00a0\u00a01024 pixels by 1024 pixels.\nFollowing this document:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/image-data\/object-detection\/prepare-data\nImage size should be\u00a01024 pixels by 1024 pixels suggested maximum. If I resize images, the quality of images is low.\n\nWhat should I do?\nThank you and\nRegards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Table models not deploying",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-02T11:30:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Table-models-not-deploying\/m-p\/495557#M895",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":75,
        "Question_body":"I have a bunch of AutoML table models. I have been able to deploy them and do online prediction till last week. But starting this week, when I try to deploy the models for doing online prediction, I see an INTERNAL error every time. There has been no change in the models in the meantime. Any idea how this can be fixed?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Text to Speech - Custom Voice",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-05T01:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Text-to-Speech-Custom-Voice\/m-p\/540806#M1586",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":181,
        "Question_body":"Hello,\n\nwe are interesting in a custom voice model which should be used for Dialogflow voicebot. As per this link https:\/\/cloud.google.com\/text-to-speech\/custom-voice\/docs \u00a0 I am trying to submit a new model \/ request but was not able to finish it. Everything is prepared but when apply the curl command it ends with two different errors.\n\n\u00a0\n\ncurl -X POST \\\n-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n-H \"Content-Type: application\/json; charset=utf-8\" \\\n-d @request.json \\\nhttps:\/\/texttospeech.googleapis.com\/v1beta1\/text:synthesiz\ncurl -X POST \\\n-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n-H \"Content-Type: application\/json; charset=utf-8\" \\\n-d @request.json \\\nhttps:\/\/texttospeech.googleapis.com\/v1beta1\/text:synthesize\n\n\n{\n\"input\":{\n\"text\":\"Android is a mobile operating system developed by Google, based on the Linux kernel and designed primarily for touchscreen mobile devices such as smartphones and tablets.\"\n},\n\"voice\":{\n\"languageCode\":\"en-US\",\n\"custom_voice\":{\n\"reportedUsage\":\"REALTIME\",\n\"model\":\"projects\/<xxx>\/locations\/us-central1\/models\/cctr\",\n}\n},\n\"audioConfig\":{\n\"audioEncoding\":\"LINEAR16\"\n}\n}\n\nFirst is missing language code where I add\u00a0\"languageCode\":\"en-US\",\u00a0 based on the documentation but then it ends with missing model error. I think I missed some step but was not able to find which one, probably some step with model creation?\n\nAny help will be appreciated.\n\nThank you,\n\nRoman",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow Messenger and richcontent from a webhook",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-14T16:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-Messenger-and-richcontent-from-a-webhook\/m-p\/544027#M1674",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":206,
        "Question_body":"Hi,\n\nI'm trying to add a fulfilment response using Webhook.\n\nI built a kind of simple Python API with Flask to provide a rich content answer to Dialogflow.\n\nThe problem is that Dialogflow accepts only messages\u00a0and not richcontents from my API. ( not rendering, for example, chips).\n\nHere is my code :\n\n\u00a0\n\ndef answer_webhook():\n    message= {\"fulfillment_response\": {\n      \n        \"messages\": [\n        {\n          \"text\": {\n            \"text\": \"My text here\"\n          }\n        }\n      ],\"richContent\": [\n    [\n      {\n        \"type\": \"description\",\n        \"title\": \"Description title\",\n        \"text\": [\n          \"This is text line 1.\",\n          \"This is text line 2.\"\n        ]\n      }\n    ]\n  ]\n      \n    }\n    }\n    return Response(json.dumps(message), 200, mimetype='application\/json')\n\n\u00a0\n\nI tried different ways to write or construct the JSON, but nothing works\u00a0\n\nCould you please advise, knowing that I'm testing on an embedded Dialogflow Messenger UI\n\nThanks\n\nYassine",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Cloud Platform - Vertex AI - Workbench JupyterLab - Spark\/Hadoop - JAVA_HOME is not set error",
        "Question_tag_count":1,
        "Question_created_time":"2022-04-13T15:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Cloud-Platform-Vertex-AI-Workbench-JupyterLab-Spark\/m-p\/413482#M272",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":762,
        "Question_body":"Hi All,\n\nI am trying to connect to a SparkSession on Vertex AI's Workbench JupyterLab, but receive this error. Locally, my JAVA_HOME system environments and path environments are already set, and can work when I run Jupyter locally. But only on\u00a0Vertex AI's Workbench JupyterLab I get this error.\n\n\u00a0\n\nCode:\u00a0\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.appName('Jupyter BigQuery Storage')\\\n.config('spark.jars', 'gs:\/\/spark-lib\/bigquery\/spark-bigquery-latest_2.12.jar') \\\n.getOrCreate()\n\nFull Error:\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_3404\/1949393828.py in <module>\n      9 spark = SparkSession.builder \\\n     10   .appName('Jupyter BigQuery Storage')\\\n---> 11   .config('spark.jars', 'gs:\/\/spark-lib\/bigquery\/spark-bigquery-latest_2.12.jar') \\\n     12   .getOrCreate()\n     13 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    226                             sparkConf.set(key, value)\n    227                         # This SparkContext may be an existing one.\n--> 228                         sc = SparkContext.getOrCreate(sparkConf)\n    229                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    230                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    390         with SparkContext._lock:\n    391             if SparkContext._active_spark_context is None:\n--> 392                 SparkContext(conf=conf or SparkConf())\n    393             return SparkContext._active_spark_context\n    394 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    142                 \" is not allowed as it is a security risk.\")\n    143 \n--> 144         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    145         try:\n    146             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    337         with SparkContext._lock:\n    338             if not SparkContext._gateway:\n--> 339                 SparkContext._gateway = gateway or launch_gateway(conf)\n    340                 SparkContext._jvm = SparkContext._gateway.jvm\n    341 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--> 108                 raise RuntimeError(\"Java gateway process exited before sending its port number\")\n    109 \n    110             with open(conn_info_file, \"rb\") as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n\nDo let me know if you have advice or help, thank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GPU shortage on europe-west1",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-07T09:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GPU-shortage-on-europe-west1\/m-p\/529808#M1385",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":258,
        "Question_body":"Hi everyone,\n\nI am trying to launch a Vertex AI CustomJob Training on europe-west1 using a T4 GPU.\n\nIt's been two days I keep receiving a \"Insufficient Ressources\" denial and after like 50 trials, I wonder if I am the only one experimenting this issue. Does anyone managed to trigger a training job on T4 ?\n\nI've checked and my quotas are ok.\u00a0\n\nThanks for the feedback",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GETTING A PART OF PROMPT IN COMPLETION WHEN FINETUNING TEXT DAVINCI MODEL",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-12T21:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GETTING-A-PART-OF-PROMPT-IN-COMPLETION-WHEN-FINETUNING-TEXT\/m-p\/602427#M2138",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":19,
        "Question_body":"Hi,I tried fine tuning the text davinci model with a classification data set. Although the model is trained multiple number if times im getting a part of prompt in the completion.\u00a0For example if the prompt is given as \"Classify the following text into one of the following classes: [spam,ham] Text:\u00a0 Are you this much buzy\" the completion is given as : \"of the following classes: [spam,ham] Text:\u00a0 Are you this much buzy\"\u00a0 ham\". Kindly help with this",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Access to PaLM 2 unicorn",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-27T03:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Access-to-PaLM-2-unicorn\/m-p\/606875#M2243",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":254,
        "Question_body":"Hi,\n\nI already use Vertex AI bison model, but the results are not as good as using BARD (which I understand that works with the bigger\u00a0text-unicorn@001\u00a0model).\n\nHow can I get access to the unicorn model?",
        "Question_closed_time":"06-30-2023 12:41 PM",
        "Answer_score_count":2.0,
        "Answer_body":"Good day,\n\nUnfortunately this is not yet available on Vertex AI and there is no ETA when it will be available, you can check the\u00a0current available foundation models in Vertex AI using this link:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/generative-ai\/learn\/models#foundation_models\nBut if you like this feature, I would suggest that you submit a feature request using this link:\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers\n\nYou can also try to register in a waitlist for a public preview of Palm API for experimentation and internal prototyping without any cost, but please note that the estimated preview period will end in H2 2023. You can check this link for more information regarding the public preview:\u00a0https:\/\/developers.generativeai.google\/guide\/preview_faq\nYou can register using this link:\u00a0https:\/\/developers.generativeai.google\/\n\nI hope this helps!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How do I delete all conversations in the Dialogflow history?",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-08T08:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-I-delete-all-conversations-in-the-Dialogflow-history\/m-p\/519907#M1249",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":143,
        "Question_body":"Hello, I know that you can delete conversations through the Dialogflow ES user interface, as described here: https:\/\/cloud.google.com\/dialogflow\/es\/docs\/history. However, it's only possible to delete individual conversations. I would like to delete all conversations. Is there a way to accomplish this via API?\n\nThanks for your help",
        "Question_closed_time":"02-09-2023 11:08 PM",
        "Answer_score_count":0.0,
        "Answer_body":"According to the current API[1], bulk delete is not supported. Meanwhile, there is already a feature request filed for this case scenario. But, you can try these alternative steps to delete conversation history in your Dialogflow:\n\n1. Backup your Dialogflow agent by exporting it.\n2. Delete your agent.\n3. Restore your backup agent in the same project.\n4. Disable interaction logs in agent settings.\n\nNote: Disabling interaction logs will prevent logging of new logs and the deletion of agent can only affect the logs on the history page.\n\n[1]\u00a0https:\/\/cloud.google.com\/dialogflow\/priv\/docs\/insights\/reference\/rest\/v1alpha1\/projects.locations.co...\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"No results with is_final true for single utterance set to true",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-11T06:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/No-results-with-is-final-true-for-single-utterance-set-to-true\/m-p\/476809#M639",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":157,
        "Question_body":"I am using below configuration to identify my voice input stream (Hindi language)\u00a0 :\n\n\u00a0\nconfig.set_language_code(\"hi-IN\");\nconfig.set_sample_rate_hertz(8000);\nconfig.set_encoding(RecognitionConfig::LINEAR16);\nconfig.set_model(\"command_and_search\");\nstreaming_config.set_single_utterance(true);\nstreaming_config.set_interim_results(true);\n\u00a0\nAfter setting single utterance to true I am not getting any results with is_final as \"True\" in the sample application :\u00a0https:\/\/github.com\/GoogleCloudPlatform\/cpp-samples\/blob\/main\/speech\/api\/streaming_transcribe.cc\u00a0\n\u00a0\nfor (auto response = read(); response.has_value(); response = read()) {\n\/\/ Dump the transcript of all the results.\nstd::cout<<\"Printing speech - event type = \"<<response->speech_event_type()<<std::endl;\n\u00a0\nfor (auto const& result : response->results()) {\nstd::cout<<\"Is final = \"<<result.is_final()<<std::endl;\nfor (auto const& alternative : result.alternatives()) {\nif(!result.is_final())\n{\nstd::cout<<\"Interim results ==-=========\"<<\"\\t\";\nstd::cout \/\/<< alternative.confidence() << \"\\t\"\n<< alternative.transcript() << \"\\n\";\n}\nelse\n{\nstd::cout<<\"is_final has returned true\"<<\"\\t\";\nstd::cout \/\/<< alternative.confidence() << \"\\t\"\n<< alternative.transcript() << \"\\n\";\n}\nstd::cout<<\"Printing speech - event type = \"<<response->speech_event_type()<<std::endl;\n}\n}\n}\n\u00a0\nFor Hindi inputs like single words :\u00a0\u00a0\u0907\u0938,\u00a0 21(\u0907\u0915\u094d\u0915\u0940\u0938)\nI never get a response with is_final set to true even after getting\u00a0\u00a0response->speech_event_type() as '1' (END_OF_SINGLE_UTTERANCE)",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Does custom processor supports indian regional languages",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-09T22:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-custom-processor-supports-indian-regional-languages\/m-p\/530869#M1411",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":64,
        "Question_body":"We are creating a custom document data extractor in document AI for electricity bills.\u00a0\n\nSome of the electricity bills are in indian regional languages and when we are trying to tag values in regional language it automatically translates them into english (often translated wrong) automatically. We would like to know whether custom processors supports regional indian languages like Malayalam , Kannada, Punjabi, Gujarati, Hindi, Tamil, Telugu, etc or not. If it does not support these language's , then is there is any workaround for this?\u00a0\n\nWe are referring to this link for languages it supports:\u00a0https:\/\/cloud.google.com\/document-ai\/docs\/languages",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Remove Roboto Google Font from Dialogflow",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-13T11:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Remove-Roboto-Google-Font-from-Dialogflow\/m-p\/499260#M937",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":167,
        "Question_body":"Hello dears,\n\n\u00a0\n\nI have been asked to remove the call to this Google Font\n\nhttps:\/\/fonts.gstatic.com\/s\/roboto\/v30\/KFOmCnqEu92Fr1Mu4mxK.woff2\n\nfrom a website using Dialogflow Messenger. I tried plugins, local loading of fonts and everything else but it is working for everything but the chatbot. Any idea how can fix this?\n\nThank you.\n\nLorenzo",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to disable TLS 1.0 and 1.1 in dialogflow?",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-16T08:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-disable-TLS-1-0-and-1-1-in-dialogflow\/m-p\/467528#M583",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":72,
        "Question_body":"Need to disable TLS 1.0 and 1.1 for oauth api and events api in the dialogflow. We get those apis while integrating with slack.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Attempt to Populate System Parameters with API Response from Dialogflow Webhook Not Working?",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-05T09:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Attempt-to-Populate-System-Parameters-with-API-Response-from\/m-p\/496266#M907",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":60,
        "Question_body":"Hi,\u00a0\n\nI've been able to make a webhook that calls my own nodejs api where i maintain a database of faux employees. The API responds, and I have been able to populate the Agent's response in the fulfillmentResponse Object. I've tried formatting the sessionInfo Object in a number of ways and can't update system parameters like\u00a0@sys.last-name (Note: I also tried \"@sys.last-name\" as the parameters key, but it didn't work)? Is there something wrong w\/ my JSON formatting? Here's my JSON formatting, and the console.log of what the app sent:\n\n\n\n\u00a0\n\n\u00a0\n\n  \/\/ Return a successful response\n      let jsonResponse = {\n        \"fulfillmentResponse\":{\n          \"messages\": [\n            {\n              \"text\": [message]\n            }\n          ],\n        },\n        \"sessionInfo\" :{\n          \"parameters\": {\n            \"last-name\" : result[0].lastName,\n            \"location\" : \"Miami\",\n          }\n        }\n      }\n  res.status(200).send(jsonResponse); \n  console.log(JSON.stringify(jsonResponse));\n\n----------------------------------console.log output----------------\n{\"fulfillmentResponse\":\n   {\"messages\":[\n      {\"text\":[\n         \"This is a copy of the employee record I was able to find in my database: \\n \n         \\nName: Charlie Brown\\nPhone: 7045555551\\nLocation: Anderson\\nEmployee ID: \n         7\\nEmail: cbrown@elux.com\\nAddress: 1770 James Street Charlotte NC \n         28277\\n\\nSupervisor: Snoopy\"\n         ]\n      }\n     ]\n    },\n   \"sessionInfo\":{\n      \"parameters\":{\n          \"last-name\":\"Brown\",\"location\":\"Miami\"\n         }\n      }\n    }",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Chatting with Dialogflow ES right after deployment fails",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-12T08:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Chatting-with-Dialogflow-ES-right-after-deployment-fails\/m-p\/543132#M1644",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":101,
        "Question_body":"Hi,\n\nI have been having issues lately with Dialogflow ES and talking to the bot\/agent after I deploy and train the agent on any new content. This is all done programmatically through the Dialogflow APIs, which I am on v2 of.\n\nMy deployment process first is deploying which uses the: \"agentClient.restoreAgent();\"\u00a0function within the api. (This is a node app)\n\nThen I after successfully uploading the new content, I train the agent: \"agentClient.trainAgent();\"\n\nThis process has not changed for quite a long time and this issue has only recently started and has gradually gotten worse.\n\nTo describe the issues further, when I deploy all steps come through as successful and done, leading me to believe the agent has been properly uploaded and trained with the training successful and complete. I then go the chat with the bot\/agent and get the below error in my console. This persists for a fair few minutes now (around 5 minutes) before the messages are successfully handled and the intent is recognised correctly. As mentioned, nothing has changed for a long time in my code for how we deploy, train and chat to a Dialogflow agent.\n\nThis is quite a serious issue since it also effects client production apps as well as my own development environment, and its not great for clients to deploy then be told it is successful from Dialogflow and then still have to wait another 5 minutes because of this error Dialogflow gives for the bot to actually be usable.\n\nOne other thing worth noting, is that this error also occurs on the Dialogflow test console on the Dialogflow site (https:\/\/dialogflow.cloud.google.com\/) so it is not something unique to my app. I.e. after I deploy, I try and test here and the same issue I get in my app occurs on the official Dialogflow console.\n\nAnother extra point, this is not on\u00a0every deployment, but it is on most of them now.\n\nThanks!\n\nImage of error on Dialogflow itself:\n\n\u00a0\n\nError:\n\n\u00a0\n\nError: 9 FAILED_PRECONDITION: Intent with id '<UUID here>' not found among intents in environment '' for agent with id '<UUID here>'.   \ncom.google.apps.framework.request.StatusException: <eye3 title='FAILED_PRECONDITION'\/> generic::FAILED_PRECONDITION: Intent with id '<UUID here>' not found among intents in environment '' for agent with id '<UUID here>'.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"impact on dialog.flow es service due to the termination of the google action",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-24T22:06:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/impact-on-dialog-flow-es-service-due-to-the-termination-of-the\/m-p\/546887#M1738",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":36,
        "Question_body":"I saw a notice like this on google action. \"This Conversational Action will be removed on June 13, 2023.\"\nI use response designated as google assistant in dialog.flow ES, will it affect the dialog.flow ES service?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What difference environments\/domains does vertex ai is being in use ?",
        "Question_tag_count":4,
        "Question_created_time":"2022-09-02T02:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-difference-environments-domains-does-vertex-ai-is-being-in\/m-p\/462390#M559",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":53,
        "Question_body":"I wanna learn more about how to use vertex ai in more domains than in recognition and things , so i can learn how to use Vertex ai in other fields and domain ,\u00a0\n\nI want to learn like projects using live vertex ai auto ml or realated to those .",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Service account not having storage account create access",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-25T06:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Service-account-not-having-storage-account-create-access\/m-p\/513839#M1133",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":655,
        "Question_body":"Trying to connect glossary in phrase and using service account key from google for our trained MT engines.\u00a0\n\nReceiving the error that the service account 'does not have\u00a0storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist).\", \"domain\": \"global\", \"reason\": \"forbidden\" } ] } }'\n\nBut we have granted this account the following access in google console already:\n\nCloud translation API editor\n\nStorage object viewer\n\nand previously:\n\nStorage object creator\n\nlogged out of phrase, recreated the MT profile, uploaded glossary again and getting same error.\n\n\u00a0\n\nPlease help urgently to resolve.\n\nThanks!\n\nPoppy",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Wrong no-inputs transition",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-09T02:04:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Wrong-no-inputs-transition\/m-p\/497783#M926",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":121,
        "Question_body":"For quite a long time we have been facing a problem connected to a\u00a0wrong transition between Event Handlers\u00a0(\"no-input-1\"\/\"no-input-2\"\/\"no-input-n\") OR even a\u00a0missing events.\nThat is\u00a0recurring problem\u00a0that appears from time to time on different agents\/flows.\n\n---\n\n#1 Example\n\nWrong transition between no-inputs. From \"no-input-1\" we went to \"no-input-default\" instead of \"no-input-2\", that was present on the Page.\n\nAgent structure:\n1) Agent starts with a \"Default Start Flow\", that forwards customer to another 4 flows.\n2) These flows have the same construction \u2014 one by one questions with some input from a user.\nQuestion 1: Please name the letters for Zinc (Zn);\nQuestion 2: Please name the letter for Carbon (C);\nQuestion n: ...\n3) On the page we have several no-input handlers (no-input-1, no-input-2, ...).\n4) The last no-input event moves user to the \"End Session\" Page.\n\n\nExpected Agent behaviour:\n1) If user is silent for the first time -> GDF raises no-input-1 event\n\u00a0 \u00a0and replies with a predefined Fulfilment.\n2) If user is silent for the second time -> GDF raises no-input-2 event\n\u00a0 \u00a0and replies with a predefined Fulfilment + moves customer to the \"End Session\" page.\n\n\nReal Agent behaviour:\n1) If user is silent for the first time -> GDF raises no-input-1 event\n\u00a0 \u00a0and replies with a predefined Fulfilment.\n2) If user is silent for the second time -> GDF raises \"no-input-default\" event (instead of \"no-input-2\")\n\u00a0 \u00a0and moves user to the \"Start\" page in the \"Default Start Flow\".\n\n---\n\n#2 Example\n\nMissed one event handler. We had two Event Handlers on a single Page: \"no-input-1\", \"no-input-2\".\nOn empty user input, \"no-input-2\" raised instead of \"no-input-1\"\n\n---\n\nTemporary solution (but\u00a0sometimes it doesn't work!)\n\n1) Go to \"Agent Settings\" => \"ML\"\n2) Select all flows and press \"Train\"\n\n---\n\nHas someone faced this issue? How can we fix it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Stable Diffusion 1.5 from Vertex AI's model garden: what parameters and instance values are allowed?",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-11T21:10:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Stable-Diffusion-1-5-from-Vertex-AI-s-model-garden-what\/m-p\/611394#M2341",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":118,
        "Question_body":"I have deployed stable diffusion 1.5 from Vertex AI's model garden. I have an endpoint on which I am successfully running online inference with a request payload like the following:\n\n{ \"instances\": [{ \"prompt\": interesting_prompt }] }\n\nStable diffusion should be able to accept many parameters, including width\/height, sampling steps, n_samples\/batch size, etc.\u00a0 But I've been unable to figure out how to pass these parameters in a Vertex AI payload. It makes the most sense to me to pass them as key\/value pairs alongside the prompt in the same object in `instances`, but no key names besides prompt that I've tried have changed the output at all. I haven't found any documentation to support these use cases.\n\nHow can I pass these additional parameters for online inference?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialog flow cx Integration Twilio",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-11T08:23:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialog-flow-cx-Integration-Twilio\/m-p\/521061#M1267",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":124,
        "Question_body":"Dialog flow CX having problems in integration twilio options is no more visible.\nPreviously I was able to change\/update the integration easily till 11-02-2023.\n\nNow it just doesn't show any option\u00a0for TWILIO.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"text-bison@001model usage",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-26T02:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/text-bison-001model-usage\/m-p\/606457#M2229",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":23,
        "Question_body":"Hi I tried to use\u00a0text-bison@001\u00a0to generate chapter title from\u00a0 subtitle transcript.\u00a0 But I always I got no result\u00a0 for some chunks, which never happens with openai's model. Anyone can help me on this? Thanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Safe Search Vision moderations API for detecting child porn",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-20T19:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Safe-Search-Vision-moderations-API-for-detecting-child-porn\/m-p\/595139#M1972",
        "Question_answer_count":3,
        "Question_score_count":1,
        "Question_view_count":211,
        "Question_body":"Hello everyone,\n\nWe just started using the Google Safe Search Vision API to provide image upload moderation on our platform.\n\nAt first, we moderated images flagged as \"adult\", \"medical\" and \"racy\".\n\nThen, we realized \"Racy\" is a little too strict, it easily rejects images with fully clothed cleavage (perhaps because it was a drawing maybe, I don't know). So we started allowing \"racy\".\n\nI want to know what category \"child porn\" and \"inappropriately dressed children\" images are flagged as. We had one instance where a user uploaded child porn images that could have been interpreted as a scantily clad pre-teen\/baby. Will images like that be flagged as \"adult\" or \"racy\"?\n\nI want to make sure that by denying images moderated as \"adult\" (VERY_LIKELY), that we will be able to detect & filter out images that are:\n\n1. Child porn that includes sex\n2. Child porn that includes no sex but inappropriately dressed children\n3. Nude photos of children in any pose (even if it's something like a baby being powdered or diapered)\n4. Clothed children in inappropriate poses\n\nFurthermore, I'm curious if there's an option to detect \"offensive\" images such as violent vomiting or literal toilet\/diarrhea photos, etcetera. Which of the existing labels would detect those kind of images (if any)? If those images are not detectable with Cloud Vision Safe Search API, what are our best options?\n\nThank you for your help, as this is very important to us.\n\nI look forward to your reply.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Soft Handover in Infinite streaming",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-14T02:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Soft-Handover-in-Infinite-streaming\/m-p\/602877#M2153",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":341,
        "Question_body":"This feature suggestion is inspired from \"Soft Handover\" that happens in mobile towers. For details:\u00a0https:\/\/en.wikipedia.org\/wiki\/Handover#Types\n\nBasically, with a 5 min streaming limit for Google Speech-To-Text (STT), there is an example provided in the `java-docs-samples` repo (and python and JS too) which continues to stream audio - infinitely -by opening a new stream and resending the past N audio packets. This is calculated from last final transcript's end time.\n\nThis uses what is referred to as \"break-and-make\" approach. This has some problems:\n\nWhen we can't establish a new stream (for whatever reason), there is unavailability.\nIt's highly coupled with word timings which may not be supported by all ASRs. I understand that's not very relevant to Google but, to an application that uses Google along with some other ASRs (For ex, specifically trained on a target corpus), this is becomes a bottleneck for extensibility.\nCalculation of audio packets to be sent is dependent on encoding and sampling. While this is acceptable as a specification, in some cases it might be a severe limitation.\nAlternative:\n\nI tried to work with a \"Soft handover\" method (\"make-and-break\") where we open another stream early (Say, 1 min before the limit) and send audio to both streams until the transcripts \"align\". When they do, we switch to new stream.\n\nWhile this has a few unique aspects to take care of, I believe this gives better control to the application and is not tied to word timings.\n\nI wanted to get feedback on the same. Attaching the code file (in java) here. For testing purposes, I have kept the\u00a0STREAMING_LIMIT\u00a0as 30 seconds.\n\nI am aware of some areas where I can sharpen this more (Ex: alignment algorithm), but I am looking for concrete and major concerns from experts or authors.\n\nHigh level overview of code\nThis class demonstrates how to perform infinite streaming speech recognition using theStreamingRecognize functionality of the Speech API. This class is almost identical to the\u00a0InfiniteStreamRecognize.java\u00a0class, except that it demonstrates how to perform \"soft handover\" between two streams.\nA \"soft handover\" is making a new stream before breaking the old stream. As against a \"hard handover\" where you break the old stream before making the new stream. This is useful in situations where you want to perform speech recognition on a continuous audio input, but need to periodically restart the stream to avoid exceeding the maximum allowed continuous streaming duration. For demonstration purposes only, this sample uses a reset duration of 30 seconds whereas the actual allowed duration is 5 minutes per Google documentation for Streaming API.\nThis class uses two streams, STREAM1 and STREAM2, and alternates between them. When one stream is active, the other stream is used to buffer audio input. When the active stream is stopped, the buffered audio input is used to create a new stream.\nThis class also demonstrates how to align the transcript of the previous stream with the transcript of the current stream using a very simplistic algorithm.\n\n\u00a0\n\n\/*\n * Copyright 2023 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage com.example.speech;\n\n\/\/ [START speech_transcribe_infinite_streaming_soft_handover]\n\nimport com.google.api.gax.rpc.ClientStream;\nimport com.google.api.gax.rpc.ResponseObserver;\nimport com.google.api.gax.rpc.StreamController;\nimport com.google.cloud.speech.v1p1beta1.RecognitionConfig;\nimport com.google.cloud.speech.v1p1beta1.SpeechClient;\nimport com.google.cloud.speech.v1p1beta1.SpeechRecognitionAlternative;\nimport com.google.cloud.speech.v1p1beta1.StreamingRecognitionConfig;\nimport com.google.cloud.speech.v1p1beta1.StreamingRecognitionResult;\nimport com.google.cloud.speech.v1p1beta1.StreamingRecognizeRequest;\nimport com.google.cloud.speech.v1p1beta1.StreamingRecognizeResponse;\nimport com.google.protobuf.ByteString;\nimport com.google.protobuf.Duration;\nimport java.text.DecimalFormat;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.LinkedBlockingQueue;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport javax.sound.sampled.AudioFormat;\nimport javax.sound.sampled.AudioSystem;\nimport javax.sound.sampled.DataLine;\nimport javax.sound.sampled.DataLine.Info;\nimport javax.sound.sampled.TargetDataLine;\n\n\/*\n * This class demonstrates how to perform infinite streaming speech recognition using the\n * StreamingRecognize functionality of the Speech API.\n * This class is almost identical to the InfiniteStreamRecognize.java class, except that it\n * demonstrates how to perform \"soft handover\" between two streams.\n * A \"soft handover\" is making a new stream before breaking the old stream. As against a \"hard\n * handover\" where you break the old stream before making the new stream.\n * This is useful in situations where you want to perform speech recognition on a continuous audio\n * input, but need to periodically restart the stream to avoid exceeding the maximum allowed\n * continuous streaming duration of 30 seconds.\n * This class uses two streams, STREAM1 and STREAM2, and alternates between them.\n * When one stream is active, the other stream is used to buffer audio input.\n * When the active stream is stopped, the buffered audio input is used to create a new stream.\n * This class also demonstrates how to align the transcript of the previous stream with the\n * transcript of the current stream. This is useful in situations where you want to perform\n * continuous speech recognition on a continuous audio input, but need to restart the stream\n *\/\n\npublic class InfiniteStreamRecognizeSoftHandOver {\n\n  private static final int STREAMING_LIMIT = 30000; \/\/ resetting 30 seconds to test\n\n  enum Stream {\n    STREAM1,\n    STREAM2;\n  }\n\n  public static final String RED = \"\\033[0;31m\";\n  public static final String GREEN = \"\\033[0;32m\";\n  public static final String YELLOW = \"\\033[0;33m\";\n\n  \/\/ Creating shared object\n  private static volatile BlockingQueue<byte[]> sharedQueue = new LinkedBlockingQueue<byte[]>();\n  private static TargetDataLine targetDataLine;\n  private static int BYTES_PER_BUFFER = 6400; \/\/ buffer size in bytes\n\n  private static int restartCounter = 0;\n  private static ArrayList<ByteString> audioInput = new ArrayList<ByteString>();\n  private static int resultEndTimeInMS = 0;\n  private static boolean newStream = false;\n  private static double bridgingOffset = 0;\n  private static volatile Stream activeStream = Stream.STREAM1;\n  private static ByteString tempByteString;\n  private static Map<String, StreamingRecognitionResult> streamWiseLatestResults =\n      new HashMap<String, StreamingRecognitionResult>();\n\n  public static void main(String... args) {\n    InfiniteStreamRecognizeOptions options = InfiniteStreamRecognizeOptions.fromFlags(args);\n    if (options == null) {\n      \/\/ Could not parse.\n      System.out.println(\"Failed to parse options.\");\n      System.exit(1);\n    }\n\n    try {\n      infiniteStreamingRecognize(options.langCode);\n    } catch (Exception e) {\n      System.out.println(\"Exception caught: \" + e);\n    }\n  }\n\n  public static String convertMillisToDate(double milliSeconds) {\n    long millis = (long) milliSeconds;\n    DecimalFormat format = new DecimalFormat();\n    format.setMinimumIntegerDigits(2);\n    return String.format(\n        \"%s:%s \/\",\n        format.format(TimeUnit.MILLISECONDS.toMinutes(millis)),\n        format.format(\n            TimeUnit.MILLISECONDS.toSeconds(millis)\n                - TimeUnit.MINUTES.toSeconds(TimeUnit.MILLISECONDS.toMinutes(millis))));\n  }\n\n  private static boolean computeIntersectionRatio(\n      String previousTranscript, String currentTranscript) { \/\/ Compare the\n    \/\/ transcripts\n    \/\/ for\n    \/\/ alignment\n    int commonWordCount = 0;\n    String[] previousWords = previousTranscript.split(\"\\\\s+\");\n    String[] currentWords = currentTranscript.split(\"\\\\s+\");\n\n    \/\/ Compute word intersection ratio between previous and current by reverse traversing the arrays\n    for (int i = previousWords.length - 1; i >= 0; i--) {\n      for (int j = currentWords.length - 1; j >= 0; j--) {\n        if (previousWords[i].equalsIgnoreCase(currentWords[j])) {\n          commonWordCount++;\n          \/\/ This is a simplistic alignment algorithm which works ok.\n          \/\/ However, real world use cases may require more sophisticated algorithms\n          if (commonWordCount >= 3) {\n            return true;\n          }\n        }\n      }\n    }\n\n    return false;\n  }\n\n  \/** Performs infinite streaming speech recognition *\/\n  public static void infiniteStreamingRecognize(String languageCode) throws Exception {\n\n    \/\/ Microphone Input buffering\n    class MicBuffer implements Runnable {\n\n      @Override\n      public void run() {\n        System.out.println(YELLOW);\n        System.out.println(\"Start speaking...Press Ctrl-C to stop\");\n        targetDataLine.start();\n        byte[] data = new byte[BYTES_PER_BUFFER];\n        while (targetDataLine.isOpen()) {\n          try {\n            int numBytesRead = targetDataLine.read(data, 0, data.length);\n            if ((numBytesRead <= 0) && (targetDataLine.isOpen())) {\n              continue;\n            }\n            sharedQueue.put(data.clone());\n          } catch (InterruptedException e) {\n            System.out.println(\"Microphone input buffering interrupted : \" + e.getMessage());\n          }\n        }\n      }\n    }\n\n    \/\/ Creating microphone input buffer thread\n    MicBuffer micrunnable = new MicBuffer();\n    Thread micThread = new Thread(micrunnable);\n    \/\/ one for each stream, just for clarity. Can be optimized.\n    ResponseObserver<StreamingRecognizeResponse> responseObserver1 = null;\n    ResponseObserver<StreamingRecognizeResponse> responseObserver2 = null;\n    try (SpeechClient client = SpeechClient.create()) {\n      \/\/ Active stream is used to send data.\n      ClientStream<StreamingRecognizeRequest> stream1 = null;\n      ClientStream<StreamingRecognizeRequest> stream2 = null;\n\n      responseObserver1 =\n          new ResponseObserver<StreamingRecognizeResponse>() {\n            ArrayList<StreamingRecognizeResponse> responses = new ArrayList<>();\n\n            public void onStart(StreamController controller) {}\n\n            public void onResponse(StreamingRecognizeResponse response) {\n              responses.add(response);\n              if (response.getResults(0).getIsFinal()) { \/\/ we only compare final and finals has\n                \/\/ only one results\n                streamWiseLatestResults.put(\"stream1\", response.getResults(0));\n              }\n              StreamingRecognitionResult result = response.getResultsList().get(0);\n              Duration resultEndTime = result.getResultEndTime();\n              resultEndTimeInMS =\n                  (int)\n                      ((resultEndTime.getSeconds() * 1000) + (resultEndTime.getNanos() \/ 1000000));\n              double correctedTime =\n                  resultEndTimeInMS - bridgingOffset + (STREAMING_LIMIT * restartCounter);\n\n              if (activeStream == Stream.STREAM1) {\n                SpeechRecognitionAlternative alternative = result.getAlternativesList().get(0);\n                if (result.getIsFinal()) {\n                  System.out.print(GREEN);\n                  System.out.print(\"\\033[2K\\r\");\n                  System.out.printf(\n                      \"%s: %s [confidence: %.2f] [stream: %s]\\n\",\n                      convertMillisToDate(correctedTime),\n                      alternative.getTranscript(),\n                      alternative.getConfidence(),\n                      \"stream1\");\n                } else {\n                  System.out.print(RED);\n                  System.out.print(\"\\033[2K\\r\");\n                  System.out.printf(\n                      \"%s: %s\", convertMillisToDate(correctedTime), alternative.getTranscript());\n                }\n              }\n            }\n\n            public void onComplete() {}\n\n            @Override\n            public void onError(Throwable t) {}\n          };\n      stream1 = client.streamingRecognizeCallable().splitCall(responseObserver1);\n\n      responseObserver2 =\n          new ResponseObserver<StreamingRecognizeResponse>() {\n            ArrayList<StreamingRecognizeResponse> responses = new ArrayList<>();\n\n            public void onStart(StreamController controller) {}\n\n            public void onResponse(StreamingRecognizeResponse response) {\n              responses.add(response);\n              if (response.getResults(0).getIsFinal()) { \/\/ we only compare final and finals has\n                \/\/ only one results\n                streamWiseLatestResults.put(\"stream2\", response.getResults(0));\n              }\n              StreamingRecognitionResult result = response.getResultsList().get(0);\n              Duration resultEndTime = result.getResultEndTime();\n              resultEndTimeInMS =\n                  (int)\n                      ((resultEndTime.getSeconds() * 1000) + (resultEndTime.getNanos() \/ 1000000));\n              double correctedTime =\n                  resultEndTimeInMS - bridgingOffset + (STREAMING_LIMIT * restartCounter);\n\n              if (activeStream == Stream.STREAM2) {\n                SpeechRecognitionAlternative alternative = result.getAlternativesList().get(0);\n                if (result.getIsFinal()) {\n                  System.out.print(GREEN);\n                  System.out.print(\"\\033[2K\\r\");\n                  System.out.printf(\n                      \"%s: %s [confidence: %.2f] [stream: %s]\\n\",\n                      convertMillisToDate(correctedTime),\n                      alternative.getTranscript(),\n                      alternative.getConfidence(),\n                      \"stream2\");\n\n                } else {\n                  System.out.print(RED);\n                  System.out.print(\"\\033[2K\\r\");\n                  System.out.printf(\n                      \"%s: %s\", convertMillisToDate(correctedTime), alternative.getTranscript());\n                }\n              }\n            }\n\n            public void onComplete() {}\n\n            public void onError(Throwable t) {}\n          };\n\n      RecognitionConfig recognitionConfig =\n          RecognitionConfig.newBuilder()\n              .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16)\n              .setLanguageCode(languageCode)\n              .setSampleRateHertz(16000)\n              .build();\n\n      StreamingRecognitionConfig streamingRecognitionConfig =\n          StreamingRecognitionConfig.newBuilder()\n              .setConfig(recognitionConfig)\n              .setInterimResults(true)\n              .build();\n\n      StreamingRecognizeRequest request =\n          StreamingRecognizeRequest.newBuilder()\n              .setStreamingConfig(streamingRecognitionConfig)\n              .build(); \/\/ The first request in a streaming call has to be a config\n\n      stream1.send(request);\n\n      try {\n        \/\/ SampleRate:16000Hz, SampleSizeInBits: 16, Number of channels: 1, Signed:\n        \/\/ true,\n        \/\/ bigEndian: false\n        AudioFormat audioFormat = new AudioFormat(16000, 16, 1, true, false);\n        DataLine.Info targetInfo =\n            new Info(\n                TargetDataLine.class,\n                audioFormat); \/\/ Set the system information to read from the microphone audio\n        \/\/ stream\n\n        if (!AudioSystem.isLineSupported(targetInfo)) {\n          System.out.println(\"Microphone not supported\");\n          System.exit(0);\n        }\n        \/\/ Target data line captures the audio stream the microphone produces.\n        targetDataLine = (TargetDataLine) AudioSystem.getLine(targetInfo);\n        targetDataLine.open(audioFormat);\n        micThread.start();\n\n        long startTime = System.currentTimeMillis();\n        AtomicBoolean isAligned = new AtomicBoolean(false);\n\n        while (true) {\n\n          long estimatedTime = System.currentTimeMillis() - startTime;\n\n          if (estimatedTime >= STREAMING_LIMIT) {\n            \/\/ reset started, bring passive stream to life!\n            restartCounter++;\n            newStream = true;\n\n            request =\n                StreamingRecognizeRequest.newBuilder()\n                    .setStreamingConfig(streamingRecognitionConfig)\n                    .build();\n\n            if (activeStream\n                == Stream.STREAM2) { \/\/ meaning stream2 is active, so prepare stream 1 for reset\n              stream1 = client.streamingRecognizeCallable().splitCall(responseObserver1);\n              while (!stream1.isSendReady()) {\n                Thread.sleep(5);\n              }\n              stream1.send(request);\n            } else { \/\/ meaning stream1 is active, so prepare stream 2 for reset\n              stream2 = client.streamingRecognizeCallable().splitCall(responseObserver2);\n              while (!stream2.isSendReady()) {\n                Thread.sleep(5);\n              }\n              stream2.send(request);\n            }\n\n            System.out.println(YELLOW);\n            System.out.printf(\"%d: SOFT RESTARTING REQUEST\\n\", restartCounter * STREAMING_LIMIT);\n\n            \/\/ reset startTime\n            startTime = System.currentTimeMillis();\n\n          } else {\n            if (newStream) {\n              \/\/ we're here, that means the second stream has been opened now\n\n              if (!isAligned.get()) {\n                \/\/ now we're comparing for aligment\n                \/\/ Send audio to two streams to check for alignment:\n                request =\n                    StreamingRecognizeRequest.newBuilder().setAudioContent(tempByteString).build();\n\n                stream1.send(request);\n                stream2.send(request);\n\n                if (streamWiseLatestResults.size() > 0) {\n                  \/\/ we have results to compare!\n                  StreamingRecognitionResult result1 = streamWiseLatestResults.get(\"stream1\");\n                  StreamingRecognitionResult result2 = streamWiseLatestResults.get(\"stream2\");\n\n                  if (result1 != null && result2 != null) {\n                    isAligned.set(\n                        computeIntersectionRatio(\n                            result1.getAlternatives(0).getTranscript(),\n                            result2.getAlternatives(0).getTranscript()));\n                  }\n                }\n              } else {\n                newStream = false;\n                isAligned.set(false);\n                \/\/ toggle streams\n                if (activeStream == Stream.STREAM1) {\n                  activeStream = Stream.STREAM2;\n                  stream1.closeSend();\n                  stream1 = null;\n                } else {\n                  activeStream = Stream.STREAM1;\n                  stream2.closeSend();\n                  stream2 = null;\n                }\n              }\n            }\n            tempByteString = ByteString.copyFrom(sharedQueue.take());\n\n            request =\n                StreamingRecognizeRequest.newBuilder().setAudioContent(tempByteString).build();\n\n            audioInput.add(tempByteString);\n\n            if (activeStream == Stream.STREAM1) {\n              stream1.send(request);\n            } else {\n              stream2.send(request);\n            }\n          }\n        }\n      } catch (Exception e) {\n        System.out.println(e);\n      }\n    }\n  }\n}\n\/\/ [END speech_transcribe_infinite_streaming_soft_handover]\n\n\u00a0\n\nThanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Is there a way to resend invitation emails to labelers inside a Vertex AI Labeler group?",
        "Question_tag_count":1,
        "Question_created_time":"2021-10-26T09:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Is-there-a-way-to-resend-invitation-emails-to-labelers-inside-a\/m-p\/173962#M68",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":420,
        "Question_body":"I have added a Labeler Group to my Labeling task in Vertex AI but none of the members of the labeler group received an invite email.\n\nHow can I ensure this invite email is sent?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google cloud text to speech",
        "Question_tag_count":2,
        "Question_created_time":"2021-09-26T06:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-text-to-speech\/m-p\/171231#M54",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":823,
        "Question_body":"I registered myself for the google cloud text-to-speech service recently. Speech Studio worked just fine for the first few days, but today, to my dismay, there is distortion in the text reader's voice.\n\nWhat can I do about it?\n\nThanks.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Issue with DialogFlow ES Caching and Entity Annotation",
        "Question_tag_count":3,
        "Question_created_time":"2023-05-30T03:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-with-DialogFlow-ES-Caching-and-Entity-Annotation\/m-p\/598132#M2047",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":48,
        "Question_body":"Hi everyone,\n\nI'm running into a perplexing issue with DialogFlow ES - the system seems to be holding onto old data, even though I've removed certain entities.\n\nHere's the situation: I've created new composite entities, one of which has an alias that contains the entry \"Technical Manager\". When I annotate a training phrase to this entity, I'm hit with a validation error, saying \"Technical Manager\" doesn't correspond to the entity, even though it should. If I add the phrase without annotation, the system links it to an old, deleted entity which indicates that the system still remember this old entity.\n\nAdditionally, during training, I noticed the agent initiates multiple training cycles with only seconds between them.\n\nAs a result the behavior of the agent is quite unpredicted, sometimes it fill slots with \"NO\" or even empty \"\" values, I think there's some sort of caching issue on sever side that does this and affects the training process.\n\nMy attempts to resolve this (backing up and restoring the agent, even with different agents) haven't worked. Oddly, I don't see the old entity that the system still remembers in the JSON files of the backup.\n\nAnyone encountered this or have any ideas to fix it?\n\nThanks in advance!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow ES: Keeping session open while user is visiting multiple pages on a site",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-28T15:46:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-Keeping-session-open-while-user-is-visiting\/m-p\/548662#M1760",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":165,
        "Question_body":"I'm working on a project using Dialogflow ES.\u00a0 When testing on the site, I see that a Dialogflow session lasts as long as a user is on the same page.\u00a0 If a user navigates to a different page on the site, the Dialogflow session resets and the user can't see the history of the conversation.\u00a0 Is there a way to keep a Dialogflow ES session open as long as the user is on the site domain so he\/she can navigate the site while interacting with Dialogflow?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Batch prediction forecasting",
        "Question_tag_count":3,
        "Question_created_time":"2022-08-08T09:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Batch-prediction-forecasting\/m-p\/451758#M494",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":96,
        "Question_body":"Dear ALL;\n\nI have made a sales and demand forecasting autoML model. It trained well and is working. I am looking for a way to format input data for batch prediction forecasting where I would like to do more than one forecast horizon predictions.\n\nBasically my model uses monthly data granularity with 12 month context length and 6 month forecast horizon. (I have 15 covariate features and predict for 14 separate identifiers).\n\nWhat I would like to do is configure the input data for a batch forecasting where the forecasting would start 3 month earlier than where the feature data end and make forecasts for these three time periods AND when the feature data ends do the normal 6 month forecasting so I end up with 3 month forecasting where I know the actuals and the normal 6 month forecasting.\n\nAt this point no matter how I format my input data I only get the 6 month forecast horizon.\n\nThanks",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to get textStyle in JSON response with Document ai",
        "Question_tag_count":1,
        "Question_created_time":"2022-08-04T05:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-get-textStyle-in-JSON-response-with-Document-ai\/m-p\/450434#M484",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":89,
        "Question_body":"I was trying to get text style or Font style with document ai but was getting null list..\n\nThis is the file I wanted text style to be extracted\n\nThis was the response I received.\n\ncan someone help me with this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"VM Ram vs Google Colab Ram",
        "Question_tag_count":2,
        "Question_created_time":"2022-07-27T12:51:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VM-Ram-vs-Google-Colab-Ram\/m-p\/447466#M452",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":119,
        "Question_body":"Hi, @Eduardo_Ortiz\u00a0 @josegutierrez\u00a0sorry to bother but I`m completely lost\n\nDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.\n\nVM Configuration : GPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nRam obtained in Google Colab from the VM: 1.31 Gb \/ 51.01 Gb Disc 43.79 \/ 186.52\n\nAs you realized,\u00a0 althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....\n\nWhat could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?\n\nIn the next screen shot the ram and disck that I got from Google Colab:\n\nThanks a lot for any help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX: Inhibit automatic annotation of training phrases",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-11T02:55:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-Inhibit-automatic-annotation-of-training-phrases\/m-p\/508942#M1046",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":87,
        "Question_body":"I have training phrases like this:\n\n\u00a0\u00a0\u00a0 I want to order a new project\n\nand I enter many versions of that and it all works: Dialogflow understands my intent is to order a new project. But sometimes the input is\n\n\u00a0\u00a0\u00a0 I want to order a new project called SalesPitch\n\nwhere SalesPitch has to conform to a projectName entity type - [A-Za-z0-9]+, in regex terms. When I define this custom entity, and enter a new training phrase, the Dialogflow CX console tends to break the phrase down into individual words, and annotate every word as a projectName, and create a parameter for every word. It would be much more convenient if Dialogflow would not attempt auto annotation, leaving me to annotate the word I wanted myself.\n\nIs there any way to turn off the auto annotation?\n\nHere's what happens:\n\n\u00a0\n\n\u2003\n\nThis gets even more ridiculous. Now I'm editing a different intent, no parameter capture at all, and this is what happens with automated annotation:",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow cx caller abandoned call event",
        "Question_tag_count":1,
        "Question_created_time":"2022-10-26T05:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-caller-abandoned-call-event\/m-p\/482361#M688",
        "Question_answer_count":2,
        "Question_score_count":2,
        "Question_view_count":142,
        "Question_body":"Is there a way to trigger a webhook, as soon as the caller abandons the call?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Fine tuning text-bison model",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-03T08:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Fine-tuning-text-bison-model\/m-p\/608823#M2272",
        "Question_answer_count":0,
        "Question_score_count":1,
        "Question_view_count":234,
        "Question_body":"Hi everyone\n\nI currently have a pipeline using OpenAI where I pass information about my internal company database tables as a prompt, and then ask a user defined question, and then get an SQL query and a response.\n\nAs you might have guessed, this takes alot of tokens since I need to describe my tables in the prompt and costs alot.\n\nI am trying to now fine tune a text-bison model by passing it training examples of the input text along with an appropriate output response. For the training, I can pass the same prompt as the OpenAI pipeline, where I describe my tables and then ask the model to generate a query.\n\nBut, the Vertex AI page on fine tuning says to use training examples which will be the same as you would get an input in production. This would mean that I also pass the whole table description in the production pipeline as well, and this is exactly what I am trying to avoid.\n\nAs an example:\n\nFor training:\n\nContext: You have the following tables to gather data from:\n\nTable 1 description\nTable 2 description\n\nInput: What is the price of Lockheed Martin stocks?\n\nOutput: The price of stock is X.\n\nIn the above example, the model knows the tables through the context and then finds the appropriate table for the text it was given and generates a response.\n\nBut in a production environment, I want to give only the 'input', and not the table descriptions (i.e. context), since that would take up tokens and cost more, and that is what I am trying to avoid in the first place.\n\nAny idea how to go about this or am I approaching the problem in the wrong way?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Manage Labeling Assignments on DataCompute",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-11T20:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Manage-Labeling-Assignments-on-DataCompute\/m-p\/175499#M83",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":0,
        "Question_body":"Our team has started to use the DataCompute console to assign labelers to labeling tasks created in Vertex AI.\u00a0\n\nCurrently, the Assignments tab requires the Labeling Manager to Populate the Specialists Column and Populate the Tasks Column\u00a0\n\nI wanted to highlight some issues we are facing and ask if there's any plan to implement fixes.\n\nIssues:\u00a0\n\n1. The dropdown for task selection does not order the tasks alphabetically so it is difficult to find a specific task.\n\n2. There's no \"Select All\" option, instead, the manager must select each task individually.\n\n3. There is no drop down for the specialist emails even though they are available under the Specialists tab.\n\nGenerally, it would be nice to see the entire assignment table by default rather than nothing on this page.\n\nLet me know if some of these issues can be addressed!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Glossary application in Google Translate custom AutoML models",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-05T14:43:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Glossary-application-in-Google-Translate-custom-AutoML-models\/m-p\/550784#M1805",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":133,
        "Question_body":"I am using a TMS to perform translations by connecting to my custom AutoML models.\u00a0 When I don't use glossaries in the TMS setup, the engine seems to behave normally (with the usual mistakes an MT engine can make), but when I use a complex glossary made out of several thousand entries of product names my company makes, I start seeing weird behaviors such as duplication of terms and dropping of important part of text.\n\nIs there a document that explains what mechanism Google Translate V3 is going through when applying a glossary to a sentence?\u00a0 I don't\u00a0 think it's just a straight search and replace.\u00a0 Can you shed some light?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BigQuery ML Evaluation Metrics",
        "Question_tag_count":1,
        "Question_created_time":"2023-03-27T09:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BigQuery-ML-Evaluation-Metrics\/m-p\/537516#M1512",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":90,
        "Question_body":"Hi,\n\nWondering for BigQuery ML model evaluation, is it possible to pass custom metrics for model evaluation or model comparison purpose while using BigQuery. I am planning to do a binary classification and have some self defined metrics that I want to use to compare and pick the best model.\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AI Augmented Sensory Headset",
        "Question_tag_count":2,
        "Question_created_time":"2022-10-07T20:18:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-Augmented-Sensory-Headset\/m-p\/475836#M629",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":40,
        "Question_body":"Wondering when Google will develop olfactory sensor addition to VR headsets and technology. In laymens terms, adding the sense of smell to VR headsets using an add on similar to a printer ink cartridge, but designed specifically for the sense of smell. Theoretically, it is possible, but to manufacture it in a large scale. It can change the way programs, especially helping boost the food and hospitality industry as well as giving everyday people a very good reason to smell fresh food and drink... from their phone! Where and how can we further this research for this wonderful idea?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Failed to submit prompt Error message: \"Internal error encountered.\" Status: 500 Error code: 500",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-09T03:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Failed-to-submit-prompt-Error-message-quot-Internal-error\/m-p\/601579#M2111",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":72,
        "Question_body":"Hi all,\n\nCreated following a sample JSONL format,\nUploaded in the GCS bucket. gs:\/\/xxx\/newsam.jsonl\nwe have created our own custom model using the curl command.\n\ncurl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application\/json; charset=utf-8\" -d \"@model-request.json\"\n\n\nThe pipeline was successfully executed and deployed in the model registry.\nModel name is : LLM\n\nFrom the model registry, we have created the end point.\nEnd point name :\nsample1\nEndpoint id: 59XXX079194845184\n\nWhen we test the end point using the curl command, it throws \"internal error encountered,\" error 500.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Does Dialog flow (ES or CX) have the Capability?",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-01T21:25:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Dialog-flow-ES-or-CX-have-the-Capability\/m-p\/516750#M1171",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":128,
        "Question_body":"Dear Team,\n\n\u00a0\n\nbefore proceeding with Dialogflow , we need to confirm if we can achieve below requirements with Google dialogflow\n\n1. Passing values to the Bot: When Bot is initialized on webpage, we want to send some values to the Bot directly (like passing values from backend), and the Bot should not need to take inputs from the user. Can we do this here with any workaround?\n\n2. Show Values in Tabular Format: Bot should display values received from API in the tabular format (or any other presentable format). Need guidance here.\n\n3.\u00a0Load existing chat history: Whenever user opens the Bot, it should load existing chat history in the chat window. Can we do it with this?\n\n4.\u00a0Send Notifications to Bot: We need to send confirmation notifications to the bot, we are connecting to the other services\u00a0\n\nIf we get initial heads up, we can spend time on exploration on this type of customization.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"fail counter - Dialogflow",
        "Question_tag_count":1,
        "Question_created_time":"2022-12-22T07:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/fail-counter-Dialogflow\/m-p\/502611#M982",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":73,
        "Question_body":"I require in dialogflow to control that when a user makes a mistake in the answer, it does not let him make a mistake more than x times. Who can have control of those fail\n\nCreate a custom \"fail\"\n\n\u00a0\n\nI require in dialogflow to control that when a user makes a mistake in the answer, it does not let him make a mistake more than x times. Who can have control of those fail\n\nCreate a custom \"fail\"\n\n\u00a0\n\nI require in dialogflow to control that when a user makes a mistake in the answer, it does not let him make a mistake more than x times. Who can have control of those fail\n\nCreate a custom \"fail\"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI: Workbench - Failing to Created a schedule based recurrent execution",
        "Question_tag_count":2,
        "Question_created_time":"2022-12-12T23:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Workbench-Failing-to-Created-a-schedule-based\/m-p\/498980#M934",
        "Question_answer_count":8,
        "Question_score_count":0,
        "Question_view_count":296,
        "Question_body":"Hello,\n\nI am trying to create a schedule-based recurrent execution via the Vertex AI workbench. It fails with error code 2- unknown error. Attaching the error snippet.\u00a0\n\nAny help to understand the root cause is highly appreciated.\u00a0\nappreciated.\u00a0\n\nNote: I can do standalone execution successfully. It's only when trying to create a schedule based event, this error occurs.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Google Vision, Text detection, bug : ImportError: cannot import name 'cygrpc' from 'grpc._cython'",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-08T06:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vision-Text-detection-bug-ImportError-cannot-import-name\/m-p\/519815#M1247",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":217,
        "Question_body":"Hello,\n\nWhen running recommanded code for text detection on page\u00a0D\u00e9tecter le texte dans les images \u00a0|\u00a0 API Cloud\u00a0Vision \u00a0|\u00a0 Google Cloud\u00a0:\n\n\u00a0\n\ndef detect_text(path):\n    \"\"\"Detects text in the file.\"\"\"\n    from google.cloud import vision\n    import io\n    client = vision.ImageAnnotatorClient()\n\n    with io.open(path, 'rb') as image_file:\n        content = image_file.read()\n\n    image = vision.Image(content=content)\n\n    response = client.text_detection(image=image)\n    texts = response.text_annotations\n    print('Texts:')\n\n    for text in texts:\n        print('\\n\"{}\"'.format(text.description))\n\n        vertices = (['({},{})'.format(vertex.x, vertex.y)\n                    for vertex in text.bounding_poly.vertices])\n\n        print('bounds: {}'.format(','.join(vertices)))\n\n    if response.error.message:\n        raise Exception(\n            '{}\\nFor more info on error messages, check: '\n            'https:\/\/cloud.google.com\/apis\/design\/errors'.format(\n                response.error.message))\n\n\u00a0\n\n... I get following error:\n\n\u00a0\n\nImportError: cannot import name 'cygrpc' from 'grpc._cython'\n\n\u00a0\n\nI found somewhere that it might be my python version. So, I changed from Python310 to Python377. But nothing changed.\n\nWould you have any idea?\n\nKind regards.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Training phrases are not returned via python API",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-23T01:27:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Training-phrases-are-not-returned-via-python-API\/m-p\/595897#M1995",
        "Question_answer_count":10,
        "Question_score_count":0,
        "Question_view_count":114,
        "Question_body":"I am trying to get an overview of the training phrases per intent from Dialogflow in python. However, for every intent I get an empty list when accessing the training phrases (while I can see the training phrases in the portal). A more extensive description of this issue can be found\n\nhere:\u00a0https:\/\/stackoverflow.com\/questions\/76153606\/dialogflow-doesnt-return-training-phrases.",
        "Question_closed_time":"05-26-2023 02:50 AM",
        "Answer_score_count":0.0,
        "Answer_body":"The solution for getting all the training phrases per intent was to pass the intent_view=\"INTENT_VIEW_FULL\" to the request. In the stackoverflow post, a full code snippet can be found (https:\/\/stackoverflow.com\/questions\/76153606\/dialogflow-doesnt-return-training-phrases\/)\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"Google cloud functions",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-03T04:28:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-cloud-functions\/m-p\/599739#M2073",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":36,
        "Question_body":"Hi guys how to solve this error plz\n\nERROR: (gcloud.functions.deploy) Uncompressed deployment is 1486838328B, bigger than maximum allowed size of 536870912B.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"BatchAnnotateImagesResponse images info (context support) for web based images",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-23T01:21:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/BatchAnnotateImagesResponse-images-info-context-support-for-web\/m-p\/181129#M127",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":172,
        "Question_body":"Hi all. How could one know in what exact web based image the text was detected when multiple web based images are sent to the cloud vision api in a single request using BatchAnnotateImagesRequest? BatchAnnotateImagesResponse doesn't return that information which is kinda odd... It has ImageAnnotationContext, which holds image details, but it's reserved only for files and not web based images.\n\nIs there some way to do this? Maybe like preserving order of images in request \/ response or something down that line.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Receiving quota error when trying to use bison chat model in Vertex AI",
        "Question_tag_count":3,
        "Question_created_time":"2023-05-11T07:39:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Receiving-quota-error-when-trying-to-use-bison-chat-model-in\/m-p\/552421#M1857",
        "Question_answer_count":42,
        "Question_score_count":7,
        "Question_view_count":0,
        "Question_body":"Hi, I want to try out the new bison chat model. However, when I'm asking anything I'm receiving this error:\u00a0\n\nQuota exceeded for aiplatform.googleapis.com\/online_prediction_requests_per_base_model with base model: chat-bison. Please submit a quota increase request.",
        "Question_closed_time":"05-24-2023 09:39 AM",
        "Answer_score_count":2.0,
        "Answer_body":"UPDATE: We have raised the default quotas for everyone.\u00a0 This roll out may take a day to reach everyone so.\u00a0 Thank you everyone for your patience and flagging this to us!\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Document AI Invoice Parser - different results in Training tab vs Test&Analyze",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-22T03:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-Invoice-Parser-different-results-in-Training-tab-vs\/m-p\/605606#M2205",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":85,
        "Question_body":"Hi all,\n\nWe're testing Document AI Invoice Parser processor for parsing .pdf invoices, and one thing I find confusing is that sometimes the \"Train\" tab has way more parsed data elements than the \"Evaluate & Test\" tab, given the same exact invoice.\u00a0\n\nFor example, if I open the invoice in the \"Train\" tab, I can see the `vendor_name`, `line_item_description` etc, while in the \"Evaluate & Test\" tab I can't see those data points parsed.\n\nWe would integrate with Document AI via API, so I'm wondering what results we can expect - the richer ones from the Train tab, or the more rudimentary ones from the Evaluate & Test tab. Also would like to understand why there's a difference between the two.\n\nThank you!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to correct labels that have been read",
        "Question_tag_count":2,
        "Question_created_time":"2022-12-19T22:13:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-correct-labels-that-have-been-read\/m-p\/501393#M970",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":94,
        "Question_body":"Hello,\u00a0\n\nI used Document AI form processor to convert pdf file.\n\nI hit the confirm button to edit the label to the correct one because it was not reading correctly, but nothing responded.\n\u203b I`ve added the text and tried to save it.\n\nIt seems to be a javascript error, but I don't think this has happened before.\n\nDo you have any info on this?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Loading Google Docs and Google Sheets into Bard",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-24T13:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Loading-Google-Docs-and-Google-Sheets-into-Bard\/m-p\/615286#M2448",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":21,
        "Question_body":"I can't load docs or sheets into Bard. Think I'm following the instructions correctly using the command (for Docs): import_google_doc(document id)\n\nI always get an error message 'the google doc id you provided is invalid' but it appears correct (ie the string after \/d\/\u00a0 which is copied from the URL in the search bar.\u00a0\n\nAny clues anyone?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Cloud Vision API in Vertex AI?",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-28T02:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-API-in-Vertex-AI\/m-p\/493648#M867",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":726,
        "Question_body":"Hi,\n\nI am a newbie in Google Cloud and i have an elementary conceptual question about the dependency between\u00a0Cloud Vision API and Vertex AI or the recently launched Vertex Vision AI.\n\nI have an app that makes predictions on images using Google Vision AI API\u00a0ImageAnnotatorClient()\u00a0\n\nIs this API going to be part of\u00a0 Vertex AI\u00a0 or Vertex Vision AI?\n\nOr in other words, should I modify the below code to make it part of Vertex AI\/Vertex Vision AI?\n\n\u00a0\n\nfrom google.cloud import vision\n\ndef detect_labels_uri(uri):\n    client = vision.ImageAnnotatorClient()\n    image = vision.Image()\n    image.source.image_uri = uri\n\n    response = client.label_detection(image=image)\n    labels = response.label_annotations\n    return(labels)",
        "Question_closed_time":"11-29-2022 10:23 AM",
        "Answer_score_count":1.0,
        "Answer_body":"Vision API should be affected by Vertex Vision AI and your code should stay as is. Vision API is primarily used to detect vision features like\u00a0image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content. While Vertex Vision AI is an end to end solution to ingest, analyze and store video and image data.\u00a0Vertex AI Vision lets users build and deploy applications with a simplified UX.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Internal error while trying to import csv for object detection AutoML",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-08T23:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Internal-error-while-trying-to-import-csv-for-object-detection\/m-p\/520214#M1256",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":69,
        "Question_body":"I have created a csv according to the format required by gcp for object detection. there are about 50,000 images in my folder. when i try to import the csv for object detection, it is giving me an internal error.\u00a0\n\n\u00a0\n\ni have cross checked everything, and everything seems right. can someone help me with why this is happening?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unstructured data in Vertex AI feature store",
        "Question_tag_count":1,
        "Question_created_time":"2021-11-30T16:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unstructured-data-in-Vertex-AI-feature-store\/m-p\/176796#M101",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":459,
        "Question_body":"Does Vertex AI feature store support ingestion, transformation and storage of unstructured data like images and audio?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can we recover a deleted dialogflow cx agent",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-26T00:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-we-recover-a-deleted-dialogflow-cx-agent\/m-p\/547328#M1746",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":51,
        "Question_body":"Hi need help with recovering an agent that was accidentally deleted from the Dialogflow CX Console. Can someone guide with potential steps that can be followed to recover this.\n\nFYI\n\n1. We donot have any backup blob files to restore from\n\n2. We had audit logging enabled incase if that can help",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Model training never finished",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-23T23:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Model-training-never-finished\/m-p\/546578#M1729",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":93,
        "Question_body":"Hi,\n\n\u00a0\nFrom the document, it says that the initial model training and tuning takes 2-5 days to complete. But I created few models (Similar items, Frequently bought together,\u00a0Buy it again) and started training on Apr 16, and it's not finished yet after over 8 days. Anybody knows what the problem may be? Thank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Error in GCP Doc AI project",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-09T03:59:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-in-GCP-Doc-AI-project\/m-p\/487561#M756",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":70,
        "Question_body":"Good evening . My peer while try to access Document AI page is getting the below error . Facing this issue from 2 PM yesterday. We are working for a POC project from LTI organization.\u00a0\n\nBasically, it should show some processors or specialized processors. Please can you guide us.\n\nRegards,\n\nVamsi",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Create TPU Node - Malformed Name",
        "Question_tag_count":1,
        "Question_created_time":"2022-02-26T05:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-TPU-Node-Malformed-Name\/m-p\/397566#M211",
        "Question_answer_count":5,
        "Question_score_count":1,
        "Question_view_count":299,
        "Question_body":"Hi! Im trying to create a Google Cloud TPU node using\u00a0TPU client API\u00a0and I cannot figure out the parent resource name of a TPU node in Google Cloud.I tried all the possible combinations, for example:\n\nprojects\/my-project-id\/locations\/europe-west4-a\nhttp:\/\/tpu.googleapis.com\/v2alpha1\/projects\/my-project-id\/locations\/europe-west4-a\n\/\/tpu.googleapis.com\/v2alpha1\/projects\/my-project-id\/locations\/europe-west4-a\netc...\n\nAnd I always get the same error (google.api_core.exceptions.InvalidArgument: 400 Malformed name) :\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Smarthank\\anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 67, in error_remapped_callable\n    return callable_(*args, **kwargs)\n  File \"C:\\Users\\Smarthank\\anaconda3\\lib\\site-packages\\grpc\\_channel.py\", line 923, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"C:\\Users\\Smarthank\\anaconda3\\lib\\site-packages\\grpc\\_channel.py\", line 826, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Malformed name: 'projects\/my-project-id\/locations\/europe-west4-a\/nodes\/'\"\n\tdebug_error_string = \"{\"created\":\"@1645878700.379000000\",\"description\":\"Error received from peer ipv4:142.250.179.170:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1068,\"grpc_message\":\"Malformed name: 'projects\/my-project-id\/locations\/europe-west4-a\/nodes\/'\",\"grpc_status\":3}\"\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nBelow you can find the full code I'm using to create the node. Im using Python 3.8, google-cloud-tpu v1.2.1, on a Conda virtualenv.\n\n\u00a0\n\nfrom google.cloud import tpu_v2alpha1\n\ndef sample_create_node():\n    # Create a client\n    client = tpu_v2alpha1.TpuClient()\n\n    # Initialize request argument(s)\n    node = tpu_v2alpha1.Node()\n    node.accelerator_type = \"accelerator_type_value\"\n    node.runtime_version = \"runtime_version_value\"\n\n    request = tpu_v2alpha1.CreateNodeRequest(\n        parent=\"parent_value\",\n        node=node,\n    )\n\n    # Make the request\n    operation = client.create_node(request=request)\n\n    print(\"Waiting for operation to complete...\")\n\n    response = operation.result()\n\n    # Handle the response\n    print(response)\n\nAny help would be much apprecciated!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How start Learn GENAI?",
        "Question_tag_count":2,
        "Question_created_time":"2023-06-08T07:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-start-Learn-GENAI\/m-p\/601296#M2106",
        "Question_answer_count":2,
        "Question_score_count":1,
        "Question_view_count":151,
        "Question_body":"How to start learning GENAI using Google with free?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to insert JSON along with PDF into Document AI Warehouse using API",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-19T07:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-insert-JSON-along-with-PDF-into-Document-AI-Warehouse\/m-p\/545296#M1699",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":260,
        "Question_body":"Hi,\u00a0\n\nOur usecase is to process the PDF documents from Document AI process and pass the JSON file along with the PDF to the document warehouse. I am using\u00a0contentwarehouse.CreateDocumentRequest function, the function works well if I only supply the PDF document, but if I process the file from the document and push the JSON along with the PDF the it gives an error saying the following:\n\n\nFile \"<ipython-input-63-d034c5106990>\", line 1, in <module>\nrunfile('C:\/Users\/HP\/Downloads\/test simple.py', wdir='C:\/Users\/HP\/Downloads')\n\nFile \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\nexecfile(filename, namespace)\n\nFile \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\nexec(compile(f.read(), filename, 'exec'), namespace)\n\nFile \"C:\/Users\/HP\/Downloads\/test simple.py\", line 110, in <module>\nprocess_document_sample(config['project_id'],config['location'],config['Custom_processor_id'],file_path,mime_type)\n\nFile \"C:\/Users\/HP\/Downloads\/test simple.py\", line 88, in process_document_sample\ndoc=documentai.types.Document(docDictionary)\n\nFile \"D:\\Anaconda3\\lib\\site-packages\\proto\\message.py\", line 566, in __init__\n\"Unknown field for {}: {}\".format(self.__class__.__name__, key)\n\nValueError: Unknown field for Document: _pb\n\n\u00a0\n\n\nFollowing is the code Snippet:\n\ndef process_document_sample(\nproject_id: str,\nlocation: str,\nprocessor_id: str,\nfile_path: str,\nmime_type: str,\nfield_mask: str = None,\n\n# You must set the api_endpoint if you use a location other than 'us'.\nopts = storage.Client(project_id)\n\nclient = documentai.DocumentProcessorServiceClient()\n\n# The full resource name of the processor, e.g.:\n# projects\/{project_id}\/locations\/{location}\/processors\/{processor_id}\nname = client.processor_path(project_id, location, processor_id)\n\n# Read the file into memory\nwith open(file_path, \"rb\") as image:\nimage_content = image.read()\n\n# Load Binary Data into Document AI RawDocument Object\nraw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n\n# Configure the process request\nrequest = documentai.ProcessRequest(\nname=name, raw_document=raw_document, field_mask=field_mask\n)\n\nresult = client.process_document(request=request)\n\n# return result\n# TODO(developer): Uncomment these variables before running the sample.\n# project_number = 'YOUR_PROJECT_NUMBER'\n# location = 'YOUR_PROJECT_LOCATION' # Format is 'us' or 'eu'\n#print(result.document.entities)\n\n# Create a Schema Service client\nimport json\nwith open('test.json','w') as f:\njson.dump(documentai.Document.to_dict(result.document),f)\n# documentai.Document.to_dict(result)\ndocument_schema_client = contentwarehouse.DocumentSchemaServiceClient()\n\n# The full resource name of the location, e.g.:\n# projects\/{project_number}\/locations\/{location}\nparent = document_schema_client.common_location_path(\nproject=config['project_number'], location=config['location']\n)\n\n\u00a0\n\n# Create a Document Service client\ndocument_client = contentwarehouse.DocumentServiceClient()\n\n# The full resource name of the location, e.g.:\n# projects\/{project_number}\/locations\/{location}\nparent = document_client.common_location_path(\nproject=config['project_number'], location=config['location']\n)\n#print(result.document._pb)\ndocDictionary = result.document.__dict__\ndoc=documentai.types.Document(docDictionary)\n# Define Document\ndocument = contentwarehouse.Document(\n# raw_document_file_type=1,\ndisplay_name=\"60.pdf\",\ndocument_schema_name=schema_URI,\ninline_raw_document=open('60.pdf','rb').read(),\n#plain_text=str(result.document)\ncloud_ai_document=doc\n)\n\n# Define Request\ncreate_document_request = contentwarehouse.CreateDocumentRequest(\nparent=parent, document=document\n)\n\n# Create a Document for the given schema\nresponse = document_client.create_document(request=create_document_request)\n# print(response)\n\n\nprocess_document_sample(config['project_id'],config['location'],config['Custom_processor_id'],file_path,mime_type)\n\n\nI have read all the documentation, but couldn't find why the dictionary is not being picked by the object.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Personal information \/ security assessment for GC translation service",
        "Question_tag_count":1,
        "Question_created_time":"2023-01-25T20:47:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Personal-information-security-assessment-for-GC-translation\/m-p\/514131#M1142",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":103,
        "Question_body":"Hi Team,\n\nI was searching for PIA and data retention policy for translation service. Any resources list for the same would be appreciated.\n\nAlso, what process i can follow to ensure maximum confidentiality and higher security to limit the data exposure limit.\n\nRegards",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"VertexAI: a way to post-process results without usign custom containers?",
        "Question_tag_count":1,
        "Question_created_time":"2021-12-24T10:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-a-way-to-post-process-results-without-usign-custom\/m-p\/181244#M132",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":99,
        "Question_body":"Hello,\n\nI have been searching on how to deploy models on VertexAI in AI Platform manner. Most tutorials shows using pre-built container which seems to load the model and return inference results.\n\nMy current requirement needs to post-process. This was easy to do with AI platform's Predictor class. Is something like that doable with pre-built containers on VertexAI (where we upload a package for inference by inheriting Predictor class and specifyinh the class name)? Using custom container makes it complex to handle and response to requests.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow ES Training: End-User Result is Different than what shows on the Training Page",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-20T10:33:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-ES-Training-End-User-Result-is-Different-than-what\/m-p\/604935#M2199",
        "Question_answer_count":1,
        "Question_score_count":1,
        "Question_view_count":90,
        "Question_body":"Hello,\n\nI ran into a discrepancy when testing my Dialogflow ES agent between the result I am seeing on the training page and what ten end-user is seeing and would like to see if anybody here has an explanation of what might be going on.\u00a0\n\nHere's my scenario:\n\nTest phrase being used:\n\"What are the contract access fees in assisted acquisitions\"\n\nThe result I'm seeing on the training page, which is correct where the end-user should be going to the Assisted Acquisitions flow:\n\n\n\n\n\nThe actual result the end-user seeing on the chatbot, which is our default fallback instead Assisted Acquisitions:\n\n\n\n\n\nDoes anybody know what's happening here to get this result?\u00a0\u00a0Is there anything I can do to make this consistent?. I did make sure the Assisted Acquisitions flow is available by having it as an output context.\n\nI have another flow in the system that has \"Contract Access Fees\" as a training phrase, but it is not accessible from where the user is coming from in this case.\u00a0 Could the system be confused on this that that is causing this error message?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to organize intents\/pages for a non-service\/support application",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-21T04:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-organize-intents-pages-for-a-non-service-support\/m-p\/491306#M827",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":121,
        "Question_body":"I am a new Dialogflow user and I need to create an agent for an application that is not service or support oriented.\u00a0 The application is educational and has a large collection of questions and answers (1,000s) with no concrete conclusion (for example,\u00a0 to renew a driver's license).\u00a0 For the POC I did in Watson I was able to use folders to organize sub-topics.\u00a0 What is the best way to group intents and responses by topic and sub-topic (for example, President Lincoln's early life President Lincoln's career)?\u00a0 I expect it would be difficult to manage a list of 1,000s of pages in the left pane.\u00a0 Thank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Extracting data from newspaper articles",
        "Question_tag_count":1,
        "Question_created_time":"2023-07-06T12:53:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Extracting-data-from-newspaper-articles\/m-p\/609844#M2312",
        "Question_answer_count":4,
        "Question_score_count":1,
        "Question_view_count":94,
        "Question_body":"I am working on a historical project and am hoping for some tips.\u00a0 We have about 2500 newspaper articles as PDFs from the last 100 years, but we would like to pull some basic information (article date, meeting speaker and topic, location, etc) for analysis to show trends over the years.\u00a0 I was hoping to be able to at least do a rough pull using ML, but to be honest I am stuck trying to find my starting point.\u00a0 Any tips or guidance would be greatly appreciated!\u00a0 Identifying the useful data from unstructured newspaper articles is something I am hoping to technology can do at this point.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Getting error in fetching the text-bison model for custom tuning using python.",
        "Question_tag_count":7,
        "Question_created_time":"2023-06-20T09:34:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-error-in-fetching-the-text-bison-model-for-custom-tuning\/m-p\/604919#M2197",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":239,
        "Question_body":"I am trying to tune a model using Python. Incorporated the code that has been given in the documentation as below:\n\n\u00a0\n\nfrom __future__ import annotations\n\n\n\nfrom google.auth import default\nimport pandas as pd\nimport vertexai\nfrom vertexai.preview.language_models import TextGenerationModel\n\n\ncredentials, _ = default(scopes=[\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"])\n\n\n\ndef tuning(\n\u00a0 \u00a0 project_id: str,\n\u00a0 \u00a0 location: str,\n\u00a0 \u00a0 training_data: pd.DataFrame | str,\n\u00a0 \u00a0 train_steps: int = 10,\n) -> None:\n\u00a0 \u00a0 \"\"\"Tune a new model, based on a prompt-response data.\n\u00a0 \u00a0 \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n\u00a0 \u00a0 (for example: training_data=f'gs:\/\/bb_bucket2023\/Vertex AI data (3).jsonl'), or a pandas\n\u00a0 \u00a0 DataFrame. Each training example should be JSONL record with two keys, for\n\u00a0 \u00a0 example:\n\u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \"input_text\": <input prompt>,\n\u00a0 \u00a0 \u00a0 \u00a0 \"output_text\": <associated output>\n\u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 or the pandas DataFame should contain two columns:\n\u00a0 \u00a0 \u00a0 ['input_text', 'output_text']\n\u00a0 \u00a0 with rows for each training example.\n\u00a0 \u00a0 Args:\n\u00a0 \u00a0 \u00a0 project_id: GCP Project ID, used to initialize vertexai\n\u00a0 \u00a0 \u00a0 location: GCP Region, used to initialize vertexai\n\u00a0 \u00a0 \u00a0 training_data: GCS URI of jsonl file or pandas dataframe of training data\n\u00a0 \u00a0 \u00a0 train_steps: Number of training steps to use when tuning the model.\n\u00a0 \u00a0 \"\"\"\n\u00a0 \u00a0 vertexai.init(project=project_id, location=location, credentials=credentials)\n\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n\n\n\u00a0 \u00a0 model.tune_model(\n\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\n\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\n\u00a0 \u00a0 \u00a0 \u00a0 train_steps=train_steps,\n\u00a0 \u00a0 \u00a0 \u00a0 tuning_job_location=\"europe-west4\", \u00a0# Only supported in europe-west4 for Public Preview\n\u00a0 \u00a0 \u00a0 \u00a0 tuned_model_location=location,\n\u00a0 \u00a0 )\n\n\n\u00a0 \u00a0 print(model._job.status)\n\u00a0 \u00a0 # [END aiplatform_sdk_tuning]\n\u00a0 \u00a0 return model\n\n\n\nif __name__ == \"__main__\":\n\u00a0 \u00a0 tuning(project_id=\"xxxxxxxxx\",\n\u00a0 \u00a0 \u00a0 \u00a0 location=\"europe-west4\", \u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 training_data='gs:\/\/xxxxxx\/Vertex AI data (3).jsonl')\n\nAfter running this code I get an error as below:\n\n\n---------------------------------------------------------------------------\n\n_InactiveRpcError                         Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     71         try:\n---> 72             return callable_(*args, **kwargs)\n     73         except grpc.RpcError as exc:\n\n\n11 frames\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Publisher Model `publishers\/google\/models\/text-bison@001` is not found.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.8.95:443 {created_time:\"2023-06-20T16:09:33.304268493+00:00\", grpc_status:5, grpc_message:\"Publisher Model `publishers\/google\/models\/text-bison@001` is not found.\"}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\n\nNotFound                                  Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     72             return callable_(*args, **kwargs)\n     73         except grpc.RpcError as exc:\n---> 74             raise exceptions.from_grpc_error(exc) from exc\n     75 \n     76     return error_remapped_callable\n\n\nNotFound: 404 Publisher Model `publishers\/google\/models\/text-bison@001` is not found.\n\nPlease guide me through the steps that can resolve the above issue.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Vertex AI Resource Exhaustion Error but resources are not even close to exhausted?...",
        "Question_tag_count":3,
        "Question_created_time":"2023-05-04T02:22:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Resource-Exhaustion-Error-but-resources-are-not-even\/m-p\/550175#M1784",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":104,
        "Question_body":"I'm attempting to run a basic pipeline using Kubeflow in Vertex AI. However, when I run it I receive a\u00a0RESOURCE_EXHAUSTED error in the logs relating to\u00a0aiplatform.googleapis.com\/custom_model_training_cpus.\n\nChecking my quotas, I can see that I am not anywhere close to exhausting any of the quotas under\u00a0\u00a0aiplatform.googleapis.com\/custom_model_training_cpus\u00a0(including the region I'm using - us-central1).\n\nHas anyone had a similar issue and know what is going on here?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What are you looking forward to in Google Cloud NEXT '21?",
        "Question_tag_count":1,
        "Question_created_time":"2021-06-23T09:19:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-are-you-looking-forward-to-in-Google-Cloud-NEXT-21\/m-p\/161539#M3",
        "Question_answer_count":0,
        "Question_score_count":2,
        "Question_view_count":401,
        "Question_body":"Google Cloud Next conference is happening this year on October 12-14, 2021!\n\nWhat exciting new technologies are you looking forward to?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Agent Assist - Simulator View - no smart reply suggestion",
        "Question_tag_count":5,
        "Question_created_time":"2023-02-01T05:03:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Agent-Assist-Simulator-View-no-smart-reply-suggestion\/m-p\/516396#M1164",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":95,
        "Question_body":"I was trying to use \"smart reply\" option in the google agent assist but no suggestion were populated.\n\nI have tried \"smart reply\" using sample data with option \"I would like to try it with sample data\" which autopopulated the conversation UI as snapshot given below. After this it took good amount of time approx 10-12 hrs to train with given data. Once conversation profile was set with trained data, I tried smart reply on simulator view but was not getting any suggestion as smart reply. Please suggest where I am going wrong.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"AutoML Translation: 30% of all our requests finishes with timeout",
        "Question_tag_count":3,
        "Question_created_time":"2023-03-29T07:42:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Translation-30-of-all-our-requests-finishes-with-timeout\/m-p\/538365#M1523",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":104,
        "Question_body":"In our project we use 21 custom trained models to translate from EN to target_language.\n\nLast week 30% of all our requests finishes with timeout! What is the problem?!\n\nHow can you\/we fix it?",
        "Question_closed_time":"03-30-2023 12:40 PM",
        "Answer_score_count":1.0,
        "Answer_body":"Hi\u00a0@ochkarik\u00a0\n\nWelcome back to Google Cloud Community.\n\nSetting request timeout (services)\nFor Cloud Run services, the request timeout setting specifies the time within which a response must be returned by services deployed to Cloud Run. If a response isn't returned within the time specified, the request ends and error 504 is returned.\n\nThe timeout is set by default to 5 minutes and can be extended up to 60 minutes.\n\nHere are some articles that might help you:\nhttps:\/\/cloud.google.com\/run\/docs\/configuring\/request-timeout\n\nhttps:\/\/cloud.google.com\/python\/docs\/reference\/storage\/1.39.0\/retry_timeout?_ga=2.21056062.-48059091...\n\nhttps:\/\/cloud.google.com\/translate\/docs\/reference\/rpc\/google.longrunning?_ga=2.25258296.-480590913.1...\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"How to use ARIMA coefficients from BigQuery",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-19T12:20:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-ARIMA-coefficients-from-BigQuery\/m-p\/468311#M586",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":115,
        "Question_body":"I am trying to use Auto ARIMA from BigQuery and I just want to understand the results. That's what BigQuery is giving me:\n\nStore A\n\nNon Seasonal P = 0\nNon Seasonal D = 1\nNon Seasonal Q = 1\nHas Drift = False\nLog Likelihood = -7,073.79\nAIC = 14,153.579\nVariance = 4,954,056.028\nSeasonal Period = Yearly\n\nCoeficients from Store A\n\nar_coefficients = NULL\nma_coefficients = -0.80490889915091468\nintercept_or_drift = 42.211438084963184\n\nI trained the model using\u00a0weekly\u00a0income\n\nHow to fit this information in an equation?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"VertexAI notebook does not obtain Shared VPC IP",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-05T07:50:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VertexAI-notebook-does-not-obtain-Shared-VPC-IP\/m-p\/550677#M1799",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":96,
        "Question_body":"Hello,\u00a0\n\nwhen I create a new VertexAI notebook I can select:\n\nnetworking:\u00a0 Shared network (from host project: XXXC)\u00a0\n\nGCP then asks me the network and the subnet.\nIn the subnet dropdown the tooltip reads:\n\nAssigns the notebook an IPv4 address from the subnetwork's range. Notebooks in different subnetworks can communicate with each other using their internal IPs as long as they belong to the same network.\n\nBut this does not seem to work at all ?\n\nOnce I create the notebook I can run ifconfig in the terminal and there it's clear that the notebook never got an IP from the subnet I selected above.\n\nAnyone knows how I can use this feature?\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Can the Google Translate API v2 be used in the frontend?",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-29T03:01:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-the-Google-Translate-API-v2-be-used-in-the-frontend\/m-p\/597834#M2039",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":95,
        "Question_body":"Hello? By any chance, can the Google Translate API v2 be used in the frontend?\n\nInitially, I thought the Google Translate API v2 couldn't be used in the frontend due to CORS issues. However, when I input the code below into the browser, the API is being called normally.\n\nI'm curious if it's okay to call the API directly from the browser from the beginning.\n\nconst url = 'https:\/\/translation.googleapis.com\/language\/translate\/v2?key={{API_KEY}}';\nconst headers = {\n  'Content-Type': 'application\/json; charset=utf-8',\n};\n\nfetch(url, {\n  method: 'POST',\n  headers: headers,\n  body: JSON.stringify({\n    \"q\": [\"Hello world\", \"My name is Jeff\"],\n    \"target\": \"de\"\n  }),\n})\n  .then((response) => response.json())\n  .then((data) => console.log(data))\n  .catch((error) => console.error(error));",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"GPU Machines in Vertex AI Pipelines failing with RuntimeError: NCCL Error 2: unhandled system error",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-12T12:24:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/GPU-Machines-in-Vertex-AI-Pipelines-failing-with-RuntimeError\/m-p\/543230#M1645",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":113,
        "Question_body":"Hi AI platform, I am using Vertex AI Pipelines to train detection models, and a since today my pipelines are failing due to a NCCL error on the GPU machine, I am creating the task as follows:\n\n\n\ntrain_task = train_model(\n        centernet_container_trainer_uri=centernet_uri_task.output,\n        train_dataset=train_importer_task.output,\n        test_dataset=test_importer_task.output,\n        categories_json=categories_importer_task.output,\n        pretrained_model=pretrained_model.output,\n        num_iters=num_iters,\n        batch_size=batch_size,\n        lr=lr,\n        num_epochs=num_epochs,\n        lr_step=lr_step,\n        gpus=gpus,\n        num_workers=num_workers,\n        val_intervals=val_intervals,\n    )\n    train_task.set_display_name(\"Train model\").set_cpu_limit(\"12\").set_memory_limit(\n        \"170G\"\n    ).add_node_selector_constraint(\"NVIDIA_TESLA_A100\").set_gpu_limit(\n        \"2\"\n    ).set_env_variable(\n        name=\"NCCL_SHM_DISABLE\", value=\"1\"\n    )\n\nThen after a few minutes my training step fails due to:\n\nNCCL Error 2: unhandled system error\n\nWhich seems to be related to shm size on the container. I have tried setting and unsetting the variable `NCCL_SHM_DISABLE` but to no avail, any idea on how to get support for this issue?\u00a0\n\nThanks\n\nI am using vertex and kubeflow",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How do you get access to bard Api",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-20T20:26:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-do-you-get-access-to-bard-Api\/m-p\/605111#M2201",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"How do you get access to bard Api",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow CX logs sink to BigQuery. sink error - field: value is not a record",
        "Question_tag_count":1,
        "Question_created_time":"2022-11-16T07:48:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-logs-sink-to-BigQuery-sink-error-field-value-is\/m-p\/490079#M798",
        "Question_answer_count":4,
        "Question_score_count":0,
        "Question_view_count":405,
        "Question_body":"I am using google cloud logging to sink Dialogflow CX requests data to big query. BigQuery tables are auto generated when you create the sink via Google Logging.\n\nWe keep getting a sink error - field: value is not a record.\n\nThis is because pageInfo\/formInfo\/parameterInfo\/value is of type String in BigQuery BUT there are values that are records, not strings. One example is @sys.date-time\n\nHow do we fix this?\n\nWe have not tried anything at this point since the BigQuery dataset is auto created via a Logging Filter. We cannot modify the logs and if we could modify the table schema, what would we change it to since most of the time \"Value\" is a String but other times it is a Record",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Genesys - Dialogflow cx error",
        "Question_tag_count":1,
        "Question_created_time":"2023-06-27T00:45:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Genesys-Dialogflow-cx-error\/m-p\/606840#M2239",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":47,
        "Question_body":"I have an integration with Genesys and Dialogflow CX working on voice, chat, and WhatsApp channels. I have encountered the following problem: suddenly, after working correctly for over a month, some voice requests from the Genesys voice channel stopped reaching the bot, while other voice requests and all WhatsApp and chat requests were functioning properly. The Dialogflow agent remains the same. Genesys showed a connectivity error. We had the issue for three days, escalated it to Genesys, but they couldn't resolve it. Suddenly, today it resolved itself without making any changes. However, we don't know what could have triggered it. Has anyone else experienced this or have any idea what could have caused it? Thank you.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Using other API to translate PDF documents",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-01T23:17:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Using-other-API-to-translate-PDF-documents\/m-p\/484845#M714",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":226,
        "Question_body":"Hello all,\u00a0\n\nI am trying to find a way to translate English PDF documents to a target language(Korean) without messing up the original PDF page format(pictures, headers, tables, etc.)\n\nThe only problem with the default google translation is that many of the words that appear in the document are very industry-specific and need to be translated accordingly through AutoML translation.\u00a0\n\nHowever, we'd like to use our own language model (i.e. fine-tuned GPT3) to translate just the text and feed the translated text to the output stream to get the final pdf output.\n\nI'm yet to see any other company that maintains PDF formatting as well as Google while translating, so I'd really like to use Cloud Translation API with our own translation module for optimal accuracy.\n\nIs there a way to do this? I've tried reaching out to the local Google branch to no avail. Please help!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How to compare and evaluate all trial models in AutoML?",
        "Question_tag_count":2,
        "Question_created_time":"2023-04-05T17:40:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-compare-and-evaluate-all-trial-models-in-AutoML\/m-p\/541098#M1596",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":76,
        "Question_body":"I have ran an AutoML forecasting job to predict sales. The training job modeled 69 models (from trial models) and 9 of them were selected to create an ensemble model as final model.\n\nQ1: I want to evaluate and compare all trial models that autoML performed in the training, is it possible?\nQ2: From the logs, I have all 69 models with their hyperparameters, how do I recreate all these model and get another predictions over it?\n\nThank you for your interest and please let me know if you have more questions.\n\n\u00a0\n\nlogs from the trial training job with total 69 trial models and hyperparameters",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Code Chat Generative AI model",
        "Question_tag_count":3,
        "Question_created_time":"2023-07-16T23:14:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Code-Chat-Generative-AI-model\/m-p\/612812#M2380",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":28,
        "Question_body":"Hi All,\n\nI would like to train Gen AI model which can answer\/create code based on the table fed as input.\nHow to achieve this?\n\nExample: If i ask prompt to write a sql query to join two tables. It should join two tables from the bigquery table which i mentioned and the required fields.\n\nThanks,\n\nRajavelu",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unprocessed or partial text in a few images",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-12T02:57:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unprocessed-or-partial-text-in-a-few-images\/m-p\/521178#M1269",
        "Question_answer_count":3,
        "Question_score_count":0,
        "Question_view_count":97,
        "Question_body":"I attached a few images that are a bit problematic for vision api, note that i'll keep updating this topic with more images and their area of problem if there are.\n\nDescription for each image is in its caption.\n\nthe string \"omerat\" not being picked\n\nthe string \"jEWONDER\" is being picked with an upper-case \"J\"",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"timeSegments vs timeSegmentAnnotations",
        "Question_tag_count":2,
        "Question_created_time":"2022-11-16T08:02:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/timeSegments-vs-timeSegmentAnnotations\/m-p\/490092#M799",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":75,
        "Question_body":"timeSegments vs timeSegmentAnnotations\n\nCan anyone explain what's the difference between these 2 fields described here?\u00a0https:\/\/storage.cloud.google.com\/google-cloud-aiplatform\/schema\/dataset\/ioformat\/video_action_recogn...\n\nwhy would I want to tag timeSegments? what's the objective of this? associate a label to a time segment?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Unable to increase quota for base_mode: chat-bison and text-bison",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-16T10:11:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unable-to-increase-quota-for-base-mode-chat-bison-and-text-bison\/m-p\/553927#M1926",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":140,
        "Question_body":"Hi,\n\nI have paid GCP account. I am unable to use any Vertex AI services where\u00a0chat-bison and text-bison are required. The quota is set at 0 and I am not able to increase it.",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"What is the best way to use credentials for API calls from databricks notebook?",
        "Question_tag_count":1,
        "Question_created_time":"2023-04-19T07:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-is-the-best-way-to-use-credentials-for-API-calls-from\/m-p\/545303#M1700",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":383,
        "Question_body":"Hello, I have an Databricks account on Azure, and the goal is to compare different image tagging services from GCP and other providers via corresponding API calls, with Python notebook. I have problems with GCP vision API calls, specifically with credentials: as far as I understand, the one necessary step is to set 'GOOGLE_APPLICATION_CREDENTIALS' environment variable in my databricks notebook with something like\u00a0\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='\/folder1\/credentials.json'\u00a0\n\nwhere '\/folder1\/credentials.json' is the place my notebook looks for json file with credentials (notebook is in the same folder,\u00a0\/folder1\/notebook_api_test).\n\nI am getting this path by looking into\u00a0Workspace->\u00a0Copy file path\u00a0in the Databricks web page.\n\nBut this approach doesn't work, when cell is executed, I am getting this error:\u00a0\n\nDefaultCredentialsError: File \/folder1\/credentials.json was not found.\u00a0\n\nWhat is the right way to deal with credentials to access google vision API from Databricks notebook?",
        "Question_closed_time":"04-20-2023 04:17 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Ok, here is a trick: in my case, the file with GCP credentials is stored in notebook workspace storage, which is not visible to os.environ() command. So solution is to read a content of this file, and save it to the cluster storage attached to the notebook, which is created with the cluster and is erased when cluster is gone (so we need to repeat this procedure every time the cluster is re-created). According to this doc, we can read the content of the credentials json file stored in notebook workspace with\n\n\u00a0 \u00a0 \u00a0 \u00a0 with open('\/Workspace\/folder1\/cred.json'): #note that I need a full path here, for some reason\n\u00a0 \u00a0 \u00a0 \u00a0 content = f.read()\n\nand then according to this doc, we need to save it on another place in a new file (with the same name in my case, cred.json), namely on cluster storage attached to the notebook (which is visible to os-related functions, like os.environ()), with\n\n\u00a0 \u00a0 \u00a0 \u00a0 fd = os.open(\"cred.json\", os.O_RDWR|os.O_CREAT)\n\u00a0 \u00a0 \u00a0 \u00a0 ret = os.write(fd,content.encode())\n\u00a0 \u00a0 \u00a0 \u00a0 #need to add .encode(), or will get TypeError: a bytes-like object is required, not 'str'\n\u00a0 \u00a0 \u00a0 \u00a0 os.close(fd)\n\nOnly after that we can continue with setting an environment variable, required for GCP authentication:\n\n\u00a0 \u00a0 \u00a0 \u00a0 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='.\/cred.json'\n\nand then API calls should work fine, without DefaultCredentialsError.\n\nView solution in original post",
        "Question_self_closed":1.0
    },
    {
        "Question_title":"[VERTEX AI] Is it possible to load in GCP Vertex AI Dataset a set of files along with their training",
        "Question_tag_count":2,
        "Question_created_time":"2023-07-10T02:44:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VERTEX-AI-Is-it-possible-to-load-in-GCP-Vertex-AI-Dataset-a-set\/m-p\/610653#M2330",
        "Question_answer_count":3,
        "Question_score_count":2,
        "Question_view_count":264,
        "Question_body":"I have to report here my homonym StackOverflow question:\u00a0https:\/\/stackoverflow.com\/questions\/76413616\/is-it-possible-to-load-in-gcp-vertex-ai-dataset-a-set-o...\u00a0\n\nDoes anybody know how to solve it?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Dialogflow cx Language Issue in Console",
        "Question_tag_count":1,
        "Question_created_time":"2023-02-01T01:29:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-Language-Issue-in-Console\/m-p\/516356#M1162",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":175,
        "Question_body":"\"Dialog flow CX having problems while changing language in the console. It doesn't matter how many times I change it in the console it just comes back to English(Default Language). Previously I was able to change it pretty easily till01-02-2023.\n\nNow it just gives an Error: An error has occurred during knowledge base API calls.\"\n\n\u00a0\n\n@",
        "Question_closed_time":"02-02-2023 10:20 PM",
        "Answer_score_count":0.0,
        "Answer_body":"Hi,\n\nI have seen other cases regarding this issue in changing language in Dialogflow CX which also occurred on 01-02-2023 and this was categorized as a bug issue. According to the latest update regarding this, the bug has been fixed and rolled out to production.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"7 days left: learn new skills with free access to data, ML and AI labs!",
        "Question_tag_count":1,
        "Question_created_time":"2021-08-02T09:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/7-days-left-learn-new-skills-with-free-access-to-data-ML-and-AI\/m-p\/165897#M33",
        "Question_answer_count":0,
        "Question_score_count":0,
        "Question_view_count":392,
        "Question_body":"Don't miss this great, free ML\/AI learning opportunity!\n\n\u00a0\n\nCrossposting from the Learning Forums:\n\nhttps:\/\/www.googlecloudcommunity.com\/gc\/Learning-Forums\/Your-mission-should-you-choose-to-accept-it\/...",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Matching Engine: Queries with filtering do not work as expected",
        "Question_tag_count":3,
        "Question_created_time":"2022-11-10T07:31:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Matching-Engine-Queries-with-filtering-do-not-work-as-expected\/m-p\/488086#M758",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":290,
        "Question_body":"I tried to run this notebook: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/matching_engi...\nI got an issue with the filtering step:\nLet's say glove100.json is:\n\n{\"id\":\"0\",\"embedding\":[-0.99544,-2.3651],\"restricts\":[{\"namespace\": \"class\", \"allow_list\": [\"0\"]}],\"crowding_tag\":\"a\"}\n{\"id\":\"1\",\"embedding\":[0.42052,-1.1817],\"restricts\":[{\"namespace\": \"class\", \"allow_list\": [\"1\"]}],\"crowding_tag\":\"b\"}\n{\"id\":\"2\",\"embedding\":[-0.10185,0.59817],\"restricts\":[{\"namespace\": \"class\", \"allow_list\": [\"2\"]}],\"crowding_tag\":\"a\"}\n\nIf I try to filter in this way:\n\nfor val in query:\n    request.float_val.append(val)\n\nrestrict = match_service_pb2.Namespace()\nrestrict.name = \"class\"\nrestrict.allow_tokens.append(\"1\")\nrequest.restricts.append(restrict)\nresponse = stub.Match(request)\nresponse\n\n\nI do not get any response (empty result). If from the above code I remove the \"restrict block\", it works (of course without filtering).\n\nrequest = match_service_pb2.MatchRequest()\nrequest.deployed_index_id = DEPLOYED_INDEX_ID\nfor val in query:\n   request.float_val.append(val)\n# restrict = match_service_pb2.Namespace()\n# restrict.name = \"class\"\n# restrict.allow_tokens.append(\"1\")\n# request.restricts.append(restrict)\nresponse = stub.Match(request)\nresponse\n\n\nresponse:\n\nneighbor {\nid: \"0\"\ndistance: 17.592369079589844\n}\nneighbor {\nid: \"1\"\ndistance: 17.592369079589844\n}\n\nBut, if I add a new vector by Vertex SDK for Python (link\u00a0), in this way:\n\ninsert_datapoints_payload = aiplatform_v1.IndexDatapoint(\ndatapoint_id=\"3\",\nfeature_vector=query,\nrestricts=[{\"namespace\": \"class\", \"allow_list\": [\"3\"]}],\n# crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"), <-- this does not seem to change anything\n)\n\nupsert_request = aiplatform_v1.UpsertDatapointsRequest(\nindex=INDEX_RESOURCE_NAME, datapoints=[insert_datapoints_payload]\n)\n\nindex_client.upsert_datapoints(request=upsert_request)\n\nand then I try to filter in the same way above with filter class == \"3\", I get the right response.\n\nIt seems like the allow_tokens are \"seen\" by vertex only when I insert a new vector by Vertex SDK and not when I specify them in the initial glove100.json.\n\nMoreover, if I update a datapoint where the filter did not work, for example id=1:\n\nupdate_datapoints_payload = aiplatform_v1.IndexDatapoint(\ndatapoint_id=\"1\",\nfeature_vector=embedding,\nrestricts=[{\"namespace\": \"class\", \"allow_list\": [\"1\"]}],\n# crowding_tag=aiplatform_v1.IndexDatapoint.CrowdingTag(crowding_attribute=\"b\"),\n)\n\nupsert_request = aiplatform_v1.UpsertDatapointsRequest(\nindex=INDEX_RESOURCE_NAME, datapoints=[update_datapoints_payload]\n)\n\nindex_client.upsert_datapoints(request=upsert_request)\n\nresponse = stub.Match(request)\nresponse\n\n\nThe filter for class=1 starts to work.\n\nIs there a way to know what is actually stored in the index? I mean a kind of \"SELECT * FROM myindex\" in order to check embeddings and tokens stored.\n\nAny ideas on how to solve this issue?\n\nThanks in advance\n\nSpecifications\n\nI tried from local and from workbench, the result is the same.\n\n- Version: Python 3.7.9\n- Platform: Matching Engine. zone: europe-west1",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"How does google cloud speech to text api deals with invalid inputs ?",
        "Question_tag_count":1,
        "Question_created_time":"2022-09-25T10:32:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-does-google-cloud-speech-to-text-api-deals-with-invalid\/m-p\/470810#M600",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":229,
        "Question_body":"I am using google cloud\u00a0stream(AsyncStreamingRecognize) for speech to text conversion in my applications. I have gone through the below link to understand the structure of response returned by the apis :\n\nStreamingRecognizeResponse \u00a0\n\nI can have various scenarios where I can end up with various invalid scenarios and I do not understand what could be the responses. I can invalid scenarios like :\n\n- User speaks in a different language than what is passed in configuration .\n\n- User does not speak anything \/ no input\n\n- Only noise gets passed \/ Data loss\n\nIs there any parameter inside my response which can point to above scenarios ?",
        "Question_closed_time":"09-27-2022 02:56 PM",
        "Answer_score_count":0.0,
        "Answer_body":"1.- If a user speaks a different language you can use language recognition in audio requests. Speech-to-Text supports alternative language codes for all speech recognition methods. Also, one good practice is to show a phrase that can be used or advice on what language you select to be recognized by Speech-to-Text.\u00a0\n\n2.- There are multiple ways that Speech to text can return an empty response. The source of the problem could be the RecognitionConfig\u00a0or the audio itself.\n\n3.-To avoid that only the noise gets passed and the data is lost you can pre-process the audio just as the best practices doc\u00a0mentions.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Console Speech to text - transcription",
        "Question_tag_count":1,
        "Question_created_time":"2023-05-15T12:54:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Console-Speech-to-text-transcription\/m-p\/553603#M1916",
        "Question_answer_count":1,
        "Question_score_count":0,
        "Question_view_count":57,
        "Question_body":"When I give a 45 second mp3 file to the transcription program, this program generates only 15 seconds of transcription.\u00a0 Why does it not transcribe the whole file ????",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"Agent Assist Summarization Model Fail to Deploy",
        "Question_tag_count":2,
        "Question_created_time":"2023-02-13T10:38:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Agent-Assist-Summarization-Model-Fail-to-Deploy\/m-p\/521639#M1276",
        "Question_answer_count":7,
        "Question_score_count":0,
        "Question_view_count":202,
        "Question_body":"Hello, I attempted to create a custom model in agent assist using the public summarization dataset (gs:\/\/summarization_integration_test_data\/data\/*). The model finished training, but it fails to deploy. The only error message I receive is \"operation failed\". What might be causing this issue\/is there away to see agent assist logs?",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    },
    {
        "Question_title":"DocumentAI - How to highlight bounding box after ProcessDocument(Request)",
        "Question_tag_count":3,
        "Question_created_time":"2023-04-09T18:58:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocumentAI-How-to-highlight-bounding-box-after-ProcessDocument\/m-p\/542055#M1610",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":146,
        "Question_body":"Hello,\n\nAfter following this example with success:\u00a0https:\/\/cloud.google.com\/document-ai\/docs\/libraries#client-libraries-usage-csharp\n\nQuestion is: How to highlight each detected returned value on source PDF as Google does in their example? If I press F12 on their example it shows tags like:\n\nai-document-view\nai-labeling-layout\nai-annotated-document-view\nai-document-tree-row-annotation-panel\nai-document-row-annotation\nai-connecting-line\n\nI'd like to perform something similar so my users will be able to fix values with low confidence or just let them improving the results viewing the highlighted results on PDF.\n\nAny clue or code snipped will be appreciated.\n\nkind regards,\n\nFrancisco",
        "Question_closed_time":"04-09-2023 08:07 PM",
        "Answer_score_count":0.0,
        "Answer_body":"If we look at the genetic response from a Document AI processing output we see that it is a structured document (JSON).\u00a0 Within the JSON output we get indications of what was detected but also\u00a0where\u00a0it was detected.\u00a0 This makes me think that the PDF is converted into an image representation and the image processed using OCR.\u00a0 The results would then be the bounding polygons of an image representation of where each items was found.\n\nView solution in original post",
        "Question_self_closed":0.0
    },
    {
        "Question_title":"Vertex AI Video Classification failed with error message: Internal error occurred",
        "Question_tag_count":2,
        "Question_created_time":"2023-05-31T17:12:00",
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Video-Classification-failed-with-error-message\/m-p\/598861#M2058",
        "Question_answer_count":2,
        "Question_score_count":0,
        "Question_view_count":75,
        "Question_body":"Hi,\n\nI got an error message when training using\u00a0VertexAI Video Classification. It run for many hours and then failed with the following message: \"Training pipeline failed with error message: Internal error occurred. \"\n\nThis is unfortunate since I'm worried that now I'll get charged the 22hours of training but without any results.\n\nShould I try to run it again? How can I get more information about the causes of failure?\n\nThanks!",
        "Question_closed_time":null,
        "Answer_score_count":null,
        "Answer_body":null,
        "Question_self_closed":null
    }
]